Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: hydra-core==1.1.1 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (1.1.1)
Requirement already satisfied: icecream~=2.1.1 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (2.1.5)
Requirement already satisfied: matplotlib~=3.5.0 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (3.5.3)
Requirement already satisfied: numpy~=1.21.4 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (1.21.6)
Requirement already satisfied: omegaconf~=2.1.1 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (2.1.2)
Requirement already satisfied: pandas~=2.0.3 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (2.0.3)
Requirement already satisfied: Pillow==8.2.0 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (8.2.0)
Requirement already satisfied: scipy==1.7.0 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (1.7.0)
Requirement already satisfied: seaborn~=0.11.2 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (0.11.2)
Requirement already satisfied: svglib==1.1.0 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (1.1.0)
Requirement already satisfied: tensorboard==2.2.2 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (2.2.2)
Requirement already satisfied: tensorboard-plugin-wit==1.8.0 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 12)) (1.8.0)
Requirement already satisfied: torch~=2.4.1 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 13)) (2.4.1)
Requirement already satisfied: torchvision==0.19.1 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 14)) (0.19.1)
Requirement already satisfied: tqdm~=4.62.3 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 15)) (4.62.3)
Requirement already satisfied: openml in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 16)) (0.15.1)
Requirement already satisfied: optuna in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 17)) (4.4.0)
Loading dataset: ic_upstream3 for task: regression at stage: pretrain
Target columns: 66
[08/12/2025 16:36:47 INFO]: Building Dataset
[08/12/2025 16:36:47 INFO]: pre normalizer.fit

[08/12/2025 16:36:47 INFO]: pos normalizer.fit

Normalizer saved to ../../../data/ic_upstream3/normalizer.pkl
dataser shape: 282
x shape: torch.Size([282, 100]), torch.Size([282, 0]), y shape: torch.Size([282, 1])
[08/12/2025 16:36:48 INFO]: Task: regression, Dataset: ic_upstream3, n_numerical: 100, n_categorical: 0, n_classes: 1, n_train_samples: 282, n_val_samples: 66, n_test_samples: 87
Train set features: torch.Size([282, 100]), 
Running trial 1/180
Running trial 1/180
Running trial 1/180
Running trial 1/180
Running trial 1/180
Running trial 1/180
Running trial 1/180
Running trial 1/180
Running trial 1/180
Running trial 1/180
[08/12/2025 16:36:49 INFO]: 
_________________________________________________

[08/12/2025 16:36:49 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:36:49 INFO]: 
_________________________________________________

[08/12/2025 16:36:49 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:36:49 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 12
  d_ffn_factor: 1.9822324352683127
  attention_dropout: 0.40655619925192205
  ffn_dropout: 0.40655619925192205
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0003917037976639179
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: -1
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: revolved-Rody

[08/12/2025 16:36:49 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 4
  d_ffn_factor: 1.873802219943948
  attention_dropout: 0.45353164974231497
  ffn_dropout: 0.45353164974231497
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00011070888813770057
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: revolved-Rody

[08/12/2025 16:36:49 INFO]: 
_________________________________________________

[08/12/2025 16:36:49 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:36:49 INFO]: 
_________________________________________________

[08/12/2025 16:36:49 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:36:49 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 2.61923853304754
  attention_dropout: 0.4199929106407093
  ffn_dropout: 0.4199929106407093
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 5.472817999009264e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: revolved-Rody

[08/12/2025 16:36:49 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 8
  d_ffn_factor: 1.2030227537351803
  attention_dropout: 0.3097260491115211
  ffn_dropout: 0.3097260491115211
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.4473341990664206e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: revolved-Rody

[08/12/2025 16:36:49 INFO]: 
_________________________________________________

[08/12/2025 16:36:49 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:36:49 INFO]: 
_________________________________________________

[08/12/2025 16:36:49 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:36:49 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 4
  d_ffn_factor: 2.5778506396706278
  attention_dropout: 0.18971357605831296
  ffn_dropout: 0.18971357605831296
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.0737124121097217e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: revolved-Rody

[08/12/2025 16:36:49 INFO]: 
_________________________________________________

[08/12/2025 16:36:49 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:36:49 INFO]: 
_________________________________________________

[08/12/2025 16:36:49 INFO]: 
_________________________________________________

[08/12/2025 16:36:49 INFO]: 
_________________________________________________

[08/12/2025 16:36:49 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:36:49 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:36:49 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 0.8663550642853822
  attention_dropout: 0.4743480359190597
  ffn_dropout: 0.4743480359190597
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00021432046573591673
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: revolved-Rody

[08/12/2025 16:36:49 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:36:49 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 1.0728724718098297
  attention_dropout: 0.2816706765849131
  ffn_dropout: 0.2816706765849131
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00020972477966317145
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: revolved-Rody

[08/12/2025 16:36:49 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.3032801700380832
  attention_dropout: 0.09256676393661445
  ffn_dropout: 0.09256676393661445
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00026963662655384897
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: revolved-Rody

[08/12/2025 16:36:49 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 0.9957126569236997
  attention_dropout: 0.05533102397217843
  ffn_dropout: 0.05533102397217843
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.5677973629970916e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: revolved-Rody

[08/12/2025 16:36:49 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 2.133941302571719
  attention_dropout: 0.07555298598593957
  ffn_dropout: 0.07555298598593957
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0003839557775831153
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: revolved-Rody

[08/12/2025 16:36:49 INFO]: This ft_transformer has 1.318 million parameters.
[08/12/2025 16:36:49 INFO]: Training will start at epoch 0.
[08/12/2025 16:36:49 INFO]: This ft_transformer has 0.328 million parameters.
[08/12/2025 16:36:49 INFO]: Training will start at epoch 0.
[08/12/2025 16:36:50 INFO]: This ft_transformer has 2.033 million parameters.
[08/12/2025 16:36:50 INFO]: Training will start at epoch 0.
[08/12/2025 16:36:50 INFO]: This ft_transformer has 5.963 million parameters.
[08/12/2025 16:36:50 INFO]: Training will start at epoch 0.
[08/12/2025 16:36:50 INFO]: This ft_transformer has 0.489 million parameters.
[08/12/2025 16:36:50 INFO]: Training will start at epoch 0.
[08/12/2025 16:36:50 INFO]: This ft_transformer has 2.583 million parameters.
[08/12/2025 16:36:50 INFO]: Training will start at epoch 0.
[08/12/2025 16:36:50 INFO]: This ft_transformer has 1.248 million parameters.
[08/12/2025 16:36:50 INFO]: Training will start at epoch 0.
[08/12/2025 16:36:50 INFO]: This ft_transformer has 0.289 million parameters.
[08/12/2025 16:36:50 INFO]: Training will start at epoch 0.
[08/12/2025 16:36:50 INFO]: This ft_transformer has 0.786 million parameters.
[08/12/2025 16:36:50 INFO]: Training will start at epoch 0.
[08/12/2025 16:36:50 INFO]: This ft_transformer has 4.102 million parameters.
[08/12/2025 16:36:50 INFO]: Training will start at epoch 0.
[08/12/2025 16:36:53 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:36:53 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:36:53 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:36:53 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:36:53 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:36:53 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:36:53 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:36:53 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:36:53 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:36:53 INFO]: ==> Starting training for 200 epochs...
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:36:59 INFO]: Training loss at epoch 0: 1.1012073159217834
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:36:59 INFO]: New best epoch, val score: -1.026129071970133
[08/12/2025 16:36:59 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:37:05 INFO]: Training loss at epoch 1: 1.1481715440750122
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:37:06 INFO]: New best epoch, val score: -1.0024477020559333
[08/12/2025 16:37:06 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:37:07 INFO]: Training loss at epoch 0: 0.9587528109550476
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:37:09 INFO]: Training loss at epoch 0: 0.7359506636857986
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:37:09 INFO]: New best epoch, val score: -0.9406939270456589
[08/12/2025 16:37:09 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:37:10 INFO]: Training loss at epoch 2: 1.1844565272331238
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:37:11 INFO]: New best epoch, val score: -0.9613217362128006
[08/12/2025 16:37:11 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:37:11 INFO]: New best epoch, val score: -0.9737705575164803
[08/12/2025 16:37:11 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:37:15 INFO]: Training loss at epoch 0: 1.0407490134239197
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:37:16 INFO]: Training loss at epoch 3: 0.8665212988853455
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:37:17 INFO]: Training loss at epoch 0: 1.269885003566742
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:37:17 INFO]: New best epoch, val score: -0.9492016303111095
[08/12/2025 16:37:17 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:37:18 INFO]: Training loss at epoch 0: 1.2965067625045776
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:37:19 INFO]: New best epoch, val score: -1.0111649980175892
[08/12/2025 16:37:19 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:37:20 INFO]: New best epoch, val score: -1.0424839924730929
[08/12/2025 16:37:20 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:37:21 INFO]: New best epoch, val score: -0.9326561150997175
[08/12/2025 16:37:21 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:37:22 INFO]: Training loss at epoch 4: 1.182047724723816
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:37:23 INFO]: Training loss at epoch 1: 1.0921076536178589
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:37:23 INFO]: New best epoch, val score: -0.9387656010403264
[08/12/2025 16:37:23 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:37:26 INFO]: Training loss at epoch 1: 0.933782547712326
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:37:28 INFO]: Training loss at epoch 5: 1.262984037399292
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:37:28 INFO]: New best epoch, val score: -0.9374170244577028
[08/12/2025 16:37:28 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:37:29 INFO]: New best epoch, val score: -0.9384767346457683
[08/12/2025 16:37:29 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:37:33 INFO]: Training loss at epoch 0: 1.253211259841919
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:37:34 INFO]: Training loss at epoch 6: 1.321324646472931
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:37:34 INFO]: New best epoch, val score: -0.9384489389338904
[08/12/2025 16:37:34 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:37:38 INFO]: Training loss at epoch 2: 1.1323821544647217
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:37:39 INFO]: New best epoch, val score: -1.0456655960507024
[08/12/2025 16:37:39 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:37:39 INFO]: Training loss at epoch 7: 0.8447199463844299
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:37:40 INFO]: Training loss at epoch 1: 1.393949568271637
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:37:43 INFO]: Training loss at epoch 2: 0.9923237562179565
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:37:43 INFO]: Training loss at epoch 0: 1.2219508290290833
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:37:44 INFO]: Training loss at epoch 1: 1.3135617971420288
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:37:45 INFO]: Training loss at epoch 8: 0.9144402742385864
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:37:45 INFO]: New best epoch, val score: -0.9184559171395567
[08/12/2025 16:37:45 INFO]: Saving model to: model_best.pth
[08/12/2025 16:37:45 INFO]: Training loss at epoch 1: 1.0900540351867676
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:37:50 INFO]: New best epoch, val score: -1.1528981010900319
[08/12/2025 16:37:50 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:37:51 INFO]: Training loss at epoch 9: 1.0491855144500732
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:37:53 INFO]: Training stats: {
    "score": -1.0111248946677995,
    "rmse": 1.0111248946677995
}
[08/12/2025 16:37:53 INFO]: Val stats: {
    "score": -0.9390041637611424,
    "rmse": 0.9390041637611424
}
[08/12/2025 16:37:53 INFO]: Test stats: {
    "score": -0.9205308727702172,
    "rmse": 0.9205308727702172
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:37:54 INFO]: Training loss at epoch 3: 1.0143715143203735
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:37:56 INFO]: New best epoch, val score: -0.9122285721939644
[08/12/2025 16:37:56 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:37:58 INFO]: Training loss at epoch 10: 0.9071287214756012
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:00 INFO]: Training loss at epoch 3: 0.967646062374115
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:02 INFO]: New best epoch, val score: -0.9051999160864623
[08/12/2025 16:38:02 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:04 INFO]: Training loss at epoch 11: 0.9480614364147186
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:05 INFO]: Training loss at epoch 2: 1.292238175868988
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:10 INFO]: Training loss at epoch 12: 0.8836740553379059
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:10 INFO]: Training loss at epoch 4: 0.8122335076332092
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:11 INFO]: Training loss at epoch 2: 1.1847774982452393
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:12 INFO]: New best epoch, val score: -0.8752045880318362
[08/12/2025 16:38:12 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:38:12 INFO]: Training loss at epoch 2: 0.9341329336166382
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:38:14 INFO]: Training loss at epoch 0: 0.9757947027683258
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:16 INFO]: Training loss at epoch 13: 1.378928780555725
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:18 INFO]: Training loss at epoch 4: 1.05142343044281
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:18 INFO]: Training loss at epoch 0: 2.1891469955444336
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:38:18 INFO]: Training loss at epoch 1: 0.9877391159534454
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:20 INFO]: New best epoch, val score: -0.899394222581064
[08/12/2025 16:38:20 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:21 INFO]: Training loss at epoch 14: 1.1871425211429596
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:24 INFO]: New best epoch, val score: -0.9337423399941697
[08/12/2025 16:38:24 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:24 INFO]: New best epoch, val score: -0.9588796151773155
[08/12/2025 16:38:24 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:26 INFO]: Training loss at epoch 5: 1.0289786458015442
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:27 INFO]: Training loss at epoch 15: 1.023529589176178
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:30 INFO]: New best epoch, val score: -1.5055488387600064
[08/12/2025 16:38:30 INFO]: Saving model to: model_best.pth
[08/12/2025 16:38:30 INFO]: Training loss at epoch 3: 1.362618863582611
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:38:33 INFO]: Training loss at epoch 16: 0.8911102116107941
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:35 INFO]: Training loss at epoch 5: 0.8515084385871887
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:38:37 INFO]: Training loss at epoch 3: 1.097830444574356
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:38:38 INFO]: Training loss at epoch 17: 1.113754689693451
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:39 INFO]: Training loss at epoch 3: 1.2278252243995667
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:39 INFO]: Training loss at epoch 1: 1.1744391918182373
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:40 INFO]: New best epoch, val score: -0.9749564632372607
[08/12/2025 16:38:40 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:41 INFO]: Training loss at epoch 6: 0.9972465932369232
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:44 INFO]: Training loss at epoch 18: 1.166014313697815
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:38:46 INFO]: New best epoch, val score: -0.9855966671567433
[08/12/2025 16:38:46 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:50 INFO]: Training loss at epoch 19: 1.0049816370010376
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:52 INFO]: Training stats: {
    "score": -1.0020097545108677,
    "rmse": 1.0020097545108677
}
[08/12/2025 16:38:52 INFO]: Val stats: {
    "score": -0.9474796067061715,
    "rmse": 0.9474796067061715
}
[08/12/2025 16:38:52 INFO]: Test stats: {
    "score": -0.9206738166729399,
    "rmse": 0.9206738166729399
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:52 INFO]: Training loss at epoch 6: 0.8245396018028259
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:54 INFO]: Training loss at epoch 4: 1.0112097561359406
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:38:56 INFO]: Training loss at epoch 7: 1.0273256301879883
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:38:57 INFO]: New best epoch, val score: -0.954733301102211
[08/12/2025 16:38:57 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:38:58 INFO]: Training loss at epoch 20: 0.9411973655223846
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:39:03 INFO]: Training loss at epoch 2: 1.1151814460754395
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:39:03 INFO]: Training loss at epoch 21: 1.060305118560791
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:39:04 INFO]: Training loss at epoch 4: 0.8436163365840912
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:39:05 INFO]: Training loss at epoch 4: 1.0183651447296143
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:39:07 INFO]: New best epoch, val score: -0.9342183489929684
[08/12/2025 16:39:07 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:39:09 INFO]: Training loss at epoch 22: 0.8874058127403259
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:39:09 INFO]: Training loss at epoch 7: 0.9093505442142487
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:39:12 INFO]: Training loss at epoch 8: 0.7014574706554413
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:39:15 INFO]: Training loss at epoch 23: 1.2394028306007385
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:39:19 INFO]: Training loss at epoch 5: 0.999795138835907
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:39:20 INFO]: Training loss at epoch 24: 1.2749955356121063
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:39:22 INFO]: New best epoch, val score: -0.9540908939795427
[08/12/2025 16:39:22 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:39:26 INFO]: Training loss at epoch 25: 1.0598074197769165
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:39:27 INFO]: Training loss at epoch 8: 1.0115356147289276
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:39:27 INFO]: Training loss at epoch 9: 0.853004664182663
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:39:30 INFO]: Training loss at epoch 5: 1.2622941732406616
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:39:32 INFO]: Training loss at epoch 5: 1.101623773574829
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:39:32 INFO]: Training loss at epoch 26: 0.9404763579368591
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:39:32 INFO]: Training stats: {
    "score": -0.9197153979403976,
    "rmse": 0.9197153979403976
}
[08/12/2025 16:39:32 INFO]: Val stats: {
    "score": -0.881501022933089,
    "rmse": 0.881501022933089
}
[08/12/2025 16:39:32 INFO]: Test stats: {
    "score": -0.8485639773630758,
    "rmse": 0.8485639773630758
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:39:34 INFO]: New best epoch, val score: -0.9342106384114567
[08/12/2025 16:39:34 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:39:36 INFO]: Training loss at epoch 2: 1.0183565020561218
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:39:38 INFO]: Training loss at epoch 27: 0.7429004609584808
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:39:42 INFO]: New best epoch, val score: -0.9240615198127707
[08/12/2025 16:39:42 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:39:44 INFO]: Training loss at epoch 6: 1.1353201270103455
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:39:44 INFO]: Training loss at epoch 28: 1.1355035901069641
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:39:44 INFO]: Training loss at epoch 9: 1.2865224480628967
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:39:44 INFO]: Training loss at epoch 1: 1.1158502101898193
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:39:47 INFO]: New best epoch, val score: -0.9500850050644049
[08/12/2025 16:39:47 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:39:48 INFO]: Training loss at epoch 3: 1.0332041680812836
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:39:48 INFO]: Training loss at epoch 10: 0.9594245553016663
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:39:49 INFO]: Training loss at epoch 29: 0.9803741872310638
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:39:50 INFO]: Training stats: {
    "score": -0.9744337810790737,
    "rmse": 0.9744337810790737
}
[08/12/2025 16:39:50 INFO]: Val stats: {
    "score": -0.9012330636184485,
    "rmse": 0.9012330636184485
}
[08/12/2025 16:39:50 INFO]: Test stats: {
    "score": -0.878672950711697,
    "rmse": 0.878672950711697
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:39:51 INFO]: Training stats: {
    "score": -1.0005492765450679,
    "rmse": 1.0005492765450679
}
[08/12/2025 16:39:51 INFO]: Val stats: {
    "score": -0.9487069387004635,
    "rmse": 0.9487069387004635
}
[08/12/2025 16:39:51 INFO]: Test stats: {
    "score": -0.9205167579408432,
    "rmse": 0.9205167579408432
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:39:54 INFO]: Training loss at epoch 1: 1.7461297512054443
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:39:55 INFO]: New best epoch, val score: -0.8645327648432732
[08/12/2025 16:39:55 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:39:57 INFO]: Training loss at epoch 6: 0.8722569644451141
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:39:57 INFO]: Training loss at epoch 30: 1.0152872204780579
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:39:59 INFO]: Training loss at epoch 6: 1.0701490342617035
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:40:00 INFO]: New best epoch, val score: -0.9341854456734985
[08/12/2025 16:40:00 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:40:03 INFO]: Training loss at epoch 31: 1.1174052357673645
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:40:03 INFO]: Training loss at epoch 11: 1.1318361163139343
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:40:06 INFO]: New best epoch, val score: -1.392542551455649
[08/12/2025 16:40:06 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:40:07 INFO]: Training loss at epoch 10: 0.9416227638721466
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:40:08 INFO]: Training loss at epoch 7: 1.1610621809959412
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:40:08 INFO]: Training loss at epoch 32: 1.0167681574821472
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:40:11 INFO]: New best epoch, val score: -0.9439500653489328
[08/12/2025 16:40:11 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:40:14 INFO]: Training loss at epoch 33: 0.9597019255161285
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:40:19 INFO]: Training loss at epoch 12: 0.8676142394542694
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:40:20 INFO]: Training loss at epoch 34: 1.048933744430542
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:40:23 INFO]: Training loss at epoch 7: 1.0520856380462646
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:40:24 INFO]: Training loss at epoch 11: 0.8644145727157593
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:40:25 INFO]: Training loss at epoch 7: 1.0082051753997803
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:40:26 INFO]: Training loss at epoch 35: 1.1366461217403412
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:40:27 INFO]: New best epoch, val score: -0.934167794115332
[08/12/2025 16:40:27 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:40:31 INFO]: Training loss at epoch 3: 0.997014045715332
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:40:31 INFO]: Training loss at epoch 36: 0.9511235058307648
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:40:32 INFO]: Training loss at epoch 4: 1.0993645191192627
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:40:33 INFO]: Training loss at epoch 8: 0.9193726778030396
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:40:34 INFO]: Training loss at epoch 13: 0.7532675862312317
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:40:36 INFO]: New best epoch, val score: -0.9372415383620946
[08/12/2025 16:40:36 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:40:37 INFO]: Training loss at epoch 37: 1.1807482242584229
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:40:38 INFO]: Running Final Evaluation...
[W 2025-08-12 16:40:38,210] Trial 7 failed with parameters: {'embedding_head_pair': (128, 4), 'n_layers': 2, 'd_ffn_factor': 2.5778506396706278, 'attention_dropout': 0.18971357605831296, 'activation': 'relu', 'lr': 1.0737124121097217e-05} because of the following error: RuntimeError('Error(s) in loading state_dict for FTTransformer:\n\tsize mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 384]) from checkpoint, the shape in current model is torch.Size([101, 128]).\n\tsize mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 384]) from checkpoint, the shape in current model is torch.Size([100, 128]).\n\tsize mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([500, 384]) from checkpoint, the shape in current model is torch.Size([329, 128]).\n\tsize mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([500]) from checkpoint, the shape in current model is torch.Size([329]).\n\tsize mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([384, 500]) from checkpoint, the shape in current model is torch.Size([128, 329]).\n\tsize mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([500, 384]) from checkpoint, the shape in current model is torch.Size([329, 128]).\n\tsize mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([500]) from checkpoint, the shape in current model is torch.Size([329]).\n\tsize mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([384, 500]) from checkpoint, the shape in current model is torch.Size([128, 329]).\n\tsize mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for last_normalization.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for last_normalization.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for head.weight: copying a param with shape torch.Size([1, 384]) from checkpoint, the shape in current model is torch.Size([1, 128]).').
Traceback (most recent call last):
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 145, in <lambda>
    func = lambda trial: objective(trial, cfg, trial_stats, trial_counter, n_optuna_trials,
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 104, in objective
    stats = train_net_for_optuna.main(config, loaders, unique_categories, n_numerical, n_classes)
  File "/temporario2/14570653/tabular-transfer-learning/train_net_for_optuna.py", line 127, in main
    net.load_state_dict(torch.load(checkpoint_path)["net"])
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2215, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for FTTransformer:
	size mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 384]) from checkpoint, the shape in current model is torch.Size([101, 128]).
	size mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 384]) from checkpoint, the shape in current model is torch.Size([100, 128]).
	size mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([500, 384]) from checkpoint, the shape in current model is torch.Size([329, 128]).
	size mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([500]) from checkpoint, the shape in current model is torch.Size([329]).
	size mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([384, 500]) from checkpoint, the shape in current model is torch.Size([128, 329]).
	size mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([500, 384]) from checkpoint, the shape in current model is torch.Size([329, 128]).
	size mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([500]) from checkpoint, the shape in current model is torch.Size([329]).
	size mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([384, 500]) from checkpoint, the shape in current model is torch.Size([128, 329]).
	size mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for last_normalization.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for last_normalization.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for head.weight: copying a param with shape torch.Size([1, 384]) from checkpoint, the shape in current model is torch.Size([1, 128]).
[W 2025-08-12 16:40:38,222] Trial 7 failed with value None.
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:40:38 INFO]: New best epoch, val score: -0.9278772204708421
[08/12/2025 16:40:38 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:40:40 INFO]: Training loss at epoch 12: 0.9334631860256195
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:40:49 INFO]: Training loss at epoch 14: 1.0823600590229034
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:40:50 INFO]: Training loss at epoch 8: 1.0048970580101013
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:40:52 INFO]: Training loss at epoch 8: 1.266289472579956
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:40:57 INFO]: Training loss at epoch 13: 0.8184030950069427
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:40:57 INFO]: Training loss at epoch 9: 1.2934501767158508
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:41:04 INFO]: Training loss at epoch 15: 0.8294035792350769
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:41:05 INFO]: Training stats: {
    "score": -1.03884375750661,
    "rmse": 1.03884375750661
}
[08/12/2025 16:41:05 INFO]: Val stats: {
    "score": -0.9309157091837241,
    "rmse": 0.9309157091837241
}
[08/12/2025 16:41:05 INFO]: Test stats: {
    "score": -0.9320275591479777,
    "rmse": 0.9320275591479777
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:41:08 INFO]: New best epoch, val score: -0.9309157091837241
[08/12/2025 16:41:08 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:41:14 INFO]: Training loss at epoch 14: 0.7938386797904968
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:41:14 INFO]: Training loss at epoch 2: 1.6690431237220764
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:41:15 INFO]: Training loss at epoch 9: 0.8578701019287109
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:41:16 INFO]: Training loss at epoch 5: 1.1408393383026123
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:41:18 INFO]: Training loss at epoch 9: 1.0130402445793152
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:41:19 INFO]: Training loss at epoch 16: 0.909996509552002
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:41:22 INFO]: New best epoch, val score: -0.9272657302942963
[08/12/2025 16:41:22 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:41:24 INFO]: New best epoch, val score: -0.8479094656025333
[08/12/2025 16:41:24 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:41:25 INFO]: Training stats: {
    "score": -1.0171401451454174,
    "rmse": 1.0171401451454174
}
[08/12/2025 16:41:25 INFO]: Val stats: {
    "score": -0.9342089738562342,
    "rmse": 0.9342089738562342
}
[08/12/2025 16:41:25 INFO]: Test stats: {
    "score": -0.9219608764921607,
    "rmse": 0.9219608764921607
}
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:41:25 INFO]: Training loss at epoch 4: 0.9430606067180634
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:41:27 INFO]: Training stats: {
    "score": -0.9772570278175696,
    "rmse": 0.9772570278175696
}
[08/12/2025 16:41:27 INFO]: Val stats: {
    "score": -0.9464630744402386,
    "rmse": 0.9464630744402386
}
[08/12/2025 16:41:27 INFO]: Test stats: {
    "score": -0.9078804687628006,
    "rmse": 0.9078804687628006
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:41:29 INFO]: Training loss at epoch 2: 1.7927438020706177
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:41:30 INFO]: Training loss at epoch 10: 1.0485678911209106
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:41:30 INFO]: Training loss at epoch 15: 0.9052414000034332
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:41:34 INFO]: Training loss at epoch 17: 1.058339536190033
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:41:40 INFO]: New best epoch, val score: -0.9385363085942037
[08/12/2025 16:41:40 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:41:47 INFO]: Training loss at epoch 16: 0.8909057974815369
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:41:49 INFO]: Training loss at epoch 18: 0.97265625
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:41:50 INFO]: Training loss at epoch 10: 1.043708860874176
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:41:54 INFO]: Training loss at epoch 10: 1.083891212940216
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:41:54 INFO]: Training loss at epoch 11: 0.9463574290275574
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:42:01 INFO]: Training loss at epoch 6: 1.1152728199958801
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:42:03 INFO]: Training loss at epoch 17: 1.002993881702423
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:42:04 INFO]: Training loss at epoch 19: 1.0687467753887177
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:42:06 INFO]: New best epoch, val score: -0.926662617477544
[08/12/2025 16:42:06 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:42:10 INFO]: Training stats: {
    "score": -0.8797474034811076,
    "rmse": 0.8797474034811076
}
[08/12/2025 16:42:10 INFO]: Val stats: {
    "score": -0.8907740626452948,
    "rmse": 0.8907740626452948
}
[08/12/2025 16:42:10 INFO]: Test stats: {
    "score": -0.8246447254177207,
    "rmse": 0.8246447254177207
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:42:17 INFO]: Training loss at epoch 11: 1.009088933467865
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:42:19 INFO]: Training loss at epoch 12: 1.0258206725120544
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:42:20 INFO]: Training loss at epoch 5: 0.8631511926651001
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:42:21 INFO]: Training loss at epoch 11: 1.0609175562858582
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:42:21 INFO]: Training loss at epoch 18: 0.9635098576545715
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:42:24 INFO]: New best epoch, val score: -0.9274618246872607
[08/12/2025 16:42:24 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:42:25 INFO]: Training loss at epoch 20: 0.7444573044776917
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:42:38 INFO]: Training loss at epoch 19: 1.0101321339607239
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:42:40 INFO]: Training loss at epoch 21: 0.905639261007309
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:42:43 INFO]: Training loss at epoch 12: 0.8555178344249725
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:42:43 INFO]: Training loss at epoch 13: 1.041455328464508
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:42:44 INFO]: Training stats: {
    "score": -0.9491198331758518,
    "rmse": 0.9491198331758518
}
[08/12/2025 16:42:44 INFO]: Val stats: {
    "score": -0.9179714395527135,
    "rmse": 0.9179714395527135
}
[08/12/2025 16:42:44 INFO]: Test stats: {
    "score": -0.8735198545850203,
    "rmse": 0.8735198545850203
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:42:44 INFO]: Training loss at epoch 3: 1.1057780385017395
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:42:46 INFO]: Training loss at epoch 7: 0.91082364320755
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:42:47 INFO]: Training loss at epoch 12: 1.1102259755134583
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:42:50 INFO]: New best epoch, val score: -0.9223043302905142
[08/12/2025 16:42:50 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:42:51 INFO]: New best epoch, val score: -0.9262165714324031
[08/12/2025 16:42:51 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:42:55 INFO]: Training loss at epoch 22: 0.9017126262187958
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:42:57 INFO]: New best epoch, val score: -0.8717916362256681
[08/12/2025 16:42:57 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:43:01 INFO]: Training loss at epoch 20: 0.7133551090955734
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:43:04 INFO]: Training loss at epoch 3: 1.0440818071365356
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:43:08 INFO]: Training loss at epoch 14: 0.9242308139801025
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:43:09 INFO]: Training loss at epoch 13: 1.1161612272262573
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:43:10 INFO]: Training loss at epoch 23: 0.9594331085681915
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:43:13 INFO]: Training loss at epoch 13: 0.9132785797119141
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:43:14 INFO]: Training loss at epoch 6: 1.0327346622943878
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:43:17 INFO]: New best epoch, val score: -0.920215374078749
[08/12/2025 16:43:17 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:43:17 INFO]: Training loss at epoch 21: 0.98214191198349
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:43:25 INFO]: Training loss at epoch 24: 0.7991461157798767
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:43:30 INFO]: Training loss at epoch 8: 0.9700906574726105
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:43:32 INFO]: Training loss at epoch 15: 1.03995019197464
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:43:34 INFO]: Training loss at epoch 22: 1.028484284877777
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:43:34 INFO]: Training loss at epoch 14: 0.9559969902038574
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:43:36 INFO]: New best epoch, val score: -0.9259671510489826
[08/12/2025 16:43:36 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:43:40 INFO]: Training loss at epoch 14: 1.0123696625232697
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:43:40 INFO]: Training loss at epoch 25: 0.8422912657260895
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:43:50 INFO]: Training loss at epoch 23: 0.8714327216148376
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:43:55 INFO]: Training loss at epoch 26: 0.7925840020179749
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:43:56 INFO]: Training loss at epoch 16: 1.1888719499111176
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:43:59 INFO]: New best epoch, val score: -0.9247122044289893
[08/12/2025 16:43:59 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:44:00 INFO]: Training loss at epoch 15: 1.116772174835205
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:44:06 INFO]: Training loss at epoch 15: 0.9295877814292908
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:44:07 INFO]: Training loss at epoch 24: 1.1624548435211182
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:44:08 INFO]: Training loss at epoch 7: 0.9761329591274261
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:44:10 INFO]: Training loss at epoch 27: 0.8195247650146484
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:44:14 INFO]: Training loss at epoch 4: 0.9316723942756653
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:44:14 INFO]: Training loss at epoch 9: 0.9859010577201843
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:44:21 INFO]: Training loss at epoch 17: 0.9897448420524597
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:44:23 INFO]: Training loss at epoch 25: 0.9053022563457489
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:44:23 INFO]: New best epoch, val score: -0.9104730608383431
[08/12/2025 16:44:23 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:44:24 INFO]: Training loss at epoch 28: 0.7882613241672516
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:44:26 INFO]: Training loss at epoch 16: 1.2791383862495422
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:44:26 INFO]: New best epoch, val score: -0.8635908549043765
[08/12/2025 16:44:26 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:44:30 INFO]: Training stats: {
    "score": -1.011840515978456,
    "rmse": 1.011840515978456
}
[08/12/2025 16:44:30 INFO]: Val stats: {
    "score": -0.9258474476718002,
    "rmse": 0.9258474476718002
}
[08/12/2025 16:44:30 INFO]: Test stats: {
    "score": -0.9164244294633659,
    "rmse": 0.9164244294633659
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:44:32 INFO]: Training loss at epoch 16: 1.0357813835144043
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:44:35 INFO]: New best epoch, val score: -0.9258474476718002
[08/12/2025 16:44:35 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:44:39 INFO]: Training loss at epoch 4: 1.2612510323524475
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:44:39 INFO]: Training loss at epoch 29: 0.7876070439815521
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:44:40 INFO]: Training loss at epoch 26: 0.7979742884635925
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:44:44 INFO]: Training stats: {
    "score": -0.8426125332970319,
    "rmse": 0.8426125332970319
}
[08/12/2025 16:44:44 INFO]: Val stats: {
    "score": -0.8578778616748249,
    "rmse": 0.8578778616748249
}
[08/12/2025 16:44:44 INFO]: Test stats: {
    "score": -0.7857168656337211,
    "rmse": 0.7857168656337211
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:44:45 INFO]: Training loss at epoch 18: 0.8500237464904785
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:44:46 INFO]: New best epoch, val score: -0.8578778616748249
[08/12/2025 16:44:46 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:44:48 INFO]: New best epoch, val score: -0.9076979983522322
[08/12/2025 16:44:48 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:44:51 INFO]: Training loss at epoch 17: 0.9388070404529572
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:44:56 INFO]: Training loss at epoch 27: 0.8263942301273346
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:44:59 INFO]: Training loss at epoch 17: 0.9040302634239197
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:44:59 INFO]: Training loss at epoch 30: 0.960147887468338
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:45:01 INFO]: New best epoch, val score: -0.8552426103285747
[08/12/2025 16:45:01 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:45:02 INFO]: Training loss at epoch 8: 0.9926235675811768
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:45:10 INFO]: Training loss at epoch 19: 0.9491569399833679
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:45:13 INFO]: Training loss at epoch 28: 0.7492274045944214
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:45:14 INFO]: Training loss at epoch 10: 1.0915158987045288
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:45:14 INFO]: Training loss at epoch 31: 0.923365592956543
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:45:17 INFO]: Training loss at epoch 18: 0.9062442183494568
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:45:18 INFO]: Training stats: {
    "score": -0.9821690062504984,
    "rmse": 0.9821690062504984
}
[08/12/2025 16:45:18 INFO]: Val stats: {
    "score": -0.9054939537733097,
    "rmse": 0.9054939537733097
}
[08/12/2025 16:45:18 INFO]: Test stats: {
    "score": -0.8897031731263688,
    "rmse": 0.8897031731263688
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:45:21 INFO]: New best epoch, val score: -0.9054939537733097
[08/12/2025 16:45:21 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:45:25 INFO]: Training loss at epoch 18: 0.8822973072528839
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:45:29 INFO]: Training loss at epoch 32: 0.7833922207355499
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:45:30 INFO]: Training loss at epoch 29: 0.9411272704601288
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:45:35 INFO]: Training stats: {
    "score": -0.9336405247328139,
    "rmse": 0.9336405247328139
}
[08/12/2025 16:45:35 INFO]: Val stats: {
    "score": -0.8979365392059746,
    "rmse": 0.8979365392059746
}
[08/12/2025 16:45:35 INFO]: Test stats: {
    "score": -0.8557657705303734,
    "rmse": 0.8557657705303734
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:45:37 INFO]: New best epoch, val score: -0.8979365392059746
[08/12/2025 16:45:37 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:45:42 INFO]: Training loss at epoch 20: 1.0353015065193176
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:45:42 INFO]: Training loss at epoch 19: 0.9964739978313446
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:45:44 INFO]: Training loss at epoch 33: 0.8530706167221069
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:45:44 INFO]: Training loss at epoch 5: 1.0914744138717651
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:45:45 INFO]: New best epoch, val score: -0.9035109893121597
[08/12/2025 16:45:45 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:45:51 INFO]: Training loss at epoch 19: 0.9065445065498352
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:45:52 INFO]: Training stats: {
    "score": -1.0006452460115611,
    "rmse": 1.0006452460115611
}
[08/12/2025 16:45:52 INFO]: Val stats: {
    "score": -0.9759447637594283,
    "rmse": 0.9759447637594283
}
[08/12/2025 16:45:52 INFO]: Test stats: {
    "score": -0.9369640877359359,
    "rmse": 0.9369640877359359
}
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:45:52 INFO]: Training loss at epoch 30: 0.8251749277114868
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:45:54 INFO]: New best epoch, val score: -0.895434192411224
[08/12/2025 16:45:54 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:45:57 INFO]: Training loss at epoch 9: 1.0543450117111206
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:45:58 INFO]: Training loss at epoch 34: 0.7350481748580933
[08/12/2025 16:45:58 INFO]: Training loss at epoch 11: 0.8685943484306335
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:46:01 INFO]: Training stats: {
    "score": -0.9575656043401222,
    "rmse": 0.9575656043401222
}
[08/12/2025 16:46:01 INFO]: Val stats: {
    "score": -0.9264992446997523,
    "rmse": 0.9264992446997523
}
[08/12/2025 16:46:01 INFO]: Test stats: {
    "score": -0.8858694004222603,
    "rmse": 0.8858694004222603
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:46:07 INFO]: Training loss at epoch 21: 0.7911444902420044
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:46:08 INFO]: Training loss at epoch 31: 0.9309636950492859
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:46:10 INFO]: New best epoch, val score: -0.8937609078731803
[08/12/2025 16:46:10 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:46:13 INFO]: Training loss at epoch 35: 0.8354643285274506
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:46:14 INFO]: Training loss at epoch 5: 1.1714670062065125
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:46:15 INFO]: Training stats: {
    "score": -0.9908297914376429,
    "rmse": 0.9908297914376429
}
[08/12/2025 16:46:15 INFO]: Val stats: {
    "score": -0.9417696452573632,
    "rmse": 0.9417696452573632
}
[08/12/2025 16:46:15 INFO]: Test stats: {
    "score": -0.9186900426577513,
    "rmse": 0.9186900426577513
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:46:17 INFO]: Training loss at epoch 20: 1.079022079706192
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:46:25 INFO]: Training loss at epoch 32: 0.9925548434257507
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:46:25 INFO]: New best epoch, val score: -0.9384166722537292
[08/12/2025 16:46:25 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:46:27 INFO]: Training loss at epoch 20: 1.0139304399490356
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:46:28 INFO]: Training loss at epoch 36: 0.7931925654411316
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:46:31 INFO]: Training loss at epoch 22: 1.07963627576828
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:46:42 INFO]: Training loss at epoch 33: 0.8220838010311127
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:46:42 INFO]: Training loss at epoch 12: 0.9648796916007996
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:46:43 INFO]: Training loss at epoch 21: 1.1586445569992065
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:46:43 INFO]: Training loss at epoch 37: 0.672110989689827
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:46:53 INFO]: Training loss at epoch 21: 0.9397442638874054
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:46:55 INFO]: Training loss at epoch 23: 1.0214689671993256
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:46:58 INFO]: Training loss at epoch 38: 0.7807708382606506
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:46:58 INFO]: Training loss at epoch 34: 0.7284293174743652
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:47:00 INFO]: New best epoch, val score: -0.8927280000824158
[08/12/2025 16:47:00 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:47:08 INFO]: Training loss at epoch 22: 1.0493131577968597
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:47:09 INFO]: Training loss at epoch 10: 1.1457684636116028
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:47:13 INFO]: Training loss at epoch 39: 0.9887345731258392
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:47:18 INFO]: Training loss at epoch 6: 1.3616148829460144
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:47:19 INFO]: Training loss at epoch 35: 0.9085354804992676
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:47:20 INFO]: Training loss at epoch 22: 0.9386385679244995
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:47:20 INFO]: Training loss at epoch 24: 1.020909696817398
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:47:22 INFO]: New best epoch, val score: -0.88932989004216
[08/12/2025 16:47:22 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:47:24 INFO]: Training stats: {
    "score": -0.8131651738232998,
    "rmse": 0.8131651738232998
}
[08/12/2025 16:47:24 INFO]: Val stats: {
    "score": -0.8827113025702185,
    "rmse": 0.8827113025702185
}
[08/12/2025 16:47:24 INFO]: Test stats: {
    "score": -0.7818972626936105,
    "rmse": 0.7818972626936105
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:47:26 INFO]: Training loss at epoch 13: 0.9119002223014832
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:47:36 INFO]: Training loss at epoch 36: 0.9183351993560791
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:47:38 INFO]: New best epoch, val score: -0.8858319972246778
[08/12/2025 16:47:38 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:47:41 INFO]: Training loss at epoch 23: 1.0670556426048279
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:47:42 INFO]: Training loss at epoch 40: 0.8973880410194397
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:47:44 INFO]: Training loss at epoch 25: 0.7261613309383392
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:47:46 INFO]: Training loss at epoch 23: 0.9489540159702301
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:47:48 INFO]: Training loss at epoch 6: 0.9534800350666046
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:47:53 INFO]: Training loss at epoch 37: 0.7353805005550385
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:47:55 INFO]: New best epoch, val score: -0.8837272873907877
[08/12/2025 16:47:55 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:47:57 INFO]: Training loss at epoch 41: 0.7846933901309967
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:47:59 INFO]: New best epoch, val score: -0.9380430878201288
[08/12/2025 16:47:59 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:48:06 INFO]: Training loss at epoch 11: 1.2573687434196472
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:48:07 INFO]: Training loss at epoch 24: 1.0713315606117249
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:48:09 INFO]: Training loss at epoch 26: 0.7690941095352173
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:48:09 INFO]: Training loss at epoch 38: 0.9379382431507111
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:48:10 INFO]: Training loss at epoch 14: 1.0539470911026
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:48:11 INFO]: New best epoch, val score: -0.8817909479352909
[08/12/2025 16:48:11 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:48:12 INFO]: Training loss at epoch 42: 0.9227156043052673
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:48:13 INFO]: Training loss at epoch 24: 0.8844212591648102
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:48:26 INFO]: Training loss at epoch 39: 0.7676645517349243
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:48:26 INFO]: Training loss at epoch 43: 1.0370291471481323
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:48:32 INFO]: Training stats: {
    "score": -0.9200182398923327,
    "rmse": 0.9200182398923327
}
[08/12/2025 16:48:32 INFO]: Val stats: {
    "score": -0.8798710394562788,
    "rmse": 0.8798710394562788
}
[08/12/2025 16:48:32 INFO]: Test stats: {
    "score": -0.8411291594844732,
    "rmse": 0.8411291594844732
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:48:32 INFO]: Training loss at epoch 25: 1.05893474817276
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:48:33 INFO]: Training loss at epoch 27: 0.9840587675571442
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:48:34 INFO]: New best epoch, val score: -0.8798710394562788
[08/12/2025 16:48:34 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:48:36 INFO]: New best epoch, val score: -0.8962795821044333
[08/12/2025 16:48:36 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:48:39 INFO]: Training loss at epoch 25: 1.0483081340789795
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:48:41 INFO]: Training loss at epoch 44: 0.6894761323928833
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:48:48 INFO]: Training loss at epoch 7: 0.9749393463134766
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:48:48 INFO]: Training loss at epoch 40: 1.0601413249969482
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:48:50 INFO]: New best epoch, val score: -0.879853593259697
[08/12/2025 16:48:50 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:48:55 INFO]: Training loss at epoch 15: 0.8854523301124573
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:48:56 INFO]: Training loss at epoch 45: 0.6910992562770844
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:48:57 INFO]: Training loss at epoch 28: 0.9448334574699402
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:48:58 INFO]: Training loss at epoch 26: 1.0603840947151184
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:48:59 INFO]: Training loss at epoch 12: 0.9212097227573395
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:49:00 INFO]: New best epoch, val score: -0.884265282867831
[08/12/2025 16:49:00 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:49:05 INFO]: Training loss at epoch 26: 0.8999338150024414
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:49:05 INFO]: Training loss at epoch 41: 0.8871512413024902
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:49:11 INFO]: Training loss at epoch 46: 0.6896948516368866
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:49:21 INFO]: Training loss at epoch 29: 0.8345537781715393
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:49:22 INFO]: Training loss at epoch 7: 1.0454291701316833
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:49:22 INFO]: Training loss at epoch 42: 0.7809828221797943
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:49:24 INFO]: Training loss at epoch 27: 0.9335969090461731
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:49:26 INFO]: Training loss at epoch 47: 0.7320093214511871
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:49:29 INFO]: Training stats: {
    "score": -0.9107112762019708,
    "rmse": 0.9107112762019708
}
[08/12/2025 16:49:29 INFO]: Val stats: {
    "score": -0.8787829913125821,
    "rmse": 0.8787829913125821
}
[08/12/2025 16:49:29 INFO]: Test stats: {
    "score": -0.8394209265656485,
    "rmse": 0.8394209265656485
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:49:30 INFO]: Training loss at epoch 27: 0.9836365282535553
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:49:32 INFO]: New best epoch, val score: -0.8787829913125821
[08/12/2025 16:49:32 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:49:39 INFO]: Training loss at epoch 43: 0.9754596948623657
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:49:40 INFO]: Training loss at epoch 16: 0.9227226972579956
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:49:41 INFO]: Training loss at epoch 48: 0.7801218330860138
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:49:50 INFO]: Training loss at epoch 28: 1.0524277687072754
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:49:52 INFO]: Training loss at epoch 30: 0.8902029991149902
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:49:54 INFO]: Training loss at epoch 13: 1.035000503063202
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:49:55 INFO]: New best epoch, val score: -0.8778676149144539
[08/12/2025 16:49:55 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:49:56 INFO]: Training loss at epoch 28: 0.9798513650894165
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:49:56 INFO]: Training loss at epoch 44: 0.7671033143997192
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:49:56 INFO]: Training loss at epoch 49: 1.0172423422336578
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:50:02 INFO]: Training stats: {
    "score": -0.7988565391213377,
    "rmse": 0.7988565391213377
}
[08/12/2025 16:50:02 INFO]: Val stats: {
    "score": -0.8877909817618048,
    "rmse": 0.8877909817618048
}
[08/12/2025 16:50:02 INFO]: Test stats: {
    "score": -0.7903440233418187,
    "rmse": 0.7903440233418187
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:50:13 INFO]: Training loss at epoch 45: 0.7590933740139008
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:50:15 INFO]: Training loss at epoch 8: 0.8858789205551147
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:50:16 INFO]: Training loss at epoch 29: 0.8357813656330109
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:50:16 INFO]: Training loss at epoch 31: 0.6725380271673203
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:50:17 INFO]: Training loss at epoch 50: 0.7297110557556152
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:50:21 INFO]: Training loss at epoch 29: 0.8993209302425385
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:50:24 INFO]: Training loss at epoch 17: 1.1261138916015625
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:50:26 INFO]: Training stats: {
    "score": -0.9957768112962255,
    "rmse": 0.9957768112962255
}
[08/12/2025 16:50:26 INFO]: Val stats: {
    "score": -0.9478614172692775,
    "rmse": 0.9478614172692775
}
[08/12/2025 16:50:26 INFO]: Test stats: {
    "score": -0.9188726549228892,
    "rmse": 0.9188726549228892
}
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:50:30 INFO]: Training loss at epoch 46: 0.6614560037851334
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:50:30 INFO]: Training stats: {
    "score": -0.9151620027790592,
    "rmse": 0.9151620027790592
}
[08/12/2025 16:50:30 INFO]: Val stats: {
    "score": -0.9174585702151022,
    "rmse": 0.9174585702151022
}
[08/12/2025 16:50:30 INFO]: Test stats: {
    "score": -0.8550346116141679,
    "rmse": 0.8550346116141679
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:50:32 INFO]: Training loss at epoch 51: 0.9054625332355499
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:50:33 INFO]: New best epoch, val score: -0.9174585702151022
[08/12/2025 16:50:33 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:50:40 INFO]: Training loss at epoch 32: 0.8083702623844147
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:50:46 INFO]: Training loss at epoch 47: 0.8296926915645599
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:50:47 INFO]: Training loss at epoch 52: 0.9121384024620056
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:50:49 INFO]: Training loss at epoch 14: 0.9162211418151855
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:50:52 INFO]: Training loss at epoch 30: 0.9177019596099854
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:50:53 INFO]: Training loss at epoch 8: 0.8679255247116089
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:50:55 INFO]: Training loss at epoch 30: 0.7472718060016632
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:50:59 INFO]: New best epoch, val score: -0.9035279018563898
[08/12/2025 16:50:59 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:51:02 INFO]: Training loss at epoch 53: 0.7108938694000244
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:51:03 INFO]: Training loss at epoch 33: 0.7621616721153259
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:51:03 INFO]: Training loss at epoch 48: 0.8105148375034332
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:51:05 INFO]: New best epoch, val score: -0.8795572991173229
[08/12/2025 16:51:05 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:51:09 INFO]: Training loss at epoch 18: 0.8782253861427307
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:51:17 INFO]: Training loss at epoch 54: 0.606098398566246
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:51:18 INFO]: Training loss at epoch 31: 0.9869923889636993
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:51:20 INFO]: Training loss at epoch 49: 0.8050497472286224
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:51:21 INFO]: Training loss at epoch 31: 0.9317002594470978
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:51:24 INFO]: New best epoch, val score: -0.8773162108319655
[08/12/2025 16:51:24 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:51:26 INFO]: Training stats: {
    "score": -0.9021496478011499,
    "rmse": 0.9021496478011499
}
[08/12/2025 16:51:26 INFO]: Val stats: {
    "score": -0.8780532598734337,
    "rmse": 0.8780532598734337
}
[08/12/2025 16:51:26 INFO]: Test stats: {
    "score": -0.8305791208956097,
    "rmse": 0.8305791208956097
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:51:27 INFO]: Training loss at epoch 34: 0.7337611615657806
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:51:28 INFO]: New best epoch, val score: -0.8780532598734337
[08/12/2025 16:51:28 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:51:32 INFO]: Training loss at epoch 55: 0.5612216591835022
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:51:42 INFO]: Training loss at epoch 9: 0.9061834812164307
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:51:43 INFO]: Training loss at epoch 50: 0.7823752760887146
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:51:44 INFO]: Training loss at epoch 15: 0.9682780206203461
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:51:44 INFO]: Training loss at epoch 32: 0.9567042887210846
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:51:45 INFO]: New best epoch, val score: -0.8766367888456225
[08/12/2025 16:51:45 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:51:47 INFO]: Training loss at epoch 32: 1.1256092190742493
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:51:47 INFO]: Training loss at epoch 56: 0.7925132513046265
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:51:50 INFO]: New best epoch, val score: -0.863084162051884
[08/12/2025 16:51:50 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:51:50 INFO]: Training loss at epoch 35: 0.5782674252986908
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:51:53 INFO]: Training loss at epoch 19: 0.9321271181106567
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:52:00 INFO]: Training loss at epoch 51: 1.070135772228241
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:52:02 INFO]: New best epoch, val score: -0.875390069772131
[08/12/2025 16:52:02 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:52:03 INFO]: Training loss at epoch 57: 0.7461794316768646
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:52:08 INFO]: Training stats: {
    "score": -0.9858578607706892,
    "rmse": 0.9858578607706892
}
[08/12/2025 16:52:08 INFO]: Val stats: {
    "score": -0.953471914921676,
    "rmse": 0.953471914921676
}
[08/12/2025 16:52:08 INFO]: Test stats: {
    "score": -0.9169179095641451,
    "rmse": 0.9169179095641451
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:52:10 INFO]: Training loss at epoch 33: 0.8028326630592346
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:52:11 INFO]: Training stats: {
    "score": -1.05586158109362,
    "rmse": 1.05586158109362
}
[08/12/2025 16:52:11 INFO]: Val stats: {
    "score": -0.9348711099124006,
    "rmse": 0.9348711099124006
}
[08/12/2025 16:52:11 INFO]: Test stats: {
    "score": -0.9554470266142605,
    "rmse": 0.9554470266142605
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:52:12 INFO]: Training loss at epoch 33: 0.8492230474948883
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:52:14 INFO]: Training loss at epoch 36: 0.6617651283740997
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:52:15 INFO]: New best epoch, val score: -0.8588165594522321
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:52:15 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:52:16 INFO]: New best epoch, val score: -0.866284565775614
[08/12/2025 16:52:16 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:52:17 INFO]: Training loss at epoch 52: 0.7534891963005066
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:52:18 INFO]: Training loss at epoch 58: 0.7753376960754395
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:52:19 INFO]: New best epoch, val score: -0.8738897164883443
[08/12/2025 16:52:19 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:52:25 INFO]: Training loss at epoch 9: 0.8678141534328461
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:52:33 INFO]: Training loss at epoch 59: 0.7077129781246185
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:52:35 INFO]: Training loss at epoch 53: 0.9719038903713226
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:52:36 INFO]: Training loss at epoch 34: 0.8372741937637329
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:52:37 INFO]: New best epoch, val score: -0.8727297137043125
[08/12/2025 16:52:37 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:52:37 INFO]: Training loss at epoch 37: 0.8153476715087891
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:52:38 INFO]: Training loss at epoch 34: 0.7649179399013519
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:52:38 INFO]: Training stats: {
    "score": -0.8301565508397026,
    "rmse": 0.8301565508397026
}
[08/12/2025 16:52:38 INFO]: Val stats: {
    "score": -0.9197588715460535,
    "rmse": 0.9197588715460535
}
[08/12/2025 16:52:38 INFO]: Test stats: {
    "score": -0.8473687817901218,
    "rmse": 0.8473687817901218
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:52:39 INFO]: Training loss at epoch 16: 1.015549659729004
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:52:40 INFO]: New best epoch, val score: -0.8561107124655722
[08/12/2025 16:52:40 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:52:41 INFO]: New best epoch, val score: -0.8539732281528596
[08/12/2025 16:52:41 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:52:52 INFO]: Training loss at epoch 54: 0.8680494725704193
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:52:53 INFO]: Training loss at epoch 20: 1.098561942577362
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:52:54 INFO]: Training loss at epoch 60: 0.7691755592823029
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:52:56 INFO]: Training stats: {
    "score": -1.0034932583168865,
    "rmse": 1.0034932583168865
}
[08/12/2025 16:52:56 INFO]: Val stats: {
    "score": -0.9383280168844679,
    "rmse": 0.9383280168844679
}
[08/12/2025 16:52:56 INFO]: Test stats: {
    "score": -0.9174377856037338,
    "rmse": 0.9174377856037338
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:53:01 INFO]: Training loss at epoch 38: 0.6182896196842194
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:53:03 INFO]: Training loss at epoch 35: 1.171916663646698
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:53:03 INFO]: Training loss at epoch 35: 0.9006442129611969
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:53:04 INFO]: New best epoch, val score: -0.8475124466726025
[08/12/2025 16:53:04 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:53:08 INFO]: Training loss at epoch 55: 0.8601878881454468
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:53:09 INFO]: Training loss at epoch 61: 0.6993358135223389
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:53:11 INFO]: Running Final Evaluation...
[W 2025-08-12 16:53:11,353] Trial 0 failed with parameters: {'embedding_head_pair': (256, 4), 'n_layers': 2, 'd_ffn_factor': 1.873802219943948, 'attention_dropout': 0.45353164974231497, 'activation': 'reglu', 'lr': 0.00011070888813770057} because of the following error: RuntimeError('Error(s) in loading state_dict for FTTransformer:\n\tsize mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 384]) from checkpoint, the shape in current model is torch.Size([101, 256]).\n\tsize mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 384]) from checkpoint, the shape in current model is torch.Size([100, 256]).\n\tsize mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([500, 384]) from checkpoint, the shape in current model is torch.Size([958, 256]).\n\tsize mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([500]) from checkpoint, the shape in current model is torch.Size([958]).\n\tsize mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([384, 500]) from checkpoint, the shape in current model is torch.Size([256, 479]).\n\tsize mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([500, 384]) from checkpoint, the shape in current model is torch.Size([958, 256]).\n\tsize mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([500]) from checkpoint, the shape in current model is torch.Size([958]).\n\tsize mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([384, 500]) from checkpoint, the shape in current model is torch.Size([256, 479]).\n\tsize mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for last_normalization.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for last_normalization.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for head.weight: copying a param with shape torch.Size([1, 384]) from checkpoint, the shape in current model is torch.Size([1, 256]).').
Traceback (most recent call last):
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 145, in <lambda>
    func = lambda trial: objective(trial, cfg, trial_stats, trial_counter, n_optuna_trials,
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 104, in objective
    stats = train_net_for_optuna.main(config, loaders, unique_categories, n_numerical, n_classes)
  File "/temporario2/14570653/tabular-transfer-learning/train_net_for_optuna.py", line 127, in main
    net.load_state_dict(torch.load(checkpoint_path)["net"])
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2215, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for FTTransformer:
	size mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 384]) from checkpoint, the shape in current model is torch.Size([101, 256]).
	size mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 384]) from checkpoint, the shape in current model is torch.Size([100, 256]).
	size mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([500, 384]) from checkpoint, the shape in current model is torch.Size([958, 256]).
	size mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([500]) from checkpoint, the shape in current model is torch.Size([958]).
	size mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([384, 500]) from checkpoint, the shape in current model is torch.Size([256, 479]).
	size mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([500, 384]) from checkpoint, the shape in current model is torch.Size([958, 256]).
	size mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([500]) from checkpoint, the shape in current model is torch.Size([958]).
	size mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([384, 500]) from checkpoint, the shape in current model is torch.Size([256, 479]).
	size mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for last_normalization.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for last_normalization.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for head.weight: copying a param with shape torch.Size([1, 384]) from checkpoint, the shape in current model is torch.Size([1, 256]).
[W 2025-08-12 16:53:11,359] Trial 0 failed with value None.
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:53:25 INFO]: Training loss at epoch 39: 0.6238765716552734
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:53:25 INFO]: Training loss at epoch 56: 0.7239271700382233
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:53:28 INFO]: Training loss at epoch 36: 0.8028440773487091
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:53:29 INFO]: Training loss at epoch 36: 0.8540826141834259
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:53:33 INFO]: Training stats: {
    "score": -0.8084099995454681,
    "rmse": 0.8084099995454681
}
[08/12/2025 16:53:33 INFO]: Val stats: {
    "score": -0.8526975604985659,
    "rmse": 0.8526975604985659
}
[08/12/2025 16:53:33 INFO]: Test stats: {
    "score": -0.7886201901038837,
    "rmse": 0.7886201901038837
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:53:33 INFO]: Training loss at epoch 17: 0.9373595118522644
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:53:36 INFO]: Training loss at epoch 21: 0.8621900081634521
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:53:38 INFO]: Training loss at epoch 10: 1.0068269968032837
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:53:41 INFO]: Training loss at epoch 57: 0.8042330741882324
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:53:54 INFO]: Training loss at epoch 37: 0.827298492193222
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:53:54 INFO]: Training loss at epoch 37: 0.8454272449016571
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:53:56 INFO]: Training loss at epoch 40: 0.6209202706813812
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:53:58 INFO]: Training loss at epoch 58: 0.6252782493829727
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:54:00 INFO]: New best epoch, val score: -0.8710995921077103
[08/12/2025 16:54:00 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:54:14 INFO]: Training loss at epoch 59: 0.8219397366046906
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:54:19 INFO]: Training loss at epoch 38: 0.7366683781147003
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:54:19 INFO]: Training loss at epoch 22: 1.0894729495048523
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:54:20 INFO]: Training loss at epoch 41: 0.5557540506124496
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:54:20 INFO]: Training loss at epoch 38: 0.9227764904499054
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:54:20 INFO]: Training stats: {
    "score": -0.8815994437292402,
    "rmse": 0.8815994437292402
}
[08/12/2025 16:54:20 INFO]: Val stats: {
    "score": -0.8687859772525567,
    "rmse": 0.8687859772525567
}
[08/12/2025 16:54:20 INFO]: Test stats: {
    "score": -0.8157966975326847,
    "rmse": 0.8157966975326847
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:54:22 INFO]: New best epoch, val score: -0.8687859772525567
[08/12/2025 16:54:22 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:54:23 INFO]: Running Final Evaluation...
[W 2025-08-12 16:54:23,676] Trial 8 failed with parameters: {'embedding_head_pair': (64, 8), 'n_layers': 8, 'd_ffn_factor': 2.133941302571719, 'attention_dropout': 0.07555298598593957, 'activation': 'relu', 'lr': 0.0003839557775831153} because of the following error: RuntimeError('Error(s) in loading state_dict for FTTransformer:\n\tMissing key(s) in state_dict: "layers.4.attention.W_q.weight", "layers.4.attention.W_q.bias", "layers.4.attention.W_k.weight", "layers.4.attention.W_k.bias", "layers.4.attention.W_v.weight", "layers.4.attention.W_v.bias", "layers.4.attention.W_out.weight", "layers.4.attention.W_out.bias", "layers.4.linear0.weight", "layers.4.linear0.bias", "layers.4.linear1.weight", "layers.4.linear1.bias", "layers.4.norm1.weight", "layers.4.norm1.bias", "layers.4.norm0.weight", "layers.4.norm0.bias", "layers.5.attention.W_q.weight", "layers.5.attention.W_q.bias", "layers.5.attention.W_k.weight", "layers.5.attention.W_k.bias", "layers.5.attention.W_v.weight", "layers.5.attention.W_v.bias", "layers.5.attention.W_out.weight", "layers.5.attention.W_out.bias", "layers.5.linear0.weight", "layers.5.linear0.bias", "layers.5.linear1.weight", "layers.5.linear1.bias", "layers.5.norm1.weight", "layers.5.norm1.bias", "layers.5.norm0.weight", "layers.5.norm0.bias", "layers.6.attention.W_q.weight", "layers.6.attention.W_q.bias", "layers.6.attention.W_k.weight", "layers.6.attention.W_k.bias", "layers.6.attention.W_v.weight", "layers.6.attention.W_v.bias", "layers.6.attention.W_out.weight", "layers.6.attention.W_out.bias", "layers.6.linear0.weight", "layers.6.linear0.bias", "layers.6.linear1.weight", "layers.6.linear1.bias", "layers.6.norm1.weight", "layers.6.norm1.bias", "layers.6.norm0.weight", "layers.6.norm0.bias", "layers.7.attention.W_q.weight", "layers.7.attention.W_q.bias", "layers.7.attention.W_k.weight", "layers.7.attention.W_k.bias", "layers.7.attention.W_v.weight", "layers.7.attention.W_v.bias", "layers.7.attention.W_out.weight", "layers.7.attention.W_out.bias", "layers.7.linear0.weight", "layers.7.linear0.bias", "layers.7.linear1.weight", "layers.7.linear1.bias", "layers.7.norm1.weight", "layers.7.norm1.bias", "layers.7.norm0.weight", "layers.7.norm0.bias". \n\tsize mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 128]) from checkpoint, the shape in current model is torch.Size([100, 64]).\n\tsize mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([254, 128]) from checkpoint, the shape in current model is torch.Size([136, 64]).\n\tsize mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([254]) from checkpoint, the shape in current model is torch.Size([136]).\n\tsize mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([128, 127]) from checkpoint, the shape in current model is torch.Size([64, 136]).\n\tsize mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([254, 128]) from checkpoint, the shape in current model is torch.Size([136, 64]).\n\tsize mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([254]) from checkpoint, the shape in current model is torch.Size([136]).\n\tsize mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([128, 127]) from checkpoint, the shape in current model is torch.Size([64, 136]).\n\tsize mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.2.attention.W_q.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.2.attention.W_q.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.2.attention.W_k.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.2.attention.W_k.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.2.attention.W_v.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.2.attention.W_v.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.2.attention.W_out.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.2.attention.W_out.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.2.linear0.weight: copying a param with shape torch.Size([254, 128]) from checkpoint, the shape in current model is torch.Size([136, 64]).\n\tsize mismatch for layers.2.linear0.bias: copying a param with shape torch.Size([254]) from checkpoint, the shape in current model is torch.Size([136]).\n\tsize mismatch for layers.2.linear1.weight: copying a param with shape torch.Size([128, 127]) from checkpoint, the shape in current model is torch.Size([64, 136]).\n\tsize mismatch for layers.2.linear1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.2.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.2.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.2.norm0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.2.norm0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.3.attention.W_q.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.3.attention.W_q.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.3.attention.W_k.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.3.attention.W_k.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.3.attention.W_v.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.3.attention.W_v.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.3.attention.W_out.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.3.attention.W_out.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.3.linear0.weight: copying a param with shape torch.Size([254, 128]) from checkpoint, the shape in current model is torch.Size([136, 64]).\n\tsize mismatch for layers.3.linear0.bias: copying a param with shape torch.Size([254]) from checkpoint, the shape in current model is torch.Size([136]).\n\tsize mismatch for layers.3.linear1.weight: copying a param with shape torch.Size([128, 127]) from checkpoint, the shape in current model is torch.Size([64, 136]).\n\tsize mismatch for layers.3.linear1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.3.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.3.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.3.norm0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.3.norm0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for last_normalization.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for last_normalization.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for head.weight: copying a param with shape torch.Size([1, 128]) from checkpoint, the shape in current model is torch.Size([1, 64]).').
Traceback (most recent call last):
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 145, in <lambda>
    func = lambda trial: objective(trial, cfg, trial_stats, trial_counter, n_optuna_trials,
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 104, in objective
    stats = train_net_for_optuna.main(config, loaders, unique_categories, n_numerical, n_classes)
  File "/temporario2/14570653/tabular-transfer-learning/train_net_for_optuna.py", line 127, in main
    net.load_state_dict(torch.load(checkpoint_path)["net"])
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2215, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for FTTransformer:
	Missing key(s) in state_dict: "layers.4.attention.W_q.weight", "layers.4.attention.W_q.bias", "layers.4.attention.W_k.weight", "layers.4.attention.W_k.bias", "layers.4.attention.W_v.weight", "layers.4.attention.W_v.bias", "layers.4.attention.W_out.weight", "layers.4.attention.W_out.bias", "layers.4.linear0.weight", "layers.4.linear0.bias", "layers.4.linear1.weight", "layers.4.linear1.bias", "layers.4.norm1.weight", "layers.4.norm1.bias", "layers.4.norm0.weight", "layers.4.norm0.bias", "layers.5.attention.W_q.weight", "layers.5.attention.W_q.bias", "layers.5.attention.W_k.weight", "layers.5.attention.W_k.bias", "layers.5.attention.W_v.weight", "layers.5.attention.W_v.bias", "layers.5.attention.W_out.weight", "layers.5.attention.W_out.bias", "layers.5.linear0.weight", "layers.5.linear0.bias", "layers.5.linear1.weight", "layers.5.linear1.bias", "layers.5.norm1.weight", "layers.5.norm1.bias", "layers.5.norm0.weight", "layers.5.norm0.bias", "layers.6.attention.W_q.weight", "layers.6.attention.W_q.bias", "layers.6.attention.W_k.weight", "layers.6.attention.W_k.bias", "layers.6.attention.W_v.weight", "layers.6.attention.W_v.bias", "layers.6.attention.W_out.weight", "layers.6.attention.W_out.bias", "layers.6.linear0.weight", "layers.6.linear0.bias", "layers.6.linear1.weight", "layers.6.linear1.bias", "layers.6.norm1.weight", "layers.6.norm1.bias", "layers.6.norm0.weight", "layers.6.norm0.bias", "layers.7.attention.W_q.weight", "layers.7.attention.W_q.bias", "layers.7.attention.W_k.weight", "layers.7.attention.W_k.bias", "layers.7.attention.W_v.weight", "layers.7.attention.W_v.bias", "layers.7.attention.W_out.weight", "layers.7.attention.W_out.bias", "layers.7.linear0.weight", "layers.7.linear0.bias", "layers.7.linear1.weight", "layers.7.linear1.bias", "layers.7.norm1.weight", "layers.7.norm1.bias", "layers.7.norm0.weight", "layers.7.norm0.bias". 
	size mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).
	size mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 128]) from checkpoint, the shape in current model is torch.Size([100, 64]).
	size mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([254, 128]) from checkpoint, the shape in current model is torch.Size([136, 64]).
	size mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([254]) from checkpoint, the shape in current model is torch.Size([136]).
	size mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([128, 127]) from checkpoint, the shape in current model is torch.Size([64, 136]).
	size mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([254, 128]) from checkpoint, the shape in current model is torch.Size([136, 64]).
	size mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([254]) from checkpoint, the shape in current model is torch.Size([136]).
	size mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([128, 127]) from checkpoint, the shape in current model is torch.Size([64, 136]).
	size mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.2.attention.W_q.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.2.attention.W_q.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.2.attention.W_k.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.2.attention.W_k.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.2.attention.W_v.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.2.attention.W_v.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.2.attention.W_out.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.2.attention.W_out.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.2.linear0.weight: copying a param with shape torch.Size([254, 128]) from checkpoint, the shape in current model is torch.Size([136, 64]).
	size mismatch for layers.2.linear0.bias: copying a param with shape torch.Size([254]) from checkpoint, the shape in current model is torch.Size([136]).
	size mismatch for layers.2.linear1.weight: copying a param with shape torch.Size([128, 127]) from checkpoint, the shape in current model is torch.Size([64, 136]).
	size mismatch for layers.2.linear1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.2.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.2.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.2.norm0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.2.norm0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.3.attention.W_q.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.3.attention.W_q.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.3.attention.W_k.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.3.attention.W_k.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.3.attention.W_v.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.3.attention.W_v.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.3.attention.W_out.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.3.attention.W_out.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.3.linear0.weight: copying a param with shape torch.Size([254, 128]) from checkpoint, the shape in current model is torch.Size([136, 64]).
	size mismatch for layers.3.linear0.bias: copying a param with shape torch.Size([254]) from checkpoint, the shape in current model is torch.Size([136]).
	size mismatch for layers.3.linear1.weight: copying a param with shape torch.Size([128, 127]) from checkpoint, the shape in current model is torch.Size([64, 136]).
	size mismatch for layers.3.linear1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.3.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.3.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.3.norm0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.3.norm0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for last_normalization.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for last_normalization.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for head.weight: copying a param with shape torch.Size([1, 128]) from checkpoint, the shape in current model is torch.Size([1, 64]).
[W 2025-08-12 16:54:23,683] Trial 8 failed with value None.
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:54:27 INFO]: Training loss at epoch 18: 1.0186675488948822
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:54:28 INFO]: Training loss at epoch 10: 1.175411731004715
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:54:36 INFO]: Training loss at epoch 60: 0.801904946565628
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:54:43 INFO]: Training loss at epoch 42: 0.5681826770305634
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:54:44 INFO]: Training loss at epoch 39: 0.9496535062789917
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:54:53 INFO]: Training stats: {
    "score": -0.8300322632031821,
    "rmse": 0.8300322632031821
}
[08/12/2025 16:54:53 INFO]: Val stats: {
    "score": -0.9021628781838219,
    "rmse": 0.9021628781838219
}
[08/12/2025 16:54:53 INFO]: Test stats: {
    "score": -0.7928622787309073,
    "rmse": 0.7928622787309073
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:54:53 INFO]: Training loss at epoch 61: 0.6804541945457458
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:55:02 INFO]: Training loss at epoch 23: 0.9824987053871155
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:55:05 INFO]: Training loss at epoch 11: 1.2405508160591125
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:55:06 INFO]: Training loss at epoch 43: 0.6657252311706543
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:55:09 INFO]: Training loss at epoch 62: 0.895669162273407
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:55:11 INFO]: New best epoch, val score: -0.8667255010079468
[08/12/2025 16:55:11 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:55:18 INFO]: Training loss at epoch 40: 0.7963515222072601
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:55:20 INFO]: Training loss at epoch 19: 0.9457167088985443
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:55:25 INFO]: Training loss at epoch 63: 0.8151869475841522
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:55:27 INFO]: New best epoch, val score: -0.8652957542446097
[08/12/2025 16:55:27 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:55:30 INFO]: Training loss at epoch 44: 0.5681548714637756
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:55:38 INFO]: Training stats: {
    "score": -0.9974058817368413,
    "rmse": 0.9974058817368413
}
[08/12/2025 16:55:38 INFO]: Val stats: {
    "score": -0.9869166814979501,
    "rmse": 0.9869166814979501
}
[08/12/2025 16:55:38 INFO]: Test stats: {
    "score": -0.9472179056373672,
    "rmse": 0.9472179056373672
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:55:42 INFO]: Training loss at epoch 64: 0.8531780540943146
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:55:42 INFO]: Training loss at epoch 41: 0.7980968654155731
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:55:44 INFO]: New best epoch, val score: -0.8650016152327884
[08/12/2025 16:55:44 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:55:45 INFO]: Training loss at epoch 24: 0.8714694976806641
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:55:53 INFO]: Training loss at epoch 45: 0.6208983361721039
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:55:58 INFO]: Training loss at epoch 11: 0.9838035404682159
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:55:58 INFO]: Training loss at epoch 65: 0.7923720479011536
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:56:07 INFO]: Training loss at epoch 42: 0.7610271275043488
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:56:15 INFO]: Training loss at epoch 66: 0.9180958271026611
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:56:16 INFO]: Training loss at epoch 46: 0.5040386468172073
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:56:28 INFO]: Training loss at epoch 25: 1.0629770755767822
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:56:31 INFO]: Training loss at epoch 67: 0.8867400288581848
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:56:31 INFO]: Training loss at epoch 20: 0.9502221345901489
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:56:32 INFO]: Training loss at epoch 43: 0.7301134169101715
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:56:33 INFO]: New best epoch, val score: -0.8616363736253089
[08/12/2025 16:56:33 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:56:33 INFO]: Training loss at epoch 12: 1.4800215363502502
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:56:40 INFO]: Training loss at epoch 47: 0.5558490753173828
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:56:47 INFO]: Training loss at epoch 68: 0.6550683975219727
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:56:49 INFO]: New best epoch, val score: -0.8567549605809003
[08/12/2025 16:56:49 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:56:57 INFO]: Training loss at epoch 44: 0.7431451678276062
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:57:03 INFO]: Training loss at epoch 69: 0.8324477672576904
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:57:03 INFO]: Training loss at epoch 48: 0.7327958643436432
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:57:09 INFO]: Training stats: {
    "score": -0.8610500125232781,
    "rmse": 0.8610500125232781
}
[08/12/2025 16:57:09 INFO]: Val stats: {
    "score": -0.8523472944731023,
    "rmse": 0.8523472944731023
}
[08/12/2025 16:57:09 INFO]: Test stats: {
    "score": -0.7987035232291897,
    "rmse": 0.7987035232291897
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:57:10 INFO]: Training loss at epoch 26: 0.9601622819900513
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:57:11 INFO]: New best epoch, val score: -0.8523472944731023
[08/12/2025 16:57:11 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:57:22 INFO]: Training loss at epoch 45: 0.6400695443153381
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:57:23 INFO]: Training loss at epoch 21: 1.0919766426086426
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:57:25 INFO]: Training loss at epoch 70: 0.8808808028697968
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:57:27 INFO]: New best epoch, val score: -0.848438567386806
[08/12/2025 16:57:27 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:57:27 INFO]: Training loss at epoch 49: 0.6510366499423981
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:57:29 INFO]: Training loss at epoch 12: 0.844107985496521
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:57:35 INFO]: Training stats: {
    "score": -0.7255576873092862,
    "rmse": 0.7255576873092862
}
[08/12/2025 16:57:35 INFO]: Val stats: {
    "score": -0.8485545884490747,
    "rmse": 0.8485545884490747
}
[08/12/2025 16:57:35 INFO]: Test stats: {
    "score": -0.7286709599494076,
    "rmse": 0.7286709599494076
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:57:41 INFO]: Training loss at epoch 71: 0.7323607504367828
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:57:43 INFO]: New best epoch, val score: -0.8467548078458284
[08/12/2025 16:57:43 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:57:48 INFO]: Training loss at epoch 46: 0.8288609683513641
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:57:52 INFO]: Training loss at epoch 27: 0.9851265251636505
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:57:57 INFO]: Training loss at epoch 72: 0.7207098603248596
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:57:58 INFO]: Training loss at epoch 50: 0.5273239016532898
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:57:59 INFO]: New best epoch, val score: -0.8459027999079162
[08/12/2025 16:57:59 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:58:00 INFO]: Training loss at epoch 13: 1.0886061489582062
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:58:01 INFO]: New best epoch, val score: -0.8317227043086819
[08/12/2025 16:58:01 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:58:13 INFO]: Training loss at epoch 47: 0.7126178443431854
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:58:13 INFO]: Training loss at epoch 73: 0.7014011144638062
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:58:15 INFO]: New best epoch, val score: -0.8437942343961925
[08/12/2025 16:58:15 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:58:16 INFO]: Training loss at epoch 22: 0.8841287791728973
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:58:21 INFO]: Training loss at epoch 51: 0.5288369953632355
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:58:24 INFO]: New best epoch, val score: -0.8279088879871501
[08/12/2025 16:58:24 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:58:30 INFO]: Training loss at epoch 74: 0.7653427124023438
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:58:32 INFO]: New best epoch, val score: -0.8417075802364007
[08/12/2025 16:58:32 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:58:35 INFO]: Training loss at epoch 28: 1.1202912032604218
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:58:41 INFO]: Training loss at epoch 48: 0.8273186087608337
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:58:45 INFO]: Training loss at epoch 52: 0.5172024965286255
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:58:46 INFO]: Training loss at epoch 75: 0.7293404340744019
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:58:48 INFO]: New best epoch, val score: -0.8270499426434432
[08/12/2025 16:58:48 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:58:49 INFO]: New best epoch, val score: -0.8375132639020997
[08/12/2025 16:58:49 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:59:00 INFO]: Training loss at epoch 13: 0.923536479473114
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:59:04 INFO]: Training loss at epoch 76: 0.6524124443531036
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:59:06 INFO]: New best epoch, val score: -0.832415453234751
[08/12/2025 16:59:06 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:59:09 INFO]: Training loss at epoch 49: 0.8213585317134857
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:59:09 INFO]: Training loss at epoch 53: 0.5514542162418365
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:59:10 INFO]: Training loss at epoch 23: 0.9509662985801697
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:59:11 INFO]: New best epoch, val score: -0.8218903624918301
[08/12/2025 16:59:11 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:59:18 INFO]: Training stats: {
    "score": -0.8755863780646856,
    "rmse": 0.8755863780646856
}
[08/12/2025 16:59:18 INFO]: Val stats: {
    "score": -0.9748622546377068,
    "rmse": 0.9748622546377068
}
[08/12/2025 16:59:18 INFO]: Test stats: {
    "score": -0.8919257923091832,
    "rmse": 0.8919257923091832
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:59:19 INFO]: Training loss at epoch 29: 1.0898972153663635
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:59:20 INFO]: Training loss at epoch 77: 0.6378558427095413
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:59:22 INFO]: New best epoch, val score: -0.8277460043468338
[08/12/2025 16:59:22 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:59:27 INFO]: Training loss at epoch 14: 0.7577029466629028
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:59:32 INFO]: Training loss at epoch 54: 0.4446942061185837
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:59:34 INFO]: Training stats: {
    "score": -0.9796808370899668,
    "rmse": 0.9796808370899668
}
[08/12/2025 16:59:34 INFO]: Val stats: {
    "score": -0.9525471089033016,
    "rmse": 0.9525471089033016
}
[08/12/2025 16:59:34 INFO]: Test stats: {
    "score": -0.9136308358362567,
    "rmse": 0.9136308358362567
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:59:36 INFO]: New best epoch, val score: -0.837027698166757
[08/12/2025 16:59:36 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:59:37 INFO]: Training loss at epoch 78: 0.6326245069503784
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:59:39 INFO]: New best epoch, val score: -0.8241658820232904
[08/12/2025 16:59:39 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:59:44 INFO]: Training loss at epoch 50: 0.6725557148456573
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:59:54 INFO]: Training loss at epoch 79: 0.7270158231258392
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 16:59:55 INFO]: Training loss at epoch 55: 0.474445641040802
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:00:00 INFO]: Training stats: {
    "score": -0.8463904989362321,
    "rmse": 0.8463904989362321
}
[08/12/2025 17:00:00 INFO]: Val stats: {
    "score": -0.8219133296357412,
    "rmse": 0.8219133296357412
}
[08/12/2025 17:00:00 INFO]: Test stats: {
    "score": -0.7823346691665699,
    "rmse": 0.7823346691665699
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:00:02 INFO]: New best epoch, val score: -0.8219133296357412
[08/12/2025 17:00:02 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:00:04 INFO]: Training loss at epoch 24: 1.044418215751648
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:00:10 INFO]: Training loss at epoch 51: 0.8400881588459015
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:00:16 INFO]: Training loss at epoch 80: 0.628859132528305
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:00:17 INFO]: Training loss at epoch 30: 0.9169781804084778
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:00:18 INFO]: New best epoch, val score: -0.8217451522234176
[08/12/2025 17:00:18 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:00:19 INFO]: Training loss at epoch 56: 0.40921083092689514
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:00:29 INFO]: Training loss at epoch 14: 0.9971930682659149
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:00:33 INFO]: Training loss at epoch 81: 0.6329051852226257
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:00:35 INFO]: New best epoch, val score: -0.820722005234171
[08/12/2025 17:00:35 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:00:36 INFO]: Training loss at epoch 52: 0.6236719489097595
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:00:42 INFO]: Training loss at epoch 57: 0.48885537683963776
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:00:44 INFO]: New best epoch, val score: -0.8142106523234147
[08/12/2025 17:00:44 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:00:50 INFO]: Training loss at epoch 82: 0.7400796115398407
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:00:52 INFO]: New best epoch, val score: -0.8204844400026076
[08/12/2025 17:00:52 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:00:52 INFO]: Training loss at epoch 15: 0.6390053182840347
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:00:58 INFO]: Training loss at epoch 25: 0.9819690585136414
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:01:01 INFO]: Training loss at epoch 31: 1.0302200317382812
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:01:01 INFO]: Training loss at epoch 53: 0.7458082735538483
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:01:05 INFO]: Training loss at epoch 58: 0.4782925546169281
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:01:06 INFO]: Training loss at epoch 83: 0.8592252135276794
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:01:08 INFO]: New best epoch, val score: -0.8082432829901974
[08/12/2025 17:01:08 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:01:23 INFO]: Training loss at epoch 84: 0.7373582720756531
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:01:28 INFO]: Training loss at epoch 54: 0.6686837673187256
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:01:29 INFO]: Training loss at epoch 59: 0.43292538821697235
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:01:36 INFO]: Training stats: {
    "score": -0.6780770993132983,
    "rmse": 0.6780770993132983
}
[08/12/2025 17:01:36 INFO]: Val stats: {
    "score": -0.8183132209436518,
    "rmse": 0.8183132209436518
}
[08/12/2025 17:01:36 INFO]: Test stats: {
    "score": -0.7186198464607791,
    "rmse": 0.7186198464607791
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:01:39 INFO]: Training loss at epoch 85: 0.9291923344135284
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:01:44 INFO]: Training loss at epoch 32: 1.0505266189575195
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:01:51 INFO]: Training loss at epoch 26: 1.0628138780593872
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:01:53 INFO]: Training loss at epoch 55: 0.7848667502403259
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:01:56 INFO]: Training loss at epoch 86: 0.9218459129333496
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:02:00 INFO]: Training loss at epoch 60: 0.46831488609313965
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:02:00 INFO]: Training loss at epoch 15: 1.064142495393753
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:02:12 INFO]: Training loss at epoch 87: 0.7651170194149017
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:02:18 INFO]: Training loss at epoch 16: 0.9229593276977539
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:02:19 INFO]: Training loss at epoch 56: 0.5684700310230255
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:02:23 INFO]: Training loss at epoch 61: 0.5494515746831894
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:02:27 INFO]: Training loss at epoch 33: 0.8281269669532776
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:02:28 INFO]: Training loss at epoch 88: 0.6342134475708008
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:02:44 INFO]: Training loss at epoch 57: 0.7120146751403809
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:02:44 INFO]: Training loss at epoch 27: 0.9575284719467163
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:02:45 INFO]: Training loss at epoch 89: 0.7505929470062256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:02:46 INFO]: Training loss at epoch 62: 0.3913854509592056
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:02:51 INFO]: Training stats: {
    "score": -0.8318306361643326,
    "rmse": 0.8318306361643326
}
[08/12/2025 17:02:51 INFO]: Val stats: {
    "score": -0.8715464662981545,
    "rmse": 0.8715464662981545
}
[08/12/2025 17:02:51 INFO]: Test stats: {
    "score": -0.7932500364918427,
    "rmse": 0.7932500364918427
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:03:07 INFO]: Training loss at epoch 90: 0.7729185521602631
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:03:09 INFO]: Training loss at epoch 63: 0.5773443281650543
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:03:10 INFO]: Training loss at epoch 58: 0.7072644531726837
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:03:11 INFO]: Training loss at epoch 34: 1.078406810760498
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:03:24 INFO]: Training loss at epoch 91: 0.720047652721405
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:03:29 INFO]: Training loss at epoch 16: 1.0438814461231232
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:03:32 INFO]: Training loss at epoch 64: 0.3783288896083832
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:03:35 INFO]: Training loss at epoch 59: 0.581336498260498
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:03:38 INFO]: Training loss at epoch 28: 0.9288430511951447
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:03:40 INFO]: Training loss at epoch 92: 0.7762739360332489
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:03:43 INFO]: Training loss at epoch 17: 1.148143857717514
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:03:44 INFO]: Training stats: {
    "score": -0.9195349950465165,
    "rmse": 0.9195349950465165
}
[08/12/2025 17:03:44 INFO]: Val stats: {
    "score": -1.1127040167588793,
    "rmse": 1.1127040167588793
}
[08/12/2025 17:03:44 INFO]: Test stats: {
    "score": -0.9371317845575925,
    "rmse": 0.9371317845575925
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:03:54 INFO]: Training loss at epoch 35: 0.9791663587093353
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:03:55 INFO]: Training loss at epoch 65: 0.39693333208560944
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:03:56 INFO]: Training loss at epoch 93: 0.7709291875362396
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:03:58 INFO]: New best epoch, val score: -0.8141662287784147
[08/12/2025 17:03:58 INFO]: Saving model to: model_best.pth
[08/12/2025 17:03:58 INFO]: New best epoch, val score: -0.8063026305488337
[08/12/2025 17:03:58 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:04:10 INFO]: Training loss at epoch 60: 0.6949208676815033
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:04:13 INFO]: Training loss at epoch 94: 0.7452206611633301
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:04:15 INFO]: New best epoch, val score: -0.8044827075033794
[08/12/2025 17:04:15 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:04:20 INFO]: Training loss at epoch 66: 0.3741008788347244
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:04:22 INFO]: New best epoch, val score: -0.7946490509462977
[08/12/2025 17:04:22 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:04:30 INFO]: Training loss at epoch 95: 0.614937424659729
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:04:32 INFO]: Training loss at epoch 29: 1.168770283460617
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:04:32 INFO]: New best epoch, val score: -0.7998692732403736
[08/12/2025 17:04:32 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:04:36 INFO]: Training loss at epoch 61: 0.7515788376331329
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:04:37 INFO]: Training loss at epoch 36: 0.9616752564907074
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:04:43 INFO]: Training loss at epoch 67: 0.35403886437416077
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:04:46 INFO]: Training loss at epoch 96: 0.8497694432735443
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:04:48 INFO]: New best epoch, val score: -0.7981289548083309
[08/12/2025 17:04:48 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:04:51 INFO]: Training stats: {
    "score": -0.978823941026545,
    "rmse": 0.978823941026545
}
[08/12/2025 17:04:51 INFO]: Val stats: {
    "score": -0.9379313382555292,
    "rmse": 0.9379313382555292
}
[08/12/2025 17:04:51 INFO]: Test stats: {
    "score": -0.9101652075892517,
    "rmse": 0.9101652075892517
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:05:00 INFO]: Training loss at epoch 17: 0.9844024777412415
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:05:02 INFO]: Training loss at epoch 62: 0.7064261138439178
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:05:03 INFO]: Training loss at epoch 97: 0.6249971091747284
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:05:06 INFO]: New best epoch, val score: -0.7977396353857322
[08/12/2025 17:05:06 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:05:07 INFO]: Training loss at epoch 68: 0.3544531464576721
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:05:09 INFO]: Training loss at epoch 18: 0.7010741233825684
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:05:10 INFO]: New best epoch, val score: -0.7905691818384908
[08/12/2025 17:05:10 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:05:20 INFO]: Training loss at epoch 98: 0.5861596763134003
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:05:21 INFO]: Training loss at epoch 37: 0.9331698715686798
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:05:28 INFO]: Training loss at epoch 63: 0.613554447889328
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:05:31 INFO]: Training loss at epoch 69: 0.43773338198661804
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:05:31 INFO]: New best epoch, val score: -0.8343294854288938
[08/12/2025 17:05:31 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:05:37 INFO]: Training loss at epoch 99: 0.7649282217025757
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:05:39 INFO]: Training stats: {
    "score": -0.6277699036822837,
    "rmse": 0.6277699036822837
}
[08/12/2025 17:05:39 INFO]: Val stats: {
    "score": -0.806844581401928,
    "rmse": 0.806844581401928
}
[08/12/2025 17:05:39 INFO]: Test stats: {
    "score": -0.687320605461471,
    "rmse": 0.687320605461471
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:05:43 INFO]: Training stats: {
    "score": -0.8140145225394108,
    "rmse": 0.8140145225394108
}
[08/12/2025 17:05:43 INFO]: Val stats: {
    "score": -0.8029607976688873,
    "rmse": 0.8029607976688873
}
[08/12/2025 17:05:43 INFO]: Test stats: {
    "score": -0.7619995498397839,
    "rmse": 0.7619995498397839
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:05:45 INFO]: Training loss at epoch 30: 1.0761127471923828
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:05:54 INFO]: Training loss at epoch 64: 0.6094111353158951
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:05:57 INFO]: New best epoch, val score: -0.8315242540277863
[08/12/2025 17:05:57 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:05:59 INFO]: Training loss at epoch 100: 0.6307215392589569
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:06:02 INFO]: Training loss at epoch 70: 0.4391278922557831
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:06:04 INFO]: Training loss at epoch 38: 1.111155778169632
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:06:16 INFO]: Training loss at epoch 101: 0.6769483983516693
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:06:19 INFO]: Training loss at epoch 65: 0.7850200533866882
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:06:25 INFO]: Training loss at epoch 71: 0.33976083993911743
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:06:28 INFO]: New best epoch, val score: -0.7722138977055634
[08/12/2025 17:06:28 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:06:31 INFO]: Training loss at epoch 18: 1.1131321489810944
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:06:32 INFO]: Training loss at epoch 102: 0.5937133729457855
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:06:35 INFO]: Training loss at epoch 19: 0.6531085968017578
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:06:38 INFO]: Training loss at epoch 31: 0.9054474234580994
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:06:45 INFO]: Training loss at epoch 66: 0.6692107915878296
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:06:47 INFO]: Training loss at epoch 39: 0.9592965543270111
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:06:48 INFO]: Training loss at epoch 72: 0.39441899955272675
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:06:49 INFO]: Training loss at epoch 103: 0.5778293013572693
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:07:02 INFO]: Training stats: {
    "score": -0.9711504438569897,
    "rmse": 0.9711504438569897
}
[08/12/2025 17:07:02 INFO]: Val stats: {
    "score": -0.9382841360257343,
    "rmse": 0.9382841360257343
}
[08/12/2025 17:07:02 INFO]: Test stats: {
    "score": -0.9014326279149452,
    "rmse": 0.9014326279149452
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:07:04 INFO]: Training stats: {
    "score": -0.8427571631089281,
    "rmse": 0.8427571631089281
}
[08/12/2025 17:07:04 INFO]: Val stats: {
    "score": -0.9539874492502117,
    "rmse": 0.9539874492502117
}
[08/12/2025 17:07:04 INFO]: Test stats: {
    "score": -0.8217618691741827,
    "rmse": 0.8217618691741827
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:07:05 INFO]: Training loss at epoch 104: 0.8088894188404083
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:07:11 INFO]: Training loss at epoch 67: 0.5294068306684494
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:07:12 INFO]: Training loss at epoch 73: 0.47021186351776123
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:07:21 INFO]: Training loss at epoch 105: 0.6684034764766693
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:07:32 INFO]: Training loss at epoch 32: 1.0811509490013123
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:07:35 INFO]: Training loss at epoch 74: 0.3525565266609192
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:07:36 INFO]: Training loss at epoch 68: 0.548904225230217
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:07:38 INFO]: Training loss at epoch 106: 0.7486066520214081
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:07:45 INFO]: Training loss at epoch 40: 0.8710676431655884
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:07:50 INFO]: Running Final Evaluation...
[W 2025-08-12 17:07:50,850] Trial 2 failed with parameters: {'embedding_head_pair': (128, 8), 'n_layers': 8, 'd_ffn_factor': 2.61923853304754, 'attention_dropout': 0.4199929106407093, 'activation': 'gelu', 'lr': 5.472817999009264e-05} because of the following error: RuntimeError('Error(s) in loading state_dict for FTTransformer:\n\tMissing key(s) in state_dict: "layers.2.attention.W_q.weight", "layers.2.attention.W_q.bias", "layers.2.attention.W_k.weight", "layers.2.attention.W_k.bias", "layers.2.attention.W_v.weight", "layers.2.attention.W_v.bias", "layers.2.attention.W_out.weight", "layers.2.attention.W_out.bias", "layers.2.linear0.weight", "layers.2.linear0.bias", "layers.2.linear1.weight", "layers.2.linear1.bias", "layers.2.norm1.weight", "layers.2.norm1.bias", "layers.2.norm0.weight", "layers.2.norm0.bias", "layers.3.attention.W_q.weight", "layers.3.attention.W_q.bias", "layers.3.attention.W_k.weight", "layers.3.attention.W_k.bias", "layers.3.attention.W_v.weight", "layers.3.attention.W_v.bias", "layers.3.attention.W_out.weight", "layers.3.attention.W_out.bias", "layers.3.linear0.weight", "layers.3.linear0.bias", "layers.3.linear1.weight", "layers.3.linear1.bias", "layers.3.norm1.weight", "layers.3.norm1.bias", "layers.3.norm0.weight", "layers.3.norm0.bias", "layers.4.attention.W_q.weight", "layers.4.attention.W_q.bias", "layers.4.attention.W_k.weight", "layers.4.attention.W_k.bias", "layers.4.attention.W_v.weight", "layers.4.attention.W_v.bias", "layers.4.attention.W_out.weight", "layers.4.attention.W_out.bias", "layers.4.linear0.weight", "layers.4.linear0.bias", "layers.4.linear1.weight", "layers.4.linear1.bias", "layers.4.norm1.weight", "layers.4.norm1.bias", "layers.4.norm0.weight", "layers.4.norm0.bias", "layers.5.attention.W_q.weight", "layers.5.attention.W_q.bias", "layers.5.attention.W_k.weight", "layers.5.attention.W_k.bias", "layers.5.attention.W_v.weight", "layers.5.attention.W_v.bias", "layers.5.attention.W_out.weight", "layers.5.attention.W_out.bias", "layers.5.linear0.weight", "layers.5.linear0.bias", "layers.5.linear1.weight", "layers.5.linear1.bias", "layers.5.norm1.weight", "layers.5.norm1.bias", "layers.5.norm0.weight", "layers.5.norm0.bias", "layers.6.attention.W_q.weight", "layers.6.attention.W_q.bias", "layers.6.attention.W_k.weight", "layers.6.attention.W_k.bias", "layers.6.attention.W_v.weight", "layers.6.attention.W_v.bias", "layers.6.attention.W_out.weight", "layers.6.attention.W_out.bias", "layers.6.linear0.weight", "layers.6.linear0.bias", "layers.6.linear1.weight", "layers.6.linear1.bias", "layers.6.norm1.weight", "layers.6.norm1.bias", "layers.6.norm0.weight", "layers.6.norm0.bias", "layers.7.attention.W_q.weight", "layers.7.attention.W_q.bias", "layers.7.attention.W_k.weight", "layers.7.attention.W_k.bias", "layers.7.attention.W_v.weight", "layers.7.attention.W_v.bias", "layers.7.attention.W_out.weight", "layers.7.attention.W_out.bias", "layers.7.linear0.weight", "layers.7.linear0.bias", "layers.7.linear1.weight", "layers.7.linear1.bias", "layers.7.norm1.weight", "layers.7.norm1.bias", "layers.7.norm0.weight", "layers.7.norm0.bias". \n\tsize mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 384]) from checkpoint, the shape in current model is torch.Size([101, 128]).\n\tsize mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 384]) from checkpoint, the shape in current model is torch.Size([100, 128]).\n\tsize mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([500, 384]) from checkpoint, the shape in current model is torch.Size([335, 128]).\n\tsize mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([500]) from checkpoint, the shape in current model is torch.Size([335]).\n\tsize mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([384, 500]) from checkpoint, the shape in current model is torch.Size([128, 335]).\n\tsize mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([500, 384]) from checkpoint, the shape in current model is torch.Size([335, 128]).\n\tsize mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([500]) from checkpoint, the shape in current model is torch.Size([335]).\n\tsize mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([384, 500]) from checkpoint, the shape in current model is torch.Size([128, 335]).\n\tsize mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for last_normalization.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for last_normalization.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for head.weight: copying a param with shape torch.Size([1, 384]) from checkpoint, the shape in current model is torch.Size([1, 128]).').
Traceback (most recent call last):
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 145, in <lambda>
    func = lambda trial: objective(trial, cfg, trial_stats, trial_counter, n_optuna_trials,
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 104, in objective
    stats = train_net_for_optuna.main(config, loaders, unique_categories, n_numerical, n_classes)
  File "/temporario2/14570653/tabular-transfer-learning/train_net_for_optuna.py", line 127, in main
    net.load_state_dict(torch.load(checkpoint_path)["net"])
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2215, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for FTTransformer:
	Missing key(s) in state_dict: "layers.2.attention.W_q.weight", "layers.2.attention.W_q.bias", "layers.2.attention.W_k.weight", "layers.2.attention.W_k.bias", "layers.2.attention.W_v.weight", "layers.2.attention.W_v.bias", "layers.2.attention.W_out.weight", "layers.2.attention.W_out.bias", "layers.2.linear0.weight", "layers.2.linear0.bias", "layers.2.linear1.weight", "layers.2.linear1.bias", "layers.2.norm1.weight", "layers.2.norm1.bias", "layers.2.norm0.weight", "layers.2.norm0.bias", "layers.3.attention.W_q.weight", "layers.3.attention.W_q.bias", "layers.3.attention.W_k.weight", "layers.3.attention.W_k.bias", "layers.3.attention.W_v.weight", "layers.3.attention.W_v.bias", "layers.3.attention.W_out.weight", "layers.3.attention.W_out.bias", "layers.3.linear0.weight", "layers.3.linear0.bias", "layers.3.linear1.weight", "layers.3.linear1.bias", "layers.3.norm1.weight", "layers.3.norm1.bias", "layers.3.norm0.weight", "layers.3.norm0.bias", "layers.4.attention.W_q.weight", "layers.4.attention.W_q.bias", "layers.4.attention.W_k.weight", "layers.4.attention.W_k.bias", "layers.4.attention.W_v.weight", "layers.4.attention.W_v.bias", "layers.4.attention.W_out.weight", "layers.4.attention.W_out.bias", "layers.4.linear0.weight", "layers.4.linear0.bias", "layers.4.linear1.weight", "layers.4.linear1.bias", "layers.4.norm1.weight", "layers.4.norm1.bias", "layers.4.norm0.weight", "layers.4.norm0.bias", "layers.5.attention.W_q.weight", "layers.5.attention.W_q.bias", "layers.5.attention.W_k.weight", "layers.5.attention.W_k.bias", "layers.5.attention.W_v.weight", "layers.5.attention.W_v.bias", "layers.5.attention.W_out.weight", "layers.5.attention.W_out.bias", "layers.5.linear0.weight", "layers.5.linear0.bias", "layers.5.linear1.weight", "layers.5.linear1.bias", "layers.5.norm1.weight", "layers.5.norm1.bias", "layers.5.norm0.weight", "layers.5.norm0.bias", "layers.6.attention.W_q.weight", "layers.6.attention.W_q.bias", "layers.6.attention.W_k.weight", "layers.6.attention.W_k.bias", "layers.6.attention.W_v.weight", "layers.6.attention.W_v.bias", "layers.6.attention.W_out.weight", "layers.6.attention.W_out.bias", "layers.6.linear0.weight", "layers.6.linear0.bias", "layers.6.linear1.weight", "layers.6.linear1.bias", "layers.6.norm1.weight", "layers.6.norm1.bias", "layers.6.norm0.weight", "layers.6.norm0.bias", "layers.7.attention.W_q.weight", "layers.7.attention.W_q.bias", "layers.7.attention.W_k.weight", "layers.7.attention.W_k.bias", "layers.7.attention.W_v.weight", "layers.7.attention.W_v.bias", "layers.7.attention.W_out.weight", "layers.7.attention.W_out.bias", "layers.7.linear0.weight", "layers.7.linear0.bias", "layers.7.linear1.weight", "layers.7.linear1.bias", "layers.7.norm1.weight", "layers.7.norm1.bias", "layers.7.norm0.weight", "layers.7.norm0.bias". 
	size mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 384]) from checkpoint, the shape in current model is torch.Size([101, 128]).
	size mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 384]) from checkpoint, the shape in current model is torch.Size([100, 128]).
	size mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([500, 384]) from checkpoint, the shape in current model is torch.Size([335, 128]).
	size mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([500]) from checkpoint, the shape in current model is torch.Size([335]).
	size mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([384, 500]) from checkpoint, the shape in current model is torch.Size([128, 335]).
	size mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([500, 384]) from checkpoint, the shape in current model is torch.Size([335, 128]).
	size mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([500]) from checkpoint, the shape in current model is torch.Size([335]).
	size mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([384, 500]) from checkpoint, the shape in current model is torch.Size([128, 335]).
	size mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for last_normalization.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for last_normalization.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for head.weight: copying a param with shape torch.Size([1, 384]) from checkpoint, the shape in current model is torch.Size([1, 128]).
[W 2025-08-12 17:07:50,858] Trial 2 failed with value None.
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:07:54 INFO]: Training loss at epoch 107: 0.6310380101203918
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:07:59 INFO]: Training loss at epoch 75: 0.505951315164566
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:08:01 INFO]: Training loss at epoch 69: 0.7359122633934021
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:08:01 INFO]: Training loss at epoch 19: 0.9536879658699036
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:08:10 INFO]: Training loss at epoch 108: 0.5898045748472214
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:08:10 INFO]: Training stats: {
    "score": -0.7632385261982996,
    "rmse": 0.7632385261982996
}
[08/12/2025 17:08:10 INFO]: Val stats: {
    "score": -0.8939424042925863,
    "rmse": 0.8939424042925863
}
[08/12/2025 17:08:10 INFO]: Test stats: {
    "score": -0.8097220328984049,
    "rmse": 0.8097220328984049
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:08:22 INFO]: Training loss at epoch 76: 0.4099854826927185
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:08:24 INFO]: Training loss at epoch 33: 1.1812866926193237
[08/12/2025 17:08:24 INFO]: New best epoch, val score: -0.7581759781549399
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:08:24 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:08:26 INFO]: Training loss at epoch 109: 0.8365471363067627
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:08:30 INFO]: Training loss at epoch 20: 0.7386954724788666
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:08:31 INFO]: Running Final Evaluation...
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[W 2025-08-12 17:08:31,242] Trial 4 failed with parameters: {'embedding_head_pair': (256, 8), 'n_layers': 6, 'd_ffn_factor': 1.2030227537351803, 'attention_dropout': 0.3097260491115211, 'activation': 'relu', 'lr': 2.4473341990664206e-05} because of the following error: RuntimeError('Error(s) in loading state_dict for FTTransformer:\n\tMissing key(s) in state_dict: "layers.2.attention.W_q.weight", "layers.2.attention.W_q.bias", "layers.2.attention.W_k.weight", "layers.2.attention.W_k.bias", "layers.2.attention.W_v.weight", "layers.2.attention.W_v.bias", "layers.2.attention.W_out.weight", "layers.2.attention.W_out.bias", "layers.2.linear0.weight", "layers.2.linear0.bias", "layers.2.linear1.weight", "layers.2.linear1.bias", "layers.2.norm1.weight", "layers.2.norm1.bias", "layers.2.norm0.weight", "layers.2.norm0.bias", "layers.3.attention.W_q.weight", "layers.3.attention.W_q.bias", "layers.3.attention.W_k.weight", "layers.3.attention.W_k.bias", "layers.3.attention.W_v.weight", "layers.3.attention.W_v.bias", "layers.3.attention.W_out.weight", "layers.3.attention.W_out.bias", "layers.3.linear0.weight", "layers.3.linear0.bias", "layers.3.linear1.weight", "layers.3.linear1.bias", "layers.3.norm1.weight", "layers.3.norm1.bias", "layers.3.norm0.weight", "layers.3.norm0.bias", "layers.4.attention.W_q.weight", "layers.4.attention.W_q.bias", "layers.4.attention.W_k.weight", "layers.4.attention.W_k.bias", "layers.4.attention.W_v.weight", "layers.4.attention.W_v.bias", "layers.4.attention.W_out.weight", "layers.4.attention.W_out.bias", "layers.4.linear0.weight", "layers.4.linear0.bias", "layers.4.linear1.weight", "layers.4.linear1.bias", "layers.4.norm1.weight", "layers.4.norm1.bias", "layers.4.norm0.weight", "layers.4.norm0.bias", "layers.5.attention.W_q.weight", "layers.5.attention.W_q.bias", "layers.5.attention.W_k.weight", "layers.5.attention.W_k.bias", "layers.5.attention.W_v.weight", "layers.5.attention.W_v.bias", "layers.5.attention.W_out.weight", "layers.5.attention.W_out.bias", "layers.5.linear0.weight", "layers.5.linear0.bias", "layers.5.linear1.weight", "layers.5.linear1.bias", "layers.5.norm1.weight", "layers.5.norm1.bias", "layers.5.norm0.weight", "layers.5.norm0.bias". \n\tsize mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 384]) from checkpoint, the shape in current model is torch.Size([101, 256]).\n\tsize mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 384]) from checkpoint, the shape in current model is torch.Size([100, 256]).\n\tsize mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([500, 384]) from checkpoint, the shape in current model is torch.Size([307, 256]).\n\tsize mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([500]) from checkpoint, the shape in current model is torch.Size([307]).\n\tsize mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([384, 500]) from checkpoint, the shape in current model is torch.Size([256, 307]).\n\tsize mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([500, 384]) from checkpoint, the shape in current model is torch.Size([307, 256]).\n\tsize mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([500]) from checkpoint, the shape in current model is torch.Size([307]).\n\tsize mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([384, 500]) from checkpoint, the shape in current model is torch.Size([256, 307]).\n\tsize mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for last_normalization.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for last_normalization.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for head.weight: copying a param with shape torch.Size([1, 384]) from checkpoint, the shape in current model is torch.Size([1, 256]).').
Traceback (most recent call last):
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 145, in <lambda>
    func = lambda trial: objective(trial, cfg, trial_stats, trial_counter, n_optuna_trials,
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 104, in objective
    stats = train_net_for_optuna.main(config, loaders, unique_categories, n_numerical, n_classes)
  File "/temporario2/14570653/tabular-transfer-learning/train_net_for_optuna.py", line 127, in main
    net.load_state_dict(torch.load(checkpoint_path)["net"])
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2215, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for FTTransformer:
	Missing key(s) in state_dict: "layers.2.attention.W_q.weight", "layers.2.attention.W_q.bias", "layers.2.attention.W_k.weight", "layers.2.attention.W_k.bias", "layers.2.attention.W_v.weight", "layers.2.attention.W_v.bias", "layers.2.attention.W_out.weight", "layers.2.attention.W_out.bias", "layers.2.linear0.weight", "layers.2.linear0.bias", "layers.2.linear1.weight", "layers.2.linear1.bias", "layers.2.norm1.weight", "layers.2.norm1.bias", "layers.2.norm0.weight", "layers.2.norm0.bias", "layers.3.attention.W_q.weight", "layers.3.attention.W_q.bias", "layers.3.attention.W_k.weight", "layers.3.attention.W_k.bias", "layers.3.attention.W_v.weight", "layers.3.attention.W_v.bias", "layers.3.attention.W_out.weight", "layers.3.attention.W_out.bias", "layers.3.linear0.weight", "layers.3.linear0.bias", "layers.3.linear1.weight", "layers.3.linear1.bias", "layers.3.norm1.weight", "layers.3.norm1.bias", "layers.3.norm0.weight", "layers.3.norm0.bias", "layers.4.attention.W_q.weight", "layers.4.attention.W_q.bias", "layers.4.attention.W_k.weight", "layers.4.attention.W_k.bias", "layers.4.attention.W_v.weight", "layers.4.attention.W_v.bias", "layers.4.attention.W_out.weight", "layers.4.attention.W_out.bias", "layers.4.linear0.weight", "layers.4.linear0.bias", "layers.4.linear1.weight", "layers.4.linear1.bias", "layers.4.norm1.weight", "layers.4.norm1.bias", "layers.4.norm0.weight", "layers.4.norm0.bias", "layers.5.attention.W_q.weight", "layers.5.attention.W_q.bias", "layers.5.attention.W_k.weight", "layers.5.attention.W_k.bias", "layers.5.attention.W_v.weight", "layers.5.attention.W_v.bias", "layers.5.attention.W_out.weight", "layers.5.attention.W_out.bias", "layers.5.linear0.weight", "layers.5.linear0.bias", "layers.5.linear1.weight", "layers.5.linear1.bias", "layers.5.norm1.weight", "layers.5.norm1.bias", "layers.5.norm0.weight", "layers.5.norm0.bias". 
	size mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 384]) from checkpoint, the shape in current model is torch.Size([101, 256]).
	size mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 384]) from checkpoint, the shape in current model is torch.Size([100, 256]).
	size mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([500, 384]) from checkpoint, the shape in current model is torch.Size([307, 256]).
	size mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([500]) from checkpoint, the shape in current model is torch.Size([307]).
	size mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([384, 500]) from checkpoint, the shape in current model is torch.Size([256, 307]).
	size mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([500, 384]) from checkpoint, the shape in current model is torch.Size([307, 256]).
	size mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([500]) from checkpoint, the shape in current model is torch.Size([307]).
	size mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([384, 500]) from checkpoint, the shape in current model is torch.Size([256, 307]).
	size mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for last_normalization.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for last_normalization.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for head.weight: copying a param with shape torch.Size([1, 384]) from checkpoint, the shape in current model is torch.Size([1, 256]).
[W 2025-08-12 17:08:31,244] Trial 4 failed with value None.
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:08:31 INFO]: Training stats: {
    "score": -0.7961117697188042,
    "rmse": 0.7961117697188042
}
[08/12/2025 17:08:31 INFO]: Val stats: {
    "score": -0.8088108081563729,
    "rmse": 0.8088108081563729
}
[08/12/2025 17:08:31 INFO]: Test stats: {
    "score": -0.7557424986076324,
    "rmse": 0.7557424986076324
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:08:33 INFO]: Training stats: {
    "score": -1.0004673157054191,
    "rmse": 1.0004673157054191
}
[08/12/2025 17:08:33 INFO]: Val stats: {
    "score": -0.9316011710402619,
    "rmse": 0.9316011710402619
}
[08/12/2025 17:08:33 INFO]: Test stats: {
    "score": -0.9124384288886574,
    "rmse": 0.9124384288886574
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:08:34 INFO]: Training loss at epoch 70: 0.6738703548908234
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:08:43 INFO]: New best epoch, val score: -0.9316011710402619
[08/12/2025 17:08:43 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:08:45 INFO]: Training loss at epoch 77: 0.3428986221551895
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:08:48 INFO]: Training loss at epoch 110: 0.6025280058383942
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:08:48 INFO]: New best epoch, val score: -0.7571786591004867
[08/12/2025 17:08:48 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:09:01 INFO]: Training loss at epoch 71: 0.6039569675922394
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:09:05 INFO]: Training loss at epoch 111: 0.7470181882381439
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:09:07 INFO]: New best epoch, val score: -0.796507593496295
[08/12/2025 17:09:07 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:09:09 INFO]: Training loss at epoch 78: 0.37375248968601227
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:09:22 INFO]: Training loss at epoch 112: 0.7685751914978027
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:09:24 INFO]: New best epoch, val score: -0.7959599707909609
[08/12/2025 17:09:24 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:09:27 INFO]: Training loss at epoch 72: 0.6011636257171631
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:09:32 INFO]: Training loss at epoch 79: 0.32813574373722076
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:09:38 INFO]: Training loss at epoch 113: 0.565703347325325
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:09:40 INFO]: Training stats: {
    "score": -0.5678103919401044,
    "rmse": 0.5678103919401044
}
[08/12/2025 17:09:40 INFO]: Val stats: {
    "score": -0.7617364489106156,
    "rmse": 0.7617364489106156
}
[08/12/2025 17:09:40 INFO]: Test stats: {
    "score": -0.7024457274235774,
    "rmse": 0.7024457274235774
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:09:40 INFO]: New best epoch, val score: -0.7945199586574412
[08/12/2025 17:09:40 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:09:52 INFO]: Training loss at epoch 73: 0.630171149969101
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:09:54 INFO]: Training loss at epoch 114: 0.7016632258892059
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:09:54 INFO]: Training loss at epoch 21: 0.7234295010566711
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:09:56 INFO]: New best epoch, val score: -0.7923781428787575
[08/12/2025 17:09:56 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:10:02 INFO]: Training loss at epoch 20: 1.019551932811737
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:10:03 INFO]: Training loss at epoch 80: 0.3216541111469269
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:10:10 INFO]: Training loss at epoch 115: 0.6978419423103333
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:10:12 INFO]: New best epoch, val score: -0.7917180563261914
[08/12/2025 17:10:12 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:10:13 INFO]: New best epoch, val score: -0.9303635397828494
[08/12/2025 17:10:13 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:10:17 INFO]: Training loss at epoch 74: 0.6442190110683441
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:10:26 INFO]: Training loss at epoch 81: 0.2775236442685127
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:10:26 INFO]: Training loss at epoch 116: 0.7635000050067902
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:10:42 INFO]: Training loss at epoch 75: 0.5604592263698578
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:10:42 INFO]: Training loss at epoch 117: 0.7456803321838379
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:10:49 INFO]: Training loss at epoch 82: 0.3882288783788681
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:10:58 INFO]: Training loss at epoch 118: 0.6793640851974487
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:11:07 INFO]: Training loss at epoch 76: 0.6504172682762146
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:11:12 INFO]: Training loss at epoch 83: 0.4187670946121216
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:11:15 INFO]: Training loss at epoch 119: 0.579819917678833
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:11:19 INFO]: Training loss at epoch 22: 0.5358680784702301
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:11:20 INFO]: Training stats: {
    "score": -0.7822012135487144,
    "rmse": 0.7822012135487144
}
[08/12/2025 17:11:20 INFO]: Val stats: {
    "score": -0.8118614922003625,
    "rmse": 0.8118614922003625
}
[08/12/2025 17:11:20 INFO]: Test stats: {
    "score": -0.7496916495504372,
    "rmse": 0.7496916495504372
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:11:32 INFO]: Training loss at epoch 21: 0.910303920507431
[08/12/2025 17:11:32 INFO]: Training loss at epoch 77: 0.5793665647506714
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:11:35 INFO]: Training loss at epoch 84: 0.34007038176059723
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:11:37 INFO]: Training loss at epoch 120: 0.5318964719772339
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:11:54 INFO]: Training loss at epoch 121: 0.7954727113246918
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:11:58 INFO]: Training loss at epoch 78: 0.585066944360733
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:11:58 INFO]: Training loss at epoch 85: 0.42379336059093475
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:12:09 INFO]: Training loss at epoch 122: 0.6097164452075958
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:12:22 INFO]: Training loss at epoch 86: 0.29825107753276825
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:12:23 INFO]: Training loss at epoch 79: 0.5456084907054901
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:12:26 INFO]: Training loss at epoch 123: 0.6391691863536835
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:12:29 INFO]: New best epoch, val score: -0.790271980311551
[08/12/2025 17:12:29 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:12:33 INFO]: Training stats: {
    "score": -0.7520012122563233,
    "rmse": 0.7520012122563233
}
[08/12/2025 17:12:33 INFO]: Val stats: {
    "score": -0.8683514282721407,
    "rmse": 0.8683514282721407
}
[08/12/2025 17:12:33 INFO]: Test stats: {
    "score": -0.8310750003361779,
    "rmse": 0.8310750003361779
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:12:44 INFO]: Training loss at epoch 23: 0.5851065814495087
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:12:44 INFO]: Training loss at epoch 124: 0.5691067427396774
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:12:44 INFO]: Training loss at epoch 87: 0.40838953852653503
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:12:59 INFO]: Training loss at epoch 80: 0.6840704083442688
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:13:00 INFO]: Training loss at epoch 125: 0.7762772440910339
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:13:01 INFO]: Training loss at epoch 22: 1.0257930159568787
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:13:07 INFO]: Training loss at epoch 88: 0.34274402260780334
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:13:16 INFO]: Training loss at epoch 126: 0.5275130867958069
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:13:24 INFO]: Training loss at epoch 81: 0.8161301612854004
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:13:31 INFO]: Training loss at epoch 89: 0.3521493524312973
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:13:32 INFO]: Training loss at epoch 127: 0.6155270934104919
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:13:39 INFO]: Training stats: {
    "score": -0.5556962425207954,
    "rmse": 0.5556962425207954
}
[08/12/2025 17:13:39 INFO]: Val stats: {
    "score": -0.7366557197154887,
    "rmse": 0.7366557197154887
}
[08/12/2025 17:13:39 INFO]: Test stats: {
    "score": -0.7161337778471218,
    "rmse": 0.7161337778471218
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:13:42 INFO]: New best epoch, val score: -0.7366557197154887
[08/12/2025 17:13:42 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:13:49 INFO]: Training loss at epoch 128: 0.6198202073574066
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:13:49 INFO]: Training loss at epoch 82: 0.6263783574104309
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:13:51 INFO]: New best epoch, val score: -0.783623712533883
[08/12/2025 17:13:51 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:14:03 INFO]: Training loss at epoch 90: 0.3894728720188141
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:14:05 INFO]: Training loss at epoch 129: 0.5825300961732864
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:14:10 INFO]: Training stats: {
    "score": -0.7716130125499024,
    "rmse": 0.7716130125499024
}
[08/12/2025 17:14:10 INFO]: Val stats: {
    "score": -0.7778428952716501,
    "rmse": 0.7778428952716501
}
[08/12/2025 17:14:10 INFO]: Test stats: {
    "score": -0.7339377880780101,
    "rmse": 0.7339377880780101
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:14:12 INFO]: New best epoch, val score: -0.7778428952716501
[08/12/2025 17:14:12 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:14:13 INFO]: Training loss at epoch 24: 0.6543957889080048
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:14:14 INFO]: Training loss at epoch 83: 0.6189738214015961
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:14:26 INFO]: Training loss at epoch 130: 0.6532959342002869
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:14:26 INFO]: Training loss at epoch 91: 0.30465199053287506
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:14:28 INFO]: New best epoch, val score: -0.7756523296807992
[08/12/2025 17:14:28 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:14:35 INFO]: Training loss at epoch 23: 0.9319043159484863
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:14:39 INFO]: Training loss at epoch 84: 0.5992943048477173
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:14:42 INFO]: Training loss at epoch 131: 0.6010135412216187
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:14:49 INFO]: Training loss at epoch 92: 0.23877794295549393
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:14:52 INFO]: New best epoch, val score: -0.7096891167413316
[08/12/2025 17:14:52 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:14:58 INFO]: Training loss at epoch 132: 0.5312042534351349
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:15:03 INFO]: Training loss at epoch 85: 0.4899560660123825
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:15:13 INFO]: Training loss at epoch 93: 0.23302147537469864
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:15:14 INFO]: Training loss at epoch 133: 0.5897984206676483
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:15:28 INFO]: Training loss at epoch 86: 0.6031176745891571
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:15:30 INFO]: Training loss at epoch 134: 0.5562655627727509
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:15:36 INFO]: Training loss at epoch 94: 0.33309486508369446
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:15:41 INFO]: Training loss at epoch 25: 0.4855833947658539
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:15:46 INFO]: Training loss at epoch 135: 0.5723748207092285
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:15:53 INFO]: Training loss at epoch 87: 0.45833730697631836
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:15:59 INFO]: Training loss at epoch 95: 0.39502139389514923
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:16:02 INFO]: Training loss at epoch 136: 0.7235671579837799
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:16:07 INFO]: Training loss at epoch 24: 1.037993460893631
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:16:17 INFO]: Training loss at epoch 88: 0.7898691892623901
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:16:18 INFO]: Training loss at epoch 137: 0.566779613494873
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:16:22 INFO]: Training loss at epoch 96: 0.26549582928419113
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:16:33 INFO]: Training loss at epoch 138: 0.619957447052002
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:16:42 INFO]: Training loss at epoch 89: 0.6142955124378204
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:16:45 INFO]: Training loss at epoch 97: 0.2851756513118744
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:16:50 INFO]: Training loss at epoch 139: 0.6738220751285553
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:16:51 INFO]: Training stats: {
    "score": -0.7783490177835569,
    "rmse": 0.7783490177835569
}
[08/12/2025 17:16:51 INFO]: Val stats: {
    "score": -0.9419572360338693,
    "rmse": 0.9419572360338693
}
[08/12/2025 17:16:51 INFO]: Test stats: {
    "score": -0.8580482242361964,
    "rmse": 0.8580482242361964
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:16:55 INFO]: Training stats: {
    "score": -0.7569905428313668,
    "rmse": 0.7569905428313668
}
[08/12/2025 17:16:55 INFO]: Val stats: {
    "score": -0.7757575820886201,
    "rmse": 0.7757575820886201
}
[08/12/2025 17:16:55 INFO]: Test stats: {
    "score": -0.7257141411321195,
    "rmse": 0.7257141411321195
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:17:07 INFO]: Training loss at epoch 26: 0.5562877357006073
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:17:09 INFO]: Training loss at epoch 98: 0.3319559544324875
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:17:11 INFO]: Training loss at epoch 140: 0.521971806883812
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:17:13 INFO]: New best epoch, val score: -0.7743404525906038
[08/12/2025 17:17:13 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:17:16 INFO]: Training loss at epoch 90: 0.5969880223274231
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:17:27 INFO]: Training loss at epoch 141: 0.6012122631072998
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:17:29 INFO]: New best epoch, val score: -0.7737552489880838
[08/12/2025 17:17:29 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:17:32 INFO]: Training loss at epoch 99: 0.348577082157135
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:17:37 INFO]: Training loss at epoch 25: 1.0902296304702759
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:17:39 INFO]: Training stats: {
    "score": -0.5559084379700037,
    "rmse": 0.5559084379700037
}
[08/12/2025 17:17:39 INFO]: Val stats: {
    "score": -0.7513753254025991,
    "rmse": 0.7513753254025991
}
[08/12/2025 17:17:39 INFO]: Test stats: {
    "score": -0.731043044446735,
    "rmse": 0.731043044446735
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:17:40 INFO]: Training loss at epoch 91: 0.4211142808198929
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:17:43 INFO]: Training loss at epoch 142: 0.5824917256832123
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:17:59 INFO]: Training loss at epoch 143: 0.5791557431221008
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:18:02 INFO]: Training loss at epoch 100: 0.3455767184495926
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:18:05 INFO]: Training loss at epoch 92: 0.5268041640520096
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:18:05 INFO]: New best epoch, val score: -0.6988413054272423
[08/12/2025 17:18:05 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:18:14 INFO]: Training loss at epoch 144: 0.7029770910739899
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:18:25 INFO]: Training loss at epoch 101: 0.28510361909866333
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:18:30 INFO]: Training loss at epoch 93: 0.6414233446121216
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:18:30 INFO]: Training loss at epoch 145: 0.6473570764064789
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:18:33 INFO]: New best epoch, val score: -0.8267962363001529
[08/12/2025 17:18:33 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:18:33 INFO]: Training loss at epoch 27: 0.5407624840736389
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:18:47 INFO]: Training loss at epoch 146: 0.6617234647274017
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:18:49 INFO]: Training loss at epoch 102: 0.4327966570854187
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:18:55 INFO]: Training loss at epoch 94: 0.5920204520225525
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:18:58 INFO]: New best epoch, val score: -0.8208916767060646
[08/12/2025 17:18:58 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:19:03 INFO]: Training loss at epoch 147: 0.6013714373111725
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:19:08 INFO]: Training loss at epoch 26: 1.0266433656215668
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:19:12 INFO]: Training loss at epoch 103: 0.3527078926563263
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:19:19 INFO]: Training loss at epoch 148: 0.6339353322982788
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:19:20 INFO]: Training loss at epoch 95: 0.6325879096984863
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:19:35 INFO]: Training loss at epoch 149: 0.6216672658920288
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:19:35 INFO]: Training loss at epoch 104: 0.32661257684230804
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:19:40 INFO]: Training stats: {
    "score": -0.7438862743876252,
    "rmse": 0.7438862743876252
}
[08/12/2025 17:19:40 INFO]: Val stats: {
    "score": -0.7762428392169367,
    "rmse": 0.7762428392169367
}
[08/12/2025 17:19:40 INFO]: Test stats: {
    "score": -0.7198472317368892,
    "rmse": 0.7198472317368892
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:19:44 INFO]: Training loss at epoch 96: 0.6078780591487885
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:19:56 INFO]: Training loss at epoch 150: 0.5432641357183456
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:19:58 INFO]: New best epoch, val score: -0.7705661844912217
[08/12/2025 17:19:58 INFO]: Saving model to: model_best.pth
[08/12/2025 17:19:58 INFO]: Training loss at epoch 105: 0.2513866499066353
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:20:00 INFO]: Training loss at epoch 28: 0.6159733831882477
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:20:09 INFO]: Training loss at epoch 97: 0.5414470285177231
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:20:13 INFO]: Training loss at epoch 151: 0.5796796679496765
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:20:15 INFO]: New best epoch, val score: -0.7663148287673733
[08/12/2025 17:20:15 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:20:22 INFO]: Training loss at epoch 106: 0.26024869829416275
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:20:29 INFO]: Training loss at epoch 152: 0.582452118396759
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:20:31 INFO]: New best epoch, val score: -0.7633794449720729
[08/12/2025 17:20:31 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:20:34 INFO]: Training loss at epoch 98: 0.43607373535633087
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:20:40 INFO]: Training loss at epoch 27: 0.9385550320148468
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:20:45 INFO]: Training loss at epoch 153: 0.6256941854953766
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:20:45 INFO]: Training loss at epoch 107: 0.3070462495088577
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:20:47 INFO]: New best epoch, val score: -0.7563459581164174
[08/12/2025 17:20:47 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:20:59 INFO]: Training loss at epoch 99: 0.48857179284095764
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:21:01 INFO]: Training loss at epoch 154: 0.5644371807575226
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:21:03 INFO]: New best epoch, val score: -0.7495503345077998
[08/12/2025 17:21:03 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:21:07 INFO]: Training stats: {
    "score": -0.7166792069908784,
    "rmse": 0.7166792069908784
}
[08/12/2025 17:21:07 INFO]: Val stats: {
    "score": -0.8198441510573369,
    "rmse": 0.8198441510573369
}
[08/12/2025 17:21:07 INFO]: Test stats: {
    "score": -0.8154759274463188,
    "rmse": 0.8154759274463188
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:21:08 INFO]: Training loss at epoch 108: 0.27615639567375183
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:21:11 INFO]: New best epoch, val score: -0.8198441510573369
[08/12/2025 17:21:11 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:21:17 INFO]: Training loss at epoch 155: 0.6356098055839539
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:21:19 INFO]: New best epoch, val score: -0.7456461065943893
[08/12/2025 17:21:19 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:21:28 INFO]: Training loss at epoch 29: 0.5664254128932953
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:21:32 INFO]: Training loss at epoch 109: 0.32122859358787537
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:21:33 INFO]: Training loss at epoch 100: 0.5411044955253601
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:21:33 INFO]: Training loss at epoch 156: 0.580798476934433
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:21:35 INFO]: New best epoch, val score: -0.7434659241339523
[08/12/2025 17:21:35 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:21:36 INFO]: New best epoch, val score: -0.8113860271174376
[08/12/2025 17:21:36 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:21:40 INFO]: Training stats: {
    "score": -0.49882878186861346,
    "rmse": 0.49882878186861346
}
[08/12/2025 17:21:40 INFO]: Val stats: {
    "score": -0.716343757048036,
    "rmse": 0.716343757048036
}
[08/12/2025 17:21:40 INFO]: Test stats: {
    "score": -0.6916049512708732,
    "rmse": 0.6916049512708732
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:21:49 INFO]: Training loss at epoch 157: 0.5548427700996399
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:21:51 INFO]: New best epoch, val score: -0.7431514993623519
[08/12/2025 17:21:51 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:21:57 INFO]: Training stats: {
    "score": -0.7775579141946019,
    "rmse": 0.7775579141946019
}
[08/12/2025 17:21:57 INFO]: Val stats: {
    "score": -0.8611524047153524,
    "rmse": 0.8611524047153524
}
[08/12/2025 17:21:57 INFO]: Test stats: {
    "score": -0.815803398905584,
    "rmse": 0.815803398905584
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:21:57 INFO]: Training loss at epoch 101: 0.6217053830623627
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:22:03 INFO]: Training loss at epoch 110: 0.27419789135456085
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:22:05 INFO]: Training loss at epoch 158: 0.6809226870536804
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:22:12 INFO]: Training loss at epoch 28: 1.074078619480133
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:22:21 INFO]: Training loss at epoch 159: 0.5853641331195831
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:22:22 INFO]: Training loss at epoch 102: 0.60334712266922
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:22:26 INFO]: Training loss at epoch 111: 0.24684889614582062
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:22:27 INFO]: Training stats: {
    "score": -0.7329200100189361,
    "rmse": 0.7329200100189361
}
[08/12/2025 17:22:27 INFO]: Val stats: {
    "score": -0.7468679510732883,
    "rmse": 0.7468679510732883
}
[08/12/2025 17:22:27 INFO]: Test stats: {
    "score": -0.7028326490059204,
    "rmse": 0.7028326490059204
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:22:43 INFO]: Training loss at epoch 160: 0.5545315146446228
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:22:46 INFO]: Training loss at epoch 103: 0.5531454384326935
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:22:49 INFO]: Training loss at epoch 112: 0.22516898810863495
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:22:58 INFO]: Training loss at epoch 161: 0.5117695331573486
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:23:11 INFO]: Training loss at epoch 104: 0.4747552275657654
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:23:12 INFO]: Training loss at epoch 113: 0.2565859854221344
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:23:14 INFO]: Training loss at epoch 162: 0.485907644033432
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:23:22 INFO]: Training loss at epoch 30: 0.5493078231811523
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:23:30 INFO]: Training loss at epoch 163: 0.5104666948318481
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:23:35 INFO]: Training loss at epoch 114: 0.214604452252388
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:23:36 INFO]: Training loss at epoch 105: 0.43410253524780273
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:23:41 INFO]: Training loss at epoch 29: 1.017323762178421
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:23:46 INFO]: Training loss at epoch 164: 0.6114267110824585
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:23:58 INFO]: Training loss at epoch 115: 0.23858345299959183
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:24:00 INFO]: Training loss at epoch 106: 0.45288093388080597
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:24:02 INFO]: Training loss at epoch 165: 0.7532879710197449
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:24:12 INFO]: Training stats: {
    "score": -0.9915530165692432,
    "rmse": 0.9915530165692432
}
[08/12/2025 17:24:12 INFO]: Val stats: {
    "score": -0.9270192444743446,
    "rmse": 0.9270192444743446
}
[08/12/2025 17:24:12 INFO]: Test stats: {
    "score": -0.9054511696558584,
    "rmse": 0.9054511696558584
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:24:18 INFO]: Training loss at epoch 166: 0.45559021830558777
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:24:21 INFO]: Training loss at epoch 116: 0.28249359130859375
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:24:23 INFO]: New best epoch, val score: -0.9270192444743446
[08/12/2025 17:24:23 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:24:25 INFO]: Training loss at epoch 107: 0.5128249973058701
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:24:33 INFO]: Training loss at epoch 167: 0.5479187369346619
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:24:44 INFO]: Training loss at epoch 117: 0.23886368423700333
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:24:49 INFO]: Training loss at epoch 168: 0.5048874467611313
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:24:50 INFO]: Training loss at epoch 108: 0.6608645915985107
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:24:50 INFO]: Training loss at epoch 31: 0.549574226140976
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:25:00 INFO]: New best epoch, val score: -0.8259687850861581
[08/12/2025 17:25:00 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:25:05 INFO]: Training loss at epoch 169: 0.603338748216629
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:25:07 INFO]: Training loss at epoch 118: 0.22085999697446823
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:25:11 INFO]: Training stats: {
    "score": -0.7177656231061449,
    "rmse": 0.7177656231061449
}
[08/12/2025 17:25:11 INFO]: Val stats: {
    "score": -0.7455532091263087,
    "rmse": 0.7455532091263087
}
[08/12/2025 17:25:11 INFO]: Test stats: {
    "score": -0.6955971575315185,
    "rmse": 0.6955971575315185
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:25:14 INFO]: Training loss at epoch 109: 0.48895470798015594
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:25:23 INFO]: Training stats: {
    "score": -0.7412243657605645,
    "rmse": 0.7412243657605645
}
[08/12/2025 17:25:23 INFO]: Val stats: {
    "score": -0.9032972830997801,
    "rmse": 0.9032972830997801
}
[08/12/2025 17:25:23 INFO]: Test stats: {
    "score": -0.8338735905531549,
    "rmse": 0.8338735905531549
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:25:27 INFO]: Training loss at epoch 170: 0.5350764989852905
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:25:28 INFO]: New best epoch, val score: -0.7413495265394213
[08/12/2025 17:25:28 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:25:30 INFO]: Training loss at epoch 119: 0.2043089121580124
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:25:38 INFO]: Training stats: {
    "score": -0.5091365170745268,
    "rmse": 0.5091365170745268
}
[08/12/2025 17:25:38 INFO]: Val stats: {
    "score": -0.7288459581161812,
    "rmse": 0.7288459581161812
}
[08/12/2025 17:25:38 INFO]: Test stats: {
    "score": -0.6778358155581412,
    "rmse": 0.6778358155581412
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:25:43 INFO]: Training loss at epoch 171: 0.4875079095363617
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:25:45 INFO]: New best epoch, val score: -0.7356018503616533
[08/12/2025 17:25:45 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:25:45 INFO]: Training loss at epoch 30: 1.2270541191101074
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:25:48 INFO]: Training loss at epoch 110: 0.5940306484699249
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:25:59 INFO]: Training loss at epoch 172: 0.6026041805744171
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:26:01 INFO]: New best epoch, val score: -0.7296254659961126
[08/12/2025 17:26:01 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:26:01 INFO]: Training loss at epoch 120: 0.2310151308774948
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:26:13 INFO]: Training loss at epoch 111: 0.46713797748088837
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:26:15 INFO]: Training loss at epoch 173: 0.5600389540195465
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:26:16 INFO]: New best epoch, val score: -0.7258958715578026
[08/12/2025 17:26:16 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:26:18 INFO]: Training loss at epoch 32: 0.5281565189361572
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:26:24 INFO]: Training loss at epoch 121: 0.23176563531160355
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:26:27 INFO]: New best epoch, val score: -0.6931715564508402
[08/12/2025 17:26:27 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:26:31 INFO]: Training loss at epoch 174: 0.5471701323986053
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:26:38 INFO]: Training loss at epoch 112: 0.6771082878112793
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:26:47 INFO]: Training loss at epoch 175: 0.5113467425107956
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:26:47 INFO]: Training loss at epoch 122: 0.22335974872112274
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:27:02 INFO]: Training loss at epoch 113: 0.5973678231239319
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:27:03 INFO]: Training loss at epoch 176: 0.49146339297294617
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:27:10 INFO]: Training loss at epoch 123: 0.22798460721969604
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:27:16 INFO]: Training loss at epoch 31: 1.0046414732933044
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:27:19 INFO]: Training loss at epoch 177: 0.578957200050354
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:27:27 INFO]: Training loss at epoch 114: 0.4674421697854996
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:27:33 INFO]: Training loss at epoch 124: 0.2230917513370514
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:27:34 INFO]: Training loss at epoch 178: 0.6014364361763
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:27:44 INFO]: Training loss at epoch 33: 0.5227365493774414
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:27:50 INFO]: Training loss at epoch 179: 0.4716865122318268
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:27:52 INFO]: Training loss at epoch 115: 0.5144217610359192
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:27:57 INFO]: Training loss at epoch 125: 0.2310296669602394
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:27:57 INFO]: Training stats: {
    "score": -0.707693176870152,
    "rmse": 0.707693176870152
}
[08/12/2025 17:27:57 INFO]: Val stats: {
    "score": -0.7374290982875756,
    "rmse": 0.7374290982875756
}
[08/12/2025 17:27:57 INFO]: Test stats: {
    "score": -0.6927371257309778,
    "rmse": 0.6927371257309778
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:28:14 INFO]: Training loss at epoch 180: 0.5754534304141998
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:28:19 INFO]: Training loss at epoch 116: 0.5488079786300659
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:28:20 INFO]: Training loss at epoch 126: 0.244988352060318
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:28:29 INFO]: Training loss at epoch 181: 0.5712812840938568
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:28:43 INFO]: Training loss at epoch 117: 0.46028588712215424
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:28:43 INFO]: Training loss at epoch 127: 0.26375311613082886
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:28:45 INFO]: Training loss at epoch 182: 0.5664379298686981
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:28:47 INFO]: Training loss at epoch 32: 0.988529235124588
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:29:01 INFO]: Training loss at epoch 183: 0.4259783625602722
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:29:06 INFO]: Training loss at epoch 128: 0.2952481806278229
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:29:08 INFO]: Training loss at epoch 118: 0.5701612532138824
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:29:12 INFO]: Training loss at epoch 34: 0.5129294693470001
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:29:17 INFO]: Training loss at epoch 184: 0.4238172918558121
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:29:29 INFO]: Training loss at epoch 129: 0.2317976877093315
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:29:32 INFO]: Training loss at epoch 119: 0.5427111983299255
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:29:33 INFO]: Training loss at epoch 185: 0.5181651711463928
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:29:37 INFO]: Training stats: {
    "score": -0.4530124364418104,
    "rmse": 0.4530124364418104
}
[08/12/2025 17:29:37 INFO]: Val stats: {
    "score": -0.6731474745806645,
    "rmse": 0.6731474745806645
}
[08/12/2025 17:29:37 INFO]: Test stats: {
    "score": -0.675888452118352,
    "rmse": 0.675888452118352
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:29:40 INFO]: New best epoch, val score: -0.6731474745806645
[08/12/2025 17:29:40 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:29:41 INFO]: Training stats: {
    "score": -0.6832802746701835,
    "rmse": 0.6832802746701835
}
[08/12/2025 17:29:41 INFO]: Val stats: {
    "score": -0.8170069490589713,
    "rmse": 0.8170069490589713
}
[08/12/2025 17:29:41 INFO]: Test stats: {
    "score": -0.7742727019112423,
    "rmse": 0.7742727019112423
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:29:49 INFO]: Training loss at epoch 186: 0.42890897393226624
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:29:51 INFO]: New best epoch, val score: -0.7245466813574611
[08/12/2025 17:29:51 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:30:00 INFO]: Training loss at epoch 130: 0.244168221950531
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:30:05 INFO]: Training loss at epoch 187: 0.4736444354057312
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:30:05 INFO]: Training loss at epoch 120: 0.5053976625204086
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:30:07 INFO]: New best epoch, val score: -0.7216090466010706
[08/12/2025 17:30:07 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:30:08 INFO]: New best epoch, val score: -0.7942961953196457
[08/12/2025 17:30:08 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:30:16 INFO]: Training loss at epoch 33: 1.0864253342151642
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:30:21 INFO]: Training loss at epoch 188: 0.5683271884918213
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:30:22 INFO]: New best epoch, val score: -0.7190280477873496
[08/12/2025 17:30:22 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:30:24 INFO]: Training loss at epoch 131: 0.24599937349557877
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:30:31 INFO]: Training loss at epoch 121: 0.4727261960506439
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:30:37 INFO]: Training loss at epoch 189: 0.4884915202856064
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:30:38 INFO]: Training loss at epoch 35: 0.4921340346336365
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:30:43 INFO]: Training stats: {
    "score": -0.6981756105691306,
    "rmse": 0.6981756105691306
}
[08/12/2025 17:30:43 INFO]: Val stats: {
    "score": -0.717271131091159,
    "rmse": 0.717271131091159
}
[08/12/2025 17:30:43 INFO]: Test stats: {
    "score": -0.6855130529538782,
    "rmse": 0.6855130529538782
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:30:45 INFO]: New best epoch, val score: -0.717271131091159
[08/12/2025 17:30:45 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:30:47 INFO]: Training loss at epoch 132: 0.26240888237953186
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:30:48 INFO]: New best epoch, val score: -0.8236785962979529
[08/12/2025 17:30:48 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:30:55 INFO]: Training loss at epoch 122: 0.4804200530052185
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:30:59 INFO]: Training loss at epoch 190: 0.4606492221355438
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:31:01 INFO]: New best epoch, val score: -0.7169276812521534
[08/12/2025 17:31:01 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:31:11 INFO]: Training loss at epoch 133: 0.34122611582279205
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:31:17 INFO]: Training loss at epoch 191: 0.4273872822523117
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:31:19 INFO]: New best epoch, val score: -0.7162982308102163
[08/12/2025 17:31:19 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:31:23 INFO]: Training loss at epoch 123: 0.4433978497982025
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:31:33 INFO]: Training loss at epoch 192: 0.6501492857933044
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:31:35 INFO]: Training loss at epoch 134: 0.2103104218840599
[08/12/2025 17:31:35 INFO]: New best epoch, val score: -0.7153836135802916
[08/12/2025 17:31:35 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:31:48 INFO]: Training loss at epoch 34: 0.7871862649917603
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:31:50 INFO]: Training loss at epoch 124: 0.5944896638393402
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:31:50 INFO]: Training loss at epoch 193: 0.49332065880298615
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:31:52 INFO]: New best epoch, val score: -0.714948058971155
[08/12/2025 17:31:52 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:31:59 INFO]: Training loss at epoch 135: 0.2652556821703911
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:32:05 INFO]: Training loss at epoch 36: 0.4974517822265625
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:32:06 INFO]: Training loss at epoch 194: 0.5105797946453094
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:32:08 INFO]: New best epoch, val score: -0.7133828236443163
[08/12/2025 17:32:08 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:32:15 INFO]: New best epoch, val score: -0.8227421617745853
[08/12/2025 17:32:15 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:32:16 INFO]: Training loss at epoch 125: 0.5001641809940338
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:32:23 INFO]: Training loss at epoch 195: 0.40053753554821014
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:32:24 INFO]: Training loss at epoch 136: 0.2791108638048172
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:32:25 INFO]: New best epoch, val score: -0.7104778867271349
[08/12/2025 17:32:25 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:32:40 INFO]: Training loss at epoch 196: 0.5134977102279663
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:32:41 INFO]: Training loss at epoch 126: 0.3981190621852875
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:32:42 INFO]: New best epoch, val score: -0.7076783019565711
[08/12/2025 17:32:42 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:32:49 INFO]: Training loss at epoch 137: 0.23994290828704834
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:32:57 INFO]: Training loss at epoch 197: 0.5449121594429016
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:32:59 INFO]: New best epoch, val score: -0.7070450221380218
[08/12/2025 17:32:59 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:33:06 INFO]: Training loss at epoch 127: 0.4447249174118042
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:33:13 INFO]: Training loss at epoch 138: 0.30414605140686035
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:33:13 INFO]: Training loss at epoch 198: 0.45333003997802734
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:33:16 INFO]: New best epoch, val score: -0.7061450981222543
[08/12/2025 17:33:16 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:33:19 INFO]: Training loss at epoch 35: 0.9560830593109131
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:33:30 INFO]: Training loss at epoch 199: 0.5338899791240692
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:33:31 INFO]: Training loss at epoch 128: 0.5230891704559326
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:33:31 INFO]: Training loss at epoch 37: 0.472659632563591
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:33:36 INFO]: Training stats: {
    "score": -0.6863962544639226,
    "rmse": 0.6863962544639226
}
[08/12/2025 17:33:36 INFO]: Val stats: {
    "score": -0.7049337494332752,
    "rmse": 0.7049337494332752
}
[08/12/2025 17:33:36 INFO]: Test stats: {
    "score": -0.6697873606453464,
    "rmse": 0.6697873606453464
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:33:38 INFO]: Training loss at epoch 139: 0.3447968512773514
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:33:38 INFO]: New best epoch, val score: -0.7049337494332752
[08/12/2025 17:33:38 INFO]: Saving model to: model_best.pth
[08/12/2025 17:33:38 INFO]: Running Final Evaluation...
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:33:44 INFO]: Training accuracy: {
    "score": -0.6863962536709325,
    "rmse": 0.6863962536709325
}
[08/12/2025 17:33:44 INFO]: Val accuracy: {
    "score": -0.7049337494332752,
    "rmse": 0.7049337494332752
}
[08/12/2025 17:33:44 INFO]: Test accuracy: {
    "score": -0.6697873606453464,
    "rmse": 0.6697873606453464
}
[08/12/2025 17:33:44 INFO]: {
    "dataset": "ic_upstream3",
    "model_name": "ft_transformer",
    "run_id": "revolved-Rody",
    "best_epoch": 199,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.6697873606453464,
        "rmse": 0.6697873606453464
    },
    "train_stats": {
        "score": -0.6863962536709325,
        "rmse": 0.6863962536709325
    },
    "val_stats": {
        "score": -0.7049337494332752,
        "rmse": 0.7049337494332752
    }
}
[I 2025-08-12 17:33:44,869] Trial 9 finished with value: -0.7049337494332752 and parameters: {'embedding_head_pair': (128, 8), 'n_layers': 4, 'd_ffn_factor': 0.9957126569236997, 'attention_dropout': 0.05533102397217843, 'activation': 'reglu', 'lr': 1.5677973629970916e-05}. Best is trial 9 with value: -0.7049337494332752.
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:33:46 INFO]: Training stats: {
    "score": -0.4486740206857249,
    "rmse": 0.4486740206857249
}
[08/12/2025 17:33:46 INFO]: Val stats: {
    "score": -0.687506034552352,
    "rmse": 0.687506034552352
}
[08/12/2025 17:33:46 INFO]: Test stats: {
    "score": -0.6768295364960735,
    "rmse": 0.6768295364960735
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:33:56 INFO]: Training loss at epoch 129: 0.48773935437202454
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:34:04 INFO]: Training stats: {
    "score": -0.7263438092910655,
    "rmse": 0.7263438092910655
}
[08/12/2025 17:34:04 INFO]: Val stats: {
    "score": -0.8876670380627847,
    "rmse": 0.8876670380627847
}
[08/12/2025 17:34:04 INFO]: Test stats: {
    "score": -0.8016901195546895,
    "rmse": 0.8016901195546895
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:34:12 INFO]: Training loss at epoch 140: 0.2589145749807358
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:34:30 INFO]: Training loss at epoch 130: 0.513592392206192
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:34:36 INFO]: Training loss at epoch 141: 0.23663616180419922
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:34:48 INFO]: Training loss at epoch 36: 0.9220163226127625
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:34:57 INFO]: Training loss at epoch 131: 0.5219589471817017
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:34:59 INFO]: Training loss at epoch 38: 0.4063384383916855
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:35:01 INFO]: Training loss at epoch 142: 0.201824352145195
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:35:04 INFO]: New best epoch, val score: -0.6600348025480707
[08/12/2025 17:35:04 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:35:24 INFO]: Training loss at epoch 132: 0.534287303686142
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:35:24 INFO]: Training loss at epoch 143: 0.2370133250951767
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:35:27 INFO]: New best epoch, val score: -0.6501189367886525
[08/12/2025 17:35:27 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:35:27 INFO]: New best epoch, val score: -0.7925302297926174
[08/12/2025 17:35:27 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:35:49 INFO]: Training loss at epoch 144: 0.24319887906312943
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:35:50 INFO]: Training loss at epoch 133: 0.4442107677459717
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:35:53 INFO]: New best epoch, val score: -0.7903845985124677
[08/12/2025 17:35:53 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:36:13 INFO]: Training loss at epoch 145: 0.2903624027967453
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:36:15 INFO]: Training loss at epoch 134: 0.453633576631546
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:36:20 INFO]: Training loss at epoch 37: 0.9391053020954132
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:36:26 INFO]: Training loss at epoch 39: 0.3617476746439934
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:36:36 INFO]: Training loss at epoch 146: 0.3133767992258072
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:36:40 INFO]: Training loss at epoch 135: 0.6015780866146088
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:36:54 INFO]: Training stats: {
    "score": -0.6720783625347992,
    "rmse": 0.6720783625347992
}
[08/12/2025 17:36:54 INFO]: Val stats: {
    "score": -0.8310631225981799,
    "rmse": 0.8310631225981799
}
[08/12/2025 17:36:54 INFO]: Test stats: {
    "score": -0.7883445659974001,
    "rmse": 0.7883445659974001
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:36:59 INFO]: Training loss at epoch 147: 0.23816993087530136
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:37:05 INFO]: Training loss at epoch 136: 0.4881969094276428
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:37:22 INFO]: Training loss at epoch 148: 0.24555149674415588
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:37:30 INFO]: Training loss at epoch 137: 0.5653094947338104
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:37:45 INFO]: Training loss at epoch 149: 0.22423936426639557
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:37:51 INFO]: Training loss at epoch 38: 1.055504858493805
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:37:53 INFO]: Training stats: {
    "score": -0.4961287695592651,
    "rmse": 0.4961287695592651
}
[08/12/2025 17:37:53 INFO]: Val stats: {
    "score": -0.739864986896283,
    "rmse": 0.739864986896283
}
[08/12/2025 17:37:53 INFO]: Test stats: {
    "score": -0.774727549000556,
    "rmse": 0.774727549000556
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:37:56 INFO]: Training loss at epoch 138: 0.48680131137371063
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:38:16 INFO]: Training loss at epoch 150: 0.21711044013500214
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:38:19 INFO]: Training loss at epoch 40: 0.38946433365345
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:38:21 INFO]: Training loss at epoch 139: 0.5237395465373993
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:38:29 INFO]: Training stats: {
    "score": -0.6743415823461143,
    "rmse": 0.6743415823461143
}
[08/12/2025 17:38:29 INFO]: Val stats: {
    "score": -0.8054403647326853,
    "rmse": 0.8054403647326853
}
[08/12/2025 17:38:29 INFO]: Test stats: {
    "score": -0.7624210471755096,
    "rmse": 0.7624210471755096
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:38:39 INFO]: Training loss at epoch 151: 0.25144387036561966
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:38:55 INFO]: Training loss at epoch 140: 0.4460037499666214
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:39:03 INFO]: Training loss at epoch 152: 0.23370064049959183
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:39:20 INFO]: Training loss at epoch 141: 0.502187192440033
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:39:23 INFO]: Training loss at epoch 39: 0.8752690553665161
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:39:26 INFO]: Training loss at epoch 153: 0.2834222912788391
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:39:44 INFO]: Training loss at epoch 41: 0.5292434692382812
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:39:45 INFO]: Training loss at epoch 142: 0.4788367450237274
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:39:49 INFO]: Training loss at epoch 154: 0.22113323956727982
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:39:54 INFO]: New best epoch, val score: -0.8180363679973256
[08/12/2025 17:39:54 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:39:55 INFO]: Training stats: {
    "score": -0.9776933737289861,
    "rmse": 0.9776933737289861
}
[08/12/2025 17:39:55 INFO]: Val stats: {
    "score": -0.9168821979409574,
    "rmse": 0.9168821979409574
}
[08/12/2025 17:39:55 INFO]: Test stats: {
    "score": -0.893052622687322,
    "rmse": 0.893052622687322
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:40:06 INFO]: New best epoch, val score: -0.9168821979409574
[08/12/2025 17:40:06 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:40:11 INFO]: Training loss at epoch 143: 0.5623588562011719
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:40:12 INFO]: Training loss at epoch 155: 0.26180899143218994
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:40:35 INFO]: Training loss at epoch 156: 0.24614955484867096
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:40:36 INFO]: Training loss at epoch 144: 0.4992281347513199
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:40:58 INFO]: Training loss at epoch 157: 0.2012975811958313
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:41:02 INFO]: Training loss at epoch 145: 0.3847154974937439
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:41:09 INFO]: Training loss at epoch 42: 0.40283840894699097
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:41:21 INFO]: Training loss at epoch 158: 0.22956153750419617
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:41:28 INFO]: Training loss at epoch 146: 0.4029994159936905
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:41:28 INFO]: Training loss at epoch 40: 0.9046197831630707
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:41:39 INFO]: New best epoch, val score: -0.9162353585176073
[08/12/2025 17:41:39 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:41:44 INFO]: Training loss at epoch 159: 0.18031911924481392
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:41:53 INFO]: Training stats: {
    "score": -0.41909801521835194,
    "rmse": 0.41909801521835194
}
[08/12/2025 17:41:53 INFO]: Val stats: {
    "score": -0.6330029866518093,
    "rmse": 0.6330029866518093
}
[08/12/2025 17:41:53 INFO]: Test stats: {
    "score": -0.6570551645426088,
    "rmse": 0.6570551645426088
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:41:53 INFO]: Training loss at epoch 147: 0.4781227111816406
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:41:55 INFO]: New best epoch, val score: -0.6330029866518093
[08/12/2025 17:41:55 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:41:56 INFO]: New best epoch, val score: -0.777095598337471
[08/12/2025 17:41:56 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:42:17 INFO]: Training loss at epoch 160: 0.20317666977643967
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:42:19 INFO]: Training loss at epoch 148: 0.4372880905866623
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:42:22 INFO]: New best epoch, val score: -0.7731695618200517
[08/12/2025 17:42:22 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:42:36 INFO]: Training loss at epoch 43: 0.3488658666610718
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:42:41 INFO]: Training loss at epoch 161: 0.25038085132837296
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:42:44 INFO]: Training loss at epoch 149: 0.4728799909353256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:42:53 INFO]: Training stats: {
    "score": -0.6567553902673636,
    "rmse": 0.6567553902673636
}
[08/12/2025 17:42:53 INFO]: Val stats: {
    "score": -0.8120923882371068,
    "rmse": 0.8120923882371068
}
[08/12/2025 17:42:53 INFO]: Test stats: {
    "score": -0.73582756306966,
    "rmse": 0.73582756306966
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:43:01 INFO]: Training loss at epoch 41: 0.8513848781585693
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:43:04 INFO]: Training loss at epoch 162: 0.21733201295137405
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:43:12 INFO]: New best epoch, val score: -0.913356083121974
[08/12/2025 17:43:12 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:43:18 INFO]: Training loss at epoch 150: 0.5130668729543686
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:43:29 INFO]: Training loss at epoch 163: 0.2147945836186409
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:43:43 INFO]: Training loss at epoch 151: 0.45824217796325684
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:43:52 INFO]: Training loss at epoch 164: 0.2458127737045288
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:44:03 INFO]: Training loss at epoch 44: 0.4768773764371872
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:44:07 INFO]: Training loss at epoch 152: 0.5511614829301834
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:44:15 INFO]: Training loss at epoch 165: 0.20797712355852127
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:44:30 INFO]: Training loss at epoch 42: 1.084029197692871
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:44:32 INFO]: Training loss at epoch 153: 0.4952280670404434
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:44:38 INFO]: Training loss at epoch 166: 0.19895289838314056
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:44:56 INFO]: Training loss at epoch 154: 0.499224990606308
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:45:02 INFO]: Training loss at epoch 167: 0.21294111013412476
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:45:21 INFO]: Training loss at epoch 155: 0.5471969693899155
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:45:26 INFO]: Training loss at epoch 168: 0.22697707265615463
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:45:29 INFO]: Training loss at epoch 45: 0.433795228600502
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:45:46 INFO]: Training loss at epoch 156: 0.5534885972738266
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:45:49 INFO]: Training loss at epoch 169: 0.20242763310670853
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:45:57 INFO]: Training stats: {
    "score": -0.42559917176096124,
    "rmse": 0.42559917176096124
}
[08/12/2025 17:45:57 INFO]: Val stats: {
    "score": -0.6877141451162289,
    "rmse": 0.6877141451162289
}
[08/12/2025 17:45:57 INFO]: Test stats: {
    "score": -0.7229779666149769,
    "rmse": 0.7229779666149769
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:46:00 INFO]: Training loss at epoch 43: 0.9562166333198547
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:46:10 INFO]: Training loss at epoch 157: 0.3516724556684494
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:46:20 INFO]: Training loss at epoch 170: 0.21545839309692383
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:46:34 INFO]: Training loss at epoch 158: 0.40316805243492126
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:46:44 INFO]: Training loss at epoch 171: 0.18656299263238907
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:46:55 INFO]: Training loss at epoch 46: 0.443091943860054
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:46:59 INFO]: Training loss at epoch 159: 0.4471901208162308
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:47:07 INFO]: Training loss at epoch 172: 0.16838912665843964
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:47:08 INFO]: Training stats: {
    "score": -0.714254403370907,
    "rmse": 0.714254403370907
}
[08/12/2025 17:47:08 INFO]: Val stats: {
    "score": -0.8842463197244909,
    "rmse": 0.8842463197244909
}
[08/12/2025 17:47:08 INFO]: Test stats: {
    "score": -0.7762927052357439,
    "rmse": 0.7762927052357439
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:47:30 INFO]: Training loss at epoch 44: 0.8769554495811462
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:47:30 INFO]: Training loss at epoch 173: 0.17741841077804565
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:47:33 INFO]: Training loss at epoch 160: 0.5270887613296509
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:47:53 INFO]: Training loss at epoch 174: 0.14324087277054787
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:47:58 INFO]: Training loss at epoch 161: 0.4044468253850937
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:48:17 INFO]: Training loss at epoch 175: 0.18593138456344604
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:48:20 INFO]: Training loss at epoch 47: 0.3957909196615219
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:48:23 INFO]: Training loss at epoch 162: 0.4365936368703842
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:48:40 INFO]: Training loss at epoch 176: 0.17676571756601334
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:48:47 INFO]: Training loss at epoch 163: 0.41561056673526764
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:49:00 INFO]: Training loss at epoch 45: 0.9606791436672211
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:49:03 INFO]: Training loss at epoch 177: 0.1840079426765442
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:49:12 INFO]: Training loss at epoch 164: 0.6279346197843552
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:49:26 INFO]: Training loss at epoch 178: 0.14199259504675865
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:49:38 INFO]: Training loss at epoch 165: 0.542488157749176
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:49:44 INFO]: Training loss at epoch 48: 0.35298237204551697
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:49:49 INFO]: Training loss at epoch 179: 0.12948476523160934
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:49:54 INFO]: New best epoch, val score: -0.8018378226919894
[08/12/2025 17:49:54 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:49:57 INFO]: Training stats: {
    "score": -0.388831316282665,
    "rmse": 0.388831316282665
}
[08/12/2025 17:49:57 INFO]: Val stats: {
    "score": -0.6693739354844179,
    "rmse": 0.6693739354844179
}
[08/12/2025 17:49:57 INFO]: Test stats: {
    "score": -0.6989676535413761,
    "rmse": 0.6989676535413761
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:50:04 INFO]: Training loss at epoch 166: 0.3530426099896431
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:50:19 INFO]: Training loss at epoch 180: 0.18143537640571594
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:50:29 INFO]: Training loss at epoch 167: 0.44522418081760406
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:50:32 INFO]: New best epoch, val score: -0.768777594233584
[08/12/2025 17:50:32 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:50:33 INFO]: Training loss at epoch 46: 1.0708311200141907
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:50:43 INFO]: Training loss at epoch 181: 0.24797382205724716
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:50:54 INFO]: Training loss at epoch 168: 0.4362002909183502
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:51:05 INFO]: Training loss at epoch 182: 0.1683027148246765
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:51:09 INFO]: Training loss at epoch 49: 0.47485461831092834
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:51:19 INFO]: Training loss at epoch 169: 0.5496037155389786
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:51:28 INFO]: Training stats: {
    "score": -0.6423548701309834,
    "rmse": 0.6423548701309834
}
[08/12/2025 17:51:28 INFO]: Val stats: {
    "score": -0.7925299420655423,
    "rmse": 0.7925299420655423
}
[08/12/2025 17:51:28 INFO]: Test stats: {
    "score": -0.7330722811130611,
    "rmse": 0.7330722811130611
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:51:29 INFO]: Training loss at epoch 183: 0.1686473712325096
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:51:37 INFO]: Training stats: {
    "score": -0.6010608413639608,
    "rmse": 0.6010608413639608
}
[08/12/2025 17:51:37 INFO]: Val stats: {
    "score": -0.792743519294158,
    "rmse": 0.792743519294158
}
[08/12/2025 17:51:37 INFO]: Test stats: {
    "score": -0.777852938937594,
    "rmse": 0.777852938937594
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:51:47 INFO]: New best epoch, val score: -0.792743519294158
[08/12/2025 17:51:47 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:51:52 INFO]: Training loss at epoch 184: 0.20400042086839676
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:51:52 INFO]: Training loss at epoch 170: 0.36295388638973236
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:52:03 INFO]: Training loss at epoch 47: 0.795930027961731
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:52:15 INFO]: Training loss at epoch 185: 0.15097957476973534
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:52:17 INFO]: Training loss at epoch 171: 0.577976644039154
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:52:38 INFO]: Training loss at epoch 186: 0.19117804616689682
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:52:43 INFO]: Training loss at epoch 172: 0.43496812880039215
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:53:01 INFO]: Training loss at epoch 187: 0.15666374564170837
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:53:03 INFO]: Training loss at epoch 50: 0.3395245373249054
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:53:07 INFO]: Training loss at epoch 173: 0.4150979816913605
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:53:25 INFO]: Training loss at epoch 188: 0.1342686228454113
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:53:32 INFO]: Training loss at epoch 174: 0.3849598914384842
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:53:34 INFO]: Training loss at epoch 48: 0.8410715162754059
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:53:44 INFO]: New best epoch, val score: -0.9002647936849182
[08/12/2025 17:53:44 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:53:48 INFO]: Training loss at epoch 189: 0.1387515515089035
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:53:56 INFO]: Training stats: {
    "score": -0.3550696219224343,
    "rmse": 0.3550696219224343
}
[08/12/2025 17:53:56 INFO]: Val stats: {
    "score": -0.6605889025461062,
    "rmse": 0.6605889025461062
}
[08/12/2025 17:53:56 INFO]: Test stats: {
    "score": -0.6594295387538718,
    "rmse": 0.6594295387538718
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:53:57 INFO]: Training loss at epoch 175: 0.3981298506259918
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:54:20 INFO]: Training loss at epoch 190: 0.19998887926340103
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:54:22 INFO]: Training loss at epoch 176: 0.4416201561689377
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:54:22 INFO]: Running Final Evaluation...
[W 2025-08-12 17:54:23,162] Trial 1 failed with parameters: {'embedding_head_pair': (384, 8), 'n_layers': 2, 'd_ffn_factor': 1.3032801700380832, 'attention_dropout': 0.09256676393661445, 'activation': 'relu', 'lr': 0.00026963662655384897} because of the following error: RuntimeError('Error(s) in loading state_dict for FTTransformer:\n\tUnexpected key(s) in state_dict: "layers.2.attention.W_q.weight", "layers.2.attention.W_q.bias", "layers.2.attention.W_k.weight", "layers.2.attention.W_k.bias", "layers.2.attention.W_v.weight", "layers.2.attention.W_v.bias", "layers.2.attention.W_out.weight", "layers.2.attention.W_out.bias", "layers.2.linear0.weight", "layers.2.linear0.bias", "layers.2.linear1.weight", "layers.2.linear1.bias", "layers.2.norm1.weight", "layers.2.norm1.bias", "layers.2.norm0.weight", "layers.2.norm0.bias", "layers.3.attention.W_q.weight", "layers.3.attention.W_q.bias", "layers.3.attention.W_k.weight", "layers.3.attention.W_k.bias", "layers.3.attention.W_v.weight", "layers.3.attention.W_v.bias", "layers.3.attention.W_out.weight", "layers.3.attention.W_out.bias", "layers.3.linear0.weight", "layers.3.linear0.bias", "layers.3.linear1.weight", "layers.3.linear1.bias", "layers.3.norm1.weight", "layers.3.norm1.bias", "layers.3.norm0.weight", "layers.3.norm0.bias", "layers.4.attention.W_q.weight", "layers.4.attention.W_q.bias", "layers.4.attention.W_k.weight", "layers.4.attention.W_k.bias", "layers.4.attention.W_v.weight", "layers.4.attention.W_v.bias", "layers.4.attention.W_out.weight", "layers.4.attention.W_out.bias", "layers.4.linear0.weight", "layers.4.linear0.bias", "layers.4.linear1.weight", "layers.4.linear1.bias", "layers.4.norm1.weight", "layers.4.norm1.bias", "layers.4.norm0.weight", "layers.4.norm0.bias", "layers.5.attention.W_q.weight", "layers.5.attention.W_q.bias", "layers.5.attention.W_k.weight", "layers.5.attention.W_k.bias", "layers.5.attention.W_v.weight", "layers.5.attention.W_v.bias", "layers.5.attention.W_out.weight", "layers.5.attention.W_out.bias", "layers.5.linear0.weight", "layers.5.linear0.bias", "layers.5.linear1.weight", "layers.5.linear1.bias", "layers.5.norm1.weight", "layers.5.norm1.bias", "layers.5.norm0.weight", "layers.5.norm0.bias", "layers.6.attention.W_q.weight", "layers.6.attention.W_q.bias", "layers.6.attention.W_k.weight", "layers.6.attention.W_k.bias", "layers.6.attention.W_v.weight", "layers.6.attention.W_v.bias", "layers.6.attention.W_out.weight", "layers.6.attention.W_out.bias", "layers.6.linear0.weight", "layers.6.linear0.bias", "layers.6.linear1.weight", "layers.6.linear1.bias", "layers.6.norm1.weight", "layers.6.norm1.bias", "layers.6.norm0.weight", "layers.6.norm0.bias", "layers.7.attention.W_q.weight", "layers.7.attention.W_q.bias", "layers.7.attention.W_k.weight", "layers.7.attention.W_k.bias", "layers.7.attention.W_v.weight", "layers.7.attention.W_v.bias", "layers.7.attention.W_out.weight", "layers.7.attention.W_out.bias", "layers.7.linear0.weight", "layers.7.linear0.bias", "layers.7.linear1.weight", "layers.7.linear1.bias", "layers.7.norm1.weight", "layers.7.norm1.bias", "layers.7.norm0.weight", "layers.7.norm0.bias", "layers.8.attention.W_q.weight", "layers.8.attention.W_q.bias", "layers.8.attention.W_k.weight", "layers.8.attention.W_k.bias", "layers.8.attention.W_v.weight", "layers.8.attention.W_v.bias", "layers.8.attention.W_out.weight", "layers.8.attention.W_out.bias", "layers.8.linear0.weight", "layers.8.linear0.bias", "layers.8.linear1.weight", "layers.8.linear1.bias", "layers.8.norm1.weight", "layers.8.norm1.bias", "layers.8.norm0.weight", "layers.8.norm0.bias", "layers.9.attention.W_q.weight", "layers.9.attention.W_q.bias", "layers.9.attention.W_k.weight", "layers.9.attention.W_k.bias", "layers.9.attention.W_v.weight", "layers.9.attention.W_v.bias", "layers.9.attention.W_out.weight", "layers.9.attention.W_out.bias", "layers.9.linear0.weight", "layers.9.linear0.bias", "layers.9.linear1.weight", "layers.9.linear1.bias", "layers.9.norm1.weight", "layers.9.norm1.bias", "layers.9.norm0.weight", "layers.9.norm0.bias". \n\tsize mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 256]) from checkpoint, the shape in current model is torch.Size([101, 384]).\n\tsize mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 256]) from checkpoint, the shape in current model is torch.Size([100, 384]).\n\tsize mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([274, 256]) from checkpoint, the shape in current model is torch.Size([500, 384]).\n\tsize mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([274]) from checkpoint, the shape in current model is torch.Size([500]).\n\tsize mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([256, 274]) from checkpoint, the shape in current model is torch.Size([384, 500]).\n\tsize mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([274, 256]) from checkpoint, the shape in current model is torch.Size([500, 384]).\n\tsize mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([274]) from checkpoint, the shape in current model is torch.Size([500]).\n\tsize mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([256, 274]) from checkpoint, the shape in current model is torch.Size([384, 500]).\n\tsize mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for last_normalization.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for last_normalization.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for head.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 384]).').
Traceback (most recent call last):
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 145, in <lambda>
    func = lambda trial: objective(trial, cfg, trial_stats, trial_counter, n_optuna_trials,
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 104, in objective
    stats = train_net_for_optuna.main(config, loaders, unique_categories, n_numerical, n_classes)
  File "/temporario2/14570653/tabular-transfer-learning/train_net_for_optuna.py", line 127, in main
    net.load_state_dict(torch.load(checkpoint_path)["net"])
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2215, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for FTTransformer:
	Unexpected key(s) in state_dict: "layers.2.attention.W_q.weight", "layers.2.attention.W_q.bias", "layers.2.attention.W_k.weight", "layers.2.attention.W_k.bias", "layers.2.attention.W_v.weight", "layers.2.attention.W_v.bias", "layers.2.attention.W_out.weight", "layers.2.attention.W_out.bias", "layers.2.linear0.weight", "layers.2.linear0.bias", "layers.2.linear1.weight", "layers.2.linear1.bias", "layers.2.norm1.weight", "layers.2.norm1.bias", "layers.2.norm0.weight", "layers.2.norm0.bias", "layers.3.attention.W_q.weight", "layers.3.attention.W_q.bias", "layers.3.attention.W_k.weight", "layers.3.attention.W_k.bias", "layers.3.attention.W_v.weight", "layers.3.attention.W_v.bias", "layers.3.attention.W_out.weight", "layers.3.attention.W_out.bias", "layers.3.linear0.weight", "layers.3.linear0.bias", "layers.3.linear1.weight", "layers.3.linear1.bias", "layers.3.norm1.weight", "layers.3.norm1.bias", "layers.3.norm0.weight", "layers.3.norm0.bias", "layers.4.attention.W_q.weight", "layers.4.attention.W_q.bias", "layers.4.attention.W_k.weight", "layers.4.attention.W_k.bias", "layers.4.attention.W_v.weight", "layers.4.attention.W_v.bias", "layers.4.attention.W_out.weight", "layers.4.attention.W_out.bias", "layers.4.linear0.weight", "layers.4.linear0.bias", "layers.4.linear1.weight", "layers.4.linear1.bias", "layers.4.norm1.weight", "layers.4.norm1.bias", "layers.4.norm0.weight", "layers.4.norm0.bias", "layers.5.attention.W_q.weight", "layers.5.attention.W_q.bias", "layers.5.attention.W_k.weight", "layers.5.attention.W_k.bias", "layers.5.attention.W_v.weight", "layers.5.attention.W_v.bias", "layers.5.attention.W_out.weight", "layers.5.attention.W_out.bias", "layers.5.linear0.weight", "layers.5.linear0.bias", "layers.5.linear1.weight", "layers.5.linear1.bias", "layers.5.norm1.weight", "layers.5.norm1.bias", "layers.5.norm0.weight", "layers.5.norm0.bias", "layers.6.attention.W_q.weight", "layers.6.attention.W_q.bias", "layers.6.attention.W_k.weight", "layers.6.attention.W_k.bias", "layers.6.attention.W_v.weight", "layers.6.attention.W_v.bias", "layers.6.attention.W_out.weight", "layers.6.attention.W_out.bias", "layers.6.linear0.weight", "layers.6.linear0.bias", "layers.6.linear1.weight", "layers.6.linear1.bias", "layers.6.norm1.weight", "layers.6.norm1.bias", "layers.6.norm0.weight", "layers.6.norm0.bias", "layers.7.attention.W_q.weight", "layers.7.attention.W_q.bias", "layers.7.attention.W_k.weight", "layers.7.attention.W_k.bias", "layers.7.attention.W_v.weight", "layers.7.attention.W_v.bias", "layers.7.attention.W_out.weight", "layers.7.attention.W_out.bias", "layers.7.linear0.weight", "layers.7.linear0.bias", "layers.7.linear1.weight", "layers.7.linear1.bias", "layers.7.norm1.weight", "layers.7.norm1.bias", "layers.7.norm0.weight", "layers.7.norm0.bias", "layers.8.attention.W_q.weight", "layers.8.attention.W_q.bias", "layers.8.attention.W_k.weight", "layers.8.attention.W_k.bias", "layers.8.attention.W_v.weight", "layers.8.attention.W_v.bias", "layers.8.attention.W_out.weight", "layers.8.attention.W_out.bias", "layers.8.linear0.weight", "layers.8.linear0.bias", "layers.8.linear1.weight", "layers.8.linear1.bias", "layers.8.norm1.weight", "layers.8.norm1.bias", "layers.8.norm0.weight", "layers.8.norm0.bias", "layers.9.attention.W_q.weight", "layers.9.attention.W_q.bias", "layers.9.attention.W_k.weight", "layers.9.attention.W_k.bias", "layers.9.attention.W_v.weight", "layers.9.attention.W_v.bias", "layers.9.attention.W_out.weight", "layers.9.attention.W_out.bias", "layers.9.linear0.weight", "layers.9.linear0.bias", "layers.9.linear1.weight", "layers.9.linear1.bias", "layers.9.norm1.weight", "layers.9.norm1.bias", "layers.9.norm0.weight", "layers.9.norm0.bias". 
	size mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 256]) from checkpoint, the shape in current model is torch.Size([101, 384]).
	size mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 256]) from checkpoint, the shape in current model is torch.Size([100, 384]).
	size mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([274, 256]) from checkpoint, the shape in current model is torch.Size([500, 384]).
	size mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([274]) from checkpoint, the shape in current model is torch.Size([500]).
	size mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([256, 274]) from checkpoint, the shape in current model is torch.Size([384, 500]).
	size mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([274, 256]) from checkpoint, the shape in current model is torch.Size([500, 384]).
	size mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([274]) from checkpoint, the shape in current model is torch.Size([500]).
	size mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([256, 274]) from checkpoint, the shape in current model is torch.Size([384, 500]).
	size mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for last_normalization.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for last_normalization.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for head.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 384]).
[W 2025-08-12 17:54:23,167] Trial 1 failed with value None.
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:54:29 INFO]: Training loss at epoch 51: 0.32563065737485886
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:54:40 INFO]: New best epoch, val score: -0.7648567003160333
[08/12/2025 17:54:40 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:54:47 INFO]: Training loss at epoch 177: 0.38281868398189545
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:55:04 INFO]: Training loss at epoch 49: 0.8541606068611145
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:55:11 INFO]: Training loss at epoch 178: 0.3790876418352127
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:55:35 INFO]: Training stats: {
    "score": -0.9713226076026238,
    "rmse": 0.9713226076026238
}
[08/12/2025 17:55:35 INFO]: Val stats: {
    "score": -0.8954842475107938,
    "rmse": 0.8954842475107938
}
[08/12/2025 17:55:35 INFO]: Test stats: {
    "score": -0.8815510732185207,
    "rmse": 0.8815510732185207
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:55:36 INFO]: Training loss at epoch 179: 0.45760923624038696
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:55:44 INFO]: Training stats: {
    "score": -0.7225975776080281,
    "rmse": 0.7225975776080281
}
[08/12/2025 17:55:44 INFO]: Val stats: {
    "score": -0.8938432264008627,
    "rmse": 0.8938432264008627
}
[08/12/2025 17:55:44 INFO]: Test stats: {
    "score": -0.7933987134485903,
    "rmse": 0.7933987134485903
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:55:46 INFO]: New best epoch, val score: -0.8954842475107938
[08/12/2025 17:55:46 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:56:01 INFO]: Training loss at epoch 52: 0.40777236223220825
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:56:10 INFO]: Training loss at epoch 180: 0.43358103930950165
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:56:34 INFO]: Training loss at epoch 181: 0.4481489658355713
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:56:59 INFO]: Training loss at epoch 182: 0.4619251787662506
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:57:06 INFO]: Training loss at epoch 50: 1.1607178449630737
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:57:17 INFO]: New best epoch, val score: -0.8907138972610972
[08/12/2025 17:57:17 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:57:25 INFO]: Training loss at epoch 183: 0.44809235632419586
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:57:34 INFO]: Training loss at epoch 53: 0.36004742980003357
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:57:49 INFO]: Training loss at epoch 184: 0.4756912589073181
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:58:13 INFO]: Training loss at epoch 185: 0.4643588960170746
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:58:36 INFO]: Training loss at epoch 51: 0.8766402900218964
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:58:38 INFO]: Training loss at epoch 186: 0.3798159956932068
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:59:03 INFO]: Training loss at epoch 187: 0.47631336748600006
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:59:06 INFO]: Training loss at epoch 54: 0.40435458719730377
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:59:27 INFO]: Training loss at epoch 188: 0.44877535104751587
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 17:59:52 INFO]: Training loss at epoch 189: 0.34879378229379654
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 18:00:00 INFO]: Training stats: {
    "score": -0.6336005852727488,
    "rmse": 0.6336005852727488
}
[08/12/2025 18:00:00 INFO]: Val stats: {
    "score": -0.7986211573243892,
    "rmse": 0.7986211573243892
}
[08/12/2025 18:00:00 INFO]: Test stats: {
    "score": -0.7173560772718973,
    "rmse": 0.7173560772718973
}
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 18:00:06 INFO]: Training loss at epoch 52: 0.8109092116355896
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 18:00:25 INFO]: Training loss at epoch 190: 0.4101412147283554
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 18:00:38 INFO]: Training loss at epoch 55: 0.38681723177433014
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:00:50 INFO]: Training loss at epoch 191: 0.4845573753118515
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 18:01:15 INFO]: Training loss at epoch 192: 0.3953539729118347
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:01:36 INFO]: Training loss at epoch 53: 0.9295170903205872
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 18:01:40 INFO]: Training loss at epoch 193: 0.4527572840452194
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:02:05 INFO]: Training loss at epoch 194: 0.472132071852684
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:02:10 INFO]: Training loss at epoch 56: 0.37305502593517303
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 18:02:32 INFO]: Training loss at epoch 195: 0.4848892390727997
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 18:02:57 INFO]: Training loss at epoch 196: 0.4848635345697403
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:03:12 INFO]: Training loss at epoch 54: 0.7232789993286133
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
[08/12/2025 18:03:21 INFO]: Training loss at epoch 197: 0.45556640625
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 128])
Tokenizer  -1: 128
[08/12/2025 18:03:35 INFO]: Training loss at epoch 57: 0.32660311460494995
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:03:45 INFO]: Training loss at epoch 198: 0.5221863985061646
Tokenizer output shape: torch.Size([66, 101, 128])
Tokenizer  -1: 128
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 128])
Tokenizer  -1: 128
[08/12/2025 18:03:48 INFO]: Running Final Evaluation...
[W 2025-08-12 18:03:49,297] Trial 6 failed with parameters: {'embedding_head_pair': (128, 4), 'n_layers': 8, 'd_ffn_factor': 0.8663550642853822, 'attention_dropout': 0.4743480359190597, 'activation': 'gelu', 'lr': 0.00021432046573591673} because of the following error: RuntimeError('Error(s) in loading state_dict for FTTransformer:\n\tUnexpected key(s) in state_dict: "layers.8.attention.W_q.weight", "layers.8.attention.W_q.bias", "layers.8.attention.W_k.weight", "layers.8.attention.W_k.bias", "layers.8.attention.W_v.weight", "layers.8.attention.W_v.bias", "layers.8.attention.W_out.weight", "layers.8.attention.W_out.bias", "layers.8.linear0.weight", "layers.8.linear0.bias", "layers.8.linear1.weight", "layers.8.linear1.bias", "layers.8.norm1.weight", "layers.8.norm1.bias", "layers.8.norm0.weight", "layers.8.norm0.bias", "layers.9.attention.W_q.weight", "layers.9.attention.W_q.bias", "layers.9.attention.W_k.weight", "layers.9.attention.W_k.bias", "layers.9.attention.W_v.weight", "layers.9.attention.W_v.bias", "layers.9.attention.W_out.weight", "layers.9.attention.W_out.bias", "layers.9.linear0.weight", "layers.9.linear0.bias", "layers.9.linear1.weight", "layers.9.linear1.bias", "layers.9.norm1.weight", "layers.9.norm1.bias", "layers.9.norm0.weight", "layers.9.norm0.bias". \n\tsize mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 256]) from checkpoint, the shape in current model is torch.Size([101, 128]).\n\tsize mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 256]) from checkpoint, the shape in current model is torch.Size([100, 128]).\n\tsize mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([274, 256]) from checkpoint, the shape in current model is torch.Size([110, 128]).\n\tsize mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([274]) from checkpoint, the shape in current model is torch.Size([110]).\n\tsize mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([256, 274]) from checkpoint, the shape in current model is torch.Size([128, 110]).\n\tsize mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([274, 256]) from checkpoint, the shape in current model is torch.Size([110, 128]).\n\tsize mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([274]) from checkpoint, the shape in current model is torch.Size([110]).\n\tsize mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([256, 274]) from checkpoint, the shape in current model is torch.Size([128, 110]).\n\tsize mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.2.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.2.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.2.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.2.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.2.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.2.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.2.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.2.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.2.linear0.weight: copying a param with shape torch.Size([274, 256]) from checkpoint, the shape in current model is torch.Size([110, 128]).\n\tsize mismatch for layers.2.linear0.bias: copying a param with shape torch.Size([274]) from checkpoint, the shape in current model is torch.Size([110]).\n\tsize mismatch for layers.2.linear1.weight: copying a param with shape torch.Size([256, 274]) from checkpoint, the shape in current model is torch.Size([128, 110]).\n\tsize mismatch for layers.2.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.2.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.2.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.2.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.2.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.3.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.3.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.3.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.3.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.3.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.3.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.3.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.3.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.3.linear0.weight: copying a param with shape torch.Size([274, 256]) from checkpoint, the shape in current model is torch.Size([110, 128]).\n\tsize mismatch for layers.3.linear0.bias: copying a param with shape torch.Size([274]) from checkpoint, the shape in current model is torch.Size([110]).\n\tsize mismatch for layers.3.linear1.weight: copying a param with shape torch.Size([256, 274]) from checkpoint, the shape in current model is torch.Size([128, 110]).\n\tsize mismatch for layers.3.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.3.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.3.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.3.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.3.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.4.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.4.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.4.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.4.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.4.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.4.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.4.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.4.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.4.linear0.weight: copying a param with shape torch.Size([274, 256]) from checkpoint, the shape in current model is torch.Size([110, 128]).\n\tsize mismatch for layers.4.linear0.bias: copying a param with shape torch.Size([274]) from checkpoint, the shape in current model is torch.Size([110]).\n\tsize mismatch for layers.4.linear1.weight: copying a param with shape torch.Size([256, 274]) from checkpoint, the shape in current model is torch.Size([128, 110]).\n\tsize mismatch for layers.4.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.4.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.4.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.4.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.4.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.5.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.5.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.5.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.5.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.5.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.5.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.5.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.5.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.5.linear0.weight: copying a param with shape torch.Size([274, 256]) from checkpoint, the shape in current model is torch.Size([110, 128]).\n\tsize mismatch for layers.5.linear0.bias: copying a param with shape torch.Size([274]) from checkpoint, the shape in current model is torch.Size([110]).\n\tsize mismatch for layers.5.linear1.weight: copying a param with shape torch.Size([256, 274]) from checkpoint, the shape in current model is torch.Size([128, 110]).\n\tsize mismatch for layers.5.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.5.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.5.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.5.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.5.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.6.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.6.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.6.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.6.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.6.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.6.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.6.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.6.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.6.linear0.weight: copying a param with shape torch.Size([274, 256]) from checkpoint, the shape in current model is torch.Size([110, 128]).\n\tsize mismatch for layers.6.linear0.bias: copying a param with shape torch.Size([274]) from checkpoint, the shape in current model is torch.Size([110]).\n\tsize mismatch for layers.6.linear1.weight: copying a param with shape torch.Size([256, 274]) from checkpoint, the shape in current model is torch.Size([128, 110]).\n\tsize mismatch for layers.6.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.6.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.6.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.6.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.6.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.7.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.7.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.7.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.7.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.7.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.7.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.7.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for layers.7.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.7.linear0.weight: copying a param with shape torch.Size([274, 256]) from checkpoint, the shape in current model is torch.Size([110, 128]).\n\tsize mismatch for layers.7.linear0.bias: copying a param with shape torch.Size([274]) from checkpoint, the shape in current model is torch.Size([110]).\n\tsize mismatch for layers.7.linear1.weight: copying a param with shape torch.Size([256, 274]) from checkpoint, the shape in current model is torch.Size([128, 110]).\n\tsize mismatch for layers.7.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.7.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.7.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.7.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layers.7.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for last_normalization.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for last_normalization.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for head.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 128]).').
Traceback (most recent call last):
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 145, in <lambda>
    func = lambda trial: objective(trial, cfg, trial_stats, trial_counter, n_optuna_trials,
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 104, in objective
    stats = train_net_for_optuna.main(config, loaders, unique_categories, n_numerical, n_classes)
  File "/temporario2/14570653/tabular-transfer-learning/train_net_for_optuna.py", line 127, in main
    net.load_state_dict(torch.load(checkpoint_path)["net"])
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2215, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for FTTransformer:
	Unexpected key(s) in state_dict: "layers.8.attention.W_q.weight", "layers.8.attention.W_q.bias", "layers.8.attention.W_k.weight", "layers.8.attention.W_k.bias", "layers.8.attention.W_v.weight", "layers.8.attention.W_v.bias", "layers.8.attention.W_out.weight", "layers.8.attention.W_out.bias", "layers.8.linear0.weight", "layers.8.linear0.bias", "layers.8.linear1.weight", "layers.8.linear1.bias", "layers.8.norm1.weight", "layers.8.norm1.bias", "layers.8.norm0.weight", "layers.8.norm0.bias", "layers.9.attention.W_q.weight", "layers.9.attention.W_q.bias", "layers.9.attention.W_k.weight", "layers.9.attention.W_k.bias", "layers.9.attention.W_v.weight", "layers.9.attention.W_v.bias", "layers.9.attention.W_out.weight", "layers.9.attention.W_out.bias", "layers.9.linear0.weight", "layers.9.linear0.bias", "layers.9.linear1.weight", "layers.9.linear1.bias", "layers.9.norm1.weight", "layers.9.norm1.bias", "layers.9.norm0.weight", "layers.9.norm0.bias". 
	size mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 256]) from checkpoint, the shape in current model is torch.Size([101, 128]).
	size mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 256]) from checkpoint, the shape in current model is torch.Size([100, 128]).
	size mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([274, 256]) from checkpoint, the shape in current model is torch.Size([110, 128]).
	size mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([274]) from checkpoint, the shape in current model is torch.Size([110]).
	size mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([256, 274]) from checkpoint, the shape in current model is torch.Size([128, 110]).
	size mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([274, 256]) from checkpoint, the shape in current model is torch.Size([110, 128]).
	size mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([274]) from checkpoint, the shape in current model is torch.Size([110]).
	size mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([256, 274]) from checkpoint, the shape in current model is torch.Size([128, 110]).
	size mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.2.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.2.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.2.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.2.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.2.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.2.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.2.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.2.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.2.linear0.weight: copying a param with shape torch.Size([274, 256]) from checkpoint, the shape in current model is torch.Size([110, 128]).
	size mismatch for layers.2.linear0.bias: copying a param with shape torch.Size([274]) from checkpoint, the shape in current model is torch.Size([110]).
	size mismatch for layers.2.linear1.weight: copying a param with shape torch.Size([256, 274]) from checkpoint, the shape in current model is torch.Size([128, 110]).
	size mismatch for layers.2.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.2.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.2.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.2.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.2.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.3.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.3.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.3.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.3.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.3.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.3.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.3.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.3.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.3.linear0.weight: copying a param with shape torch.Size([274, 256]) from checkpoint, the shape in current model is torch.Size([110, 128]).
	size mismatch for layers.3.linear0.bias: copying a param with shape torch.Size([274]) from checkpoint, the shape in current model is torch.Size([110]).
	size mismatch for layers.3.linear1.weight: copying a param with shape torch.Size([256, 274]) from checkpoint, the shape in current model is torch.Size([128, 110]).
	size mismatch for layers.3.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.3.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.3.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.3.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.3.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.4.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.4.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.4.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.4.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.4.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.4.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.4.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.4.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.4.linear0.weight: copying a param with shape torch.Size([274, 256]) from checkpoint, the shape in current model is torch.Size([110, 128]).
	size mismatch for layers.4.linear0.bias: copying a param with shape torch.Size([274]) from checkpoint, the shape in current model is torch.Size([110]).
	size mismatch for layers.4.linear1.weight: copying a param with shape torch.Size([256, 274]) from checkpoint, the shape in current model is torch.Size([128, 110]).
	size mismatch for layers.4.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.4.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.4.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.4.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.4.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.5.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.5.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.5.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.5.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.5.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.5.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.5.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.5.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.5.linear0.weight: copying a param with shape torch.Size([274, 256]) from checkpoint, the shape in current model is torch.Size([110, 128]).
	size mismatch for layers.5.linear0.bias: copying a param with shape torch.Size([274]) from checkpoint, the shape in current model is torch.Size([110]).
	size mismatch for layers.5.linear1.weight: copying a param with shape torch.Size([256, 274]) from checkpoint, the shape in current model is torch.Size([128, 110]).
	size mismatch for layers.5.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.5.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.5.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.5.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.5.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.6.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.6.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.6.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.6.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.6.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.6.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.6.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.6.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.6.linear0.weight: copying a param with shape torch.Size([274, 256]) from checkpoint, the shape in current model is torch.Size([110, 128]).
	size mismatch for layers.6.linear0.bias: copying a param with shape torch.Size([274]) from checkpoint, the shape in current model is torch.Size([110]).
	size mismatch for layers.6.linear1.weight: copying a param with shape torch.Size([256, 274]) from checkpoint, the shape in current model is torch.Size([128, 110]).
	size mismatch for layers.6.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.6.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.6.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.6.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.6.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.7.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.7.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.7.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.7.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.7.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.7.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.7.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
	size mismatch for layers.7.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.7.linear0.weight: copying a param with shape torch.Size([274, 256]) from checkpoint, the shape in current model is torch.Size([110, 128]).
	size mismatch for layers.7.linear0.bias: copying a param with shape torch.Size([274]) from checkpoint, the shape in current model is torch.Size([110]).
	size mismatch for layers.7.linear1.weight: copying a param with shape torch.Size([256, 274]) from checkpoint, the shape in current model is torch.Size([128, 110]).
	size mismatch for layers.7.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.7.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.7.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.7.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.7.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for last_normalization.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for last_normalization.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for head.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 128]).
[W 2025-08-12 18:03:49,304] Trial 6 failed with value None.
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:04:48 INFO]: Training loss at epoch 55: 0.9316678047180176
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:05:03 INFO]: Training loss at epoch 58: 0.3528873175382614
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:05:14 INFO]: New best epoch, val score: -0.7564894449199219
[08/12/2025 18:05:14 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:06:24 INFO]: Training loss at epoch 56: 0.8978887796401978
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:06:34 INFO]: Training loss at epoch 59: 0.34610477089881897
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:07:05 INFO]: Training stats: {
    "score": -0.563261726436006,
    "rmse": 0.563261726436006
}
[08/12/2025 18:07:05 INFO]: Val stats: {
    "score": -0.7268971092072873,
    "rmse": 0.7268971092072873
}
[08/12/2025 18:07:05 INFO]: Test stats: {
    "score": -0.7654689911724456,
    "rmse": 0.7654689911724456
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:07:15 INFO]: New best epoch, val score: -0.7268971092072873
[08/12/2025 18:07:15 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:08:00 INFO]: Training loss at epoch 57: 0.6728496700525284
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:08:11 INFO]: New best epoch, val score: -0.8791307508673033
[08/12/2025 18:08:11 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:08:35 INFO]: Training loss at epoch 60: 0.2930978462100029
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:08:46 INFO]: New best epoch, val score: -0.7193595197042832
[08/12/2025 18:08:46 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:09:36 INFO]: Training loss at epoch 58: 0.7896602749824524
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:09:47 INFO]: New best epoch, val score: -0.8581171934050411
[08/12/2025 18:09:47 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:10:06 INFO]: Training loss at epoch 61: 0.33324871957302094
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:11:12 INFO]: Training loss at epoch 59: 1.0110146403312683
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:11:34 INFO]: Training loss at epoch 62: 0.308877095580101
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:11:44 INFO]: Training stats: {
    "score": -0.8486744135124592,
    "rmse": 0.8486744135124592
}
[08/12/2025 18:11:44 INFO]: Val stats: {
    "score": -0.8615529089112407,
    "rmse": 0.8615529089112407
}
[08/12/2025 18:11:44 INFO]: Test stats: {
    "score": -0.8025017823994243,
    "rmse": 0.8025017823994243
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:13:07 INFO]: Training loss at epoch 63: 0.3360135406255722
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:13:19 INFO]: Training loss at epoch 60: 0.7286824584007263
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:14:38 INFO]: Training loss at epoch 64: 0.3623012751340866
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:14:53 INFO]: Training loss at epoch 61: 0.7736698687076569
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:16:11 INFO]: Training loss at epoch 65: 0.3318290710449219
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:16:25 INFO]: Training loss at epoch 62: 0.8405576348304749
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:17:43 INFO]: Training loss at epoch 66: 0.3053363114595413
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:17:59 INFO]: Training loss at epoch 63: 0.6651864945888519
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:19:12 INFO]: Training loss at epoch 67: 0.3481336832046509
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:19:36 INFO]: Training loss at epoch 64: 0.6449942886829376
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:19:47 INFO]: New best epoch, val score: -0.8484747004333896
[08/12/2025 18:19:47 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:20:43 INFO]: Training loss at epoch 68: 0.2930065169930458
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:21:07 INFO]: Training loss at epoch 65: 0.7367376387119293
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:22:14 INFO]: Training loss at epoch 69: 0.283584401011467
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:22:42 INFO]: Training loss at epoch 66: 0.57415571808815
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:22:43 INFO]: Training stats: {
    "score": -0.5436134015055359,
    "rmse": 0.5436134015055359
}
[08/12/2025 18:22:43 INFO]: Val stats: {
    "score": -0.7714782472379941,
    "rmse": 0.7714782472379941
}
[08/12/2025 18:22:43 INFO]: Test stats: {
    "score": -0.7895704710766933,
    "rmse": 0.7895704710766933
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:24:14 INFO]: Training loss at epoch 70: 0.3975343257188797
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:24:19 INFO]: Training loss at epoch 67: 0.7732642292976379
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:25:47 INFO]: Training loss at epoch 71: 0.2446463331580162
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:25:55 INFO]: Training loss at epoch 68: 0.580178290605545
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:27:19 INFO]: Training loss at epoch 72: 0.23211189359426498
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:27:32 INFO]: Training loss at epoch 69: 0.612705409526825
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:28:05 INFO]: Training stats: {
    "score": -0.8311508974787974,
    "rmse": 0.8311508974787974
}
[08/12/2025 18:28:05 INFO]: Val stats: {
    "score": -0.9666146668203909,
    "rmse": 0.9666146668203909
}
[08/12/2025 18:28:05 INFO]: Test stats: {
    "score": -0.904012031035859,
    "rmse": 0.904012031035859
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:28:52 INFO]: Training loss at epoch 73: 0.29846662282943726
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:29:42 INFO]: Training loss at epoch 70: 0.616879940032959
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:30:20 INFO]: Training loss at epoch 74: 0.31478914618492126
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:31:17 INFO]: Training loss at epoch 71: 0.6704050302505493
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:31:51 INFO]: Training loss at epoch 75: 0.2827947437763214
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:32:53 INFO]: Training loss at epoch 72: 0.7079009711742401
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:33:22 INFO]: Training loss at epoch 76: 0.3086143583059311
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:34:28 INFO]: Training loss at epoch 73: 0.6245662569999695
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:34:51 INFO]: Training loss at epoch 77: 0.2819449454545975
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:36:02 INFO]: Training loss at epoch 74: 0.6612982451915741
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:36:23 INFO]: Training loss at epoch 78: 0.23506063222885132
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:37:36 INFO]: Training loss at epoch 75: 0.7434407472610474
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:37:48 INFO]: New best epoch, val score: -0.7970977205948137
[08/12/2025 18:37:48 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:37:54 INFO]: Training loss at epoch 79: 0.23772969841957092
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:38:25 INFO]: Training stats: {
    "score": -0.5255537349792905,
    "rmse": 0.5255537349792905
}
[08/12/2025 18:38:25 INFO]: Val stats: {
    "score": -0.7390100186936945,
    "rmse": 0.7390100186936945
}
[08/12/2025 18:38:25 INFO]: Test stats: {
    "score": -0.8184268297809185,
    "rmse": 0.8184268297809185
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:39:10 INFO]: Training loss at epoch 76: 0.5884168744087219
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:39:55 INFO]: Training loss at epoch 80: 0.27025048434734344
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:40:42 INFO]: Training loss at epoch 77: 0.6640960574150085
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:41:23 INFO]: Training loss at epoch 81: 0.2217877134680748
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:42:19 INFO]: Training loss at epoch 78: 0.6797138452529907
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:42:56 INFO]: Training loss at epoch 82: 0.26939044892787933
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:43:54 INFO]: Training loss at epoch 79: 0.5918125212192535
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:44:26 INFO]: Training loss at epoch 83: 0.22890786826610565
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:44:28 INFO]: Training stats: {
    "score": -0.744646025581402,
    "rmse": 0.744646025581402
}
[08/12/2025 18:44:28 INFO]: Val stats: {
    "score": -0.7910553268404237,
    "rmse": 0.7910553268404237
}
[08/12/2025 18:44:28 INFO]: Test stats: {
    "score": -0.7930527601937993,
    "rmse": 0.7930527601937993
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:44:40 INFO]: New best epoch, val score: -0.7910553268404237
[08/12/2025 18:44:40 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:45:58 INFO]: Training loss at epoch 84: 0.2853539511561394
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:46:03 INFO]: Training loss at epoch 80: 0.5600065886974335
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:47:29 INFO]: Training loss at epoch 85: 0.2061961144208908
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:47:38 INFO]: Training loss at epoch 81: 0.5889375507831573
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:49:02 INFO]: Training loss at epoch 86: 0.285023957490921
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:49:15 INFO]: Training loss at epoch 82: 0.5830259323120117
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:50:34 INFO]: Training loss at epoch 87: 0.25199446082115173
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:50:50 INFO]: Training loss at epoch 83: 0.5507018864154816
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:52:07 INFO]: Training loss at epoch 88: 0.22881846129894257
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:52:25 INFO]: Training loss at epoch 84: 0.4291892498731613
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:53:37 INFO]: Training loss at epoch 89: 0.2692996487021446
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:54:01 INFO]: Training loss at epoch 85: 0.4536411166191101
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:54:07 INFO]: Training stats: {
    "score": -0.47560764543495243,
    "rmse": 0.47560764543495243
}
[08/12/2025 18:54:07 INFO]: Val stats: {
    "score": -0.758327312160908,
    "rmse": 0.758327312160908
}
[08/12/2025 18:54:07 INFO]: Test stats: {
    "score": -0.7777313033433149,
    "rmse": 0.7777313033433149
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:55:36 INFO]: Training loss at epoch 86: 0.6358064264059067
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:55:38 INFO]: Training loss at epoch 90: 0.2129632607102394
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:57:11 INFO]: Training loss at epoch 91: 0.24026888608932495
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:57:12 INFO]: Training loss at epoch 87: 0.4282955229282379
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:57:22 INFO]: Running Final Evaluation...
[W 2025-08-12 18:57:23,112] Trial 3 failed with parameters: {'embedding_head_pair': (384, 12), 'n_layers': 4, 'd_ffn_factor': 1.9822324352683127, 'attention_dropout': 0.40655619925192205, 'activation': 'reglu', 'lr': 0.0003917037976639179} because of the following error: RuntimeError('Error(s) in loading state_dict for FTTransformer:\n\tUnexpected key(s) in state_dict: "layers.4.attention.W_q.weight", "layers.4.attention.W_q.bias", "layers.4.attention.W_k.weight", "layers.4.attention.W_k.bias", "layers.4.attention.W_v.weight", "layers.4.attention.W_v.bias", "layers.4.attention.W_out.weight", "layers.4.attention.W_out.bias", "layers.4.linear0.weight", "layers.4.linear0.bias", "layers.4.linear1.weight", "layers.4.linear1.bias", "layers.4.norm1.weight", "layers.4.norm1.bias", "layers.4.norm0.weight", "layers.4.norm0.bias", "layers.5.attention.W_q.weight", "layers.5.attention.W_q.bias", "layers.5.attention.W_k.weight", "layers.5.attention.W_k.bias", "layers.5.attention.W_v.weight", "layers.5.attention.W_v.bias", "layers.5.attention.W_out.weight", "layers.5.attention.W_out.bias", "layers.5.linear0.weight", "layers.5.linear0.bias", "layers.5.linear1.weight", "layers.5.linear1.bias", "layers.5.norm1.weight", "layers.5.norm1.bias", "layers.5.norm0.weight", "layers.5.norm0.bias", "layers.6.attention.W_q.weight", "layers.6.attention.W_q.bias", "layers.6.attention.W_k.weight", "layers.6.attention.W_k.bias", "layers.6.attention.W_v.weight", "layers.6.attention.W_v.bias", "layers.6.attention.W_out.weight", "layers.6.attention.W_out.bias", "layers.6.linear0.weight", "layers.6.linear0.bias", "layers.6.linear1.weight", "layers.6.linear1.bias", "layers.6.norm1.weight", "layers.6.norm1.bias", "layers.6.norm0.weight", "layers.6.norm0.bias", "layers.7.attention.W_q.weight", "layers.7.attention.W_q.bias", "layers.7.attention.W_k.weight", "layers.7.attention.W_k.bias", "layers.7.attention.W_v.weight", "layers.7.attention.W_v.bias", "layers.7.attention.W_out.weight", "layers.7.attention.W_out.bias", "layers.7.linear0.weight", "layers.7.linear0.bias", "layers.7.linear1.weight", "layers.7.linear1.bias", "layers.7.norm1.weight", "layers.7.norm1.bias", "layers.7.norm0.weight", "layers.7.norm0.bias", "layers.8.attention.W_q.weight", "layers.8.attention.W_q.bias", "layers.8.attention.W_k.weight", "layers.8.attention.W_k.bias", "layers.8.attention.W_v.weight", "layers.8.attention.W_v.bias", "layers.8.attention.W_out.weight", "layers.8.attention.W_out.bias", "layers.8.linear0.weight", "layers.8.linear0.bias", "layers.8.linear1.weight", "layers.8.linear1.bias", "layers.8.norm1.weight", "layers.8.norm1.bias", "layers.8.norm0.weight", "layers.8.norm0.bias", "layers.9.attention.W_q.weight", "layers.9.attention.W_q.bias", "layers.9.attention.W_k.weight", "layers.9.attention.W_k.bias", "layers.9.attention.W_v.weight", "layers.9.attention.W_v.bias", "layers.9.attention.W_out.weight", "layers.9.attention.W_out.bias", "layers.9.linear0.weight", "layers.9.linear0.bias", "layers.9.linear1.weight", "layers.9.linear1.bias", "layers.9.norm1.weight", "layers.9.norm1.bias", "layers.9.norm0.weight", "layers.9.norm0.bias". \n\tsize mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 256]) from checkpoint, the shape in current model is torch.Size([101, 384]).\n\tsize mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 256]) from checkpoint, the shape in current model is torch.Size([100, 384]).\n\tsize mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([274, 256]) from checkpoint, the shape in current model is torch.Size([1522, 384]).\n\tsize mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([274]) from checkpoint, the shape in current model is torch.Size([1522]).\n\tsize mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([256, 274]) from checkpoint, the shape in current model is torch.Size([384, 761]).\n\tsize mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([274, 256]) from checkpoint, the shape in current model is torch.Size([1522, 384]).\n\tsize mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([274]) from checkpoint, the shape in current model is torch.Size([1522]).\n\tsize mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([256, 274]) from checkpoint, the shape in current model is torch.Size([384, 761]).\n\tsize mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.2.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.2.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.2.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.2.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.linear0.weight: copying a param with shape torch.Size([274, 256]) from checkpoint, the shape in current model is torch.Size([1522, 384]).\n\tsize mismatch for layers.2.linear0.bias: copying a param with shape torch.Size([274]) from checkpoint, the shape in current model is torch.Size([1522]).\n\tsize mismatch for layers.2.linear1.weight: copying a param with shape torch.Size([256, 274]) from checkpoint, the shape in current model is torch.Size([384, 761]).\n\tsize mismatch for layers.2.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.3.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.3.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.3.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.3.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.linear0.weight: copying a param with shape torch.Size([274, 256]) from checkpoint, the shape in current model is torch.Size([1522, 384]).\n\tsize mismatch for layers.3.linear0.bias: copying a param with shape torch.Size([274]) from checkpoint, the shape in current model is torch.Size([1522]).\n\tsize mismatch for layers.3.linear1.weight: copying a param with shape torch.Size([256, 274]) from checkpoint, the shape in current model is torch.Size([384, 761]).\n\tsize mismatch for layers.3.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for last_normalization.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for last_normalization.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for head.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 384]).').
Traceback (most recent call last):
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 145, in <lambda>
    func = lambda trial: objective(trial, cfg, trial_stats, trial_counter, n_optuna_trials,
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 104, in objective
    stats = train_net_for_optuna.main(config, loaders, unique_categories, n_numerical, n_classes)
  File "/temporario2/14570653/tabular-transfer-learning/train_net_for_optuna.py", line 127, in main
    net.load_state_dict(torch.load(checkpoint_path)["net"])
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2215, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for FTTransformer:
	Unexpected key(s) in state_dict: "layers.4.attention.W_q.weight", "layers.4.attention.W_q.bias", "layers.4.attention.W_k.weight", "layers.4.attention.W_k.bias", "layers.4.attention.W_v.weight", "layers.4.attention.W_v.bias", "layers.4.attention.W_out.weight", "layers.4.attention.W_out.bias", "layers.4.linear0.weight", "layers.4.linear0.bias", "layers.4.linear1.weight", "layers.4.linear1.bias", "layers.4.norm1.weight", "layers.4.norm1.bias", "layers.4.norm0.weight", "layers.4.norm0.bias", "layers.5.attention.W_q.weight", "layers.5.attention.W_q.bias", "layers.5.attention.W_k.weight", "layers.5.attention.W_k.bias", "layers.5.attention.W_v.weight", "layers.5.attention.W_v.bias", "layers.5.attention.W_out.weight", "layers.5.attention.W_out.bias", "layers.5.linear0.weight", "layers.5.linear0.bias", "layers.5.linear1.weight", "layers.5.linear1.bias", "layers.5.norm1.weight", "layers.5.norm1.bias", "layers.5.norm0.weight", "layers.5.norm0.bias", "layers.6.attention.W_q.weight", "layers.6.attention.W_q.bias", "layers.6.attention.W_k.weight", "layers.6.attention.W_k.bias", "layers.6.attention.W_v.weight", "layers.6.attention.W_v.bias", "layers.6.attention.W_out.weight", "layers.6.attention.W_out.bias", "layers.6.linear0.weight", "layers.6.linear0.bias", "layers.6.linear1.weight", "layers.6.linear1.bias", "layers.6.norm1.weight", "layers.6.norm1.bias", "layers.6.norm0.weight", "layers.6.norm0.bias", "layers.7.attention.W_q.weight", "layers.7.attention.W_q.bias", "layers.7.attention.W_k.weight", "layers.7.attention.W_k.bias", "layers.7.attention.W_v.weight", "layers.7.attention.W_v.bias", "layers.7.attention.W_out.weight", "layers.7.attention.W_out.bias", "layers.7.linear0.weight", "layers.7.linear0.bias", "layers.7.linear1.weight", "layers.7.linear1.bias", "layers.7.norm1.weight", "layers.7.norm1.bias", "layers.7.norm0.weight", "layers.7.norm0.bias", "layers.8.attention.W_q.weight", "layers.8.attention.W_q.bias", "layers.8.attention.W_k.weight", "layers.8.attention.W_k.bias", "layers.8.attention.W_v.weight", "layers.8.attention.W_v.bias", "layers.8.attention.W_out.weight", "layers.8.attention.W_out.bias", "layers.8.linear0.weight", "layers.8.linear0.bias", "layers.8.linear1.weight", "layers.8.linear1.bias", "layers.8.norm1.weight", "layers.8.norm1.bias", "layers.8.norm0.weight", "layers.8.norm0.bias", "layers.9.attention.W_q.weight", "layers.9.attention.W_q.bias", "layers.9.attention.W_k.weight", "layers.9.attention.W_k.bias", "layers.9.attention.W_v.weight", "layers.9.attention.W_v.bias", "layers.9.attention.W_out.weight", "layers.9.attention.W_out.bias", "layers.9.linear0.weight", "layers.9.linear0.bias", "layers.9.linear1.weight", "layers.9.linear1.bias", "layers.9.norm1.weight", "layers.9.norm1.bias", "layers.9.norm0.weight", "layers.9.norm0.bias". 
	size mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 256]) from checkpoint, the shape in current model is torch.Size([101, 384]).
	size mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 256]) from checkpoint, the shape in current model is torch.Size([100, 384]).
	size mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([274, 256]) from checkpoint, the shape in current model is torch.Size([1522, 384]).
	size mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([274]) from checkpoint, the shape in current model is torch.Size([1522]).
	size mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([256, 274]) from checkpoint, the shape in current model is torch.Size([384, 761]).
	size mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([274, 256]) from checkpoint, the shape in current model is torch.Size([1522, 384]).
	size mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([274]) from checkpoint, the shape in current model is torch.Size([1522]).
	size mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([256, 274]) from checkpoint, the shape in current model is torch.Size([384, 761]).
	size mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.2.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.2.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.2.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.2.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.2.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.2.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.2.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.2.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.2.linear0.weight: copying a param with shape torch.Size([274, 256]) from checkpoint, the shape in current model is torch.Size([1522, 384]).
	size mismatch for layers.2.linear0.bias: copying a param with shape torch.Size([274]) from checkpoint, the shape in current model is torch.Size([1522]).
	size mismatch for layers.2.linear1.weight: copying a param with shape torch.Size([256, 274]) from checkpoint, the shape in current model is torch.Size([384, 761]).
	size mismatch for layers.2.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.2.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.2.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.2.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.2.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.3.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.3.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.3.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.3.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.3.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.3.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.3.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.3.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.3.linear0.weight: copying a param with shape torch.Size([274, 256]) from checkpoint, the shape in current model is torch.Size([1522, 384]).
	size mismatch for layers.3.linear0.bias: copying a param with shape torch.Size([274]) from checkpoint, the shape in current model is torch.Size([1522]).
	size mismatch for layers.3.linear1.weight: copying a param with shape torch.Size([256, 274]) from checkpoint, the shape in current model is torch.Size([384, 761]).
	size mismatch for layers.3.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.3.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.3.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.3.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.3.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for last_normalization.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for last_normalization.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for head.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 384]).
[W 2025-08-12 18:57:23,118] Trial 3 failed with value None.
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:58:45 INFO]: Training loss at epoch 88: 0.5696341693401337
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:00:15 INFO]: Training loss at epoch 89: 0.5196355581283569
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:00:46 INFO]: Training stats: {
    "score": -0.7109501597481396,
    "rmse": 0.7109501597481396
}
[08/12/2025 19:00:46 INFO]: Val stats: {
    "score": -0.8058695206395736,
    "rmse": 0.8058695206395736
}
[08/12/2025 19:00:46 INFO]: Test stats: {
    "score": -0.8028362528537115,
    "rmse": 0.8028362528537115
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:02:16 INFO]: Training loss at epoch 90: 0.43494701385498047
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:03:48 INFO]: Training loss at epoch 91: 0.4222726970911026
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:05:18 INFO]: Training loss at epoch 92: 0.5185230523347855
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:06:49 INFO]: Training loss at epoch 93: 0.5445816516876221
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:08:20 INFO]: Training loss at epoch 94: 0.6368153840303421
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:09:51 INFO]: Training loss at epoch 95: 0.5251375734806061
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:11:21 INFO]: Training loss at epoch 96: 0.35501260310411453
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:11:33 INFO]: New best epoch, val score: -0.7772845007262754
[08/12/2025 19:11:33 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:12:53 INFO]: Training loss at epoch 97: 0.43712931871414185
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:14:27 INFO]: Training loss at epoch 98: 0.4669840335845947
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:15:56 INFO]: Training loss at epoch 99: 0.4701375365257263
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:16:27 INFO]: Training stats: {
    "score": -0.6716974756388696,
    "rmse": 0.6716974756388696
}
[08/12/2025 19:16:27 INFO]: Val stats: {
    "score": -0.8384397135645933,
    "rmse": 0.8384397135645933
}
[08/12/2025 19:16:27 INFO]: Test stats: {
    "score": -0.7751866443859255,
    "rmse": 0.7751866443859255
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:17:58 INFO]: Training loss at epoch 100: 0.485187366604805
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:19:29 INFO]: Training loss at epoch 101: 0.49143746495246887
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:21:00 INFO]: Training loss at epoch 102: 0.3664880692958832
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:22:29 INFO]: Training loss at epoch 103: 0.37655021250247955
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:24:16 INFO]: Training loss at epoch 104: 0.3953136205673218
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:26:07 INFO]: Training loss at epoch 105: 0.4280976355075836
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:27:58 INFO]: Training loss at epoch 106: 0.4755991995334625
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:29:49 INFO]: Training loss at epoch 107: 0.5216068923473358
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:31:40 INFO]: Training loss at epoch 108: 0.5179262608289719
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:33:30 INFO]: Training loss at epoch 109: 0.4708481878042221
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:34:08 INFO]: Training stats: {
    "score": -0.6687949226304781,
    "rmse": 0.6687949226304781
}
[08/12/2025 19:34:08 INFO]: Val stats: {
    "score": -0.7578570294055021,
    "rmse": 0.7578570294055021
}
[08/12/2025 19:34:08 INFO]: Test stats: {
    "score": -0.7603168365745975,
    "rmse": 0.7603168365745975
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:34:22 INFO]: New best epoch, val score: -0.7578570294055021
[08/12/2025 19:34:22 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:35:59 INFO]: Training loss at epoch 110: 0.3913554847240448
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:36:12 INFO]: New best epoch, val score: -0.754127078864875
[08/12/2025 19:36:12 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:37:50 INFO]: Training loss at epoch 111: 0.5627211928367615
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:39:41 INFO]: Training loss at epoch 112: 0.510398805141449
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:41:32 INFO]: Training loss at epoch 113: 0.4832407385110855
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:43:23 INFO]: Training loss at epoch 114: 0.3604668080806732
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:45:15 INFO]: Training loss at epoch 115: 0.5070283859968185
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:47:06 INFO]: Training loss at epoch 116: 0.5801467299461365
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:48:57 INFO]: Training loss at epoch 117: 0.36896440386772156
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:50:48 INFO]: Training loss at epoch 118: 0.4084034711122513
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:52:39 INFO]: Training loss at epoch 119: 0.42525342106819153
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:53:17 INFO]: Training stats: {
    "score": -0.6729165348587801,
    "rmse": 0.6729165348587801
}
[08/12/2025 19:53:17 INFO]: Val stats: {
    "score": -0.7918688623418247,
    "rmse": 0.7918688623418247
}
[08/12/2025 19:53:17 INFO]: Test stats: {
    "score": -0.788872890889186,
    "rmse": 0.788872890889186
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:55:08 INFO]: Training loss at epoch 120: 0.4700523018836975
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:56:59 INFO]: Training loss at epoch 121: 0.48965364694595337
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:58:49 INFO]: Training loss at epoch 122: 0.530071422457695
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:00:39 INFO]: Training loss at epoch 123: 0.37197625637054443
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:02:30 INFO]: Training loss at epoch 124: 0.39976489543914795
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:04:21 INFO]: Training loss at epoch 125: 0.4383438527584076
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:06:11 INFO]: Training loss at epoch 126: 0.4724489897489548
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:08:01 INFO]: Training loss at epoch 127: 0.34391336143016815
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:09:52 INFO]: Training loss at epoch 128: 0.4013676047325134
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:11:43 INFO]: Training loss at epoch 129: 0.34910009801387787
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:12:21 INFO]: Training stats: {
    "score": -0.6241072262975765,
    "rmse": 0.6241072262975765
}
[08/12/2025 20:12:21 INFO]: Val stats: {
    "score": -0.8131277695090195,
    "rmse": 0.8131277695090195
}
[08/12/2025 20:12:21 INFO]: Test stats: {
    "score": -0.7379021000913406,
    "rmse": 0.7379021000913406
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:14:12 INFO]: Training loss at epoch 130: 0.34881503880023956
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:16:02 INFO]: Training loss at epoch 131: 0.31325697898864746
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:17:53 INFO]: Training loss at epoch 132: 0.37359902262687683
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:19:44 INFO]: Training loss at epoch 133: 0.30193567276000977
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:21:35 INFO]: Training loss at epoch 134: 0.5156541764736176
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:23:25 INFO]: Training loss at epoch 135: 0.4444376826286316
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:25:16 INFO]: Training loss at epoch 136: 0.46323801577091217
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:27:07 INFO]: Training loss at epoch 137: 0.3854387700557709
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:28:58 INFO]: Training loss at epoch 138: 0.36126065254211426
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:30:48 INFO]: Training loss at epoch 139: 0.4169795364141464
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:31:27 INFO]: Training stats: {
    "score": -0.6114990395465663,
    "rmse": 0.6114990395465663
}
[08/12/2025 20:31:27 INFO]: Val stats: {
    "score": -0.8174849570957936,
    "rmse": 0.8174849570957936
}
[08/12/2025 20:31:27 INFO]: Test stats: {
    "score": -0.7241185339763765,
    "rmse": 0.7241185339763765
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:33:18 INFO]: Training loss at epoch 140: 0.3529791086912155
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:35:07 INFO]: Training loss at epoch 141: 0.3905086815357208
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:35:21 INFO]: Running Final Evaluation...
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:35:59 INFO]: Training accuracy: {
    "score": -0.6665243986724892,
    "rmse": 0.6665243986724892
}
[08/12/2025 20:35:59 INFO]: Val accuracy: {
    "score": -0.754127078864875,
    "rmse": 0.754127078864875
}
[08/12/2025 20:35:59 INFO]: Test accuracy: {
    "score": -0.7533046049425218,
    "rmse": 0.7533046049425218
}
[08/12/2025 20:35:59 INFO]: {
    "dataset": "ic_upstream3",
    "model_name": "ft_transformer",
    "run_id": "revolved-Rody",
    "best_epoch": 110,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.7533046049425218,
        "rmse": 0.7533046049425218
    },
    "train_stats": {
        "score": -0.6665243986724892,
        "rmse": 0.6665243986724892
    },
    "val_stats": {
        "score": -0.754127078864875,
        "rmse": 0.754127078864875
    }
}
[I 2025-08-12 20:35:59,633] Trial 5 finished with value: -0.754127078864875 and parameters: {'embedding_head_pair': (256, 8), 'n_layers': 10, 'd_ffn_factor': 1.0728724718098297, 'attention_dropout': 0.2816706765849131, 'activation': 'relu', 'lr': 0.00020972477966317145}. Best is trial 9 with value: -0.7049337494332752.
