[08/07/2025 10:01:05 INFO]: Building Dataset
[08/07/2025 10:01:05 INFO]: pre normalizer.fit

[08/07/2025 10:01:05 INFO]: pos normalizer.fit

[08/07/2025 10:01:09 INFO]: Task: regression, Dataset: ic_upstream3, n_numerical: 100, n_categorical: 0, n_classes: 1, n_train_samples: 282, n_val_samples: 66, n_test_samples: 87
[08/07/2025 10:01:33 INFO]: 
_________________________________________________

[08/07/2025 10:01:33 INFO]: train_net_for_optune.py main() running.
[08/07/2025 10:01:33 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 1.50591093005661
  attention_dropout: 0.08129444311723372
  ffn_dropout: 0.08129444311723372
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.71212549689093e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: -1
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: renowned-Tressie

[08/07/2025 10:01:35 INFO]: This ft_transformer has 0.306 million parameters.
[08/07/2025 10:01:35 INFO]: Training will start at epoch 0.
[08/07/2025 10:02:25 INFO]: ==> Starting training for 200 epochs...
[08/07/2025 10:03:07 INFO]: Training loss at epoch 0: 1.1656736135482788
[08/07/2025 10:03:11 INFO]: New best epoch, val score: -0.9550440351692315
[08/07/2025 10:03:11 INFO]: Saving model to: model_best.pth
[08/07/2025 10:03:39 INFO]: Training loss at epoch 1: 1.0700299143791199
[08/07/2025 10:04:10 INFO]: Training loss at epoch 2: 0.9423669278621674
[08/07/2025 10:04:41 INFO]: Training loss at epoch 3: 1.0388861894607544
[08/07/2025 10:05:12 INFO]: Training loss at epoch 4: 1.0377653241157532
[08/07/2025 10:05:42 INFO]: Training loss at epoch 5: 0.9096492528915405
[08/07/2025 10:06:12 INFO]: Training loss at epoch 6: 1.169205516576767
[08/07/2025 10:06:43 INFO]: Training loss at epoch 7: 0.8622035384178162
[08/07/2025 10:07:13 INFO]: Training loss at epoch 8: 1.0733164250850677
[08/07/2025 10:07:43 INFO]: Training loss at epoch 9: 1.1618346571922302
[08/07/2025 10:07:54 INFO]: Training stats: {
    "score": -1.0046855545198514,
    "rmse": 1.0046855545198514
}
[08/07/2025 10:07:54 INFO]: Val stats: {
    "score": -0.9575012940604208,
    "rmse": 0.9575012940604208
}
[08/07/2025 10:07:54 INFO]: Test stats: {
    "score": -0.9347106315564774,
    "rmse": 0.9347106315564774
}
[08/07/2025 10:08:24 INFO]: Training loss at epoch 10: 1.0168472528457642
[08/07/2025 10:08:28 INFO]: New best epoch, val score: -0.9545750041354967
[08/07/2025 10:08:28 INFO]: Saving model to: model_best.pth
[08/07/2025 10:08:55 INFO]: Training loss at epoch 11: 1.0905719995498657
[08/07/2025 10:08:59 INFO]: New best epoch, val score: -0.9516813039709421
[08/07/2025 10:08:59 INFO]: Saving model to: model_best.pth
[08/07/2025 10:09:25 INFO]: Training loss at epoch 12: 0.8486404716968536
[08/07/2025 10:09:29 INFO]: New best epoch, val score: -0.9496642213341226
[08/07/2025 10:09:29 INFO]: Saving model to: model_best.pth
[08/07/2025 10:09:56 INFO]: Training loss at epoch 13: 1.0649199783802032
[08/07/2025 10:10:00 INFO]: New best epoch, val score: -0.9483839013626282
[08/07/2025 10:10:00 INFO]: Saving model to: model_best.pth
[08/07/2025 10:10:27 INFO]: Training loss at epoch 14: 1.0355308055877686
[08/07/2025 10:10:30 INFO]: New best epoch, val score: -0.9478347802720173
[08/07/2025 10:10:30 INFO]: Saving model to: model_best.pth
[08/07/2025 10:10:57 INFO]: Training loss at epoch 15: 1.001976639032364
[08/07/2025 10:11:01 INFO]: New best epoch, val score: -0.94712396126207
[08/07/2025 10:11:01 INFO]: Saving model to: model_best.pth
[08/07/2025 10:11:27 INFO]: Training loss at epoch 16: 1.0982355773448944
[08/07/2025 10:11:31 INFO]: New best epoch, val score: -0.9465415317331203
[08/07/2025 10:11:31 INFO]: Saving model to: model_best.pth
[08/07/2025 10:11:57 INFO]: Training loss at epoch 17: 1.159862756729126
[08/07/2025 10:12:27 INFO]: Training loss at epoch 18: 0.9463244378566742
[08/07/2025 10:12:57 INFO]: Training loss at epoch 19: 1.1143631339073181
[08/07/2025 10:13:08 INFO]: Training stats: {
    "score": -1.0035506991700491,
    "rmse": 1.0035506991700491
}
[08/07/2025 10:13:08 INFO]: Val stats: {
    "score": -0.9489503359002985,
    "rmse": 0.9489503359002985
}
[08/07/2025 10:13:08 INFO]: Test stats: {
    "score": -0.9289204105424332,
    "rmse": 0.9289204105424332
}
[08/07/2025 10:13:39 INFO]: Training loss at epoch 20: 0.9123766720294952
[08/07/2025 10:14:09 INFO]: Training loss at epoch 21: 1.0815685391426086
[08/07/2025 10:14:39 INFO]: Training loss at epoch 22: 0.8730047345161438
[08/07/2025 10:15:09 INFO]: Training loss at epoch 23: 1.3113463819026947
[08/07/2025 10:15:40 INFO]: Training loss at epoch 24: 1.0086986124515533
[08/07/2025 10:16:10 INFO]: Training loss at epoch 25: 0.8782801032066345
[08/07/2025 10:16:14 INFO]: New best epoch, val score: -0.9452369746923538
[08/07/2025 10:16:14 INFO]: Saving model to: model_best.pth
[08/07/2025 10:16:40 INFO]: Training loss at epoch 26: 0.8267604410648346
[08/07/2025 10:16:44 INFO]: New best epoch, val score: -0.9433058590535232
[08/07/2025 10:16:44 INFO]: Saving model to: model_best.pth
[08/07/2025 10:17:10 INFO]: Training loss at epoch 27: 1.1339067816734314
[08/07/2025 10:17:14 INFO]: New best epoch, val score: -0.9417983069054476
[08/07/2025 10:17:14 INFO]: Saving model to: model_best.pth
[08/07/2025 10:17:40 INFO]: Training loss at epoch 28: 0.9254181981086731
[08/07/2025 10:17:44 INFO]: New best epoch, val score: -0.9408822760573632
[08/07/2025 10:17:44 INFO]: Saving model to: model_best.pth
[08/07/2025 10:18:10 INFO]: Training loss at epoch 29: 0.9528649151325226
[08/07/2025 10:18:22 INFO]: Training stats: {
    "score": -1.0048276611265632,
    "rmse": 1.0048276611265632
}
[08/07/2025 10:18:22 INFO]: Val stats: {
    "score": -0.9402866083300694,
    "rmse": 0.9402866083300694
}
[08/07/2025 10:18:22 INFO]: Test stats: {
    "score": -0.9239178826894099,
    "rmse": 0.9239178826894099
}
[08/07/2025 10:18:26 INFO]: New best epoch, val score: -0.9402866083300694
[08/07/2025 10:18:26 INFO]: Saving model to: model_best.pth
[08/07/2025 10:18:53 INFO]: Training loss at epoch 30: 1.083539456129074
[08/07/2025 10:18:57 INFO]: New best epoch, val score: -0.9401445601401942
[08/07/2025 10:18:57 INFO]: Saving model to: model_best.pth
[08/07/2025 10:19:23 INFO]: Training loss at epoch 31: 1.1438738405704498
[08/07/2025 10:19:54 INFO]: Training loss at epoch 32: 1.0827943086624146
[08/07/2025 10:20:25 INFO]: Training loss at epoch 33: 0.989940881729126
[08/07/2025 10:20:56 INFO]: Training loss at epoch 34: 1.087539553642273
[08/07/2025 10:21:26 INFO]: Training loss at epoch 35: 0.8616306781768799
[08/07/2025 10:21:58 INFO]: Training loss at epoch 36: 0.9598330557346344
[08/07/2025 10:22:30 INFO]: Training loss at epoch 37: 1.080406367778778
[08/07/2025 10:23:01 INFO]: Training loss at epoch 38: 1.1773271560668945
[08/07/2025 10:23:32 INFO]: Training loss at epoch 39: 1.02528715133667
[08/07/2025 10:23:44 INFO]: Training stats: {
    "score": -0.9996322546489321,
    "rmse": 0.9996322546489321
}
[08/07/2025 10:23:44 INFO]: Val stats: {
    "score": -0.9507282074632795,
    "rmse": 0.9507282074632795
}
[08/07/2025 10:23:44 INFO]: Test stats: {
    "score": -0.9269532868174487,
    "rmse": 0.9269532868174487
}
[08/07/2025 10:24:15 INFO]: Training loss at epoch 40: 0.812160462141037
[08/07/2025 10:24:46 INFO]: Training loss at epoch 41: 0.9271713495254517
[08/07/2025 10:25:16 INFO]: Training loss at epoch 42: 1.1487602889537811
[08/07/2025 10:25:47 INFO]: Training loss at epoch 43: 1.028647541999817
[08/07/2025 10:26:19 INFO]: Training loss at epoch 44: 0.8557976484298706
[08/07/2025 10:26:50 INFO]: Training loss at epoch 45: 1.1479251384735107
[08/07/2025 10:27:23 INFO]: Training loss at epoch 46: 1.1722954511642456
[08/07/2025 10:27:54 INFO]: Training loss at epoch 47: 1.1922889649868011
[08/07/2025 10:28:27 INFO]: Training loss at epoch 48: 0.96870356798172
[08/07/2025 10:28:59 INFO]: Training loss at epoch 49: 0.8421452343463898
[08/07/2025 10:29:10 INFO]: Training stats: {
    "score": -0.9984481663814306,
    "rmse": 0.9984481663814306
}
[08/07/2025 10:29:10 INFO]: Val stats: {
    "score": -0.9485369609310161,
    "rmse": 0.9485369609310161
}
[08/07/2025 10:29:10 INFO]: Test stats: {
    "score": -0.9251561639529038,
    "rmse": 0.9251561639529038
}
[08/07/2025 10:29:41 INFO]: Training loss at epoch 50: 0.9711803793907166
[08/07/2025 10:30:11 INFO]: Training loss at epoch 51: 1.0679429173469543
[08/07/2025 10:30:41 INFO]: Training loss at epoch 52: 1.0827102661132812
[08/07/2025 10:31:14 INFO]: Training loss at epoch 53: 0.8811669051647186
[08/07/2025 10:31:46 INFO]: Training loss at epoch 54: 0.8146818280220032
[08/07/2025 10:32:16 INFO]: Training loss at epoch 55: 0.8916894793510437
[08/07/2025 10:32:47 INFO]: Training loss at epoch 56: 0.9356555044651031
[08/07/2025 10:33:17 INFO]: Training loss at epoch 57: 0.7287176847457886
[08/07/2025 10:33:48 INFO]: Training loss at epoch 58: 0.7780001759529114
[08/07/2025 10:34:18 INFO]: Training loss at epoch 59: 1.0080467462539673
[08/07/2025 10:34:30 INFO]: Training stats: {
    "score": -0.9975527815601903,
    "rmse": 0.9975527815601903
}
[08/07/2025 10:34:30 INFO]: Val stats: {
    "score": -0.9424603977895785,
    "rmse": 0.9424603977895785
}
[08/07/2025 10:34:30 INFO]: Test stats: {
    "score": -0.9217166784283304,
    "rmse": 0.9217166784283304
}
[08/07/2025 10:35:00 INFO]: Training loss at epoch 60: 1.1350414752960205
[08/07/2025 10:35:30 INFO]: Training loss at epoch 61: 0.8951295018196106
[08/07/2025 10:35:34 INFO]: New best epoch, val score: -0.9399829690190271
[08/07/2025 10:35:34 INFO]: Saving model to: model_best.pth
[08/07/2025 10:36:01 INFO]: Training loss at epoch 62: 0.9616318643093109
[08/07/2025 10:36:05 INFO]: New best epoch, val score: -0.9390539886460911
[08/07/2025 10:36:05 INFO]: Saving model to: model_best.pth
[08/07/2025 10:36:31 INFO]: Training loss at epoch 63: 0.9883074164390564
[08/07/2025 10:36:35 INFO]: New best epoch, val score: -0.9387289456361477
[08/07/2025 10:36:35 INFO]: Saving model to: model_best.pth
[08/07/2025 10:37:02 INFO]: Training loss at epoch 64: 0.9063215255737305
[08/07/2025 10:37:32 INFO]: Training loss at epoch 65: 0.9381230175495148
[08/07/2025 10:38:02 INFO]: Training loss at epoch 66: 0.9151003360748291
[08/07/2025 10:38:33 INFO]: Training loss at epoch 67: 0.9033472537994385
[08/07/2025 10:39:03 INFO]: Training loss at epoch 68: 1.0415008664131165
[08/07/2025 10:39:34 INFO]: Training loss at epoch 69: 1.2138676047325134
[08/07/2025 10:39:45 INFO]: Training stats: {
    "score": -0.9961110336715747,
    "rmse": 0.9961110336715747
}
[08/07/2025 10:39:45 INFO]: Val stats: {
    "score": -0.9413741546247403,
    "rmse": 0.9413741546247403
}
[08/07/2025 10:39:45 INFO]: Test stats: {
    "score": -0.9202558246379587,
    "rmse": 0.9202558246379587
}
[08/07/2025 10:40:15 INFO]: Training loss at epoch 70: 0.8483471870422363
[08/07/2025 10:40:46 INFO]: Training loss at epoch 71: 1.043907791376114
[08/07/2025 10:41:16 INFO]: Training loss at epoch 72: 0.873729795217514
[08/07/2025 10:41:46 INFO]: Training loss at epoch 73: 1.024231642484665
[08/07/2025 10:42:17 INFO]: Training loss at epoch 74: 1.064282476902008
[08/07/2025 10:42:48 INFO]: Training loss at epoch 75: 1.1294901072978973
[08/07/2025 10:43:18 INFO]: Training loss at epoch 76: 0.9631931185722351
[08/07/2025 10:43:49 INFO]: Training loss at epoch 77: 0.9202753305435181
[08/07/2025 10:44:20 INFO]: Training loss at epoch 78: 1.0679189264774323
[08/07/2025 10:44:51 INFO]: Training loss at epoch 79: 1.0458281934261322
[08/07/2025 10:45:02 INFO]: Training stats: {
    "score": -0.9939037565598583,
    "rmse": 0.9939037565598583
}
[08/07/2025 10:45:02 INFO]: Val stats: {
    "score": -0.9567230656055297,
    "rmse": 0.9567230656055297
}
[08/07/2025 10:45:02 INFO]: Test stats: {
    "score": -0.9269815502112843,
    "rmse": 0.9269815502112843
}
[08/07/2025 10:45:33 INFO]: Training loss at epoch 80: 0.9555870294570923
[08/07/2025 10:46:04 INFO]: Training loss at epoch 81: 0.9065273404121399
[08/07/2025 10:46:34 INFO]: Training loss at epoch 82: 1.0873412489891052
[08/07/2025 10:47:06 INFO]: Training loss at epoch 83: 1.0746099948883057
[08/07/2025 10:47:37 INFO]: Training loss at epoch 84: 1.072100281715393
[08/07/2025 10:48:08 INFO]: Training loss at epoch 85: 0.9431584179401398
[08/07/2025 10:48:39 INFO]: Training loss at epoch 86: 0.9711576700210571
[08/07/2025 10:49:10 INFO]: Training loss at epoch 87: 0.9167626202106476
[08/07/2025 10:49:40 INFO]: Training loss at epoch 88: 1.03423672914505
[08/07/2025 10:50:11 INFO]: Training loss at epoch 89: 1.0473579168319702
[08/07/2025 10:50:22 INFO]: Training stats: {
    "score": -0.9922769971833117,
    "rmse": 0.9922769971833117
}
[08/07/2025 10:50:22 INFO]: Val stats: {
    "score": -0.955518228250399,
    "rmse": 0.955518228250399
}
[08/07/2025 10:50:22 INFO]: Test stats: {
    "score": -0.9253159256477731,
    "rmse": 0.9253159256477731
}
[08/07/2025 10:50:51 INFO]: Training loss at epoch 90: 1.0135012865066528
[08/07/2025 10:51:21 INFO]: Training loss at epoch 91: 0.8808999955654144
[08/07/2025 10:51:50 INFO]: Training loss at epoch 92: 0.7923215329647064
[08/07/2025 10:52:20 INFO]: Training loss at epoch 93: 0.9425564706325531
[08/07/2025 10:52:49 INFO]: Training loss at epoch 94: 0.9806807339191437
[08/07/2025 10:52:53 INFO]: Running Final Evaluation...
[08/07/2025 10:53:07 INFO]: Training accuracy: {
    "score": -0.9981339200624781,
    "rmse": 0.9981339200624781
}
[08/07/2025 10:53:07 INFO]: Val accuracy: {
    "score": -0.9387289456361477,
    "rmse": 0.9387289456361477
}
[08/07/2025 10:53:07 INFO]: Test accuracy: {
    "score": -0.9200287345721458,
    "rmse": 0.9200287345721458
}
[08/07/2025 10:53:07 INFO]: {
    "dataset": "ic_upstream3",
    "model_name": "ft_transformer",
    "run_id": "renowned-Tressie",
    "best_epoch": 63,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9200287345721458,
        "rmse": 0.9200287345721458
    },
    "train_stats": {
        "score": -0.9981339200624781,
        "rmse": 0.9981339200624781
    },
    "val_stats": {
        "score": -0.9387289456361477,
        "rmse": 0.9387289456361477
    }
}
[08/07/2025 10:53:07 INFO]: 
_________________________________________________

[08/07/2025 10:53:07 INFO]: train_net_for_optune.py main() running.
[08/07/2025 10:53:07 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.3365315908404805
  attention_dropout: 0.10553758509839267
  ffn_dropout: 0.10553758509839267
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 6.142976837509995e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: renowned-Tressie

[08/07/2025 10:53:07 INFO]: This ft_transformer has 1.437 million parameters.
[08/07/2025 10:53:07 INFO]: Training will start at epoch 0.
[08/07/2025 10:53:07 INFO]: ==> Starting training for 200 epochs...
[08/07/2025 10:53:22 INFO]: Training loss at epoch 0: 0.8483389914035797
[08/07/2025 10:53:24 INFO]: New best epoch, val score: -0.9380119108428046
[08/07/2025 10:53:24 INFO]: Saving model to: model_best.pth
[08/07/2025 10:53:39 INFO]: Training loss at epoch 1: 1.1559779047966003
[08/07/2025 10:53:57 INFO]: Training loss at epoch 2: 1.2070571780204773
[08/07/2025 10:54:14 INFO]: Training loss at epoch 3: 1.0347136855125427
[08/07/2025 10:54:16 INFO]: New best epoch, val score: -0.9239308253279517
[08/07/2025 10:54:16 INFO]: Saving model to: model_best.pth
[08/07/2025 10:54:31 INFO]: Training loss at epoch 4: 1.0263384282588959
[08/07/2025 10:54:33 INFO]: New best epoch, val score: -0.9147852349457417
[08/07/2025 10:54:33 INFO]: Saving model to: model_best.pth
[08/07/2025 10:54:50 INFO]: Training loss at epoch 5: 0.9738630652427673
[08/07/2025 10:55:09 INFO]: Training loss at epoch 6: 0.9894691705703735
[08/07/2025 10:55:27 INFO]: Training loss at epoch 7: 1.2116042971611023
[08/07/2025 10:55:45 INFO]: Training loss at epoch 8: 1.0542272925376892
[08/07/2025 10:56:02 INFO]: Training loss at epoch 9: 0.7644971162080765
[08/07/2025 10:56:08 INFO]: Training stats: {
    "score": -0.9810106695801781,
    "rmse": 0.9810106695801781
}
[08/07/2025 10:56:08 INFO]: Val stats: {
    "score": -0.9203649513656303,
    "rmse": 0.9203649513656303
}
[08/07/2025 10:56:08 INFO]: Test stats: {
    "score": -0.8986217367154343,
    "rmse": 0.8986217367154343
}
[08/07/2025 10:56:25 INFO]: Training loss at epoch 10: 0.8892757594585419
[08/07/2025 10:56:42 INFO]: Training loss at epoch 11: 1.0475475192070007
[08/07/2025 10:57:00 INFO]: Training loss at epoch 12: 1.067420780658722
[08/07/2025 10:57:17 INFO]: Training loss at epoch 13: 0.8455676138401031
[08/07/2025 10:57:34 INFO]: Training loss at epoch 14: 0.9079697132110596
[08/07/2025 10:57:51 INFO]: Training loss at epoch 15: 1.0477334558963776
[08/07/2025 10:58:09 INFO]: Training loss at epoch 16: 0.9569786787033081
[08/07/2025 10:58:26 INFO]: Training loss at epoch 17: 0.7657806873321533
[08/07/2025 10:58:28 INFO]: New best epoch, val score: -0.9116802949105763
[08/07/2025 10:58:28 INFO]: Saving model to: model_best.pth
[08/07/2025 10:58:43 INFO]: Training loss at epoch 18: 1.0262462198734283
[08/07/2025 10:58:45 INFO]: New best epoch, val score: -0.9056932338012135
[08/07/2025 10:58:45 INFO]: Saving model to: model_best.pth
[08/07/2025 10:59:01 INFO]: Training loss at epoch 19: 0.9872288405895233
[08/07/2025 10:59:07 INFO]: Training stats: {
    "score": -0.9617154317977562,
    "rmse": 0.9617154317977562
}
[08/07/2025 10:59:07 INFO]: Val stats: {
    "score": -0.9060455741769878,
    "rmse": 0.9060455741769878
}
[08/07/2025 10:59:07 INFO]: Test stats: {
    "score": -0.881608411768959,
    "rmse": 0.881608411768959
}
[08/07/2025 10:59:24 INFO]: Training loss at epoch 20: 1.0466378331184387
[08/07/2025 10:59:42 INFO]: Training loss at epoch 21: 0.9569043517112732
[08/07/2025 10:59:59 INFO]: Training loss at epoch 22: 0.9448308348655701
[08/07/2025 11:00:16 INFO]: Training loss at epoch 23: 1.1699009239673615
[08/07/2025 11:00:33 INFO]: Training loss at epoch 24: 0.8533813655376434
[08/07/2025 11:00:51 INFO]: Training loss at epoch 25: 0.8295904099941254
[08/07/2025 11:01:08 INFO]: Training loss at epoch 26: 0.7043535709381104
[08/07/2025 11:01:25 INFO]: Training loss at epoch 27: 0.904678463935852
[08/07/2025 11:01:43 INFO]: Training loss at epoch 28: 0.8275854587554932
[08/07/2025 11:01:45 INFO]: New best epoch, val score: -0.8998944732795017
[08/07/2025 11:01:45 INFO]: Saving model to: model_best.pth
[08/07/2025 11:02:00 INFO]: Training loss at epoch 29: 1.0447444915771484
[08/07/2025 11:02:06 INFO]: Training stats: {
    "score": -0.9336708179754183,
    "rmse": 0.9336708179754183
}
[08/07/2025 11:02:06 INFO]: Val stats: {
    "score": -0.8972728022814052,
    "rmse": 0.8972728022814052
}
[08/07/2025 11:02:06 INFO]: Test stats: {
    "score": -0.862342516319835,
    "rmse": 0.862342516319835
}
[08/07/2025 11:02:08 INFO]: New best epoch, val score: -0.8972728022814052
[08/07/2025 11:02:08 INFO]: Saving model to: model_best.pth
[08/07/2025 11:02:24 INFO]: Training loss at epoch 30: 0.9526195228099823
[08/07/2025 11:02:41 INFO]: Training loss at epoch 31: 0.9265317022800446
[08/07/2025 11:02:59 INFO]: Training loss at epoch 32: 0.7313536703586578
[08/07/2025 11:03:16 INFO]: Training loss at epoch 33: 0.7683553099632263
[08/07/2025 11:03:33 INFO]: Training loss at epoch 34: 0.9904665350914001
[08/07/2025 11:03:51 INFO]: Training loss at epoch 35: 0.8180486261844635
[08/07/2025 11:04:08 INFO]: Training loss at epoch 36: 0.9920638203620911
[08/07/2025 11:04:25 INFO]: Training loss at epoch 37: 0.8524065017700195
[08/07/2025 11:04:42 INFO]: Training loss at epoch 38: 0.8982174396514893
[08/07/2025 11:04:45 INFO]: New best epoch, val score: -0.8881806092789817
[08/07/2025 11:04:45 INFO]: Saving model to: model_best.pth
[08/07/2025 11:05:00 INFO]: Training loss at epoch 39: 0.6899682134389877
[08/07/2025 11:05:06 INFO]: Training stats: {
    "score": -0.9021977141089255,
    "rmse": 0.9021977141089255
}
[08/07/2025 11:05:06 INFO]: Val stats: {
    "score": -0.8694081281226749,
    "rmse": 0.8694081281226749
}
[08/07/2025 11:05:06 INFO]: Test stats: {
    "score": -0.8333616246953203,
    "rmse": 0.8333616246953203
}
[08/07/2025 11:05:08 INFO]: New best epoch, val score: -0.8694081281226749
[08/07/2025 11:05:08 INFO]: Saving model to: model_best.pth
[08/07/2025 11:05:24 INFO]: Training loss at epoch 40: 0.7278944253921509
[08/07/2025 11:05:26 INFO]: New best epoch, val score: -0.8686138910647573
[08/07/2025 11:05:26 INFO]: Saving model to: model_best.pth
[08/07/2025 11:05:42 INFO]: Training loss at epoch 41: 0.883987307548523
[08/07/2025 11:05:44 INFO]: New best epoch, val score: -0.8624773255076688
[08/07/2025 11:05:44 INFO]: Saving model to: model_best.pth
[08/07/2025 11:05:59 INFO]: Training loss at epoch 42: 0.8925839066505432
[08/07/2025 11:06:01 INFO]: New best epoch, val score: -0.8589302096555673
[08/07/2025 11:06:01 INFO]: Saving model to: model_best.pth
[08/07/2025 11:06:16 INFO]: Training loss at epoch 43: 0.8956746757030487
[08/07/2025 11:06:34 INFO]: Training loss at epoch 44: 0.8633646667003632
[08/07/2025 11:06:51 INFO]: Training loss at epoch 45: 0.7354693710803986
[08/07/2025 11:07:08 INFO]: Training loss at epoch 46: 0.7899555563926697
[08/07/2025 11:07:26 INFO]: Training loss at epoch 47: 0.8755859732627869
[08/07/2025 11:07:43 INFO]: Training loss at epoch 48: 0.8734914660453796
[08/07/2025 11:08:00 INFO]: Training loss at epoch 49: 0.6978159248828888
[08/07/2025 11:08:06 INFO]: Training stats: {
    "score": -0.8580456573902138,
    "rmse": 0.8580456573902138
}
[08/07/2025 11:08:06 INFO]: Val stats: {
    "score": -0.8915706000130501,
    "rmse": 0.8915706000130501
}
[08/07/2025 11:08:06 INFO]: Test stats: {
    "score": -0.817739438933278,
    "rmse": 0.817739438933278
}
[08/07/2025 11:08:24 INFO]: Training loss at epoch 50: 0.9114986956119537
[08/07/2025 11:08:26 INFO]: New best epoch, val score: -0.8568723130891182
[08/07/2025 11:08:26 INFO]: Saving model to: model_best.pth
[08/07/2025 11:08:41 INFO]: Training loss at epoch 51: 0.6932311654090881
[08/07/2025 11:08:43 INFO]: New best epoch, val score: -0.8405014463512128
[08/07/2025 11:08:43 INFO]: Saving model to: model_best.pth
[08/07/2025 11:08:58 INFO]: Training loss at epoch 52: 0.7583138048648834
[08/07/2025 11:09:01 INFO]: New best epoch, val score: -0.8348563754511059
[08/07/2025 11:09:01 INFO]: Saving model to: model_best.pth
[08/07/2025 11:09:16 INFO]: Training loss at epoch 53: 0.8704967498779297
[08/07/2025 11:09:33 INFO]: Training loss at epoch 54: 0.6620903015136719
[08/07/2025 11:09:51 INFO]: Training loss at epoch 55: 0.683645486831665
[08/07/2025 11:10:08 INFO]: Training loss at epoch 56: 0.7385625243186951
[08/07/2025 11:10:25 INFO]: Training loss at epoch 57: 0.7719583809375763
[08/07/2025 11:10:42 INFO]: Training loss at epoch 58: 0.6798714399337769
[08/07/2025 11:11:00 INFO]: Training loss at epoch 59: 0.6123678088188171
[08/07/2025 11:11:06 INFO]: Training stats: {
    "score": -0.7993165749913701,
    "rmse": 0.7993165749913701
}
[08/07/2025 11:11:06 INFO]: Val stats: {
    "score": -0.8563643441457724,
    "rmse": 0.8563643441457724
}
[08/07/2025 11:11:06 INFO]: Test stats: {
    "score": -0.7705100695372877,
    "rmse": 0.7705100695372877
}
[08/07/2025 11:11:23 INFO]: Training loss at epoch 60: 0.5541888326406479
[08/07/2025 11:11:25 INFO]: New best epoch, val score: -0.8246236693821435
[08/07/2025 11:11:25 INFO]: Saving model to: model_best.pth
[08/07/2025 11:11:40 INFO]: Training loss at epoch 61: 0.6613986194133759
[08/07/2025 11:11:57 INFO]: Training loss at epoch 62: 0.5898248106241226
[08/07/2025 11:12:15 INFO]: Training loss at epoch 63: 0.7600424885749817
[08/07/2025 11:12:32 INFO]: Training loss at epoch 64: 0.6495534777641296
[08/07/2025 11:12:49 INFO]: Training loss at epoch 65: 0.6411601305007935
[08/07/2025 11:13:06 INFO]: Training loss at epoch 66: 0.5620823502540588
[08/07/2025 11:13:24 INFO]: Training loss at epoch 67: 0.677442729473114
[08/07/2025 11:13:26 INFO]: New best epoch, val score: -0.8143464687066863
[08/07/2025 11:13:26 INFO]: Saving model to: model_best.pth
[08/07/2025 11:13:41 INFO]: Training loss at epoch 68: 0.6901246309280396
[08/07/2025 11:13:58 INFO]: Training loss at epoch 69: 0.7593600451946259
[08/07/2025 11:14:04 INFO]: Training stats: {
    "score": -0.7647521133831832,
    "rmse": 0.7647521133831832
}
[08/07/2025 11:14:04 INFO]: Val stats: {
    "score": -0.858910501250156,
    "rmse": 0.858910501250156
}
[08/07/2025 11:14:04 INFO]: Test stats: {
    "score": -0.7527258326719429,
    "rmse": 0.7527258326719429
}
[08/07/2025 11:14:21 INFO]: Training loss at epoch 70: 0.5109278559684753
[08/07/2025 11:14:39 INFO]: Training loss at epoch 71: 0.6649976372718811
[08/07/2025 11:14:56 INFO]: Training loss at epoch 72: 0.7726192772388458
[08/07/2025 11:15:13 INFO]: Training loss at epoch 73: 0.5928794741630554
[08/07/2025 11:15:30 INFO]: Training loss at epoch 74: 0.7237993478775024
[08/07/2025 11:15:48 INFO]: Training loss at epoch 75: 0.6202243566513062
[08/07/2025 11:16:05 INFO]: Training loss at epoch 76: 0.5980090796947479
[08/07/2025 11:16:22 INFO]: Training loss at epoch 77: 0.6529636085033417
[08/07/2025 11:16:39 INFO]: Training loss at epoch 78: 0.5075175017118454
[08/07/2025 11:16:57 INFO]: Training loss at epoch 79: 0.6150647699832916
[08/07/2025 11:17:02 INFO]: Training stats: {
    "score": -0.7446499733993246,
    "rmse": 0.7446499733993246
}
[08/07/2025 11:17:02 INFO]: Val stats: {
    "score": -0.851824970095246,
    "rmse": 0.851824970095246
}
[08/07/2025 11:17:02 INFO]: Test stats: {
    "score": -0.7479898142494147,
    "rmse": 0.7479898142494147
}
[08/07/2025 11:17:20 INFO]: Training loss at epoch 80: 0.639255166053772
[08/07/2025 11:17:37 INFO]: Training loss at epoch 81: 0.5214569419622421
[08/07/2025 11:17:54 INFO]: Training loss at epoch 82: 0.5431296825408936
[08/07/2025 11:18:11 INFO]: Training loss at epoch 83: 0.49122095108032227
[08/07/2025 11:18:29 INFO]: Training loss at epoch 84: 0.5918321311473846
[08/07/2025 11:18:46 INFO]: Training loss at epoch 85: 0.5759808123111725
[08/07/2025 11:19:03 INFO]: Training loss at epoch 86: 0.6611252725124359
[08/07/2025 11:19:20 INFO]: Training loss at epoch 87: 0.5389196276664734
[08/07/2025 11:19:38 INFO]: Training loss at epoch 88: 0.7047200202941895
[08/07/2025 11:19:56 INFO]: Training loss at epoch 89: 0.6646594107151031
[08/07/2025 11:20:02 INFO]: Training stats: {
    "score": -0.7262764952074826,
    "rmse": 0.7262764952074826
}
[08/07/2025 11:20:02 INFO]: Val stats: {
    "score": -0.8440608065106838,
    "rmse": 0.8440608065106838
}
[08/07/2025 11:20:02 INFO]: Test stats: {
    "score": -0.7357181596396934,
    "rmse": 0.7357181596396934
}
[08/07/2025 11:20:19 INFO]: Training loss at epoch 90: 0.516922190785408
[08/07/2025 11:20:37 INFO]: Training loss at epoch 91: 0.588032454252243
[08/07/2025 11:20:54 INFO]: Training loss at epoch 92: 0.5167403072118759
[08/07/2025 11:21:11 INFO]: Training loss at epoch 93: 0.5547464489936829
[08/07/2025 11:21:28 INFO]: Training loss at epoch 94: 0.5277131199836731
[08/07/2025 11:21:46 INFO]: Training loss at epoch 95: 0.5572352707386017
[08/07/2025 11:22:03 INFO]: Training loss at epoch 96: 0.6309670507907867
[08/07/2025 11:22:20 INFO]: Training loss at epoch 97: 0.5690327733755112
[08/07/2025 11:22:37 INFO]: Training loss at epoch 98: 0.5104149729013443
[08/07/2025 11:22:39 INFO]: Running Final Evaluation...
[08/07/2025 11:22:48 INFO]: Training accuracy: {
    "score": -0.7872619629964532,
    "rmse": 0.7872619629964532
}
[08/07/2025 11:22:48 INFO]: Val accuracy: {
    "score": -0.8143464687066863,
    "rmse": 0.8143464687066863
}
[08/07/2025 11:22:48 INFO]: Test accuracy: {
    "score": -0.7611277406143442,
    "rmse": 0.7611277406143442
}
[08/07/2025 11:22:48 INFO]: {
    "dataset": "ic_upstream3",
    "model_name": "ft_transformer",
    "run_id": "renowned-Tressie",
    "best_epoch": 67,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.7611277406143442,
        "rmse": 0.7611277406143442
    },
    "train_stats": {
        "score": -0.7872619629964532,
        "rmse": 0.7872619629964532
    },
    "val_stats": {
        "score": -0.8143464687066863,
        "rmse": 0.8143464687066863
    }
}
[08/07/2025 11:22:48 INFO]: 
_________________________________________________

[08/07/2025 11:22:48 INFO]: train_net_for_optune.py main() running.
[08/07/2025 11:22:48 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 4
  d_ffn_factor: 1.9127536581266473
  attention_dropout: 0.46145764785183835
  ffn_dropout: 0.46145764785183835
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0001517697721664735
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: renowned-Tressie

[08/07/2025 11:22:48 INFO]: This ft_transformer has 0.285 million parameters.
[08/07/2025 11:22:48 INFO]: Training will start at epoch 0.
[08/07/2025 11:22:48 INFO]: ==> Starting training for 200 epochs...
[08/07/2025 11:22:53 INFO]: Training loss at epoch 0: 0.8561201989650726
[08/07/2025 11:22:53 INFO]: New best epoch, val score: -0.9480051672290201
[08/07/2025 11:22:53 INFO]: Saving model to: model_best.pth
[08/07/2025 11:22:57 INFO]: Training loss at epoch 1: 1.081352949142456
[08/07/2025 11:22:58 INFO]: New best epoch, val score: -0.933226860221652
[08/07/2025 11:22:58 INFO]: Saving model to: model_best.pth
[08/07/2025 11:23:02 INFO]: Training loss at epoch 2: 1.0741620659828186
[08/07/2025 11:23:08 INFO]: Training loss at epoch 3: 1.1652819514274597
[08/07/2025 11:23:13 INFO]: Training loss at epoch 4: 1.0307943522930145
[08/07/2025 11:23:17 INFO]: Training loss at epoch 5: 1.0233445763587952
[08/07/2025 11:23:22 INFO]: Training loss at epoch 6: 1.1098960638046265
[08/07/2025 11:23:27 INFO]: Training loss at epoch 7: 1.0750685930252075
[08/07/2025 11:23:31 INFO]: Training loss at epoch 8: 1.072137713432312
[08/07/2025 11:23:36 INFO]: Training loss at epoch 9: 1.1549817323684692
[08/07/2025 11:23:38 INFO]: Training stats: {
    "score": -1.0217856894291162,
    "rmse": 1.0217856894291162
}
[08/07/2025 11:23:38 INFO]: Val stats: {
    "score": -1.0271551698131698,
    "rmse": 1.0271551698131698
}
[08/07/2025 11:23:38 INFO]: Test stats: {
    "score": -0.979254385153968,
    "rmse": 0.979254385153968
}
[08/07/2025 11:23:43 INFO]: Training loss at epoch 10: 0.9992417693138123
[08/07/2025 11:23:47 INFO]: Training loss at epoch 11: 0.9691955149173737
[08/07/2025 11:23:52 INFO]: Training loss at epoch 12: 1.1005945205688477
[08/07/2025 11:23:57 INFO]: Training loss at epoch 13: 1.0771363973617554
[08/07/2025 11:24:02 INFO]: Training loss at epoch 14: 0.9207766056060791
[08/07/2025 11:24:06 INFO]: Training loss at epoch 15: 1.0758945941925049
[08/07/2025 11:24:11 INFO]: Training loss at epoch 16: 0.8789529502391815
[08/07/2025 11:24:16 INFO]: Training loss at epoch 17: 0.8313131034374237
[08/07/2025 11:24:20 INFO]: Training loss at epoch 18: 1.00404691696167
[08/07/2025 11:24:25 INFO]: Training loss at epoch 19: 1.1007584929466248
[08/07/2025 11:24:27 INFO]: Training stats: {
    "score": -0.9897361957706673,
    "rmse": 0.9897361957706673
}
[08/07/2025 11:24:27 INFO]: Val stats: {
    "score": -0.9345073444338707,
    "rmse": 0.9345073444338707
}
[08/07/2025 11:24:27 INFO]: Test stats: {
    "score": -0.9113847991563795,
    "rmse": 0.9113847991563795
}
[08/07/2025 11:24:31 INFO]: Training loss at epoch 20: 1.0657796561717987
[08/07/2025 11:24:36 INFO]: Training loss at epoch 21: 0.9054087102413177
[08/07/2025 11:24:41 INFO]: Training loss at epoch 22: 0.8868494927883148
[08/07/2025 11:24:45 INFO]: Training loss at epoch 23: 1.0209366083145142
[08/07/2025 11:24:50 INFO]: Training loss at epoch 24: 0.9401902556419373
[08/07/2025 11:24:55 INFO]: Training loss at epoch 25: 0.9744154214859009
[08/07/2025 11:25:00 INFO]: Training loss at epoch 26: 0.8794899582862854
[08/07/2025 11:25:04 INFO]: Training loss at epoch 27: 0.8661235868930817
[08/07/2025 11:25:09 INFO]: Training loss at epoch 28: 1.1484233736991882
[08/07/2025 11:25:14 INFO]: Training loss at epoch 29: 0.8855551183223724
[08/07/2025 11:25:15 INFO]: Training stats: {
    "score": -0.9846805827823604,
    "rmse": 0.9846805827823604
}
[08/07/2025 11:25:15 INFO]: Val stats: {
    "score": -0.961147311863538,
    "rmse": 0.961147311863538
}
[08/07/2025 11:25:15 INFO]: Test stats: {
    "score": -0.9228403763053232,
    "rmse": 0.9228403763053232
}
[08/07/2025 11:25:20 INFO]: Training loss at epoch 30: 0.8283172249794006
[08/07/2025 11:25:25 INFO]: Training loss at epoch 31: 0.9286341071128845
[08/07/2025 11:25:30 INFO]: Training loss at epoch 32: 1.0735878348350525
[08/07/2025 11:25:30 INFO]: Running Final Evaluation...
[08/07/2025 11:25:32 INFO]: Training accuracy: {
    "score": -1.0204678426127505,
    "rmse": 1.0204678426127505
}
[08/07/2025 11:25:32 INFO]: Val accuracy: {
    "score": -0.933226860221652,
    "rmse": 0.933226860221652
}
[08/07/2025 11:25:32 INFO]: Test accuracy: {
    "score": -0.9258353240758299,
    "rmse": 0.9258353240758299
}
[08/07/2025 11:25:32 INFO]: {
    "dataset": "ic_upstream3",
    "model_name": "ft_transformer",
    "run_id": "renowned-Tressie",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9258353240758299,
        "rmse": 0.9258353240758299
    },
    "train_stats": {
        "score": -1.0204678426127505,
        "rmse": 1.0204678426127505
    },
    "val_stats": {
        "score": -0.933226860221652,
        "rmse": 0.933226860221652
    }
}
[08/07/2025 11:25:32 INFO]: 
_________________________________________________

[08/07/2025 11:25:32 INFO]: train_net_for_optune.py main() running.
[08/07/2025 11:25:32 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 0.679027343323812
  attention_dropout: 0.06279698444523124
  ffn_dropout: 0.06279698444523124
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0001288817855195597
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: renowned-Tressie

[08/07/2025 11:25:32 INFO]: This ft_transformer has 0.381 million parameters.
[08/07/2025 11:25:32 INFO]: Training will start at epoch 0.
[08/07/2025 11:25:32 INFO]: ==> Starting training for 200 epochs...
[08/07/2025 11:25:45 INFO]: Training loss at epoch 0: 1.2444425225257874
[08/07/2025 11:25:46 INFO]: New best epoch, val score: -1.1710149490010167
[08/07/2025 11:25:46 INFO]: Saving model to: model_best.pth
[08/07/2025 11:25:57 INFO]: Training loss at epoch 1: 1.1780670285224915
[08/07/2025 11:25:58 INFO]: New best epoch, val score: -0.9384229443066547
[08/07/2025 11:25:58 INFO]: Saving model to: model_best.pth
[08/07/2025 11:26:08 INFO]: Training loss at epoch 2: 0.7837717235088348
[08/07/2025 11:26:20 INFO]: Training loss at epoch 3: 1.2440632581710815
[08/07/2025 11:26:31 INFO]: Training loss at epoch 4: 0.9802708327770233
[08/07/2025 11:26:32 INFO]: New best epoch, val score: -0.9208518728783159
[08/07/2025 11:26:32 INFO]: Saving model to: model_best.pth
[08/07/2025 11:26:43 INFO]: Training loss at epoch 5: 0.8940273523330688
[08/07/2025 11:26:53 INFO]: Training loss at epoch 6: 1.0453884601593018
[08/07/2025 11:27:04 INFO]: Training loss at epoch 7: 1.0929015278816223
[08/07/2025 11:27:14 INFO]: Training loss at epoch 8: 1.031453937292099
[08/07/2025 11:27:25 INFO]: Training loss at epoch 9: 0.8693225085735321
[08/07/2025 11:27:29 INFO]: Training stats: {
    "score": -0.9848220980211482,
    "rmse": 0.9848220980211482
}
[08/07/2025 11:27:29 INFO]: Val stats: {
    "score": -0.9243563916618046,
    "rmse": 0.9243563916618046
}
[08/07/2025 11:27:29 INFO]: Test stats: {
    "score": -0.9017069909584198,
    "rmse": 0.9017069909584198
}
[08/07/2025 11:27:39 INFO]: Training loss at epoch 10: 0.8765428066253662
[08/07/2025 11:27:50 INFO]: Training loss at epoch 11: 1.0441624224185944
[08/07/2025 11:28:00 INFO]: Training loss at epoch 12: 0.7897873222827911
[08/07/2025 11:28:11 INFO]: Training loss at epoch 13: 1.044625848531723
[08/07/2025 11:28:22 INFO]: Training loss at epoch 14: 0.9382934272289276
[08/07/2025 11:28:32 INFO]: Training loss at epoch 15: 0.9451230764389038
[08/07/2025 11:28:43 INFO]: Training loss at epoch 16: 0.7513463795185089
[08/07/2025 11:28:54 INFO]: Training loss at epoch 17: 1.028786838054657
[08/07/2025 11:29:04 INFO]: Training loss at epoch 18: 0.8569721281528473
[08/07/2025 11:29:15 INFO]: Training loss at epoch 19: 1.1387476027011871
[08/07/2025 11:29:18 INFO]: Training stats: {
    "score": -0.9660062397856904,
    "rmse": 0.9660062397856904
}
[08/07/2025 11:29:18 INFO]: Val stats: {
    "score": -0.917392051829941,
    "rmse": 0.917392051829941
}
[08/07/2025 11:29:18 INFO]: Test stats: {
    "score": -0.8897117625868056,
    "rmse": 0.8897117625868056
}
[08/07/2025 11:29:20 INFO]: New best epoch, val score: -0.917392051829941
[08/07/2025 11:29:20 INFO]: Saving model to: model_best.pth
[08/07/2025 11:29:29 INFO]: Training loss at epoch 20: 1.012631356716156
[08/07/2025 11:29:30 INFO]: New best epoch, val score: -0.9153632379951753
[08/07/2025 11:29:30 INFO]: Saving model to: model_best.pth
[08/07/2025 11:29:40 INFO]: Training loss at epoch 21: 0.9630889594554901
[08/07/2025 11:29:50 INFO]: Training loss at epoch 22: 1.0286817252635956
[08/07/2025 11:30:01 INFO]: Training loss at epoch 23: 0.7786829769611359
[08/07/2025 11:30:11 INFO]: Training loss at epoch 24: 0.982359766960144
[08/07/2025 11:30:22 INFO]: Training loss at epoch 25: 0.8301730751991272
[08/07/2025 11:30:33 INFO]: Training loss at epoch 26: 0.9354932308197021
[08/07/2025 11:30:43 INFO]: Training loss at epoch 27: 0.9005594551563263
[08/07/2025 11:30:54 INFO]: Training loss at epoch 28: 0.841133177280426
[08/07/2025 11:31:04 INFO]: Training loss at epoch 29: 1.0108370780944824
[08/07/2025 11:31:08 INFO]: Training stats: {
    "score": -0.9379010710903095,
    "rmse": 0.9379010710903095
}
[08/07/2025 11:31:08 INFO]: Val stats: {
    "score": -0.9159059247669157,
    "rmse": 0.9159059247669157
}
[08/07/2025 11:31:08 INFO]: Test stats: {
    "score": -0.8749493969127229,
    "rmse": 0.8749493969127229
}
[08/07/2025 11:31:19 INFO]: Training loss at epoch 30: 0.8755058646202087
[08/07/2025 11:31:20 INFO]: New best epoch, val score: -0.9044573559639697
[08/07/2025 11:31:20 INFO]: Saving model to: model_best.pth
[08/07/2025 11:31:29 INFO]: Training loss at epoch 31: 0.9383165836334229
[08/07/2025 11:31:31 INFO]: New best epoch, val score: -0.9001775351328634
[08/07/2025 11:31:31 INFO]: Saving model to: model_best.pth
[08/07/2025 11:31:40 INFO]: Training loss at epoch 32: 0.8926101326942444
[08/07/2025 11:31:41 INFO]: New best epoch, val score: -0.8982210596639023
[08/07/2025 11:31:41 INFO]: Saving model to: model_best.pth
[08/07/2025 11:31:51 INFO]: Training loss at epoch 33: 1.0252177119255066
[08/07/2025 11:31:52 INFO]: New best epoch, val score: -0.8965107674382665
[08/07/2025 11:31:52 INFO]: Saving model to: model_best.pth
[08/07/2025 11:32:02 INFO]: Training loss at epoch 34: 0.6670611351728439
[08/07/2025 11:32:03 INFO]: New best epoch, val score: -0.8927433955622299
[08/07/2025 11:32:03 INFO]: Saving model to: model_best.pth
[08/07/2025 11:32:12 INFO]: Training loss at epoch 35: 0.7498852610588074
[08/07/2025 11:32:14 INFO]: New best epoch, val score: -0.8864765812403343
[08/07/2025 11:32:14 INFO]: Saving model to: model_best.pth
[08/07/2025 11:32:23 INFO]: Training loss at epoch 36: 0.8470041453838348
[08/07/2025 11:32:25 INFO]: New best epoch, val score: -0.879334076907922
[08/07/2025 11:32:25 INFO]: Saving model to: model_best.pth
[08/07/2025 11:32:34 INFO]: Training loss at epoch 37: 0.8962612748146057
[08/07/2025 11:32:35 INFO]: New best epoch, val score: -0.8737999346268719
[08/07/2025 11:32:35 INFO]: Saving model to: model_best.pth
[08/07/2025 11:32:45 INFO]: Training loss at epoch 38: 0.757338672876358
[08/07/2025 11:32:46 INFO]: New best epoch, val score: -0.8725465796741197
[08/07/2025 11:32:46 INFO]: Saving model to: model_best.pth
[08/07/2025 11:32:56 INFO]: Training loss at epoch 39: 0.8608891069889069
[08/07/2025 11:32:59 INFO]: Training stats: {
    "score": -0.8885409871655183,
    "rmse": 0.8885409871655183
}
[08/07/2025 11:32:59 INFO]: Val stats: {
    "score": -0.8779136401516812,
    "rmse": 0.8779136401516812
}
[08/07/2025 11:32:59 INFO]: Test stats: {
    "score": -0.8313884704262594,
    "rmse": 0.8313884704262594
}
[08/07/2025 11:33:10 INFO]: Training loss at epoch 40: 0.7976996004581451
[08/07/2025 11:33:21 INFO]: Training loss at epoch 41: 0.761782705783844
[08/07/2025 11:33:31 INFO]: Training loss at epoch 42: 0.7840829789638519
[08/07/2025 11:33:42 INFO]: Training loss at epoch 43: 0.7294991910457611
[08/07/2025 11:33:52 INFO]: Training loss at epoch 44: 0.8888980150222778
[08/07/2025 11:34:03 INFO]: Training loss at epoch 45: 0.7370937466621399
[08/07/2025 11:34:04 INFO]: New best epoch, val score: -0.8566170702785498
[08/07/2025 11:34:04 INFO]: Saving model to: model_best.pth
[08/07/2025 11:34:14 INFO]: Training loss at epoch 46: 0.771715521812439
[08/07/2025 11:34:15 INFO]: New best epoch, val score: -0.843437481807054
[08/07/2025 11:34:15 INFO]: Saving model to: model_best.pth
[08/07/2025 11:34:25 INFO]: Training loss at epoch 47: 0.7198365330696106
[08/07/2025 11:34:26 INFO]: New best epoch, val score: -0.832631044546269
[08/07/2025 11:34:26 INFO]: Saving model to: model_best.pth
[08/07/2025 11:34:35 INFO]: Training loss at epoch 48: 0.774773508310318
[08/07/2025 11:34:37 INFO]: New best epoch, val score: -0.8258521157752277
[08/07/2025 11:34:37 INFO]: Saving model to: model_best.pth
[08/07/2025 11:34:46 INFO]: Training loss at epoch 49: 0.6404458582401276
[08/07/2025 11:34:50 INFO]: Training stats: {
    "score": -0.8151935445563936,
    "rmse": 0.8151935445563936
}
[08/07/2025 11:34:50 INFO]: Val stats: {
    "score": -0.82710939248283,
    "rmse": 0.82710939248283
}
[08/07/2025 11:34:50 INFO]: Test stats: {
    "score": -0.7749753698904032,
    "rmse": 0.7749753698904032
}
[08/07/2025 11:35:00 INFO]: Training loss at epoch 50: 0.6022967547178268
[08/07/2025 11:35:11 INFO]: Training loss at epoch 51: 0.6694963872432709
[08/07/2025 11:35:21 INFO]: Training loss at epoch 52: 0.6091360747814178
[08/07/2025 11:35:32 INFO]: Training loss at epoch 53: 0.6482346653938293
[08/07/2025 11:35:33 INFO]: New best epoch, val score: -0.8174529343801928
[08/07/2025 11:35:33 INFO]: Saving model to: model_best.pth
[08/07/2025 11:35:43 INFO]: Training loss at epoch 54: 0.7048565149307251
[08/07/2025 11:35:44 INFO]: New best epoch, val score: -0.8070565274391774
[08/07/2025 11:35:44 INFO]: Saving model to: model_best.pth
[08/07/2025 11:35:53 INFO]: Training loss at epoch 55: 0.7978632152080536
[08/07/2025 11:36:04 INFO]: Training loss at epoch 56: 0.6662527322769165
[08/07/2025 11:36:15 INFO]: Training loss at epoch 57: 0.6290500164031982
[08/07/2025 11:36:25 INFO]: Training loss at epoch 58: 0.6199113428592682
[08/07/2025 11:36:36 INFO]: Training loss at epoch 59: 0.6334251463413239
[08/07/2025 11:36:40 INFO]: Training stats: {
    "score": -0.7570721448603617,
    "rmse": 0.7570721448603617
}
[08/07/2025 11:36:40 INFO]: Val stats: {
    "score": -0.8231255359776206,
    "rmse": 0.8231255359776206
}
[08/07/2025 11:36:40 INFO]: Test stats: {
    "score": -0.7648962892608635,
    "rmse": 0.7648962892608635
}
[08/07/2025 11:36:50 INFO]: Training loss at epoch 60: 0.4978708028793335
[08/07/2025 11:37:01 INFO]: Training loss at epoch 61: 0.49360424280166626
[08/07/2025 11:37:11 INFO]: Training loss at epoch 62: 0.558575302362442
[08/07/2025 11:37:22 INFO]: Training loss at epoch 63: 0.5323364585638046
[08/07/2025 11:37:33 INFO]: Training loss at epoch 64: 0.6612194776535034
[08/07/2025 11:37:43 INFO]: Training loss at epoch 65: 0.5738261044025421
[08/07/2025 11:37:54 INFO]: Training loss at epoch 66: 0.5904125869274139
[08/07/2025 11:38:05 INFO]: Training loss at epoch 67: 0.6520386338233948
[08/07/2025 11:38:15 INFO]: Training loss at epoch 68: 0.6311107873916626
[08/07/2025 11:38:26 INFO]: Training loss at epoch 69: 0.5058692395687103
[08/07/2025 11:38:30 INFO]: Training stats: {
    "score": -0.7277073061781355,
    "rmse": 0.7277073061781355
}
[08/07/2025 11:38:30 INFO]: Val stats: {
    "score": -0.8308556134883127,
    "rmse": 0.8308556134883127
}
[08/07/2025 11:38:30 INFO]: Test stats: {
    "score": -0.7558609552988836,
    "rmse": 0.7558609552988836
}
[08/07/2025 11:38:40 INFO]: Training loss at epoch 70: 0.5499767661094666
[08/07/2025 11:38:51 INFO]: Training loss at epoch 71: 0.5184318274259567
[08/07/2025 11:39:02 INFO]: Training loss at epoch 72: 0.48404036462306976
[08/07/2025 11:39:13 INFO]: Training loss at epoch 73: 0.49509304761886597
[08/07/2025 11:39:14 INFO]: New best epoch, val score: -0.8011300702445247
[08/07/2025 11:39:14 INFO]: Saving model to: model_best.pth
[08/07/2025 11:39:23 INFO]: Training loss at epoch 74: 0.5954633355140686
[08/07/2025 11:39:25 INFO]: New best epoch, val score: -0.7997546438326445
[08/07/2025 11:39:25 INFO]: Saving model to: model_best.pth
[08/07/2025 11:39:34 INFO]: Training loss at epoch 75: 0.5241059511899948
[08/07/2025 11:39:45 INFO]: Training loss at epoch 76: 0.5413352847099304
[08/07/2025 11:39:56 INFO]: Training loss at epoch 77: 0.5823061168193817
[08/07/2025 11:40:06 INFO]: Training loss at epoch 78: 0.5494317710399628
[08/07/2025 11:40:17 INFO]: Training loss at epoch 79: 0.5170156061649323
[08/07/2025 11:40:21 INFO]: Training stats: {
    "score": -0.7098601005344826,
    "rmse": 0.7098601005344826
}
[08/07/2025 11:40:21 INFO]: Val stats: {
    "score": -0.8156839344691271,
    "rmse": 0.8156839344691271
}
[08/07/2025 11:40:21 INFO]: Test stats: {
    "score": -0.7553619745515399,
    "rmse": 0.7553619745515399
}
[08/07/2025 11:40:32 INFO]: Training loss at epoch 80: 0.49508820474147797
[08/07/2025 11:40:42 INFO]: Training loss at epoch 81: 0.49611103534698486
[08/07/2025 11:40:53 INFO]: Training loss at epoch 82: 0.5175570696592331
[08/07/2025 11:41:03 INFO]: Training loss at epoch 83: 0.647329181432724
[08/07/2025 11:41:14 INFO]: Training loss at epoch 84: 0.3868100494146347
[08/07/2025 11:41:25 INFO]: Training loss at epoch 85: 0.5090804994106293
[08/07/2025 11:41:36 INFO]: Training loss at epoch 86: 0.5083410888910294
[08/07/2025 11:41:47 INFO]: Training loss at epoch 87: 0.4343642443418503
[08/07/2025 11:41:48 INFO]: New best epoch, val score: -0.7918385151388321
[08/07/2025 11:41:48 INFO]: Saving model to: model_best.pth
[08/07/2025 11:41:58 INFO]: Training loss at epoch 88: 0.43480947613716125
[08/07/2025 11:42:09 INFO]: Training loss at epoch 89: 0.5070388615131378
[08/07/2025 11:42:12 INFO]: Training stats: {
    "score": -0.6850899621358663,
    "rmse": 0.6850899621358663
}
[08/07/2025 11:42:12 INFO]: Val stats: {
    "score": -0.8124804479935808,
    "rmse": 0.8124804479935808
}
[08/07/2025 11:42:12 INFO]: Test stats: {
    "score": -0.7430751809325573,
    "rmse": 0.7430751809325573
}
[08/07/2025 11:42:23 INFO]: Training loss at epoch 90: 0.46118828654289246
[08/07/2025 11:42:34 INFO]: Training loss at epoch 91: 0.4611973464488983
[08/07/2025 11:42:35 INFO]: New best epoch, val score: -0.7817762449430506
[08/07/2025 11:42:35 INFO]: Saving model to: model_best.pth
[08/07/2025 11:42:45 INFO]: Training loss at epoch 92: 0.489591509103775
[08/07/2025 11:42:46 INFO]: New best epoch, val score: -0.7726036913262928
[08/07/2025 11:42:46 INFO]: Saving model to: model_best.pth
[08/07/2025 11:42:56 INFO]: Training loss at epoch 93: 0.7044191658496857
[08/07/2025 11:43:06 INFO]: Training loss at epoch 94: 0.5296532809734344
[08/07/2025 11:43:17 INFO]: Training loss at epoch 95: 0.4410773068666458
[08/07/2025 11:43:27 INFO]: Training loss at epoch 96: 0.6055195033550262
[08/07/2025 11:43:38 INFO]: Training loss at epoch 97: 0.5816160589456558
[08/07/2025 11:43:49 INFO]: Training loss at epoch 98: 0.469281867146492
[08/07/2025 11:43:59 INFO]: Training loss at epoch 99: 0.46113650500774384
[08/07/2025 11:44:03 INFO]: Training stats: {
    "score": -0.7021856751162676,
    "rmse": 0.7021856751162676
}
[08/07/2025 11:44:03 INFO]: Val stats: {
    "score": -0.7715811025500826,
    "rmse": 0.7715811025500826
}
[08/07/2025 11:44:03 INFO]: Test stats: {
    "score": -0.7358457471706995,
    "rmse": 0.7358457471706995
}
[08/07/2025 11:44:04 INFO]: New best epoch, val score: -0.7715811025500826
[08/07/2025 11:44:04 INFO]: Saving model to: model_best.pth
[08/07/2025 11:44:13 INFO]: Training loss at epoch 100: 0.4759827256202698
[08/07/2025 11:44:24 INFO]: Training loss at epoch 101: 0.5340297073125839
[08/07/2025 11:44:35 INFO]: Training loss at epoch 102: 0.5620425194501877
[08/07/2025 11:44:46 INFO]: Training loss at epoch 103: 0.531829759478569
[08/07/2025 11:44:57 INFO]: Training loss at epoch 104: 0.5306403338909149
[08/07/2025 11:45:08 INFO]: Training loss at epoch 105: 0.48061950504779816
[08/07/2025 11:45:18 INFO]: Training loss at epoch 106: 0.5133907347917557
[08/07/2025 11:45:20 INFO]: New best epoch, val score: -0.7698815826840526
[08/07/2025 11:45:20 INFO]: Saving model to: model_best.pth
[08/07/2025 11:45:29 INFO]: Training loss at epoch 107: 0.4295361638069153
[08/07/2025 11:45:40 INFO]: Training loss at epoch 108: 0.49533382058143616
[08/07/2025 11:45:51 INFO]: Training loss at epoch 109: 0.5393578857183456
[08/07/2025 11:45:54 INFO]: Training stats: {
    "score": -0.6544430787972659,
    "rmse": 0.6544430787972659
}
[08/07/2025 11:45:54 INFO]: Val stats: {
    "score": -0.7963573027428612,
    "rmse": 0.7963573027428612
}
[08/07/2025 11:45:54 INFO]: Test stats: {
    "score": -0.7127826731913024,
    "rmse": 0.7127826731913024
}
[08/07/2025 11:46:05 INFO]: Training loss at epoch 110: 0.3816441148519516
[08/07/2025 11:46:16 INFO]: Training loss at epoch 111: 0.44516895711421967
[08/07/2025 11:46:26 INFO]: Training loss at epoch 112: 0.5293201059103012
[08/07/2025 11:46:37 INFO]: Training loss at epoch 113: 0.43976691365242004
[08/07/2025 11:46:47 INFO]: Training loss at epoch 114: 0.38471055030822754
[08/07/2025 11:46:58 INFO]: Training loss at epoch 115: 0.42808006703853607
[08/07/2025 11:47:09 INFO]: Training loss at epoch 116: 0.5730026364326477
[08/07/2025 11:47:19 INFO]: Training loss at epoch 117: 0.3795787990093231
[08/07/2025 11:47:30 INFO]: Training loss at epoch 118: 0.3968733251094818
[08/07/2025 11:47:40 INFO]: Training loss at epoch 119: 0.4536302834749222
[08/07/2025 11:47:44 INFO]: Training stats: {
    "score": -0.6501683773556236,
    "rmse": 0.6501683773556236
}
[08/07/2025 11:47:44 INFO]: Val stats: {
    "score": -0.7814817459314214,
    "rmse": 0.7814817459314214
}
[08/07/2025 11:47:44 INFO]: Test stats: {
    "score": -0.7212077795048812,
    "rmse": 0.7212077795048812
}
[08/07/2025 11:47:55 INFO]: Training loss at epoch 120: 0.38028550148010254
[08/07/2025 11:48:05 INFO]: Training loss at epoch 121: 0.41960301995277405
[08/07/2025 11:48:16 INFO]: Training loss at epoch 122: 0.5081110298633575
[08/07/2025 11:48:27 INFO]: Training loss at epoch 123: 0.4933238625526428
[08/07/2025 11:48:37 INFO]: Training loss at epoch 124: 0.3986862897872925
[08/07/2025 11:48:48 INFO]: Training loss at epoch 125: 0.5391636192798615
[08/07/2025 11:48:49 INFO]: New best epoch, val score: -0.7577139838401132
[08/07/2025 11:48:49 INFO]: Saving model to: model_best.pth
[08/07/2025 11:48:59 INFO]: Training loss at epoch 126: 0.36014342308044434
[08/07/2025 11:49:00 INFO]: New best epoch, val score: -0.755622304139906
[08/07/2025 11:49:00 INFO]: Saving model to: model_best.pth
[08/07/2025 11:49:10 INFO]: Training loss at epoch 127: 0.4559105634689331
[08/07/2025 11:49:20 INFO]: Training loss at epoch 128: 0.39153146743774414
[08/07/2025 11:49:31 INFO]: Training loss at epoch 129: 0.41135428845882416
[08/07/2025 11:49:35 INFO]: Training stats: {
    "score": -0.6351183456125792,
    "rmse": 0.6351183456125792
}
[08/07/2025 11:49:35 INFO]: Val stats: {
    "score": -0.7975660319628797,
    "rmse": 0.7975660319628797
}
[08/07/2025 11:49:35 INFO]: Test stats: {
    "score": -0.7063985729320976,
    "rmse": 0.7063985729320976
}
[08/07/2025 11:49:45 INFO]: Training loss at epoch 130: 0.5446018427610397
[08/07/2025 11:49:56 INFO]: Training loss at epoch 131: 0.35831065475940704
[08/07/2025 11:50:07 INFO]: Training loss at epoch 132: 0.5113223940134048
[08/07/2025 11:50:17 INFO]: Training loss at epoch 133: 0.43551309406757355
[08/07/2025 11:50:28 INFO]: Training loss at epoch 134: 0.4234222173690796
[08/07/2025 11:50:39 INFO]: Training loss at epoch 135: 0.46266932785511017
[08/07/2025 11:50:51 INFO]: Training loss at epoch 136: 0.42204830050468445
[08/07/2025 11:51:03 INFO]: Training loss at epoch 137: 0.44150038063526154
[08/07/2025 11:51:14 INFO]: Training loss at epoch 138: 0.3769160509109497
[08/07/2025 11:51:26 INFO]: Training loss at epoch 139: 0.4575076550245285
[08/07/2025 11:51:30 INFO]: Training stats: {
    "score": -0.6231344260013874,
    "rmse": 0.6231344260013874
}
[08/07/2025 11:51:30 INFO]: Val stats: {
    "score": -0.7784126281549617,
    "rmse": 0.7784126281549617
}
[08/07/2025 11:51:30 INFO]: Test stats: {
    "score": -0.712489760269102,
    "rmse": 0.712489760269102
}
[08/07/2025 11:51:41 INFO]: Training loss at epoch 140: 0.5320844501256943
[08/07/2025 11:51:53 INFO]: Training loss at epoch 141: 0.43537481129169464
[08/07/2025 11:52:04 INFO]: Training loss at epoch 142: 0.419973760843277
[08/07/2025 11:52:05 INFO]: New best epoch, val score: -0.7507100367437574
[08/07/2025 11:52:05 INFO]: Saving model to: model_best.pth
[08/07/2025 11:52:16 INFO]: Training loss at epoch 143: 0.42667074501514435
[08/07/2025 11:52:28 INFO]: Training loss at epoch 144: 0.4304313659667969
[08/07/2025 11:52:39 INFO]: Training loss at epoch 145: 0.41649091243743896
[08/07/2025 11:52:51 INFO]: Training loss at epoch 146: 0.3527035564184189
[08/07/2025 11:53:02 INFO]: Training loss at epoch 147: 0.40781252086162567
[08/07/2025 11:53:03 INFO]: New best epoch, val score: -0.7419568560489608
[08/07/2025 11:53:03 INFO]: Saving model to: model_best.pth
[08/07/2025 11:53:12 INFO]: Training loss at epoch 148: 0.34465591609477997
[08/07/2025 11:53:14 INFO]: New best epoch, val score: -0.741348374231431
[08/07/2025 11:53:14 INFO]: Saving model to: model_best.pth
[08/07/2025 11:53:24 INFO]: Training loss at epoch 149: 0.36776506900787354
[08/07/2025 11:53:27 INFO]: Training stats: {
    "score": -0.6123496034657147,
    "rmse": 0.6123496034657147
}
[08/07/2025 11:53:27 INFO]: Val stats: {
    "score": -0.7453556374048859,
    "rmse": 0.7453556374048859
}
[08/07/2025 11:53:27 INFO]: Test stats: {
    "score": -0.6925697828497308,
    "rmse": 0.6925697828497308
}
[08/07/2025 11:53:38 INFO]: Training loss at epoch 150: 0.36086583137512207
[08/07/2025 11:53:49 INFO]: Training loss at epoch 151: 0.368838906288147
[08/07/2025 11:54:00 INFO]: Training loss at epoch 152: 0.39895741641521454
[08/07/2025 11:54:11 INFO]: Training loss at epoch 153: 0.42100974917411804
[08/07/2025 11:54:21 INFO]: Training loss at epoch 154: 0.3842732012271881
[08/07/2025 11:54:32 INFO]: Training loss at epoch 155: 0.4264837056398392
[08/07/2025 11:54:42 INFO]: Training loss at epoch 156: 0.4257154017686844
[08/07/2025 11:54:53 INFO]: Training loss at epoch 157: 0.3873358964920044
[08/07/2025 11:55:04 INFO]: Training loss at epoch 158: 0.4423723667860031
[08/07/2025 11:55:15 INFO]: Training loss at epoch 159: 0.4163271188735962
[08/07/2025 11:55:18 INFO]: Training stats: {
    "score": -0.6082429655269934,
    "rmse": 0.6082429655269934
}
[08/07/2025 11:55:18 INFO]: Val stats: {
    "score": -0.740668563415264,
    "rmse": 0.740668563415264
}
[08/07/2025 11:55:18 INFO]: Test stats: {
    "score": -0.7132797316357484,
    "rmse": 0.7132797316357484
}
[08/07/2025 11:55:20 INFO]: New best epoch, val score: -0.740668563415264
[08/07/2025 11:55:20 INFO]: Saving model to: model_best.pth
[08/07/2025 11:55:29 INFO]: Training loss at epoch 160: 0.3960370272397995
[08/07/2025 11:55:40 INFO]: Training loss at epoch 161: 0.40804214775562286
[08/07/2025 11:55:51 INFO]: Training loss at epoch 162: 0.36093296110630035
[08/07/2025 11:56:02 INFO]: Training loss at epoch 163: 0.35487572848796844
[08/07/2025 11:56:12 INFO]: Training loss at epoch 164: 0.40644821524620056
[08/07/2025 11:56:23 INFO]: Training loss at epoch 165: 0.3966429829597473
[08/07/2025 11:56:34 INFO]: Training loss at epoch 166: 0.4075150489807129
[08/07/2025 11:56:35 INFO]: New best epoch, val score: -0.7336409929853428
[08/07/2025 11:56:35 INFO]: Saving model to: model_best.pth
[08/07/2025 11:56:44 INFO]: Training loss at epoch 167: 0.4130294919013977
[08/07/2025 11:56:55 INFO]: Training loss at epoch 168: 0.3409331291913986
[08/07/2025 11:57:06 INFO]: Training loss at epoch 169: 0.408132866024971
[08/07/2025 11:57:12 INFO]: Training stats: {
    "score": -0.585662505063125,
    "rmse": 0.585662505063125
}
[08/07/2025 11:57:12 INFO]: Val stats: {
    "score": -0.7492892569249192,
    "rmse": 0.7492892569249192
}
[08/07/2025 11:57:12 INFO]: Test stats: {
    "score": -0.6881920196624846,
    "rmse": 0.6881920196624846
}
[08/07/2025 11:57:22 INFO]: Training loss at epoch 170: 0.45317259430885315
[08/07/2025 11:57:24 INFO]: New best epoch, val score: -0.722013265603896
[08/07/2025 11:57:24 INFO]: Saving model to: model_best.pth
[08/07/2025 11:57:33 INFO]: Training loss at epoch 171: 0.394785076379776
[08/07/2025 11:57:34 INFO]: New best epoch, val score: -0.7212593225482318
[08/07/2025 11:57:34 INFO]: Saving model to: model_best.pth
[08/07/2025 11:57:44 INFO]: Training loss at epoch 172: 0.31028202176094055
[08/07/2025 11:57:54 INFO]: Training loss at epoch 173: 0.35370928049087524
[08/07/2025 11:58:05 INFO]: Training loss at epoch 174: 0.3222063332796097
[08/07/2025 11:58:16 INFO]: Training loss at epoch 175: 0.3992150276899338
[08/07/2025 11:58:26 INFO]: Training loss at epoch 176: 0.328085720539093
[08/07/2025 11:58:37 INFO]: Training loss at epoch 177: 0.2992003783583641
[08/07/2025 11:58:48 INFO]: Training loss at epoch 178: 0.3556130826473236
[08/07/2025 11:58:59 INFO]: Training loss at epoch 179: 0.47878308594226837
[08/07/2025 11:59:02 INFO]: Training stats: {
    "score": -0.5789661117239501,
    "rmse": 0.5789661117239501
}
[08/07/2025 11:59:02 INFO]: Val stats: {
    "score": -0.752007036535635,
    "rmse": 0.752007036535635
}
[08/07/2025 11:59:02 INFO]: Test stats: {
    "score": -0.6879116649204203,
    "rmse": 0.6879116649204203
}
[08/07/2025 11:59:13 INFO]: Training loss at epoch 180: 0.3395601511001587
[08/07/2025 11:59:24 INFO]: Training loss at epoch 181: 0.35836081206798553
[08/07/2025 11:59:34 INFO]: Training loss at epoch 182: 0.35127510130405426
[08/07/2025 11:59:45 INFO]: Training loss at epoch 183: 0.3509042263031006
[08/07/2025 11:59:55 INFO]: Training loss at epoch 184: 0.3012863025069237
[08/07/2025 12:00:06 INFO]: Training loss at epoch 185: 0.4004736691713333
[08/07/2025 12:00:07 INFO]: New best epoch, val score: -0.7203324771401511
[08/07/2025 12:00:07 INFO]: Saving model to: model_best.pth
[08/07/2025 12:00:17 INFO]: Training loss at epoch 186: 0.2681228816509247
[08/07/2025 12:00:28 INFO]: Training loss at epoch 187: 0.32645179331302643
[08/07/2025 12:00:39 INFO]: Training loss at epoch 188: 0.38469691574573517
[08/07/2025 12:00:49 INFO]: Training loss at epoch 189: 0.32704250514507294
[08/07/2025 12:00:53 INFO]: Training stats: {
    "score": -0.5695548121679745,
    "rmse": 0.5695548121679745
}
[08/07/2025 12:00:53 INFO]: Val stats: {
    "score": -0.739718586952333,
    "rmse": 0.739718586952333
}
[08/07/2025 12:00:53 INFO]: Test stats: {
    "score": -0.6905224865999045,
    "rmse": 0.6905224865999045
}
[08/07/2025 12:01:04 INFO]: Training loss at epoch 190: 0.3730567544698715
[08/07/2025 12:01:14 INFO]: Training loss at epoch 191: 0.3563520908355713
[08/07/2025 12:01:16 INFO]: New best epoch, val score: -0.7200285120977808
[08/07/2025 12:01:16 INFO]: Saving model to: model_best.pth
[08/07/2025 12:01:25 INFO]: Training loss at epoch 192: 0.26683128625154495
[08/07/2025 12:01:36 INFO]: Training loss at epoch 193: 0.34174223244190216
[08/07/2025 12:01:46 INFO]: Training loss at epoch 194: 0.29177601635456085
[08/07/2025 12:01:57 INFO]: Training loss at epoch 195: 0.44851377606391907
[08/07/2025 12:02:07 INFO]: Training loss at epoch 196: 0.344239741563797
[08/07/2025 12:02:18 INFO]: Training loss at epoch 197: 0.3324686884880066
[08/07/2025 12:02:19 INFO]: New best epoch, val score: -0.719469189914669
[08/07/2025 12:02:19 INFO]: Saving model to: model_best.pth
[08/07/2025 12:02:29 INFO]: Training loss at epoch 198: 0.42689356207847595
[08/07/2025 12:02:40 INFO]: Training loss at epoch 199: 0.3606052100658417
[08/07/2025 12:02:43 INFO]: Training stats: {
    "score": -0.5623687155360131,
    "rmse": 0.5623687155360131
}
[08/07/2025 12:02:43 INFO]: Val stats: {
    "score": -0.7372625528455952,
    "rmse": 0.7372625528455952
}
[08/07/2025 12:02:43 INFO]: Test stats: {
    "score": -0.6886332975482817,
    "rmse": 0.6886332975482817
}
[08/07/2025 12:02:45 INFO]: Running Final Evaluation...
[08/07/2025 12:02:49 INFO]: Training accuracy: {
    "score": -0.5795479614259419,
    "rmse": 0.5795479614259419
}
[08/07/2025 12:02:49 INFO]: Val accuracy: {
    "score": -0.719469189914669,
    "rmse": 0.719469189914669
}
[08/07/2025 12:02:49 INFO]: Test accuracy: {
    "score": -0.7088797682746952,
    "rmse": 0.7088797682746952
}
[08/07/2025 12:02:49 INFO]: {
    "dataset": "ic_upstream3",
    "model_name": "ft_transformer",
    "run_id": "renowned-Tressie",
    "best_epoch": 197,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.7088797682746952,
        "rmse": 0.7088797682746952
    },
    "train_stats": {
        "score": -0.5795479614259419,
        "rmse": 0.5795479614259419
    },
    "val_stats": {
        "score": -0.719469189914669,
        "rmse": 0.719469189914669
    }
}
[08/07/2025 12:02:49 INFO]: 
_________________________________________________

[08/07/2025 12:02:49 INFO]: train_net_for_optune.py main() running.
[08/07/2025 12:02:49 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 0.8491066451748289
  attention_dropout: 0.2367922201560117
  ffn_dropout: 0.2367922201560117
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.6708692499542695e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: renowned-Tressie

[08/07/2025 12:02:49 INFO]: This ft_transformer has 0.970 million parameters.
[08/07/2025 12:02:49 INFO]: Training will start at epoch 0.
[08/07/2025 12:02:49 INFO]: ==> Starting training for 200 epochs...
[08/07/2025 12:03:16 INFO]: Training loss at epoch 0: 1.036686897277832
[08/07/2025 12:03:20 INFO]: New best epoch, val score: -0.9908123542716465
[08/07/2025 12:03:20 INFO]: Saving model to: model_best.pth
[08/07/2025 12:03:47 INFO]: Training loss at epoch 1: 1.0665801763534546
[08/07/2025 12:03:51 INFO]: New best epoch, val score: -0.928656839519046
[08/07/2025 12:03:51 INFO]: Saving model to: model_best.pth
[08/07/2025 12:04:18 INFO]: Training loss at epoch 2: 1.0848295092582703
[08/07/2025 12:04:49 INFO]: Training loss at epoch 3: 0.8604536354541779
[08/07/2025 12:05:20 INFO]: Training loss at epoch 4: 1.2301118969917297
[08/07/2025 12:05:51 INFO]: Training loss at epoch 5: 1.0838946998119354
[08/07/2025 12:06:21 INFO]: Training loss at epoch 6: 1.1526923179626465
[08/07/2025 12:06:52 INFO]: Training loss at epoch 7: 1.2549992203712463
[08/07/2025 12:07:23 INFO]: Training loss at epoch 8: 1.2003918290138245
[08/07/2025 12:07:54 INFO]: Training loss at epoch 9: 1.0687662959098816
[08/07/2025 12:08:04 INFO]: Training stats: {
    "score": -1.0964145252921114,
    "rmse": 1.0964145252921114
}
[08/07/2025 12:08:04 INFO]: Val stats: {
    "score": -1.1452924315148876,
    "rmse": 1.1452924315148876
}
[08/07/2025 12:08:04 INFO]: Test stats: {
    "score": -1.0798986805186808,
    "rmse": 1.0798986805186808
}
[08/07/2025 12:08:35 INFO]: Training loss at epoch 10: 0.9654370844364166
[08/07/2025 12:09:06 INFO]: Training loss at epoch 11: 1.0917828679084778
[08/07/2025 12:09:37 INFO]: Training loss at epoch 12: 1.1034785509109497
[08/07/2025 12:10:08 INFO]: Training loss at epoch 13: 1.0221518278121948
[08/07/2025 12:10:39 INFO]: Training loss at epoch 14: 0.9246467351913452
[08/07/2025 12:11:10 INFO]: Training loss at epoch 15: 1.1609225273132324
[08/07/2025 12:11:41 INFO]: Training loss at epoch 16: 1.0091346204280853
[08/07/2025 12:12:11 INFO]: Training loss at epoch 17: 0.997576504945755
[08/07/2025 12:12:42 INFO]: Training loss at epoch 18: 0.8878312706947327
[08/07/2025 12:13:13 INFO]: Training loss at epoch 19: 0.9116493463516235
[08/07/2025 12:13:24 INFO]: Training stats: {
    "score": -0.9934045773138518,
    "rmse": 0.9934045773138518
}
[08/07/2025 12:13:24 INFO]: Val stats: {
    "score": -0.9294417010936525,
    "rmse": 0.9294417010936525
}
[08/07/2025 12:13:24 INFO]: Test stats: {
    "score": -0.9100738019610736,
    "rmse": 0.9100738019610736
}
[08/07/2025 12:13:55 INFO]: Training loss at epoch 20: 1.0365397930145264
[08/07/2025 12:14:25 INFO]: Training loss at epoch 21: 0.9324105381965637
[08/07/2025 12:14:56 INFO]: Training loss at epoch 22: 1.0053834021091461
[08/07/2025 12:15:27 INFO]: Training loss at epoch 23: 1.0471882820129395
[08/07/2025 12:15:58 INFO]: Training loss at epoch 24: 0.8405960500240326
[08/07/2025 12:16:29 INFO]: Training loss at epoch 25: 0.9773681163787842
[08/07/2025 12:17:00 INFO]: Training loss at epoch 26: 0.933794230222702
[08/07/2025 12:17:30 INFO]: Training loss at epoch 27: 0.9220212996006012
[08/07/2025 12:18:01 INFO]: Training loss at epoch 28: 1.1298221051692963
[08/07/2025 12:18:32 INFO]: Training loss at epoch 29: 1.055798500776291
[08/07/2025 12:18:43 INFO]: Training stats: {
    "score": -0.9857809510561778,
    "rmse": 0.9857809510561778
}
[08/07/2025 12:18:43 INFO]: Val stats: {
    "score": -0.9662313043806441,
    "rmse": 0.9662313043806441
}
[08/07/2025 12:18:43 INFO]: Test stats: {
    "score": -0.9267489153390464,
    "rmse": 0.9267489153390464
}
[08/07/2025 12:19:13 INFO]: Training loss at epoch 30: 1.120985060930252
[08/07/2025 12:19:44 INFO]: Training loss at epoch 31: 0.9837453365325928
[08/07/2025 12:20:15 INFO]: Training loss at epoch 32: 1.0584242939949036
[08/07/2025 12:20:19 INFO]: Running Final Evaluation...
[08/07/2025 12:20:31 INFO]: Training accuracy: {
    "score": -1.0280370405842139,
    "rmse": 1.0280370405842139
}
[08/07/2025 12:20:31 INFO]: Val accuracy: {
    "score": -0.928656839519046,
    "rmse": 0.928656839519046
}
[08/07/2025 12:20:31 INFO]: Test accuracy: {
    "score": -0.927802135472704,
    "rmse": 0.927802135472704
}
[08/07/2025 12:20:31 INFO]: {
    "dataset": "ic_upstream3",
    "model_name": "ft_transformer",
    "run_id": "renowned-Tressie",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.927802135472704,
        "rmse": 0.927802135472704
    },
    "train_stats": {
        "score": -1.0280370405842139,
        "rmse": 1.0280370405842139
    },
    "val_stats": {
        "score": -0.928656839519046,
        "rmse": 0.928656839519046
    }
}
[08/07/2025 12:20:31 INFO]: 
_________________________________________________

[08/07/2025 12:20:31 INFO]: train_net_for_optune.py main() running.
[08/07/2025 12:20:31 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 2.1342188936023287
  attention_dropout: 0.007265598117921157
  ffn_dropout: 0.007265598117921157
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 9.556681355771317e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: renowned-Tressie

[08/07/2025 12:20:31 INFO]: This ft_transformer has 2.230 million parameters.
[08/07/2025 12:20:31 INFO]: Training will start at epoch 0.
[08/07/2025 12:20:31 INFO]: ==> Starting training for 200 epochs...
[08/07/2025 12:21:01 INFO]: Training loss at epoch 0: 1.056860625743866
[08/07/2025 12:21:05 INFO]: New best epoch, val score: -1.0375177523974075
[08/07/2025 12:21:05 INFO]: Saving model to: model_best.pth
[08/07/2025 12:21:35 INFO]: Training loss at epoch 1: 1.176152229309082
[08/07/2025 12:21:39 INFO]: New best epoch, val score: -1.0147818599991885
[08/07/2025 12:21:39 INFO]: Saving model to: model_best.pth
[08/07/2025 12:22:09 INFO]: Training loss at epoch 2: 1.274239420890808
[08/07/2025 12:22:13 INFO]: New best epoch, val score: -0.9368111919639037
[08/07/2025 12:22:13 INFO]: Saving model to: model_best.pth
[08/07/2025 12:22:43 INFO]: Training loss at epoch 3: 1.0313458740711212
[08/07/2025 12:23:17 INFO]: Training loss at epoch 4: 1.2264981269836426
[08/07/2025 12:23:53 INFO]: Training loss at epoch 5: 0.9560847878456116
[08/07/2025 12:24:28 INFO]: Training loss at epoch 6: 1.0787882804870605
[08/07/2025 12:25:03 INFO]: Training loss at epoch 7: 0.9816470146179199
[08/07/2025 12:25:41 INFO]: Training loss at epoch 8: 0.8417280614376068
[08/07/2025 12:26:15 INFO]: Training loss at epoch 9: 1.009609580039978
[08/07/2025 12:26:27 INFO]: Training stats: {
    "score": -0.9878055019397755,
    "rmse": 0.9878055019397755
}
[08/07/2025 12:26:27 INFO]: Val stats: {
    "score": -0.9825925817426614,
    "rmse": 0.9825925817426614
}
[08/07/2025 12:26:27 INFO]: Test stats: {
    "score": -0.9330544337926105,
    "rmse": 0.9330544337926105
}
[08/07/2025 12:27:01 INFO]: Training loss at epoch 10: 1.0816459953784943
[08/07/2025 12:27:05 INFO]: New best epoch, val score: -0.9294625724295105
[08/07/2025 12:27:05 INFO]: Saving model to: model_best.pth
[08/07/2025 12:27:36 INFO]: Training loss at epoch 11: 0.867538332939148
[08/07/2025 12:27:40 INFO]: New best epoch, val score: -0.9159830085185174
[08/07/2025 12:27:40 INFO]: Saving model to: model_best.pth
[08/07/2025 12:28:10 INFO]: Training loss at epoch 12: 0.9441550970077515
[08/07/2025 12:28:14 INFO]: New best epoch, val score: -0.9132676318028464
[08/07/2025 12:28:14 INFO]: Saving model to: model_best.pth
[08/07/2025 12:28:44 INFO]: Training loss at epoch 13: 1.0045761466026306
[08/07/2025 12:28:48 INFO]: New best epoch, val score: -0.9122348457010458
[08/07/2025 12:28:48 INFO]: Saving model to: model_best.pth
[08/07/2025 12:29:18 INFO]: Training loss at epoch 14: 1.002688467502594
[08/07/2025 12:29:52 INFO]: Training loss at epoch 15: 0.9271572232246399
[08/07/2025 12:30:26 INFO]: Training loss at epoch 16: 1.0207738876342773
[08/07/2025 12:31:01 INFO]: Training loss at epoch 17: 0.9583145081996918
[08/07/2025 12:31:35 INFO]: Training loss at epoch 18: 0.9671909511089325
[08/07/2025 12:32:09 INFO]: Training loss at epoch 19: 1.0339663624763489
[08/07/2025 12:32:21 INFO]: Training stats: {
    "score": -0.9772557436719225,
    "rmse": 0.9772557436719225
}
[08/07/2025 12:32:21 INFO]: Val stats: {
    "score": -0.9907592260223782,
    "rmse": 0.9907592260223782
}
[08/07/2025 12:32:21 INFO]: Test stats: {
    "score": -0.9320707056571539,
    "rmse": 0.9320707056571539
}
[08/07/2025 12:32:55 INFO]: Training loss at epoch 20: 1.0723094642162323
[08/07/2025 12:33:29 INFO]: Training loss at epoch 21: 0.9692696928977966
[08/07/2025 12:33:33 INFO]: New best epoch, val score: -0.9122038554407553
[08/07/2025 12:33:33 INFO]: Saving model to: model_best.pth
[08/07/2025 12:34:03 INFO]: Training loss at epoch 22: 0.7343919277191162
[08/07/2025 12:34:07 INFO]: New best epoch, val score: -0.8981454403350614
[08/07/2025 12:34:07 INFO]: Saving model to: model_best.pth
[08/07/2025 12:34:39 INFO]: Training loss at epoch 23: 0.9128139019012451
[08/07/2025 12:34:44 INFO]: New best epoch, val score: -0.8938164408658137
[08/07/2025 12:34:44 INFO]: Saving model to: model_best.pth
[08/07/2025 12:35:16 INFO]: Training loss at epoch 24: 1.0084757506847382
[08/07/2025 12:35:54 INFO]: Training loss at epoch 25: 0.941295713186264
[08/07/2025 12:36:28 INFO]: Training loss at epoch 26: 0.7846298813819885
[08/07/2025 12:37:02 INFO]: Training loss at epoch 27: 0.9810640811920166
[08/07/2025 12:37:37 INFO]: Training loss at epoch 28: 0.9583690166473389
[08/07/2025 12:38:11 INFO]: Training loss at epoch 29: 0.9844823479652405
[08/07/2025 12:38:23 INFO]: Training stats: {
    "score": -0.9223545603818533,
    "rmse": 0.9223545603818533
}
[08/07/2025 12:38:23 INFO]: Val stats: {
    "score": -0.9131425006650871,
    "rmse": 0.9131425006650871
}
[08/07/2025 12:38:23 INFO]: Test stats: {
    "score": -0.864534729093111,
    "rmse": 0.864534729093111
}
[08/07/2025 12:38:57 INFO]: Training loss at epoch 30: 0.9655567109584808
[08/07/2025 12:39:31 INFO]: Training loss at epoch 31: 0.7455434799194336
[08/07/2025 12:39:35 INFO]: New best epoch, val score: -0.8894549060800969
[08/07/2025 12:39:35 INFO]: Saving model to: model_best.pth
[08/07/2025 12:40:05 INFO]: Training loss at epoch 32: 0.7875275909900665
[08/07/2025 12:40:09 INFO]: New best epoch, val score: -0.88915034134701
[08/07/2025 12:40:09 INFO]: Saving model to: model_best.pth
[08/07/2025 12:40:39 INFO]: Training loss at epoch 33: 0.8742600083351135
[08/07/2025 12:40:43 INFO]: New best epoch, val score: -0.8818869120649564
[08/07/2025 12:40:43 INFO]: Saving model to: model_best.pth
[08/07/2025 12:41:13 INFO]: Training loss at epoch 34: 0.7297824621200562
[08/07/2025 12:41:17 INFO]: New best epoch, val score: -0.867207197273747
[08/07/2025 12:41:17 INFO]: Saving model to: model_best.pth
[08/07/2025 12:41:47 INFO]: Training loss at epoch 35: 0.7158324122428894
[08/07/2025 12:41:51 INFO]: New best epoch, val score: -0.8578141538313496
[08/07/2025 12:41:51 INFO]: Saving model to: model_best.pth
[08/07/2025 12:42:21 INFO]: Training loss at epoch 36: 0.963616132736206
[08/07/2025 12:42:55 INFO]: Training loss at epoch 37: 0.7862592339515686
[08/07/2025 12:43:29 INFO]: Training loss at epoch 38: 0.6907784640789032
[08/07/2025 12:44:03 INFO]: Training loss at epoch 39: 0.6888331472873688
[08/07/2025 12:44:14 INFO]: Training stats: {
    "score": -0.8469163400032776,
    "rmse": 0.8469163400032776
}
[08/07/2025 12:44:14 INFO]: Val stats: {
    "score": -0.8714373702812234,
    "rmse": 0.8714373702812234
}
[08/07/2025 12:44:14 INFO]: Test stats: {
    "score": -0.8087290708438593,
    "rmse": 0.8087290708438593
}
[08/07/2025 12:44:48 INFO]: Training loss at epoch 40: 0.7951879799365997
[08/07/2025 12:45:21 INFO]: Training loss at epoch 41: 0.5926028341054916
[08/07/2025 12:45:25 INFO]: New best epoch, val score: -0.8305003805204266
[08/07/2025 12:45:25 INFO]: Saving model to: model_best.pth
[08/07/2025 12:45:55 INFO]: Training loss at epoch 42: 0.807459682226181
[08/07/2025 12:45:59 INFO]: New best epoch, val score: -0.8274021939179503
[08/07/2025 12:45:59 INFO]: Saving model to: model_best.pth
[08/07/2025 12:46:29 INFO]: Training loss at epoch 43: 0.7047669887542725
[08/07/2025 12:47:03 INFO]: Training loss at epoch 44: 0.7159923017024994
[08/07/2025 12:47:38 INFO]: Training loss at epoch 45: 0.7942564785480499
[08/07/2025 12:48:12 INFO]: Training loss at epoch 46: 0.5661263018846512
[08/07/2025 12:48:46 INFO]: Training loss at epoch 47: 0.5740708708763123
[08/07/2025 12:48:50 INFO]: New best epoch, val score: -0.8116140706815046
[08/07/2025 12:48:50 INFO]: Saving model to: model_best.pth
[08/07/2025 12:49:20 INFO]: Training loss at epoch 48: 0.5023807734251022
[08/07/2025 12:49:24 INFO]: New best epoch, val score: -0.804782368732203
[08/07/2025 12:49:24 INFO]: Saving model to: model_best.pth
[08/07/2025 12:49:54 INFO]: Training loss at epoch 49: 0.5743627548217773
[08/07/2025 12:50:06 INFO]: Training stats: {
    "score": -0.7619456745564552,
    "rmse": 0.7619456745564552
}
[08/07/2025 12:50:06 INFO]: Val stats: {
    "score": -0.8348928238509145,
    "rmse": 0.8348928238509145
}
[08/07/2025 12:50:06 INFO]: Test stats: {
    "score": -0.7779966502424188,
    "rmse": 0.7779966502424188
}
[08/07/2025 12:50:42 INFO]: Training loss at epoch 50: 0.561162918806076
[08/07/2025 12:51:16 INFO]: Training loss at epoch 51: 0.5389437824487686
[08/07/2025 12:51:50 INFO]: Training loss at epoch 52: 0.47234998643398285
[08/07/2025 12:52:24 INFO]: Training loss at epoch 53: 0.5634750127792358
[08/07/2025 12:52:58 INFO]: Training loss at epoch 54: 0.6513671875
[08/07/2025 12:53:32 INFO]: Training loss at epoch 55: 0.5711997449398041
[08/07/2025 12:54:06 INFO]: Training loss at epoch 56: 0.5846135020256042
[08/07/2025 12:54:10 INFO]: New best epoch, val score: -0.7960999520184724
[08/07/2025 12:54:10 INFO]: Saving model to: model_best.pth
[08/07/2025 12:54:42 INFO]: Training loss at epoch 57: 0.6723668575286865
[08/07/2025 12:55:16 INFO]: Training loss at epoch 58: 0.5223475992679596
[08/07/2025 12:55:50 INFO]: Training loss at epoch 59: 0.4715411067008972
[08/07/2025 12:56:02 INFO]: Training stats: {
    "score": -0.719495490754548,
    "rmse": 0.719495490754548
}
[08/07/2025 12:56:02 INFO]: Val stats: {
    "score": -0.7806643432544612,
    "rmse": 0.7806643432544612
}
[08/07/2025 12:56:02 INFO]: Test stats: {
    "score": -0.7460853770167867,
    "rmse": 0.7460853770167867
}
[08/07/2025 12:56:06 INFO]: New best epoch, val score: -0.7806643432544612
[08/07/2025 12:56:06 INFO]: Saving model to: model_best.pth
[08/07/2025 12:56:35 INFO]: Training loss at epoch 60: 0.4812498092651367
[08/07/2025 12:57:09 INFO]: Training loss at epoch 61: 0.5498552024364471
[08/07/2025 12:57:46 INFO]: Training loss at epoch 62: 0.5300749838352203
[08/07/2025 12:58:19 INFO]: Training loss at epoch 63: 0.4733118712902069
[08/07/2025 12:58:53 INFO]: Training loss at epoch 64: 0.5678930133581161
[08/07/2025 12:59:27 INFO]: Training loss at epoch 65: 0.4456555098295212
[08/07/2025 13:00:01 INFO]: Training loss at epoch 66: 0.4960629642009735
[08/07/2025 13:00:05 INFO]: New best epoch, val score: -0.7791157055654486
[08/07/2025 13:00:05 INFO]: Saving model to: model_best.pth
[08/07/2025 13:00:36 INFO]: Training loss at epoch 67: 0.5423211306333542
[08/07/2025 13:00:40 INFO]: New best epoch, val score: -0.7751334033367392
[08/07/2025 13:00:40 INFO]: Saving model to: model_best.pth
[08/07/2025 13:01:10 INFO]: Training loss at epoch 68: 0.43699951469898224
[08/07/2025 13:01:44 INFO]: Training loss at epoch 69: 0.5354671478271484
[08/07/2025 13:01:55 INFO]: Training stats: {
    "score": -0.6659354166930764,
    "rmse": 0.6659354166930764
}
[08/07/2025 13:01:55 INFO]: Val stats: {
    "score": -0.7734622028026542,
    "rmse": 0.7734622028026542
}
[08/07/2025 13:01:55 INFO]: Test stats: {
    "score": -0.7393505259303904,
    "rmse": 0.7393505259303904
}
[08/07/2025 13:01:59 INFO]: New best epoch, val score: -0.7734622028026542
[08/07/2025 13:01:59 INFO]: Saving model to: model_best.pth
[08/07/2025 13:02:32 INFO]: Training loss at epoch 70: 0.5359507948160172
[08/07/2025 13:02:35 INFO]: New best epoch, val score: -0.7478939215276224
[08/07/2025 13:02:35 INFO]: Saving model to: model_best.pth
[08/07/2025 13:03:06 INFO]: Training loss at epoch 71: 0.4455959051847458
[08/07/2025 13:03:40 INFO]: Training loss at epoch 72: 0.40232284367084503
[08/07/2025 13:04:13 INFO]: Training loss at epoch 73: 0.5111808776855469
[08/07/2025 13:04:17 INFO]: New best epoch, val score: -0.7446601849912401
[08/07/2025 13:04:17 INFO]: Saving model to: model_best.pth
[08/07/2025 13:04:50 INFO]: Training loss at epoch 74: 0.42192938923835754
[08/07/2025 13:05:24 INFO]: Training loss at epoch 75: 0.5175873041152954
[08/07/2025 13:05:58 INFO]: Training loss at epoch 76: 0.5258793979883194
[08/07/2025 13:06:32 INFO]: Training loss at epoch 77: 0.45696963369846344
[08/07/2025 13:06:36 INFO]: New best epoch, val score: -0.737010598993926
[08/07/2025 13:06:36 INFO]: Saving model to: model_best.pth
[08/07/2025 13:07:07 INFO]: Training loss at epoch 78: 0.35856055468320847
[08/07/2025 13:07:11 INFO]: New best epoch, val score: -0.7364656116474959
[08/07/2025 13:07:11 INFO]: Saving model to: model_best.pth
[08/07/2025 13:07:41 INFO]: Training loss at epoch 79: 0.463966965675354
[08/07/2025 13:07:52 INFO]: Training stats: {
    "score": -0.6450500391956518,
    "rmse": 0.6450500391956518
}
[08/07/2025 13:07:52 INFO]: Val stats: {
    "score": -0.7902226797313552,
    "rmse": 0.7902226797313552
}
[08/07/2025 13:07:52 INFO]: Test stats: {
    "score": -0.7356477486501267,
    "rmse": 0.7356477486501267
}
[08/07/2025 13:08:26 INFO]: Training loss at epoch 80: 0.43933524191379547
[08/07/2025 13:09:00 INFO]: Training loss at epoch 81: 0.4115854501724243
[08/07/2025 13:09:34 INFO]: Training loss at epoch 82: 0.45425957441329956
[08/07/2025 13:10:07 INFO]: Training loss at epoch 83: 0.42425286769866943
[08/07/2025 13:10:41 INFO]: Training loss at epoch 84: 0.39999115467071533
[08/07/2025 13:11:15 INFO]: Training loss at epoch 85: 0.38638728857040405
[08/07/2025 13:11:49 INFO]: Training loss at epoch 86: 0.4063926190137863
[08/07/2025 13:12:22 INFO]: Training loss at epoch 87: 0.3589775413274765
[08/07/2025 13:12:56 INFO]: Training loss at epoch 88: 0.3784850835800171
[08/07/2025 13:13:30 INFO]: Training loss at epoch 89: 0.31969042867422104
[08/07/2025 13:13:42 INFO]: Training stats: {
    "score": -0.619015463247692,
    "rmse": 0.619015463247692
}
[08/07/2025 13:13:42 INFO]: Val stats: {
    "score": -0.7969799256153666,
    "rmse": 0.7969799256153666
}
[08/07/2025 13:13:42 INFO]: Test stats: {
    "score": -0.743464389555475,
    "rmse": 0.743464389555475
}
[08/07/2025 13:14:15 INFO]: Training loss at epoch 90: 0.3167191445827484
[08/07/2025 13:14:49 INFO]: Training loss at epoch 91: 0.35698215663433075
[08/07/2025 13:14:53 INFO]: New best epoch, val score: -0.7289690638265265
[08/07/2025 13:14:53 INFO]: Saving model to: model_best.pth
[08/07/2025 13:15:23 INFO]: Training loss at epoch 92: 0.3578508049249649
[08/07/2025 13:15:27 INFO]: New best epoch, val score: -0.7285001410547242
[08/07/2025 13:15:27 INFO]: Saving model to: model_best.pth
[08/07/2025 13:15:57 INFO]: Training loss at epoch 93: 0.32263222336769104
[08/07/2025 13:16:31 INFO]: Training loss at epoch 94: 0.3739045262336731
[08/07/2025 13:17:05 INFO]: Training loss at epoch 95: 0.4697130173444748
[08/07/2025 13:17:39 INFO]: Training loss at epoch 96: 0.4679143726825714
[08/07/2025 13:18:12 INFO]: Training loss at epoch 97: 0.37020884454250336
[08/07/2025 13:18:16 INFO]: New best epoch, val score: -0.7137261330616291
[08/07/2025 13:18:16 INFO]: Saving model to: model_best.pth
[08/07/2025 13:18:47 INFO]: Training loss at epoch 98: 0.33265601098537445
[08/07/2025 13:19:20 INFO]: Training loss at epoch 99: 0.4362485408782959
[08/07/2025 13:19:32 INFO]: Training stats: {
    "score": -0.5816099756508649,
    "rmse": 0.5816099756508649
}
[08/07/2025 13:19:32 INFO]: Val stats: {
    "score": -0.7487256380973972,
    "rmse": 0.7487256380973972
}
[08/07/2025 13:19:32 INFO]: Test stats: {
    "score": -0.7080023946193,
    "rmse": 0.7080023946193
}
[08/07/2025 13:20:05 INFO]: Training loss at epoch 100: 0.29651989787817
[08/07/2025 13:20:39 INFO]: Training loss at epoch 101: 0.3802489638328552
[08/07/2025 13:21:13 INFO]: Training loss at epoch 102: 0.4813135117292404
[08/07/2025 13:21:47 INFO]: Training loss at epoch 103: 0.31705503165721893
[08/07/2025 13:22:20 INFO]: Training loss at epoch 104: 0.36891452968120575
[08/07/2025 13:22:54 INFO]: Training loss at epoch 105: 0.409568727016449
[08/07/2025 13:23:28 INFO]: Training loss at epoch 106: 0.3102322071790695
[08/07/2025 13:24:01 INFO]: Training loss at epoch 107: 0.4633445590734482
[08/07/2025 13:24:35 INFO]: Training loss at epoch 108: 0.316655695438385
[08/07/2025 13:25:09 INFO]: Training loss at epoch 109: 0.44538336992263794
[08/07/2025 13:25:20 INFO]: Training stats: {
    "score": -0.5629010647720322,
    "rmse": 0.5629010647720322
}
[08/07/2025 13:25:20 INFO]: Val stats: {
    "score": -0.7135618618078995,
    "rmse": 0.7135618618078995
}
[08/07/2025 13:25:20 INFO]: Test stats: {
    "score": -0.6921495899203371,
    "rmse": 0.6921495899203371
}
[08/07/2025 13:25:24 INFO]: New best epoch, val score: -0.7135618618078995
[08/07/2025 13:25:24 INFO]: Saving model to: model_best.pth
[08/07/2025 13:25:54 INFO]: Training loss at epoch 110: 0.3132741153240204
[08/07/2025 13:25:58 INFO]: New best epoch, val score: -0.7044393075354064
[08/07/2025 13:25:58 INFO]: Saving model to: model_best.pth
[08/07/2025 13:26:28 INFO]: Training loss at epoch 111: 0.4292064607143402
[08/07/2025 13:27:03 INFO]: Training loss at epoch 112: 0.2799649313092232
[08/07/2025 13:27:37 INFO]: Training loss at epoch 113: 0.33768244087696075
[08/07/2025 13:28:12 INFO]: Training loss at epoch 114: 0.3220980614423752
[08/07/2025 13:28:46 INFO]: Training loss at epoch 115: 0.29610200226306915
[08/07/2025 13:29:20 INFO]: Training loss at epoch 116: 0.31463247537612915
[08/07/2025 13:29:54 INFO]: Training loss at epoch 117: 0.3561244457960129
[08/07/2025 13:30:28 INFO]: Training loss at epoch 118: 0.33513596653938293
[08/07/2025 13:31:03 INFO]: Training loss at epoch 119: 0.30467137694358826
[08/07/2025 13:31:14 INFO]: Training stats: {
    "score": -0.5532968560739552,
    "rmse": 0.5532968560739552
}
[08/07/2025 13:31:14 INFO]: Val stats: {
    "score": -0.7427319045380505,
    "rmse": 0.7427319045380505
}
[08/07/2025 13:31:14 INFO]: Test stats: {
    "score": -0.7004141448633056,
    "rmse": 0.7004141448633056
}
[08/07/2025 13:31:49 INFO]: Training loss at epoch 120: 0.255928136408329
[08/07/2025 13:32:23 INFO]: Training loss at epoch 121: 0.2631932646036148
[08/07/2025 13:32:58 INFO]: Training loss at epoch 122: 0.384084552526474
[08/07/2025 13:33:32 INFO]: Training loss at epoch 123: 0.3018869012594223
[08/07/2025 13:34:06 INFO]: Training loss at epoch 124: 0.2920480966567993
[08/07/2025 13:34:40 INFO]: Training loss at epoch 125: 0.3671233654022217
[08/07/2025 13:35:14 INFO]: Training loss at epoch 126: 0.3758549392223358
[08/07/2025 13:35:48 INFO]: Training loss at epoch 127: 0.29380111396312714
[08/07/2025 13:35:52 INFO]: New best epoch, val score: -0.6973470936397839
[08/07/2025 13:35:52 INFO]: Saving model to: model_best.pth
[08/07/2025 13:36:22 INFO]: Training loss at epoch 128: 0.3508123457431793
[08/07/2025 13:36:26 INFO]: New best epoch, val score: -0.6895239095829355
[08/07/2025 13:36:26 INFO]: Saving model to: model_best.pth
[08/07/2025 13:36:57 INFO]: Training loss at epoch 129: 0.2846347391605377
[08/07/2025 13:37:08 INFO]: Training stats: {
    "score": -0.5819438606647439,
    "rmse": 0.5819438606647439
}
[08/07/2025 13:37:08 INFO]: Val stats: {
    "score": -0.7726526878926965,
    "rmse": 0.7726526878926965
}
[08/07/2025 13:37:08 INFO]: Test stats: {
    "score": -0.7319762059982258,
    "rmse": 0.7319762059982258
}
[08/07/2025 13:37:43 INFO]: Training loss at epoch 130: 0.33510853350162506
[08/07/2025 13:38:17 INFO]: Training loss at epoch 131: 0.2835318446159363
[08/07/2025 13:38:21 INFO]: New best epoch, val score: -0.6770469292697795
[08/07/2025 13:38:21 INFO]: Saving model to: model_best.pth
[08/07/2025 13:38:51 INFO]: Training loss at epoch 132: 0.31293532252311707
[08/07/2025 13:39:25 INFO]: Training loss at epoch 133: 0.3394503742456436
[08/07/2025 13:39:59 INFO]: Training loss at epoch 134: 0.347999706864357
[08/07/2025 13:40:34 INFO]: Training loss at epoch 135: 0.3006862848997116
[08/07/2025 13:41:08 INFO]: Training loss at epoch 136: 0.35616089403629303
[08/07/2025 13:41:42 INFO]: Training loss at epoch 137: 0.2922827899456024
[08/07/2025 13:41:46 INFO]: New best epoch, val score: -0.6739568689856457
[08/07/2025 13:41:46 INFO]: Saving model to: model_best.pth
[08/07/2025 13:42:17 INFO]: Training loss at epoch 138: 0.26101909577846527
[08/07/2025 13:42:21 INFO]: New best epoch, val score: -0.6704922317400429
[08/07/2025 13:42:21 INFO]: Saving model to: model_best.pth
[08/07/2025 13:42:51 INFO]: Training loss at epoch 139: 0.2567226514220238
[08/07/2025 13:43:03 INFO]: Training stats: {
    "score": -0.5194422091600492,
    "rmse": 0.5194422091600492
}
[08/07/2025 13:43:03 INFO]: Val stats: {
    "score": -0.71224907641621,
    "rmse": 0.71224907641621
}
[08/07/2025 13:43:03 INFO]: Test stats: {
    "score": -0.6892841407702022,
    "rmse": 0.6892841407702022
}
[08/07/2025 13:43:36 INFO]: Training loss at epoch 140: 0.2517334520816803
[08/07/2025 13:44:10 INFO]: Training loss at epoch 141: 0.30013446509838104
[08/07/2025 13:44:44 INFO]: Training loss at epoch 142: 0.2161712497472763
[08/07/2025 13:45:18 INFO]: Training loss at epoch 143: 0.2575128823518753
[08/07/2025 13:45:53 INFO]: Training loss at epoch 144: 0.24883466213941574
[08/07/2025 13:46:27 INFO]: Training loss at epoch 145: 0.2652027755975723
[08/07/2025 13:47:01 INFO]: Training loss at epoch 146: 0.24475112557411194
[08/07/2025 13:47:35 INFO]: Training loss at epoch 147: 0.3488529399037361
[08/07/2025 13:48:09 INFO]: Training loss at epoch 148: 0.2559406906366348
[08/07/2025 13:48:13 INFO]: New best epoch, val score: -0.6686166064101879
[08/07/2025 13:48:13 INFO]: Saving model to: model_best.pth
[08/07/2025 13:48:43 INFO]: Training loss at epoch 149: 0.22126425802707672
[08/07/2025 13:48:54 INFO]: Training stats: {
    "score": -0.5140570850635416,
    "rmse": 0.5140570850635416
}
[08/07/2025 13:48:54 INFO]: Val stats: {
    "score": -0.6620722877893304,
    "rmse": 0.6620722877893304
}
[08/07/2025 13:48:54 INFO]: Test stats: {
    "score": -0.6730036334978987,
    "rmse": 0.6730036334978987
}
[08/07/2025 13:48:58 INFO]: New best epoch, val score: -0.6620722877893304
[08/07/2025 13:48:58 INFO]: Saving model to: model_best.pth
[08/07/2025 13:49:28 INFO]: Training loss at epoch 150: 0.2721620947122574
[08/07/2025 13:50:03 INFO]: Training loss at epoch 151: 0.23187287151813507
[08/07/2025 13:50:37 INFO]: Training loss at epoch 152: 0.26674601435661316
[08/07/2025 13:51:12 INFO]: Training loss at epoch 153: 0.22489409148693085
[08/07/2025 13:51:45 INFO]: Training loss at epoch 154: 0.18387174606323242
[08/07/2025 13:52:19 INFO]: Training loss at epoch 155: 0.2335350140929222
[08/07/2025 13:52:53 INFO]: Training loss at epoch 156: 0.21190161257982254
[08/07/2025 13:53:27 INFO]: Training loss at epoch 157: 0.31294921040534973
[08/07/2025 13:54:01 INFO]: Training loss at epoch 158: 0.19491275399923325
[08/07/2025 13:54:34 INFO]: Training loss at epoch 159: 0.2509487494826317
[08/07/2025 13:54:45 INFO]: Training stats: {
    "score": -0.4623241871102115,
    "rmse": 0.4623241871102115
}
[08/07/2025 13:54:45 INFO]: Val stats: {
    "score": -0.6840435729425004,
    "rmse": 0.6840435729425004
}
[08/07/2025 13:54:45 INFO]: Test stats: {
    "score": -0.7001313850684576,
    "rmse": 0.7001313850684576
}
[08/07/2025 13:55:19 INFO]: Training loss at epoch 160: 0.2141156569123268
[08/07/2025 13:55:53 INFO]: Training loss at epoch 161: 0.18737754225730896
[08/07/2025 13:56:27 INFO]: Training loss at epoch 162: 0.2212429717183113
[08/07/2025 13:57:00 INFO]: Training loss at epoch 163: 0.23961299657821655
[08/07/2025 13:57:34 INFO]: Training loss at epoch 164: 0.35817138850688934
[08/07/2025 13:58:08 INFO]: Training loss at epoch 165: 0.2868267744779587
[08/07/2025 13:58:42 INFO]: Training loss at epoch 166: 0.31661444902420044
[08/07/2025 13:59:16 INFO]: Training loss at epoch 167: 0.30931951105594635
[08/07/2025 13:59:50 INFO]: Training loss at epoch 168: 0.23708203434944153
[08/07/2025 14:00:24 INFO]: Training loss at epoch 169: 0.26384152472019196
[08/07/2025 14:00:35 INFO]: Training stats: {
    "score": -0.4603847334497511,
    "rmse": 0.4603847334497511
}
[08/07/2025 14:00:35 INFO]: Val stats: {
    "score": -0.6941779704083751,
    "rmse": 0.6941779704083751
}
[08/07/2025 14:00:35 INFO]: Test stats: {
    "score": -0.6789839769338389,
    "rmse": 0.6789839769338389
}
[08/07/2025 14:01:09 INFO]: Training loss at epoch 170: 0.2384493499994278
[08/07/2025 14:01:43 INFO]: Training loss at epoch 171: 0.18996427953243256
[08/07/2025 14:02:17 INFO]: Training loss at epoch 172: 0.22606659680604935
[08/07/2025 14:02:50 INFO]: Training loss at epoch 173: 0.2442593276500702
[08/07/2025 14:03:24 INFO]: Training loss at epoch 174: 0.17432869970798492
[08/07/2025 14:03:58 INFO]: Training loss at epoch 175: 0.27296940982341766
[08/07/2025 14:04:32 INFO]: Training loss at epoch 176: 0.34788575768470764
[08/07/2025 14:05:05 INFO]: Training loss at epoch 177: 0.21214303374290466
[08/07/2025 14:05:39 INFO]: Training loss at epoch 178: 0.28215472400188446
[08/07/2025 14:06:13 INFO]: Training loss at epoch 179: 0.21691343188285828
[08/07/2025 14:06:24 INFO]: Training stats: {
    "score": -0.5124772732330545,
    "rmse": 0.5124772732330545
}
[08/07/2025 14:06:24 INFO]: Val stats: {
    "score": -0.7648582553409478,
    "rmse": 0.7648582553409478
}
[08/07/2025 14:06:24 INFO]: Test stats: {
    "score": -0.7441123825752254,
    "rmse": 0.7441123825752254
}
[08/07/2025 14:06:58 INFO]: Training loss at epoch 180: 0.23165161162614822
[08/07/2025 14:07:02 INFO]: New best epoch, val score: -0.6606528588787933
[08/07/2025 14:07:02 INFO]: Saving model to: model_best.pth
[08/07/2025 14:07:32 INFO]: Training loss at epoch 181: 0.2168264091014862
[08/07/2025 14:07:36 INFO]: New best epoch, val score: -0.6604114312816968
[08/07/2025 14:07:36 INFO]: Saving model to: model_best.pth
[08/07/2025 14:08:06 INFO]: Training loss at epoch 182: 0.223800390958786
[08/07/2025 14:08:40 INFO]: Training loss at epoch 183: 0.2638401836156845
[08/07/2025 14:09:14 INFO]: Training loss at epoch 184: 0.19520148634910583
[08/07/2025 14:09:48 INFO]: Training loss at epoch 185: 0.18031888455152512
[08/07/2025 14:10:22 INFO]: Training loss at epoch 186: 0.22009340673685074
[08/07/2025 14:10:55 INFO]: Training loss at epoch 187: 0.22161204367876053
[08/07/2025 14:11:29 INFO]: Training loss at epoch 188: 0.2459365874528885
[08/07/2025 14:12:03 INFO]: Training loss at epoch 189: 0.22585198283195496
[08/07/2025 14:12:15 INFO]: Training stats: {
    "score": -0.43264897657733725,
    "rmse": 0.43264897657733725
}
[08/07/2025 14:12:15 INFO]: Val stats: {
    "score": -0.6842325532450092,
    "rmse": 0.6842325532450092
}
[08/07/2025 14:12:15 INFO]: Test stats: {
    "score": -0.7119432314983137,
    "rmse": 0.7119432314983137
}
[08/07/2025 14:12:49 INFO]: Training loss at epoch 190: 0.21410877257585526
[08/07/2025 14:13:23 INFO]: Training loss at epoch 191: 0.25297075510025024
[08/07/2025 14:13:57 INFO]: Training loss at epoch 192: 0.16869551688432693
[08/07/2025 14:14:30 INFO]: Training loss at epoch 193: 0.2373364493250847
[08/07/2025 14:15:04 INFO]: Training loss at epoch 194: 0.22608330100774765
[08/07/2025 14:15:38 INFO]: Training loss at epoch 195: 0.22876524925231934
[08/07/2025 14:15:42 INFO]: New best epoch, val score: -0.6587574886284823
[08/07/2025 14:15:42 INFO]: Saving model to: model_best.pth
[08/07/2025 14:16:12 INFO]: Training loss at epoch 196: 0.160897608846426
[08/07/2025 14:16:16 INFO]: New best epoch, val score: -0.6552748302309053
[08/07/2025 14:16:16 INFO]: Saving model to: model_best.pth
[08/07/2025 14:16:46 INFO]: Training loss at epoch 197: 0.19699379056692123
[08/07/2025 14:17:20 INFO]: Training loss at epoch 198: 0.2218911126255989
[08/07/2025 14:17:54 INFO]: Training loss at epoch 199: 0.2080828845500946
[08/07/2025 14:18:05 INFO]: Training stats: {
    "score": -0.4051203167363851,
    "rmse": 0.4051203167363851
}
[08/07/2025 14:18:05 INFO]: Val stats: {
    "score": -0.686423038147977,
    "rmse": 0.686423038147977
}
[08/07/2025 14:18:05 INFO]: Test stats: {
    "score": -0.7375192408070597,
    "rmse": 0.7375192408070597
}
[08/07/2025 14:18:09 INFO]: Running Final Evaluation...
[08/07/2025 14:18:20 INFO]: Training accuracy: {
    "score": -0.42159350037541027,
    "rmse": 0.42159350037541027
}
[08/07/2025 14:18:20 INFO]: Val accuracy: {
    "score": -0.6552748302309053,
    "rmse": 0.6552748302309053
}
[08/07/2025 14:18:20 INFO]: Test accuracy: {
    "score": -0.6904055973813628,
    "rmse": 0.6904055973813628
}
[08/07/2025 14:18:20 INFO]: {
    "dataset": "ic_upstream3",
    "model_name": "ft_transformer",
    "run_id": "renowned-Tressie",
    "best_epoch": 196,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.6904055973813628,
        "rmse": 0.6904055973813628
    },
    "train_stats": {
        "score": -0.42159350037541027,
        "rmse": 0.42159350037541027
    },
    "val_stats": {
        "score": -0.6552748302309053,
        "rmse": 0.6552748302309053
    }
}
[08/07/2025 14:18:20 INFO]: 
_________________________________________________

[08/07/2025 14:18:20 INFO]: train_net_for_optune.py main() running.
[08/07/2025 14:18:20 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 8
  d_ffn_factor: 2.40037583331714
  attention_dropout: 0.3337154047808305
  ffn_dropout: 0.3337154047808305
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 7.679033849415801e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: renowned-Tressie

[08/07/2025 14:18:20 INFO]: This ft_transformer has 1.137 million parameters.
[08/07/2025 14:18:20 INFO]: Training will start at epoch 0.
[08/07/2025 14:18:20 INFO]: ==> Starting training for 200 epochs...
[08/07/2025 14:18:49 INFO]: Training loss at epoch 0: 1.0726584792137146
[08/07/2025 14:18:52 INFO]: New best epoch, val score: -1.001519390364613
[08/07/2025 14:18:52 INFO]: Saving model to: model_best.pth
[08/07/2025 14:19:21 INFO]: Training loss at epoch 1: 1.27013099193573
[08/07/2025 14:19:25 INFO]: New best epoch, val score: -0.9393594708510891
[08/07/2025 14:19:25 INFO]: Saving model to: model_best.pth
[08/07/2025 14:19:53 INFO]: Training loss at epoch 2: 1.0836724042892456
[08/07/2025 14:19:56 INFO]: New best epoch, val score: -0.9201683294594818
[08/07/2025 14:19:56 INFO]: Saving model to: model_best.pth
[08/07/2025 14:20:24 INFO]: Training loss at epoch 3: 0.9791329205036163
[08/07/2025 14:20:28 INFO]: New best epoch, val score: -0.9038582275034206
[08/07/2025 14:20:28 INFO]: Saving model to: model_best.pth
[08/07/2025 14:20:56 INFO]: Training loss at epoch 4: 0.8359975814819336
[08/07/2025 14:21:00 INFO]: New best epoch, val score: -0.8996974689771214
[08/07/2025 14:21:00 INFO]: Saving model to: model_best.pth
[08/07/2025 14:21:28 INFO]: Training loss at epoch 5: 0.974460780620575
[08/07/2025 14:22:00 INFO]: Training loss at epoch 6: 1.110922783613205
[08/07/2025 14:22:04 INFO]: New best epoch, val score: -0.8996260594660729
[08/07/2025 14:22:04 INFO]: Saving model to: model_best.pth
[08/07/2025 14:22:32 INFO]: Training loss at epoch 7: 1.092103660106659
[08/07/2025 14:23:05 INFO]: Training loss at epoch 8: 1.0281170010566711
[08/07/2025 14:23:36 INFO]: Training loss at epoch 9: 0.9040310978889465
[08/07/2025 14:23:47 INFO]: Training stats: {
    "score": -0.9491178356673518,
    "rmse": 0.9491178356673518
}
[08/07/2025 14:23:47 INFO]: Val stats: {
    "score": -0.899765568511746,
    "rmse": 0.899765568511746
}
[08/07/2025 14:23:47 INFO]: Test stats: {
    "score": -0.8516151362253639,
    "rmse": 0.8516151362253639
}
[08/07/2025 14:24:19 INFO]: Training loss at epoch 10: 0.7807649374008179
[08/07/2025 14:24:23 INFO]: New best epoch, val score: -0.8929489188586623
[08/07/2025 14:24:23 INFO]: Saving model to: model_best.pth
[08/07/2025 14:24:52 INFO]: Training loss at epoch 11: 0.9472125470638275
[08/07/2025 14:24:55 INFO]: New best epoch, val score: -0.8879417302466873
[08/07/2025 14:24:55 INFO]: Saving model to: model_best.pth
[08/07/2025 14:25:23 INFO]: Training loss at epoch 12: 1.0036714971065521
[08/07/2025 14:25:27 INFO]: New best epoch, val score: -0.8855759738264408
[08/07/2025 14:25:27 INFO]: Saving model to: model_best.pth
[08/07/2025 14:25:55 INFO]: Training loss at epoch 13: 0.82804936170578
[08/07/2025 14:25:59 INFO]: New best epoch, val score: -0.8823313995769569
[08/07/2025 14:25:59 INFO]: Saving model to: model_best.pth
[08/07/2025 14:26:28 INFO]: Training loss at epoch 14: 0.8909648358821869
[08/07/2025 14:26:32 INFO]: New best epoch, val score: -0.8777076492772391
[08/07/2025 14:26:32 INFO]: Saving model to: model_best.pth
[08/07/2025 14:27:03 INFO]: Training loss at epoch 15: 0.7494235932826996
[08/07/2025 14:27:07 INFO]: New best epoch, val score: -0.8742604572753018
[08/07/2025 14:27:07 INFO]: Saving model to: model_best.pth
[08/07/2025 14:27:36 INFO]: Training loss at epoch 16: 0.8950389623641968
[08/07/2025 14:27:39 INFO]: New best epoch, val score: -0.8714939505989221
[08/07/2025 14:27:39 INFO]: Saving model to: model_best.pth
[08/07/2025 14:28:07 INFO]: Training loss at epoch 17: 0.795766681432724
[08/07/2025 14:28:11 INFO]: New best epoch, val score: -0.8697411615579285
[08/07/2025 14:28:11 INFO]: Saving model to: model_best.pth
[08/07/2025 14:28:39 INFO]: Training loss at epoch 18: 0.8649149835109711
[08/07/2025 14:28:43 INFO]: New best epoch, val score: -0.8680638442222441
[08/07/2025 14:28:43 INFO]: Saving model to: model_best.pth
[08/07/2025 14:29:11 INFO]: Training loss at epoch 19: 1.0274289846420288
[08/07/2025 14:29:22 INFO]: Training stats: {
    "score": -0.9049703114330564,
    "rmse": 0.9049703114330564
}
[08/07/2025 14:29:22 INFO]: Val stats: {
    "score": -0.8661006377700318,
    "rmse": 0.8661006377700318
}
[08/07/2025 14:29:22 INFO]: Test stats: {
    "score": -0.8086447666068466,
    "rmse": 0.8086447666068466
}
[08/07/2025 14:29:26 INFO]: New best epoch, val score: -0.8661006377700318
[08/07/2025 14:29:26 INFO]: Saving model to: model_best.pth
[08/07/2025 14:29:54 INFO]: Training loss at epoch 20: 0.8780268132686615
[08/07/2025 14:29:58 INFO]: New best epoch, val score: -0.8619570537450105
[08/07/2025 14:29:58 INFO]: Saving model to: model_best.pth
[08/07/2025 14:30:26 INFO]: Training loss at epoch 21: 0.8232646286487579
[08/07/2025 14:30:30 INFO]: New best epoch, val score: -0.8561563355385763
[08/07/2025 14:30:30 INFO]: Saving model to: model_best.pth
[08/07/2025 14:30:58 INFO]: Training loss at epoch 22: 0.9393968880176544
[08/07/2025 14:31:02 INFO]: New best epoch, val score: -0.8490385846434213
[08/07/2025 14:31:02 INFO]: Saving model to: model_best.pth
[08/07/2025 14:31:30 INFO]: Training loss at epoch 23: 0.6339622288942337
[08/07/2025 14:31:34 INFO]: New best epoch, val score: -0.8424299059949496
[08/07/2025 14:31:34 INFO]: Saving model to: model_best.pth
[08/07/2025 14:32:02 INFO]: Training loss at epoch 24: 1.0037724375724792
[08/07/2025 14:32:06 INFO]: New best epoch, val score: -0.8372196699256985
[08/07/2025 14:32:06 INFO]: Saving model to: model_best.pth
[08/07/2025 14:32:34 INFO]: Training loss at epoch 25: 0.9951433539390564
[08/07/2025 14:32:38 INFO]: New best epoch, val score: -0.8325695341372957
[08/07/2025 14:32:38 INFO]: Saving model to: model_best.pth
[08/07/2025 14:33:06 INFO]: Training loss at epoch 26: 0.9760096967220306
[08/07/2025 14:33:10 INFO]: New best epoch, val score: -0.8294061094022009
[08/07/2025 14:33:10 INFO]: Saving model to: model_best.pth
[08/07/2025 14:33:38 INFO]: Training loss at epoch 27: 0.7294993698596954
[08/07/2025 14:34:10 INFO]: Training loss at epoch 28: 0.8360562920570374
[08/07/2025 14:34:42 INFO]: Training loss at epoch 29: 0.8338985443115234
[08/07/2025 14:34:53 INFO]: Training stats: {
    "score": -0.8314681947109879,
    "rmse": 0.8314681947109879
}
[08/07/2025 14:34:53 INFO]: Val stats: {
    "score": -0.8315887189916569,
    "rmse": 0.8315887189916569
}
[08/07/2025 14:34:53 INFO]: Test stats: {
    "score": -0.7504619393568785,
    "rmse": 0.7504619393568785
}
[08/07/2025 14:35:24 INFO]: Training loss at epoch 30: 1.0125510394573212
[08/07/2025 14:35:28 INFO]: New best epoch, val score: -0.8247108345127082
[08/07/2025 14:35:28 INFO]: Saving model to: model_best.pth
[08/07/2025 14:35:56 INFO]: Training loss at epoch 31: 0.8855842649936676
[08/07/2025 14:36:00 INFO]: New best epoch, val score: -0.8163439632865597
[08/07/2025 14:36:00 INFO]: Saving model to: model_best.pth
[08/07/2025 14:36:28 INFO]: Training loss at epoch 32: 0.7754126191139221
[08/07/2025 14:36:32 INFO]: New best epoch, val score: -0.8040045822594319
[08/07/2025 14:36:32 INFO]: Saving model to: model_best.pth
[08/07/2025 14:37:00 INFO]: Training loss at epoch 33: 0.9286299645900726
[08/07/2025 14:37:04 INFO]: New best epoch, val score: -0.8019023334192816
[08/07/2025 14:37:04 INFO]: Saving model to: model_best.pth
[08/07/2025 14:37:32 INFO]: Training loss at epoch 34: 0.6957409083843231
[08/07/2025 14:38:04 INFO]: Training loss at epoch 35: 0.7883216738700867
[08/07/2025 14:38:36 INFO]: Training loss at epoch 36: 0.6921277940273285
[08/07/2025 14:38:40 INFO]: New best epoch, val score: -0.7997762891458299
[08/07/2025 14:38:40 INFO]: Saving model to: model_best.pth
[08/07/2025 14:39:08 INFO]: Training loss at epoch 37: 0.8800660967826843
[08/07/2025 14:39:11 INFO]: New best epoch, val score: -0.7923027065930034
[08/07/2025 14:39:11 INFO]: Saving model to: model_best.pth
[08/07/2025 14:39:40 INFO]: Training loss at epoch 38: 0.951322615146637
[08/07/2025 14:39:43 INFO]: New best epoch, val score: -0.7887283127741462
[08/07/2025 14:39:43 INFO]: Saving model to: model_best.pth
[08/07/2025 14:40:12 INFO]: Training loss at epoch 39: 0.8948359787464142
[08/07/2025 14:40:23 INFO]: Training stats: {
    "score": -0.796438988722391,
    "rmse": 0.796438988722391
}
[08/07/2025 14:40:23 INFO]: Val stats: {
    "score": -0.7943036174980687,
    "rmse": 0.7943036174980687
}
[08/07/2025 14:40:23 INFO]: Test stats: {
    "score": -0.7372677384126408,
    "rmse": 0.7372677384126408
}
[08/07/2025 14:40:56 INFO]: Training loss at epoch 40: 0.6371405124664307
[08/07/2025 14:41:28 INFO]: Training loss at epoch 41: 0.7944931387901306
[08/07/2025 14:42:00 INFO]: Training loss at epoch 42: 0.8987053036689758
[08/07/2025 14:42:32 INFO]: Training loss at epoch 43: 0.9106824100017548
[08/07/2025 14:43:04 INFO]: Training loss at epoch 44: 0.7399528324604034
[08/07/2025 14:43:36 INFO]: Training loss at epoch 45: 0.839359313249588
[08/07/2025 14:44:07 INFO]: Training loss at epoch 46: 0.5652806609869003
[08/07/2025 14:44:39 INFO]: Training loss at epoch 47: 0.6687643229961395
[08/07/2025 14:45:11 INFO]: Training loss at epoch 48: 0.5265752524137497
[08/07/2025 14:45:43 INFO]: Training loss at epoch 49: 0.7150917947292328
[08/07/2025 14:45:54 INFO]: Training stats: {
    "score": -0.8510270873161144,
    "rmse": 0.8510270873161144
}
[08/07/2025 14:45:54 INFO]: Val stats: {
    "score": -0.8079556385881038,
    "rmse": 0.8079556385881038
}
[08/07/2025 14:45:54 INFO]: Test stats: {
    "score": -0.8258227529495142,
    "rmse": 0.8258227529495142
}
[08/07/2025 14:46:26 INFO]: Training loss at epoch 50: 0.654342770576477
[08/07/2025 14:46:58 INFO]: Training loss at epoch 51: 0.7276456952095032
[08/07/2025 14:47:29 INFO]: Training loss at epoch 52: 0.6600857079029083
[08/07/2025 14:48:01 INFO]: Training loss at epoch 53: 0.803459107875824
[08/07/2025 14:48:33 INFO]: Training loss at epoch 54: 0.675848662853241
[08/07/2025 14:49:05 INFO]: Training loss at epoch 55: 0.6812836229801178
[08/07/2025 14:49:36 INFO]: Training loss at epoch 56: 0.6936264336109161
[08/07/2025 14:50:08 INFO]: Training loss at epoch 57: 0.617920458316803
[08/07/2025 14:50:40 INFO]: Training loss at epoch 58: 0.7426077723503113
[08/07/2025 14:51:13 INFO]: Training loss at epoch 59: 0.6436306834220886
[08/07/2025 14:51:24 INFO]: Training stats: {
    "score": -0.7737186566127469,
    "rmse": 0.7737186566127469
}
[08/07/2025 14:51:24 INFO]: Val stats: {
    "score": -0.8069120911875892,
    "rmse": 0.8069120911875892
}
[08/07/2025 14:51:24 INFO]: Test stats: {
    "score": -0.7984309894663414,
    "rmse": 0.7984309894663414
}
[08/07/2025 14:51:56 INFO]: Training loss at epoch 60: 0.5812583863735199
[08/07/2025 14:52:28 INFO]: Training loss at epoch 61: 0.795747846364975
[08/07/2025 14:53:00 INFO]: Training loss at epoch 62: 0.6870047450065613
[08/07/2025 14:53:32 INFO]: Training loss at epoch 63: 0.587696984410286
[08/07/2025 14:54:04 INFO]: Training loss at epoch 64: 0.6314768195152283
[08/07/2025 14:54:37 INFO]: Training loss at epoch 65: 0.8086294531822205
[08/07/2025 14:55:09 INFO]: Training loss at epoch 66: 0.6247283816337585
[08/07/2025 14:55:41 INFO]: Training loss at epoch 67: 0.942842572927475
[08/07/2025 14:56:12 INFO]: Training loss at epoch 68: 0.595989853143692
[08/07/2025 14:56:45 INFO]: Training loss at epoch 69: 0.5964759886264801
[08/07/2025 14:56:56 INFO]: Training stats: {
    "score": -0.7337680275028716,
    "rmse": 0.7337680275028716
}
[08/07/2025 14:56:56 INFO]: Val stats: {
    "score": -0.8104523339198628,
    "rmse": 0.8104523339198628
}
[08/07/2025 14:56:56 INFO]: Test stats: {
    "score": -0.7858029201316816,
    "rmse": 0.7858029201316816
}
[08/07/2025 14:57:00 INFO]: Running Final Evaluation...
[08/07/2025 14:57:12 INFO]: Training accuracy: {
    "score": -0.8141311218571646,
    "rmse": 0.8141311218571646
}
[08/07/2025 14:57:12 INFO]: Val accuracy: {
    "score": -0.7887283127741462,
    "rmse": 0.7887283127741462
}
[08/07/2025 14:57:12 INFO]: Test accuracy: {
    "score": -0.7440934401793811,
    "rmse": 0.7440934401793811
}
[08/07/2025 14:57:12 INFO]: {
    "dataset": "ic_upstream3",
    "model_name": "ft_transformer",
    "run_id": "renowned-Tressie",
    "best_epoch": 38,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.7440934401793811,
        "rmse": 0.7440934401793811
    },
    "train_stats": {
        "score": -0.8141311218571646,
        "rmse": 0.8141311218571646
    },
    "val_stats": {
        "score": -0.7887283127741462,
        "rmse": 0.7887283127741462
    }
}
[08/07/2025 14:57:12 INFO]: 
_________________________________________________

[08/07/2025 14:57:12 INFO]: train_net_for_optune.py main() running.
[08/07/2025 14:57:12 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 2.660233891865051
  attention_dropout: 0.40775973364281115
  ffn_dropout: 0.40775973364281115
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00019249504304982322
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: renowned-Tressie

[08/07/2025 14:57:12 INFO]: This ft_transformer has 0.642 million parameters.
[08/07/2025 14:57:12 INFO]: Training will start at epoch 0.
[08/07/2025 14:57:12 INFO]: ==> Starting training for 200 epochs...
[08/07/2025 14:57:28 INFO]: Training loss at epoch 0: 1.1121825575828552
[08/07/2025 14:57:30 INFO]: New best epoch, val score: -0.9508558930971439
[08/07/2025 14:57:30 INFO]: Saving model to: model_best.pth
[08/07/2025 14:57:45 INFO]: Training loss at epoch 1: 1.1216704845428467
[08/07/2025 14:58:03 INFO]: Training loss at epoch 2: 1.0237399637699127
[08/07/2025 14:58:20 INFO]: Training loss at epoch 3: 0.983305960893631
[08/07/2025 14:58:23 INFO]: New best epoch, val score: -0.9405706630370625
[08/07/2025 14:58:23 INFO]: Saving model to: model_best.pth
[08/07/2025 14:58:38 INFO]: Training loss at epoch 4: 1.2615219354629517
[08/07/2025 14:58:40 INFO]: New best epoch, val score: -0.9312570087104595
[08/07/2025 14:58:40 INFO]: Saving model to: model_best.pth
[08/07/2025 14:58:56 INFO]: Training loss at epoch 5: 1.3047844171524048
[08/07/2025 14:59:14 INFO]: Training loss at epoch 6: 0.9364901781082153
[08/07/2025 14:59:32 INFO]: Training loss at epoch 7: 1.2063383758068085
[08/07/2025 14:59:49 INFO]: Training loss at epoch 8: 1.21897354722023
[08/07/2025 15:00:07 INFO]: Training loss at epoch 9: 0.8833377659320831
[08/07/2025 15:00:14 INFO]: Training stats: {
    "score": -0.9973247301932187,
    "rmse": 0.9973247301932187
}
[08/07/2025 15:00:14 INFO]: Val stats: {
    "score": -0.934626103771734,
    "rmse": 0.934626103771734
}
[08/07/2025 15:00:14 INFO]: Test stats: {
    "score": -0.9120559727776605,
    "rmse": 0.9120559727776605
}
[08/07/2025 15:00:31 INFO]: Training loss at epoch 10: 0.840285986661911
[08/07/2025 15:00:49 INFO]: Training loss at epoch 11: 0.9846723675727844
[08/07/2025 15:01:07 INFO]: Training loss at epoch 12: 1.1774632334709167
[08/07/2025 15:01:25 INFO]: Training loss at epoch 13: 1.1091222763061523
[08/07/2025 15:01:43 INFO]: Training loss at epoch 14: 1.0548349022865295
[08/07/2025 15:02:01 INFO]: Training loss at epoch 15: 0.8221317827701569
[08/07/2025 15:02:18 INFO]: Training loss at epoch 16: 1.0278912782669067
[08/07/2025 15:02:36 INFO]: Training loss at epoch 17: 1.035694181919098
[08/07/2025 15:02:54 INFO]: Training loss at epoch 18: 0.9391872584819794
[08/07/2025 15:03:11 INFO]: Training loss at epoch 19: 1.1274215579032898
[08/07/2025 15:03:17 INFO]: Training stats: {
    "score": -0.9878929955609568,
    "rmse": 0.9878929955609568
}
[08/07/2025 15:03:17 INFO]: Val stats: {
    "score": -0.9333759359977242,
    "rmse": 0.9333759359977242
}
[08/07/2025 15:03:17 INFO]: Test stats: {
    "score": -0.9061552618675673,
    "rmse": 0.9061552618675673
}
[08/07/2025 15:03:35 INFO]: Training loss at epoch 20: 1.0148658454418182
[08/07/2025 15:03:52 INFO]: Training loss at epoch 21: 0.9193131923675537
[08/07/2025 15:04:10 INFO]: Training loss at epoch 22: 1.118539422750473
[08/07/2025 15:04:27 INFO]: Training loss at epoch 23: 0.9091550707817078
[08/07/2025 15:04:45 INFO]: Training loss at epoch 24: 0.8803618252277374
[08/07/2025 15:05:02 INFO]: Training loss at epoch 25: 1.0156184434890747
[08/07/2025 15:05:20 INFO]: Training loss at epoch 26: 1.0481724739074707
[08/07/2025 15:05:37 INFO]: Training loss at epoch 27: 0.9590132832527161
[08/07/2025 15:05:55 INFO]: Training loss at epoch 28: 1.1551146805286407
[08/07/2025 15:06:12 INFO]: Training loss at epoch 29: 1.099213421344757
[08/07/2025 15:06:19 INFO]: Training stats: {
    "score": -0.9895693850605212,
    "rmse": 0.9895693850605212
}
[08/07/2025 15:06:19 INFO]: Val stats: {
    "score": -0.9887514657973838,
    "rmse": 0.9887514657973838
}
[08/07/2025 15:06:19 INFO]: Test stats: {
    "score": -0.9375323265118205,
    "rmse": 0.9375323265118205
}
[08/07/2025 15:06:36 INFO]: Training loss at epoch 30: 1.0609681010246277
[08/07/2025 15:06:54 INFO]: Training loss at epoch 31: 0.9306708872318268
[08/07/2025 15:07:12 INFO]: Training loss at epoch 32: 1.021399736404419
[08/07/2025 15:07:29 INFO]: Training loss at epoch 33: 1.0290561318397522
[08/07/2025 15:07:32 INFO]: New best epoch, val score: -0.9266116254642995
[08/07/2025 15:07:32 INFO]: Saving model to: model_best.pth
[08/07/2025 15:07:47 INFO]: Training loss at epoch 34: 1.0639404952526093
[08/07/2025 15:07:50 INFO]: New best epoch, val score: -0.9232136239556696
[08/07/2025 15:07:50 INFO]: Saving model to: model_best.pth
[08/07/2025 15:08:05 INFO]: Training loss at epoch 35: 0.9172282814979553
[08/07/2025 15:08:24 INFO]: Training loss at epoch 36: 1.044882446527481
[08/07/2025 15:08:43 INFO]: Training loss at epoch 37: 1.0549155473709106
[08/07/2025 15:09:03 INFO]: Training loss at epoch 38: 0.8467583656311035
[08/07/2025 15:09:22 INFO]: Training loss at epoch 39: 1.0067138671875
[08/07/2025 15:09:28 INFO]: Training stats: {
    "score": -0.967905519344872,
    "rmse": 0.967905519344872
}
[08/07/2025 15:09:28 INFO]: Val stats: {
    "score": -0.9468086264547092,
    "rmse": 0.9468086264547092
}
[08/07/2025 15:09:28 INFO]: Test stats: {
    "score": -0.9025865538603906,
    "rmse": 0.9025865538603906
}
[08/07/2025 15:09:46 INFO]: Training loss at epoch 40: 1.096310317516327
[08/07/2025 15:10:04 INFO]: Training loss at epoch 41: 0.815841406583786
[08/07/2025 15:10:21 INFO]: Training loss at epoch 42: 0.7246202528476715
[08/07/2025 15:10:39 INFO]: Training loss at epoch 43: 1.0973029434680939
[08/07/2025 15:10:56 INFO]: Training loss at epoch 44: 0.9221855998039246
[08/07/2025 15:11:14 INFO]: Training loss at epoch 45: 0.8824998438358307
[08/07/2025 15:11:31 INFO]: Training loss at epoch 46: 0.8926786482334137
[08/07/2025 15:11:49 INFO]: Training loss at epoch 47: 0.984811007976532
[08/07/2025 15:12:06 INFO]: Training loss at epoch 48: 0.8464950323104858
[08/07/2025 15:12:24 INFO]: Training loss at epoch 49: 1.2046165466308594
[08/07/2025 15:12:30 INFO]: Training stats: {
    "score": -0.9533653781487491,
    "rmse": 0.9533653781487491
}
[08/07/2025 15:12:30 INFO]: Val stats: {
    "score": -0.938955022562889,
    "rmse": 0.938955022562889
}
[08/07/2025 15:12:30 INFO]: Test stats: {
    "score": -0.8893444046453154,
    "rmse": 0.8893444046453154
}
[08/07/2025 15:12:47 INFO]: Training loss at epoch 50: 0.838984489440918
[08/07/2025 15:13:05 INFO]: Training loss at epoch 51: 1.0847002863883972
[08/07/2025 15:13:22 INFO]: Training loss at epoch 52: 0.9914876222610474
[08/07/2025 15:13:40 INFO]: Training loss at epoch 53: 0.9086108505725861
[08/07/2025 15:13:57 INFO]: Training loss at epoch 54: 0.9512507915496826
[08/07/2025 15:14:15 INFO]: Training loss at epoch 55: 1.0236522853374481
[08/07/2025 15:14:32 INFO]: Training loss at epoch 56: 1.0214159488677979
[08/07/2025 15:14:50 INFO]: Training loss at epoch 57: 0.7830047905445099
[08/07/2025 15:15:07 INFO]: Training loss at epoch 58: 0.7873477935791016
[08/07/2025 15:15:09 INFO]: New best epoch, val score: -0.9030078138875469
[08/07/2025 15:15:09 INFO]: Saving model to: model_best.pth
[08/07/2025 15:15:25 INFO]: Training loss at epoch 59: 1.1925744414329529
[08/07/2025 15:15:31 INFO]: Training stats: {
    "score": -0.927670858603032,
    "rmse": 0.927670858603032
}
[08/07/2025 15:15:31 INFO]: Val stats: {
    "score": -0.8902561848636188,
    "rmse": 0.8902561848636188
}
[08/07/2025 15:15:31 INFO]: Test stats: {
    "score": -0.8501167784589098,
    "rmse": 0.8501167784589098
}
[08/07/2025 15:15:33 INFO]: New best epoch, val score: -0.8902561848636188
[08/07/2025 15:15:33 INFO]: Saving model to: model_best.pth
[08/07/2025 15:15:49 INFO]: Training loss at epoch 60: 1.165990948677063
[08/07/2025 15:15:51 INFO]: New best epoch, val score: -0.8870693599660635
[08/07/2025 15:15:51 INFO]: Saving model to: model_best.pth
[08/07/2025 15:16:06 INFO]: Training loss at epoch 61: 0.9018422067165375
[08/07/2025 15:16:24 INFO]: Training loss at epoch 62: 0.944253146648407
[08/07/2025 15:16:41 INFO]: Training loss at epoch 63: 0.9689694941043854
[08/07/2025 15:16:59 INFO]: Training loss at epoch 64: 0.9579753875732422
[08/07/2025 15:17:16 INFO]: Training loss at epoch 65: 1.0743421912193298
[08/07/2025 15:17:34 INFO]: Training loss at epoch 66: 0.8719326257705688
[08/07/2025 15:17:36 INFO]: New best epoch, val score: -0.8868313056997726
[08/07/2025 15:17:36 INFO]: Saving model to: model_best.pth
[08/07/2025 15:17:52 INFO]: Training loss at epoch 67: 0.728273868560791
[08/07/2025 15:17:54 INFO]: New best epoch, val score: -0.873250992747699
[08/07/2025 15:17:54 INFO]: Saving model to: model_best.pth
[08/07/2025 15:18:09 INFO]: Training loss at epoch 68: 0.9356701970100403
[08/07/2025 15:18:11 INFO]: New best epoch, val score: -0.8713641570441734
[08/07/2025 15:18:11 INFO]: Saving model to: model_best.pth
[08/07/2025 15:18:27 INFO]: Training loss at epoch 69: 0.81715327501297
[08/07/2025 15:18:33 INFO]: Training stats: {
    "score": -0.8675550354354715,
    "rmse": 0.8675550354354715
}
[08/07/2025 15:18:33 INFO]: Val stats: {
    "score": -0.882386851878337,
    "rmse": 0.882386851878337
}
[08/07/2025 15:18:33 INFO]: Test stats: {
    "score": -0.8105128382097541,
    "rmse": 0.8105128382097541
}
[08/07/2025 15:18:50 INFO]: Training loss at epoch 70: 0.9384923875331879
[08/07/2025 15:19:08 INFO]: Training loss at epoch 71: 0.8524681627750397
[08/07/2025 15:19:25 INFO]: Training loss at epoch 72: 0.8906497359275818
[08/07/2025 15:19:43 INFO]: Training loss at epoch 73: 1.0418568849563599
[08/07/2025 15:20:00 INFO]: Training loss at epoch 74: 0.7751120030879974
[08/07/2025 15:20:18 INFO]: Training loss at epoch 75: 0.7345969080924988
[08/07/2025 15:20:20 INFO]: New best epoch, val score: -0.8450385157750576
[08/07/2025 15:20:20 INFO]: Saving model to: model_best.pth
[08/07/2025 15:20:35 INFO]: Training loss at epoch 76: 0.7399032413959503
[08/07/2025 15:20:53 INFO]: Training loss at epoch 77: 0.7107994854450226
[08/07/2025 15:21:11 INFO]: Training loss at epoch 78: 0.8768516778945923
[08/07/2025 15:21:28 INFO]: Training loss at epoch 79: 0.5969565510749817
[08/07/2025 15:21:34 INFO]: Training stats: {
    "score": -0.8551512732862434,
    "rmse": 0.8551512732862434
}
[08/07/2025 15:21:34 INFO]: Val stats: {
    "score": -0.9890195966147943,
    "rmse": 0.9890195966147943
}
[08/07/2025 15:21:34 INFO]: Test stats: {
    "score": -0.8475526131797677,
    "rmse": 0.8475526131797677
}
[08/07/2025 15:21:52 INFO]: Training loss at epoch 80: 0.6642603576183319
[08/07/2025 15:22:09 INFO]: Training loss at epoch 81: 0.8224783837795258
[08/07/2025 15:22:27 INFO]: Training loss at epoch 82: 0.7584984600543976
[08/07/2025 15:22:44 INFO]: Training loss at epoch 83: 0.6870790421962738
[08/07/2025 15:23:02 INFO]: Training loss at epoch 84: 0.7292793095111847
[08/07/2025 15:23:20 INFO]: Training loss at epoch 85: 0.6784700155258179
[08/07/2025 15:23:37 INFO]: Training loss at epoch 86: 0.6383406817913055
[08/07/2025 15:23:55 INFO]: Training loss at epoch 87: 0.7252980470657349
[08/07/2025 15:24:12 INFO]: Training loss at epoch 88: 0.7597528696060181
[08/07/2025 15:24:30 INFO]: Training loss at epoch 89: 0.6547394394874573
[08/07/2025 15:24:36 INFO]: Training stats: {
    "score": -0.8828245098178034,
    "rmse": 0.8828245098178034
}
[08/07/2025 15:24:36 INFO]: Val stats: {
    "score": -0.9232199823458284,
    "rmse": 0.9232199823458284
}
[08/07/2025 15:24:36 INFO]: Test stats: {
    "score": -0.886801278113001,
    "rmse": 0.886801278113001
}
[08/07/2025 15:24:53 INFO]: Training loss at epoch 90: 0.7783740162849426
[08/07/2025 15:25:11 INFO]: Training loss at epoch 91: 0.7840421795845032
[08/07/2025 15:25:28 INFO]: Training loss at epoch 92: 0.6682246029376984
[08/07/2025 15:25:46 INFO]: Training loss at epoch 93: 0.7061501145362854
[08/07/2025 15:26:03 INFO]: Training loss at epoch 94: 0.7048298120498657
[08/07/2025 15:26:21 INFO]: Training loss at epoch 95: 0.7469041049480438
[08/07/2025 15:26:38 INFO]: Training loss at epoch 96: 0.688124030828476
[08/07/2025 15:26:56 INFO]: Training loss at epoch 97: 0.6273944675922394
[08/07/2025 15:27:13 INFO]: Training loss at epoch 98: 0.5305434465408325
[08/07/2025 15:27:30 INFO]: Training loss at epoch 99: 0.5815552026033401
[08/07/2025 15:27:37 INFO]: Training stats: {
    "score": -0.8414623741926198,
    "rmse": 0.8414623741926198
}
[08/07/2025 15:27:37 INFO]: Val stats: {
    "score": -0.8865973107824721,
    "rmse": 0.8865973107824721
}
[08/07/2025 15:27:37 INFO]: Test stats: {
    "score": -0.8470657599370967,
    "rmse": 0.8470657599370967
}
[08/07/2025 15:27:54 INFO]: Training loss at epoch 100: 0.49448320269584656
[08/07/2025 15:28:12 INFO]: Training loss at epoch 101: 0.7164882123470306
[08/07/2025 15:28:29 INFO]: Training loss at epoch 102: 0.5430474728345871
[08/07/2025 15:28:46 INFO]: Training loss at epoch 103: 0.5971062183380127
[08/07/2025 15:29:04 INFO]: Training loss at epoch 104: 0.6118948757648468
[08/07/2025 15:29:21 INFO]: Training loss at epoch 105: 0.6410908102989197
[08/07/2025 15:29:39 INFO]: Training loss at epoch 106: 0.6115448772907257
[08/07/2025 15:29:41 INFO]: Running Final Evaluation...
[08/07/2025 15:29:47 INFO]: Training accuracy: {
    "score": -0.8401591699528151,
    "rmse": 0.8401591699528151
}
[08/07/2025 15:29:47 INFO]: Val accuracy: {
    "score": -0.8450385157750576,
    "rmse": 0.8450385157750576
}
[08/07/2025 15:29:47 INFO]: Test accuracy: {
    "score": -0.7877050625420556,
    "rmse": 0.7877050625420556
}
[08/07/2025 15:29:47 INFO]: {
    "dataset": "ic_upstream3",
    "model_name": "ft_transformer",
    "run_id": "renowned-Tressie",
    "best_epoch": 75,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.7877050625420556,
        "rmse": 0.7877050625420556
    },
    "train_stats": {
        "score": -0.8401591699528151,
        "rmse": 0.8401591699528151
    },
    "val_stats": {
        "score": -0.8450385157750576,
        "rmse": 0.8450385157750576
    }
}
[08/07/2025 15:29:47 INFO]: 
_________________________________________________

[08/07/2025 15:29:47 INFO]: train_net_for_optune.py main() running.
[08/07/2025 15:29:47 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 1.806850208315033
  attention_dropout: 0.43202804743100354
  ffn_dropout: 0.43202804743100354
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.3926563077356732e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: renowned-Tressie

[08/07/2025 15:29:48 INFO]: This ft_transformer has 16.114 million parameters.
[08/07/2025 15:29:48 INFO]: Training will start at epoch 0.
[08/07/2025 15:29:48 INFO]: ==> Starting training for 200 epochs...
[08/07/2025 15:33:16 INFO]: Training loss at epoch 0: 1.0598540902137756
[08/07/2025 15:33:45 INFO]: New best epoch, val score: -0.9886934558967465
[08/07/2025 15:33:45 INFO]: Saving model to: model_best.pth
[08/07/2025 15:37:14 INFO]: Training loss at epoch 1: 1.2959018349647522
[08/07/2025 15:41:11 INFO]: Training loss at epoch 2: 1.2956019043922424
[08/07/2025 15:45:08 INFO]: Training loss at epoch 3: 1.36841881275177
[08/07/2025 15:49:05 INFO]: Training loss at epoch 4: 0.9501345753669739
[08/07/2025 15:53:02 INFO]: Training loss at epoch 5: 1.4197611212730408
[08/07/2025 15:56:59 INFO]: Training loss at epoch 6: 1.940131664276123
[08/07/2025 16:00:56 INFO]: Training loss at epoch 7: 1.2167601585388184
[08/07/2025 16:04:53 INFO]: Training loss at epoch 8: 1.229990303516388
[08/07/2025 16:08:50 INFO]: Training loss at epoch 9: 1.4524980187416077
[08/07/2025 16:10:09 INFO]: Training stats: {
    "score": -1.1164243244174055,
    "rmse": 1.1164243244174055
}
[08/07/2025 16:10:09 INFO]: Val stats: {
    "score": -0.9837414827765381,
    "rmse": 0.9837414827765381
}
[08/07/2025 16:10:09 INFO]: Test stats: {
    "score": -1.0013003549596071,
    "rmse": 1.0013003549596071
}
[08/07/2025 16:10:37 INFO]: New best epoch, val score: -0.9837414827765381
[08/07/2025 16:10:37 INFO]: Saving model to: model_best.pth
[08/07/2025 16:14:07 INFO]: Training loss at epoch 10: 1.1688544750213623
[08/07/2025 16:14:35 INFO]: New best epoch, val score: -0.9236561655733343
[08/07/2025 16:14:35 INFO]: Saving model to: model_best.pth
[08/07/2025 16:18:05 INFO]: Training loss at epoch 11: 1.2047038674354553
[08/07/2025 16:22:02 INFO]: Training loss at epoch 12: 0.8549187481403351
[08/07/2025 16:25:59 INFO]: Training loss at epoch 13: 1.138795793056488
[08/07/2025 16:29:56 INFO]: Training loss at epoch 14: 1.2034971117973328
[08/07/2025 16:33:54 INFO]: Training loss at epoch 15: 1.2898660898208618
[08/07/2025 16:37:52 INFO]: Training loss at epoch 16: 1.4241135716438293
[08/07/2025 16:41:49 INFO]: Training loss at epoch 17: 1.022026687860489
[08/07/2025 16:45:49 INFO]: Training loss at epoch 18: 1.1652217507362366
[08/07/2025 16:49:50 INFO]: Training loss at epoch 19: 1.0579291582107544
[08/07/2025 16:51:11 INFO]: Training stats: {
    "score": -0.9623938841489952,
    "rmse": 0.9623938841489952
}
[08/07/2025 16:51:11 INFO]: Val stats: {
    "score": -0.9520281074781584,
    "rmse": 0.9520281074781584
}
[08/07/2025 16:51:11 INFO]: Test stats: {
    "score": -0.9043369336478078,
    "rmse": 0.9043369336478078
}
[08/07/2025 16:55:12 INFO]: Training loss at epoch 20: 0.8560196757316589
[08/07/2025 16:55:40 INFO]: New best epoch, val score: -0.9159650582021736
[08/07/2025 16:55:40 INFO]: Saving model to: model_best.pth
[08/07/2025 16:59:13 INFO]: Training loss at epoch 21: 0.8848409950733185
[08/07/2025 16:59:42 INFO]: New best epoch, val score: -0.9051318973776102
[08/07/2025 16:59:42 INFO]: Saving model to: model_best.pth
[08/07/2025 17:03:14 INFO]: Training loss at epoch 22: 1.2876828610897064
[08/07/2025 17:07:11 INFO]: Training loss at epoch 23: 1.131527304649353
[08/07/2025 17:11:12 INFO]: Training loss at epoch 24: 0.8757362365722656
[08/07/2025 17:11:42 INFO]: New best epoch, val score: -0.9035324609612592
[08/07/2025 17:11:42 INFO]: Saving model to: model_best.pth
[08/07/2025 17:15:24 INFO]: Training loss at epoch 25: 1.1966955661773682
[08/07/2025 17:19:21 INFO]: Training loss at epoch 26: 0.9971212148666382
[08/07/2025 17:23:18 INFO]: Training loss at epoch 27: 0.9880658090114594
[08/07/2025 17:27:15 INFO]: Training loss at epoch 28: 0.9216490685939789
[08/07/2025 17:31:11 INFO]: Training loss at epoch 29: 1.037283480167389
[08/07/2025 17:32:31 INFO]: Training stats: {
    "score": -0.9705076634557686,
    "rmse": 0.9705076634557686
}
[08/07/2025 17:32:31 INFO]: Val stats: {
    "score": -0.9923957427310912,
    "rmse": 0.9923957427310912
}
[08/07/2025 17:32:31 INFO]: Test stats: {
    "score": -0.9296182553100201,
    "rmse": 0.9296182553100201
}
[08/07/2025 17:36:28 INFO]: Training loss at epoch 30: 1.0336797833442688
[08/07/2025 17:40:25 INFO]: Training loss at epoch 31: 0.9960188269615173
[08/07/2025 17:44:22 INFO]: Training loss at epoch 32: 0.9529168903827667
[08/07/2025 17:48:19 INFO]: Training loss at epoch 33: 1.0244928300380707
[08/07/2025 17:52:16 INFO]: Training loss at epoch 34: 0.9957170486450195
[08/07/2025 17:56:13 INFO]: Training loss at epoch 35: 0.8615594804286957
[08/07/2025 18:00:10 INFO]: Training loss at epoch 36: 0.8110934495925903
[08/07/2025 18:04:07 INFO]: Training loss at epoch 37: 0.847169816493988
[08/07/2025 18:04:35 INFO]: New best epoch, val score: -0.8948524696506993
[08/07/2025 18:04:35 INFO]: Saving model to: model_best.pth
[08/07/2025 18:08:06 INFO]: Training loss at epoch 38: 0.9643913209438324
[08/07/2025 18:08:34 INFO]: New best epoch, val score: -0.8889684074529794
[08/07/2025 18:08:34 INFO]: Saving model to: model_best.pth
[08/07/2025 18:12:03 INFO]: Training loss at epoch 39: 0.9444935321807861
[08/07/2025 18:13:23 INFO]: Training stats: {
    "score": -0.9225415538082797,
    "rmse": 0.9225415538082797
}
[08/07/2025 18:13:23 INFO]: Val stats: {
    "score": -0.8888256506281715,
    "rmse": 0.8888256506281715
}
[08/07/2025 18:13:23 INFO]: Test stats: {
    "score": -0.8501757191493312,
    "rmse": 0.8501757191493312
}
[08/07/2025 18:13:51 INFO]: New best epoch, val score: -0.8888256506281715
[08/07/2025 18:13:51 INFO]: Saving model to: model_best.pth
[08/07/2025 18:17:21 INFO]: Training loss at epoch 40: 0.8686879873275757
[08/07/2025 18:21:18 INFO]: Training loss at epoch 41: 0.8649352192878723
[08/07/2025 18:25:15 INFO]: Training loss at epoch 42: 0.8053331673145294
[08/07/2025 18:29:12 INFO]: Training loss at epoch 43: 0.9000978171825409
[08/07/2025 18:33:09 INFO]: Training loss at epoch 44: 0.9387243986129761
[08/07/2025 18:37:06 INFO]: Training loss at epoch 45: 0.8614121675491333
[08/07/2025 18:41:03 INFO]: Training loss at epoch 46: 0.8839340507984161
[08/07/2025 18:45:00 INFO]: Training loss at epoch 47: 0.8947750329971313
[08/07/2025 18:48:57 INFO]: Training loss at epoch 48: 0.8629084527492523
[08/07/2025 18:52:53 INFO]: Training loss at epoch 49: 0.8293355703353882
[08/07/2025 18:54:13 INFO]: Training stats: {
    "score": -0.883544672838742,
    "rmse": 0.883544672838742
}
[08/07/2025 18:54:13 INFO]: Val stats: {
    "score": -0.8861050120121172,
    "rmse": 0.8861050120121172
}
[08/07/2025 18:54:13 INFO]: Test stats: {
    "score": -0.8274624842596644,
    "rmse": 0.8274624842596644
}
[08/07/2025 18:54:42 INFO]: New best epoch, val score: -0.8861050120121172
[08/07/2025 18:54:42 INFO]: Saving model to: model_best.pth
[08/07/2025 18:58:12 INFO]: Training loss at epoch 50: 0.9760980308055878
[08/07/2025 18:58:40 INFO]: New best epoch, val score: -0.8724347548583776
[08/07/2025 18:58:40 INFO]: Saving model to: model_best.pth
[08/07/2025 19:02:09 INFO]: Training loss at epoch 51: 0.9444948434829712
[08/07/2025 19:02:38 INFO]: New best epoch, val score: -0.8681847128507796
[08/07/2025 19:02:38 INFO]: Saving model to: model_best.pth
[08/07/2025 19:06:08 INFO]: Training loss at epoch 52: 0.9044420719146729
[08/07/2025 19:10:09 INFO]: Training loss at epoch 53: 0.9831829071044922
[08/07/2025 19:14:09 INFO]: Training loss at epoch 54: 0.855789989233017
[08/07/2025 19:18:10 INFO]: Training loss at epoch 55: 0.9540828466415405
[08/07/2025 19:22:10 INFO]: Training loss at epoch 56: 0.9619393944740295
[08/07/2025 19:26:10 INFO]: Training loss at epoch 57: 0.8753862082958221
[08/07/2025 19:30:09 INFO]: Training loss at epoch 58: 0.850282222032547
[08/07/2025 19:34:09 INFO]: Training loss at epoch 59: 1.000119000673294
[08/07/2025 19:35:30 INFO]: Training stats: {
    "score": -0.8388452654161698,
    "rmse": 0.8388452654161698
}
[08/07/2025 19:35:30 INFO]: Val stats: {
    "score": -0.8614132562909319,
    "rmse": 0.8614132562909319
}
[08/07/2025 19:35:30 INFO]: Test stats: {
    "score": -0.7952773993708109,
    "rmse": 0.7952773993708109
}
[08/07/2025 19:35:58 INFO]: New best epoch, val score: -0.8614132562909319
[08/07/2025 19:35:58 INFO]: Saving model to: model_best.pth
[08/07/2025 19:39:27 INFO]: Training loss at epoch 60: 0.8778336942195892
[08/07/2025 19:39:55 INFO]: New best epoch, val score: -0.8609281935800047
[08/07/2025 19:39:55 INFO]: Saving model to: model_best.pth
[08/07/2025 19:43:25 INFO]: Training loss at epoch 61: 0.7945242524147034
[08/07/2025 19:43:53 INFO]: New best epoch, val score: -0.8564967893799652
[08/07/2025 19:43:53 INFO]: Saving model to: model_best.pth
[08/07/2025 19:47:25 INFO]: Training loss at epoch 62: 0.8796937167644501
[08/07/2025 19:47:54 INFO]: New best epoch, val score: -0.8507164565775922
[08/07/2025 19:47:54 INFO]: Saving model to: model_best.pth
[08/07/2025 19:51:26 INFO]: Training loss at epoch 63: 0.8953743577003479
[08/07/2025 19:51:54 INFO]: New best epoch, val score: -0.8449137280992405
[08/07/2025 19:51:54 INFO]: Saving model to: model_best.pth
[08/07/2025 19:55:23 INFO]: Training loss at epoch 64: 0.914864182472229
[08/07/2025 19:55:51 INFO]: New best epoch, val score: -0.8437106639513927
[08/07/2025 19:55:51 INFO]: Saving model to: model_best.pth
[08/07/2025 19:59:20 INFO]: Training loss at epoch 65: 0.7749318182468414
[08/07/2025 20:03:16 INFO]: Training loss at epoch 66: 0.8479326069355011
[08/07/2025 20:07:15 INFO]: Training loss at epoch 67: 0.724608987569809
[08/07/2025 20:11:10 INFO]: Training loss at epoch 68: 0.6022717505693436
[08/07/2025 20:15:07 INFO]: Training loss at epoch 69: 0.7966078519821167
[08/07/2025 20:16:27 INFO]: Training stats: {
    "score": -0.8221813496422703,
    "rmse": 0.8221813496422703
}
[08/07/2025 20:16:27 INFO]: Val stats: {
    "score": -0.8461453216197928,
    "rmse": 0.8461453216197928
}
[08/07/2025 20:16:27 INFO]: Test stats: {
    "score": -0.7911209696987572,
    "rmse": 0.7911209696987572
}
[08/07/2025 20:20:23 INFO]: Training loss at epoch 70: 0.8709605634212494
[08/07/2025 20:24:20 INFO]: Training loss at epoch 71: 0.8052782118320465
[08/07/2025 20:28:16 INFO]: Training loss at epoch 72: 0.7914848625659943
[08/07/2025 20:32:12 INFO]: Training loss at epoch 73: 0.8811650574207306
[08/07/2025 20:36:08 INFO]: Training loss at epoch 74: 0.7410154938697815
[08/07/2025 20:40:04 INFO]: Training loss at epoch 75: 0.6321241110563278
[08/07/2025 20:44:01 INFO]: Training loss at epoch 76: 0.6857473850250244
[08/07/2025 20:47:57 INFO]: Training loss at epoch 77: 0.9397381842136383
[08/07/2025 20:51:53 INFO]: Training loss at epoch 78: 0.63800248503685
[08/07/2025 20:55:50 INFO]: Training loss at epoch 79: 0.7265369892120361
[08/07/2025 20:57:09 INFO]: Training stats: {
    "score": -0.8486946594105855,
    "rmse": 0.8486946594105855
}
[08/07/2025 20:57:09 INFO]: Val stats: {
    "score": -0.8980580773549722,
    "rmse": 0.8980580773549722
}
[08/07/2025 20:57:09 INFO]: Test stats: {
    "score": -0.8552198597482901,
    "rmse": 0.8552198597482901
}
[08/07/2025 21:01:05 INFO]: Training loss at epoch 80: 0.7417886853218079
[08/07/2025 21:05:01 INFO]: Training loss at epoch 81: 0.691437840461731
[08/07/2025 21:08:57 INFO]: Training loss at epoch 82: 0.6697098612785339
[08/07/2025 21:12:54 INFO]: Training loss at epoch 83: 0.6327601075172424
[08/07/2025 21:16:50 INFO]: Training loss at epoch 84: 0.8263543546199799
[08/07/2025 21:20:46 INFO]: Training loss at epoch 85: 0.7194644510746002
[08/07/2025 21:24:43 INFO]: Training loss at epoch 86: 0.7323166131973267
[08/07/2025 21:28:39 INFO]: Training loss at epoch 87: 0.7606098651885986
[08/07/2025 21:32:35 INFO]: Training loss at epoch 88: 0.6840976476669312
[08/07/2025 21:36:31 INFO]: Training loss at epoch 89: 0.6762641668319702
[08/07/2025 21:37:51 INFO]: Training stats: {
    "score": -0.944179165924302,
    "rmse": 0.944179165924302
}
[08/07/2025 21:37:51 INFO]: Val stats: {
    "score": -0.9908966076075887,
    "rmse": 0.9908966076075887
}
[08/07/2025 21:37:51 INFO]: Test stats: {
    "score": -0.9875763522911419,
    "rmse": 0.9875763522911419
}
[08/07/2025 21:41:48 INFO]: Training loss at epoch 90: 0.5572117120027542
[08/07/2025 21:45:44 INFO]: Training loss at epoch 91: 0.6023541837930679
[08/07/2025 21:49:40 INFO]: Training loss at epoch 92: 0.7765121161937714
[08/07/2025 21:53:37 INFO]: Training loss at epoch 93: 0.6718550324440002
[08/07/2025 21:57:34 INFO]: Training loss at epoch 94: 0.7061618268489838
[08/07/2025 22:01:39 INFO]: Training loss at epoch 95: 0.7836255729198456
[08/07/2025 22:02:09 INFO]: Running Final Evaluation...
[08/07/2025 22:03:59 INFO]: Training accuracy: {
    "score": -0.8287848596748824,
    "rmse": 0.8287848596748824
}
[08/07/2025 22:03:59 INFO]: Val accuracy: {
    "score": -0.8437106639513927,
    "rmse": 0.8437106639513927
}
[08/07/2025 22:03:59 INFO]: Test accuracy: {
    "score": -0.7866935535984395,
    "rmse": 0.7866935535984395
}
[08/07/2025 22:03:59 INFO]: {
    "dataset": "ic_upstream3",
    "model_name": "ft_transformer",
    "run_id": "renowned-Tressie",
    "best_epoch": 64,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.7866935535984395,
        "rmse": 0.7866935535984395
    },
    "train_stats": {
        "score": -0.8287848596748824,
        "rmse": 0.8287848596748824
    },
    "val_stats": {
        "score": -0.8437106639513927,
        "rmse": 0.8437106639513927
    }
}
[08/07/2025 22:03:59 INFO]: 
_________________________________________________

[08/07/2025 22:03:59 INFO]: train_net_for_optune.py main() running.
[08/07/2025 22:03:59 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.9209836895721875
  attention_dropout: 0.248494305492181
  ffn_dropout: 0.248494305492181
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00036088704900905647
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: renowned-Tressie

[08/07/2025 22:03:59 INFO]: This ft_transformer has 5.234 million parameters.
[08/07/2025 22:03:59 INFO]: Training will start at epoch 0.
[08/07/2025 22:03:59 INFO]: ==> Starting training for 200 epochs...
[08/07/2025 22:04:42 INFO]: Training loss at epoch 0: 1.101944625377655
[08/07/2025 22:04:48 INFO]: New best epoch, val score: -0.9621071168127783
[08/07/2025 22:04:48 INFO]: Saving model to: model_best.pth
[08/07/2025 22:05:32 INFO]: Training loss at epoch 1: 1.0404402613639832
[08/07/2025 22:06:22 INFO]: Training loss at epoch 2: 1.338116466999054
[08/07/2025 22:06:28 INFO]: New best epoch, val score: -0.8487521799752706
[08/07/2025 22:06:28 INFO]: Saving model to: model_best.pth
[08/07/2025 22:07:12 INFO]: Training loss at epoch 3: 1.3397805392742157
[08/07/2025 22:08:02 INFO]: Training loss at epoch 4: 1.1638476848602295
[08/07/2025 22:08:51 INFO]: Training loss at epoch 5: 1.358577311038971
[08/07/2025 22:09:41 INFO]: Training loss at epoch 6: 0.7851601392030716
[08/07/2025 22:10:31 INFO]: Training loss at epoch 7: 1.204853892326355
[08/07/2025 22:11:24 INFO]: Training loss at epoch 8: 1.076069951057434
[08/07/2025 22:12:16 INFO]: Training loss at epoch 9: 0.9126935005187988
[08/07/2025 22:12:32 INFO]: Training stats: {
    "score": -1.0145757621480445,
    "rmse": 1.0145757621480445
}
[08/07/2025 22:12:32 INFO]: Val stats: {
    "score": -0.8998862303047216,
    "rmse": 0.8998862303047216
}
[08/07/2025 22:12:32 INFO]: Test stats: {
    "score": -0.9186555533114962,
    "rmse": 0.9186555533114962
}
[08/07/2025 22:13:22 INFO]: Training loss at epoch 10: 0.8597005605697632
[08/07/2025 22:14:12 INFO]: Training loss at epoch 11: 1.1295724213123322
[08/07/2025 22:15:01 INFO]: Training loss at epoch 12: 1.313488483428955
[08/07/2025 22:15:51 INFO]: Training loss at epoch 13: 1.0477559268474579
[08/07/2025 22:16:41 INFO]: Training loss at epoch 14: 0.7759168148040771
[08/07/2025 22:16:47 INFO]: New best epoch, val score: -0.843969351235538
[08/07/2025 22:16:47 INFO]: Saving model to: model_best.pth
[08/07/2025 22:17:31 INFO]: Training loss at epoch 15: 0.7599105834960938
[08/07/2025 22:17:37 INFO]: New best epoch, val score: -0.8402855139172893
[08/07/2025 22:17:37 INFO]: Saving model to: model_best.pth
[08/07/2025 22:18:21 INFO]: Training loss at epoch 16: 0.9674508571624756
[08/07/2025 22:19:11 INFO]: Training loss at epoch 17: 0.9970865547657013
[08/07/2025 22:20:01 INFO]: Training loss at epoch 18: 0.7033346593379974
[08/07/2025 22:20:07 INFO]: New best epoch, val score: -0.8024444614074915
[08/07/2025 22:20:07 INFO]: Saving model to: model_best.pth
[08/07/2025 22:20:51 INFO]: Training loss at epoch 19: 0.6419583261013031
[08/07/2025 22:21:09 INFO]: Training stats: {
    "score": -0.7756734984303484,
    "rmse": 0.7756734984303484
}
[08/07/2025 22:21:09 INFO]: Val stats: {
    "score": -0.8269613763330649,
    "rmse": 0.8269613763330649
}
[08/07/2025 22:21:09 INFO]: Test stats: {
    "score": -0.7449103899669078,
    "rmse": 0.7449103899669078
}
[08/07/2025 22:22:02 INFO]: Training loss at epoch 20: 0.6669056713581085
[08/07/2025 22:22:56 INFO]: Training loss at epoch 21: 0.6516728103160858
[08/07/2025 22:23:48 INFO]: Training loss at epoch 22: 0.6036438643932343
[08/07/2025 22:24:38 INFO]: Training loss at epoch 23: 0.7407907843589783
[08/07/2025 22:25:28 INFO]: Training loss at epoch 24: 0.6521042883396149
[08/07/2025 22:26:18 INFO]: Training loss at epoch 25: 0.7080981135368347
[08/07/2025 22:27:08 INFO]: Training loss at epoch 26: 0.5713693499565125
[08/07/2025 22:27:57 INFO]: Training loss at epoch 27: 0.5208289176225662
[08/07/2025 22:28:47 INFO]: Training loss at epoch 28: 0.562993198633194
[08/07/2025 22:29:37 INFO]: Training loss at epoch 29: 0.5476096272468567
[08/07/2025 22:29:54 INFO]: Training stats: {
    "score": -0.8633823044182248,
    "rmse": 0.8633823044182248
}
[08/07/2025 22:29:54 INFO]: Val stats: {
    "score": -1.0738236584093286,
    "rmse": 1.0738236584093286
}
[08/07/2025 22:29:54 INFO]: Test stats: {
    "score": -0.9446746638343783,
    "rmse": 0.9446746638343783
}
[08/07/2025 22:30:44 INFO]: Training loss at epoch 30: 0.5729112327098846
[08/07/2025 22:31:34 INFO]: Training loss at epoch 31: 0.5821373462677002
[08/07/2025 22:32:23 INFO]: Training loss at epoch 32: 0.6143116056919098
[08/07/2025 22:33:13 INFO]: Training loss at epoch 33: 0.5696914196014404
[08/07/2025 22:34:03 INFO]: Training loss at epoch 34: 0.5550210177898407
[08/07/2025 22:34:53 INFO]: Training loss at epoch 35: 0.510778546333313
[08/07/2025 22:35:42 INFO]: Training loss at epoch 36: 0.49104735255241394
[08/07/2025 22:36:32 INFO]: Training loss at epoch 37: 0.49802257120609283
[08/07/2025 22:37:22 INFO]: Training loss at epoch 38: 0.3726707696914673
[08/07/2025 22:38:12 INFO]: Training loss at epoch 39: 0.3859332352876663
[08/07/2025 22:38:29 INFO]: Training stats: {
    "score": -0.6668233658067483,
    "rmse": 0.6668233658067483
}
[08/07/2025 22:38:29 INFO]: Val stats: {
    "score": -0.8632300170027443,
    "rmse": 0.8632300170027443
}
[08/07/2025 22:38:29 INFO]: Test stats: {
    "score": -0.8125123828438024,
    "rmse": 0.8125123828438024
}
[08/07/2025 22:39:19 INFO]: Training loss at epoch 40: 0.291537132114172
[08/07/2025 22:40:09 INFO]: Training loss at epoch 41: 0.4026830643415451
[08/07/2025 22:40:58 INFO]: Training loss at epoch 42: 0.42081283032894135
[08/07/2025 22:41:47 INFO]: Training loss at epoch 43: 0.3785729557275772
[08/07/2025 22:42:36 INFO]: Training loss at epoch 44: 0.3590094596147537
[08/07/2025 22:43:25 INFO]: Training loss at epoch 45: 0.39241716265678406
[08/07/2025 22:44:14 INFO]: Training loss at epoch 46: 0.49239224195480347
[08/07/2025 22:45:04 INFO]: Training loss at epoch 47: 0.48892343044281006
[08/07/2025 22:45:53 INFO]: Training loss at epoch 48: 0.36111025512218475
[08/07/2025 22:46:43 INFO]: Training loss at epoch 49: 0.3357556164264679
[08/07/2025 22:47:00 INFO]: Training stats: {
    "score": -0.6625885296725527,
    "rmse": 0.6625885296725527
}
[08/07/2025 22:47:00 INFO]: Val stats: {
    "score": -0.8846105336577157,
    "rmse": 0.8846105336577157
}
[08/07/2025 22:47:00 INFO]: Test stats: {
    "score": -0.8532357038779119,
    "rmse": 0.8532357038779119
}
[08/07/2025 22:47:06 INFO]: Running Final Evaluation...
[08/07/2025 22:47:25 INFO]: Training accuracy: {
    "score": -0.8394145891988561,
    "rmse": 0.8394145891988561
}
[08/07/2025 22:47:25 INFO]: Val accuracy: {
    "score": -0.8024444614074915,
    "rmse": 0.8024444614074915
}
[08/07/2025 22:47:25 INFO]: Test accuracy: {
    "score": -0.7816580916129195,
    "rmse": 0.7816580916129195
}
[08/07/2025 22:47:25 INFO]: {
    "dataset": "ic_upstream3",
    "model_name": "ft_transformer",
    "run_id": "renowned-Tressie",
    "best_epoch": 18,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.7816580916129195,
        "rmse": 0.7816580916129195
    },
    "train_stats": {
        "score": -0.8394145891988561,
        "rmse": 0.8394145891988561
    },
    "val_stats": {
        "score": -0.8024444614074915,
        "rmse": 0.8024444614074915
    }
}
[08/07/2025 22:47:26 INFO]: 
_________________________________________________

[08/07/2025 22:47:26 INFO]: train_net_for_optune.py main() running.
[08/07/2025 22:47:26 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 2.238503654376808
  attention_dropout: 0.17643085320192153
  ffn_dropout: 0.17643085320192153
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0009880052130358296
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: renowned-Tressie

[08/07/2025 22:47:26 INFO]: This ft_transformer has 3.402 million parameters.
[08/07/2025 22:47:26 INFO]: Training will start at epoch 0.
[08/07/2025 22:47:26 INFO]: ==> Starting training for 200 epochs...
[08/07/2025 22:48:18 INFO]: Training loss at epoch 0: 2.10739403963089
[08/07/2025 22:48:25 INFO]: New best epoch, val score: -1.139433161282011
[08/07/2025 22:48:25 INFO]: Saving model to: model_best.pth
[08/07/2025 22:49:17 INFO]: Training loss at epoch 1: 3.618664801120758
[08/07/2025 22:50:16 INFO]: Training loss at epoch 2: 1.5835353136062622
[08/07/2025 22:50:23 INFO]: New best epoch, val score: -1.04120900701711
[08/07/2025 22:50:23 INFO]: Saving model to: model_best.pth
[08/07/2025 22:51:16 INFO]: Training loss at epoch 3: 1.3405534029006958
[08/07/2025 22:52:15 INFO]: Training loss at epoch 4: 1.3467082977294922
[08/07/2025 22:53:14 INFO]: Training loss at epoch 5: 1.1472622752189636
[08/07/2025 22:54:12 INFO]: Training loss at epoch 6: 1.1479580998420715
[08/07/2025 22:55:11 INFO]: Training loss at epoch 7: 1.121200978755951
[08/07/2025 22:56:10 INFO]: Training loss at epoch 8: 1.0993077158927917
[08/07/2025 22:57:09 INFO]: Training loss at epoch 9: 0.9477560818195343
[08/07/2025 22:57:28 INFO]: Training stats: {
    "score": -1.059929840337152,
    "rmse": 1.059929840337152
}
[08/07/2025 22:57:28 INFO]: Val stats: {
    "score": -1.0858819852573929,
    "rmse": 1.0858819852573929
}
[08/07/2025 22:57:28 INFO]: Test stats: {
    "score": -1.0288027719794206,
    "rmse": 1.0288027719794206
}
[08/07/2025 22:58:27 INFO]: Training loss at epoch 10: 1.0603354573249817
[08/07/2025 22:58:34 INFO]: New best epoch, val score: -0.9618219266723254
[08/07/2025 22:58:34 INFO]: Saving model to: model_best.pth
[08/07/2025 22:59:26 INFO]: Training loss at epoch 11: 1.0567997097969055
[08/07/2025 22:59:33 INFO]: New best epoch, val score: -0.9337723545414938
[08/07/2025 22:59:33 INFO]: Saving model to: model_best.pth
[08/07/2025 23:00:25 INFO]: Training loss at epoch 12: 1.1081594228744507
[08/07/2025 23:01:24 INFO]: Training loss at epoch 13: 1.010707288980484
[08/07/2025 23:02:23 INFO]: Training loss at epoch 14: 1.1410304307937622
[08/07/2025 23:03:22 INFO]: Training loss at epoch 15: 1.3134931921958923
[08/07/2025 23:04:21 INFO]: Training loss at epoch 16: 1.0437952876091003
[08/07/2025 23:05:19 INFO]: Training loss at epoch 17: 0.9609927237033844
[08/07/2025 23:06:18 INFO]: Training loss at epoch 18: 1.1025017499923706
[08/07/2025 23:07:17 INFO]: Training loss at epoch 19: 1.1562599539756775
[08/07/2025 23:07:37 INFO]: Training stats: {
    "score": -1.0326661536434047,
    "rmse": 1.0326661536434047
}
[08/07/2025 23:07:37 INFO]: Val stats: {
    "score": -1.042290680494717,
    "rmse": 1.042290680494717
}
[08/07/2025 23:07:37 INFO]: Test stats: {
    "score": -0.9907997884742192,
    "rmse": 0.9907997884742192
}
[08/07/2025 23:08:35 INFO]: Training loss at epoch 20: 1.2095050811767578
[08/07/2025 23:09:34 INFO]: Training loss at epoch 21: 0.9925315380096436
[08/07/2025 23:10:33 INFO]: Training loss at epoch 22: 1.0068911612033844
[08/07/2025 23:11:32 INFO]: Training loss at epoch 23: 1.2470325231552124
[08/07/2025 23:12:31 INFO]: Training loss at epoch 24: 1.0525206327438354
[08/07/2025 23:13:30 INFO]: Training loss at epoch 25: 0.9062426686286926
[08/07/2025 23:14:29 INFO]: Training loss at epoch 26: 0.8339621722698212
[08/07/2025 23:15:27 INFO]: Training loss at epoch 27: 0.991597980260849
[08/07/2025 23:15:34 INFO]: New best epoch, val score: -0.93285284793214
[08/07/2025 23:15:34 INFO]: Saving model to: model_best.pth
[08/07/2025 23:16:30 INFO]: Training loss at epoch 28: 1.0445352792739868
[08/07/2025 23:16:37 INFO]: New best epoch, val score: -0.932671709195416
[08/07/2025 23:16:37 INFO]: Saving model to: model_best.pth
[08/07/2025 23:17:29 INFO]: Training loss at epoch 29: 0.9803106784820557
[08/07/2025 23:17:49 INFO]: Training stats: {
    "score": -1.0116077134107866,
    "rmse": 1.0116077134107866
}
[08/07/2025 23:17:49 INFO]: Val stats: {
    "score": -0.9332207887321975,
    "rmse": 0.9332207887321975
}
[08/07/2025 23:17:49 INFO]: Test stats: {
    "score": -0.9193229636596603,
    "rmse": 0.9193229636596603
}
[08/07/2025 23:18:48 INFO]: Training loss at epoch 30: 1.1067164540290833
[08/07/2025 23:19:47 INFO]: Training loss at epoch 31: 1.1100225150585175
[08/07/2025 23:20:45 INFO]: Training loss at epoch 32: 0.9868573844432831
[08/07/2025 23:21:44 INFO]: Training loss at epoch 33: 0.9325902163982391
[08/07/2025 23:22:43 INFO]: Training loss at epoch 34: 1.0135555267333984
[08/07/2025 23:23:41 INFO]: Training loss at epoch 35: 1.1487215757369995
[08/07/2025 23:24:40 INFO]: Training loss at epoch 36: 1.2358902096748352
[08/07/2025 23:25:39 INFO]: Training loss at epoch 37: 0.9351425170898438
[08/07/2025 23:26:38 INFO]: Training loss at epoch 38: 0.8813053071498871
[08/07/2025 23:27:37 INFO]: Training loss at epoch 39: 1.0555697679519653
[08/07/2025 23:27:56 INFO]: Training stats: {
    "score": -0.9967639960359362,
    "rmse": 0.9967639960359362
}
[08/07/2025 23:27:56 INFO]: Val stats: {
    "score": -0.9513783269400775,
    "rmse": 0.9513783269400775
}
[08/07/2025 23:27:56 INFO]: Test stats: {
    "score": -0.9220280379848673,
    "rmse": 0.9220280379848673
}
[08/07/2025 23:28:55 INFO]: Training loss at epoch 40: 1.0078751742839813
[08/07/2025 23:29:54 INFO]: Training loss at epoch 41: 1.0369637310504913
[08/07/2025 23:30:53 INFO]: Training loss at epoch 42: 0.8531083464622498
[08/07/2025 23:31:52 INFO]: Training loss at epoch 43: 1.2467105984687805
[08/07/2025 23:32:50 INFO]: Training loss at epoch 44: 1.265297919511795
[08/07/2025 23:33:49 INFO]: Training loss at epoch 45: 1.2331580817699432
[08/07/2025 23:34:48 INFO]: Training loss at epoch 46: 1.0643959045410156
[08/07/2025 23:35:47 INFO]: Training loss at epoch 47: 1.1046475768089294
[08/07/2025 23:36:45 INFO]: Training loss at epoch 48: 1.146499514579773
[08/07/2025 23:37:44 INFO]: Training loss at epoch 49: 0.982781171798706
[08/07/2025 23:38:04 INFO]: Training stats: {
    "score": -1.0034342631791877,
    "rmse": 1.0034342631791877
}
[08/07/2025 23:38:04 INFO]: Val stats: {
    "score": -0.9882765097494538,
    "rmse": 0.9882765097494538
}
[08/07/2025 23:38:04 INFO]: Test stats: {
    "score": -0.9460587652367026,
    "rmse": 0.9460587652367026
}
[08/07/2025 23:39:03 INFO]: Training loss at epoch 50: 0.8358966112136841
[08/07/2025 23:40:01 INFO]: Training loss at epoch 51: 0.9103817343711853
[08/07/2025 23:41:00 INFO]: Training loss at epoch 52: 0.9057251513004303
[08/07/2025 23:41:59 INFO]: Training loss at epoch 53: 0.8668859302997589
[08/07/2025 23:42:58 INFO]: Training loss at epoch 54: 1.0266124904155731
[08/07/2025 23:43:57 INFO]: Training loss at epoch 55: 0.9036339521408081
[08/07/2025 23:44:55 INFO]: Training loss at epoch 56: 1.044495016336441
[08/07/2025 23:45:54 INFO]: Training loss at epoch 57: 0.9490818083286285
[08/07/2025 23:46:53 INFO]: Training loss at epoch 58: 1.124411016702652
[08/07/2025 23:47:52 INFO]: Training loss at epoch 59: 0.9826976656913757
[08/07/2025 23:48:12 INFO]: Training stats: {
    "score": -0.991821495818294,
    "rmse": 0.991821495818294
}
[08/07/2025 23:48:12 INFO]: Val stats: {
    "score": -0.9617617179518752,
    "rmse": 0.9617617179518752
}
[08/07/2025 23:48:12 INFO]: Test stats: {
    "score": -0.9256669739045911,
    "rmse": 0.9256669739045911
}
[08/07/2025 23:48:19 INFO]: Running Final Evaluation...
[08/07/2025 23:48:52 INFO]: Training accuracy: {
    "score": -1.0166274044890817,
    "rmse": 1.0166274044890817
}
[08/07/2025 23:48:52 INFO]: Val accuracy: {
    "score": -0.932671709195416,
    "rmse": 0.932671709195416
}
[08/07/2025 23:48:52 INFO]: Test accuracy: {
    "score": -0.9215459678654198,
    "rmse": 0.9215459678654198
}
[08/07/2025 23:48:52 INFO]: {
    "dataset": "ic_upstream3",
    "model_name": "ft_transformer",
    "run_id": "renowned-Tressie",
    "best_epoch": 28,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9215459678654198,
        "rmse": 0.9215459678654198
    },
    "train_stats": {
        "score": -1.0166274044890817,
        "rmse": 1.0166274044890817
    },
    "val_stats": {
        "score": -0.932671709195416,
        "rmse": 0.932671709195416
    }
}
[08/07/2025 23:48:52 INFO]: 
_________________________________________________

[08/07/2025 23:48:52 INFO]: train_net_for_optune.py main() running.
[08/07/2025 23:48:52 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 0.8267560491813991
  attention_dropout: 0.008113056273910155
  ffn_dropout: 0.008113056273910155
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0004227124263701302
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: renowned-Tressie

[08/07/2025 23:48:52 INFO]: This ft_transformer has 1.542 million parameters.
[08/07/2025 23:48:52 INFO]: Training will start at epoch 0.
[08/07/2025 23:48:52 INFO]: ==> Starting training for 200 epochs...
[08/07/2025 23:49:19 INFO]: Training loss at epoch 0: 1.1765047311782837
[08/07/2025 23:49:22 INFO]: New best epoch, val score: -0.9495174104609287
[08/07/2025 23:49:22 INFO]: Saving model to: model_best.pth
[08/07/2025 23:49:49 INFO]: Training loss at epoch 1: 3.1151353120803833
[08/07/2025 23:50:20 INFO]: Training loss at epoch 2: 1.3377007842063904
[08/07/2025 23:50:50 INFO]: Training loss at epoch 3: 1.5856915712356567
[08/07/2025 23:50:54 INFO]: New best epoch, val score: -0.9372292986337147
[08/07/2025 23:50:54 INFO]: Saving model to: model_best.pth
[08/07/2025 23:51:23 INFO]: Training loss at epoch 4: 1.004688948392868
[08/07/2025 23:51:56 INFO]: Training loss at epoch 5: 1.164933204650879
[08/07/2025 23:52:28 INFO]: Training loss at epoch 6: 1.276114821434021
[08/07/2025 23:53:01 INFO]: Training loss at epoch 7: 1.2397178411483765
[08/07/2025 23:53:34 INFO]: Training loss at epoch 8: 1.2681815028190613
[08/07/2025 23:54:07 INFO]: Training loss at epoch 9: 1.2002995014190674
[08/07/2025 23:54:18 INFO]: Training stats: {
    "score": -1.096042047621521,
    "rmse": 1.096042047621521
}
[08/07/2025 23:54:18 INFO]: Val stats: {
    "score": -1.145660505766866,
    "rmse": 1.145660505766866
}
[08/07/2025 23:54:18 INFO]: Test stats: {
    "score": -1.07992152903493,
    "rmse": 1.07992152903493
}
[08/07/2025 23:54:49 INFO]: Training loss at epoch 10: 1.1095604300498962
[08/07/2025 23:55:22 INFO]: Training loss at epoch 11: 1.0695262551307678
[08/07/2025 23:55:26 INFO]: New best epoch, val score: -0.924103752203753
[08/07/2025 23:55:26 INFO]: Saving model to: model_best.pth
[08/07/2025 23:55:53 INFO]: Training loss at epoch 12: 1.0747118592262268
[08/07/2025 23:56:23 INFO]: Training loss at epoch 13: 0.9290231466293335
[08/07/2025 23:56:53 INFO]: Training loss at epoch 14: 1.1430665254592896
[08/07/2025 23:57:24 INFO]: Training loss at epoch 15: 1.2884553670883179
[08/07/2025 23:57:55 INFO]: Training loss at epoch 16: 1.0867873430252075
[08/07/2025 23:57:59 INFO]: New best epoch, val score: -0.9199089954348799
[08/07/2025 23:57:59 INFO]: Saving model to: model_best.pth
[08/07/2025 23:58:25 INFO]: Training loss at epoch 17: 0.8708517253398895
[08/07/2025 23:58:56 INFO]: Training loss at epoch 18: 0.8977853655815125
[08/07/2025 23:59:27 INFO]: Training loss at epoch 19: 0.9124827980995178
[08/07/2025 23:59:37 INFO]: Training stats: {
    "score": -1.0021232778612796,
    "rmse": 1.0021232778612796
}
[08/07/2025 23:59:37 INFO]: Val stats: {
    "score": -1.0098338557627862,
    "rmse": 1.0098338557627862
}
[08/07/2025 23:59:37 INFO]: Test stats: {
    "score": -0.956057070781408,
    "rmse": 0.956057070781408
}
[08/08/2025 00:00:08 INFO]: Training loss at epoch 20: 0.915845662355423
[08/08/2025 00:00:38 INFO]: Training loss at epoch 21: 1.0588223338127136
[08/08/2025 00:01:09 INFO]: Training loss at epoch 22: 0.9457103610038757
[08/08/2025 00:01:39 INFO]: Training loss at epoch 23: 0.987731546163559
[08/08/2025 00:02:10 INFO]: Training loss at epoch 24: 0.80415278673172
[08/08/2025 00:02:40 INFO]: Training loss at epoch 25: 0.8123892545700073
[08/08/2025 00:03:11 INFO]: Training loss at epoch 26: 1.0782844722270966
[08/08/2025 00:03:41 INFO]: Training loss at epoch 27: 0.8390535712242126
[08/08/2025 00:03:45 INFO]: New best epoch, val score: -0.9184783738731102
[08/08/2025 00:03:45 INFO]: Saving model to: model_best.pth
[08/08/2025 00:04:12 INFO]: Training loss at epoch 28: 0.8268480896949768
[08/08/2025 00:04:16 INFO]: New best epoch, val score: -0.917493367724672
[08/08/2025 00:04:16 INFO]: Saving model to: model_best.pth
[08/08/2025 00:04:43 INFO]: Training loss at epoch 29: 0.9184469282627106
[08/08/2025 00:04:53 INFO]: Training stats: {
    "score": -0.956601363835982,
    "rmse": 0.956601363835982
}
[08/08/2025 00:04:53 INFO]: Val stats: {
    "score": -0.9166310577195427,
    "rmse": 0.9166310577195427
}
[08/08/2025 00:04:53 INFO]: Test stats: {
    "score": -0.8810865394060806,
    "rmse": 0.8810865394060806
}
[08/08/2025 00:04:57 INFO]: New best epoch, val score: -0.9166310577195427
[08/08/2025 00:04:57 INFO]: Saving model to: model_best.pth
[08/08/2025 00:05:24 INFO]: Training loss at epoch 30: 0.8246816098690033
[08/08/2025 00:05:28 INFO]: New best epoch, val score: -0.9157160707980267
[08/08/2025 00:05:28 INFO]: Saving model to: model_best.pth
[08/08/2025 00:05:55 INFO]: Training loss at epoch 31: 0.893253892660141
[08/08/2025 00:06:28 INFO]: Training loss at epoch 32: 1.2063696682453156
[08/08/2025 00:07:01 INFO]: Training loss at epoch 33: 0.8577910661697388
[08/08/2025 00:07:33 INFO]: Training loss at epoch 34: 0.8673112690448761
[08/08/2025 00:08:06 INFO]: Training loss at epoch 35: 0.9547677040100098
[08/08/2025 00:08:37 INFO]: Training loss at epoch 36: 0.7543970942497253
[08/08/2025 00:08:41 INFO]: New best epoch, val score: -0.9119500164943439
[08/08/2025 00:08:41 INFO]: Saving model to: model_best.pth
[08/08/2025 00:09:08 INFO]: Training loss at epoch 37: 0.8068018853664398
[08/08/2025 00:09:12 INFO]: New best epoch, val score: -0.8970387218666287
[08/08/2025 00:09:12 INFO]: Saving model to: model_best.pth
[08/08/2025 00:09:39 INFO]: Training loss at epoch 38: 0.7369623780250549
[08/08/2025 00:09:43 INFO]: New best epoch, val score: -0.8844737199158997
[08/08/2025 00:09:43 INFO]: Saving model to: model_best.pth
[08/08/2025 00:10:10 INFO]: Training loss at epoch 39: 0.8219477832317352
[08/08/2025 00:10:20 INFO]: Training stats: {
    "score": -0.8868316312986542,
    "rmse": 0.8868316312986542
}
[08/08/2025 00:10:20 INFO]: Val stats: {
    "score": -0.8730940877986239,
    "rmse": 0.8730940877986239
}
[08/08/2025 00:10:20 INFO]: Test stats: {
    "score": -0.8209501404549597,
    "rmse": 0.8209501404549597
}
[08/08/2025 00:10:24 INFO]: New best epoch, val score: -0.8730940877986239
[08/08/2025 00:10:24 INFO]: Saving model to: model_best.pth
[08/08/2025 00:10:51 INFO]: Training loss at epoch 40: 0.6909518837928772
[08/08/2025 00:10:55 INFO]: New best epoch, val score: -0.8720466778751224
[08/08/2025 00:10:55 INFO]: Saving model to: model_best.pth
[08/08/2025 00:11:22 INFO]: Training loss at epoch 41: 0.8918144702911377
[08/08/2025 00:11:26 INFO]: New best epoch, val score: -0.864136075165003
[08/08/2025 00:11:26 INFO]: Saving model to: model_best.pth
[08/08/2025 00:11:54 INFO]: Training loss at epoch 42: 0.7027338147163391
[08/08/2025 00:11:58 INFO]: New best epoch, val score: -0.8606829091152522
[08/08/2025 00:11:58 INFO]: Saving model to: model_best.pth
[08/08/2025 00:12:25 INFO]: Training loss at epoch 43: 0.6255978047847748
[08/08/2025 00:12:29 INFO]: New best epoch, val score: -0.8560562921473175
[08/08/2025 00:12:29 INFO]: Saving model to: model_best.pth
[08/08/2025 00:12:56 INFO]: Training loss at epoch 44: 0.7026473581790924
[08/08/2025 00:13:00 INFO]: New best epoch, val score: -0.8404941705909603
[08/08/2025 00:13:00 INFO]: Saving model to: model_best.pth
[08/08/2025 00:13:29 INFO]: Training loss at epoch 45: 0.6309599876403809
[08/08/2025 00:13:33 INFO]: New best epoch, val score: -0.8239732828936006
[08/08/2025 00:13:33 INFO]: Saving model to: model_best.pth
[08/08/2025 00:14:00 INFO]: Training loss at epoch 46: 0.7655852138996124
[08/08/2025 00:14:30 INFO]: Training loss at epoch 47: 0.5752980709075928
[08/08/2025 00:15:01 INFO]: Training loss at epoch 48: 0.6323681175708771
[08/08/2025 00:15:31 INFO]: Training loss at epoch 49: 0.5639573037624359
[08/08/2025 00:15:42 INFO]: Training stats: {
    "score": -0.7712179445876016,
    "rmse": 0.7712179445876016
}
[08/08/2025 00:15:42 INFO]: Val stats: {
    "score": -0.8642781761434735,
    "rmse": 0.8642781761434735
}
[08/08/2025 00:15:42 INFO]: Test stats: {
    "score": -0.7733059123501482,
    "rmse": 0.7733059123501482
}
[08/08/2025 00:16:12 INFO]: Training loss at epoch 50: 0.5724663138389587
[08/08/2025 00:16:43 INFO]: Training loss at epoch 51: 0.6270065009593964
[08/08/2025 00:17:13 INFO]: Training loss at epoch 52: 0.506850004196167
[08/08/2025 00:17:44 INFO]: Training loss at epoch 53: 0.6384356319904327
[08/08/2025 00:18:14 INFO]: Training loss at epoch 54: 0.5023478269577026
[08/08/2025 00:18:18 INFO]: New best epoch, val score: -0.821851603233275
[08/08/2025 00:18:18 INFO]: Saving model to: model_best.pth
[08/08/2025 00:18:46 INFO]: Training loss at epoch 55: 0.6541481614112854
[08/08/2025 00:19:16 INFO]: Training loss at epoch 56: 0.4937888979911804
[08/08/2025 00:19:20 INFO]: New best epoch, val score: -0.8100211425524497
[08/08/2025 00:19:20 INFO]: Saving model to: model_best.pth
[08/08/2025 00:19:47 INFO]: Training loss at epoch 57: 0.5717967748641968
[08/08/2025 00:19:50 INFO]: New best epoch, val score: -0.7864170998280506
[08/08/2025 00:19:50 INFO]: Saving model to: model_best.pth
[08/08/2025 00:20:17 INFO]: Training loss at epoch 58: 0.497764527797699
[08/08/2025 00:20:21 INFO]: New best epoch, val score: -0.776082434401138
[08/08/2025 00:20:21 INFO]: Saving model to: model_best.pth
[08/08/2025 00:20:48 INFO]: Training loss at epoch 59: 0.4874297082424164
[08/08/2025 00:21:00 INFO]: Training stats: {
    "score": -0.6628900248147528,
    "rmse": 0.6628900248147528
}
[08/08/2025 00:21:00 INFO]: Val stats: {
    "score": -0.7784115927688456,
    "rmse": 0.7784115927688456
}
[08/08/2025 00:21:00 INFO]: Test stats: {
    "score": -0.6645574766665407,
    "rmse": 0.6645574766665407
}
[08/08/2025 00:21:33 INFO]: Training loss at epoch 60: 0.4749196320772171
[08/08/2025 00:22:03 INFO]: Training loss at epoch 61: 0.4817112684249878
[08/08/2025 00:22:34 INFO]: Training loss at epoch 62: 0.4635569602251053
[08/08/2025 00:23:04 INFO]: Training loss at epoch 63: 0.6018220782279968
[08/08/2025 00:23:35 INFO]: Training loss at epoch 64: 0.4120526611804962
[08/08/2025 00:24:05 INFO]: Training loss at epoch 65: 0.48813050985336304
[08/08/2025 00:24:36 INFO]: Training loss at epoch 66: 0.40887582302093506
[08/08/2025 00:24:40 INFO]: New best epoch, val score: -0.7562393255472966
[08/08/2025 00:24:40 INFO]: Saving model to: model_best.pth
[08/08/2025 00:25:09 INFO]: Training loss at epoch 67: 0.43996527791023254
[08/08/2025 00:25:39 INFO]: Training loss at epoch 68: 0.4483206570148468
[08/08/2025 00:26:10 INFO]: Training loss at epoch 69: 0.3776341527700424
[08/08/2025 00:26:21 INFO]: Training stats: {
    "score": -0.6486523004370963,
    "rmse": 0.6486523004370963
}
[08/08/2025 00:26:21 INFO]: Val stats: {
    "score": -0.8253669747686995,
    "rmse": 0.8253669747686995
}
[08/08/2025 00:26:21 INFO]: Test stats: {
    "score": -0.6911089057385298,
    "rmse": 0.6911089057385298
}
[08/08/2025 00:26:51 INFO]: Training loss at epoch 70: 0.46348464488983154
[08/08/2025 00:27:22 INFO]: Training loss at epoch 71: 0.3552072048187256
[08/08/2025 00:27:26 INFO]: New best epoch, val score: -0.7527989801640209
[08/08/2025 00:27:26 INFO]: Saving model to: model_best.pth
[08/08/2025 00:27:55 INFO]: Training loss at epoch 72: 0.38700759410858154
[08/08/2025 00:27:59 INFO]: New best epoch, val score: -0.7423832864188823
[08/08/2025 00:27:59 INFO]: Saving model to: model_best.pth
[08/08/2025 00:28:28 INFO]: Training loss at epoch 73: 0.3348965048789978
[08/08/2025 00:29:00 INFO]: Training loss at epoch 74: 0.4119245558977127
[08/08/2025 00:29:33 INFO]: Training loss at epoch 75: 0.4321620762348175
[08/08/2025 00:30:06 INFO]: Training loss at epoch 76: 0.3684975653886795
[08/08/2025 00:30:38 INFO]: Training loss at epoch 77: 0.3338581621646881
[08/08/2025 00:31:11 INFO]: Training loss at epoch 78: 0.3198203295469284
[08/08/2025 00:31:44 INFO]: Training loss at epoch 79: 0.2914295345544815
[08/08/2025 00:31:56 INFO]: Training stats: {
    "score": -0.6161701307901777,
    "rmse": 0.6161701307901777
}
[08/08/2025 00:31:56 INFO]: Val stats: {
    "score": -0.7508777722746806,
    "rmse": 0.7508777722746806
}
[08/08/2025 00:31:56 INFO]: Test stats: {
    "score": -0.6591214029792278,
    "rmse": 0.6591214029792278
}
[08/08/2025 00:32:28 INFO]: Training loss at epoch 80: 0.32857581973075867
[08/08/2025 00:33:01 INFO]: Training loss at epoch 81: 0.34253622591495514
[08/08/2025 00:33:34 INFO]: Training loss at epoch 82: 0.36250221729278564
[08/08/2025 00:34:07 INFO]: Training loss at epoch 83: 0.3608831614255905
[08/08/2025 00:34:11 INFO]: New best epoch, val score: -0.737302249159714
[08/08/2025 00:34:11 INFO]: Saving model to: model_best.pth
[08/08/2025 00:34:38 INFO]: Training loss at epoch 84: 0.3942531794309616
[08/08/2025 00:34:42 INFO]: New best epoch, val score: -0.7205833691426319
[08/08/2025 00:34:42 INFO]: Saving model to: model_best.pth
[08/08/2025 00:35:09 INFO]: Training loss at epoch 85: 0.41303424537181854
[08/08/2025 00:35:12 INFO]: New best epoch, val score: -0.7180806878343832
[08/08/2025 00:35:12 INFO]: Saving model to: model_best.pth
[08/08/2025 00:35:39 INFO]: Training loss at epoch 86: 0.3349258303642273
[08/08/2025 00:36:10 INFO]: Training loss at epoch 87: 0.3318176567554474
[08/08/2025 00:36:40 INFO]: Training loss at epoch 88: 0.39407551288604736
[08/08/2025 00:37:11 INFO]: Training loss at epoch 89: 0.2884547710418701
[08/08/2025 00:37:22 INFO]: Training stats: {
    "score": -0.5636927554774401,
    "rmse": 0.5636927554774401
}
[08/08/2025 00:37:22 INFO]: Val stats: {
    "score": -0.710801618066461,
    "rmse": 0.710801618066461
}
[08/08/2025 00:37:22 INFO]: Test stats: {
    "score": -0.6434554030106835,
    "rmse": 0.6434554030106835
}
[08/08/2025 00:37:25 INFO]: New best epoch, val score: -0.710801618066461
[08/08/2025 00:37:25 INFO]: Saving model to: model_best.pth
[08/08/2025 00:37:52 INFO]: Training loss at epoch 90: 0.2815818786621094
[08/08/2025 00:37:56 INFO]: New best epoch, val score: -0.7065780434458506
[08/08/2025 00:37:56 INFO]: Saving model to: model_best.pth
[08/08/2025 00:38:23 INFO]: Training loss at epoch 91: 0.27115776389837265
[08/08/2025 00:38:27 INFO]: New best epoch, val score: -0.7063072829824036
[08/08/2025 00:38:27 INFO]: Saving model to: model_best.pth
[08/08/2025 00:38:54 INFO]: Training loss at epoch 92: 0.29564985632896423
[08/08/2025 00:38:58 INFO]: New best epoch, val score: -0.7048237656719912
[08/08/2025 00:38:58 INFO]: Saving model to: model_best.pth
[08/08/2025 00:39:25 INFO]: Training loss at epoch 93: 0.26906608790159225
[08/08/2025 00:39:55 INFO]: Training loss at epoch 94: 0.3545293062925339
[08/08/2025 00:40:26 INFO]: Training loss at epoch 95: 0.27148041129112244
[08/08/2025 00:40:56 INFO]: Training loss at epoch 96: 0.32714584469795227
[08/08/2025 00:41:27 INFO]: Training loss at epoch 97: 0.24232687056064606
[08/08/2025 00:41:57 INFO]: Training loss at epoch 98: 0.3205775022506714
[08/08/2025 00:42:28 INFO]: Training loss at epoch 99: 0.31654949486255646
[08/08/2025 00:42:38 INFO]: Training stats: {
    "score": -0.5650346149453503,
    "rmse": 0.5650346149453503
}
[08/08/2025 00:42:38 INFO]: Val stats: {
    "score": -0.7520280657733731,
    "rmse": 0.7520280657733731
}
[08/08/2025 00:42:38 INFO]: Test stats: {
    "score": -0.6626229053337415,
    "rmse": 0.6626229053337415
}
[08/08/2025 00:43:09 INFO]: Training loss at epoch 100: 0.3459465503692627
[08/08/2025 00:43:40 INFO]: Training loss at epoch 101: 0.24820013344287872
[08/08/2025 00:43:43 INFO]: New best epoch, val score: -0.6952160513035054
[08/08/2025 00:43:43 INFO]: Saving model to: model_best.pth
[08/08/2025 00:44:12 INFO]: Training loss at epoch 102: 0.4286253750324249
[08/08/2025 00:44:43 INFO]: Training loss at epoch 103: 0.31072773039340973
[08/08/2025 00:45:13 INFO]: Training loss at epoch 104: 0.3226195275783539
[08/08/2025 00:45:44 INFO]: Training loss at epoch 105: 0.3211958259344101
[08/08/2025 00:45:47 INFO]: New best epoch, val score: -0.6929058774842745
[08/08/2025 00:45:47 INFO]: Saving model to: model_best.pth
[08/08/2025 00:46:14 INFO]: Training loss at epoch 106: 0.2752923220396042
[08/08/2025 00:46:18 INFO]: New best epoch, val score: -0.6925983629916109
[08/08/2025 00:46:18 INFO]: Saving model to: model_best.pth
[08/08/2025 00:46:46 INFO]: Training loss at epoch 107: 0.2650882750749588
[08/08/2025 00:47:16 INFO]: Training loss at epoch 108: 0.3200666308403015
[08/08/2025 00:47:47 INFO]: Training loss at epoch 109: 0.26768721640110016
[08/08/2025 00:47:57 INFO]: Training stats: {
    "score": -0.5271550004288483,
    "rmse": 0.5271550004288483
}
[08/08/2025 00:47:57 INFO]: Val stats: {
    "score": -0.6922138064961856,
    "rmse": 0.6922138064961856
}
[08/08/2025 00:47:57 INFO]: Test stats: {
    "score": -0.6336566325124888,
    "rmse": 0.6336566325124888
}
[08/08/2025 00:48:01 INFO]: New best epoch, val score: -0.6922138064961856
[08/08/2025 00:48:01 INFO]: Saving model to: model_best.pth
[08/08/2025 00:48:28 INFO]: Training loss at epoch 110: 0.3386821448802948
[08/08/2025 00:48:58 INFO]: Training loss at epoch 111: 0.2283436879515648
[08/08/2025 00:49:29 INFO]: Training loss at epoch 112: 0.376626692712307
[08/08/2025 00:50:00 INFO]: Training loss at epoch 113: 0.2801557183265686
[08/08/2025 00:50:30 INFO]: Training loss at epoch 114: 0.24675564467906952
[08/08/2025 00:51:01 INFO]: Training loss at epoch 115: 0.2829621732234955
[08/08/2025 00:51:31 INFO]: Training loss at epoch 116: 0.23055562376976013
[08/08/2025 00:51:35 INFO]: New best epoch, val score: -0.688722796986962
[08/08/2025 00:51:35 INFO]: Saving model to: model_best.pth
[08/08/2025 00:52:02 INFO]: Training loss at epoch 117: 0.3248390704393387
[08/08/2025 00:52:32 INFO]: Training loss at epoch 118: 0.2533176690340042
[08/08/2025 00:53:03 INFO]: Training loss at epoch 119: 0.2578704506158829
[08/08/2025 00:53:14 INFO]: Training stats: {
    "score": -0.5001239055403068,
    "rmse": 0.5001239055403068
}
[08/08/2025 00:53:14 INFO]: Val stats: {
    "score": -0.6819757538630464,
    "rmse": 0.6819757538630464
}
[08/08/2025 00:53:14 INFO]: Test stats: {
    "score": -0.603436833042545,
    "rmse": 0.603436833042545
}
[08/08/2025 00:53:17 INFO]: New best epoch, val score: -0.6819757538630464
[08/08/2025 00:53:17 INFO]: Saving model to: model_best.pth
[08/08/2025 00:53:46 INFO]: Training loss at epoch 120: 0.2668434977531433
[08/08/2025 00:53:50 INFO]: New best epoch, val score: -0.6756400288025661
[08/08/2025 00:53:50 INFO]: Saving model to: model_best.pth
[08/08/2025 00:54:17 INFO]: Training loss at epoch 121: 0.19765867292881012
[08/08/2025 00:54:47 INFO]: Training loss at epoch 122: 0.21617397665977478
[08/08/2025 00:55:18 INFO]: Training loss at epoch 123: 0.282210610806942
[08/08/2025 00:55:48 INFO]: Training loss at epoch 124: 0.29494011402130127
[08/08/2025 00:56:19 INFO]: Training loss at epoch 125: 0.2617736905813217
[08/08/2025 00:56:50 INFO]: Training loss at epoch 126: 0.23648521304130554
[08/08/2025 00:57:20 INFO]: Training loss at epoch 127: 0.21981580555438995
[08/08/2025 00:57:51 INFO]: Training loss at epoch 128: 0.1628410890698433
[08/08/2025 00:58:21 INFO]: Training loss at epoch 129: 0.26846618205308914
[08/08/2025 00:58:32 INFO]: Training stats: {
    "score": -0.4654532871277618,
    "rmse": 0.4654532871277618
}
[08/08/2025 00:58:32 INFO]: Val stats: {
    "score": -0.685639764073694,
    "rmse": 0.685639764073694
}
[08/08/2025 00:58:32 INFO]: Test stats: {
    "score": -0.6351484828272151,
    "rmse": 0.6351484828272151
}
[08/08/2025 00:59:02 INFO]: Training loss at epoch 130: 0.2514815852046013
[08/08/2025 00:59:33 INFO]: Training loss at epoch 131: 0.19946878403425217
[08/08/2025 01:00:03 INFO]: Training loss at epoch 132: 0.2108594924211502
[08/08/2025 01:00:34 INFO]: Training loss at epoch 133: 0.24546588957309723
[08/08/2025 01:01:04 INFO]: Training loss at epoch 134: 0.2237224578857422
[08/08/2025 01:01:35 INFO]: Training loss at epoch 135: 0.24821067601442337
[08/08/2025 01:02:05 INFO]: Training loss at epoch 136: 0.1712530478835106
[08/08/2025 01:02:36 INFO]: Training loss at epoch 137: 0.23523367196321487
[08/08/2025 01:03:06 INFO]: Training loss at epoch 138: 0.20843854546546936
[08/08/2025 01:03:37 INFO]: Training loss at epoch 139: 0.23142866790294647
[08/08/2025 01:03:48 INFO]: Training stats: {
    "score": -0.44687649871517976,
    "rmse": 0.44687649871517976
}
[08/08/2025 01:03:48 INFO]: Val stats: {
    "score": -0.7010687522455604,
    "rmse": 0.7010687522455604
}
[08/08/2025 01:03:48 INFO]: Test stats: {
    "score": -0.6505140546772372,
    "rmse": 0.6505140546772372
}
[08/08/2025 01:04:18 INFO]: Training loss at epoch 140: 0.22875304520130157
[08/08/2025 01:04:22 INFO]: New best epoch, val score: -0.6682181224210612
[08/08/2025 01:04:22 INFO]: Saving model to: model_best.pth
[08/08/2025 01:04:51 INFO]: Training loss at epoch 141: 0.17106400430202484
[08/08/2025 01:05:24 INFO]: Training loss at epoch 142: 0.2208949625492096
[08/08/2025 01:05:56 INFO]: Training loss at epoch 143: 0.24561717361211777
[08/08/2025 01:06:29 INFO]: Training loss at epoch 144: 0.18955609947443008
[08/08/2025 01:07:02 INFO]: Training loss at epoch 145: 0.260395348072052
[08/08/2025 01:07:34 INFO]: Training loss at epoch 146: 0.2074427753686905
[08/08/2025 01:08:05 INFO]: Training loss at epoch 147: 0.21581751108169556
[08/08/2025 01:08:36 INFO]: Training loss at epoch 148: 0.21223656833171844
[08/08/2025 01:09:07 INFO]: Training loss at epoch 149: 0.22561313956975937
[08/08/2025 01:09:17 INFO]: Training stats: {
    "score": -0.4562513269361093,
    "rmse": 0.4562513269361093
}
[08/08/2025 01:09:17 INFO]: Val stats: {
    "score": -0.7139065309206607,
    "rmse": 0.7139065309206607
}
[08/08/2025 01:09:17 INFO]: Test stats: {
    "score": -0.6317198085318845,
    "rmse": 0.6317198085318845
}
[08/08/2025 01:09:48 INFO]: Training loss at epoch 150: 0.20441024005413055
[08/08/2025 01:10:19 INFO]: Training loss at epoch 151: 0.1761079505085945
[08/08/2025 01:10:49 INFO]: Training loss at epoch 152: 0.2161969691514969
[08/08/2025 01:11:20 INFO]: Training loss at epoch 153: 0.16596240550279617
[08/08/2025 01:11:50 INFO]: Training loss at epoch 154: 0.24322348833084106
[08/08/2025 01:12:21 INFO]: Training loss at epoch 155: 0.16486889123916626
[08/08/2025 01:12:25 INFO]: New best epoch, val score: -0.6597781026934558
[08/08/2025 01:12:25 INFO]: Saving model to: model_best.pth
[08/08/2025 01:12:52 INFO]: Training loss at epoch 156: 0.16865888983011246
[08/08/2025 01:13:22 INFO]: Training loss at epoch 157: 0.16806787252426147
[08/08/2025 01:13:53 INFO]: Training loss at epoch 158: 0.14834533631801605
[08/08/2025 01:13:56 INFO]: New best epoch, val score: -0.6593876505725965
[08/08/2025 01:13:56 INFO]: Saving model to: model_best.pth
[08/08/2025 01:14:23 INFO]: Training loss at epoch 159: 0.15291540324687958
[08/08/2025 01:14:34 INFO]: Training stats: {
    "score": -0.4038903724051611,
    "rmse": 0.4038903724051611
}
[08/08/2025 01:14:34 INFO]: Val stats: {
    "score": -0.6600545626220367,
    "rmse": 0.6600545626220367
}
[08/08/2025 01:14:34 INFO]: Test stats: {
    "score": -0.6282284738772258,
    "rmse": 0.6282284738772258
}
[08/08/2025 01:15:05 INFO]: Training loss at epoch 160: 0.17868418991565704
[08/08/2025 01:15:36 INFO]: Training loss at epoch 161: 0.13254816457629204
[08/08/2025 01:15:40 INFO]: New best epoch, val score: -0.6589060254370263
[08/08/2025 01:15:40 INFO]: Saving model to: model_best.pth
[08/08/2025 01:16:07 INFO]: Training loss at epoch 162: 0.13401372730731964
[08/08/2025 01:16:11 INFO]: New best epoch, val score: -0.6570750788880246
[08/08/2025 01:16:11 INFO]: Saving model to: model_best.pth
[08/08/2025 01:16:38 INFO]: Training loss at epoch 163: 0.1690756157040596
[08/08/2025 01:17:08 INFO]: Training loss at epoch 164: 0.16982169449329376
[08/08/2025 01:17:39 INFO]: Training loss at epoch 165: 0.15731938928365707
[08/08/2025 01:17:43 INFO]: New best epoch, val score: -0.6523941041806147
[08/08/2025 01:17:43 INFO]: Saving model to: model_best.pth
[08/08/2025 01:18:09 INFO]: Training loss at epoch 166: 0.12367428466677666
[08/08/2025 01:18:13 INFO]: New best epoch, val score: -0.6491343467462356
[08/08/2025 01:18:13 INFO]: Saving model to: model_best.pth
[08/08/2025 01:18:40 INFO]: Training loss at epoch 167: 0.17863574624061584
[08/08/2025 01:19:11 INFO]: Training loss at epoch 168: 0.15991456806659698
[08/08/2025 01:19:41 INFO]: Training loss at epoch 169: 0.15086403489112854
[08/08/2025 01:19:52 INFO]: Training stats: {
    "score": -0.4144280995009107,
    "rmse": 0.4144280995009107
}
[08/08/2025 01:19:52 INFO]: Val stats: {
    "score": -0.6562236146575626,
    "rmse": 0.6562236146575626
}
[08/08/2025 01:19:52 INFO]: Test stats: {
    "score": -0.6613040884763961,
    "rmse": 0.6613040884763961
}
[08/08/2025 01:20:22 INFO]: Training loss at epoch 170: 0.13141290843486786
[08/08/2025 01:20:25 INFO]: New best epoch, val score: -0.6418244098139961
[08/08/2025 01:20:25 INFO]: Saving model to: model_best.pth
[08/08/2025 01:20:52 INFO]: Training loss at epoch 171: 0.14787186682224274
[08/08/2025 01:21:22 INFO]: Training loss at epoch 172: 0.1375236064195633
[08/08/2025 01:21:26 INFO]: New best epoch, val score: -0.6362420927959785
[08/08/2025 01:21:26 INFO]: Saving model to: model_best.pth
[08/08/2025 01:21:52 INFO]: Training loss at epoch 173: 0.12609737738966942
[08/08/2025 01:22:22 INFO]: Training loss at epoch 174: 0.17120454460382462
[08/08/2025 01:22:52 INFO]: Training loss at epoch 175: 0.1349758803844452
[08/08/2025 01:23:23 INFO]: Training loss at epoch 176: 0.15023666620254517
[08/08/2025 01:23:53 INFO]: Training loss at epoch 177: 0.11246940866112709
[08/08/2025 01:24:23 INFO]: Training loss at epoch 178: 0.23298805207014084
[08/08/2025 01:24:53 INFO]: Training loss at epoch 179: 0.1939457356929779
[08/08/2025 01:25:03 INFO]: Training stats: {
    "score": -0.4802115799320306,
    "rmse": 0.4802115799320306
}
[08/08/2025 01:25:03 INFO]: Val stats: {
    "score": -0.7735371267881223,
    "rmse": 0.7735371267881223
}
[08/08/2025 01:25:03 INFO]: Test stats: {
    "score": -0.7150498428235214,
    "rmse": 0.7150498428235214
}
[08/08/2025 01:25:34 INFO]: Training loss at epoch 180: 0.16581998020410538
[08/08/2025 01:26:04 INFO]: Training loss at epoch 181: 0.2539077401161194
[08/08/2025 01:26:34 INFO]: Training loss at epoch 182: 0.11448794230818748
[08/08/2025 01:27:05 INFO]: Training loss at epoch 183: 0.2782263830304146
[08/08/2025 01:27:35 INFO]: Training loss at epoch 184: 0.15931278467178345
[08/08/2025 01:28:06 INFO]: Training loss at epoch 185: 0.250397153198719
[08/08/2025 01:28:10 INFO]: New best epoch, val score: -0.6344550181007853
[08/08/2025 01:28:10 INFO]: Saving model to: model_best.pth
[08/08/2025 01:28:36 INFO]: Training loss at epoch 186: 0.15476427972316742
[08/08/2025 01:29:07 INFO]: Training loss at epoch 187: 0.25338877737522125
[08/08/2025 01:29:38 INFO]: Training loss at epoch 188: 0.1417160965502262
[08/08/2025 01:30:08 INFO]: Training loss at epoch 189: 0.21839890629053116
[08/08/2025 01:30:19 INFO]: Training stats: {
    "score": -0.39229413903761146,
    "rmse": 0.39229413903761146
}
[08/08/2025 01:30:19 INFO]: Val stats: {
    "score": -0.6452165037462828,
    "rmse": 0.6452165037462828
}
[08/08/2025 01:30:19 INFO]: Test stats: {
    "score": -0.6333693033857029,
    "rmse": 0.6333693033857029
}
[08/08/2025 01:30:49 INFO]: Training loss at epoch 190: 0.17525462806224823
[08/08/2025 01:31:20 INFO]: Training loss at epoch 191: 0.14968514442443848
[08/08/2025 01:31:53 INFO]: Training loss at epoch 192: 0.15744011104106903
[08/08/2025 01:32:23 INFO]: Training loss at epoch 193: 0.14992877840995789
[08/08/2025 01:32:54 INFO]: Training loss at epoch 194: 0.1299608275294304
[08/08/2025 01:33:24 INFO]: Training loss at epoch 195: 0.11800595000386238
[08/08/2025 01:33:28 INFO]: New best epoch, val score: -0.6310893636555126
[08/08/2025 01:33:28 INFO]: Saving model to: model_best.pth
[08/08/2025 01:33:55 INFO]: Training loss at epoch 196: 0.09625989198684692
[08/08/2025 01:33:59 INFO]: New best epoch, val score: -0.6252941327161761
[08/08/2025 01:33:59 INFO]: Saving model to: model_best.pth
[08/08/2025 01:34:26 INFO]: Training loss at epoch 197: 0.12690306454896927
[08/08/2025 01:34:56 INFO]: Training loss at epoch 198: 0.08065872825682163
[08/08/2025 01:35:27 INFO]: Training loss at epoch 199: 0.11496856436133385
[08/08/2025 01:35:38 INFO]: Training stats: {
    "score": -0.34771991578273315,
    "rmse": 0.34771991578273315
}
[08/08/2025 01:35:38 INFO]: Val stats: {
    "score": -0.6492485606160884,
    "rmse": 0.6492485606160884
}
[08/08/2025 01:35:38 INFO]: Test stats: {
    "score": -0.7153981170321372,
    "rmse": 0.7153981170321372
}
[08/08/2025 01:35:41 INFO]: Running Final Evaluation...
[08/08/2025 01:35:52 INFO]: Training accuracy: {
    "score": -0.3356724481901505,
    "rmse": 0.3356724481901505
}
[08/08/2025 01:35:52 INFO]: Val accuracy: {
    "score": -0.6252941327161761,
    "rmse": 0.6252941327161761
}
[08/08/2025 01:35:52 INFO]: Test accuracy: {
    "score": -0.6354266476481477,
    "rmse": 0.6354266476481477
}
[08/08/2025 01:35:52 INFO]: {
    "dataset": "ic_upstream3",
    "model_name": "ft_transformer",
    "run_id": "renowned-Tressie",
    "best_epoch": 196,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.6354266476481477,
        "rmse": 0.6354266476481477
    },
    "train_stats": {
        "score": -0.3356724481901505,
        "rmse": 0.3356724481901505
    },
    "val_stats": {
        "score": -0.6252941327161761,
        "rmse": 0.6252941327161761
    }
}
[08/08/2025 01:35:52 INFO]: 
_________________________________________________

[08/08/2025 01:35:52 INFO]: train_net_for_optune.py main() running.
[08/08/2025 01:35:52 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 1.0806695740046905
  attention_dropout: 0.003994797697839675
  ffn_dropout: 0.003994797697839675
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00047297570555955164
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: renowned-Tressie

[08/08/2025 01:35:52 INFO]: This ft_transformer has 1.676 million parameters.
[08/08/2025 01:35:52 INFO]: Training will start at epoch 0.
[08/08/2025 01:35:52 INFO]: ==> Starting training for 200 epochs...
[08/08/2025 01:36:21 INFO]: Training loss at epoch 0: 1.237204223871231
[08/08/2025 01:36:25 INFO]: New best epoch, val score: -0.9432565744205402
[08/08/2025 01:36:25 INFO]: Saving model to: model_best.pth
[08/08/2025 01:36:53 INFO]: Training loss at epoch 1: 2.7298290729522705
[08/08/2025 01:37:25 INFO]: Training loss at epoch 2: 1.5086570978164673
[08/08/2025 01:37:57 INFO]: Training loss at epoch 3: 1.004559874534607
[08/08/2025 01:38:01 INFO]: New best epoch, val score: -0.9357524528411498
[08/08/2025 01:38:01 INFO]: Saving model to: model_best.pth
[08/08/2025 01:38:29 INFO]: Training loss at epoch 4: 1.0658348202705383
[08/08/2025 01:39:01 INFO]: Training loss at epoch 5: 0.93893963098526
[08/08/2025 01:39:34 INFO]: Training loss at epoch 6: 0.9821562767028809
[08/08/2025 01:40:06 INFO]: Training loss at epoch 7: 1.1315430998802185
[08/08/2025 01:40:38 INFO]: Training loss at epoch 8: 1.2640578150749207
[08/08/2025 01:41:11 INFO]: Training loss at epoch 9: 1.0425097942352295
[08/08/2025 01:41:22 INFO]: Training stats: {
    "score": -1.0103123905954554,
    "rmse": 1.0103123905954554
}
[08/08/2025 01:41:22 INFO]: Val stats: {
    "score": -1.0115091076885465,
    "rmse": 1.0115091076885465
}
[08/08/2025 01:41:22 INFO]: Test stats: {
    "score": -0.9619918854796802,
    "rmse": 0.9619918854796802
}
[08/08/2025 01:41:54 INFO]: Training loss at epoch 10: 0.909063845872879
[08/08/2025 01:42:26 INFO]: Training loss at epoch 11: 0.9925448894500732
[08/08/2025 01:42:30 INFO]: New best epoch, val score: -0.92367809091266
[08/08/2025 01:42:30 INFO]: Saving model to: model_best.pth
[08/08/2025 01:42:59 INFO]: Training loss at epoch 12: 1.0008591413497925
[08/08/2025 01:43:31 INFO]: Training loss at epoch 13: 1.2348386645317078
[08/08/2025 01:44:03 INFO]: Training loss at epoch 14: 1.232574462890625
[08/08/2025 01:44:07 INFO]: New best epoch, val score: -0.9229261639657342
[08/08/2025 01:44:07 INFO]: Saving model to: model_best.pth
[08/08/2025 01:44:35 INFO]: Training loss at epoch 15: 1.3699693977832794
[08/08/2025 01:45:07 INFO]: Training loss at epoch 16: 0.9952611327171326
[08/08/2025 01:45:39 INFO]: Training loss at epoch 17: 0.8837166428565979
[08/08/2025 01:46:11 INFO]: Training loss at epoch 18: 1.244556725025177
[08/08/2025 01:46:42 INFO]: Training loss at epoch 19: 1.0326560735702515
[08/08/2025 01:46:53 INFO]: Training stats: {
    "score": -0.9913235877143475,
    "rmse": 0.9913235877143475
}
[08/08/2025 01:46:53 INFO]: Val stats: {
    "score": -0.9908412184459112,
    "rmse": 0.9908412184459112
}
[08/08/2025 01:46:53 INFO]: Test stats: {
    "score": -0.9401457261855902,
    "rmse": 0.9401457261855902
}
[08/08/2025 01:47:25 INFO]: Training loss at epoch 20: 1.039837509393692
[08/08/2025 01:47:57 INFO]: Training loss at epoch 21: 0.8808226883411407
[08/08/2025 01:48:29 INFO]: Training loss at epoch 22: 0.826647162437439
[08/08/2025 01:48:33 INFO]: New best epoch, val score: -0.9139921031914564
[08/08/2025 01:48:33 INFO]: Saving model to: model_best.pth
[08/08/2025 01:49:01 INFO]: Training loss at epoch 23: 0.8525815308094025
[08/08/2025 01:49:05 INFO]: New best epoch, val score: -0.9111893575565599
[08/08/2025 01:49:05 INFO]: Saving model to: model_best.pth
[08/08/2025 01:49:33 INFO]: Training loss at epoch 24: 0.9534024298191071
[08/08/2025 01:49:37 INFO]: New best epoch, val score: -0.9095844555521582
[08/08/2025 01:49:37 INFO]: Saving model to: model_best.pth
[08/08/2025 01:50:07 INFO]: Training loss at epoch 25: 1.1783543229103088
[08/08/2025 01:50:42 INFO]: Training loss at epoch 26: 0.7971900701522827
[08/08/2025 01:51:14 INFO]: Training loss at epoch 27: 1.0588963329792023
[08/08/2025 01:51:46 INFO]: Training loss at epoch 28: 0.9103874266147614
[08/08/2025 01:52:21 INFO]: Training loss at epoch 29: 0.898728996515274
[08/08/2025 01:52:33 INFO]: Training stats: {
    "score": -0.9570693746637561,
    "rmse": 0.9570693746637561
}
[08/08/2025 01:52:33 INFO]: Val stats: {
    "score": -0.9536626032090301,
    "rmse": 0.9536626032090301
}
[08/08/2025 01:52:33 INFO]: Test stats: {
    "score": -0.9015833709226102,
    "rmse": 0.9015833709226102
}
[08/08/2025 01:53:06 INFO]: Training loss at epoch 30: 1.0992894768714905
[08/08/2025 01:53:38 INFO]: Training loss at epoch 31: 1.0294696688652039
[08/08/2025 01:54:11 INFO]: Training loss at epoch 32: 0.9281558394432068
[08/08/2025 01:54:43 INFO]: Training loss at epoch 33: 0.7618288695812225
[08/08/2025 01:54:47 INFO]: New best epoch, val score: -0.9059859860465805
[08/08/2025 01:54:47 INFO]: Saving model to: model_best.pth
[08/08/2025 01:55:15 INFO]: Training loss at epoch 34: 0.9236788749694824
[08/08/2025 01:55:19 INFO]: New best epoch, val score: -0.8968003404049044
[08/08/2025 01:55:19 INFO]: Saving model to: model_best.pth
[08/08/2025 01:55:50 INFO]: Training loss at epoch 35: 0.8654977977275848
[08/08/2025 01:55:54 INFO]: New best epoch, val score: -0.8891660414244917
[08/08/2025 01:55:54 INFO]: Saving model to: model_best.pth
[08/08/2025 01:56:22 INFO]: Training loss at epoch 36: 0.8157537281513214
[08/08/2025 01:56:26 INFO]: New best epoch, val score: -0.8867254954295376
[08/08/2025 01:56:26 INFO]: Saving model to: model_best.pth
[08/08/2025 01:56:55 INFO]: Training loss at epoch 37: 0.7651467621326447
[08/08/2025 01:57:27 INFO]: Training loss at epoch 38: 0.8177812695503235
[08/08/2025 01:57:59 INFO]: Training loss at epoch 39: 0.7616240680217743
[08/08/2025 01:58:10 INFO]: Training stats: {
    "score": -0.8310473941219659,
    "rmse": 0.8310473941219659
}
[08/08/2025 01:58:10 INFO]: Val stats: {
    "score": -0.8659082312305266,
    "rmse": 0.8659082312305266
}
[08/08/2025 01:58:10 INFO]: Test stats: {
    "score": -0.79029225360296,
    "rmse": 0.79029225360296
}
[08/08/2025 01:58:14 INFO]: New best epoch, val score: -0.8659082312305266
[08/08/2025 01:58:14 INFO]: Saving model to: model_best.pth
[08/08/2025 01:58:42 INFO]: Training loss at epoch 40: 0.6473950147628784
[08/08/2025 01:58:46 INFO]: New best epoch, val score: -0.8609972223408449
[08/08/2025 01:58:46 INFO]: Saving model to: model_best.pth
[08/08/2025 01:59:14 INFO]: Training loss at epoch 41: 0.5882770717144012
[08/08/2025 01:59:18 INFO]: New best epoch, val score: -0.8338144262563739
[08/08/2025 01:59:18 INFO]: Saving model to: model_best.pth
[08/08/2025 01:59:47 INFO]: Training loss at epoch 42: 0.7461998462677002
[08/08/2025 02:00:19 INFO]: Training loss at epoch 43: 0.7046320736408234
[08/08/2025 02:00:51 INFO]: Training loss at epoch 44: 0.6456112265586853
[08/08/2025 02:01:23 INFO]: Training loss at epoch 45: 0.6772482991218567
[08/08/2025 02:01:55 INFO]: Training loss at epoch 46: 0.642823725938797
[08/08/2025 02:02:27 INFO]: Training loss at epoch 47: 0.5910831391811371
[08/08/2025 02:03:00 INFO]: Training loss at epoch 48: 0.7405923902988434
[08/08/2025 02:03:32 INFO]: Training loss at epoch 49: 0.558409184217453
[08/08/2025 02:03:43 INFO]: Training stats: {
    "score": -0.7570022461515719,
    "rmse": 0.7570022461515719
}
[08/08/2025 02:03:43 INFO]: Val stats: {
    "score": -0.8511964761114651,
    "rmse": 0.8511964761114651
}
[08/08/2025 02:03:43 INFO]: Test stats: {
    "score": -0.7705096210471324,
    "rmse": 0.7705096210471324
}
[08/08/2025 02:04:15 INFO]: Training loss at epoch 50: 0.5588272213935852
[08/08/2025 02:04:47 INFO]: Training loss at epoch 51: 0.5348799377679825
[08/08/2025 02:05:19 INFO]: Training loss at epoch 52: 0.5712587237358093
[08/08/2025 02:05:23 INFO]: New best epoch, val score: -0.8040518353574952
[08/08/2025 02:05:23 INFO]: Saving model to: model_best.pth
[08/08/2025 02:05:52 INFO]: Training loss at epoch 53: 0.5298406332731247
[08/08/2025 02:05:56 INFO]: New best epoch, val score: -0.7947185126207663
[08/08/2025 02:05:56 INFO]: Saving model to: model_best.pth
[08/08/2025 02:06:24 INFO]: Training loss at epoch 54: 0.6310696005821228
[08/08/2025 02:06:56 INFO]: Training loss at epoch 55: 0.6834074854850769
[08/08/2025 02:07:28 INFO]: Training loss at epoch 56: 0.4344615787267685
[08/08/2025 02:08:01 INFO]: Training loss at epoch 57: 0.4707721322774887
[08/08/2025 02:08:05 INFO]: New best epoch, val score: -0.7856704912309234
[08/08/2025 02:08:05 INFO]: Saving model to: model_best.pth
[08/08/2025 02:08:33 INFO]: Training loss at epoch 58: 0.43624794483184814
[08/08/2025 02:08:37 INFO]: New best epoch, val score: -0.7818153088715774
[08/08/2025 02:08:37 INFO]: Saving model to: model_best.pth
[08/08/2025 02:09:05 INFO]: Training loss at epoch 59: 0.3755144998431206
[08/08/2025 02:09:17 INFO]: Training stats: {
    "score": -0.729165988163548,
    "rmse": 0.729165988163548
}
[08/08/2025 02:09:17 INFO]: Val stats: {
    "score": -0.776743055290718,
    "rmse": 0.776743055290718
}
[08/08/2025 02:09:17 INFO]: Test stats: {
    "score": -0.7397363410974398,
    "rmse": 0.7397363410974398
}
[08/08/2025 02:09:21 INFO]: New best epoch, val score: -0.776743055290718
[08/08/2025 02:09:21 INFO]: Saving model to: model_best.pth
[08/08/2025 02:09:49 INFO]: Training loss at epoch 60: 0.3918919637799263
[08/08/2025 02:10:21 INFO]: Training loss at epoch 61: 0.5162204205989838
[08/08/2025 02:10:53 INFO]: Training loss at epoch 62: 0.45906205475330353
[08/08/2025 02:11:25 INFO]: Training loss at epoch 63: 0.5180107057094574
[08/08/2025 02:11:57 INFO]: Training loss at epoch 64: 0.4756762236356735
[08/08/2025 02:12:01 INFO]: New best epoch, val score: -0.7757831935371751
[08/08/2025 02:12:01 INFO]: Saving model to: model_best.pth
[08/08/2025 02:12:32 INFO]: Training loss at epoch 65: 0.4317430257797241
[08/08/2025 02:12:36 INFO]: New best epoch, val score: -0.7757179381112314
[08/08/2025 02:12:36 INFO]: Saving model to: model_best.pth
[08/08/2025 02:13:04 INFO]: Training loss at epoch 66: 0.5198087990283966
[08/08/2025 02:13:36 INFO]: Training loss at epoch 67: 0.364915668964386
[08/08/2025 02:14:11 INFO]: Training loss at epoch 68: 0.4266716539859772
[08/08/2025 02:14:45 INFO]: Training loss at epoch 69: 0.4763830602169037
[08/08/2025 02:14:57 INFO]: Training stats: {
    "score": -0.651638004071479,
    "rmse": 0.651638004071479
}
[08/08/2025 02:14:57 INFO]: Val stats: {
    "score": -0.7679258382726424,
    "rmse": 0.7679258382726424
}
[08/08/2025 02:14:57 INFO]: Test stats: {
    "score": -0.689437784821275,
    "rmse": 0.689437784821275
}
[08/08/2025 02:15:01 INFO]: New best epoch, val score: -0.7679258382726424
[08/08/2025 02:15:01 INFO]: Saving model to: model_best.pth
[08/08/2025 02:15:31 INFO]: Training loss at epoch 70: 0.4367169141769409
[08/08/2025 02:16:06 INFO]: Training loss at epoch 71: 0.402863085269928
[08/08/2025 02:16:40 INFO]: Training loss at epoch 72: 0.3700656145811081
[08/08/2025 02:17:13 INFO]: Training loss at epoch 73: 0.42355313897132874
[08/08/2025 02:17:17 INFO]: New best epoch, val score: -0.7514964942497414
[08/08/2025 02:17:17 INFO]: Saving model to: model_best.pth
[08/08/2025 02:17:45 INFO]: Training loss at epoch 74: 0.3052825555205345
[08/08/2025 02:17:49 INFO]: New best epoch, val score: -0.7426924316610553
[08/08/2025 02:17:49 INFO]: Saving model to: model_best.pth
[08/08/2025 02:18:17 INFO]: Training loss at epoch 75: 0.5149649232625961
[08/08/2025 02:18:21 INFO]: New best epoch, val score: -0.7413113747865473
[08/08/2025 02:18:21 INFO]: Saving model to: model_best.pth
[08/08/2025 02:18:50 INFO]: Training loss at epoch 76: 0.5139595717191696
[08/08/2025 02:19:22 INFO]: Training loss at epoch 77: 0.5243319869041443
[08/08/2025 02:19:54 INFO]: Training loss at epoch 78: 0.39885368943214417
[08/08/2025 02:20:26 INFO]: Training loss at epoch 79: 0.42375142872333527
[08/08/2025 02:20:37 INFO]: Training stats: {
    "score": -0.613489982473657,
    "rmse": 0.613489982473657
}
[08/08/2025 02:20:37 INFO]: Val stats: {
    "score": -0.7633483721222184,
    "rmse": 0.7633483721222184
}
[08/08/2025 02:20:37 INFO]: Test stats: {
    "score": -0.6668022206734029,
    "rmse": 0.6668022206734029
}
[08/08/2025 02:21:09 INFO]: Training loss at epoch 80: 0.3317182660102844
[08/08/2025 02:21:41 INFO]: Training loss at epoch 81: 0.40236544609069824
[08/08/2025 02:22:16 INFO]: Training loss at epoch 82: 0.3681112825870514
[08/08/2025 02:22:48 INFO]: Training loss at epoch 83: 0.424874022603035
[08/08/2025 02:22:52 INFO]: New best epoch, val score: -0.7405213336812844
[08/08/2025 02:22:52 INFO]: Saving model to: model_best.pth
[08/08/2025 02:23:20 INFO]: Training loss at epoch 84: 0.2835719957947731
[08/08/2025 02:23:52 INFO]: Training loss at epoch 85: 0.2746472880244255
[08/08/2025 02:23:56 INFO]: New best epoch, val score: -0.7345459882742452
[08/08/2025 02:23:56 INFO]: Saving model to: model_best.pth
[08/08/2025 02:24:25 INFO]: Training loss at epoch 86: 0.3170134276151657
[08/08/2025 02:24:29 INFO]: New best epoch, val score: -0.7209485238135109
[08/08/2025 02:24:29 INFO]: Saving model to: model_best.pth
[08/08/2025 02:24:57 INFO]: Training loss at epoch 87: 0.4238744080066681
[08/08/2025 02:25:29 INFO]: Training loss at epoch 88: 0.3553914725780487
[08/08/2025 02:26:01 INFO]: Training loss at epoch 89: 0.276189424097538
[08/08/2025 02:26:12 INFO]: Training stats: {
    "score": -0.5874377746157697,
    "rmse": 0.5874377746157697
}
[08/08/2025 02:26:12 INFO]: Val stats: {
    "score": -0.7125358792858214,
    "rmse": 0.7125358792858214
}
[08/08/2025 02:26:12 INFO]: Test stats: {
    "score": -0.6630095448452676,
    "rmse": 0.6630095448452676
}
[08/08/2025 02:26:16 INFO]: New best epoch, val score: -0.7125358792858214
[08/08/2025 02:26:16 INFO]: Saving model to: model_best.pth
[08/08/2025 02:26:45 INFO]: Training loss at epoch 90: 0.3226113021373749
[08/08/2025 02:27:17 INFO]: Training loss at epoch 91: 0.4189372658729553
[08/08/2025 02:27:49 INFO]: Training loss at epoch 92: 0.3200533390045166
[08/08/2025 02:28:21 INFO]: Training loss at epoch 93: 0.32590489089488983
[08/08/2025 02:28:53 INFO]: Training loss at epoch 94: 0.3974847048521042
[08/08/2025 02:28:57 INFO]: New best epoch, val score: -0.7065985125582424
[08/08/2025 02:28:57 INFO]: Saving model to: model_best.pth
[08/08/2025 02:29:25 INFO]: Training loss at epoch 95: 0.38198575377464294
[08/08/2025 02:29:29 INFO]: New best epoch, val score: -0.6938127764231704
[08/08/2025 02:29:29 INFO]: Saving model to: model_best.pth
[08/08/2025 02:29:58 INFO]: Training loss at epoch 96: 0.30883200466632843
[08/08/2025 02:30:30 INFO]: Training loss at epoch 97: 0.36410611867904663
[08/08/2025 02:31:02 INFO]: Training loss at epoch 98: 0.36927075684070587
[08/08/2025 02:31:06 INFO]: New best epoch, val score: -0.6855289528819928
[08/08/2025 02:31:06 INFO]: Saving model to: model_best.pth
[08/08/2025 02:31:34 INFO]: Training loss at epoch 99: 0.3469228148460388
[08/08/2025 02:31:46 INFO]: Training stats: {
    "score": -0.5491098295342268,
    "rmse": 0.5491098295342268
}
[08/08/2025 02:31:46 INFO]: Val stats: {
    "score": -0.6824835422184734,
    "rmse": 0.6824835422184734
}
[08/08/2025 02:31:46 INFO]: Test stats: {
    "score": -0.645105382286502,
    "rmse": 0.645105382286502
}
[08/08/2025 02:31:49 INFO]: New best epoch, val score: -0.6824835422184734
[08/08/2025 02:31:49 INFO]: Saving model to: model_best.pth
[08/08/2025 02:32:18 INFO]: Training loss at epoch 100: 0.3044353276491165
[08/08/2025 02:32:50 INFO]: Training loss at epoch 101: 0.2679264172911644
[08/08/2025 02:32:54 INFO]: New best epoch, val score: -0.6750038298120842
[08/08/2025 02:32:54 INFO]: Saving model to: model_best.pth
[08/08/2025 02:33:24 INFO]: Training loss at epoch 102: 0.2409667819738388
[08/08/2025 02:33:29 INFO]: New best epoch, val score: -0.6696635251812341
[08/08/2025 02:33:29 INFO]: Saving model to: model_best.pth
[08/08/2025 02:33:59 INFO]: Training loss at epoch 103: 0.31354349851608276
[08/08/2025 02:34:34 INFO]: Training loss at epoch 104: 0.22457818686962128
[08/08/2025 02:35:08 INFO]: Training loss at epoch 105: 0.3251570016145706
[08/08/2025 02:35:42 INFO]: Training loss at epoch 106: 0.3689628392457962
[08/08/2025 02:36:17 INFO]: Training loss at epoch 107: 0.3349510431289673
[08/08/2025 02:36:51 INFO]: Training loss at epoch 108: 0.4861317276954651
[08/08/2025 02:37:26 INFO]: Training loss at epoch 109: 0.42826078832149506
[08/08/2025 02:37:37 INFO]: Training stats: {
    "score": -0.5842855957443801,
    "rmse": 0.5842855957443801
}
[08/08/2025 02:37:37 INFO]: Val stats: {
    "score": -0.6668340963524655,
    "rmse": 0.6668340963524655
}
[08/08/2025 02:37:37 INFO]: Test stats: {
    "score": -0.6878371900620579,
    "rmse": 0.6878371900620579
}
[08/08/2025 02:37:41 INFO]: New best epoch, val score: -0.6668340963524655
[08/08/2025 02:37:41 INFO]: Saving model to: model_best.pth
[08/08/2025 02:38:09 INFO]: Training loss at epoch 110: 0.25346939265727997
[08/08/2025 02:38:41 INFO]: Training loss at epoch 111: 0.26863202452659607
[08/08/2025 02:39:13 INFO]: Training loss at epoch 112: 0.2560412734746933
[08/08/2025 02:39:17 INFO]: New best epoch, val score: -0.660253374072075
[08/08/2025 02:39:17 INFO]: Saving model to: model_best.pth
[08/08/2025 02:39:45 INFO]: Training loss at epoch 113: 0.27404700219631195
[08/08/2025 02:39:49 INFO]: New best epoch, val score: -0.6573146650946984
[08/08/2025 02:39:49 INFO]: Saving model to: model_best.pth
[08/08/2025 02:40:17 INFO]: Training loss at epoch 114: 0.23402996361255646
[08/08/2025 02:40:49 INFO]: Training loss at epoch 115: 0.26625604927539825
[08/08/2025 02:41:21 INFO]: Training loss at epoch 116: 0.2709844559431076
[08/08/2025 02:41:52 INFO]: Training loss at epoch 117: 0.3005493953824043
[08/08/2025 02:41:56 INFO]: New best epoch, val score: -0.655040692499072
[08/08/2025 02:41:56 INFO]: Saving model to: model_best.pth
[08/08/2025 02:42:24 INFO]: Training loss at epoch 118: 0.23922884464263916
[08/08/2025 02:42:56 INFO]: Training loss at epoch 119: 0.2780276909470558
[08/08/2025 02:43:07 INFO]: Training stats: {
    "score": -0.52968921142644,
    "rmse": 0.52968921142644
}
[08/08/2025 02:43:07 INFO]: Val stats: {
    "score": -0.7533557489495074,
    "rmse": 0.7533557489495074
}
[08/08/2025 02:43:07 INFO]: Test stats: {
    "score": -0.7521294044952959,
    "rmse": 0.7521294044952959
}
[08/08/2025 02:43:39 INFO]: Training loss at epoch 120: 0.26253676414489746
[08/08/2025 02:44:10 INFO]: Training loss at epoch 121: 0.21898528188467026
[08/08/2025 02:44:43 INFO]: Training loss at epoch 122: 0.268312007188797
[08/08/2025 02:45:15 INFO]: Training loss at epoch 123: 0.2165704295039177
[08/08/2025 02:45:46 INFO]: Training loss at epoch 124: 0.2219276875257492
[08/08/2025 02:46:18 INFO]: Training loss at epoch 125: 0.18990951776504517
[08/08/2025 02:46:50 INFO]: Training loss at epoch 126: 0.21907100081443787
[08/08/2025 02:47:21 INFO]: Training loss at epoch 127: 0.2191062793135643
[08/08/2025 02:47:53 INFO]: Training loss at epoch 128: 0.27612458169460297
[08/08/2025 02:48:25 INFO]: Training loss at epoch 129: 0.2075963020324707
[08/08/2025 02:48:36 INFO]: Training stats: {
    "score": -0.4661766391235606,
    "rmse": 0.4661766391235606
}
[08/08/2025 02:48:36 INFO]: Val stats: {
    "score": -0.7289994020043291,
    "rmse": 0.7289994020043291
}
[08/08/2025 02:48:36 INFO]: Test stats: {
    "score": -0.7220923160591373,
    "rmse": 0.7220923160591373
}
[08/08/2025 02:49:08 INFO]: Training loss at epoch 130: 0.17957376688718796
[08/08/2025 02:49:39 INFO]: Training loss at epoch 131: 0.20929799228906631
[08/08/2025 02:50:11 INFO]: Training loss at epoch 132: 0.21767649799585342
[08/08/2025 02:50:43 INFO]: Training loss at epoch 133: 0.24750228226184845
[08/08/2025 02:51:15 INFO]: Training loss at epoch 134: 0.17747677862644196
[08/08/2025 02:51:46 INFO]: Training loss at epoch 135: 0.28981201350688934
[08/08/2025 02:52:18 INFO]: Training loss at epoch 136: 0.27613896131515503
[08/08/2025 02:52:50 INFO]: Training loss at epoch 137: 0.25740673393011093
[08/08/2025 02:53:22 INFO]: Training loss at epoch 138: 0.25304006040096283
[08/08/2025 02:53:26 INFO]: New best epoch, val score: -0.6520325446183046
[08/08/2025 02:53:26 INFO]: Saving model to: model_best.pth
[08/08/2025 02:53:53 INFO]: Training loss at epoch 139: 0.2142072319984436
[08/08/2025 02:54:05 INFO]: Training stats: {
    "score": -0.4610073500437948,
    "rmse": 0.4610073500437948
}
[08/08/2025 02:54:05 INFO]: Val stats: {
    "score": -0.7173005161850379,
    "rmse": 0.7173005161850379
}
[08/08/2025 02:54:05 INFO]: Test stats: {
    "score": -0.7035045597453815,
    "rmse": 0.7035045597453815
}
[08/08/2025 02:54:36 INFO]: Training loss at epoch 140: 0.22673244774341583
[08/08/2025 02:55:08 INFO]: Training loss at epoch 141: 0.22138497978448868
[08/08/2025 02:55:40 INFO]: Training loss at epoch 142: 0.2148008793592453
[08/08/2025 02:56:11 INFO]: Training loss at epoch 143: 0.13835593312978745
[08/08/2025 02:56:43 INFO]: Training loss at epoch 144: 0.23282115161418915
[08/08/2025 02:57:15 INFO]: Training loss at epoch 145: 0.16738378256559372
[08/08/2025 02:57:47 INFO]: Training loss at epoch 146: 0.16929659247398376
[08/08/2025 02:58:18 INFO]: Training loss at epoch 147: 0.18831508606672287
[08/08/2025 02:58:50 INFO]: Training loss at epoch 148: 0.17922575026750565
[08/08/2025 02:59:22 INFO]: Training loss at epoch 149: 0.1413862630724907
[08/08/2025 02:59:33 INFO]: Training stats: {
    "score": -0.40112655027117927,
    "rmse": 0.40112655027117927
}
[08/08/2025 02:59:33 INFO]: Val stats: {
    "score": -0.7225073607427912,
    "rmse": 0.7225073607427912
}
[08/08/2025 02:59:33 INFO]: Test stats: {
    "score": -0.7388274198519567,
    "rmse": 0.7388274198519567
}
[08/08/2025 03:00:05 INFO]: Training loss at epoch 150: 0.10895208455622196
[08/08/2025 03:00:36 INFO]: Training loss at epoch 151: 0.20812131464481354
[08/08/2025 03:01:08 INFO]: Training loss at epoch 152: 0.14022138714790344
[08/08/2025 03:01:40 INFO]: Training loss at epoch 153: 0.18033738434314728
[08/08/2025 03:02:12 INFO]: Training loss at epoch 154: 0.10666496120393276
[08/08/2025 03:02:43 INFO]: Training loss at epoch 155: 0.15643319487571716
[08/08/2025 03:03:15 INFO]: Training loss at epoch 156: 0.1434302181005478
[08/08/2025 03:03:47 INFO]: Training loss at epoch 157: 0.14290801435709
[08/08/2025 03:04:18 INFO]: Training loss at epoch 158: 0.13896317780017853
[08/08/2025 03:04:50 INFO]: Training loss at epoch 159: 0.12518799677491188
[08/08/2025 03:05:01 INFO]: Training stats: {
    "score": -0.36836787805309856,
    "rmse": 0.36836787805309856
}
[08/08/2025 03:05:01 INFO]: Val stats: {
    "score": -0.6722800740827108,
    "rmse": 0.6722800740827108
}
[08/08/2025 03:05:01 INFO]: Test stats: {
    "score": -0.722985177416953,
    "rmse": 0.722985177416953
}
[08/08/2025 03:05:33 INFO]: Training loss at epoch 160: 0.12184305489063263
[08/08/2025 03:06:05 INFO]: Training loss at epoch 161: 0.13971668481826782
[08/08/2025 03:06:36 INFO]: Training loss at epoch 162: 0.13281334936618805
[08/08/2025 03:07:08 INFO]: Training loss at epoch 163: 0.12374211102724075
[08/08/2025 03:07:40 INFO]: Training loss at epoch 164: 0.10382147133350372
[08/08/2025 03:08:12 INFO]: Training loss at epoch 165: 0.11301643401384354
[08/08/2025 03:08:43 INFO]: Training loss at epoch 166: 0.15951179713010788
[08/08/2025 03:09:15 INFO]: Training loss at epoch 167: 0.16098187491297722
[08/08/2025 03:09:47 INFO]: Training loss at epoch 168: 0.11072097718715668
[08/08/2025 03:10:18 INFO]: Training loss at epoch 169: 0.13487976044416428
[08/08/2025 03:10:29 INFO]: Training stats: {
    "score": -0.36614155199587733,
    "rmse": 0.36614155199587733
}
[08/08/2025 03:10:29 INFO]: Val stats: {
    "score": -0.6690824160520301,
    "rmse": 0.6690824160520301
}
[08/08/2025 03:10:29 INFO]: Test stats: {
    "score": -0.7513239420671955,
    "rmse": 0.7513239420671955
}
[08/08/2025 03:10:33 INFO]: Running Final Evaluation...
[08/08/2025 03:10:44 INFO]: Training accuracy: {
    "score": -0.45571512253458457,
    "rmse": 0.45571512253458457
}
[08/08/2025 03:10:44 INFO]: Val accuracy: {
    "score": -0.6520325446183046,
    "rmse": 0.6520325446183046
}
[08/08/2025 03:10:44 INFO]: Test accuracy: {
    "score": -0.6776821278169068,
    "rmse": 0.6776821278169068
}
[08/08/2025 03:10:44 INFO]: {
    "dataset": "ic_upstream3",
    "model_name": "ft_transformer",
    "run_id": "renowned-Tressie",
    "best_epoch": 138,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.6776821278169068,
        "rmse": 0.6776821278169068
    },
    "train_stats": {
        "score": -0.45571512253458457,
        "rmse": 0.45571512253458457
    },
    "val_stats": {
        "score": -0.6520325446183046,
        "rmse": 0.6520325446183046
    }
}
[08/08/2025 03:10:44 INFO]: 
_________________________________________________

[08/08/2025 03:10:44 INFO]: train_net_for_optune.py main() running.
[08/08/2025 03:10:44 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 1.110522510965641
  attention_dropout: 0.005609996861745006
  ffn_dropout: 0.005609996861745006
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.000608378850699488
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: renowned-Tressie

[08/08/2025 03:10:45 INFO]: This ft_transformer has 1.692 million parameters.
[08/08/2025 03:10:45 INFO]: Training will start at epoch 0.
[08/08/2025 03:10:45 INFO]: ==> Starting training for 200 epochs...
[08/08/2025 03:11:13 INFO]: Training loss at epoch 0: 0.9775057733058929
[08/08/2025 03:11:16 INFO]: New best epoch, val score: -1.0192833770886358
[08/08/2025 03:11:16 INFO]: Saving model to: model_best.pth
[08/08/2025 03:11:45 INFO]: Training loss at epoch 1: 2.3126050233840942
[08/08/2025 03:12:16 INFO]: Training loss at epoch 2: 1.4518945813179016
[08/08/2025 03:12:48 INFO]: Training loss at epoch 3: 1.2177748680114746
[08/08/2025 03:12:52 INFO]: New best epoch, val score: -0.9780999510570426
[08/08/2025 03:12:52 INFO]: Saving model to: model_best.pth
[08/08/2025 03:13:20 INFO]: Training loss at epoch 4: 0.9175839424133301
[08/08/2025 03:13:24 INFO]: New best epoch, val score: -0.9469588705909207
[08/08/2025 03:13:24 INFO]: Saving model to: model_best.pth
[08/08/2025 03:13:55 INFO]: Training loss at epoch 5: 1.239262044429779
[08/08/2025 03:13:59 INFO]: New best epoch, val score: -0.9458387374432926
[08/08/2025 03:13:59 INFO]: Saving model to: model_best.pth
[08/08/2025 03:14:27 INFO]: Training loss at epoch 6: 1.2180264592170715
[08/08/2025 03:14:31 INFO]: New best epoch, val score: -0.9430036255921497
[08/08/2025 03:14:31 INFO]: Saving model to: model_best.pth
[08/08/2025 03:15:00 INFO]: Training loss at epoch 7: 0.9972639977931976
[08/08/2025 03:15:04 INFO]: New best epoch, val score: -0.9395765578320899
[08/08/2025 03:15:04 INFO]: Saving model to: model_best.pth
[08/08/2025 03:15:32 INFO]: Training loss at epoch 8: 1.2798054814338684
[08/08/2025 03:15:36 INFO]: New best epoch, val score: -0.9360486379077957
[08/08/2025 03:15:36 INFO]: Saving model to: model_best.pth
[08/08/2025 03:16:07 INFO]: Training loss at epoch 9: 1.1417729258537292
[08/08/2025 03:16:19 INFO]: Training stats: {
    "score": -1.0370634588995746,
    "rmse": 1.0370634588995746
}
[08/08/2025 03:16:19 INFO]: Val stats: {
    "score": -0.9327542629390904,
    "rmse": 0.9327542629390904
}
[08/08/2025 03:16:19 INFO]: Test stats: {
    "score": -0.9330643507501989,
    "rmse": 0.9330643507501989
}
[08/08/2025 03:16:23 INFO]: New best epoch, val score: -0.9327542629390904
[08/08/2025 03:16:23 INFO]: Saving model to: model_best.pth
[08/08/2025 03:16:51 INFO]: Training loss at epoch 10: 1.04295152425766
[08/08/2025 03:17:23 INFO]: Training loss at epoch 11: 0.866603672504425
[08/08/2025 03:17:55 INFO]: Training loss at epoch 12: 1.1651718616485596
[08/08/2025 03:18:27 INFO]: Training loss at epoch 13: 1.0340960919857025
[08/08/2025 03:18:59 INFO]: Training loss at epoch 14: 0.9915779531002045
[08/08/2025 03:19:30 INFO]: Training loss at epoch 15: 0.8961775302886963
[08/08/2025 03:20:02 INFO]: Training loss at epoch 16: 0.890394538640976
[08/08/2025 03:20:34 INFO]: Training loss at epoch 17: 1.0068394243717194
[08/08/2025 03:20:38 INFO]: New best epoch, val score: -0.9266070551488265
[08/08/2025 03:20:38 INFO]: Saving model to: model_best.pth
[08/08/2025 03:21:06 INFO]: Training loss at epoch 18: 1.0788111090660095
[08/08/2025 03:21:10 INFO]: New best epoch, val score: -0.923116172295548
[08/08/2025 03:21:10 INFO]: Saving model to: model_best.pth
[08/08/2025 03:21:38 INFO]: Training loss at epoch 19: 0.9987515211105347
[08/08/2025 03:21:49 INFO]: Training stats: {
    "score": -0.9927683644957634,
    "rmse": 0.9927683644957634
}
[08/08/2025 03:21:49 INFO]: Val stats: {
    "score": -0.922643702926285,
    "rmse": 0.922643702926285
}
[08/08/2025 03:21:49 INFO]: Test stats: {
    "score": -0.9041185977490019,
    "rmse": 0.9041185977490019
}
[08/08/2025 03:21:53 INFO]: New best epoch, val score: -0.922643702926285
[08/08/2025 03:21:53 INFO]: Saving model to: model_best.pth
[08/08/2025 03:22:21 INFO]: Training loss at epoch 20: 1.0106879472732544
[08/08/2025 03:22:53 INFO]: Training loss at epoch 21: 0.7956723570823669
[08/08/2025 03:23:25 INFO]: Training loss at epoch 22: 1.0059881508350372
[08/08/2025 03:23:57 INFO]: Training loss at epoch 23: 0.8400644063949585
[08/08/2025 03:24:29 INFO]: Training loss at epoch 24: 0.9394792020320892
[08/08/2025 03:25:01 INFO]: Training loss at epoch 25: 0.840230256319046
[08/08/2025 03:25:33 INFO]: Training loss at epoch 26: 0.9597706198692322
[08/08/2025 03:26:05 INFO]: Training loss at epoch 27: 1.1056414544582367
[08/08/2025 03:26:37 INFO]: Training loss at epoch 28: 0.9447379112243652
[08/08/2025 03:27:09 INFO]: Training loss at epoch 29: 0.884376049041748
[08/08/2025 03:27:20 INFO]: Training stats: {
    "score": -0.9609746459068673,
    "rmse": 0.9609746459068673
}
[08/08/2025 03:27:20 INFO]: Val stats: {
    "score": -0.9326262135802631,
    "rmse": 0.9326262135802631
}
[08/08/2025 03:27:20 INFO]: Test stats: {
    "score": -0.8924851393953959,
    "rmse": 0.8924851393953959
}
[08/08/2025 03:27:52 INFO]: Training loss at epoch 30: 0.956755131483078
[08/08/2025 03:27:55 INFO]: New best epoch, val score: -0.920555285473183
[08/08/2025 03:27:55 INFO]: Saving model to: model_best.pth
[08/08/2025 03:28:24 INFO]: Training loss at epoch 31: 0.8794961273670197
[08/08/2025 03:28:27 INFO]: New best epoch, val score: -0.9127993849861945
[08/08/2025 03:28:27 INFO]: Saving model to: model_best.pth
[08/08/2025 03:28:58 INFO]: Training loss at epoch 32: 0.9987993240356445
[08/08/2025 03:29:03 INFO]: New best epoch, val score: -0.9118060025528232
[08/08/2025 03:29:03 INFO]: Saving model to: model_best.pth
[08/08/2025 03:29:31 INFO]: Training loss at epoch 33: 0.969484806060791
[08/08/2025 03:30:03 INFO]: Training loss at epoch 34: 0.8636325299739838
[08/08/2025 03:30:35 INFO]: Training loss at epoch 35: 0.9308870136737823
[08/08/2025 03:31:07 INFO]: Training loss at epoch 36: 0.8038919270038605
[08/08/2025 03:31:10 INFO]: New best epoch, val score: -0.9026503346101787
[08/08/2025 03:31:10 INFO]: Saving model to: model_best.pth
[08/08/2025 03:31:39 INFO]: Training loss at epoch 37: 0.8391156494617462
[08/08/2025 03:31:42 INFO]: New best epoch, val score: -0.875044233896105
[08/08/2025 03:31:42 INFO]: Saving model to: model_best.pth
[08/08/2025 03:32:11 INFO]: Training loss at epoch 38: 0.7764305472373962
[08/08/2025 03:32:14 INFO]: New best epoch, val score: -0.8588731917579181
[08/08/2025 03:32:14 INFO]: Saving model to: model_best.pth
[08/08/2025 03:32:43 INFO]: Training loss at epoch 39: 0.715742439031601
[08/08/2025 03:32:54 INFO]: Training stats: {
    "score": -0.8605565377706369,
    "rmse": 0.8605565377706369
}
[08/08/2025 03:32:54 INFO]: Val stats: {
    "score": -0.8490195194146195,
    "rmse": 0.8490195194146195
}
[08/08/2025 03:32:54 INFO]: Test stats: {
    "score": -0.7963432876457528,
    "rmse": 0.7963432876457528
}
[08/08/2025 03:32:58 INFO]: New best epoch, val score: -0.8490195194146195
[08/08/2025 03:32:58 INFO]: Saving model to: model_best.pth
[08/08/2025 03:33:26 INFO]: Training loss at epoch 40: 0.8172475099563599
[08/08/2025 03:33:30 INFO]: New best epoch, val score: -0.8482142241030223
[08/08/2025 03:33:30 INFO]: Saving model to: model_best.pth
[08/08/2025 03:33:58 INFO]: Training loss at epoch 41: 0.8188567757606506
[08/08/2025 03:34:30 INFO]: Training loss at epoch 42: 0.7492823600769043
[08/08/2025 03:35:02 INFO]: Training loss at epoch 43: 0.6306614279747009
[08/08/2025 03:35:33 INFO]: Training loss at epoch 44: 0.5964399576187134
[08/08/2025 03:35:37 INFO]: New best epoch, val score: -0.8464049449095479
[08/08/2025 03:35:37 INFO]: Saving model to: model_best.pth
[08/08/2025 03:36:05 INFO]: Training loss at epoch 45: 0.6611646413803101
[08/08/2025 03:36:09 INFO]: New best epoch, val score: -0.8364016714188824
[08/08/2025 03:36:09 INFO]: Saving model to: model_best.pth
[08/08/2025 03:36:37 INFO]: Training loss at epoch 46: 0.5446475148200989
[08/08/2025 03:37:09 INFO]: Training loss at epoch 47: 0.6279259324073792
[08/08/2025 03:37:41 INFO]: Training loss at epoch 48: 0.623452752828598
[08/08/2025 03:38:13 INFO]: Training loss at epoch 49: 0.5356817245483398
[08/08/2025 03:38:24 INFO]: Training stats: {
    "score": -0.7356033647437478,
    "rmse": 0.7356033647437478
}
[08/08/2025 03:38:24 INFO]: Val stats: {
    "score": -0.8342729927529036,
    "rmse": 0.8342729927529036
}
[08/08/2025 03:38:24 INFO]: Test stats: {
    "score": -0.737217592933349,
    "rmse": 0.737217592933349
}
[08/08/2025 03:38:28 INFO]: New best epoch, val score: -0.8342729927529036
[08/08/2025 03:38:28 INFO]: Saving model to: model_best.pth
[08/08/2025 03:38:56 INFO]: Training loss at epoch 50: 0.5631592869758606
[08/08/2025 03:39:00 INFO]: New best epoch, val score: -0.799660148005385
[08/08/2025 03:39:00 INFO]: Saving model to: model_best.pth
[08/08/2025 03:39:29 INFO]: Training loss at epoch 51: 0.41041038930416107
[08/08/2025 03:39:32 INFO]: New best epoch, val score: -0.791269747931875
[08/08/2025 03:39:32 INFO]: Saving model to: model_best.pth
[08/08/2025 03:40:01 INFO]: Training loss at epoch 52: 0.505518302321434
[08/08/2025 03:40:32 INFO]: Training loss at epoch 53: 0.5308689773082733
[08/08/2025 03:41:04 INFO]: Training loss at epoch 54: 0.6155020147562027
[08/08/2025 03:41:08 INFO]: New best epoch, val score: -0.7642187579466431
[08/08/2025 03:41:08 INFO]: Saving model to: model_best.pth
[08/08/2025 03:41:36 INFO]: Training loss at epoch 55: 0.4985710680484772
[08/08/2025 03:41:40 INFO]: New best epoch, val score: -0.7599739568758324
[08/08/2025 03:41:40 INFO]: Saving model to: model_best.pth
[08/08/2025 03:42:08 INFO]: Training loss at epoch 56: 0.4596191346645355
[08/08/2025 03:42:40 INFO]: Training loss at epoch 57: 0.5666678547859192
[08/08/2025 03:43:12 INFO]: Training loss at epoch 58: 0.5229399800300598
[08/08/2025 03:43:44 INFO]: Training loss at epoch 59: 0.46395160257816315
[08/08/2025 03:43:56 INFO]: Training stats: {
    "score": -0.6639480245188765,
    "rmse": 0.6639480245188765
}
[08/08/2025 03:43:56 INFO]: Val stats: {
    "score": -0.7498469104642603,
    "rmse": 0.7498469104642603
}
[08/08/2025 03:43:56 INFO]: Test stats: {
    "score": -0.7158928892999299,
    "rmse": 0.7158928892999299
}
[08/08/2025 03:44:00 INFO]: New best epoch, val score: -0.7498469104642603
[08/08/2025 03:44:00 INFO]: Saving model to: model_best.pth
[08/08/2025 03:44:28 INFO]: Training loss at epoch 60: 0.4600975513458252
[08/08/2025 03:45:00 INFO]: Training loss at epoch 61: 0.4363801032304764
[08/08/2025 03:45:32 INFO]: Training loss at epoch 62: 0.3727942407131195
[08/08/2025 03:45:36 INFO]: New best epoch, val score: -0.7250566110885187
[08/08/2025 03:45:36 INFO]: Saving model to: model_best.pth
[08/08/2025 03:46:04 INFO]: Training loss at epoch 63: 0.35545913875102997
[08/08/2025 03:46:08 INFO]: New best epoch, val score: -0.7117191727978545
[08/08/2025 03:46:08 INFO]: Saving model to: model_best.pth
[08/08/2025 03:46:38 INFO]: Training loss at epoch 64: 0.35475246608257294
[08/08/2025 03:46:42 INFO]: New best epoch, val score: -0.7049055083415958
[08/08/2025 03:46:42 INFO]: Saving model to: model_best.pth
[08/08/2025 03:47:10 INFO]: Training loss at epoch 65: 0.36937934160232544
[08/08/2025 03:47:42 INFO]: Training loss at epoch 66: 0.4299056977033615
[08/08/2025 03:48:14 INFO]: Training loss at epoch 67: 0.4728045165538788
[08/08/2025 03:48:46 INFO]: Training loss at epoch 68: 0.3939133435487747
[08/08/2025 03:49:18 INFO]: Training loss at epoch 69: 0.4869600534439087
[08/08/2025 03:49:30 INFO]: Training stats: {
    "score": -0.6357382623363836,
    "rmse": 0.6357382623363836
}
[08/08/2025 03:49:30 INFO]: Val stats: {
    "score": -0.7175315589167055,
    "rmse": 0.7175315589167055
}
[08/08/2025 03:49:30 INFO]: Test stats: {
    "score": -0.6422727761577258,
    "rmse": 0.6422727761577258
}
[08/08/2025 03:50:05 INFO]: Training loss at epoch 70: 0.38267961144447327
[08/08/2025 03:50:37 INFO]: Training loss at epoch 71: 0.3633895218372345
[08/08/2025 03:51:09 INFO]: Training loss at epoch 72: 0.371805876493454
[08/08/2025 03:51:41 INFO]: Training loss at epoch 73: 0.29466062784194946
[08/08/2025 03:52:12 INFO]: Training loss at epoch 74: 0.32498958706855774
[08/08/2025 03:52:44 INFO]: Training loss at epoch 75: 0.35850995779037476
[08/08/2025 03:53:16 INFO]: Training loss at epoch 76: 0.5004397034645081
[08/08/2025 03:53:48 INFO]: Training loss at epoch 77: 0.48419779539108276
[08/08/2025 03:54:20 INFO]: Training loss at epoch 78: 0.2999357357621193
[08/08/2025 03:54:24 INFO]: New best epoch, val score: -0.6946830782387311
[08/08/2025 03:54:24 INFO]: Saving model to: model_best.pth
[08/08/2025 03:54:52 INFO]: Training loss at epoch 79: 0.40472620725631714
[08/08/2025 03:55:03 INFO]: Training stats: {
    "score": -0.5921331342153129,
    "rmse": 0.5921331342153129
}
[08/08/2025 03:55:03 INFO]: Val stats: {
    "score": -0.7206641599477516,
    "rmse": 0.7206641599477516
}
[08/08/2025 03:55:03 INFO]: Test stats: {
    "score": -0.6771241832758963,
    "rmse": 0.6771241832758963
}
[08/08/2025 03:55:35 INFO]: Training loss at epoch 80: 0.35923779010772705
[08/08/2025 03:56:07 INFO]: Training loss at epoch 81: 0.3610074669122696
[08/08/2025 03:56:38 INFO]: Training loss at epoch 82: 0.29704029113054276
[08/08/2025 03:57:10 INFO]: Training loss at epoch 83: 0.3987211287021637
[08/08/2025 03:57:42 INFO]: Training loss at epoch 84: 0.3567519038915634
[08/08/2025 03:58:14 INFO]: Training loss at epoch 85: 0.3937545120716095
[08/08/2025 03:58:46 INFO]: Training loss at epoch 86: 0.33353614807128906
[08/08/2025 03:59:18 INFO]: Training loss at epoch 87: 0.38228796422481537
[08/08/2025 03:59:50 INFO]: Training loss at epoch 88: 0.32544663548469543
[08/08/2025 04:00:22 INFO]: Training loss at epoch 89: 0.36558930575847626
[08/08/2025 04:00:33 INFO]: Training stats: {
    "score": -0.5650611079350439,
    "rmse": 0.5650611079350439
}
[08/08/2025 04:00:33 INFO]: Val stats: {
    "score": -0.775162220381334,
    "rmse": 0.775162220381334
}
[08/08/2025 04:00:33 INFO]: Test stats: {
    "score": -0.6951424227086774,
    "rmse": 0.6951424227086774
}
[08/08/2025 04:01:05 INFO]: Training loss at epoch 90: 0.38601481914520264
[08/08/2025 04:01:37 INFO]: Training loss at epoch 91: 0.3051653951406479
[08/08/2025 04:02:08 INFO]: Training loss at epoch 92: 0.3027369827032089
[08/08/2025 04:02:40 INFO]: Training loss at epoch 93: 0.2668524608016014
[08/08/2025 04:03:12 INFO]: Training loss at epoch 94: 0.31665994226932526
[08/08/2025 04:03:44 INFO]: Training loss at epoch 95: 0.2140466719865799
[08/08/2025 04:04:16 INFO]: Training loss at epoch 96: 0.29950547218322754
[08/08/2025 04:04:48 INFO]: Training loss at epoch 97: 0.22987991571426392
[08/08/2025 04:05:20 INFO]: Training loss at epoch 98: 0.3091154396533966
[08/08/2025 04:05:52 INFO]: Training loss at epoch 99: 0.352554053068161
[08/08/2025 04:06:03 INFO]: Training stats: {
    "score": -0.5264428449092511,
    "rmse": 0.5264428449092511
}
[08/08/2025 04:06:03 INFO]: Val stats: {
    "score": -0.7302996592950346,
    "rmse": 0.7302996592950346
}
[08/08/2025 04:06:03 INFO]: Test stats: {
    "score": -0.6387350081739009,
    "rmse": 0.6387350081739009
}
[08/08/2025 04:06:35 INFO]: Training loss at epoch 100: 0.23492951691150665
[08/08/2025 04:07:07 INFO]: Training loss at epoch 101: 0.24887648224830627
[08/08/2025 04:07:39 INFO]: Training loss at epoch 102: 0.28798408061265945
[08/08/2025 04:08:11 INFO]: Training loss at epoch 103: 0.2988475561141968
[08/08/2025 04:08:43 INFO]: Training loss at epoch 104: 0.35294194519519806
[08/08/2025 04:09:15 INFO]: Training loss at epoch 105: 0.2664138302206993
[08/08/2025 04:09:47 INFO]: Training loss at epoch 106: 0.283159464597702
[08/08/2025 04:10:18 INFO]: Training loss at epoch 107: 0.2782585993409157
[08/08/2025 04:10:50 INFO]: Training loss at epoch 108: 0.20806492120027542
[08/08/2025 04:11:22 INFO]: Training loss at epoch 109: 0.28015001118183136
[08/08/2025 04:11:33 INFO]: Training stats: {
    "score": -0.4915908108116814,
    "rmse": 0.4915908108116814
}
[08/08/2025 04:11:33 INFO]: Val stats: {
    "score": -0.7223141092508426,
    "rmse": 0.7223141092508426
}
[08/08/2025 04:11:33 INFO]: Test stats: {
    "score": -0.7487551970177534,
    "rmse": 0.7487551970177534
}
[08/08/2025 04:11:37 INFO]: Running Final Evaluation...
[08/08/2025 04:11:49 INFO]: Training accuracy: {
    "score": -0.6254981799459493,
    "rmse": 0.6254981799459493
}
[08/08/2025 04:11:49 INFO]: Val accuracy: {
    "score": -0.6946830782387311,
    "rmse": 0.6946830782387311
}
[08/08/2025 04:11:49 INFO]: Test accuracy: {
    "score": -0.6804769153389374,
    "rmse": 0.6804769153389374
}
[08/08/2025 04:11:49 INFO]: {
    "dataset": "ic_upstream3",
    "model_name": "ft_transformer",
    "run_id": "renowned-Tressie",
    "best_epoch": 78,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.6804769153389374,
        "rmse": 0.6804769153389374
    },
    "train_stats": {
        "score": -0.6254981799459493,
        "rmse": 0.6254981799459493
    },
    "val_stats": {
        "score": -0.6946830782387311,
        "rmse": 0.6946830782387311
    }
}
[08/08/2025 04:11:49 INFO]: 
_________________________________________________

[08/08/2025 04:11:49 INFO]: train_net_for_optune.py main() running.
[08/08/2025 04:11:49 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 8
  d_ffn_factor: 1.0985166169663274
  attention_dropout: 0.15388629695336414
  ffn_dropout: 0.15388629695336414
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00030811867862146487
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: renowned-Tressie

[08/08/2025 04:11:49 INFO]: This ft_transformer has 2.503 million parameters.
[08/08/2025 04:11:49 INFO]: Training will start at epoch 0.
[08/08/2025 04:11:49 INFO]: ==> Starting training for 200 epochs...
[08/08/2025 04:12:34 INFO]: Training loss at epoch 0: 1.0035480558872223
[08/08/2025 04:12:41 INFO]: New best epoch, val score: -0.9405517868999422
[08/08/2025 04:12:41 INFO]: Saving model to: model_best.pth
[08/08/2025 04:13:31 INFO]: Training loss at epoch 1: 1.8269120454788208
[08/08/2025 04:14:28 INFO]: Training loss at epoch 2: 1.498411238193512
[08/08/2025 04:15:25 INFO]: Training loss at epoch 3: 1.3914830088615417
[08/08/2025 04:16:22 INFO]: Training loss at epoch 4: 1.0436963438987732
[08/08/2025 04:17:19 INFO]: Training loss at epoch 5: 1.258572280406952
[08/08/2025 04:18:11 INFO]: Training loss at epoch 6: 1.0798055529594421
[08/08/2025 04:19:03 INFO]: Training loss at epoch 7: 1.1418786644935608
[08/08/2025 04:19:55 INFO]: Training loss at epoch 8: 1.0690528750419617
[08/08/2025 04:20:47 INFO]: Training loss at epoch 9: 0.9964814484119415
[08/08/2025 04:21:05 INFO]: Training stats: {
    "score": -1.0665208685021168,
    "rmse": 1.0665208685021168
}
[08/08/2025 04:21:05 INFO]: Val stats: {
    "score": -1.10356279122587,
    "rmse": 1.10356279122587
}
[08/08/2025 04:21:05 INFO]: Test stats: {
    "score": -1.0405387406477844,
    "rmse": 1.0405387406477844
}
[08/08/2025 04:21:57 INFO]: Training loss at epoch 10: 1.1825991868972778
[08/08/2025 04:22:49 INFO]: Training loss at epoch 11: 0.8768633604049683
[08/08/2025 04:22:55 INFO]: New best epoch, val score: -0.9349848014391178
[08/08/2025 04:22:55 INFO]: Saving model to: model_best.pth
[08/08/2025 04:23:41 INFO]: Training loss at epoch 12: 1.0058872997760773
[08/08/2025 04:24:33 INFO]: Training loss at epoch 13: 1.2361484169960022
[08/08/2025 04:25:25 INFO]: Training loss at epoch 14: 1.2213955521583557
[08/08/2025 04:26:17 INFO]: Training loss at epoch 15: 1.145480990409851
[08/08/2025 04:26:24 INFO]: New best epoch, val score: -0.9306906007549507
[08/08/2025 04:26:24 INFO]: Saving model to: model_best.pth
[08/08/2025 04:27:09 INFO]: Training loss at epoch 16: 1.0542267560958862
[08/08/2025 04:28:01 INFO]: Training loss at epoch 17: 0.9633341729640961
[08/08/2025 04:28:54 INFO]: Training loss at epoch 18: 1.0181741118431091
[08/08/2025 04:29:46 INFO]: Training loss at epoch 19: 0.8407887816429138
[08/08/2025 04:30:04 INFO]: Training stats: {
    "score": -1.0241659935005283,
    "rmse": 1.0241659935005283
}
[08/08/2025 04:30:04 INFO]: Val stats: {
    "score": -1.040132884799998,
    "rmse": 1.040132884799998
}
[08/08/2025 04:30:04 INFO]: Test stats: {
    "score": -0.9845105465896177,
    "rmse": 0.9845105465896177
}
[08/08/2025 04:30:56 INFO]: Training loss at epoch 20: 0.7839555442333221
[08/08/2025 04:31:48 INFO]: Training loss at epoch 21: 0.9457739293575287
[08/08/2025 04:32:40 INFO]: Training loss at epoch 22: 0.9262341558933258
[08/08/2025 04:33:32 INFO]: Training loss at epoch 23: 0.9018756151199341
[08/08/2025 04:34:24 INFO]: Training loss at epoch 24: 0.9519725739955902
[08/08/2025 04:34:30 INFO]: New best epoch, val score: -0.9260627516679942
[08/08/2025 04:34:30 INFO]: Saving model to: model_best.pth
[08/08/2025 04:35:16 INFO]: Training loss at epoch 25: 0.9168224632740021
[08/08/2025 04:35:23 INFO]: New best epoch, val score: -0.9224118078634345
[08/08/2025 04:35:23 INFO]: Saving model to: model_best.pth
[08/08/2025 04:36:09 INFO]: Training loss at epoch 26: 0.9487352967262268
[08/08/2025 04:36:15 INFO]: New best epoch, val score: -0.9211709156393
[08/08/2025 04:36:15 INFO]: Saving model to: model_best.pth
[08/08/2025 04:37:01 INFO]: Training loss at epoch 27: 0.8434455990791321
[08/08/2025 04:37:53 INFO]: Training loss at epoch 28: 1.1223205924034119
[08/08/2025 04:38:45 INFO]: Training loss at epoch 29: 1.0202850699424744
[08/08/2025 04:39:03 INFO]: Training stats: {
    "score": -0.9745113989213968,
    "rmse": 0.9745113989213968
}
[08/08/2025 04:39:03 INFO]: Val stats: {
    "score": -0.9406998644960873,
    "rmse": 0.9406998644960873
}
[08/08/2025 04:39:03 INFO]: Test stats: {
    "score": -0.9041081668728915,
    "rmse": 0.9041081668728915
}
[08/08/2025 04:39:55 INFO]: Training loss at epoch 30: 0.8704594969749451
[08/08/2025 04:40:47 INFO]: Training loss at epoch 31: 0.9056224226951599
[08/08/2025 04:41:39 INFO]: Training loss at epoch 32: 1.0049074590206146
[08/08/2025 04:42:31 INFO]: Training loss at epoch 33: 1.1445104777812958
[08/08/2025 04:43:23 INFO]: Training loss at epoch 34: 0.880699872970581
[08/08/2025 04:44:15 INFO]: Training loss at epoch 35: 0.9096127450466156
[08/08/2025 04:45:07 INFO]: Training loss at epoch 36: 1.0294493436813354
[08/08/2025 04:45:14 INFO]: New best epoch, val score: -0.917445583613733
[08/08/2025 04:45:14 INFO]: Saving model to: model_best.pth
[08/08/2025 04:45:59 INFO]: Training loss at epoch 37: 0.833738774061203
[08/08/2025 04:46:06 INFO]: New best epoch, val score: -0.9131841061749272
[08/08/2025 04:46:06 INFO]: Saving model to: model_best.pth
[08/08/2025 04:46:52 INFO]: Training loss at epoch 38: 0.79905566573143
[08/08/2025 04:46:59 INFO]: New best epoch, val score: -0.9097914785058676
[08/08/2025 04:46:59 INFO]: Saving model to: model_best.pth
[08/08/2025 04:47:45 INFO]: Training loss at epoch 39: 1.0238882899284363
[08/08/2025 04:48:03 INFO]: Training stats: {
    "score": -0.9306782743166991,
    "rmse": 0.9306782743166991
}
[08/08/2025 04:48:03 INFO]: Val stats: {
    "score": -0.9120653700308874,
    "rmse": 0.9120653700308874
}
[08/08/2025 04:48:03 INFO]: Test stats: {
    "score": -0.8664446490622041,
    "rmse": 0.8664446490622041
}
[08/08/2025 04:48:55 INFO]: Training loss at epoch 40: 0.8901432454586029
[08/08/2025 04:49:47 INFO]: Training loss at epoch 41: 0.8382191061973572
[08/08/2025 04:49:53 INFO]: New best epoch, val score: -0.9093221916189262
[08/08/2025 04:49:53 INFO]: Saving model to: model_best.pth
[08/08/2025 04:50:39 INFO]: Training loss at epoch 42: 0.9568212926387787
[08/08/2025 04:50:45 INFO]: New best epoch, val score: -0.900416026728442
[08/08/2025 04:50:45 INFO]: Saving model to: model_best.pth
[08/08/2025 04:51:31 INFO]: Training loss at epoch 43: 1.057847410440445
[08/08/2025 04:51:37 INFO]: New best epoch, val score: -0.8938036101808831
[08/08/2025 04:51:37 INFO]: Saving model to: model_best.pth
[08/08/2025 04:52:23 INFO]: Training loss at epoch 44: 0.6699690520763397
[08/08/2025 04:52:30 INFO]: New best epoch, val score: -0.8794513651750625
[08/08/2025 04:52:30 INFO]: Saving model to: model_best.pth
[08/08/2025 04:53:16 INFO]: Training loss at epoch 45: 0.7349852025508881
[08/08/2025 04:53:22 INFO]: New best epoch, val score: -0.8613142115752702
[08/08/2025 04:53:22 INFO]: Saving model to: model_best.pth
[08/08/2025 04:54:08 INFO]: Training loss at epoch 46: 0.773457258939743
[08/08/2025 04:55:00 INFO]: Training loss at epoch 47: 0.6084617972373962
[08/08/2025 04:55:52 INFO]: Training loss at epoch 48: 0.7209055721759796
[08/08/2025 04:56:44 INFO]: Training loss at epoch 49: 0.6445594727993011
[08/08/2025 04:57:02 INFO]: Training stats: {
    "score": -0.7975666385277675,
    "rmse": 0.7975666385277675
}
[08/08/2025 04:57:02 INFO]: Val stats: {
    "score": -0.8718410563678359,
    "rmse": 0.8718410563678359
}
[08/08/2025 04:57:02 INFO]: Test stats: {
    "score": -0.7953399893745747,
    "rmse": 0.7953399893745747
}
[08/08/2025 04:57:54 INFO]: Training loss at epoch 50: 0.6486681401729584
[08/08/2025 04:58:46 INFO]: Training loss at epoch 51: 0.597297877073288
[08/08/2025 04:59:38 INFO]: Training loss at epoch 52: 0.5830988883972168
[08/08/2025 05:00:30 INFO]: Training loss at epoch 53: 0.7592545747756958
[08/08/2025 05:01:22 INFO]: Training loss at epoch 54: 0.6358714401721954
[08/08/2025 05:01:29 INFO]: New best epoch, val score: -0.8374082605566311
[08/08/2025 05:01:29 INFO]: Saving model to: model_best.pth
[08/08/2025 05:02:15 INFO]: Training loss at epoch 55: 0.6100814342498779
[08/08/2025 05:02:21 INFO]: New best epoch, val score: -0.8169972681702491
[08/08/2025 05:02:21 INFO]: Saving model to: model_best.pth
[08/08/2025 05:03:07 INFO]: Training loss at epoch 56: 0.5132834166288376
[08/08/2025 05:03:59 INFO]: Training loss at epoch 57: 0.6089600324630737
[08/08/2025 05:04:51 INFO]: Training loss at epoch 58: 0.465090274810791
[08/08/2025 05:04:57 INFO]: New best epoch, val score: -0.8006571141300334
[08/08/2025 05:04:57 INFO]: Saving model to: model_best.pth
[08/08/2025 05:05:44 INFO]: Training loss at epoch 59: 0.7689967453479767
[08/08/2025 05:06:02 INFO]: Training stats: {
    "score": -0.7198962311051641,
    "rmse": 0.7198962311051641
}
[08/08/2025 05:06:02 INFO]: Val stats: {
    "score": -0.8224785014834357,
    "rmse": 0.8224785014834357
}
[08/08/2025 05:06:02 INFO]: Test stats: {
    "score": -0.7348706111737145,
    "rmse": 0.7348706111737145
}
[08/08/2025 05:06:54 INFO]: Training loss at epoch 60: 0.5488967597484589
[08/08/2025 05:07:46 INFO]: Training loss at epoch 61: 0.4918998032808304
[08/08/2025 05:07:52 INFO]: New best epoch, val score: -0.7997700233600463
[08/08/2025 05:07:52 INFO]: Saving model to: model_best.pth
[08/08/2025 05:08:38 INFO]: Training loss at epoch 62: 0.5183180570602417
[08/08/2025 05:08:44 INFO]: New best epoch, val score: -0.7809188243130902
[08/08/2025 05:08:44 INFO]: Saving model to: model_best.pth
[08/08/2025 05:09:30 INFO]: Training loss at epoch 63: 0.6668714880943298
[08/08/2025 05:10:22 INFO]: Training loss at epoch 64: 0.5631466209888458
[08/08/2025 05:11:14 INFO]: Training loss at epoch 65: 0.4686068296432495
[08/08/2025 05:12:06 INFO]: Training loss at epoch 66: 0.4261943846940994
[08/08/2025 05:12:58 INFO]: Training loss at epoch 67: 0.39551764726638794
[08/08/2025 05:13:50 INFO]: Training loss at epoch 68: 0.5690158009529114
[08/08/2025 05:14:42 INFO]: Training loss at epoch 69: 0.4962659776210785
[08/08/2025 05:15:00 INFO]: Training stats: {
    "score": -0.6928001915683418,
    "rmse": 0.6928001915683418
}
[08/08/2025 05:15:00 INFO]: Val stats: {
    "score": -0.7754292506165475,
    "rmse": 0.7754292506165475
}
[08/08/2025 05:15:00 INFO]: Test stats: {
    "score": -0.7485713728522457,
    "rmse": 0.7485713728522457
}
[08/08/2025 05:15:07 INFO]: New best epoch, val score: -0.7754292506165475
[08/08/2025 05:15:07 INFO]: Saving model to: model_best.pth
[08/08/2025 05:15:53 INFO]: Training loss at epoch 70: 0.48014792799949646
[08/08/2025 05:16:45 INFO]: Training loss at epoch 71: 0.7035832405090332
[08/08/2025 05:17:37 INFO]: Training loss at epoch 72: 0.4528569132089615
[08/08/2025 05:17:43 INFO]: New best epoch, val score: -0.7729720469433988
[08/08/2025 05:17:43 INFO]: Saving model to: model_best.pth
[08/08/2025 05:18:29 INFO]: Training loss at epoch 73: 0.40570779144763947
[08/08/2025 05:18:35 INFO]: New best epoch, val score: -0.7542880444055788
[08/08/2025 05:18:35 INFO]: Saving model to: model_best.pth
[08/08/2025 05:19:23 INFO]: Training loss at epoch 74: 0.37761709094047546
[08/08/2025 05:19:30 INFO]: New best epoch, val score: -0.7530762234562045
[08/08/2025 05:19:30 INFO]: Saving model to: model_best.pth
[08/08/2025 05:20:15 INFO]: Training loss at epoch 75: 0.5402654856443405
[08/08/2025 05:21:08 INFO]: Training loss at epoch 76: 0.4413879066705704
[08/08/2025 05:22:00 INFO]: Training loss at epoch 77: 0.4789789915084839
[08/08/2025 05:22:52 INFO]: Training loss at epoch 78: 0.546545997262001
[08/08/2025 05:23:44 INFO]: Training loss at epoch 79: 0.441424161195755
[08/08/2025 05:24:02 INFO]: Training stats: {
    "score": -0.6495071572643955,
    "rmse": 0.6495071572643955
}
[08/08/2025 05:24:02 INFO]: Val stats: {
    "score": -0.7570232820626633,
    "rmse": 0.7570232820626633
}
[08/08/2025 05:24:02 INFO]: Test stats: {
    "score": -0.6953129238775725,
    "rmse": 0.6953129238775725
}
[08/08/2025 05:24:54 INFO]: Training loss at epoch 80: 0.37911203503608704
[08/08/2025 05:25:46 INFO]: Training loss at epoch 81: 0.45422887802124023
[08/08/2025 05:26:38 INFO]: Training loss at epoch 82: 0.4750989079475403
[08/08/2025 05:27:30 INFO]: Training loss at epoch 83: 0.449737012386322
[08/08/2025 05:28:22 INFO]: Training loss at epoch 84: 0.5451419055461884
[08/08/2025 05:29:14 INFO]: Training loss at epoch 85: 0.33889085054397583
[08/08/2025 05:30:06 INFO]: Training loss at epoch 86: 0.39623792469501495
[08/08/2025 05:30:58 INFO]: Training loss at epoch 87: 0.3678351640701294
[08/08/2025 05:31:50 INFO]: Training loss at epoch 88: 0.38004525005817413
[08/08/2025 05:32:42 INFO]: Training loss at epoch 89: 0.35261429846286774
[08/08/2025 05:33:00 INFO]: Training stats: {
    "score": -0.6180936638578923,
    "rmse": 0.6180936638578923
}
[08/08/2025 05:33:00 INFO]: Val stats: {
    "score": -0.7794049775147802,
    "rmse": 0.7794049775147802
}
[08/08/2025 05:33:00 INFO]: Test stats: {
    "score": -0.6927650006208075,
    "rmse": 0.6927650006208075
}
[08/08/2025 05:33:52 INFO]: Training loss at epoch 90: 0.3651755601167679
[08/08/2025 05:34:44 INFO]: Training loss at epoch 91: 0.4072857052087784
[08/08/2025 05:35:36 INFO]: Training loss at epoch 92: 0.48646241426467896
[08/08/2025 05:35:42 INFO]: New best epoch, val score: -0.7467472186865856
[08/08/2025 05:35:42 INFO]: Saving model to: model_best.pth
[08/08/2025 05:36:32 INFO]: Training loss at epoch 93: 0.48494958877563477
[08/08/2025 05:36:39 INFO]: New best epoch, val score: -0.7384140118006853
[08/08/2025 05:36:39 INFO]: Saving model to: model_best.pth
[08/08/2025 05:37:24 INFO]: Training loss at epoch 94: 0.31990382075309753
[08/08/2025 05:38:16 INFO]: Training loss at epoch 95: 0.3868255913257599
[08/08/2025 05:39:08 INFO]: Training loss at epoch 96: 0.3049408346414566
[08/08/2025 05:39:15 INFO]: New best epoch, val score: -0.736712229156447
[08/08/2025 05:39:15 INFO]: Saving model to: model_best.pth
[08/08/2025 05:40:01 INFO]: Training loss at epoch 97: 0.3709162771701813
[08/08/2025 05:40:53 INFO]: Training loss at epoch 98: 0.39783015847206116
[08/08/2025 05:41:45 INFO]: Training loss at epoch 99: 0.39748677611351013
[08/08/2025 05:42:03 INFO]: Training stats: {
    "score": -0.6243704312007117,
    "rmse": 0.6243704312007117
}
[08/08/2025 05:42:03 INFO]: Val stats: {
    "score": -0.8099801667837264,
    "rmse": 0.8099801667837264
}
[08/08/2025 05:42:03 INFO]: Test stats: {
    "score": -0.7206987357741337,
    "rmse": 0.7206987357741337
}
[08/08/2025 05:42:55 INFO]: Training loss at epoch 100: 0.35861288011074066
[08/08/2025 05:43:47 INFO]: Training loss at epoch 101: 0.36823149025440216
[08/08/2025 05:44:39 INFO]: Training loss at epoch 102: 0.319583460688591
[08/08/2025 05:45:31 INFO]: Training loss at epoch 103: 0.37513577938079834
[08/08/2025 05:46:23 INFO]: Training loss at epoch 104: 0.33351078629493713
[08/08/2025 05:47:15 INFO]: Training loss at epoch 105: 0.4240789860486984
[08/08/2025 05:47:21 INFO]: New best epoch, val score: -0.7340194926056617
[08/08/2025 05:47:21 INFO]: Saving model to: model_best.pth
[08/08/2025 05:48:07 INFO]: Training loss at epoch 106: 0.33777932822704315
[08/08/2025 05:48:59 INFO]: Training loss at epoch 107: 0.36007456481456757
[08/08/2025 05:49:51 INFO]: Training loss at epoch 108: 0.3904927968978882
[08/08/2025 05:49:57 INFO]: New best epoch, val score: -0.7202632558868205
[08/08/2025 05:49:57 INFO]: Saving model to: model_best.pth
[08/08/2025 05:50:43 INFO]: Training loss at epoch 109: 0.429781436920166
[08/08/2025 05:51:01 INFO]: Training stats: {
    "score": -0.5854872267889437,
    "rmse": 0.5854872267889437
}
[08/08/2025 05:51:01 INFO]: Val stats: {
    "score": -0.7154090861865282,
    "rmse": 0.7154090861865282
}
[08/08/2025 05:51:01 INFO]: Test stats: {
    "score": -0.6754263552228104,
    "rmse": 0.6754263552228104
}
[08/08/2025 05:51:07 INFO]: New best epoch, val score: -0.7154090861865282
[08/08/2025 05:51:07 INFO]: Saving model to: model_best.pth
[08/08/2025 05:51:53 INFO]: Training loss at epoch 110: 0.3198555111885071
[08/08/2025 05:52:45 INFO]: Training loss at epoch 111: 0.3038012981414795
[08/08/2025 05:53:37 INFO]: Training loss at epoch 112: 0.3540835976600647
[08/08/2025 05:54:29 INFO]: Training loss at epoch 113: 0.31624992191791534
[08/08/2025 05:55:21 INFO]: Training loss at epoch 114: 0.3801231384277344
[08/08/2025 05:56:13 INFO]: Training loss at epoch 115: 0.4510074108839035
[08/08/2025 05:57:05 INFO]: Training loss at epoch 116: 0.37093818187713623
[08/08/2025 05:57:57 INFO]: Training loss at epoch 117: 0.4097130000591278
[08/08/2025 05:58:49 INFO]: Training loss at epoch 118: 0.37451767921447754
[08/08/2025 05:59:41 INFO]: Training loss at epoch 119: 0.3103226572275162
[08/08/2025 05:59:59 INFO]: Training stats: {
    "score": -0.5659929546898058,
    "rmse": 0.5659929546898058
}
[08/08/2025 05:59:59 INFO]: Val stats: {
    "score": -0.7294673867471699,
    "rmse": 0.7294673867471699
}
[08/08/2025 05:59:59 INFO]: Test stats: {
    "score": -0.6922138223212259,
    "rmse": 0.6922138223212259
}
[08/08/2025 06:00:51 INFO]: Training loss at epoch 120: 0.34618350863456726
[08/08/2025 06:01:43 INFO]: Training loss at epoch 121: 0.3726045489311218
[08/08/2025 06:02:35 INFO]: Training loss at epoch 122: 0.32642488181591034
[08/08/2025 06:03:27 INFO]: Training loss at epoch 123: 0.27645834535360336
[08/08/2025 06:04:19 INFO]: Training loss at epoch 124: 0.313199907541275
[08/08/2025 06:05:11 INFO]: Training loss at epoch 125: 0.3062167391180992
[08/08/2025 06:06:03 INFO]: Training loss at epoch 126: 0.3390767276287079
[08/08/2025 06:06:56 INFO]: Training loss at epoch 127: 0.36090484261512756
[08/08/2025 06:07:48 INFO]: Training loss at epoch 128: 0.3545260727405548
[08/08/2025 06:08:40 INFO]: Training loss at epoch 129: 0.27518584579229355
[08/08/2025 06:08:58 INFO]: Training stats: {
    "score": -0.5620032536345193,
    "rmse": 0.5620032536345193
}
[08/08/2025 06:08:58 INFO]: Val stats: {
    "score": -0.7481052252324737,
    "rmse": 0.7481052252324737
}
[08/08/2025 06:08:58 INFO]: Test stats: {
    "score": -0.7032180408384572,
    "rmse": 0.7032180408384572
}
[08/08/2025 06:09:50 INFO]: Training loss at epoch 130: 0.32560156285762787
[08/08/2025 06:10:42 INFO]: Training loss at epoch 131: 0.3235369622707367
[08/08/2025 06:11:39 INFO]: Training loss at epoch 132: 0.33152684569358826
[08/08/2025 06:12:31 INFO]: Training loss at epoch 133: 0.2685075104236603
[08/08/2025 06:13:23 INFO]: Training loss at epoch 134: 0.2743826135993004
[08/08/2025 06:14:15 INFO]: Training loss at epoch 135: 0.28074899315834045
[08/08/2025 06:15:07 INFO]: Training loss at epoch 136: 0.4438512623310089
[08/08/2025 06:15:59 INFO]: Training loss at epoch 137: 0.24304571002721786
[08/08/2025 06:16:51 INFO]: Training loss at epoch 138: 0.3003472536802292
[08/08/2025 06:16:57 INFO]: New best epoch, val score: -0.7099566479928134
[08/08/2025 06:16:57 INFO]: Saving model to: model_best.pth
[08/08/2025 06:17:47 INFO]: Training loss at epoch 139: 0.25646136701107025
[08/08/2025 06:18:07 INFO]: Training stats: {
    "score": -0.5259388665444775,
    "rmse": 0.5259388665444775
}
[08/08/2025 06:18:07 INFO]: Val stats: {
    "score": -0.7307715998735639,
    "rmse": 0.7307715998735639
}
[08/08/2025 06:18:07 INFO]: Test stats: {
    "score": -0.6923232849732686,
    "rmse": 0.6923232849732686
}
[08/08/2025 06:19:02 INFO]: Training loss at epoch 140: 0.35637612640857697
[08/08/2025 06:19:54 INFO]: Training loss at epoch 141: 0.2352318912744522
[08/08/2025 06:20:46 INFO]: Training loss at epoch 142: 0.25934898853302
[08/08/2025 06:21:38 INFO]: Training loss at epoch 143: 0.27294862270355225
[08/08/2025 06:22:30 INFO]: Training loss at epoch 144: 0.2683526948094368
[08/08/2025 06:23:22 INFO]: Training loss at epoch 145: 0.41353026032447815
[08/08/2025 06:23:28 INFO]: New best epoch, val score: -0.7047004756118845
[08/08/2025 06:23:28 INFO]: Saving model to: model_best.pth
[08/08/2025 06:24:14 INFO]: Training loss at epoch 146: 0.30397723615169525
[08/08/2025 06:25:06 INFO]: Training loss at epoch 147: 0.3211708813905716
[08/08/2025 06:25:58 INFO]: Training loss at epoch 148: 0.3075643330812454
[08/08/2025 06:26:50 INFO]: Training loss at epoch 149: 0.3375921696424484
[08/08/2025 06:27:08 INFO]: Training stats: {
    "score": -0.5180344495180282,
    "rmse": 0.5180344495180282
}
[08/08/2025 06:27:08 INFO]: Val stats: {
    "score": -0.7362380493166629,
    "rmse": 0.7362380493166629
}
[08/08/2025 06:27:08 INFO]: Test stats: {
    "score": -0.6947063350820014,
    "rmse": 0.6947063350820014
}
[08/08/2025 06:28:00 INFO]: Training loss at epoch 150: 0.28405801951885223
[08/08/2025 06:28:52 INFO]: Training loss at epoch 151: 0.26944899559020996
[08/08/2025 06:29:44 INFO]: Training loss at epoch 152: 0.30031803250312805
[08/08/2025 06:30:36 INFO]: Training loss at epoch 153: 0.2429649978876114
[08/08/2025 06:31:28 INFO]: Training loss at epoch 154: 0.30480071902275085
[08/08/2025 06:32:20 INFO]: Training loss at epoch 155: 0.2990220934152603
[08/08/2025 06:32:26 INFO]: New best epoch, val score: -0.6985364744659681
[08/08/2025 06:32:26 INFO]: Saving model to: model_best.pth
[08/08/2025 06:33:12 INFO]: Training loss at epoch 156: 0.336617186665535
[08/08/2025 06:33:19 INFO]: New best epoch, val score: -0.6959889204048209
[08/08/2025 06:33:19 INFO]: Saving model to: model_best.pth
[08/08/2025 06:34:09 INFO]: Training loss at epoch 157: 0.2623709440231323
[08/08/2025 06:35:01 INFO]: Training loss at epoch 158: 0.34657934308052063
[08/08/2025 06:35:58 INFO]: Training loss at epoch 159: 0.29295235872268677
[08/08/2025 06:36:18 INFO]: Training stats: {
    "score": -0.5085990848292513,
    "rmse": 0.5085990848292513
}
[08/08/2025 06:36:18 INFO]: Val stats: {
    "score": -0.7173277604198178,
    "rmse": 0.7173277604198178
}
[08/08/2025 06:36:18 INFO]: Test stats: {
    "score": -0.6904117712060931,
    "rmse": 0.6904117712060931
}
[08/08/2025 06:37:14 INFO]: Training loss at epoch 160: 0.2800198644399643
[08/08/2025 06:38:06 INFO]: Training loss at epoch 161: 0.2564128041267395
[08/08/2025 06:38:58 INFO]: Training loss at epoch 162: 0.20644808560609818
[08/08/2025 06:39:50 INFO]: Training loss at epoch 163: 0.3237392455339432
[08/08/2025 06:40:42 INFO]: Training loss at epoch 164: 0.21731331199407578
[08/08/2025 06:41:34 INFO]: Training loss at epoch 165: 0.2609107941389084
[08/08/2025 06:42:27 INFO]: Training loss at epoch 166: 0.24218181520700455
[08/08/2025 06:43:19 INFO]: Training loss at epoch 167: 0.26486343145370483
[08/08/2025 06:44:11 INFO]: Training loss at epoch 168: 0.27023182809352875
[08/08/2025 06:45:03 INFO]: Training loss at epoch 169: 0.25143416225910187
[08/08/2025 06:45:21 INFO]: Training stats: {
    "score": -0.48691530027721347,
    "rmse": 0.48691530027721347
}
[08/08/2025 06:45:21 INFO]: Val stats: {
    "score": -0.745123657630573,
    "rmse": 0.745123657630573
}
[08/08/2025 06:45:21 INFO]: Test stats: {
    "score": -0.7042406124397075,
    "rmse": 0.7042406124397075
}
[08/08/2025 06:46:14 INFO]: Training loss at epoch 170: 0.22038133442401886
[08/08/2025 06:47:06 INFO]: Training loss at epoch 171: 0.32201895117759705
[08/08/2025 06:47:58 INFO]: Training loss at epoch 172: 0.3208785131573677
[08/08/2025 06:48:50 INFO]: Training loss at epoch 173: 0.28858233988285065
[08/08/2025 06:49:42 INFO]: Training loss at epoch 174: 0.30357155203819275
[08/08/2025 06:50:34 INFO]: Training loss at epoch 175: 0.2293219268321991
[08/08/2025 06:50:40 INFO]: New best epoch, val score: -0.6832706562415761
[08/08/2025 06:50:40 INFO]: Saving model to: model_best.pth
[08/08/2025 06:51:26 INFO]: Training loss at epoch 176: 0.2722649425268173
[08/08/2025 06:52:18 INFO]: Training loss at epoch 177: 0.23083914071321487
[08/08/2025 06:53:10 INFO]: Training loss at epoch 178: 0.23674561828374863
[08/08/2025 06:54:02 INFO]: Training loss at epoch 179: 0.21765229105949402
[08/08/2025 06:54:20 INFO]: Training stats: {
    "score": -0.4937028636157532,
    "rmse": 0.4937028636157532
}
[08/08/2025 06:54:20 INFO]: Val stats: {
    "score": -0.7267683378065128,
    "rmse": 0.7267683378065128
}
[08/08/2025 06:54:20 INFO]: Test stats: {
    "score": -0.7253938144667704,
    "rmse": 0.7253938144667704
}
[08/08/2025 06:55:12 INFO]: Training loss at epoch 180: 0.19318751990795135
[08/08/2025 06:56:04 INFO]: Training loss at epoch 181: 0.22973782569169998
[08/08/2025 06:56:56 INFO]: Training loss at epoch 182: 0.26401692628860474
[08/08/2025 06:57:48 INFO]: Training loss at epoch 183: 0.21355753391981125
[08/08/2025 06:57:54 INFO]: New best epoch, val score: -0.6790291393264806
[08/08/2025 06:57:54 INFO]: Saving model to: model_best.pth
[08/08/2025 06:58:40 INFO]: Training loss at epoch 184: 0.23485299199819565
[08/08/2025 06:58:46 INFO]: New best epoch, val score: -0.676397940676883
[08/08/2025 06:58:46 INFO]: Saving model to: model_best.pth
[08/08/2025 06:59:32 INFO]: Training loss at epoch 185: 0.19981154054403305
[08/08/2025 07:00:23 INFO]: Training loss at epoch 186: 0.3627249523997307
[08/08/2025 07:01:15 INFO]: Training loss at epoch 187: 0.25105246901512146
[08/08/2025 07:02:07 INFO]: Training loss at epoch 188: 0.23767542839050293
[08/08/2025 07:02:59 INFO]: Training loss at epoch 189: 0.2575918883085251
[08/08/2025 07:03:17 INFO]: Training stats: {
    "score": -0.538864863174163,
    "rmse": 0.538864863174163
}
[08/08/2025 07:03:17 INFO]: Val stats: {
    "score": -0.811132899414977,
    "rmse": 0.811132899414977
}
[08/08/2025 07:03:17 INFO]: Test stats: {
    "score": -0.7299486755200332,
    "rmse": 0.7299486755200332
}
[08/08/2025 07:04:09 INFO]: Training loss at epoch 190: 0.2115456461906433
[08/08/2025 07:05:01 INFO]: Training loss at epoch 191: 0.20999214053153992
[08/08/2025 07:05:53 INFO]: Training loss at epoch 192: 0.2141203060746193
[08/08/2025 07:06:45 INFO]: Training loss at epoch 193: 0.19374703615903854
[08/08/2025 07:07:37 INFO]: Training loss at epoch 194: 0.26699791848659515
[08/08/2025 07:08:29 INFO]: Training loss at epoch 195: 0.21429316699504852
[08/08/2025 07:09:21 INFO]: Training loss at epoch 196: 0.19281871616840363
[08/08/2025 07:10:13 INFO]: Training loss at epoch 197: 0.2609569951891899
[08/08/2025 07:11:04 INFO]: Training loss at epoch 198: 0.2558984160423279
[08/08/2025 07:11:56 INFO]: Training loss at epoch 199: 0.18869571387767792
[08/08/2025 07:12:14 INFO]: Training stats: {
    "score": -0.4824446306655279,
    "rmse": 0.4824446306655279
}
[08/08/2025 07:12:14 INFO]: Val stats: {
    "score": -0.7039870705197459,
    "rmse": 0.7039870705197459
}
[08/08/2025 07:12:14 INFO]: Test stats: {
    "score": -0.718222912094439,
    "rmse": 0.718222912094439
}
[08/08/2025 07:12:21 INFO]: Running Final Evaluation...
[08/08/2025 07:12:39 INFO]: Training accuracy: {
    "score": -0.4696679149683995,
    "rmse": 0.4696679149683995
}
[08/08/2025 07:12:39 INFO]: Val accuracy: {
    "score": -0.676397940676883,
    "rmse": 0.676397940676883
}
[08/08/2025 07:12:39 INFO]: Test accuracy: {
    "score": -0.6463533309709952,
    "rmse": 0.6463533309709952
}
[08/08/2025 07:12:39 INFO]: {
    "dataset": "ic_upstream3",
    "model_name": "ft_transformer",
    "run_id": "renowned-Tressie",
    "best_epoch": 184,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.6463533309709952,
        "rmse": 0.6463533309709952
    },
    "train_stats": {
        "score": -0.4696679149683995,
        "rmse": 0.4696679149683995
    },
    "val_stats": {
        "score": -0.676397940676883,
        "rmse": 0.676397940676883
    }
}
[08/08/2025 07:12:39 INFO]: 
_________________________________________________

[08/08/2025 07:12:39 INFO]: train_net_for_optune.py main() running.
[08/08/2025 07:12:39 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 1.0292907364617832
  attention_dropout: 0.13751508813728358
  ffn_dropout: 0.13751508813728358
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00044802016219495987
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: renowned-Tressie

[08/08/2025 07:12:39 INFO]: This ft_transformer has 7.254 million parameters.
[08/08/2025 07:12:39 INFO]: Training will start at epoch 0.
[08/08/2025 07:12:39 INFO]: ==> Starting training for 200 epochs...
[08/08/2025 07:14:31 INFO]: Training loss at epoch 0: 0.7560718953609467
[08/08/2025 07:14:46 INFO]: New best epoch, val score: -0.9444588159892056
[08/08/2025 07:14:46 INFO]: Saving model to: model_best.pth
[08/08/2025 07:16:39 INFO]: Training loss at epoch 1: 4.2042553424835205
[08/08/2025 07:18:46 INFO]: Training loss at epoch 2: 1.6919021606445312
[08/08/2025 07:20:53 INFO]: Training loss at epoch 3: 1.2722634375095367
[08/08/2025 07:23:01 INFO]: Training loss at epoch 4: 1.5185526013374329
[08/08/2025 07:25:08 INFO]: Training loss at epoch 5: 1.1460095643997192
[08/08/2025 07:27:15 INFO]: Training loss at epoch 6: 0.9307335615158081
[08/08/2025 07:29:23 INFO]: Training loss at epoch 7: 1.0051682591438293
[08/08/2025 07:31:30 INFO]: Training loss at epoch 8: 0.9644173681735992
[08/08/2025 07:33:38 INFO]: Training loss at epoch 9: 1.0057121217250824
[08/08/2025 07:34:22 INFO]: Training stats: {
    "score": -1.0223316399357465,
    "rmse": 1.0223316399357465
}
[08/08/2025 07:34:22 INFO]: Val stats: {
    "score": -1.0243795735992183,
    "rmse": 1.0243795735992183
}
[08/08/2025 07:34:22 INFO]: Test stats: {
    "score": -0.976059189847576,
    "rmse": 0.976059189847576
}
[08/08/2025 07:36:29 INFO]: Training loss at epoch 10: 0.9168119728565216
[08/08/2025 07:36:45 INFO]: New best epoch, val score: -0.9365699933135804
[08/08/2025 07:36:45 INFO]: Saving model to: model_best.pth
[08/08/2025 07:38:38 INFO]: Training loss at epoch 11: 1.0636722147464752
[08/08/2025 07:40:45 INFO]: Training loss at epoch 12: 1.1032261848449707
[08/08/2025 07:42:52 INFO]: Training loss at epoch 13: 1.148546040058136
[08/08/2025 07:45:01 INFO]: Training loss at epoch 14: 1.1300685405731201
[08/08/2025 07:45:16 INFO]: New best epoch, val score: -0.9312613457894158
[08/08/2025 07:45:16 INFO]: Saving model to: model_best.pth
[08/08/2025 07:47:10 INFO]: Training loss at epoch 15: 1.1353075504302979
[08/08/2025 07:49:20 INFO]: Training loss at epoch 16: 0.9867286086082458
[08/08/2025 07:51:29 INFO]: Training loss at epoch 17: 1.2315894067287445
[08/08/2025 07:53:38 INFO]: Training loss at epoch 18: 0.9301802814006805
[08/08/2025 07:55:47 INFO]: Training loss at epoch 19: 1.093260407447815
[08/08/2025 07:56:34 INFO]: Training stats: {
    "score": -1.0114526396506045,
    "rmse": 1.0114526396506045
}
[08/08/2025 07:56:34 INFO]: Val stats: {
    "score": -1.0066795891987115,
    "rmse": 1.0066795891987115
}
[08/08/2025 07:56:34 INFO]: Test stats: {
    "score": -0.9607363600292127,
    "rmse": 0.9607363600292127
}
[08/08/2025 07:58:53 INFO]: Training loss at epoch 20: 1.0022657215595245
[08/08/2025 08:01:12 INFO]: Training loss at epoch 21: 1.204697847366333
[08/08/2025 08:03:21 INFO]: Training loss at epoch 22: 0.9735547602176666
[08/08/2025 08:05:30 INFO]: Training loss at epoch 23: 0.9820187389850616
[08/08/2025 08:07:39 INFO]: Training loss at epoch 24: 1.1143682301044464
[08/08/2025 08:09:48 INFO]: Training loss at epoch 25: 0.8579634428024292
[08/08/2025 08:11:57 INFO]: Training loss at epoch 26: 0.8775913119316101
[08/08/2025 08:14:06 INFO]: Training loss at epoch 27: 1.1919620633125305
[08/08/2025 08:16:15 INFO]: Training loss at epoch 28: 0.7697892189025879
[08/08/2025 08:18:24 INFO]: Training loss at epoch 29: 1.0066733658313751
[08/08/2025 08:19:08 INFO]: Training stats: {
    "score": -0.992546440439622,
    "rmse": 0.992546440439622
}
[08/08/2025 08:19:08 INFO]: Val stats: {
    "score": -0.966491996287163,
    "rmse": 0.966491996287163
}
[08/08/2025 08:19:08 INFO]: Test stats: {
    "score": -0.9285643695373971,
    "rmse": 0.9285643695373971
}
[08/08/2025 08:21:17 INFO]: Training loss at epoch 30: 1.0180961191654205
[08/08/2025 08:23:26 INFO]: Training loss at epoch 31: 0.7378574013710022
[08/08/2025 08:25:35 INFO]: Training loss at epoch 32: 1.0258515775203705
[08/08/2025 08:25:51 INFO]: New best epoch, val score: -0.9286114278678358
[08/08/2025 08:25:51 INFO]: Saving model to: model_best.pth
[08/08/2025 08:27:45 INFO]: Training loss at epoch 33: 0.8121938407421112
[08/08/2025 08:28:01 INFO]: New best epoch, val score: -0.926183045516314
[08/08/2025 08:28:01 INFO]: Saving model to: model_best.pth
[08/08/2025 08:29:55 INFO]: Training loss at epoch 34: 0.9904327988624573
[08/08/2025 08:32:04 INFO]: Training loss at epoch 35: 1.1736385524272919
[08/08/2025 08:34:14 INFO]: Training loss at epoch 36: 0.8644973933696747
[08/08/2025 08:36:23 INFO]: Training loss at epoch 37: 0.9346901476383209
[08/08/2025 08:38:32 INFO]: Training loss at epoch 38: 0.9114204347133636
[08/08/2025 08:40:42 INFO]: Training loss at epoch 39: 0.9007096290588379
[08/08/2025 08:41:26 INFO]: Training stats: {
    "score": -0.9808381714870807,
    "rmse": 0.9808381714870807
}
[08/08/2025 08:41:26 INFO]: Val stats: {
    "score": -0.9574759746812557,
    "rmse": 0.9574759746812557
}
[08/08/2025 08:41:26 INFO]: Test stats: {
    "score": -0.9174683496388124,
    "rmse": 0.9174683496388124
}
[08/08/2025 08:43:35 INFO]: Training loss at epoch 40: 0.8981773555278778
[08/08/2025 08:45:43 INFO]: Training loss at epoch 41: 1.0940046310424805
[08/08/2025 08:47:51 INFO]: Training loss at epoch 42: 1.0685423612594604
[08/08/2025 08:48:06 INFO]: New best epoch, val score: -0.9195457471639545
[08/08/2025 08:48:06 INFO]: Saving model to: model_best.pth
[08/08/2025 08:49:58 INFO]: Training loss at epoch 43: 1.1204091310501099
[08/08/2025 08:52:06 INFO]: Training loss at epoch 44: 0.9405019879341125
[08/08/2025 08:54:14 INFO]: Training loss at epoch 45: 0.9467383325099945
[08/08/2025 08:56:21 INFO]: Training loss at epoch 46: 0.9033383131027222
[08/08/2025 08:58:29 INFO]: Training loss at epoch 47: 1.0178586542606354
[08/08/2025 09:00:37 INFO]: Training loss at epoch 48: 0.9988997876644135
[08/08/2025 09:02:45 INFO]: Training loss at epoch 49: 1.0792869925498962
[08/08/2025 09:03:28 INFO]: Training stats: {
    "score": -0.9497117348451253,
    "rmse": 0.9497117348451253
}
[08/08/2025 09:03:28 INFO]: Val stats: {
    "score": -0.9048907050158831,
    "rmse": 0.9048907050158831
}
[08/08/2025 09:03:28 INFO]: Test stats: {
    "score": -0.8734841046879989,
    "rmse": 0.8734841046879989
}
[08/08/2025 09:03:43 INFO]: New best epoch, val score: -0.9048907050158831
[08/08/2025 09:03:43 INFO]: Saving model to: model_best.pth
[08/08/2025 09:05:36 INFO]: Training loss at epoch 50: 0.9225525557994843
[08/08/2025 09:05:51 INFO]: New best epoch, val score: -0.8953743225593874
[08/08/2025 09:05:51 INFO]: Saving model to: model_best.pth
[08/08/2025 09:07:54 INFO]: Training loss at epoch 51: 0.9384191930294037
[08/08/2025 09:08:11 INFO]: New best epoch, val score: -0.8936293959856028
[08/08/2025 09:08:11 INFO]: Saving model to: model_best.pth
[08/08/2025 09:10:03 INFO]: Training loss at epoch 52: 0.8846888244152069
[08/08/2025 09:12:11 INFO]: Training loss at epoch 53: 0.7108918130397797
[08/08/2025 09:14:18 INFO]: Training loss at epoch 54: 1.0472671389579773
[08/08/2025 09:16:26 INFO]: Training loss at epoch 55: 0.9220686554908752
[08/08/2025 09:18:33 INFO]: Training loss at epoch 56: 0.6990808844566345
[08/08/2025 09:20:41 INFO]: Training loss at epoch 57: 0.5683598518371582
[08/08/2025 09:20:56 INFO]: New best epoch, val score: -0.8404458661587716
[08/08/2025 09:20:56 INFO]: Saving model to: model_best.pth
[08/08/2025 09:22:49 INFO]: Training loss at epoch 58: 0.7420580387115479
[08/08/2025 09:23:04 INFO]: New best epoch, val score: -0.8364515155559458
[08/08/2025 09:23:04 INFO]: Saving model to: model_best.pth
[08/08/2025 09:24:57 INFO]: Training loss at epoch 59: 0.6497072577476501
[08/08/2025 09:25:40 INFO]: Training stats: {
    "score": -0.8262972639283962,
    "rmse": 0.8262972639283962
}
[08/08/2025 09:25:40 INFO]: Val stats: {
    "score": -0.8342467757209177,
    "rmse": 0.8342467757209177
}
[08/08/2025 09:25:40 INFO]: Test stats: {
    "score": -0.8001207042456847,
    "rmse": 0.8001207042456847
}
[08/08/2025 09:25:56 INFO]: New best epoch, val score: -0.8342467757209177
[08/08/2025 09:25:56 INFO]: Saving model to: model_best.pth
[08/08/2025 09:27:48 INFO]: Training loss at epoch 60: 0.605095624923706
[08/08/2025 09:29:56 INFO]: Training loss at epoch 61: 0.6476346254348755
[08/08/2025 09:32:03 INFO]: Training loss at epoch 62: 0.633343905210495
[08/08/2025 09:34:11 INFO]: Training loss at epoch 63: 0.8018654584884644
[08/08/2025 09:34:26 INFO]: New best epoch, val score: -0.8247644713065828
[08/08/2025 09:34:26 INFO]: Saving model to: model_best.pth
[08/08/2025 09:36:19 INFO]: Training loss at epoch 64: 0.5962736904621124
[08/08/2025 09:36:34 INFO]: New best epoch, val score: -0.8047876308272859
[08/08/2025 09:36:34 INFO]: Saving model to: model_best.pth
[08/08/2025 09:38:26 INFO]: Training loss at epoch 65: 0.536074697971344
[08/08/2025 09:40:34 INFO]: Training loss at epoch 66: 0.7599919140338898
[08/08/2025 09:42:41 INFO]: Training loss at epoch 67: 0.5405584871768951
[08/08/2025 09:44:49 INFO]: Training loss at epoch 68: 0.6708885729312897
[08/08/2025 09:45:04 INFO]: New best epoch, val score: -0.7876733037169766
[08/08/2025 09:45:04 INFO]: Saving model to: model_best.pth
[08/08/2025 09:46:57 INFO]: Training loss at epoch 69: 0.4034516215324402
[08/08/2025 09:47:40 INFO]: Training stats: {
    "score": -0.7328370139545146,
    "rmse": 0.7328370139545146
}
[08/08/2025 09:47:40 INFO]: Val stats: {
    "score": -0.7620894429206865,
    "rmse": 0.7620894429206865
}
[08/08/2025 09:47:40 INFO]: Test stats: {
    "score": -0.7084258595974516,
    "rmse": 0.7084258595974516
}
[08/08/2025 09:47:55 INFO]: New best epoch, val score: -0.7620894429206865
[08/08/2025 09:47:55 INFO]: Saving model to: model_best.pth
[08/08/2025 09:49:48 INFO]: Training loss at epoch 70: 0.6726143658161163
[08/08/2025 09:50:03 INFO]: New best epoch, val score: -0.7605109022881419
[08/08/2025 09:50:03 INFO]: Saving model to: model_best.pth
[08/08/2025 09:51:55 INFO]: Training loss at epoch 71: 0.5437657237052917
[08/08/2025 09:54:03 INFO]: Training loss at epoch 72: 0.5042658895254135
[08/08/2025 09:56:10 INFO]: Training loss at epoch 73: 0.4356094300746918
[08/08/2025 09:58:18 INFO]: Training loss at epoch 74: 0.49919433891773224
[08/08/2025 10:00:25 INFO]: Training loss at epoch 75: 0.5261110663414001
[08/08/2025 10:02:33 INFO]: Training loss at epoch 76: 0.4826613962650299
[08/08/2025 10:04:40 INFO]: Training loss at epoch 77: 0.4852490723133087
[08/08/2025 10:06:48 INFO]: Training loss at epoch 78: 0.3879773020744324
[08/08/2025 10:07:03 INFO]: New best epoch, val score: -0.75431883017334
[08/08/2025 10:07:03 INFO]: Saving model to: model_best.pth
[08/08/2025 10:08:55 INFO]: Training loss at epoch 79: 0.5543213039636612
[08/08/2025 10:09:39 INFO]: Training stats: {
    "score": -0.6593604535988308,
    "rmse": 0.6593604535988308
}
[08/08/2025 10:09:39 INFO]: Val stats: {
    "score": -0.751637231963836,
    "rmse": 0.751637231963836
}
[08/08/2025 10:09:39 INFO]: Test stats: {
    "score": -0.6651883442244638,
    "rmse": 0.6651883442244638
}
[08/08/2025 10:09:54 INFO]: New best epoch, val score: -0.751637231963836
[08/08/2025 10:09:54 INFO]: Saving model to: model_best.pth
[08/08/2025 10:11:56 INFO]: Training loss at epoch 80: 0.4938262403011322
[08/08/2025 10:14:14 INFO]: Training loss at epoch 81: 0.5054292678833008
[08/08/2025 10:16:32 INFO]: Training loss at epoch 82: 0.4487169682979584
[08/08/2025 10:18:39 INFO]: Training loss at epoch 83: 0.421183243393898
[08/08/2025 10:18:55 INFO]: New best epoch, val score: -0.7398002436406689
[08/08/2025 10:18:55 INFO]: Saving model to: model_best.pth
[08/08/2025 10:20:47 INFO]: Training loss at epoch 84: 0.42287616431713104
[08/08/2025 10:22:54 INFO]: Training loss at epoch 85: 0.5340624898672104
[08/08/2025 10:25:02 INFO]: Training loss at epoch 86: 0.41041816771030426
[08/08/2025 10:27:09 INFO]: Training loss at epoch 87: 0.4395652413368225
[08/08/2025 10:29:18 INFO]: Training loss at epoch 88: 0.3143531233072281
[08/08/2025 10:29:34 INFO]: New best epoch, val score: -0.7365395087329982
[08/08/2025 10:29:34 INFO]: Saving model to: model_best.pth
[08/08/2025 10:31:28 INFO]: Training loss at epoch 89: 0.487746000289917
[08/08/2025 10:32:12 INFO]: Training stats: {
    "score": -0.6752970672980114,
    "rmse": 0.6752970672980114
}
[08/08/2025 10:32:12 INFO]: Val stats: {
    "score": -0.7199880807008855,
    "rmse": 0.7199880807008855
}
[08/08/2025 10:32:12 INFO]: Test stats: {
    "score": -0.6564983002166837,
    "rmse": 0.6564983002166837
}
[08/08/2025 10:32:28 INFO]: New best epoch, val score: -0.7199880807008855
[08/08/2025 10:32:28 INFO]: Saving model to: model_best.pth
[08/08/2025 10:34:30 INFO]: Training loss at epoch 90: 0.4420495182275772
[08/08/2025 10:36:40 INFO]: Training loss at epoch 91: 0.36989039182662964
[08/08/2025 10:38:48 INFO]: Training loss at epoch 92: 0.4501129984855652
[08/08/2025 10:39:04 INFO]: New best epoch, val score: -0.7187234259671164
[08/08/2025 10:39:04 INFO]: Saving model to: model_best.pth
[08/08/2025 10:40:58 INFO]: Training loss at epoch 93: 0.4138224869966507
[08/08/2025 10:43:07 INFO]: Training loss at epoch 94: 0.514522984623909
[08/08/2025 10:43:23 INFO]: New best epoch, val score: -0.7129408723108721
[08/08/2025 10:43:23 INFO]: Saving model to: model_best.pth
[08/08/2025 10:45:17 INFO]: Training loss at epoch 95: 0.4313112795352936
[08/08/2025 10:47:26 INFO]: Training loss at epoch 96: 0.4025352746248245
[08/08/2025 10:49:36 INFO]: Training loss at epoch 97: 0.34646686911582947
[08/08/2025 10:51:45 INFO]: Training loss at epoch 98: 0.3938736468553543
[08/08/2025 10:53:54 INFO]: Training loss at epoch 99: 0.3710475414991379
[08/08/2025 10:54:38 INFO]: Training stats: {
    "score": -0.6016696111096931,
    "rmse": 0.6016696111096931
}
[08/08/2025 10:54:38 INFO]: Val stats: {
    "score": -0.761177181512114,
    "rmse": 0.761177181512114
}
[08/08/2025 10:54:38 INFO]: Test stats: {
    "score": -0.656439664580969,
    "rmse": 0.656439664580969
}
[08/08/2025 10:56:47 INFO]: Training loss at epoch 100: 0.4111161530017853
[08/08/2025 10:58:56 INFO]: Training loss at epoch 101: 0.31280551850795746
[08/08/2025 11:01:05 INFO]: Training loss at epoch 102: 0.36095283925533295
[08/08/2025 11:03:14 INFO]: Training loss at epoch 103: 0.3657115250825882
[08/08/2025 11:05:23 INFO]: Training loss at epoch 104: 0.4438024014234543
[08/08/2025 11:07:32 INFO]: Training loss at epoch 105: 0.36254237592220306
[08/08/2025 11:09:41 INFO]: Training loss at epoch 106: 0.4375448226928711
[08/08/2025 11:11:50 INFO]: Training loss at epoch 107: 0.35887011885643005
[08/08/2025 11:12:05 INFO]: New best epoch, val score: -0.7098390176904995
[08/08/2025 11:12:05 INFO]: Saving model to: model_best.pth
[08/08/2025 11:13:59 INFO]: Training loss at epoch 108: 0.32405930757522583
[08/08/2025 11:16:09 INFO]: Training loss at epoch 109: 0.3806862086057663
[08/08/2025 11:16:53 INFO]: Training stats: {
    "score": -0.5681800348090941,
    "rmse": 0.5681800348090941
}
[08/08/2025 11:16:53 INFO]: Val stats: {
    "score": -0.7300360812533787,
    "rmse": 0.7300360812533787
}
[08/08/2025 11:16:53 INFO]: Test stats: {
    "score": -0.6250902179453738,
    "rmse": 0.6250902179453738
}
[08/08/2025 11:19:02 INFO]: Training loss at epoch 110: 0.28124893456697464
[08/08/2025 11:21:11 INFO]: Training loss at epoch 111: 0.3048908859491348
[08/08/2025 11:23:18 INFO]: Training loss at epoch 112: 0.31406213343143463
[08/08/2025 11:23:33 INFO]: New best epoch, val score: -0.705284366693039
[08/08/2025 11:23:33 INFO]: Saving model to: model_best.pth
[08/08/2025 11:25:23 INFO]: Training loss at epoch 113: 0.2575637549161911
[08/08/2025 11:25:38 INFO]: New best epoch, val score: -0.6987803934720478
[08/08/2025 11:25:38 INFO]: Saving model to: model_best.pth
[08/08/2025 11:27:27 INFO]: Training loss at epoch 114: 0.32189764082431793
[08/08/2025 11:29:31 INFO]: Training loss at epoch 115: 0.289231076836586
[08/08/2025 11:31:35 INFO]: Training loss at epoch 116: 0.3251282870769501
[08/08/2025 11:33:39 INFO]: Training loss at epoch 117: 0.32131510972976685
[08/08/2025 11:35:43 INFO]: Training loss at epoch 118: 0.3184530586004257
[08/08/2025 11:37:47 INFO]: Training loss at epoch 119: 0.2650449723005295
[08/08/2025 11:38:30 INFO]: Training stats: {
    "score": -0.5601709526977636,
    "rmse": 0.5601709526977636
}
[08/08/2025 11:38:30 INFO]: Val stats: {
    "score": -0.7643366860690275,
    "rmse": 0.7643366860690275
}
[08/08/2025 11:38:30 INFO]: Test stats: {
    "score": -0.6533431114402478,
    "rmse": 0.6533431114402478
}
[08/08/2025 11:40:34 INFO]: Training loss at epoch 120: 0.32810232043266296
[08/08/2025 11:42:38 INFO]: Training loss at epoch 121: 0.24532989412546158
[08/08/2025 11:44:42 INFO]: Training loss at epoch 122: 0.2729138359427452
[08/08/2025 11:44:57 INFO]: New best epoch, val score: -0.6952412606071778
[08/08/2025 11:44:57 INFO]: Saving model to: model_best.pth
[08/08/2025 11:46:46 INFO]: Training loss at epoch 123: 0.2646324634552002
[08/08/2025 11:48:51 INFO]: Training loss at epoch 124: 0.305443674325943
[08/08/2025 11:49:05 INFO]: New best epoch, val score: -0.6795253929420352
[08/08/2025 11:49:05 INFO]: Saving model to: model_best.pth
[08/08/2025 11:50:57 INFO]: Training loss at epoch 125: 0.32055628299713135
[08/08/2025 11:51:12 INFO]: New best epoch, val score: -0.666103140468808
[08/08/2025 11:51:12 INFO]: Saving model to: model_best.pth
[08/08/2025 11:53:01 INFO]: Training loss at epoch 126: 0.32386212050914764
[08/08/2025 11:55:06 INFO]: Training loss at epoch 127: 0.31433796882629395
[08/08/2025 11:57:10 INFO]: Training loss at epoch 128: 0.250730499625206
[08/08/2025 11:59:14 INFO]: Training loss at epoch 129: 0.2874235212802887
[08/08/2025 11:59:56 INFO]: Training stats: {
    "score": -0.5419486923343956,
    "rmse": 0.5419486923343956
}
[08/08/2025 11:59:56 INFO]: Val stats: {
    "score": -0.6621655828879945,
    "rmse": 0.6621655828879945
}
[08/08/2025 11:59:56 INFO]: Test stats: {
    "score": -0.6124869602669714,
    "rmse": 0.6124869602669714
}
[08/08/2025 12:00:11 INFO]: New best epoch, val score: -0.6621655828879945
[08/08/2025 12:00:11 INFO]: Saving model to: model_best.pth
[08/08/2025 12:02:01 INFO]: Training loss at epoch 130: 0.26682552695274353
[08/08/2025 12:04:05 INFO]: Training loss at epoch 131: 0.33654291927814484
[08/08/2025 12:06:09 INFO]: Training loss at epoch 132: 0.2681259214878082
[08/08/2025 12:08:13 INFO]: Training loss at epoch 133: 0.2521655037999153
[08/08/2025 12:08:28 INFO]: New best epoch, val score: -0.6564500201003068
[08/08/2025 12:08:28 INFO]: Saving model to: model_best.pth
[08/08/2025 12:10:18 INFO]: Training loss at epoch 134: 0.2573109269142151
[08/08/2025 12:10:33 INFO]: New best epoch, val score: -0.6531624008155662
[08/08/2025 12:10:33 INFO]: Saving model to: model_best.pth
[08/08/2025 12:12:23 INFO]: Training loss at epoch 135: 0.290812224149704
[08/08/2025 12:14:27 INFO]: Training loss at epoch 136: 0.30041028559207916
[08/08/2025 12:16:31 INFO]: Training loss at epoch 137: 0.2584065720438957
[08/08/2025 12:18:36 INFO]: Training loss at epoch 138: 0.23340191692113876
[08/08/2025 12:20:40 INFO]: Training loss at epoch 139: 0.21499182283878326
[08/08/2025 12:21:22 INFO]: Training stats: {
    "score": -0.5365557021337087,
    "rmse": 0.5365557021337087
}
[08/08/2025 12:21:22 INFO]: Val stats: {
    "score": -0.6768997635061016,
    "rmse": 0.6768997635061016
}
[08/08/2025 12:21:22 INFO]: Test stats: {
    "score": -0.642584773966737,
    "rmse": 0.642584773966737
}
[08/08/2025 12:23:26 INFO]: Training loss at epoch 140: 0.2925950139760971
[08/08/2025 12:25:31 INFO]: Training loss at epoch 141: 0.2315988689661026
[08/08/2025 12:27:35 INFO]: Training loss at epoch 142: 0.20741012692451477
[08/08/2025 12:29:39 INFO]: Training loss at epoch 143: 0.30907580256462097
[08/08/2025 12:31:43 INFO]: Training loss at epoch 144: 0.24499022960662842
[08/08/2025 12:33:49 INFO]: Training loss at epoch 145: 0.29834386706352234
[08/08/2025 12:34:04 INFO]: New best epoch, val score: -0.6493137821820613
[08/08/2025 12:34:04 INFO]: Saving model to: model_best.pth
[08/08/2025 12:35:54 INFO]: Training loss at epoch 146: 0.3176565617322922
[08/08/2025 12:36:08 INFO]: New best epoch, val score: -0.6453852783117148
[08/08/2025 12:36:08 INFO]: Saving model to: model_best.pth
[08/08/2025 12:37:59 INFO]: Training loss at epoch 147: 0.2452036440372467
[08/08/2025 12:40:05 INFO]: Training loss at epoch 148: 0.23173508793115616
[08/08/2025 12:42:11 INFO]: Training loss at epoch 149: 0.2499869018793106
[08/08/2025 12:42:54 INFO]: Training stats: {
    "score": -0.5033298630025647,
    "rmse": 0.5033298630025647
}
[08/08/2025 12:42:54 INFO]: Val stats: {
    "score": -0.6621033765821394,
    "rmse": 0.6621033765821394
}
[08/08/2025 12:42:54 INFO]: Test stats: {
    "score": -0.6091529942533043,
    "rmse": 0.6091529942533043
}
[08/08/2025 12:45:01 INFO]: Training loss at epoch 150: 0.24767549335956573
[08/08/2025 12:47:07 INFO]: Training loss at epoch 151: 0.3001633957028389
[08/08/2025 12:49:13 INFO]: Training loss at epoch 152: 0.29314877837896347
[08/08/2025 12:51:19 INFO]: Training loss at epoch 153: 0.2530963197350502
[08/08/2025 12:53:26 INFO]: Training loss at epoch 154: 0.21818289160728455
[08/08/2025 12:55:32 INFO]: Training loss at epoch 155: 0.21266866475343704
[08/08/2025 12:57:38 INFO]: Training loss at epoch 156: 0.21065223217010498
[08/08/2025 12:59:44 INFO]: Training loss at epoch 157: 0.19077656418085098
[08/08/2025 13:01:50 INFO]: Training loss at epoch 158: 0.2393535003066063
[08/08/2025 13:03:57 INFO]: Training loss at epoch 159: 0.22571668773889542
[08/08/2025 13:04:40 INFO]: Training stats: {
    "score": -0.4657124693857109,
    "rmse": 0.4657124693857109
}
[08/08/2025 13:04:40 INFO]: Val stats: {
    "score": -0.6470444730498642,
    "rmse": 0.6470444730498642
}
[08/08/2025 13:04:40 INFO]: Test stats: {
    "score": -0.5829074685759285,
    "rmse": 0.5829074685759285
}
[08/08/2025 13:06:46 INFO]: Training loss at epoch 160: 0.24296614527702332
[08/08/2025 13:08:52 INFO]: Training loss at epoch 161: 0.26276472210884094
[08/08/2025 13:10:58 INFO]: Training loss at epoch 162: 0.19783476740121841
[08/08/2025 13:13:04 INFO]: Training loss at epoch 163: 0.23470471054315567
[08/08/2025 13:15:11 INFO]: Training loss at epoch 164: 0.1838962584733963
[08/08/2025 13:17:17 INFO]: Training loss at epoch 165: 0.23317766934633255
[08/08/2025 13:19:23 INFO]: Training loss at epoch 166: 0.1760501228272915
[08/08/2025 13:21:29 INFO]: Training loss at epoch 167: 0.24191322922706604
[08/08/2025 13:21:45 INFO]: New best epoch, val score: -0.6331712080089407
[08/08/2025 13:21:45 INFO]: Saving model to: model_best.pth
[08/08/2025 13:23:35 INFO]: Training loss at epoch 168: 0.23384171724319458
[08/08/2025 13:23:50 INFO]: New best epoch, val score: -0.6208896199880014
[08/08/2025 13:23:50 INFO]: Saving model to: model_best.pth
[08/08/2025 13:25:42 INFO]: Training loss at epoch 169: 0.19410815834999084
[08/08/2025 13:26:24 INFO]: Training stats: {
    "score": -0.45224947169246144,
    "rmse": 0.45224947169246144
}
[08/08/2025 13:26:24 INFO]: Val stats: {
    "score": -0.6164005375815088,
    "rmse": 0.6164005375815088
}
[08/08/2025 13:26:24 INFO]: Test stats: {
    "score": -0.5739015013591722,
    "rmse": 0.5739015013591722
}
[08/08/2025 13:26:39 INFO]: New best epoch, val score: -0.6164005375815088
[08/08/2025 13:26:39 INFO]: Saving model to: model_best.pth
[08/08/2025 13:28:30 INFO]: Training loss at epoch 170: 0.23598407953977585
[08/08/2025 13:30:36 INFO]: Training loss at epoch 171: 0.1667555496096611
[08/08/2025 13:32:43 INFO]: Training loss at epoch 172: 0.19975534826517105
[08/08/2025 13:34:49 INFO]: Training loss at epoch 173: 0.16873501241207123
[08/08/2025 13:36:55 INFO]: Training loss at epoch 174: 0.2584870308637619
[08/08/2025 13:39:01 INFO]: Training loss at epoch 175: 0.20963804423809052
[08/08/2025 13:41:08 INFO]: Training loss at epoch 176: 0.1899217888712883
[08/08/2025 13:43:14 INFO]: Training loss at epoch 177: 0.21292945742607117
[08/08/2025 13:45:20 INFO]: Training loss at epoch 178: 0.23880353569984436
[08/08/2025 13:47:26 INFO]: Training loss at epoch 179: 0.15393606945872307
[08/08/2025 13:48:09 INFO]: Training stats: {
    "score": -0.42707953636692514,
    "rmse": 0.42707953636692514
}
[08/08/2025 13:48:09 INFO]: Val stats: {
    "score": -0.6676237437249821,
    "rmse": 0.6676237437249821
}
[08/08/2025 13:48:09 INFO]: Test stats: {
    "score": -0.667808467745413,
    "rmse": 0.667808467745413
}
[08/08/2025 13:50:16 INFO]: Training loss at epoch 180: 0.16828136891126633
[08/08/2025 13:52:22 INFO]: Training loss at epoch 181: 0.18645250797271729
[08/08/2025 13:54:28 INFO]: Training loss at epoch 182: 0.19107764214277267
[08/08/2025 13:56:34 INFO]: Training loss at epoch 183: 0.1747373566031456
[08/08/2025 13:58:41 INFO]: Training loss at epoch 184: 0.23610251396894455
[08/08/2025 14:00:47 INFO]: Training loss at epoch 185: 0.22578182071447372
[08/08/2025 14:02:54 INFO]: Training loss at epoch 186: 0.15798944607377052
[08/08/2025 14:05:00 INFO]: Training loss at epoch 187: 0.26024649292230606
[08/08/2025 14:07:06 INFO]: Training loss at epoch 188: 0.19405551254749298
[08/08/2025 14:09:12 INFO]: Training loss at epoch 189: 0.22682178765535355
[08/08/2025 14:09:55 INFO]: Training stats: {
    "score": -0.4178450874270397,
    "rmse": 0.4178450874270397
}
[08/08/2025 14:09:55 INFO]: Val stats: {
    "score": -0.6426625166471344,
    "rmse": 0.6426625166471344
}
[08/08/2025 14:09:55 INFO]: Test stats: {
    "score": -0.6312268105199257,
    "rmse": 0.6312268105199257
}
[08/08/2025 14:12:02 INFO]: Training loss at epoch 190: 0.17413659393787384
[08/08/2025 14:14:08 INFO]: Training loss at epoch 191: 0.20653677731752396
[08/08/2025 14:16:21 INFO]: Training loss at epoch 192: 0.17861314862966537
[08/08/2025 14:18:25 INFO]: Training loss at epoch 193: 0.17966604232788086
[08/08/2025 14:18:40 INFO]: New best epoch, val score: -0.6147851012166168
[08/08/2025 14:18:40 INFO]: Saving model to: model_best.pth
[08/08/2025 14:20:30 INFO]: Training loss at epoch 194: 0.2193634957075119
[08/08/2025 14:20:45 INFO]: New best epoch, val score: -0.6021793633197913
[08/08/2025 14:20:45 INFO]: Saving model to: model_best.pth
[08/08/2025 14:22:35 INFO]: Training loss at epoch 195: 0.15898620337247849
[08/08/2025 14:24:39 INFO]: Training loss at epoch 196: 0.1562606319785118
[08/08/2025 14:26:44 INFO]: Training loss at epoch 197: 0.14697174727916718
[08/08/2025 14:28:48 INFO]: Training loss at epoch 198: 0.19468363374471664
[08/08/2025 14:30:52 INFO]: Training loss at epoch 199: 0.1534900665283203
[08/08/2025 14:31:35 INFO]: Training stats: {
    "score": -0.4010394318209619,
    "rmse": 0.4010394318209619
}
[08/08/2025 14:31:35 INFO]: Val stats: {
    "score": -0.7219523781441441,
    "rmse": 0.7219523781441441
}
[08/08/2025 14:31:35 INFO]: Test stats: {
    "score": -0.6581749577119278,
    "rmse": 0.6581749577119278
}
[08/08/2025 14:31:49 INFO]: Running Final Evaluation...
[08/08/2025 14:32:32 INFO]: Training accuracy: {
    "score": -0.3959291623477524,
    "rmse": 0.3959291623477524
}
[08/08/2025 14:32:32 INFO]: Val accuracy: {
    "score": -0.6021793633197913,
    "rmse": 0.6021793633197913
}
[08/08/2025 14:32:32 INFO]: Test accuracy: {
    "score": -0.6241033210671136,
    "rmse": 0.6241033210671136
}
[08/08/2025 14:32:32 INFO]: {
    "dataset": "ic_upstream3",
    "model_name": "ft_transformer",
    "run_id": "renowned-Tressie",
    "best_epoch": 194,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.6241033210671136,
        "rmse": 0.6241033210671136
    },
    "train_stats": {
        "score": -0.3959291623477524,
        "rmse": 0.3959291623477524
    },
    "val_stats": {
        "score": -0.6021793633197913,
        "rmse": 0.6021793633197913
    }
}
[08/08/2025 14:32:32 INFO]: 
_________________________________________________

[08/08/2025 14:32:32 INFO]: train_net_for_optune.py main() running.
[08/08/2025 14:32:32 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 0.6700947447461174
  attention_dropout: 0.16950682648492135
  ffn_dropout: 0.16950682648492135
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0009211411342795675
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: renowned-Tressie

[08/08/2025 14:32:32 INFO]: This ft_transformer has 6.405 million parameters.
[08/08/2025 14:32:32 INFO]: Training will start at epoch 0.
[08/08/2025 14:32:32 INFO]: ==> Starting training for 200 epochs...
[08/08/2025 14:34:12 INFO]: Training loss at epoch 0: 1.0365678668022156
[08/08/2025 14:34:25 INFO]: New best epoch, val score: -0.9604066406837
[08/08/2025 14:34:25 INFO]: Saving model to: model_best.pth
[08/08/2025 14:36:13 INFO]: Training loss at epoch 1: 2.165841221809387
[08/08/2025 14:38:06 INFO]: Training loss at epoch 2: 2.430112659931183
[08/08/2025 14:39:59 INFO]: Training loss at epoch 3: 1.338731586933136
[08/08/2025 14:40:13 INFO]: New best epoch, val score: -0.9322098368524664
[08/08/2025 14:40:13 INFO]: Saving model to: model_best.pth
[08/08/2025 14:41:53 INFO]: Training loss at epoch 4: 1.0889974236488342
[08/08/2025 14:43:46 INFO]: Training loss at epoch 5: 1.0381138920783997
[08/08/2025 14:45:39 INFO]: Training loss at epoch 6: 1.0309978425502777
[08/08/2025 14:47:32 INFO]: Training loss at epoch 7: 1.3032058477401733
[08/08/2025 14:49:24 INFO]: Training loss at epoch 8: 0.9578363299369812
[08/08/2025 14:51:17 INFO]: Training loss at epoch 9: 1.1947938799858093
[08/08/2025 14:51:56 INFO]: Training stats: {
    "score": -1.0323267762934265,
    "rmse": 1.0323267762934265
}
[08/08/2025 14:51:56 INFO]: Val stats: {
    "score": -0.933624108384978,
    "rmse": 0.933624108384978
}
[08/08/2025 14:51:56 INFO]: Test stats: {
    "score": -0.930361716000198,
    "rmse": 0.930361716000198
}
[08/08/2025 14:53:49 INFO]: Training loss at epoch 10: 1.030248075723648
[08/08/2025 14:55:42 INFO]: Training loss at epoch 11: 1.1637053489685059
[08/08/2025 14:57:35 INFO]: Training loss at epoch 12: 1.3488892316818237
[08/08/2025 14:59:28 INFO]: Training loss at epoch 13: 1.2620624899864197
[08/08/2025 15:03:26 INFO]: Training loss at epoch 14: 1.1418268084526062
[08/08/2025 15:05:19 INFO]: Training loss at epoch 15: 0.9637401700019836
[08/08/2025 15:07:12 INFO]: Training loss at epoch 16: 0.9293698668479919
[08/08/2025 15:07:26 INFO]: New best epoch, val score: -0.9302143406535605
[08/08/2025 15:07:26 INFO]: Saving model to: model_best.pth
[08/08/2025 15:09:07 INFO]: Training loss at epoch 17: 1.1023122072219849
[08/08/2025 15:11:01 INFO]: Training loss at epoch 18: 1.281579613685608
[08/08/2025 15:12:56 INFO]: Training loss at epoch 19: 1.2022849321365356
[08/08/2025 15:13:35 INFO]: Training stats: {
    "score": -1.0068873453492675,
    "rmse": 1.0068873453492675
}
[08/08/2025 15:13:35 INFO]: Val stats: {
    "score": -0.9279808079759082,
    "rmse": 0.9279808079759082
}
[08/08/2025 15:13:35 INFO]: Test stats: {
    "score": -0.9145511188177882,
    "rmse": 0.9145511188177882
}
[08/08/2025 15:13:49 INFO]: New best epoch, val score: -0.9279808079759082
[08/08/2025 15:13:49 INFO]: Saving model to: model_best.pth
[08/08/2025 15:15:29 INFO]: Training loss at epoch 20: 1.060078501701355
[08/08/2025 15:17:22 INFO]: Training loss at epoch 21: 1.0314235389232635
[08/08/2025 15:19:15 INFO]: Training loss at epoch 22: 0.9614467322826385
[08/08/2025 15:21:08 INFO]: Training loss at epoch 23: 0.9700374603271484
[08/08/2025 15:23:01 INFO]: Training loss at epoch 24: 0.96703240275383
[08/08/2025 15:24:53 INFO]: Training loss at epoch 25: 0.9921347200870514
[08/08/2025 15:26:46 INFO]: Training loss at epoch 26: 0.9413691759109497
[08/08/2025 15:28:40 INFO]: Training loss at epoch 27: 0.9235295355319977
[08/08/2025 15:30:34 INFO]: Training loss at epoch 28: 1.0772555768489838
[08/08/2025 15:32:29 INFO]: Training loss at epoch 29: 0.9980336129665375
[08/08/2025 15:33:08 INFO]: Training stats: {
    "score": -0.9841593217951116,
    "rmse": 0.9841593217951116
}
[08/08/2025 15:33:08 INFO]: Val stats: {
    "score": -0.9305118917953893,
    "rmse": 0.9305118917953893
}
[08/08/2025 15:33:08 INFO]: Test stats: {
    "score": -0.9046314931891963,
    "rmse": 0.9046314931891963
}
[08/08/2025 15:35:03 INFO]: Training loss at epoch 30: 1.1941294074058533
[08/08/2025 15:36:57 INFO]: Training loss at epoch 31: 0.9520779848098755
[08/08/2025 15:38:52 INFO]: Training loss at epoch 32: 0.8605454564094543
[08/08/2025 15:40:47 INFO]: Training loss at epoch 33: 1.015476554632187
[08/08/2025 15:42:44 INFO]: Training loss at epoch 34: 1.2399463653564453
[08/08/2025 15:44:42 INFO]: Training loss at epoch 35: 0.9014347791671753
[08/08/2025 15:46:39 INFO]: Training loss at epoch 36: 0.7994076311588287
[08/08/2025 15:48:37 INFO]: Training loss at epoch 37: 0.9141236841678619
[08/08/2025 15:50:35 INFO]: Training loss at epoch 38: 1.0194308757781982
[08/08/2025 15:50:49 INFO]: New best epoch, val score: -0.9215775265415646
[08/08/2025 15:50:49 INFO]: Saving model to: model_best.pth
[08/08/2025 15:52:33 INFO]: Training loss at epoch 39: 0.8816312253475189
[08/08/2025 15:53:13 INFO]: Training stats: {
    "score": -0.9285734275777674,
    "rmse": 0.9285734275777674
}
[08/08/2025 15:53:13 INFO]: Val stats: {
    "score": -0.9164116577673375,
    "rmse": 0.9164116577673375
}
[08/08/2025 15:53:13 INFO]: Test stats: {
    "score": -0.8695003024026775,
    "rmse": 0.8695003024026775
}
[08/08/2025 15:53:27 INFO]: New best epoch, val score: -0.9164116577673375
[08/08/2025 15:53:27 INFO]: Saving model to: model_best.pth
[08/08/2025 15:55:12 INFO]: Training loss at epoch 40: 0.8983576893806458
[08/08/2025 15:55:26 INFO]: New best epoch, val score: -0.9153132157189846
[08/08/2025 15:55:26 INFO]: Saving model to: model_best.pth
[08/12/2025 16:36:47 INFO]: Building Dataset
[08/12/2025 16:36:47 INFO]: pre normalizer.fit

[08/12/2025 16:36:47 INFO]: pos normalizer.fit

[08/12/2025 16:36:48 INFO]: Task: regression, Dataset: ic_upstream3, n_numerical: 100, n_categorical: 0, n_classes: 1, n_train_samples: 282, n_val_samples: 66, n_test_samples: 87
[08/12/2025 16:36:49 INFO]: 
_________________________________________________

[08/12/2025 16:36:49 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:36:49 INFO]: 
_________________________________________________

[08/12/2025 16:36:49 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:36:49 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 12
  d_ffn_factor: 1.9822324352683127
  attention_dropout: 0.40655619925192205
  ffn_dropout: 0.40655619925192205
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0003917037976639179
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: -1
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: revolved-Rody

[08/12/2025 16:36:49 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 4
  d_ffn_factor: 1.873802219943948
  attention_dropout: 0.45353164974231497
  ffn_dropout: 0.45353164974231497
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00011070888813770057
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: revolved-Rody

[08/12/2025 16:36:49 INFO]: 
_________________________________________________

[08/12/2025 16:36:49 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:36:49 INFO]: 
_________________________________________________

[08/12/2025 16:36:49 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:36:49 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 2.61923853304754
  attention_dropout: 0.4199929106407093
  ffn_dropout: 0.4199929106407093
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 5.472817999009264e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: revolved-Rody

[08/12/2025 16:36:49 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 8
  d_ffn_factor: 1.2030227537351803
  attention_dropout: 0.3097260491115211
  ffn_dropout: 0.3097260491115211
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.4473341990664206e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: revolved-Rody

[08/12/2025 16:36:49 INFO]: 
_________________________________________________

[08/12/2025 16:36:49 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:36:49 INFO]: 
_________________________________________________

[08/12/2025 16:36:49 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:36:49 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 4
  d_ffn_factor: 2.5778506396706278
  attention_dropout: 0.18971357605831296
  ffn_dropout: 0.18971357605831296
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.0737124121097217e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: revolved-Rody

[08/12/2025 16:36:49 INFO]: 
_________________________________________________

[08/12/2025 16:36:49 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:36:49 INFO]: 
_________________________________________________

[08/12/2025 16:36:49 INFO]: 
_________________________________________________

[08/12/2025 16:36:49 INFO]: 
_________________________________________________

[08/12/2025 16:36:49 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:36:49 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:36:49 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 0.8663550642853822
  attention_dropout: 0.4743480359190597
  ffn_dropout: 0.4743480359190597
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00021432046573591673
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: revolved-Rody

[08/12/2025 16:36:49 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:36:49 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 1.0728724718098297
  attention_dropout: 0.2816706765849131
  ffn_dropout: 0.2816706765849131
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00020972477966317145
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: revolved-Rody

[08/12/2025 16:36:49 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.3032801700380832
  attention_dropout: 0.09256676393661445
  ffn_dropout: 0.09256676393661445
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00026963662655384897
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: revolved-Rody

[08/12/2025 16:36:49 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 0.9957126569236997
  attention_dropout: 0.05533102397217843
  ffn_dropout: 0.05533102397217843
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.5677973629970916e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: revolved-Rody

[08/12/2025 16:36:49 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 2.133941302571719
  attention_dropout: 0.07555298598593957
  ffn_dropout: 0.07555298598593957
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0003839557775831153
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: revolved-Rody

[08/12/2025 16:36:49 INFO]: This ft_transformer has 1.318 million parameters.
[08/12/2025 16:36:49 INFO]: Training will start at epoch 0.
[08/12/2025 16:36:49 INFO]: This ft_transformer has 0.328 million parameters.
[08/12/2025 16:36:49 INFO]: Training will start at epoch 0.
[08/12/2025 16:36:50 INFO]: This ft_transformer has 2.033 million parameters.
[08/12/2025 16:36:50 INFO]: Training will start at epoch 0.
[08/12/2025 16:36:50 INFO]: This ft_transformer has 5.963 million parameters.
[08/12/2025 16:36:50 INFO]: Training will start at epoch 0.
[08/12/2025 16:36:50 INFO]: This ft_transformer has 0.489 million parameters.
[08/12/2025 16:36:50 INFO]: Training will start at epoch 0.
[08/12/2025 16:36:50 INFO]: This ft_transformer has 2.583 million parameters.
[08/12/2025 16:36:50 INFO]: Training will start at epoch 0.
[08/12/2025 16:36:50 INFO]: This ft_transformer has 1.248 million parameters.
[08/12/2025 16:36:50 INFO]: Training will start at epoch 0.
[08/12/2025 16:36:50 INFO]: This ft_transformer has 0.289 million parameters.
[08/12/2025 16:36:50 INFO]: Training will start at epoch 0.
[08/12/2025 16:36:50 INFO]: This ft_transformer has 0.786 million parameters.
[08/12/2025 16:36:50 INFO]: Training will start at epoch 0.
[08/12/2025 16:36:50 INFO]: This ft_transformer has 4.102 million parameters.
[08/12/2025 16:36:50 INFO]: Training will start at epoch 0.
[08/12/2025 16:36:53 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:36:53 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:36:53 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:36:53 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:36:53 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:36:53 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:36:53 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:36:53 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:36:53 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:36:53 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:36:59 INFO]: Training loss at epoch 0: 1.1012073159217834
[08/12/2025 16:36:59 INFO]: New best epoch, val score: -1.026129071970133
[08/12/2025 16:36:59 INFO]: Saving model to: model_best.pth
[08/12/2025 16:37:05 INFO]: Training loss at epoch 1: 1.1481715440750122
[08/12/2025 16:37:06 INFO]: New best epoch, val score: -1.0024477020559333
[08/12/2025 16:37:06 INFO]: Saving model to: model_best.pth
[08/12/2025 16:37:07 INFO]: Training loss at epoch 0: 0.9587528109550476
[08/12/2025 16:37:09 INFO]: Training loss at epoch 0: 0.7359506636857986
[08/12/2025 16:37:09 INFO]: New best epoch, val score: -0.9406939270456589
[08/12/2025 16:37:09 INFO]: Saving model to: model_best.pth
[08/12/2025 16:37:10 INFO]: Training loss at epoch 2: 1.1844565272331238
[08/12/2025 16:37:11 INFO]: New best epoch, val score: -0.9613217362128006
[08/12/2025 16:37:11 INFO]: Saving model to: model_best.pth
[08/12/2025 16:37:11 INFO]: New best epoch, val score: -0.9737705575164803
[08/12/2025 16:37:11 INFO]: Saving model to: model_best.pth
[08/12/2025 16:37:15 INFO]: Training loss at epoch 0: 1.0407490134239197
[08/12/2025 16:37:16 INFO]: Training loss at epoch 3: 0.8665212988853455
[08/12/2025 16:37:17 INFO]: Training loss at epoch 0: 1.269885003566742
[08/12/2025 16:37:17 INFO]: New best epoch, val score: -0.9492016303111095
[08/12/2025 16:37:17 INFO]: Saving model to: model_best.pth
[08/12/2025 16:37:18 INFO]: Training loss at epoch 0: 1.2965067625045776
[08/12/2025 16:37:19 INFO]: New best epoch, val score: -1.0111649980175892
[08/12/2025 16:37:19 INFO]: Saving model to: model_best.pth
[08/12/2025 16:37:20 INFO]: New best epoch, val score: -1.0424839924730929
[08/12/2025 16:37:20 INFO]: Saving model to: model_best.pth
[08/12/2025 16:37:21 INFO]: New best epoch, val score: -0.9326561150997175
[08/12/2025 16:37:21 INFO]: Saving model to: model_best.pth
[08/12/2025 16:37:22 INFO]: Training loss at epoch 4: 1.182047724723816
[08/12/2025 16:37:23 INFO]: Training loss at epoch 1: 1.0921076536178589
[08/12/2025 16:37:23 INFO]: New best epoch, val score: -0.9387656010403264
[08/12/2025 16:37:23 INFO]: Saving model to: model_best.pth
[08/12/2025 16:37:26 INFO]: Training loss at epoch 1: 0.933782547712326
[08/12/2025 16:37:28 INFO]: Training loss at epoch 5: 1.262984037399292
[08/12/2025 16:37:28 INFO]: New best epoch, val score: -0.9374170244577028
[08/12/2025 16:37:28 INFO]: Saving model to: model_best.pth
[08/12/2025 16:37:29 INFO]: New best epoch, val score: -0.9384767346457683
[08/12/2025 16:37:29 INFO]: Saving model to: model_best.pth
[08/12/2025 16:37:33 INFO]: Training loss at epoch 0: 1.253211259841919
[08/12/2025 16:37:34 INFO]: Training loss at epoch 6: 1.321324646472931
[08/12/2025 16:37:34 INFO]: New best epoch, val score: -0.9384489389338904
[08/12/2025 16:37:34 INFO]: Saving model to: model_best.pth
[08/12/2025 16:37:38 INFO]: Training loss at epoch 2: 1.1323821544647217
[08/12/2025 16:37:39 INFO]: New best epoch, val score: -1.0456655960507024
[08/12/2025 16:37:39 INFO]: Saving model to: model_best.pth
[08/12/2025 16:37:39 INFO]: Training loss at epoch 7: 0.8447199463844299
[08/12/2025 16:37:40 INFO]: Training loss at epoch 1: 1.393949568271637
[08/12/2025 16:37:43 INFO]: Training loss at epoch 2: 0.9923237562179565
[08/12/2025 16:37:43 INFO]: Training loss at epoch 0: 1.2219508290290833
[08/12/2025 16:37:44 INFO]: Training loss at epoch 1: 1.3135617971420288
[08/12/2025 16:37:45 INFO]: Training loss at epoch 8: 0.9144402742385864
[08/12/2025 16:37:45 INFO]: New best epoch, val score: -0.9184559171395567
[08/12/2025 16:37:45 INFO]: Saving model to: model_best.pth
[08/12/2025 16:37:45 INFO]: Training loss at epoch 1: 1.0900540351867676
[08/12/2025 16:37:50 INFO]: New best epoch, val score: -1.1528981010900319
[08/12/2025 16:37:50 INFO]: Saving model to: model_best.pth
[08/12/2025 16:37:51 INFO]: Training loss at epoch 9: 1.0491855144500732
[08/12/2025 16:37:53 INFO]: Training stats: {
    "score": -1.0111248946677995,
    "rmse": 1.0111248946677995
}
[08/12/2025 16:37:53 INFO]: Val stats: {
    "score": -0.9390041637611424,
    "rmse": 0.9390041637611424
}
[08/12/2025 16:37:53 INFO]: Test stats: {
    "score": -0.9205308727702172,
    "rmse": 0.9205308727702172
}
[08/12/2025 16:37:54 INFO]: Training loss at epoch 3: 1.0143715143203735
[08/12/2025 16:37:56 INFO]: New best epoch, val score: -0.9122285721939644
[08/12/2025 16:37:56 INFO]: Saving model to: model_best.pth
[08/12/2025 16:37:58 INFO]: Training loss at epoch 10: 0.9071287214756012
[08/12/2025 16:38:00 INFO]: Training loss at epoch 3: 0.967646062374115
[08/12/2025 16:38:02 INFO]: New best epoch, val score: -0.9051999160864623
[08/12/2025 16:38:02 INFO]: Saving model to: model_best.pth
[08/12/2025 16:38:04 INFO]: Training loss at epoch 11: 0.9480614364147186
[08/12/2025 16:38:05 INFO]: Training loss at epoch 2: 1.292238175868988
[08/12/2025 16:38:10 INFO]: Training loss at epoch 12: 0.8836740553379059
[08/12/2025 16:38:10 INFO]: Training loss at epoch 4: 0.8122335076332092
[08/12/2025 16:38:11 INFO]: Training loss at epoch 2: 1.1847774982452393
[08/12/2025 16:38:12 INFO]: New best epoch, val score: -0.8752045880318362
[08/12/2025 16:38:12 INFO]: Saving model to: model_best.pth
[08/12/2025 16:38:12 INFO]: Training loss at epoch 2: 0.9341329336166382
[08/12/2025 16:38:14 INFO]: Training loss at epoch 0: 0.9757947027683258
[08/12/2025 16:38:16 INFO]: Training loss at epoch 13: 1.378928780555725
[08/12/2025 16:38:18 INFO]: Training loss at epoch 4: 1.05142343044281
[08/12/2025 16:38:18 INFO]: Training loss at epoch 0: 2.1891469955444336
[08/12/2025 16:38:18 INFO]: Training loss at epoch 1: 0.9877391159534454
[08/12/2025 16:38:20 INFO]: New best epoch, val score: -0.899394222581064
[08/12/2025 16:38:20 INFO]: Saving model to: model_best.pth
[08/12/2025 16:38:21 INFO]: Training loss at epoch 14: 1.1871425211429596
[08/12/2025 16:38:24 INFO]: New best epoch, val score: -0.9337423399941697
[08/12/2025 16:38:24 INFO]: Saving model to: model_best.pth
[08/12/2025 16:38:24 INFO]: New best epoch, val score: -0.9588796151773155
[08/12/2025 16:38:24 INFO]: Saving model to: model_best.pth
[08/12/2025 16:38:26 INFO]: Training loss at epoch 5: 1.0289786458015442
[08/12/2025 16:38:27 INFO]: Training loss at epoch 15: 1.023529589176178
[08/12/2025 16:38:30 INFO]: New best epoch, val score: -1.5055488387600064
[08/12/2025 16:38:30 INFO]: Saving model to: model_best.pth
[08/12/2025 16:38:30 INFO]: Training loss at epoch 3: 1.362618863582611
[08/12/2025 16:38:33 INFO]: Training loss at epoch 16: 0.8911102116107941
[08/12/2025 16:38:35 INFO]: Training loss at epoch 5: 0.8515084385871887
[08/12/2025 16:38:37 INFO]: Training loss at epoch 3: 1.097830444574356
[08/12/2025 16:38:38 INFO]: Training loss at epoch 17: 1.113754689693451
[08/12/2025 16:38:39 INFO]: Training loss at epoch 3: 1.2278252243995667
[08/12/2025 16:38:39 INFO]: Training loss at epoch 1: 1.1744391918182373
[08/12/2025 16:38:40 INFO]: New best epoch, val score: -0.9749564632372607
[08/12/2025 16:38:40 INFO]: Saving model to: model_best.pth
[08/12/2025 16:38:41 INFO]: Training loss at epoch 6: 0.9972465932369232
[08/12/2025 16:38:44 INFO]: Training loss at epoch 18: 1.166014313697815
[08/12/2025 16:38:46 INFO]: New best epoch, val score: -0.9855966671567433
[08/12/2025 16:38:46 INFO]: Saving model to: model_best.pth
[08/12/2025 16:38:50 INFO]: Training loss at epoch 19: 1.0049816370010376
[08/12/2025 16:38:52 INFO]: Training stats: {
    "score": -1.0020097545108677,
    "rmse": 1.0020097545108677
}
[08/12/2025 16:38:52 INFO]: Val stats: {
    "score": -0.9474796067061715,
    "rmse": 0.9474796067061715
}
[08/12/2025 16:38:52 INFO]: Test stats: {
    "score": -0.9206738166729399,
    "rmse": 0.9206738166729399
}
[08/12/2025 16:38:52 INFO]: Training loss at epoch 6: 0.8245396018028259
[08/12/2025 16:38:54 INFO]: Training loss at epoch 4: 1.0112097561359406
[08/12/2025 16:38:56 INFO]: Training loss at epoch 7: 1.0273256301879883
[08/12/2025 16:38:57 INFO]: New best epoch, val score: -0.954733301102211
[08/12/2025 16:38:57 INFO]: Saving model to: model_best.pth
[08/12/2025 16:38:58 INFO]: Training loss at epoch 20: 0.9411973655223846
[08/12/2025 16:39:03 INFO]: Training loss at epoch 2: 1.1151814460754395
[08/12/2025 16:39:03 INFO]: Training loss at epoch 21: 1.060305118560791
[08/12/2025 16:39:04 INFO]: Training loss at epoch 4: 0.8436163365840912
[08/12/2025 16:39:05 INFO]: Training loss at epoch 4: 1.0183651447296143
[08/12/2025 16:39:07 INFO]: New best epoch, val score: -0.9342183489929684
[08/12/2025 16:39:07 INFO]: Saving model to: model_best.pth
[08/12/2025 16:39:09 INFO]: Training loss at epoch 22: 0.8874058127403259
[08/12/2025 16:39:09 INFO]: Training loss at epoch 7: 0.9093505442142487
[08/12/2025 16:39:12 INFO]: Training loss at epoch 8: 0.7014574706554413
[08/12/2025 16:39:15 INFO]: Training loss at epoch 23: 1.2394028306007385
[08/12/2025 16:39:19 INFO]: Training loss at epoch 5: 0.999795138835907
[08/12/2025 16:39:20 INFO]: Training loss at epoch 24: 1.2749955356121063
[08/12/2025 16:39:22 INFO]: New best epoch, val score: -0.9540908939795427
[08/12/2025 16:39:22 INFO]: Saving model to: model_best.pth
[08/12/2025 16:39:26 INFO]: Training loss at epoch 25: 1.0598074197769165
[08/12/2025 16:39:27 INFO]: Training loss at epoch 8: 1.0115356147289276
[08/12/2025 16:39:27 INFO]: Training loss at epoch 9: 0.853004664182663
[08/12/2025 16:39:30 INFO]: Training loss at epoch 5: 1.2622941732406616
[08/12/2025 16:39:32 INFO]: Training loss at epoch 5: 1.101623773574829
[08/12/2025 16:39:32 INFO]: Training loss at epoch 26: 0.9404763579368591
[08/12/2025 16:39:32 INFO]: Training stats: {
    "score": -0.9197153979403976,
    "rmse": 0.9197153979403976
}
[08/12/2025 16:39:32 INFO]: Val stats: {
    "score": -0.881501022933089,
    "rmse": 0.881501022933089
}
[08/12/2025 16:39:32 INFO]: Test stats: {
    "score": -0.8485639773630758,
    "rmse": 0.8485639773630758
}
[08/12/2025 16:39:34 INFO]: New best epoch, val score: -0.9342106384114567
[08/12/2025 16:39:34 INFO]: Saving model to: model_best.pth
[08/12/2025 16:39:36 INFO]: Training loss at epoch 2: 1.0183565020561218
[08/12/2025 16:39:38 INFO]: Training loss at epoch 27: 0.7429004609584808
[08/12/2025 16:39:42 INFO]: New best epoch, val score: -0.9240615198127707
[08/12/2025 16:39:42 INFO]: Saving model to: model_best.pth
[08/12/2025 16:39:44 INFO]: Training loss at epoch 6: 1.1353201270103455
[08/12/2025 16:39:44 INFO]: Training loss at epoch 28: 1.1355035901069641
[08/12/2025 16:39:44 INFO]: Training loss at epoch 9: 1.2865224480628967
[08/12/2025 16:39:44 INFO]: Training loss at epoch 1: 1.1158502101898193
[08/12/2025 16:39:47 INFO]: New best epoch, val score: -0.9500850050644049
[08/12/2025 16:39:47 INFO]: Saving model to: model_best.pth
[08/12/2025 16:39:48 INFO]: Training loss at epoch 3: 1.0332041680812836
[08/12/2025 16:39:48 INFO]: Training loss at epoch 10: 0.9594245553016663
[08/12/2025 16:39:49 INFO]: Training loss at epoch 29: 0.9803741872310638
[08/12/2025 16:39:50 INFO]: Training stats: {
    "score": -0.9744337810790737,
    "rmse": 0.9744337810790737
}
[08/12/2025 16:39:50 INFO]: Val stats: {
    "score": -0.9012330636184485,
    "rmse": 0.9012330636184485
}
[08/12/2025 16:39:50 INFO]: Test stats: {
    "score": -0.878672950711697,
    "rmse": 0.878672950711697
}
[08/12/2025 16:39:51 INFO]: Training stats: {
    "score": -1.0005492765450679,
    "rmse": 1.0005492765450679
}
[08/12/2025 16:39:51 INFO]: Val stats: {
    "score": -0.9487069387004635,
    "rmse": 0.9487069387004635
}
[08/12/2025 16:39:51 INFO]: Test stats: {
    "score": -0.9205167579408432,
    "rmse": 0.9205167579408432
}
[08/12/2025 16:39:54 INFO]: Training loss at epoch 1: 1.7461297512054443
[08/12/2025 16:39:55 INFO]: New best epoch, val score: -0.8645327648432732
[08/12/2025 16:39:55 INFO]: Saving model to: model_best.pth
[08/12/2025 16:39:57 INFO]: Training loss at epoch 6: 0.8722569644451141
[08/12/2025 16:39:57 INFO]: Training loss at epoch 30: 1.0152872204780579
[08/12/2025 16:39:59 INFO]: Training loss at epoch 6: 1.0701490342617035
[08/12/2025 16:40:00 INFO]: New best epoch, val score: -0.9341854456734985
[08/12/2025 16:40:00 INFO]: Saving model to: model_best.pth
[08/12/2025 16:40:03 INFO]: Training loss at epoch 31: 1.1174052357673645
[08/12/2025 16:40:03 INFO]: Training loss at epoch 11: 1.1318361163139343
[08/12/2025 16:40:06 INFO]: New best epoch, val score: -1.392542551455649
[08/12/2025 16:40:06 INFO]: Saving model to: model_best.pth
[08/12/2025 16:40:07 INFO]: Training loss at epoch 10: 0.9416227638721466
[08/12/2025 16:40:08 INFO]: Training loss at epoch 7: 1.1610621809959412
[08/12/2025 16:40:08 INFO]: Training loss at epoch 32: 1.0167681574821472
[08/12/2025 16:40:11 INFO]: New best epoch, val score: -0.9439500653489328
[08/12/2025 16:40:11 INFO]: Saving model to: model_best.pth
[08/12/2025 16:40:14 INFO]: Training loss at epoch 33: 0.9597019255161285
[08/12/2025 16:40:19 INFO]: Training loss at epoch 12: 0.8676142394542694
[08/12/2025 16:40:20 INFO]: Training loss at epoch 34: 1.048933744430542
[08/12/2025 16:40:23 INFO]: Training loss at epoch 7: 1.0520856380462646
[08/12/2025 16:40:24 INFO]: Training loss at epoch 11: 0.8644145727157593
[08/12/2025 16:40:25 INFO]: Training loss at epoch 7: 1.0082051753997803
[08/12/2025 16:40:26 INFO]: Training loss at epoch 35: 1.1366461217403412
[08/12/2025 16:40:27 INFO]: New best epoch, val score: -0.934167794115332
[08/12/2025 16:40:27 INFO]: Saving model to: model_best.pth
[08/12/2025 16:40:31 INFO]: Training loss at epoch 3: 0.997014045715332
[08/12/2025 16:40:31 INFO]: Training loss at epoch 36: 0.9511235058307648
[08/12/2025 16:40:32 INFO]: Training loss at epoch 4: 1.0993645191192627
[08/12/2025 16:40:33 INFO]: Training loss at epoch 8: 0.9193726778030396
[08/12/2025 16:40:34 INFO]: Training loss at epoch 13: 0.7532675862312317
[08/12/2025 16:40:36 INFO]: New best epoch, val score: -0.9372415383620946
[08/12/2025 16:40:36 INFO]: Saving model to: model_best.pth
[08/12/2025 16:40:37 INFO]: Training loss at epoch 37: 1.1807482242584229
[08/12/2025 16:40:38 INFO]: Running Final Evaluation...
[08/12/2025 16:40:38 INFO]: New best epoch, val score: -0.9278772204708421
[08/12/2025 16:40:38 INFO]: Saving model to: model_best.pth
[08/12/2025 16:40:40 INFO]: Training loss at epoch 12: 0.9334631860256195
[08/12/2025 16:40:49 INFO]: Training loss at epoch 14: 1.0823600590229034
[08/12/2025 16:40:50 INFO]: Training loss at epoch 8: 1.0048970580101013
[08/12/2025 16:40:52 INFO]: Training loss at epoch 8: 1.266289472579956
[08/12/2025 16:40:57 INFO]: Training loss at epoch 13: 0.8184030950069427
[08/12/2025 16:40:57 INFO]: Training loss at epoch 9: 1.2934501767158508
[08/12/2025 16:41:04 INFO]: Training loss at epoch 15: 0.8294035792350769
[08/12/2025 16:41:05 INFO]: Training stats: {
    "score": -1.03884375750661,
    "rmse": 1.03884375750661
}
[08/12/2025 16:41:05 INFO]: Val stats: {
    "score": -0.9309157091837241,
    "rmse": 0.9309157091837241
}
[08/12/2025 16:41:05 INFO]: Test stats: {
    "score": -0.9320275591479777,
    "rmse": 0.9320275591479777
}
[08/12/2025 16:41:08 INFO]: New best epoch, val score: -0.9309157091837241
[08/12/2025 16:41:08 INFO]: Saving model to: model_best.pth
[08/12/2025 16:41:14 INFO]: Training loss at epoch 14: 0.7938386797904968
[08/12/2025 16:41:14 INFO]: Training loss at epoch 2: 1.6690431237220764
[08/12/2025 16:41:15 INFO]: Training loss at epoch 9: 0.8578701019287109
[08/12/2025 16:41:16 INFO]: Training loss at epoch 5: 1.1408393383026123
[08/12/2025 16:41:18 INFO]: Training loss at epoch 9: 1.0130402445793152
[08/12/2025 16:41:19 INFO]: Training loss at epoch 16: 0.909996509552002
[08/12/2025 16:41:22 INFO]: New best epoch, val score: -0.9272657302942963
[08/12/2025 16:41:22 INFO]: Saving model to: model_best.pth
[08/12/2025 16:41:24 INFO]: New best epoch, val score: -0.8479094656025333
[08/12/2025 16:41:24 INFO]: Saving model to: model_best.pth
[08/12/2025 16:41:25 INFO]: Training stats: {
    "score": -1.0171401451454174,
    "rmse": 1.0171401451454174
}
[08/12/2025 16:41:25 INFO]: Val stats: {
    "score": -0.9342089738562342,
    "rmse": 0.9342089738562342
}
[08/12/2025 16:41:25 INFO]: Test stats: {
    "score": -0.9219608764921607,
    "rmse": 0.9219608764921607
}
[08/12/2025 16:41:25 INFO]: Training loss at epoch 4: 0.9430606067180634
[08/12/2025 16:41:27 INFO]: Training stats: {
    "score": -0.9772570278175696,
    "rmse": 0.9772570278175696
}
[08/12/2025 16:41:27 INFO]: Val stats: {
    "score": -0.9464630744402386,
    "rmse": 0.9464630744402386
}
[08/12/2025 16:41:27 INFO]: Test stats: {
    "score": -0.9078804687628006,
    "rmse": 0.9078804687628006
}
[08/12/2025 16:41:29 INFO]: Training loss at epoch 2: 1.7927438020706177
[08/12/2025 16:41:30 INFO]: Training loss at epoch 10: 1.0485678911209106
[08/12/2025 16:41:30 INFO]: Training loss at epoch 15: 0.9052414000034332
[08/12/2025 16:41:34 INFO]: Training loss at epoch 17: 1.058339536190033
[08/12/2025 16:41:40 INFO]: New best epoch, val score: -0.9385363085942037
[08/12/2025 16:41:40 INFO]: Saving model to: model_best.pth
[08/12/2025 16:41:47 INFO]: Training loss at epoch 16: 0.8909057974815369
[08/12/2025 16:41:49 INFO]: Training loss at epoch 18: 0.97265625
[08/12/2025 16:41:50 INFO]: Training loss at epoch 10: 1.043708860874176
[08/12/2025 16:41:54 INFO]: Training loss at epoch 10: 1.083891212940216
[08/12/2025 16:41:54 INFO]: Training loss at epoch 11: 0.9463574290275574
[08/12/2025 16:42:01 INFO]: Training loss at epoch 6: 1.1152728199958801
[08/12/2025 16:42:03 INFO]: Training loss at epoch 17: 1.002993881702423
[08/12/2025 16:42:04 INFO]: Training loss at epoch 19: 1.0687467753887177
[08/12/2025 16:42:06 INFO]: New best epoch, val score: -0.926662617477544
[08/12/2025 16:42:06 INFO]: Saving model to: model_best.pth
[08/12/2025 16:42:10 INFO]: Training stats: {
    "score": -0.8797474034811076,
    "rmse": 0.8797474034811076
}
[08/12/2025 16:42:10 INFO]: Val stats: {
    "score": -0.8907740626452948,
    "rmse": 0.8907740626452948
}
[08/12/2025 16:42:10 INFO]: Test stats: {
    "score": -0.8246447254177207,
    "rmse": 0.8246447254177207
}
[08/12/2025 16:42:17 INFO]: Training loss at epoch 11: 1.009088933467865
[08/12/2025 16:42:19 INFO]: Training loss at epoch 12: 1.0258206725120544
[08/12/2025 16:42:20 INFO]: Training loss at epoch 5: 0.8631511926651001
[08/12/2025 16:42:21 INFO]: Training loss at epoch 11: 1.0609175562858582
[08/12/2025 16:42:21 INFO]: Training loss at epoch 18: 0.9635098576545715
[08/12/2025 16:42:24 INFO]: New best epoch, val score: -0.9274618246872607
[08/12/2025 16:42:24 INFO]: Saving model to: model_best.pth
[08/12/2025 16:42:25 INFO]: Training loss at epoch 20: 0.7444573044776917
[08/12/2025 16:42:38 INFO]: Training loss at epoch 19: 1.0101321339607239
[08/12/2025 16:42:40 INFO]: Training loss at epoch 21: 0.905639261007309
[08/12/2025 16:42:43 INFO]: Training loss at epoch 12: 0.8555178344249725
[08/12/2025 16:42:43 INFO]: Training loss at epoch 13: 1.041455328464508
[08/12/2025 16:42:44 INFO]: Training stats: {
    "score": -0.9491198331758518,
    "rmse": 0.9491198331758518
}
[08/12/2025 16:42:44 INFO]: Val stats: {
    "score": -0.9179714395527135,
    "rmse": 0.9179714395527135
}
[08/12/2025 16:42:44 INFO]: Test stats: {
    "score": -0.8735198545850203,
    "rmse": 0.8735198545850203
}
[08/12/2025 16:42:44 INFO]: Training loss at epoch 3: 1.1057780385017395
[08/12/2025 16:42:46 INFO]: Training loss at epoch 7: 0.91082364320755
[08/12/2025 16:42:47 INFO]: Training loss at epoch 12: 1.1102259755134583
[08/12/2025 16:42:50 INFO]: New best epoch, val score: -0.9223043302905142
[08/12/2025 16:42:50 INFO]: Saving model to: model_best.pth
[08/12/2025 16:42:51 INFO]: New best epoch, val score: -0.9262165714324031
[08/12/2025 16:42:51 INFO]: Saving model to: model_best.pth
[08/12/2025 16:42:55 INFO]: Training loss at epoch 22: 0.9017126262187958
[08/12/2025 16:42:57 INFO]: New best epoch, val score: -0.8717916362256681
[08/12/2025 16:42:57 INFO]: Saving model to: model_best.pth
[08/12/2025 16:43:01 INFO]: Training loss at epoch 20: 0.7133551090955734
[08/12/2025 16:43:04 INFO]: Training loss at epoch 3: 1.0440818071365356
[08/12/2025 16:43:08 INFO]: Training loss at epoch 14: 0.9242308139801025
[08/12/2025 16:43:09 INFO]: Training loss at epoch 13: 1.1161612272262573
[08/12/2025 16:43:10 INFO]: Training loss at epoch 23: 0.9594331085681915
[08/12/2025 16:43:13 INFO]: Training loss at epoch 13: 0.9132785797119141
[08/12/2025 16:43:14 INFO]: Training loss at epoch 6: 1.0327346622943878
[08/12/2025 16:43:17 INFO]: New best epoch, val score: -0.920215374078749
[08/12/2025 16:43:17 INFO]: Saving model to: model_best.pth
[08/12/2025 16:43:17 INFO]: Training loss at epoch 21: 0.98214191198349
[08/12/2025 16:43:25 INFO]: Training loss at epoch 24: 0.7991461157798767
[08/12/2025 16:43:30 INFO]: Training loss at epoch 8: 0.9700906574726105
[08/12/2025 16:43:32 INFO]: Training loss at epoch 15: 1.03995019197464
[08/12/2025 16:43:34 INFO]: Training loss at epoch 22: 1.028484284877777
[08/12/2025 16:43:34 INFO]: Training loss at epoch 14: 0.9559969902038574
[08/12/2025 16:43:36 INFO]: New best epoch, val score: -0.9259671510489826
[08/12/2025 16:43:36 INFO]: Saving model to: model_best.pth
[08/12/2025 16:43:40 INFO]: Training loss at epoch 14: 1.0123696625232697
[08/12/2025 16:43:40 INFO]: Training loss at epoch 25: 0.8422912657260895
[08/12/2025 16:43:50 INFO]: Training loss at epoch 23: 0.8714327216148376
[08/12/2025 16:43:55 INFO]: Training loss at epoch 26: 0.7925840020179749
[08/12/2025 16:43:56 INFO]: Training loss at epoch 16: 1.1888719499111176
[08/12/2025 16:43:59 INFO]: New best epoch, val score: -0.9247122044289893
[08/12/2025 16:43:59 INFO]: Saving model to: model_best.pth
[08/12/2025 16:44:00 INFO]: Training loss at epoch 15: 1.116772174835205
[08/12/2025 16:44:06 INFO]: Training loss at epoch 15: 0.9295877814292908
[08/12/2025 16:44:07 INFO]: Training loss at epoch 24: 1.1624548435211182
[08/12/2025 16:44:08 INFO]: Training loss at epoch 7: 0.9761329591274261
[08/12/2025 16:44:10 INFO]: Training loss at epoch 27: 0.8195247650146484
[08/12/2025 16:44:14 INFO]: Training loss at epoch 4: 0.9316723942756653
[08/12/2025 16:44:14 INFO]: Training loss at epoch 9: 0.9859010577201843
[08/12/2025 16:44:21 INFO]: Training loss at epoch 17: 0.9897448420524597
[08/12/2025 16:44:23 INFO]: Training loss at epoch 25: 0.9053022563457489
[08/12/2025 16:44:23 INFO]: New best epoch, val score: -0.9104730608383431
[08/12/2025 16:44:23 INFO]: Saving model to: model_best.pth
[08/12/2025 16:44:24 INFO]: Training loss at epoch 28: 0.7882613241672516
[08/12/2025 16:44:26 INFO]: Training loss at epoch 16: 1.2791383862495422
[08/12/2025 16:44:26 INFO]: New best epoch, val score: -0.8635908549043765
[08/12/2025 16:44:26 INFO]: Saving model to: model_best.pth
[08/12/2025 16:44:30 INFO]: Training stats: {
    "score": -1.011840515978456,
    "rmse": 1.011840515978456
}
[08/12/2025 16:44:30 INFO]: Val stats: {
    "score": -0.9258474476718002,
    "rmse": 0.9258474476718002
}
[08/12/2025 16:44:30 INFO]: Test stats: {
    "score": -0.9164244294633659,
    "rmse": 0.9164244294633659
}
[08/12/2025 16:44:32 INFO]: Training loss at epoch 16: 1.0357813835144043
[08/12/2025 16:44:35 INFO]: New best epoch, val score: -0.9258474476718002
[08/12/2025 16:44:35 INFO]: Saving model to: model_best.pth
[08/12/2025 16:44:39 INFO]: Training loss at epoch 4: 1.2612510323524475
[08/12/2025 16:44:39 INFO]: Training loss at epoch 29: 0.7876070439815521
[08/12/2025 16:44:40 INFO]: Training loss at epoch 26: 0.7979742884635925
[08/12/2025 16:44:44 INFO]: Training stats: {
    "score": -0.8426125332970319,
    "rmse": 0.8426125332970319
}
[08/12/2025 16:44:44 INFO]: Val stats: {
    "score": -0.8578778616748249,
    "rmse": 0.8578778616748249
}
[08/12/2025 16:44:44 INFO]: Test stats: {
    "score": -0.7857168656337211,
    "rmse": 0.7857168656337211
}
[08/12/2025 16:44:45 INFO]: Training loss at epoch 18: 0.8500237464904785
[08/12/2025 16:44:46 INFO]: New best epoch, val score: -0.8578778616748249
[08/12/2025 16:44:46 INFO]: Saving model to: model_best.pth
[08/12/2025 16:44:48 INFO]: New best epoch, val score: -0.9076979983522322
[08/12/2025 16:44:48 INFO]: Saving model to: model_best.pth
[08/12/2025 16:44:51 INFO]: Training loss at epoch 17: 0.9388070404529572
[08/12/2025 16:44:56 INFO]: Training loss at epoch 27: 0.8263942301273346
[08/12/2025 16:44:59 INFO]: Training loss at epoch 17: 0.9040302634239197
[08/12/2025 16:44:59 INFO]: Training loss at epoch 30: 0.960147887468338
[08/12/2025 16:45:01 INFO]: New best epoch, val score: -0.8552426103285747
[08/12/2025 16:45:01 INFO]: Saving model to: model_best.pth
[08/12/2025 16:45:02 INFO]: Training loss at epoch 8: 0.9926235675811768
[08/12/2025 16:45:10 INFO]: Training loss at epoch 19: 0.9491569399833679
[08/12/2025 16:45:13 INFO]: Training loss at epoch 28: 0.7492274045944214
[08/12/2025 16:45:14 INFO]: Training loss at epoch 10: 1.0915158987045288
[08/12/2025 16:45:14 INFO]: Training loss at epoch 31: 0.923365592956543
[08/12/2025 16:45:17 INFO]: Training loss at epoch 18: 0.9062442183494568
[08/12/2025 16:45:18 INFO]: Training stats: {
    "score": -0.9821690062504984,
    "rmse": 0.9821690062504984
}
[08/12/2025 16:45:18 INFO]: Val stats: {
    "score": -0.9054939537733097,
    "rmse": 0.9054939537733097
}
[08/12/2025 16:45:18 INFO]: Test stats: {
    "score": -0.8897031731263688,
    "rmse": 0.8897031731263688
}
[08/12/2025 16:45:21 INFO]: New best epoch, val score: -0.9054939537733097
[08/12/2025 16:45:21 INFO]: Saving model to: model_best.pth
[08/12/2025 16:45:25 INFO]: Training loss at epoch 18: 0.8822973072528839
[08/12/2025 16:45:29 INFO]: Training loss at epoch 32: 0.7833922207355499
[08/12/2025 16:45:30 INFO]: Training loss at epoch 29: 0.9411272704601288
[08/12/2025 16:45:35 INFO]: Training stats: {
    "score": -0.9336405247328139,
    "rmse": 0.9336405247328139
}
[08/12/2025 16:45:35 INFO]: Val stats: {
    "score": -0.8979365392059746,
    "rmse": 0.8979365392059746
}
[08/12/2025 16:45:35 INFO]: Test stats: {
    "score": -0.8557657705303734,
    "rmse": 0.8557657705303734
}
[08/12/2025 16:45:37 INFO]: New best epoch, val score: -0.8979365392059746
[08/12/2025 16:45:37 INFO]: Saving model to: model_best.pth
[08/12/2025 16:45:42 INFO]: Training loss at epoch 20: 1.0353015065193176
[08/12/2025 16:45:42 INFO]: Training loss at epoch 19: 0.9964739978313446
[08/12/2025 16:45:44 INFO]: Training loss at epoch 33: 0.8530706167221069
[08/12/2025 16:45:44 INFO]: Training loss at epoch 5: 1.0914744138717651
[08/12/2025 16:45:45 INFO]: New best epoch, val score: -0.9035109893121597
[08/12/2025 16:45:45 INFO]: Saving model to: model_best.pth
[08/12/2025 16:45:51 INFO]: Training loss at epoch 19: 0.9065445065498352
[08/12/2025 16:45:52 INFO]: Training stats: {
    "score": -1.0006452460115611,
    "rmse": 1.0006452460115611
}
[08/12/2025 16:45:52 INFO]: Val stats: {
    "score": -0.9759447637594283,
    "rmse": 0.9759447637594283
}
[08/12/2025 16:45:52 INFO]: Test stats: {
    "score": -0.9369640877359359,
    "rmse": 0.9369640877359359
}
[08/12/2025 16:45:52 INFO]: Training loss at epoch 30: 0.8251749277114868
[08/12/2025 16:45:54 INFO]: New best epoch, val score: -0.895434192411224
[08/12/2025 16:45:54 INFO]: Saving model to: model_best.pth
[08/12/2025 16:45:57 INFO]: Training loss at epoch 9: 1.0543450117111206
[08/12/2025 16:45:58 INFO]: Training loss at epoch 34: 0.7350481748580933
[08/12/2025 16:45:58 INFO]: Training loss at epoch 11: 0.8685943484306335
[08/12/2025 16:46:01 INFO]: Training stats: {
    "score": -0.9575656043401222,
    "rmse": 0.9575656043401222
}
[08/12/2025 16:46:01 INFO]: Val stats: {
    "score": -0.9264992446997523,
    "rmse": 0.9264992446997523
}
[08/12/2025 16:46:01 INFO]: Test stats: {
    "score": -0.8858694004222603,
    "rmse": 0.8858694004222603
}
[08/12/2025 16:46:07 INFO]: Training loss at epoch 21: 0.7911444902420044
[08/12/2025 16:46:08 INFO]: Training loss at epoch 31: 0.9309636950492859
[08/12/2025 16:46:10 INFO]: New best epoch, val score: -0.8937609078731803
[08/12/2025 16:46:10 INFO]: Saving model to: model_best.pth
[08/12/2025 16:46:13 INFO]: Training loss at epoch 35: 0.8354643285274506
[08/12/2025 16:46:14 INFO]: Training loss at epoch 5: 1.1714670062065125
[08/12/2025 16:46:15 INFO]: Training stats: {
    "score": -0.9908297914376429,
    "rmse": 0.9908297914376429
}
[08/12/2025 16:46:15 INFO]: Val stats: {
    "score": -0.9417696452573632,
    "rmse": 0.9417696452573632
}
[08/12/2025 16:46:15 INFO]: Test stats: {
    "score": -0.9186900426577513,
    "rmse": 0.9186900426577513
}
[08/12/2025 16:46:17 INFO]: Training loss at epoch 20: 1.079022079706192
[08/12/2025 16:46:25 INFO]: Training loss at epoch 32: 0.9925548434257507
[08/12/2025 16:46:25 INFO]: New best epoch, val score: -0.9384166722537292
[08/12/2025 16:46:25 INFO]: Saving model to: model_best.pth
[08/12/2025 16:46:27 INFO]: Training loss at epoch 20: 1.0139304399490356
[08/12/2025 16:46:28 INFO]: Training loss at epoch 36: 0.7931925654411316
[08/12/2025 16:46:31 INFO]: Training loss at epoch 22: 1.07963627576828
[08/12/2025 16:46:42 INFO]: Training loss at epoch 33: 0.8220838010311127
[08/12/2025 16:46:42 INFO]: Training loss at epoch 12: 0.9648796916007996
[08/12/2025 16:46:43 INFO]: Training loss at epoch 21: 1.1586445569992065
[08/12/2025 16:46:43 INFO]: Training loss at epoch 37: 0.672110989689827
[08/12/2025 16:46:53 INFO]: Training loss at epoch 21: 0.9397442638874054
[08/12/2025 16:46:55 INFO]: Training loss at epoch 23: 1.0214689671993256
[08/12/2025 16:46:58 INFO]: Training loss at epoch 38: 0.7807708382606506
[08/12/2025 16:46:58 INFO]: Training loss at epoch 34: 0.7284293174743652
[08/12/2025 16:47:00 INFO]: New best epoch, val score: -0.8927280000824158
[08/12/2025 16:47:00 INFO]: Saving model to: model_best.pth
[08/12/2025 16:47:08 INFO]: Training loss at epoch 22: 1.0493131577968597
[08/12/2025 16:47:09 INFO]: Training loss at epoch 10: 1.1457684636116028
[08/12/2025 16:47:13 INFO]: Training loss at epoch 39: 0.9887345731258392
[08/12/2025 16:47:18 INFO]: Training loss at epoch 6: 1.3616148829460144
[08/12/2025 16:47:19 INFO]: Training loss at epoch 35: 0.9085354804992676
[08/12/2025 16:47:20 INFO]: Training loss at epoch 22: 0.9386385679244995
[08/12/2025 16:47:20 INFO]: Training loss at epoch 24: 1.020909696817398
[08/12/2025 16:47:22 INFO]: New best epoch, val score: -0.88932989004216
[08/12/2025 16:47:22 INFO]: Saving model to: model_best.pth
[08/12/2025 16:47:24 INFO]: Training stats: {
    "score": -0.8131651738232998,
    "rmse": 0.8131651738232998
}
[08/12/2025 16:47:24 INFO]: Val stats: {
    "score": -0.8827113025702185,
    "rmse": 0.8827113025702185
}
[08/12/2025 16:47:24 INFO]: Test stats: {
    "score": -0.7818972626936105,
    "rmse": 0.7818972626936105
}
[08/12/2025 16:47:26 INFO]: Training loss at epoch 13: 0.9119002223014832
[08/12/2025 16:47:36 INFO]: Training loss at epoch 36: 0.9183351993560791
[08/12/2025 16:47:38 INFO]: New best epoch, val score: -0.8858319972246778
[08/12/2025 16:47:38 INFO]: Saving model to: model_best.pth
[08/12/2025 16:47:41 INFO]: Training loss at epoch 23: 1.0670556426048279
[08/12/2025 16:47:42 INFO]: Training loss at epoch 40: 0.8973880410194397
[08/12/2025 16:47:44 INFO]: Training loss at epoch 25: 0.7261613309383392
[08/12/2025 16:47:46 INFO]: Training loss at epoch 23: 0.9489540159702301
[08/12/2025 16:47:48 INFO]: Training loss at epoch 6: 0.9534800350666046
[08/12/2025 16:47:53 INFO]: Training loss at epoch 37: 0.7353805005550385
[08/12/2025 16:47:55 INFO]: New best epoch, val score: -0.8837272873907877
[08/12/2025 16:47:55 INFO]: Saving model to: model_best.pth
[08/12/2025 16:47:57 INFO]: Training loss at epoch 41: 0.7846933901309967
[08/12/2025 16:47:59 INFO]: New best epoch, val score: -0.9380430878201288
[08/12/2025 16:47:59 INFO]: Saving model to: model_best.pth
[08/12/2025 16:48:06 INFO]: Training loss at epoch 11: 1.2573687434196472
[08/12/2025 16:48:07 INFO]: Training loss at epoch 24: 1.0713315606117249
[08/12/2025 16:48:09 INFO]: Training loss at epoch 26: 0.7690941095352173
[08/12/2025 16:48:09 INFO]: Training loss at epoch 38: 0.9379382431507111
[08/12/2025 16:48:10 INFO]: Training loss at epoch 14: 1.0539470911026
[08/12/2025 16:48:11 INFO]: New best epoch, val score: -0.8817909479352909
[08/12/2025 16:48:11 INFO]: Saving model to: model_best.pth
[08/12/2025 16:48:12 INFO]: Training loss at epoch 42: 0.9227156043052673
[08/12/2025 16:48:13 INFO]: Training loss at epoch 24: 0.8844212591648102
[08/12/2025 16:48:26 INFO]: Training loss at epoch 39: 0.7676645517349243
[08/12/2025 16:48:26 INFO]: Training loss at epoch 43: 1.0370291471481323
[08/12/2025 16:48:32 INFO]: Training stats: {
    "score": -0.9200182398923327,
    "rmse": 0.9200182398923327
}
[08/12/2025 16:48:32 INFO]: Val stats: {
    "score": -0.8798710394562788,
    "rmse": 0.8798710394562788
}
[08/12/2025 16:48:32 INFO]: Test stats: {
    "score": -0.8411291594844732,
    "rmse": 0.8411291594844732
}
[08/12/2025 16:48:32 INFO]: Training loss at epoch 25: 1.05893474817276
[08/12/2025 16:48:33 INFO]: Training loss at epoch 27: 0.9840587675571442
[08/12/2025 16:48:34 INFO]: New best epoch, val score: -0.8798710394562788
[08/12/2025 16:48:34 INFO]: Saving model to: model_best.pth
[08/12/2025 16:48:36 INFO]: New best epoch, val score: -0.8962795821044333
[08/12/2025 16:48:36 INFO]: Saving model to: model_best.pth
[08/12/2025 16:48:39 INFO]: Training loss at epoch 25: 1.0483081340789795
[08/12/2025 16:48:41 INFO]: Training loss at epoch 44: 0.6894761323928833
[08/12/2025 16:48:48 INFO]: Training loss at epoch 7: 0.9749393463134766
[08/12/2025 16:48:48 INFO]: Training loss at epoch 40: 1.0601413249969482
[08/12/2025 16:48:50 INFO]: New best epoch, val score: -0.879853593259697
[08/12/2025 16:48:50 INFO]: Saving model to: model_best.pth
[08/12/2025 16:48:55 INFO]: Training loss at epoch 15: 0.8854523301124573
[08/12/2025 16:48:56 INFO]: Training loss at epoch 45: 0.6910992562770844
[08/12/2025 16:48:57 INFO]: Training loss at epoch 28: 0.9448334574699402
[08/12/2025 16:48:58 INFO]: Training loss at epoch 26: 1.0603840947151184
[08/12/2025 16:48:59 INFO]: Training loss at epoch 12: 0.9212097227573395
[08/12/2025 16:49:00 INFO]: New best epoch, val score: -0.884265282867831
[08/12/2025 16:49:00 INFO]: Saving model to: model_best.pth
[08/12/2025 16:49:05 INFO]: Training loss at epoch 26: 0.8999338150024414
[08/12/2025 16:49:05 INFO]: Training loss at epoch 41: 0.8871512413024902
[08/12/2025 16:49:11 INFO]: Training loss at epoch 46: 0.6896948516368866
[08/12/2025 16:49:21 INFO]: Training loss at epoch 29: 0.8345537781715393
[08/12/2025 16:49:22 INFO]: Training loss at epoch 7: 1.0454291701316833
[08/12/2025 16:49:22 INFO]: Training loss at epoch 42: 0.7809828221797943
[08/12/2025 16:49:24 INFO]: Training loss at epoch 27: 0.9335969090461731
[08/12/2025 16:49:26 INFO]: Training loss at epoch 47: 0.7320093214511871
[08/12/2025 16:49:29 INFO]: Training stats: {
    "score": -0.9107112762019708,
    "rmse": 0.9107112762019708
}
[08/12/2025 16:49:29 INFO]: Val stats: {
    "score": -0.8787829913125821,
    "rmse": 0.8787829913125821
}
[08/12/2025 16:49:29 INFO]: Test stats: {
    "score": -0.8394209265656485,
    "rmse": 0.8394209265656485
}
[08/12/2025 16:49:30 INFO]: Training loss at epoch 27: 0.9836365282535553
[08/12/2025 16:49:32 INFO]: New best epoch, val score: -0.8787829913125821
[08/12/2025 16:49:32 INFO]: Saving model to: model_best.pth
[08/12/2025 16:49:39 INFO]: Training loss at epoch 43: 0.9754596948623657
[08/12/2025 16:49:40 INFO]: Training loss at epoch 16: 0.9227226972579956
[08/12/2025 16:49:41 INFO]: Training loss at epoch 48: 0.7801218330860138
[08/12/2025 16:49:50 INFO]: Training loss at epoch 28: 1.0524277687072754
[08/12/2025 16:49:52 INFO]: Training loss at epoch 30: 0.8902029991149902
[08/12/2025 16:49:54 INFO]: Training loss at epoch 13: 1.035000503063202
[08/12/2025 16:49:55 INFO]: New best epoch, val score: -0.8778676149144539
[08/12/2025 16:49:55 INFO]: Saving model to: model_best.pth
[08/12/2025 16:49:56 INFO]: Training loss at epoch 28: 0.9798513650894165
[08/12/2025 16:49:56 INFO]: Training loss at epoch 44: 0.7671033143997192
[08/12/2025 16:49:56 INFO]: Training loss at epoch 49: 1.0172423422336578
[08/12/2025 16:50:02 INFO]: Training stats: {
    "score": -0.7988565391213377,
    "rmse": 0.7988565391213377
}
[08/12/2025 16:50:02 INFO]: Val stats: {
    "score": -0.8877909817618048,
    "rmse": 0.8877909817618048
}
[08/12/2025 16:50:02 INFO]: Test stats: {
    "score": -0.7903440233418187,
    "rmse": 0.7903440233418187
}
[08/12/2025 16:50:13 INFO]: Training loss at epoch 45: 0.7590933740139008
[08/12/2025 16:50:15 INFO]: Training loss at epoch 8: 0.8858789205551147
[08/12/2025 16:50:16 INFO]: Training loss at epoch 29: 0.8357813656330109
[08/12/2025 16:50:16 INFO]: Training loss at epoch 31: 0.6725380271673203
[08/12/2025 16:50:17 INFO]: Training loss at epoch 50: 0.7297110557556152
[08/12/2025 16:50:21 INFO]: Training loss at epoch 29: 0.8993209302425385
[08/12/2025 16:50:24 INFO]: Training loss at epoch 17: 1.1261138916015625
[08/12/2025 16:50:26 INFO]: Training stats: {
    "score": -0.9957768112962255,
    "rmse": 0.9957768112962255
}
[08/12/2025 16:50:26 INFO]: Val stats: {
    "score": -0.9478614172692775,
    "rmse": 0.9478614172692775
}
[08/12/2025 16:50:26 INFO]: Test stats: {
    "score": -0.9188726549228892,
    "rmse": 0.9188726549228892
}
[08/12/2025 16:50:30 INFO]: Training loss at epoch 46: 0.6614560037851334
[08/12/2025 16:50:30 INFO]: Training stats: {
    "score": -0.9151620027790592,
    "rmse": 0.9151620027790592
}
[08/12/2025 16:50:30 INFO]: Val stats: {
    "score": -0.9174585702151022,
    "rmse": 0.9174585702151022
}
[08/12/2025 16:50:30 INFO]: Test stats: {
    "score": -0.8550346116141679,
    "rmse": 0.8550346116141679
}
[08/12/2025 16:50:32 INFO]: Training loss at epoch 51: 0.9054625332355499
[08/12/2025 16:50:33 INFO]: New best epoch, val score: -0.9174585702151022
[08/12/2025 16:50:33 INFO]: Saving model to: model_best.pth
[08/12/2025 16:50:40 INFO]: Training loss at epoch 32: 0.8083702623844147
[08/12/2025 16:50:46 INFO]: Training loss at epoch 47: 0.8296926915645599
[08/12/2025 16:50:47 INFO]: Training loss at epoch 52: 0.9121384024620056
[08/12/2025 16:50:49 INFO]: Training loss at epoch 14: 0.9162211418151855
[08/12/2025 16:50:52 INFO]: Training loss at epoch 30: 0.9177019596099854
[08/12/2025 16:50:53 INFO]: Training loss at epoch 8: 0.8679255247116089
[08/12/2025 16:50:55 INFO]: Training loss at epoch 30: 0.7472718060016632
[08/12/2025 16:50:59 INFO]: New best epoch, val score: -0.9035279018563898
[08/12/2025 16:50:59 INFO]: Saving model to: model_best.pth
[08/12/2025 16:51:02 INFO]: Training loss at epoch 53: 0.7108938694000244
[08/12/2025 16:51:03 INFO]: Training loss at epoch 33: 0.7621616721153259
[08/12/2025 16:51:03 INFO]: Training loss at epoch 48: 0.8105148375034332
[08/12/2025 16:51:05 INFO]: New best epoch, val score: -0.8795572991173229
[08/12/2025 16:51:05 INFO]: Saving model to: model_best.pth
[08/12/2025 16:51:09 INFO]: Training loss at epoch 18: 0.8782253861427307
[08/12/2025 16:51:17 INFO]: Training loss at epoch 54: 0.606098398566246
[08/12/2025 16:51:18 INFO]: Training loss at epoch 31: 0.9869923889636993
[08/12/2025 16:51:20 INFO]: Training loss at epoch 49: 0.8050497472286224
[08/12/2025 16:51:21 INFO]: Training loss at epoch 31: 0.9317002594470978
[08/12/2025 16:51:24 INFO]: New best epoch, val score: -0.8773162108319655
[08/12/2025 16:51:24 INFO]: Saving model to: model_best.pth
[08/12/2025 16:51:26 INFO]: Training stats: {
    "score": -0.9021496478011499,
    "rmse": 0.9021496478011499
}
[08/12/2025 16:51:26 INFO]: Val stats: {
    "score": -0.8780532598734337,
    "rmse": 0.8780532598734337
}
[08/12/2025 16:51:26 INFO]: Test stats: {
    "score": -0.8305791208956097,
    "rmse": 0.8305791208956097
}
[08/12/2025 16:51:27 INFO]: Training loss at epoch 34: 0.7337611615657806
[08/12/2025 16:51:28 INFO]: New best epoch, val score: -0.8780532598734337
[08/12/2025 16:51:28 INFO]: Saving model to: model_best.pth
[08/12/2025 16:51:32 INFO]: Training loss at epoch 55: 0.5612216591835022
[08/12/2025 16:51:42 INFO]: Training loss at epoch 9: 0.9061834812164307
[08/12/2025 16:51:43 INFO]: Training loss at epoch 50: 0.7823752760887146
[08/12/2025 16:51:44 INFO]: Training loss at epoch 15: 0.9682780206203461
[08/12/2025 16:51:44 INFO]: Training loss at epoch 32: 0.9567042887210846
[08/12/2025 16:51:45 INFO]: New best epoch, val score: -0.8766367888456225
[08/12/2025 16:51:45 INFO]: Saving model to: model_best.pth
[08/12/2025 16:51:47 INFO]: Training loss at epoch 32: 1.1256092190742493
[08/12/2025 16:51:47 INFO]: Training loss at epoch 56: 0.7925132513046265
[08/12/2025 16:51:50 INFO]: New best epoch, val score: -0.863084162051884
[08/12/2025 16:51:50 INFO]: Saving model to: model_best.pth
[08/12/2025 16:51:50 INFO]: Training loss at epoch 35: 0.5782674252986908
[08/12/2025 16:51:53 INFO]: Training loss at epoch 19: 0.9321271181106567
[08/12/2025 16:52:00 INFO]: Training loss at epoch 51: 1.070135772228241
[08/12/2025 16:52:02 INFO]: New best epoch, val score: -0.875390069772131
[08/12/2025 16:52:02 INFO]: Saving model to: model_best.pth
[08/12/2025 16:52:03 INFO]: Training loss at epoch 57: 0.7461794316768646
[08/12/2025 16:52:08 INFO]: Training stats: {
    "score": -0.9858578607706892,
    "rmse": 0.9858578607706892
}
[08/12/2025 16:52:08 INFO]: Val stats: {
    "score": -0.953471914921676,
    "rmse": 0.953471914921676
}
[08/12/2025 16:52:08 INFO]: Test stats: {
    "score": -0.9169179095641451,
    "rmse": 0.9169179095641451
}
[08/12/2025 16:52:10 INFO]: Training loss at epoch 33: 0.8028326630592346
[08/12/2025 16:52:11 INFO]: Training stats: {
    "score": -1.05586158109362,
    "rmse": 1.05586158109362
}
[08/12/2025 16:52:11 INFO]: Val stats: {
    "score": -0.9348711099124006,
    "rmse": 0.9348711099124006
}
[08/12/2025 16:52:11 INFO]: Test stats: {
    "score": -0.9554470266142605,
    "rmse": 0.9554470266142605
}
[08/12/2025 16:52:12 INFO]: Training loss at epoch 33: 0.8492230474948883
[08/12/2025 16:52:14 INFO]: Training loss at epoch 36: 0.6617651283740997
[08/12/2025 16:52:15 INFO]: New best epoch, val score: -0.8588165594522321
[08/12/2025 16:52:15 INFO]: Saving model to: model_best.pth
[08/12/2025 16:52:16 INFO]: New best epoch, val score: -0.866284565775614
[08/12/2025 16:52:16 INFO]: Saving model to: model_best.pth
[08/12/2025 16:52:17 INFO]: Training loss at epoch 52: 0.7534891963005066
[08/12/2025 16:52:18 INFO]: Training loss at epoch 58: 0.7753376960754395
[08/12/2025 16:52:19 INFO]: New best epoch, val score: -0.8738897164883443
[08/12/2025 16:52:19 INFO]: Saving model to: model_best.pth
[08/12/2025 16:52:25 INFO]: Training loss at epoch 9: 0.8678141534328461
[08/12/2025 16:52:33 INFO]: Training loss at epoch 59: 0.7077129781246185
[08/12/2025 16:52:35 INFO]: Training loss at epoch 53: 0.9719038903713226
[08/12/2025 16:52:36 INFO]: Training loss at epoch 34: 0.8372741937637329
[08/12/2025 16:52:37 INFO]: New best epoch, val score: -0.8727297137043125
[08/12/2025 16:52:37 INFO]: Saving model to: model_best.pth
[08/12/2025 16:52:37 INFO]: Training loss at epoch 37: 0.8153476715087891
[08/12/2025 16:52:38 INFO]: Training loss at epoch 34: 0.7649179399013519
[08/12/2025 16:52:38 INFO]: Training stats: {
    "score": -0.8301565508397026,
    "rmse": 0.8301565508397026
}
[08/12/2025 16:52:38 INFO]: Val stats: {
    "score": -0.9197588715460535,
    "rmse": 0.9197588715460535
}
[08/12/2025 16:52:38 INFO]: Test stats: {
    "score": -0.8473687817901218,
    "rmse": 0.8473687817901218
}
[08/12/2025 16:52:39 INFO]: Training loss at epoch 16: 1.015549659729004
[08/12/2025 16:52:40 INFO]: New best epoch, val score: -0.8561107124655722
[08/12/2025 16:52:40 INFO]: Saving model to: model_best.pth
[08/12/2025 16:52:41 INFO]: New best epoch, val score: -0.8539732281528596
[08/12/2025 16:52:41 INFO]: Saving model to: model_best.pth
[08/12/2025 16:52:52 INFO]: Training loss at epoch 54: 0.8680494725704193
[08/12/2025 16:52:53 INFO]: Training loss at epoch 20: 1.098561942577362
[08/12/2025 16:52:54 INFO]: Training loss at epoch 60: 0.7691755592823029
[08/12/2025 16:52:56 INFO]: Training stats: {
    "score": -1.0034932583168865,
    "rmse": 1.0034932583168865
}
[08/12/2025 16:52:56 INFO]: Val stats: {
    "score": -0.9383280168844679,
    "rmse": 0.9383280168844679
}
[08/12/2025 16:52:56 INFO]: Test stats: {
    "score": -0.9174377856037338,
    "rmse": 0.9174377856037338
}
[08/12/2025 16:53:01 INFO]: Training loss at epoch 38: 0.6182896196842194
[08/12/2025 16:53:03 INFO]: Training loss at epoch 35: 1.171916663646698
[08/12/2025 16:53:03 INFO]: Training loss at epoch 35: 0.9006442129611969
[08/12/2025 16:53:04 INFO]: New best epoch, val score: -0.8475124466726025
[08/12/2025 16:53:04 INFO]: Saving model to: model_best.pth
[08/12/2025 16:53:08 INFO]: Training loss at epoch 55: 0.8601878881454468
[08/12/2025 16:53:09 INFO]: Training loss at epoch 61: 0.6993358135223389
[08/12/2025 16:53:11 INFO]: Running Final Evaluation...
[08/12/2025 16:53:25 INFO]: Training loss at epoch 39: 0.6238765716552734
[08/12/2025 16:53:25 INFO]: Training loss at epoch 56: 0.7239271700382233
[08/12/2025 16:53:28 INFO]: Training loss at epoch 36: 0.8028440773487091
[08/12/2025 16:53:29 INFO]: Training loss at epoch 36: 0.8540826141834259
[08/12/2025 16:53:33 INFO]: Training stats: {
    "score": -0.8084099995454681,
    "rmse": 0.8084099995454681
}
[08/12/2025 16:53:33 INFO]: Val stats: {
    "score": -0.8526975604985659,
    "rmse": 0.8526975604985659
}
[08/12/2025 16:53:33 INFO]: Test stats: {
    "score": -0.7886201901038837,
    "rmse": 0.7886201901038837
}
[08/12/2025 16:53:33 INFO]: Training loss at epoch 17: 0.9373595118522644
[08/12/2025 16:53:36 INFO]: Training loss at epoch 21: 0.8621900081634521
[08/12/2025 16:53:38 INFO]: Training loss at epoch 10: 1.0068269968032837
[08/12/2025 16:53:41 INFO]: Training loss at epoch 57: 0.8042330741882324
[08/12/2025 16:53:54 INFO]: Training loss at epoch 37: 0.827298492193222
[08/12/2025 16:53:54 INFO]: Training loss at epoch 37: 0.8454272449016571
[08/12/2025 16:53:56 INFO]: Training loss at epoch 40: 0.6209202706813812
[08/12/2025 16:53:58 INFO]: Training loss at epoch 58: 0.6252782493829727
[08/12/2025 16:54:00 INFO]: New best epoch, val score: -0.8710995921077103
[08/12/2025 16:54:00 INFO]: Saving model to: model_best.pth
[08/12/2025 16:54:14 INFO]: Training loss at epoch 59: 0.8219397366046906
[08/12/2025 16:54:19 INFO]: Training loss at epoch 38: 0.7366683781147003
[08/12/2025 16:54:19 INFO]: Training loss at epoch 22: 1.0894729495048523
[08/12/2025 16:54:20 INFO]: Training loss at epoch 41: 0.5557540506124496
[08/12/2025 16:54:20 INFO]: Training loss at epoch 38: 0.9227764904499054
[08/12/2025 16:54:20 INFO]: Training stats: {
    "score": -0.8815994437292402,
    "rmse": 0.8815994437292402
}
[08/12/2025 16:54:20 INFO]: Val stats: {
    "score": -0.8687859772525567,
    "rmse": 0.8687859772525567
}
[08/12/2025 16:54:20 INFO]: Test stats: {
    "score": -0.8157966975326847,
    "rmse": 0.8157966975326847
}
[08/12/2025 16:54:22 INFO]: New best epoch, val score: -0.8687859772525567
[08/12/2025 16:54:22 INFO]: Saving model to: model_best.pth
[08/12/2025 16:54:23 INFO]: Running Final Evaluation...
[08/12/2025 16:54:27 INFO]: Training loss at epoch 18: 1.0186675488948822
[08/12/2025 16:54:28 INFO]: Training loss at epoch 10: 1.175411731004715
[08/12/2025 16:54:36 INFO]: Training loss at epoch 60: 0.801904946565628
[08/12/2025 16:54:43 INFO]: Training loss at epoch 42: 0.5681826770305634
[08/12/2025 16:54:44 INFO]: Training loss at epoch 39: 0.9496535062789917
[08/12/2025 16:54:53 INFO]: Training stats: {
    "score": -0.8300322632031821,
    "rmse": 0.8300322632031821
}
[08/12/2025 16:54:53 INFO]: Val stats: {
    "score": -0.9021628781838219,
    "rmse": 0.9021628781838219
}
[08/12/2025 16:54:53 INFO]: Test stats: {
    "score": -0.7928622787309073,
    "rmse": 0.7928622787309073
}
[08/12/2025 16:54:53 INFO]: Training loss at epoch 61: 0.6804541945457458
[08/12/2025 16:55:02 INFO]: Training loss at epoch 23: 0.9824987053871155
[08/12/2025 16:55:05 INFO]: Training loss at epoch 11: 1.2405508160591125
[08/12/2025 16:55:06 INFO]: Training loss at epoch 43: 0.6657252311706543
[08/12/2025 16:55:09 INFO]: Training loss at epoch 62: 0.895669162273407
[08/12/2025 16:55:11 INFO]: New best epoch, val score: -0.8667255010079468
[08/12/2025 16:55:11 INFO]: Saving model to: model_best.pth
[08/12/2025 16:55:18 INFO]: Training loss at epoch 40: 0.7963515222072601
[08/12/2025 16:55:20 INFO]: Training loss at epoch 19: 0.9457167088985443
[08/12/2025 16:55:25 INFO]: Training loss at epoch 63: 0.8151869475841522
[08/12/2025 16:55:27 INFO]: New best epoch, val score: -0.8652957542446097
[08/12/2025 16:55:27 INFO]: Saving model to: model_best.pth
[08/12/2025 16:55:30 INFO]: Training loss at epoch 44: 0.5681548714637756
[08/12/2025 16:55:38 INFO]: Training stats: {
    "score": -0.9974058817368413,
    "rmse": 0.9974058817368413
}
[08/12/2025 16:55:38 INFO]: Val stats: {
    "score": -0.9869166814979501,
    "rmse": 0.9869166814979501
}
[08/12/2025 16:55:38 INFO]: Test stats: {
    "score": -0.9472179056373672,
    "rmse": 0.9472179056373672
}
[08/12/2025 16:55:42 INFO]: Training loss at epoch 64: 0.8531780540943146
[08/12/2025 16:55:42 INFO]: Training loss at epoch 41: 0.7980968654155731
[08/12/2025 16:55:44 INFO]: New best epoch, val score: -0.8650016152327884
[08/12/2025 16:55:44 INFO]: Saving model to: model_best.pth
[08/12/2025 16:55:45 INFO]: Training loss at epoch 24: 0.8714694976806641
[08/12/2025 16:55:53 INFO]: Training loss at epoch 45: 0.6208983361721039
[08/12/2025 16:55:58 INFO]: Training loss at epoch 11: 0.9838035404682159
[08/12/2025 16:55:58 INFO]: Training loss at epoch 65: 0.7923720479011536
[08/12/2025 16:56:07 INFO]: Training loss at epoch 42: 0.7610271275043488
[08/12/2025 16:56:15 INFO]: Training loss at epoch 66: 0.9180958271026611
[08/12/2025 16:56:16 INFO]: Training loss at epoch 46: 0.5040386468172073
[08/12/2025 16:56:28 INFO]: Training loss at epoch 25: 1.0629770755767822
[08/12/2025 16:56:31 INFO]: Training loss at epoch 67: 0.8867400288581848
[08/12/2025 16:56:31 INFO]: Training loss at epoch 20: 0.9502221345901489
[08/12/2025 16:56:32 INFO]: Training loss at epoch 43: 0.7301134169101715
[08/12/2025 16:56:33 INFO]: New best epoch, val score: -0.8616363736253089
[08/12/2025 16:56:33 INFO]: Saving model to: model_best.pth
[08/12/2025 16:56:33 INFO]: Training loss at epoch 12: 1.4800215363502502
[08/12/2025 16:56:40 INFO]: Training loss at epoch 47: 0.5558490753173828
[08/12/2025 16:56:47 INFO]: Training loss at epoch 68: 0.6550683975219727
[08/12/2025 16:56:49 INFO]: New best epoch, val score: -0.8567549605809003
[08/12/2025 16:56:49 INFO]: Saving model to: model_best.pth
[08/12/2025 16:56:57 INFO]: Training loss at epoch 44: 0.7431451678276062
[08/12/2025 16:57:03 INFO]: Training loss at epoch 69: 0.8324477672576904
[08/12/2025 16:57:03 INFO]: Training loss at epoch 48: 0.7327958643436432
[08/12/2025 16:57:09 INFO]: Training stats: {
    "score": -0.8610500125232781,
    "rmse": 0.8610500125232781
}
[08/12/2025 16:57:09 INFO]: Val stats: {
    "score": -0.8523472944731023,
    "rmse": 0.8523472944731023
}
[08/12/2025 16:57:09 INFO]: Test stats: {
    "score": -0.7987035232291897,
    "rmse": 0.7987035232291897
}
[08/12/2025 16:57:10 INFO]: Training loss at epoch 26: 0.9601622819900513
[08/12/2025 16:57:11 INFO]: New best epoch, val score: -0.8523472944731023
[08/12/2025 16:57:11 INFO]: Saving model to: model_best.pth
[08/12/2025 16:57:22 INFO]: Training loss at epoch 45: 0.6400695443153381
[08/12/2025 16:57:23 INFO]: Training loss at epoch 21: 1.0919766426086426
[08/12/2025 16:57:25 INFO]: Training loss at epoch 70: 0.8808808028697968
[08/12/2025 16:57:27 INFO]: New best epoch, val score: -0.848438567386806
[08/12/2025 16:57:27 INFO]: Saving model to: model_best.pth
[08/12/2025 16:57:27 INFO]: Training loss at epoch 49: 0.6510366499423981
[08/12/2025 16:57:29 INFO]: Training loss at epoch 12: 0.844107985496521
[08/12/2025 16:57:35 INFO]: Training stats: {
    "score": -0.7255576873092862,
    "rmse": 0.7255576873092862
}
[08/12/2025 16:57:35 INFO]: Val stats: {
    "score": -0.8485545884490747,
    "rmse": 0.8485545884490747
}
[08/12/2025 16:57:35 INFO]: Test stats: {
    "score": -0.7286709599494076,
    "rmse": 0.7286709599494076
}
[08/12/2025 16:57:41 INFO]: Training loss at epoch 71: 0.7323607504367828
[08/12/2025 16:57:43 INFO]: New best epoch, val score: -0.8467548078458284
[08/12/2025 16:57:43 INFO]: Saving model to: model_best.pth
[08/12/2025 16:57:48 INFO]: Training loss at epoch 46: 0.8288609683513641
[08/12/2025 16:57:52 INFO]: Training loss at epoch 27: 0.9851265251636505
[08/12/2025 16:57:57 INFO]: Training loss at epoch 72: 0.7207098603248596
[08/12/2025 16:57:58 INFO]: Training loss at epoch 50: 0.5273239016532898
[08/12/2025 16:57:59 INFO]: New best epoch, val score: -0.8459027999079162
[08/12/2025 16:57:59 INFO]: Saving model to: model_best.pth
[08/12/2025 16:58:00 INFO]: Training loss at epoch 13: 1.0886061489582062
[08/12/2025 16:58:01 INFO]: New best epoch, val score: -0.8317227043086819
[08/12/2025 16:58:01 INFO]: Saving model to: model_best.pth
[08/12/2025 16:58:13 INFO]: Training loss at epoch 47: 0.7126178443431854
[08/12/2025 16:58:13 INFO]: Training loss at epoch 73: 0.7014011144638062
[08/12/2025 16:58:15 INFO]: New best epoch, val score: -0.8437942343961925
[08/12/2025 16:58:15 INFO]: Saving model to: model_best.pth
[08/12/2025 16:58:16 INFO]: Training loss at epoch 22: 0.8841287791728973
[08/12/2025 16:58:21 INFO]: Training loss at epoch 51: 0.5288369953632355
[08/12/2025 16:58:24 INFO]: New best epoch, val score: -0.8279088879871501
[08/12/2025 16:58:24 INFO]: Saving model to: model_best.pth
[08/12/2025 16:58:30 INFO]: Training loss at epoch 74: 0.7653427124023438
[08/12/2025 16:58:32 INFO]: New best epoch, val score: -0.8417075802364007
[08/12/2025 16:58:32 INFO]: Saving model to: model_best.pth
[08/12/2025 16:58:35 INFO]: Training loss at epoch 28: 1.1202912032604218
[08/12/2025 16:58:41 INFO]: Training loss at epoch 48: 0.8273186087608337
[08/12/2025 16:58:45 INFO]: Training loss at epoch 52: 0.5172024965286255
[08/12/2025 16:58:46 INFO]: Training loss at epoch 75: 0.7293404340744019
[08/12/2025 16:58:48 INFO]: New best epoch, val score: -0.8270499426434432
[08/12/2025 16:58:48 INFO]: Saving model to: model_best.pth
[08/12/2025 16:58:49 INFO]: New best epoch, val score: -0.8375132639020997
[08/12/2025 16:58:49 INFO]: Saving model to: model_best.pth
[08/12/2025 16:59:00 INFO]: Training loss at epoch 13: 0.923536479473114
[08/12/2025 16:59:04 INFO]: Training loss at epoch 76: 0.6524124443531036
[08/12/2025 16:59:06 INFO]: New best epoch, val score: -0.832415453234751
[08/12/2025 16:59:06 INFO]: Saving model to: model_best.pth
[08/12/2025 16:59:09 INFO]: Training loss at epoch 49: 0.8213585317134857
[08/12/2025 16:59:09 INFO]: Training loss at epoch 53: 0.5514542162418365
[08/12/2025 16:59:10 INFO]: Training loss at epoch 23: 0.9509662985801697
[08/12/2025 16:59:11 INFO]: New best epoch, val score: -0.8218903624918301
[08/12/2025 16:59:11 INFO]: Saving model to: model_best.pth
[08/12/2025 16:59:18 INFO]: Training stats: {
    "score": -0.8755863780646856,
    "rmse": 0.8755863780646856
}
[08/12/2025 16:59:18 INFO]: Val stats: {
    "score": -0.9748622546377068,
    "rmse": 0.9748622546377068
}
[08/12/2025 16:59:18 INFO]: Test stats: {
    "score": -0.8919257923091832,
    "rmse": 0.8919257923091832
}
[08/12/2025 16:59:19 INFO]: Training loss at epoch 29: 1.0898972153663635
[08/12/2025 16:59:20 INFO]: Training loss at epoch 77: 0.6378558427095413
[08/12/2025 16:59:22 INFO]: New best epoch, val score: -0.8277460043468338
[08/12/2025 16:59:22 INFO]: Saving model to: model_best.pth
[08/12/2025 16:59:27 INFO]: Training loss at epoch 14: 0.7577029466629028
[08/12/2025 16:59:32 INFO]: Training loss at epoch 54: 0.4446942061185837
[08/12/2025 16:59:34 INFO]: Training stats: {
    "score": -0.9796808370899668,
    "rmse": 0.9796808370899668
}
[08/12/2025 16:59:34 INFO]: Val stats: {
    "score": -0.9525471089033016,
    "rmse": 0.9525471089033016
}
[08/12/2025 16:59:34 INFO]: Test stats: {
    "score": -0.9136308358362567,
    "rmse": 0.9136308358362567
}
[08/12/2025 16:59:36 INFO]: New best epoch, val score: -0.837027698166757
[08/12/2025 16:59:36 INFO]: Saving model to: model_best.pth
[08/12/2025 16:59:37 INFO]: Training loss at epoch 78: 0.6326245069503784
[08/12/2025 16:59:39 INFO]: New best epoch, val score: -0.8241658820232904
[08/12/2025 16:59:39 INFO]: Saving model to: model_best.pth
[08/12/2025 16:59:44 INFO]: Training loss at epoch 50: 0.6725557148456573
[08/12/2025 16:59:54 INFO]: Training loss at epoch 79: 0.7270158231258392
[08/12/2025 16:59:55 INFO]: Training loss at epoch 55: 0.474445641040802
[08/12/2025 17:00:00 INFO]: Training stats: {
    "score": -0.8463904989362321,
    "rmse": 0.8463904989362321
}
[08/12/2025 17:00:00 INFO]: Val stats: {
    "score": -0.8219133296357412,
    "rmse": 0.8219133296357412
}
[08/12/2025 17:00:00 INFO]: Test stats: {
    "score": -0.7823346691665699,
    "rmse": 0.7823346691665699
}
[08/12/2025 17:00:02 INFO]: New best epoch, val score: -0.8219133296357412
[08/12/2025 17:00:02 INFO]: Saving model to: model_best.pth
[08/12/2025 17:00:04 INFO]: Training loss at epoch 24: 1.044418215751648
[08/12/2025 17:00:10 INFO]: Training loss at epoch 51: 0.8400881588459015
[08/12/2025 17:00:16 INFO]: Training loss at epoch 80: 0.628859132528305
[08/12/2025 17:00:17 INFO]: Training loss at epoch 30: 0.9169781804084778
[08/12/2025 17:00:18 INFO]: New best epoch, val score: -0.8217451522234176
[08/12/2025 17:00:18 INFO]: Saving model to: model_best.pth
[08/12/2025 17:00:19 INFO]: Training loss at epoch 56: 0.40921083092689514
[08/12/2025 17:00:29 INFO]: Training loss at epoch 14: 0.9971930682659149
[08/12/2025 17:00:33 INFO]: Training loss at epoch 81: 0.6329051852226257
[08/12/2025 17:00:35 INFO]: New best epoch, val score: -0.820722005234171
[08/12/2025 17:00:35 INFO]: Saving model to: model_best.pth
[08/12/2025 17:00:36 INFO]: Training loss at epoch 52: 0.6236719489097595
[08/12/2025 17:00:42 INFO]: Training loss at epoch 57: 0.48885537683963776
[08/12/2025 17:00:44 INFO]: New best epoch, val score: -0.8142106523234147
[08/12/2025 17:00:44 INFO]: Saving model to: model_best.pth
[08/12/2025 17:00:50 INFO]: Training loss at epoch 82: 0.7400796115398407
[08/12/2025 17:00:52 INFO]: New best epoch, val score: -0.8204844400026076
[08/12/2025 17:00:52 INFO]: Saving model to: model_best.pth
[08/12/2025 17:00:52 INFO]: Training loss at epoch 15: 0.6390053182840347
[08/12/2025 17:00:58 INFO]: Training loss at epoch 25: 0.9819690585136414
[08/12/2025 17:01:01 INFO]: Training loss at epoch 31: 1.0302200317382812
[08/12/2025 17:01:01 INFO]: Training loss at epoch 53: 0.7458082735538483
[08/12/2025 17:01:05 INFO]: Training loss at epoch 58: 0.4782925546169281
[08/12/2025 17:01:06 INFO]: Training loss at epoch 83: 0.8592252135276794
[08/12/2025 17:01:08 INFO]: New best epoch, val score: -0.8082432829901974
[08/12/2025 17:01:08 INFO]: Saving model to: model_best.pth
[08/12/2025 17:01:23 INFO]: Training loss at epoch 84: 0.7373582720756531
[08/12/2025 17:01:28 INFO]: Training loss at epoch 54: 0.6686837673187256
[08/12/2025 17:01:29 INFO]: Training loss at epoch 59: 0.43292538821697235
[08/12/2025 17:01:36 INFO]: Training stats: {
    "score": -0.6780770993132983,
    "rmse": 0.6780770993132983
}
[08/12/2025 17:01:36 INFO]: Val stats: {
    "score": -0.8183132209436518,
    "rmse": 0.8183132209436518
}
[08/12/2025 17:01:36 INFO]: Test stats: {
    "score": -0.7186198464607791,
    "rmse": 0.7186198464607791
}
[08/12/2025 17:01:39 INFO]: Training loss at epoch 85: 0.9291923344135284
[08/12/2025 17:01:44 INFO]: Training loss at epoch 32: 1.0505266189575195
[08/12/2025 17:01:51 INFO]: Training loss at epoch 26: 1.0628138780593872
[08/12/2025 17:01:53 INFO]: Training loss at epoch 55: 0.7848667502403259
[08/12/2025 17:01:56 INFO]: Training loss at epoch 86: 0.9218459129333496
[08/12/2025 17:02:00 INFO]: Training loss at epoch 60: 0.46831488609313965
[08/12/2025 17:02:00 INFO]: Training loss at epoch 15: 1.064142495393753
[08/12/2025 17:02:12 INFO]: Training loss at epoch 87: 0.7651170194149017
[08/12/2025 17:02:18 INFO]: Training loss at epoch 16: 0.9229593276977539
[08/12/2025 17:02:19 INFO]: Training loss at epoch 56: 0.5684700310230255
[08/12/2025 17:02:23 INFO]: Training loss at epoch 61: 0.5494515746831894
[08/12/2025 17:02:27 INFO]: Training loss at epoch 33: 0.8281269669532776
[08/12/2025 17:02:28 INFO]: Training loss at epoch 88: 0.6342134475708008
[08/12/2025 17:02:44 INFO]: Training loss at epoch 57: 0.7120146751403809
[08/12/2025 17:02:44 INFO]: Training loss at epoch 27: 0.9575284719467163
[08/12/2025 17:02:45 INFO]: Training loss at epoch 89: 0.7505929470062256
[08/12/2025 17:02:46 INFO]: Training loss at epoch 62: 0.3913854509592056
[08/12/2025 17:02:51 INFO]: Training stats: {
    "score": -0.8318306361643326,
    "rmse": 0.8318306361643326
}
[08/12/2025 17:02:51 INFO]: Val stats: {
    "score": -0.8715464662981545,
    "rmse": 0.8715464662981545
}
[08/12/2025 17:02:51 INFO]: Test stats: {
    "score": -0.7932500364918427,
    "rmse": 0.7932500364918427
}
[08/12/2025 17:03:07 INFO]: Training loss at epoch 90: 0.7729185521602631
[08/12/2025 17:03:09 INFO]: Training loss at epoch 63: 0.5773443281650543
[08/12/2025 17:03:10 INFO]: Training loss at epoch 58: 0.7072644531726837
[08/12/2025 17:03:11 INFO]: Training loss at epoch 34: 1.078406810760498
[08/12/2025 17:03:24 INFO]: Training loss at epoch 91: 0.720047652721405
[08/12/2025 17:03:29 INFO]: Training loss at epoch 16: 1.0438814461231232
[08/12/2025 17:03:32 INFO]: Training loss at epoch 64: 0.3783288896083832
[08/12/2025 17:03:35 INFO]: Training loss at epoch 59: 0.581336498260498
[08/12/2025 17:03:38 INFO]: Training loss at epoch 28: 0.9288430511951447
[08/12/2025 17:03:40 INFO]: Training loss at epoch 92: 0.7762739360332489
[08/12/2025 17:03:43 INFO]: Training loss at epoch 17: 1.148143857717514
[08/12/2025 17:03:44 INFO]: Training stats: {
    "score": -0.9195349950465165,
    "rmse": 0.9195349950465165
}
[08/12/2025 17:03:44 INFO]: Val stats: {
    "score": -1.1127040167588793,
    "rmse": 1.1127040167588793
}
[08/12/2025 17:03:44 INFO]: Test stats: {
    "score": -0.9371317845575925,
    "rmse": 0.9371317845575925
}
[08/12/2025 17:03:54 INFO]: Training loss at epoch 35: 0.9791663587093353
[08/12/2025 17:03:55 INFO]: Training loss at epoch 65: 0.39693333208560944
[08/12/2025 17:03:56 INFO]: Training loss at epoch 93: 0.7709291875362396
[08/12/2025 17:03:58 INFO]: New best epoch, val score: -0.8141662287784147
[08/12/2025 17:03:58 INFO]: Saving model to: model_best.pth
[08/12/2025 17:03:58 INFO]: New best epoch, val score: -0.8063026305488337
[08/12/2025 17:03:58 INFO]: Saving model to: model_best.pth
[08/12/2025 17:04:10 INFO]: Training loss at epoch 60: 0.6949208676815033
[08/12/2025 17:04:13 INFO]: Training loss at epoch 94: 0.7452206611633301
[08/12/2025 17:04:15 INFO]: New best epoch, val score: -0.8044827075033794
[08/12/2025 17:04:15 INFO]: Saving model to: model_best.pth
[08/12/2025 17:04:20 INFO]: Training loss at epoch 66: 0.3741008788347244
[08/12/2025 17:04:22 INFO]: New best epoch, val score: -0.7946490509462977
[08/12/2025 17:04:22 INFO]: Saving model to: model_best.pth
[08/12/2025 17:04:30 INFO]: Training loss at epoch 95: 0.614937424659729
[08/12/2025 17:04:32 INFO]: Training loss at epoch 29: 1.168770283460617
[08/12/2025 17:04:32 INFO]: New best epoch, val score: -0.7998692732403736
[08/12/2025 17:04:32 INFO]: Saving model to: model_best.pth
[08/12/2025 17:04:36 INFO]: Training loss at epoch 61: 0.7515788376331329
[08/12/2025 17:04:37 INFO]: Training loss at epoch 36: 0.9616752564907074
[08/12/2025 17:04:43 INFO]: Training loss at epoch 67: 0.35403886437416077
[08/12/2025 17:04:46 INFO]: Training loss at epoch 96: 0.8497694432735443
[08/12/2025 17:04:48 INFO]: New best epoch, val score: -0.7981289548083309
[08/12/2025 17:04:48 INFO]: Saving model to: model_best.pth
[08/12/2025 17:04:51 INFO]: Training stats: {
    "score": -0.978823941026545,
    "rmse": 0.978823941026545
}
[08/12/2025 17:04:51 INFO]: Val stats: {
    "score": -0.9379313382555292,
    "rmse": 0.9379313382555292
}
[08/12/2025 17:04:51 INFO]: Test stats: {
    "score": -0.9101652075892517,
    "rmse": 0.9101652075892517
}
[08/12/2025 17:05:00 INFO]: Training loss at epoch 17: 0.9844024777412415
[08/12/2025 17:05:02 INFO]: Training loss at epoch 62: 0.7064261138439178
[08/12/2025 17:05:03 INFO]: Training loss at epoch 97: 0.6249971091747284
[08/12/2025 17:05:06 INFO]: New best epoch, val score: -0.7977396353857322
[08/12/2025 17:05:06 INFO]: Saving model to: model_best.pth
[08/12/2025 17:05:07 INFO]: Training loss at epoch 68: 0.3544531464576721
[08/12/2025 17:05:09 INFO]: Training loss at epoch 18: 0.7010741233825684
[08/12/2025 17:05:10 INFO]: New best epoch, val score: -0.7905691818384908
[08/12/2025 17:05:10 INFO]: Saving model to: model_best.pth
[08/12/2025 17:05:20 INFO]: Training loss at epoch 98: 0.5861596763134003
[08/12/2025 17:05:21 INFO]: Training loss at epoch 37: 0.9331698715686798
[08/12/2025 17:05:28 INFO]: Training loss at epoch 63: 0.613554447889328
[08/12/2025 17:05:31 INFO]: Training loss at epoch 69: 0.43773338198661804
[08/12/2025 17:05:31 INFO]: New best epoch, val score: -0.8343294854288938
[08/12/2025 17:05:31 INFO]: Saving model to: model_best.pth
[08/12/2025 17:05:37 INFO]: Training loss at epoch 99: 0.7649282217025757
[08/12/2025 17:05:39 INFO]: Training stats: {
    "score": -0.6277699036822837,
    "rmse": 0.6277699036822837
}
[08/12/2025 17:05:39 INFO]: Val stats: {
    "score": -0.806844581401928,
    "rmse": 0.806844581401928
}
[08/12/2025 17:05:39 INFO]: Test stats: {
    "score": -0.687320605461471,
    "rmse": 0.687320605461471
}
[08/12/2025 17:05:43 INFO]: Training stats: {
    "score": -0.8140145225394108,
    "rmse": 0.8140145225394108
}
[08/12/2025 17:05:43 INFO]: Val stats: {
    "score": -0.8029607976688873,
    "rmse": 0.8029607976688873
}
[08/12/2025 17:05:43 INFO]: Test stats: {
    "score": -0.7619995498397839,
    "rmse": 0.7619995498397839
}
[08/12/2025 17:05:45 INFO]: Training loss at epoch 30: 1.0761127471923828
[08/12/2025 17:05:54 INFO]: Training loss at epoch 64: 0.6094111353158951
[08/12/2025 17:05:57 INFO]: New best epoch, val score: -0.8315242540277863
[08/12/2025 17:05:57 INFO]: Saving model to: model_best.pth
[08/12/2025 17:05:59 INFO]: Training loss at epoch 100: 0.6307215392589569
[08/12/2025 17:06:02 INFO]: Training loss at epoch 70: 0.4391278922557831
[08/12/2025 17:06:04 INFO]: Training loss at epoch 38: 1.111155778169632
[08/12/2025 17:06:16 INFO]: Training loss at epoch 101: 0.6769483983516693
[08/12/2025 17:06:19 INFO]: Training loss at epoch 65: 0.7850200533866882
[08/12/2025 17:06:25 INFO]: Training loss at epoch 71: 0.33976083993911743
[08/12/2025 17:06:28 INFO]: New best epoch, val score: -0.7722138977055634
[08/12/2025 17:06:28 INFO]: Saving model to: model_best.pth
[08/12/2025 17:06:31 INFO]: Training loss at epoch 18: 1.1131321489810944
[08/12/2025 17:06:32 INFO]: Training loss at epoch 102: 0.5937133729457855
[08/12/2025 17:06:35 INFO]: Training loss at epoch 19: 0.6531085968017578
[08/12/2025 17:06:38 INFO]: Training loss at epoch 31: 0.9054474234580994
[08/12/2025 17:06:45 INFO]: Training loss at epoch 66: 0.6692107915878296
[08/12/2025 17:06:47 INFO]: Training loss at epoch 39: 0.9592965543270111
[08/12/2025 17:06:48 INFO]: Training loss at epoch 72: 0.39441899955272675
[08/12/2025 17:06:49 INFO]: Training loss at epoch 103: 0.5778293013572693
[08/12/2025 17:07:02 INFO]: Training stats: {
    "score": -0.9711504438569897,
    "rmse": 0.9711504438569897
}
[08/12/2025 17:07:02 INFO]: Val stats: {
    "score": -0.9382841360257343,
    "rmse": 0.9382841360257343
}
[08/12/2025 17:07:02 INFO]: Test stats: {
    "score": -0.9014326279149452,
    "rmse": 0.9014326279149452
}
[08/12/2025 17:07:04 INFO]: Training stats: {
    "score": -0.8427571631089281,
    "rmse": 0.8427571631089281
}
[08/12/2025 17:07:04 INFO]: Val stats: {
    "score": -0.9539874492502117,
    "rmse": 0.9539874492502117
}
[08/12/2025 17:07:04 INFO]: Test stats: {
    "score": -0.8217618691741827,
    "rmse": 0.8217618691741827
}
[08/12/2025 17:07:05 INFO]: Training loss at epoch 104: 0.8088894188404083
[08/12/2025 17:07:11 INFO]: Training loss at epoch 67: 0.5294068306684494
[08/12/2025 17:07:12 INFO]: Training loss at epoch 73: 0.47021186351776123
[08/12/2025 17:07:21 INFO]: Training loss at epoch 105: 0.6684034764766693
[08/12/2025 17:07:32 INFO]: Training loss at epoch 32: 1.0811509490013123
[08/12/2025 17:07:35 INFO]: Training loss at epoch 74: 0.3525565266609192
[08/12/2025 17:07:36 INFO]: Training loss at epoch 68: 0.548904225230217
[08/12/2025 17:07:38 INFO]: Training loss at epoch 106: 0.7486066520214081
[08/12/2025 17:07:45 INFO]: Training loss at epoch 40: 0.8710676431655884
[08/12/2025 17:07:50 INFO]: Running Final Evaluation...
[08/12/2025 17:07:54 INFO]: Training loss at epoch 107: 0.6310380101203918
[08/12/2025 17:07:59 INFO]: Training loss at epoch 75: 0.505951315164566
[08/12/2025 17:08:01 INFO]: Training loss at epoch 69: 0.7359122633934021
[08/12/2025 17:08:01 INFO]: Training loss at epoch 19: 0.9536879658699036
[08/12/2025 17:08:10 INFO]: Training loss at epoch 108: 0.5898045748472214
[08/12/2025 17:08:10 INFO]: Training stats: {
    "score": -0.7632385261982996,
    "rmse": 0.7632385261982996
}
[08/12/2025 17:08:10 INFO]: Val stats: {
    "score": -0.8939424042925863,
    "rmse": 0.8939424042925863
}
[08/12/2025 17:08:10 INFO]: Test stats: {
    "score": -0.8097220328984049,
    "rmse": 0.8097220328984049
}
[08/12/2025 17:08:22 INFO]: Training loss at epoch 76: 0.4099854826927185
[08/12/2025 17:08:24 INFO]: Training loss at epoch 33: 1.1812866926193237
[08/12/2025 17:08:24 INFO]: New best epoch, val score: -0.7581759781549399
[08/12/2025 17:08:24 INFO]: Saving model to: model_best.pth
[08/12/2025 17:08:26 INFO]: Training loss at epoch 109: 0.8365471363067627
[08/12/2025 17:08:30 INFO]: Training loss at epoch 20: 0.7386954724788666
[08/12/2025 17:08:31 INFO]: Running Final Evaluation...
[08/12/2025 17:08:31 INFO]: Training stats: {
    "score": -0.7961117697188042,
    "rmse": 0.7961117697188042
}
[08/12/2025 17:08:31 INFO]: Val stats: {
    "score": -0.8088108081563729,
    "rmse": 0.8088108081563729
}
[08/12/2025 17:08:31 INFO]: Test stats: {
    "score": -0.7557424986076324,
    "rmse": 0.7557424986076324
}
[08/12/2025 17:08:33 INFO]: Training stats: {
    "score": -1.0004673157054191,
    "rmse": 1.0004673157054191
}
[08/12/2025 17:08:33 INFO]: Val stats: {
    "score": -0.9316011710402619,
    "rmse": 0.9316011710402619
}
[08/12/2025 17:08:33 INFO]: Test stats: {
    "score": -0.9124384288886574,
    "rmse": 0.9124384288886574
}
[08/12/2025 17:08:34 INFO]: Training loss at epoch 70: 0.6738703548908234
[08/12/2025 17:08:43 INFO]: New best epoch, val score: -0.9316011710402619
[08/12/2025 17:08:43 INFO]: Saving model to: model_best.pth
[08/12/2025 17:08:45 INFO]: Training loss at epoch 77: 0.3428986221551895
[08/12/2025 17:08:48 INFO]: Training loss at epoch 110: 0.6025280058383942
[08/12/2025 17:08:48 INFO]: New best epoch, val score: -0.7571786591004867
[08/12/2025 17:08:48 INFO]: Saving model to: model_best.pth
[08/12/2025 17:09:01 INFO]: Training loss at epoch 71: 0.6039569675922394
[08/12/2025 17:09:05 INFO]: Training loss at epoch 111: 0.7470181882381439
[08/12/2025 17:09:07 INFO]: New best epoch, val score: -0.796507593496295
[08/12/2025 17:09:07 INFO]: Saving model to: model_best.pth
[08/12/2025 17:09:09 INFO]: Training loss at epoch 78: 0.37375248968601227
[08/12/2025 17:09:22 INFO]: Training loss at epoch 112: 0.7685751914978027
[08/12/2025 17:09:24 INFO]: New best epoch, val score: -0.7959599707909609
[08/12/2025 17:09:24 INFO]: Saving model to: model_best.pth
[08/12/2025 17:09:27 INFO]: Training loss at epoch 72: 0.6011636257171631
[08/12/2025 17:09:32 INFO]: Training loss at epoch 79: 0.32813574373722076
[08/12/2025 17:09:38 INFO]: Training loss at epoch 113: 0.565703347325325
[08/12/2025 17:09:40 INFO]: Training stats: {
    "score": -0.5678103919401044,
    "rmse": 0.5678103919401044
}
[08/12/2025 17:09:40 INFO]: Val stats: {
    "score": -0.7617364489106156,
    "rmse": 0.7617364489106156
}
[08/12/2025 17:09:40 INFO]: Test stats: {
    "score": -0.7024457274235774,
    "rmse": 0.7024457274235774
}
[08/12/2025 17:09:40 INFO]: New best epoch, val score: -0.7945199586574412
[08/12/2025 17:09:40 INFO]: Saving model to: model_best.pth
[08/12/2025 17:09:52 INFO]: Training loss at epoch 73: 0.630171149969101
[08/12/2025 17:09:54 INFO]: Training loss at epoch 114: 0.7016632258892059
[08/12/2025 17:09:54 INFO]: Training loss at epoch 21: 0.7234295010566711
[08/12/2025 17:09:56 INFO]: New best epoch, val score: -0.7923781428787575
[08/12/2025 17:09:56 INFO]: Saving model to: model_best.pth
[08/12/2025 17:10:02 INFO]: Training loss at epoch 20: 1.019551932811737
[08/12/2025 17:10:03 INFO]: Training loss at epoch 80: 0.3216541111469269
[08/12/2025 17:10:10 INFO]: Training loss at epoch 115: 0.6978419423103333
[08/12/2025 17:10:12 INFO]: New best epoch, val score: -0.7917180563261914
[08/12/2025 17:10:12 INFO]: Saving model to: model_best.pth
[08/12/2025 17:10:13 INFO]: New best epoch, val score: -0.9303635397828494
[08/12/2025 17:10:13 INFO]: Saving model to: model_best.pth
[08/12/2025 17:10:17 INFO]: Training loss at epoch 74: 0.6442190110683441
[08/12/2025 17:10:26 INFO]: Training loss at epoch 81: 0.2775236442685127
[08/12/2025 17:10:26 INFO]: Training loss at epoch 116: 0.7635000050067902
[08/12/2025 17:10:42 INFO]: Training loss at epoch 75: 0.5604592263698578
[08/12/2025 17:10:42 INFO]: Training loss at epoch 117: 0.7456803321838379
[08/12/2025 17:10:49 INFO]: Training loss at epoch 82: 0.3882288783788681
[08/12/2025 17:10:58 INFO]: Training loss at epoch 118: 0.6793640851974487
[08/12/2025 17:11:07 INFO]: Training loss at epoch 76: 0.6504172682762146
[08/12/2025 17:11:12 INFO]: Training loss at epoch 83: 0.4187670946121216
[08/12/2025 17:11:15 INFO]: Training loss at epoch 119: 0.579819917678833
[08/12/2025 17:11:19 INFO]: Training loss at epoch 22: 0.5358680784702301
[08/12/2025 17:11:20 INFO]: Training stats: {
    "score": -0.7822012135487144,
    "rmse": 0.7822012135487144
}
[08/12/2025 17:11:20 INFO]: Val stats: {
    "score": -0.8118614922003625,
    "rmse": 0.8118614922003625
}
[08/12/2025 17:11:20 INFO]: Test stats: {
    "score": -0.7496916495504372,
    "rmse": 0.7496916495504372
}
[08/12/2025 17:11:32 INFO]: Training loss at epoch 21: 0.910303920507431
[08/12/2025 17:11:32 INFO]: Training loss at epoch 77: 0.5793665647506714
[08/12/2025 17:11:35 INFO]: Training loss at epoch 84: 0.34007038176059723
[08/12/2025 17:11:37 INFO]: Training loss at epoch 120: 0.5318964719772339
[08/12/2025 17:11:54 INFO]: Training loss at epoch 121: 0.7954727113246918
[08/12/2025 17:11:58 INFO]: Training loss at epoch 78: 0.585066944360733
[08/12/2025 17:11:58 INFO]: Training loss at epoch 85: 0.42379336059093475
[08/12/2025 17:12:09 INFO]: Training loss at epoch 122: 0.6097164452075958
[08/12/2025 17:12:22 INFO]: Training loss at epoch 86: 0.29825107753276825
[08/12/2025 17:12:23 INFO]: Training loss at epoch 79: 0.5456084907054901
[08/12/2025 17:12:26 INFO]: Training loss at epoch 123: 0.6391691863536835
[08/12/2025 17:12:29 INFO]: New best epoch, val score: -0.790271980311551
[08/12/2025 17:12:29 INFO]: Saving model to: model_best.pth
[08/12/2025 17:12:33 INFO]: Training stats: {
    "score": -0.7520012122563233,
    "rmse": 0.7520012122563233
}
[08/12/2025 17:12:33 INFO]: Val stats: {
    "score": -0.8683514282721407,
    "rmse": 0.8683514282721407
}
[08/12/2025 17:12:33 INFO]: Test stats: {
    "score": -0.8310750003361779,
    "rmse": 0.8310750003361779
}
[08/12/2025 17:12:44 INFO]: Training loss at epoch 23: 0.5851065814495087
[08/12/2025 17:12:44 INFO]: Training loss at epoch 124: 0.5691067427396774
[08/12/2025 17:12:44 INFO]: Training loss at epoch 87: 0.40838953852653503
[08/12/2025 17:12:59 INFO]: Training loss at epoch 80: 0.6840704083442688
[08/12/2025 17:13:00 INFO]: Training loss at epoch 125: 0.7762772440910339
[08/12/2025 17:13:01 INFO]: Training loss at epoch 22: 1.0257930159568787
[08/12/2025 17:13:07 INFO]: Training loss at epoch 88: 0.34274402260780334
[08/12/2025 17:13:16 INFO]: Training loss at epoch 126: 0.5275130867958069
[08/12/2025 17:13:24 INFO]: Training loss at epoch 81: 0.8161301612854004
[08/12/2025 17:13:31 INFO]: Training loss at epoch 89: 0.3521493524312973
[08/12/2025 17:13:32 INFO]: Training loss at epoch 127: 0.6155270934104919
[08/12/2025 17:13:39 INFO]: Training stats: {
    "score": -0.5556962425207954,
    "rmse": 0.5556962425207954
}
[08/12/2025 17:13:39 INFO]: Val stats: {
    "score": -0.7366557197154887,
    "rmse": 0.7366557197154887
}
[08/12/2025 17:13:39 INFO]: Test stats: {
    "score": -0.7161337778471218,
    "rmse": 0.7161337778471218
}
[08/12/2025 17:13:42 INFO]: New best epoch, val score: -0.7366557197154887
[08/12/2025 17:13:42 INFO]: Saving model to: model_best.pth
[08/12/2025 17:13:49 INFO]: Training loss at epoch 128: 0.6198202073574066
[08/12/2025 17:13:49 INFO]: Training loss at epoch 82: 0.6263783574104309
[08/12/2025 17:13:51 INFO]: New best epoch, val score: -0.783623712533883
[08/12/2025 17:13:51 INFO]: Saving model to: model_best.pth
[08/12/2025 17:14:03 INFO]: Training loss at epoch 90: 0.3894728720188141
[08/12/2025 17:14:05 INFO]: Training loss at epoch 129: 0.5825300961732864
[08/12/2025 17:14:10 INFO]: Training stats: {
    "score": -0.7716130125499024,
    "rmse": 0.7716130125499024
}
[08/12/2025 17:14:10 INFO]: Val stats: {
    "score": -0.7778428952716501,
    "rmse": 0.7778428952716501
}
[08/12/2025 17:14:10 INFO]: Test stats: {
    "score": -0.7339377880780101,
    "rmse": 0.7339377880780101
}
[08/12/2025 17:14:12 INFO]: New best epoch, val score: -0.7778428952716501
[08/12/2025 17:14:12 INFO]: Saving model to: model_best.pth
[08/12/2025 17:14:13 INFO]: Training loss at epoch 24: 0.6543957889080048
[08/12/2025 17:14:14 INFO]: Training loss at epoch 83: 0.6189738214015961
[08/12/2025 17:14:26 INFO]: Training loss at epoch 130: 0.6532959342002869
[08/12/2025 17:14:26 INFO]: Training loss at epoch 91: 0.30465199053287506
[08/12/2025 17:14:28 INFO]: New best epoch, val score: -0.7756523296807992
[08/12/2025 17:14:28 INFO]: Saving model to: model_best.pth
[08/12/2025 17:14:35 INFO]: Training loss at epoch 23: 0.9319043159484863
[08/12/2025 17:14:39 INFO]: Training loss at epoch 84: 0.5992943048477173
[08/12/2025 17:14:42 INFO]: Training loss at epoch 131: 0.6010135412216187
[08/12/2025 17:14:49 INFO]: Training loss at epoch 92: 0.23877794295549393
[08/12/2025 17:14:52 INFO]: New best epoch, val score: -0.7096891167413316
[08/12/2025 17:14:52 INFO]: Saving model to: model_best.pth
[08/12/2025 17:14:58 INFO]: Training loss at epoch 132: 0.5312042534351349
[08/12/2025 17:15:03 INFO]: Training loss at epoch 85: 0.4899560660123825
[08/12/2025 17:15:13 INFO]: Training loss at epoch 93: 0.23302147537469864
[08/12/2025 17:15:14 INFO]: Training loss at epoch 133: 0.5897984206676483
[08/12/2025 17:15:28 INFO]: Training loss at epoch 86: 0.6031176745891571
[08/12/2025 17:15:30 INFO]: Training loss at epoch 134: 0.5562655627727509
[08/12/2025 17:15:36 INFO]: Training loss at epoch 94: 0.33309486508369446
[08/12/2025 17:15:41 INFO]: Training loss at epoch 25: 0.4855833947658539
[08/12/2025 17:15:46 INFO]: Training loss at epoch 135: 0.5723748207092285
[08/12/2025 17:15:53 INFO]: Training loss at epoch 87: 0.45833730697631836
[08/12/2025 17:15:59 INFO]: Training loss at epoch 95: 0.39502139389514923
[08/12/2025 17:16:02 INFO]: Training loss at epoch 136: 0.7235671579837799
[08/12/2025 17:16:07 INFO]: Training loss at epoch 24: 1.037993460893631
[08/12/2025 17:16:17 INFO]: Training loss at epoch 88: 0.7898691892623901
[08/12/2025 17:16:18 INFO]: Training loss at epoch 137: 0.566779613494873
[08/12/2025 17:16:22 INFO]: Training loss at epoch 96: 0.26549582928419113
[08/12/2025 17:16:33 INFO]: Training loss at epoch 138: 0.619957447052002
[08/12/2025 17:16:42 INFO]: Training loss at epoch 89: 0.6142955124378204
[08/12/2025 17:16:45 INFO]: Training loss at epoch 97: 0.2851756513118744
[08/12/2025 17:16:50 INFO]: Training loss at epoch 139: 0.6738220751285553
[08/12/2025 17:16:51 INFO]: Training stats: {
    "score": -0.7783490177835569,
    "rmse": 0.7783490177835569
}
[08/12/2025 17:16:51 INFO]: Val stats: {
    "score": -0.9419572360338693,
    "rmse": 0.9419572360338693
}
[08/12/2025 17:16:51 INFO]: Test stats: {
    "score": -0.8580482242361964,
    "rmse": 0.8580482242361964
}
[08/12/2025 17:16:55 INFO]: Training stats: {
    "score": -0.7569905428313668,
    "rmse": 0.7569905428313668
}
[08/12/2025 17:16:55 INFO]: Val stats: {
    "score": -0.7757575820886201,
    "rmse": 0.7757575820886201
}
[08/12/2025 17:16:55 INFO]: Test stats: {
    "score": -0.7257141411321195,
    "rmse": 0.7257141411321195
}
[08/12/2025 17:17:07 INFO]: Training loss at epoch 26: 0.5562877357006073
[08/12/2025 17:17:09 INFO]: Training loss at epoch 98: 0.3319559544324875
[08/12/2025 17:17:11 INFO]: Training loss at epoch 140: 0.521971806883812
[08/12/2025 17:17:13 INFO]: New best epoch, val score: -0.7743404525906038
[08/12/2025 17:17:13 INFO]: Saving model to: model_best.pth
[08/12/2025 17:17:16 INFO]: Training loss at epoch 90: 0.5969880223274231
[08/12/2025 17:17:27 INFO]: Training loss at epoch 141: 0.6012122631072998
[08/12/2025 17:17:29 INFO]: New best epoch, val score: -0.7737552489880838
[08/12/2025 17:17:29 INFO]: Saving model to: model_best.pth
[08/12/2025 17:17:32 INFO]: Training loss at epoch 99: 0.348577082157135
[08/12/2025 17:17:37 INFO]: Training loss at epoch 25: 1.0902296304702759
[08/12/2025 17:17:39 INFO]: Training stats: {
    "score": -0.5559084379700037,
    "rmse": 0.5559084379700037
}
[08/12/2025 17:17:39 INFO]: Val stats: {
    "score": -0.7513753254025991,
    "rmse": 0.7513753254025991
}
[08/12/2025 17:17:39 INFO]: Test stats: {
    "score": -0.731043044446735,
    "rmse": 0.731043044446735
}
[08/12/2025 17:17:40 INFO]: Training loss at epoch 91: 0.4211142808198929
[08/12/2025 17:17:43 INFO]: Training loss at epoch 142: 0.5824917256832123
[08/12/2025 17:17:59 INFO]: Training loss at epoch 143: 0.5791557431221008
[08/12/2025 17:18:02 INFO]: Training loss at epoch 100: 0.3455767184495926
[08/12/2025 17:18:05 INFO]: Training loss at epoch 92: 0.5268041640520096
[08/12/2025 17:18:05 INFO]: New best epoch, val score: -0.6988413054272423
[08/12/2025 17:18:05 INFO]: Saving model to: model_best.pth
[08/12/2025 17:18:14 INFO]: Training loss at epoch 144: 0.7029770910739899
[08/12/2025 17:18:25 INFO]: Training loss at epoch 101: 0.28510361909866333
[08/12/2025 17:18:30 INFO]: Training loss at epoch 93: 0.6414233446121216
[08/12/2025 17:18:30 INFO]: Training loss at epoch 145: 0.6473570764064789
[08/12/2025 17:18:33 INFO]: New best epoch, val score: -0.8267962363001529
[08/12/2025 17:18:33 INFO]: Saving model to: model_best.pth
[08/12/2025 17:18:33 INFO]: Training loss at epoch 27: 0.5407624840736389
[08/12/2025 17:18:47 INFO]: Training loss at epoch 146: 0.6617234647274017
[08/12/2025 17:18:49 INFO]: Training loss at epoch 102: 0.4327966570854187
[08/12/2025 17:18:55 INFO]: Training loss at epoch 94: 0.5920204520225525
[08/12/2025 17:18:58 INFO]: New best epoch, val score: -0.8208916767060646
[08/12/2025 17:18:58 INFO]: Saving model to: model_best.pth
[08/12/2025 17:19:03 INFO]: Training loss at epoch 147: 0.6013714373111725
[08/12/2025 17:19:08 INFO]: Training loss at epoch 26: 1.0266433656215668
[08/12/2025 17:19:12 INFO]: Training loss at epoch 103: 0.3527078926563263
[08/12/2025 17:19:19 INFO]: Training loss at epoch 148: 0.6339353322982788
[08/12/2025 17:19:20 INFO]: Training loss at epoch 95: 0.6325879096984863
[08/12/2025 17:19:35 INFO]: Training loss at epoch 149: 0.6216672658920288
[08/12/2025 17:19:35 INFO]: Training loss at epoch 104: 0.32661257684230804
[08/12/2025 17:19:40 INFO]: Training stats: {
    "score": -0.7438862743876252,
    "rmse": 0.7438862743876252
}
[08/12/2025 17:19:40 INFO]: Val stats: {
    "score": -0.7762428392169367,
    "rmse": 0.7762428392169367
}
[08/12/2025 17:19:40 INFO]: Test stats: {
    "score": -0.7198472317368892,
    "rmse": 0.7198472317368892
}
[08/12/2025 17:19:44 INFO]: Training loss at epoch 96: 0.6078780591487885
[08/12/2025 17:19:56 INFO]: Training loss at epoch 150: 0.5432641357183456
[08/12/2025 17:19:58 INFO]: New best epoch, val score: -0.7705661844912217
[08/12/2025 17:19:58 INFO]: Saving model to: model_best.pth
[08/12/2025 17:19:58 INFO]: Training loss at epoch 105: 0.2513866499066353
[08/12/2025 17:20:00 INFO]: Training loss at epoch 28: 0.6159733831882477
[08/12/2025 17:20:09 INFO]: Training loss at epoch 97: 0.5414470285177231
[08/12/2025 17:20:13 INFO]: Training loss at epoch 151: 0.5796796679496765
[08/12/2025 17:20:15 INFO]: New best epoch, val score: -0.7663148287673733
[08/12/2025 17:20:15 INFO]: Saving model to: model_best.pth
[08/12/2025 17:20:22 INFO]: Training loss at epoch 106: 0.26024869829416275
[08/12/2025 17:20:29 INFO]: Training loss at epoch 152: 0.582452118396759
[08/12/2025 17:20:31 INFO]: New best epoch, val score: -0.7633794449720729
[08/12/2025 17:20:31 INFO]: Saving model to: model_best.pth
[08/12/2025 17:20:34 INFO]: Training loss at epoch 98: 0.43607373535633087
[08/12/2025 17:20:40 INFO]: Training loss at epoch 27: 0.9385550320148468
[08/12/2025 17:20:45 INFO]: Training loss at epoch 153: 0.6256941854953766
[08/12/2025 17:20:45 INFO]: Training loss at epoch 107: 0.3070462495088577
[08/12/2025 17:20:47 INFO]: New best epoch, val score: -0.7563459581164174
[08/12/2025 17:20:47 INFO]: Saving model to: model_best.pth
[08/12/2025 17:20:59 INFO]: Training loss at epoch 99: 0.48857179284095764
[08/12/2025 17:21:01 INFO]: Training loss at epoch 154: 0.5644371807575226
[08/12/2025 17:21:03 INFO]: New best epoch, val score: -0.7495503345077998
[08/12/2025 17:21:03 INFO]: Saving model to: model_best.pth
[08/12/2025 17:21:07 INFO]: Training stats: {
    "score": -0.7166792069908784,
    "rmse": 0.7166792069908784
}
[08/12/2025 17:21:07 INFO]: Val stats: {
    "score": -0.8198441510573369,
    "rmse": 0.8198441510573369
}
[08/12/2025 17:21:07 INFO]: Test stats: {
    "score": -0.8154759274463188,
    "rmse": 0.8154759274463188
}
[08/12/2025 17:21:08 INFO]: Training loss at epoch 108: 0.27615639567375183
[08/12/2025 17:21:11 INFO]: New best epoch, val score: -0.8198441510573369
[08/12/2025 17:21:11 INFO]: Saving model to: model_best.pth
[08/12/2025 17:21:17 INFO]: Training loss at epoch 155: 0.6356098055839539
[08/12/2025 17:21:19 INFO]: New best epoch, val score: -0.7456461065943893
[08/12/2025 17:21:19 INFO]: Saving model to: model_best.pth
[08/12/2025 17:21:28 INFO]: Training loss at epoch 29: 0.5664254128932953
[08/12/2025 17:21:32 INFO]: Training loss at epoch 109: 0.32122859358787537
[08/12/2025 17:21:33 INFO]: Training loss at epoch 100: 0.5411044955253601
[08/12/2025 17:21:33 INFO]: Training loss at epoch 156: 0.580798476934433
[08/12/2025 17:21:35 INFO]: New best epoch, val score: -0.7434659241339523
[08/12/2025 17:21:35 INFO]: Saving model to: model_best.pth
[08/12/2025 17:21:36 INFO]: New best epoch, val score: -0.8113860271174376
[08/12/2025 17:21:36 INFO]: Saving model to: model_best.pth
[08/12/2025 17:21:40 INFO]: Training stats: {
    "score": -0.49882878186861346,
    "rmse": 0.49882878186861346
}
[08/12/2025 17:21:40 INFO]: Val stats: {
    "score": -0.716343757048036,
    "rmse": 0.716343757048036
}
[08/12/2025 17:21:40 INFO]: Test stats: {
    "score": -0.6916049512708732,
    "rmse": 0.6916049512708732
}
[08/12/2025 17:21:49 INFO]: Training loss at epoch 157: 0.5548427700996399
[08/12/2025 17:21:51 INFO]: New best epoch, val score: -0.7431514993623519
[08/12/2025 17:21:51 INFO]: Saving model to: model_best.pth
[08/12/2025 17:21:57 INFO]: Training stats: {
    "score": -0.7775579141946019,
    "rmse": 0.7775579141946019
}
[08/12/2025 17:21:57 INFO]: Val stats: {
    "score": -0.8611524047153524,
    "rmse": 0.8611524047153524
}
[08/12/2025 17:21:57 INFO]: Test stats: {
    "score": -0.815803398905584,
    "rmse": 0.815803398905584
}
[08/12/2025 17:21:57 INFO]: Training loss at epoch 101: 0.6217053830623627
[08/12/2025 17:22:03 INFO]: Training loss at epoch 110: 0.27419789135456085
[08/12/2025 17:22:05 INFO]: Training loss at epoch 158: 0.6809226870536804
[08/12/2025 17:22:12 INFO]: Training loss at epoch 28: 1.074078619480133
[08/12/2025 17:22:21 INFO]: Training loss at epoch 159: 0.5853641331195831
[08/12/2025 17:22:22 INFO]: Training loss at epoch 102: 0.60334712266922
[08/12/2025 17:22:26 INFO]: Training loss at epoch 111: 0.24684889614582062
[08/12/2025 17:22:27 INFO]: Training stats: {
    "score": -0.7329200100189361,
    "rmse": 0.7329200100189361
}
[08/12/2025 17:22:27 INFO]: Val stats: {
    "score": -0.7468679510732883,
    "rmse": 0.7468679510732883
}
[08/12/2025 17:22:27 INFO]: Test stats: {
    "score": -0.7028326490059204,
    "rmse": 0.7028326490059204
}
[08/12/2025 17:22:43 INFO]: Training loss at epoch 160: 0.5545315146446228
[08/12/2025 17:22:46 INFO]: Training loss at epoch 103: 0.5531454384326935
[08/12/2025 17:22:49 INFO]: Training loss at epoch 112: 0.22516898810863495
[08/12/2025 17:22:58 INFO]: Training loss at epoch 161: 0.5117695331573486
[08/12/2025 17:23:11 INFO]: Training loss at epoch 104: 0.4747552275657654
[08/12/2025 17:23:12 INFO]: Training loss at epoch 113: 0.2565859854221344
[08/12/2025 17:23:14 INFO]: Training loss at epoch 162: 0.485907644033432
[08/12/2025 17:23:22 INFO]: Training loss at epoch 30: 0.5493078231811523
[08/12/2025 17:23:30 INFO]: Training loss at epoch 163: 0.5104666948318481
[08/12/2025 17:23:35 INFO]: Training loss at epoch 114: 0.214604452252388
[08/12/2025 17:23:36 INFO]: Training loss at epoch 105: 0.43410253524780273
[08/12/2025 17:23:41 INFO]: Training loss at epoch 29: 1.017323762178421
[08/12/2025 17:23:46 INFO]: Training loss at epoch 164: 0.6114267110824585
[08/12/2025 17:23:58 INFO]: Training loss at epoch 115: 0.23858345299959183
[08/12/2025 17:24:00 INFO]: Training loss at epoch 106: 0.45288093388080597
[08/12/2025 17:24:02 INFO]: Training loss at epoch 165: 0.7532879710197449
[08/12/2025 17:24:12 INFO]: Training stats: {
    "score": -0.9915530165692432,
    "rmse": 0.9915530165692432
}
[08/12/2025 17:24:12 INFO]: Val stats: {
    "score": -0.9270192444743446,
    "rmse": 0.9270192444743446
}
[08/12/2025 17:24:12 INFO]: Test stats: {
    "score": -0.9054511696558584,
    "rmse": 0.9054511696558584
}
[08/12/2025 17:24:18 INFO]: Training loss at epoch 166: 0.45559021830558777
[08/12/2025 17:24:21 INFO]: Training loss at epoch 116: 0.28249359130859375
[08/12/2025 17:24:23 INFO]: New best epoch, val score: -0.9270192444743446
[08/12/2025 17:24:23 INFO]: Saving model to: model_best.pth
[08/12/2025 17:24:25 INFO]: Training loss at epoch 107: 0.5128249973058701
[08/12/2025 17:24:33 INFO]: Training loss at epoch 167: 0.5479187369346619
[08/12/2025 17:24:44 INFO]: Training loss at epoch 117: 0.23886368423700333
[08/12/2025 17:24:49 INFO]: Training loss at epoch 168: 0.5048874467611313
[08/12/2025 17:24:50 INFO]: Training loss at epoch 108: 0.6608645915985107
[08/12/2025 17:24:50 INFO]: Training loss at epoch 31: 0.549574226140976
[08/12/2025 17:25:00 INFO]: New best epoch, val score: -0.8259687850861581
[08/12/2025 17:25:00 INFO]: Saving model to: model_best.pth
[08/12/2025 17:25:05 INFO]: Training loss at epoch 169: 0.603338748216629
[08/12/2025 17:25:07 INFO]: Training loss at epoch 118: 0.22085999697446823
[08/12/2025 17:25:11 INFO]: Training stats: {
    "score": -0.7177656231061449,
    "rmse": 0.7177656231061449
}
[08/12/2025 17:25:11 INFO]: Val stats: {
    "score": -0.7455532091263087,
    "rmse": 0.7455532091263087
}
[08/12/2025 17:25:11 INFO]: Test stats: {
    "score": -0.6955971575315185,
    "rmse": 0.6955971575315185
}
[08/12/2025 17:25:14 INFO]: Training loss at epoch 109: 0.48895470798015594
[08/12/2025 17:25:23 INFO]: Training stats: {
    "score": -0.7412243657605645,
    "rmse": 0.7412243657605645
}
[08/12/2025 17:25:23 INFO]: Val stats: {
    "score": -0.9032972830997801,
    "rmse": 0.9032972830997801
}
[08/12/2025 17:25:23 INFO]: Test stats: {
    "score": -0.8338735905531549,
    "rmse": 0.8338735905531549
}
[08/12/2025 17:25:27 INFO]: Training loss at epoch 170: 0.5350764989852905
[08/12/2025 17:25:28 INFO]: New best epoch, val score: -0.7413495265394213
[08/12/2025 17:25:28 INFO]: Saving model to: model_best.pth
[08/12/2025 17:25:30 INFO]: Training loss at epoch 119: 0.2043089121580124
[08/12/2025 17:25:38 INFO]: Training stats: {
    "score": -0.5091365170745268,
    "rmse": 0.5091365170745268
}
[08/12/2025 17:25:38 INFO]: Val stats: {
    "score": -0.7288459581161812,
    "rmse": 0.7288459581161812
}
[08/12/2025 17:25:38 INFO]: Test stats: {
    "score": -0.6778358155581412,
    "rmse": 0.6778358155581412
}
[08/12/2025 17:25:43 INFO]: Training loss at epoch 171: 0.4875079095363617
[08/12/2025 17:25:45 INFO]: New best epoch, val score: -0.7356018503616533
[08/12/2025 17:25:45 INFO]: Saving model to: model_best.pth
[08/12/2025 17:25:45 INFO]: Training loss at epoch 30: 1.2270541191101074
[08/12/2025 17:25:48 INFO]: Training loss at epoch 110: 0.5940306484699249
[08/12/2025 17:25:59 INFO]: Training loss at epoch 172: 0.6026041805744171
[08/12/2025 17:26:01 INFO]: New best epoch, val score: -0.7296254659961126
[08/12/2025 17:26:01 INFO]: Saving model to: model_best.pth
[08/12/2025 17:26:01 INFO]: Training loss at epoch 120: 0.2310151308774948
[08/12/2025 17:26:13 INFO]: Training loss at epoch 111: 0.46713797748088837
[08/12/2025 17:26:15 INFO]: Training loss at epoch 173: 0.5600389540195465
[08/12/2025 17:26:16 INFO]: New best epoch, val score: -0.7258958715578026
[08/12/2025 17:26:16 INFO]: Saving model to: model_best.pth
[08/12/2025 17:26:18 INFO]: Training loss at epoch 32: 0.5281565189361572
[08/12/2025 17:26:24 INFO]: Training loss at epoch 121: 0.23176563531160355
[08/12/2025 17:26:27 INFO]: New best epoch, val score: -0.6931715564508402
[08/12/2025 17:26:27 INFO]: Saving model to: model_best.pth
[08/12/2025 17:26:31 INFO]: Training loss at epoch 174: 0.5471701323986053
[08/12/2025 17:26:38 INFO]: Training loss at epoch 112: 0.6771082878112793
[08/12/2025 17:26:47 INFO]: Training loss at epoch 175: 0.5113467425107956
[08/12/2025 17:26:47 INFO]: Training loss at epoch 122: 0.22335974872112274
[08/12/2025 17:27:02 INFO]: Training loss at epoch 113: 0.5973678231239319
[08/12/2025 17:27:03 INFO]: Training loss at epoch 176: 0.49146339297294617
[08/12/2025 17:27:10 INFO]: Training loss at epoch 123: 0.22798460721969604
[08/12/2025 17:27:16 INFO]: Training loss at epoch 31: 1.0046414732933044
[08/12/2025 17:27:19 INFO]: Training loss at epoch 177: 0.578957200050354
[08/12/2025 17:27:27 INFO]: Training loss at epoch 114: 0.4674421697854996
[08/12/2025 17:27:33 INFO]: Training loss at epoch 124: 0.2230917513370514
[08/12/2025 17:27:34 INFO]: Training loss at epoch 178: 0.6014364361763
[08/12/2025 17:27:44 INFO]: Training loss at epoch 33: 0.5227365493774414
[08/12/2025 17:27:50 INFO]: Training loss at epoch 179: 0.4716865122318268
[08/12/2025 17:27:52 INFO]: Training loss at epoch 115: 0.5144217610359192
[08/12/2025 17:27:57 INFO]: Training loss at epoch 125: 0.2310296669602394
[08/12/2025 17:27:57 INFO]: Training stats: {
    "score": -0.707693176870152,
    "rmse": 0.707693176870152
}
[08/12/2025 17:27:57 INFO]: Val stats: {
    "score": -0.7374290982875756,
    "rmse": 0.7374290982875756
}
[08/12/2025 17:27:57 INFO]: Test stats: {
    "score": -0.6927371257309778,
    "rmse": 0.6927371257309778
}
[08/12/2025 17:28:14 INFO]: Training loss at epoch 180: 0.5754534304141998
[08/12/2025 17:28:19 INFO]: Training loss at epoch 116: 0.5488079786300659
[08/12/2025 17:28:20 INFO]: Training loss at epoch 126: 0.244988352060318
[08/12/2025 17:28:29 INFO]: Training loss at epoch 181: 0.5712812840938568
[08/12/2025 17:28:43 INFO]: Training loss at epoch 117: 0.46028588712215424
[08/12/2025 17:28:43 INFO]: Training loss at epoch 127: 0.26375311613082886
[08/12/2025 17:28:45 INFO]: Training loss at epoch 182: 0.5664379298686981
[08/12/2025 17:28:47 INFO]: Training loss at epoch 32: 0.988529235124588
[08/12/2025 17:29:01 INFO]: Training loss at epoch 183: 0.4259783625602722
[08/12/2025 17:29:06 INFO]: Training loss at epoch 128: 0.2952481806278229
[08/12/2025 17:29:08 INFO]: Training loss at epoch 118: 0.5701612532138824
[08/12/2025 17:29:12 INFO]: Training loss at epoch 34: 0.5129294693470001
[08/12/2025 17:29:17 INFO]: Training loss at epoch 184: 0.4238172918558121
[08/12/2025 17:29:29 INFO]: Training loss at epoch 129: 0.2317976877093315
[08/12/2025 17:29:32 INFO]: Training loss at epoch 119: 0.5427111983299255
[08/12/2025 17:29:33 INFO]: Training loss at epoch 185: 0.5181651711463928
[08/12/2025 17:29:37 INFO]: Training stats: {
    "score": -0.4530124364418104,
    "rmse": 0.4530124364418104
}
[08/12/2025 17:29:37 INFO]: Val stats: {
    "score": -0.6731474745806645,
    "rmse": 0.6731474745806645
}
[08/12/2025 17:29:37 INFO]: Test stats: {
    "score": -0.675888452118352,
    "rmse": 0.675888452118352
}
[08/12/2025 17:29:40 INFO]: New best epoch, val score: -0.6731474745806645
[08/12/2025 17:29:40 INFO]: Saving model to: model_best.pth
[08/12/2025 17:29:41 INFO]: Training stats: {
    "score": -0.6832802746701835,
    "rmse": 0.6832802746701835
}
[08/12/2025 17:29:41 INFO]: Val stats: {
    "score": -0.8170069490589713,
    "rmse": 0.8170069490589713
}
[08/12/2025 17:29:41 INFO]: Test stats: {
    "score": -0.7742727019112423,
    "rmse": 0.7742727019112423
}
[08/12/2025 17:29:49 INFO]: Training loss at epoch 186: 0.42890897393226624
[08/12/2025 17:29:51 INFO]: New best epoch, val score: -0.7245466813574611
[08/12/2025 17:29:51 INFO]: Saving model to: model_best.pth
[08/12/2025 17:30:00 INFO]: Training loss at epoch 130: 0.244168221950531
[08/12/2025 17:30:05 INFO]: Training loss at epoch 187: 0.4736444354057312
[08/12/2025 17:30:05 INFO]: Training loss at epoch 120: 0.5053976625204086
[08/12/2025 17:30:07 INFO]: New best epoch, val score: -0.7216090466010706
[08/12/2025 17:30:07 INFO]: Saving model to: model_best.pth
[08/12/2025 17:30:08 INFO]: New best epoch, val score: -0.7942961953196457
[08/12/2025 17:30:08 INFO]: Saving model to: model_best.pth
[08/12/2025 17:30:16 INFO]: Training loss at epoch 33: 1.0864253342151642
[08/12/2025 17:30:21 INFO]: Training loss at epoch 188: 0.5683271884918213
[08/12/2025 17:30:22 INFO]: New best epoch, val score: -0.7190280477873496
[08/12/2025 17:30:22 INFO]: Saving model to: model_best.pth
[08/12/2025 17:30:24 INFO]: Training loss at epoch 131: 0.24599937349557877
[08/12/2025 17:30:31 INFO]: Training loss at epoch 121: 0.4727261960506439
[08/12/2025 17:30:37 INFO]: Training loss at epoch 189: 0.4884915202856064
[08/12/2025 17:30:38 INFO]: Training loss at epoch 35: 0.4921340346336365
[08/12/2025 17:30:43 INFO]: Training stats: {
    "score": -0.6981756105691306,
    "rmse": 0.6981756105691306
}
[08/12/2025 17:30:43 INFO]: Val stats: {
    "score": -0.717271131091159,
    "rmse": 0.717271131091159
}
[08/12/2025 17:30:43 INFO]: Test stats: {
    "score": -0.6855130529538782,
    "rmse": 0.6855130529538782
}
[08/12/2025 17:30:45 INFO]: New best epoch, val score: -0.717271131091159
[08/12/2025 17:30:45 INFO]: Saving model to: model_best.pth
[08/12/2025 17:30:47 INFO]: Training loss at epoch 132: 0.26240888237953186
[08/12/2025 17:30:48 INFO]: New best epoch, val score: -0.8236785962979529
[08/12/2025 17:30:48 INFO]: Saving model to: model_best.pth
[08/12/2025 17:30:55 INFO]: Training loss at epoch 122: 0.4804200530052185
[08/12/2025 17:30:59 INFO]: Training loss at epoch 190: 0.4606492221355438
[08/12/2025 17:31:01 INFO]: New best epoch, val score: -0.7169276812521534
[08/12/2025 17:31:01 INFO]: Saving model to: model_best.pth
[08/12/2025 17:31:11 INFO]: Training loss at epoch 133: 0.34122611582279205
[08/12/2025 17:31:17 INFO]: Training loss at epoch 191: 0.4273872822523117
[08/12/2025 17:31:19 INFO]: New best epoch, val score: -0.7162982308102163
[08/12/2025 17:31:19 INFO]: Saving model to: model_best.pth
[08/12/2025 17:31:23 INFO]: Training loss at epoch 123: 0.4433978497982025
[08/12/2025 17:31:33 INFO]: Training loss at epoch 192: 0.6501492857933044
[08/12/2025 17:31:35 INFO]: Training loss at epoch 134: 0.2103104218840599
[08/12/2025 17:31:35 INFO]: New best epoch, val score: -0.7153836135802916
[08/12/2025 17:31:35 INFO]: Saving model to: model_best.pth
[08/12/2025 17:31:48 INFO]: Training loss at epoch 34: 0.7871862649917603
[08/12/2025 17:31:50 INFO]: Training loss at epoch 124: 0.5944896638393402
[08/12/2025 17:31:50 INFO]: Training loss at epoch 193: 0.49332065880298615
[08/12/2025 17:31:52 INFO]: New best epoch, val score: -0.714948058971155
[08/12/2025 17:31:52 INFO]: Saving model to: model_best.pth
[08/12/2025 17:31:59 INFO]: Training loss at epoch 135: 0.2652556821703911
[08/12/2025 17:32:05 INFO]: Training loss at epoch 36: 0.4974517822265625
[08/12/2025 17:32:06 INFO]: Training loss at epoch 194: 0.5105797946453094
[08/12/2025 17:32:08 INFO]: New best epoch, val score: -0.7133828236443163
[08/12/2025 17:32:08 INFO]: Saving model to: model_best.pth
[08/12/2025 17:32:15 INFO]: New best epoch, val score: -0.8227421617745853
[08/12/2025 17:32:15 INFO]: Saving model to: model_best.pth
[08/12/2025 17:32:16 INFO]: Training loss at epoch 125: 0.5001641809940338
[08/12/2025 17:32:23 INFO]: Training loss at epoch 195: 0.40053753554821014
[08/12/2025 17:32:24 INFO]: Training loss at epoch 136: 0.2791108638048172
[08/12/2025 17:32:25 INFO]: New best epoch, val score: -0.7104778867271349
[08/12/2025 17:32:25 INFO]: Saving model to: model_best.pth
[08/12/2025 17:32:40 INFO]: Training loss at epoch 196: 0.5134977102279663
[08/12/2025 17:32:41 INFO]: Training loss at epoch 126: 0.3981190621852875
[08/12/2025 17:32:42 INFO]: New best epoch, val score: -0.7076783019565711
[08/12/2025 17:32:42 INFO]: Saving model to: model_best.pth
[08/12/2025 17:32:49 INFO]: Training loss at epoch 137: 0.23994290828704834
[08/12/2025 17:32:57 INFO]: Training loss at epoch 197: 0.5449121594429016
[08/12/2025 17:32:59 INFO]: New best epoch, val score: -0.7070450221380218
[08/12/2025 17:32:59 INFO]: Saving model to: model_best.pth
[08/12/2025 17:33:06 INFO]: Training loss at epoch 127: 0.4447249174118042
[08/12/2025 17:33:13 INFO]: Training loss at epoch 138: 0.30414605140686035
[08/12/2025 17:33:13 INFO]: Training loss at epoch 198: 0.45333003997802734
[08/12/2025 17:33:16 INFO]: New best epoch, val score: -0.7061450981222543
[08/12/2025 17:33:16 INFO]: Saving model to: model_best.pth
[08/12/2025 17:33:19 INFO]: Training loss at epoch 35: 0.9560830593109131
[08/12/2025 17:33:30 INFO]: Training loss at epoch 199: 0.5338899791240692
[08/12/2025 17:33:31 INFO]: Training loss at epoch 128: 0.5230891704559326
[08/12/2025 17:33:31 INFO]: Training loss at epoch 37: 0.472659632563591
[08/12/2025 17:33:36 INFO]: Training stats: {
    "score": -0.6863962544639226,
    "rmse": 0.6863962544639226
}
[08/12/2025 17:33:36 INFO]: Val stats: {
    "score": -0.7049337494332752,
    "rmse": 0.7049337494332752
}
[08/12/2025 17:33:36 INFO]: Test stats: {
    "score": -0.6697873606453464,
    "rmse": 0.6697873606453464
}
[08/12/2025 17:33:38 INFO]: Training loss at epoch 139: 0.3447968512773514
[08/12/2025 17:33:38 INFO]: New best epoch, val score: -0.7049337494332752
[08/12/2025 17:33:38 INFO]: Saving model to: model_best.pth
[08/12/2025 17:33:38 INFO]: Running Final Evaluation...
[08/12/2025 17:33:44 INFO]: Training accuracy: {
    "score": -0.6863962536709325,
    "rmse": 0.6863962536709325
}
[08/12/2025 17:33:44 INFO]: Val accuracy: {
    "score": -0.7049337494332752,
    "rmse": 0.7049337494332752
}
[08/12/2025 17:33:44 INFO]: Test accuracy: {
    "score": -0.6697873606453464,
    "rmse": 0.6697873606453464
}
[08/12/2025 17:33:44 INFO]: {
    "dataset": "ic_upstream3",
    "model_name": "ft_transformer",
    "run_id": "revolved-Rody",
    "best_epoch": 199,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.6697873606453464,
        "rmse": 0.6697873606453464
    },
    "train_stats": {
        "score": -0.6863962536709325,
        "rmse": 0.6863962536709325
    },
    "val_stats": {
        "score": -0.7049337494332752,
        "rmse": 0.7049337494332752
    }
}
[08/12/2025 17:33:46 INFO]: Training stats: {
    "score": -0.4486740206857249,
    "rmse": 0.4486740206857249
}
[08/12/2025 17:33:46 INFO]: Val stats: {
    "score": -0.687506034552352,
    "rmse": 0.687506034552352
}
[08/12/2025 17:33:46 INFO]: Test stats: {
    "score": -0.6768295364960735,
    "rmse": 0.6768295364960735
}
[08/12/2025 17:33:56 INFO]: Training loss at epoch 129: 0.48773935437202454
[08/12/2025 17:34:04 INFO]: Training stats: {
    "score": -0.7263438092910655,
    "rmse": 0.7263438092910655
}
[08/12/2025 17:34:04 INFO]: Val stats: {
    "score": -0.8876670380627847,
    "rmse": 0.8876670380627847
}
[08/12/2025 17:34:04 INFO]: Test stats: {
    "score": -0.8016901195546895,
    "rmse": 0.8016901195546895
}
[08/12/2025 17:34:12 INFO]: Training loss at epoch 140: 0.2589145749807358
[08/12/2025 17:34:30 INFO]: Training loss at epoch 130: 0.513592392206192
[08/12/2025 17:34:36 INFO]: Training loss at epoch 141: 0.23663616180419922
[08/12/2025 17:34:48 INFO]: Training loss at epoch 36: 0.9220163226127625
[08/12/2025 17:34:57 INFO]: Training loss at epoch 131: 0.5219589471817017
[08/12/2025 17:34:59 INFO]: Training loss at epoch 38: 0.4063384383916855
[08/12/2025 17:35:01 INFO]: Training loss at epoch 142: 0.201824352145195
[08/12/2025 17:35:04 INFO]: New best epoch, val score: -0.6600348025480707
[08/12/2025 17:35:04 INFO]: Saving model to: model_best.pth
[08/12/2025 17:35:24 INFO]: Training loss at epoch 132: 0.534287303686142
[08/12/2025 17:35:24 INFO]: Training loss at epoch 143: 0.2370133250951767
[08/12/2025 17:35:27 INFO]: New best epoch, val score: -0.6501189367886525
[08/12/2025 17:35:27 INFO]: Saving model to: model_best.pth
[08/12/2025 17:35:27 INFO]: New best epoch, val score: -0.7925302297926174
[08/12/2025 17:35:27 INFO]: Saving model to: model_best.pth
[08/12/2025 17:35:49 INFO]: Training loss at epoch 144: 0.24319887906312943
[08/12/2025 17:35:50 INFO]: Training loss at epoch 133: 0.4442107677459717
[08/12/2025 17:35:53 INFO]: New best epoch, val score: -0.7903845985124677
[08/12/2025 17:35:53 INFO]: Saving model to: model_best.pth
[08/12/2025 17:36:13 INFO]: Training loss at epoch 145: 0.2903624027967453
[08/12/2025 17:36:15 INFO]: Training loss at epoch 134: 0.453633576631546
[08/12/2025 17:36:20 INFO]: Training loss at epoch 37: 0.9391053020954132
[08/12/2025 17:36:26 INFO]: Training loss at epoch 39: 0.3617476746439934
[08/12/2025 17:36:36 INFO]: Training loss at epoch 146: 0.3133767992258072
[08/12/2025 17:36:40 INFO]: Training loss at epoch 135: 0.6015780866146088
[08/12/2025 17:36:54 INFO]: Training stats: {
    "score": -0.6720783625347992,
    "rmse": 0.6720783625347992
}
[08/12/2025 17:36:54 INFO]: Val stats: {
    "score": -0.8310631225981799,
    "rmse": 0.8310631225981799
}
[08/12/2025 17:36:54 INFO]: Test stats: {
    "score": -0.7883445659974001,
    "rmse": 0.7883445659974001
}
[08/12/2025 17:36:59 INFO]: Training loss at epoch 147: 0.23816993087530136
[08/12/2025 17:37:05 INFO]: Training loss at epoch 136: 0.4881969094276428
[08/12/2025 17:37:22 INFO]: Training loss at epoch 148: 0.24555149674415588
[08/12/2025 17:37:30 INFO]: Training loss at epoch 137: 0.5653094947338104
[08/12/2025 17:37:45 INFO]: Training loss at epoch 149: 0.22423936426639557
[08/12/2025 17:37:51 INFO]: Training loss at epoch 38: 1.055504858493805
[08/12/2025 17:37:53 INFO]: Training stats: {
    "score": -0.4961287695592651,
    "rmse": 0.4961287695592651
}
[08/12/2025 17:37:53 INFO]: Val stats: {
    "score": -0.739864986896283,
    "rmse": 0.739864986896283
}
[08/12/2025 17:37:53 INFO]: Test stats: {
    "score": -0.774727549000556,
    "rmse": 0.774727549000556
}
[08/12/2025 17:37:56 INFO]: Training loss at epoch 138: 0.48680131137371063
[08/12/2025 17:38:16 INFO]: Training loss at epoch 150: 0.21711044013500214
[08/12/2025 17:38:19 INFO]: Training loss at epoch 40: 0.38946433365345
[08/12/2025 17:38:21 INFO]: Training loss at epoch 139: 0.5237395465373993
[08/12/2025 17:38:29 INFO]: Training stats: {
    "score": -0.6743415823461143,
    "rmse": 0.6743415823461143
}
[08/12/2025 17:38:29 INFO]: Val stats: {
    "score": -0.8054403647326853,
    "rmse": 0.8054403647326853
}
[08/12/2025 17:38:29 INFO]: Test stats: {
    "score": -0.7624210471755096,
    "rmse": 0.7624210471755096
}
[08/12/2025 17:38:39 INFO]: Training loss at epoch 151: 0.25144387036561966
[08/12/2025 17:38:55 INFO]: Training loss at epoch 140: 0.4460037499666214
[08/12/2025 17:39:03 INFO]: Training loss at epoch 152: 0.23370064049959183
[08/12/2025 17:39:20 INFO]: Training loss at epoch 141: 0.502187192440033
[08/12/2025 17:39:23 INFO]: Training loss at epoch 39: 0.8752690553665161
[08/12/2025 17:39:26 INFO]: Training loss at epoch 153: 0.2834222912788391
[08/12/2025 17:39:44 INFO]: Training loss at epoch 41: 0.5292434692382812
[08/12/2025 17:39:45 INFO]: Training loss at epoch 142: 0.4788367450237274
[08/12/2025 17:39:49 INFO]: Training loss at epoch 154: 0.22113323956727982
[08/12/2025 17:39:54 INFO]: New best epoch, val score: -0.8180363679973256
[08/12/2025 17:39:54 INFO]: Saving model to: model_best.pth
[08/12/2025 17:39:55 INFO]: Training stats: {
    "score": -0.9776933737289861,
    "rmse": 0.9776933737289861
}
[08/12/2025 17:39:55 INFO]: Val stats: {
    "score": -0.9168821979409574,
    "rmse": 0.9168821979409574
}
[08/12/2025 17:39:55 INFO]: Test stats: {
    "score": -0.893052622687322,
    "rmse": 0.893052622687322
}
[08/12/2025 17:40:06 INFO]: New best epoch, val score: -0.9168821979409574
[08/12/2025 17:40:06 INFO]: Saving model to: model_best.pth
[08/12/2025 17:40:11 INFO]: Training loss at epoch 143: 0.5623588562011719
[08/12/2025 17:40:12 INFO]: Training loss at epoch 155: 0.26180899143218994
[08/12/2025 17:40:35 INFO]: Training loss at epoch 156: 0.24614955484867096
[08/12/2025 17:40:36 INFO]: Training loss at epoch 144: 0.4992281347513199
[08/12/2025 17:40:58 INFO]: Training loss at epoch 157: 0.2012975811958313
[08/12/2025 17:41:02 INFO]: Training loss at epoch 145: 0.3847154974937439
[08/12/2025 17:41:09 INFO]: Training loss at epoch 42: 0.40283840894699097
[08/12/2025 17:41:21 INFO]: Training loss at epoch 158: 0.22956153750419617
[08/12/2025 17:41:28 INFO]: Training loss at epoch 146: 0.4029994159936905
[08/12/2025 17:41:28 INFO]: Training loss at epoch 40: 0.9046197831630707
[08/12/2025 17:41:39 INFO]: New best epoch, val score: -0.9162353585176073
[08/12/2025 17:41:39 INFO]: Saving model to: model_best.pth
[08/12/2025 17:41:44 INFO]: Training loss at epoch 159: 0.18031911924481392
[08/12/2025 17:41:53 INFO]: Training stats: {
    "score": -0.41909801521835194,
    "rmse": 0.41909801521835194
}
[08/12/2025 17:41:53 INFO]: Val stats: {
    "score": -0.6330029866518093,
    "rmse": 0.6330029866518093
}
[08/12/2025 17:41:53 INFO]: Test stats: {
    "score": -0.6570551645426088,
    "rmse": 0.6570551645426088
}
[08/12/2025 17:41:53 INFO]: Training loss at epoch 147: 0.4781227111816406
[08/12/2025 17:41:55 INFO]: New best epoch, val score: -0.6330029866518093
[08/12/2025 17:41:55 INFO]: Saving model to: model_best.pth
[08/12/2025 17:41:56 INFO]: New best epoch, val score: -0.777095598337471
[08/12/2025 17:41:56 INFO]: Saving model to: model_best.pth
[08/12/2025 17:42:17 INFO]: Training loss at epoch 160: 0.20317666977643967
[08/12/2025 17:42:19 INFO]: Training loss at epoch 148: 0.4372880905866623
[08/12/2025 17:42:22 INFO]: New best epoch, val score: -0.7731695618200517
[08/12/2025 17:42:22 INFO]: Saving model to: model_best.pth
[08/12/2025 17:42:36 INFO]: Training loss at epoch 43: 0.3488658666610718
[08/12/2025 17:42:41 INFO]: Training loss at epoch 161: 0.25038085132837296
[08/12/2025 17:42:44 INFO]: Training loss at epoch 149: 0.4728799909353256
[08/12/2025 17:42:53 INFO]: Training stats: {
    "score": -0.6567553902673636,
    "rmse": 0.6567553902673636
}
[08/12/2025 17:42:53 INFO]: Val stats: {
    "score": -0.8120923882371068,
    "rmse": 0.8120923882371068
}
[08/12/2025 17:42:53 INFO]: Test stats: {
    "score": -0.73582756306966,
    "rmse": 0.73582756306966
}
[08/12/2025 17:43:01 INFO]: Training loss at epoch 41: 0.8513848781585693
[08/12/2025 17:43:04 INFO]: Training loss at epoch 162: 0.21733201295137405
[08/12/2025 17:43:12 INFO]: New best epoch, val score: -0.913356083121974
[08/12/2025 17:43:12 INFO]: Saving model to: model_best.pth
[08/12/2025 17:43:18 INFO]: Training loss at epoch 150: 0.5130668729543686
[08/12/2025 17:43:29 INFO]: Training loss at epoch 163: 0.2147945836186409
[08/12/2025 17:43:43 INFO]: Training loss at epoch 151: 0.45824217796325684
[08/12/2025 17:43:52 INFO]: Training loss at epoch 164: 0.2458127737045288
[08/12/2025 17:44:03 INFO]: Training loss at epoch 44: 0.4768773764371872
[08/12/2025 17:44:07 INFO]: Training loss at epoch 152: 0.5511614829301834
[08/12/2025 17:44:15 INFO]: Training loss at epoch 165: 0.20797712355852127
[08/12/2025 17:44:30 INFO]: Training loss at epoch 42: 1.084029197692871
[08/12/2025 17:44:32 INFO]: Training loss at epoch 153: 0.4952280670404434
[08/12/2025 17:44:38 INFO]: Training loss at epoch 166: 0.19895289838314056
[08/12/2025 17:44:56 INFO]: Training loss at epoch 154: 0.499224990606308
[08/12/2025 17:45:02 INFO]: Training loss at epoch 167: 0.21294111013412476
[08/12/2025 17:45:21 INFO]: Training loss at epoch 155: 0.5471969693899155
[08/12/2025 17:45:26 INFO]: Training loss at epoch 168: 0.22697707265615463
[08/12/2025 17:45:29 INFO]: Training loss at epoch 45: 0.433795228600502
[08/12/2025 17:45:46 INFO]: Training loss at epoch 156: 0.5534885972738266
[08/12/2025 17:45:49 INFO]: Training loss at epoch 169: 0.20242763310670853
[08/12/2025 17:45:57 INFO]: Training stats: {
    "score": -0.42559917176096124,
    "rmse": 0.42559917176096124
}
[08/12/2025 17:45:57 INFO]: Val stats: {
    "score": -0.6877141451162289,
    "rmse": 0.6877141451162289
}
[08/12/2025 17:45:57 INFO]: Test stats: {
    "score": -0.7229779666149769,
    "rmse": 0.7229779666149769
}
[08/12/2025 17:46:00 INFO]: Training loss at epoch 43: 0.9562166333198547
[08/12/2025 17:46:10 INFO]: Training loss at epoch 157: 0.3516724556684494
[08/12/2025 17:46:20 INFO]: Training loss at epoch 170: 0.21545839309692383
[08/12/2025 17:46:34 INFO]: Training loss at epoch 158: 0.40316805243492126
[08/12/2025 17:46:44 INFO]: Training loss at epoch 171: 0.18656299263238907
[08/12/2025 17:46:55 INFO]: Training loss at epoch 46: 0.443091943860054
[08/12/2025 17:46:59 INFO]: Training loss at epoch 159: 0.4471901208162308
[08/12/2025 17:47:07 INFO]: Training loss at epoch 172: 0.16838912665843964
[08/12/2025 17:47:08 INFO]: Training stats: {
    "score": -0.714254403370907,
    "rmse": 0.714254403370907
}
[08/12/2025 17:47:08 INFO]: Val stats: {
    "score": -0.8842463197244909,
    "rmse": 0.8842463197244909
}
[08/12/2025 17:47:08 INFO]: Test stats: {
    "score": -0.7762927052357439,
    "rmse": 0.7762927052357439
}
[08/12/2025 17:47:30 INFO]: Training loss at epoch 44: 0.8769554495811462
[08/12/2025 17:47:30 INFO]: Training loss at epoch 173: 0.17741841077804565
[08/12/2025 17:47:33 INFO]: Training loss at epoch 160: 0.5270887613296509
[08/12/2025 17:47:53 INFO]: Training loss at epoch 174: 0.14324087277054787
[08/12/2025 17:47:58 INFO]: Training loss at epoch 161: 0.4044468253850937
[08/12/2025 17:48:17 INFO]: Training loss at epoch 175: 0.18593138456344604
[08/12/2025 17:48:20 INFO]: Training loss at epoch 47: 0.3957909196615219
[08/12/2025 17:48:23 INFO]: Training loss at epoch 162: 0.4365936368703842
[08/12/2025 17:48:40 INFO]: Training loss at epoch 176: 0.17676571756601334
[08/12/2025 17:48:47 INFO]: Training loss at epoch 163: 0.41561056673526764
[08/12/2025 17:49:00 INFO]: Training loss at epoch 45: 0.9606791436672211
[08/12/2025 17:49:03 INFO]: Training loss at epoch 177: 0.1840079426765442
[08/12/2025 17:49:12 INFO]: Training loss at epoch 164: 0.6279346197843552
[08/12/2025 17:49:26 INFO]: Training loss at epoch 178: 0.14199259504675865
[08/12/2025 17:49:38 INFO]: Training loss at epoch 165: 0.542488157749176
[08/12/2025 17:49:44 INFO]: Training loss at epoch 48: 0.35298237204551697
[08/12/2025 17:49:49 INFO]: Training loss at epoch 179: 0.12948476523160934
[08/12/2025 17:49:54 INFO]: New best epoch, val score: -0.8018378226919894
[08/12/2025 17:49:54 INFO]: Saving model to: model_best.pth
[08/12/2025 17:49:57 INFO]: Training stats: {
    "score": -0.388831316282665,
    "rmse": 0.388831316282665
}
[08/12/2025 17:49:57 INFO]: Val stats: {
    "score": -0.6693739354844179,
    "rmse": 0.6693739354844179
}
[08/12/2025 17:49:57 INFO]: Test stats: {
    "score": -0.6989676535413761,
    "rmse": 0.6989676535413761
}
[08/12/2025 17:50:04 INFO]: Training loss at epoch 166: 0.3530426099896431
[08/12/2025 17:50:19 INFO]: Training loss at epoch 180: 0.18143537640571594
[08/12/2025 17:50:29 INFO]: Training loss at epoch 167: 0.44522418081760406
[08/12/2025 17:50:32 INFO]: New best epoch, val score: -0.768777594233584
[08/12/2025 17:50:32 INFO]: Saving model to: model_best.pth
[08/12/2025 17:50:33 INFO]: Training loss at epoch 46: 1.0708311200141907
[08/12/2025 17:50:43 INFO]: Training loss at epoch 181: 0.24797382205724716
[08/12/2025 17:50:54 INFO]: Training loss at epoch 168: 0.4362002909183502
[08/12/2025 17:51:05 INFO]: Training loss at epoch 182: 0.1683027148246765
[08/12/2025 17:51:09 INFO]: Training loss at epoch 49: 0.47485461831092834
[08/12/2025 17:51:19 INFO]: Training loss at epoch 169: 0.5496037155389786
[08/12/2025 17:51:28 INFO]: Training stats: {
    "score": -0.6423548701309834,
    "rmse": 0.6423548701309834
}
[08/12/2025 17:51:28 INFO]: Val stats: {
    "score": -0.7925299420655423,
    "rmse": 0.7925299420655423
}
[08/12/2025 17:51:28 INFO]: Test stats: {
    "score": -0.7330722811130611,
    "rmse": 0.7330722811130611
}
[08/12/2025 17:51:29 INFO]: Training loss at epoch 183: 0.1686473712325096
[08/12/2025 17:51:37 INFO]: Training stats: {
    "score": -0.6010608413639608,
    "rmse": 0.6010608413639608
}
[08/12/2025 17:51:37 INFO]: Val stats: {
    "score": -0.792743519294158,
    "rmse": 0.792743519294158
}
[08/12/2025 17:51:37 INFO]: Test stats: {
    "score": -0.777852938937594,
    "rmse": 0.777852938937594
}
[08/12/2025 17:51:47 INFO]: New best epoch, val score: -0.792743519294158
[08/12/2025 17:51:47 INFO]: Saving model to: model_best.pth
[08/12/2025 17:51:52 INFO]: Training loss at epoch 184: 0.20400042086839676
[08/12/2025 17:51:52 INFO]: Training loss at epoch 170: 0.36295388638973236
[08/12/2025 17:52:03 INFO]: Training loss at epoch 47: 0.795930027961731
[08/12/2025 17:52:15 INFO]: Training loss at epoch 185: 0.15097957476973534
[08/12/2025 17:52:17 INFO]: Training loss at epoch 171: 0.577976644039154
[08/12/2025 17:52:38 INFO]: Training loss at epoch 186: 0.19117804616689682
[08/12/2025 17:52:43 INFO]: Training loss at epoch 172: 0.43496812880039215
[08/12/2025 17:53:01 INFO]: Training loss at epoch 187: 0.15666374564170837
[08/12/2025 17:53:03 INFO]: Training loss at epoch 50: 0.3395245373249054
[08/12/2025 17:53:07 INFO]: Training loss at epoch 173: 0.4150979816913605
[08/12/2025 17:53:25 INFO]: Training loss at epoch 188: 0.1342686228454113
[08/12/2025 17:53:32 INFO]: Training loss at epoch 174: 0.3849598914384842
[08/12/2025 17:53:34 INFO]: Training loss at epoch 48: 0.8410715162754059
[08/12/2025 17:53:44 INFO]: New best epoch, val score: -0.9002647936849182
[08/12/2025 17:53:44 INFO]: Saving model to: model_best.pth
[08/12/2025 17:53:48 INFO]: Training loss at epoch 189: 0.1387515515089035
[08/12/2025 17:53:56 INFO]: Training stats: {
    "score": -0.3550696219224343,
    "rmse": 0.3550696219224343
}
[08/12/2025 17:53:56 INFO]: Val stats: {
    "score": -0.6605889025461062,
    "rmse": 0.6605889025461062
}
[08/12/2025 17:53:56 INFO]: Test stats: {
    "score": -0.6594295387538718,
    "rmse": 0.6594295387538718
}
[08/12/2025 17:53:57 INFO]: Training loss at epoch 175: 0.3981298506259918
[08/12/2025 17:54:20 INFO]: Training loss at epoch 190: 0.19998887926340103
[08/12/2025 17:54:22 INFO]: Training loss at epoch 176: 0.4416201561689377
[08/12/2025 17:54:22 INFO]: Running Final Evaluation...
[08/12/2025 17:54:29 INFO]: Training loss at epoch 51: 0.32563065737485886
[08/12/2025 17:54:40 INFO]: New best epoch, val score: -0.7648567003160333
[08/12/2025 17:54:40 INFO]: Saving model to: model_best.pth
[08/12/2025 17:54:47 INFO]: Training loss at epoch 177: 0.38281868398189545
[08/12/2025 17:55:04 INFO]: Training loss at epoch 49: 0.8541606068611145
[08/12/2025 17:55:11 INFO]: Training loss at epoch 178: 0.3790876418352127
[08/12/2025 17:55:35 INFO]: Training stats: {
    "score": -0.9713226076026238,
    "rmse": 0.9713226076026238
}
[08/12/2025 17:55:35 INFO]: Val stats: {
    "score": -0.8954842475107938,
    "rmse": 0.8954842475107938
}
[08/12/2025 17:55:35 INFO]: Test stats: {
    "score": -0.8815510732185207,
    "rmse": 0.8815510732185207
}
[08/12/2025 17:55:36 INFO]: Training loss at epoch 179: 0.45760923624038696
[08/12/2025 17:55:44 INFO]: Training stats: {
    "score": -0.7225975776080281,
    "rmse": 0.7225975776080281
}
[08/12/2025 17:55:44 INFO]: Val stats: {
    "score": -0.8938432264008627,
    "rmse": 0.8938432264008627
}
[08/12/2025 17:55:44 INFO]: Test stats: {
    "score": -0.7933987134485903,
    "rmse": 0.7933987134485903
}
[08/12/2025 17:55:46 INFO]: New best epoch, val score: -0.8954842475107938
[08/12/2025 17:55:46 INFO]: Saving model to: model_best.pth
[08/12/2025 17:56:01 INFO]: Training loss at epoch 52: 0.40777236223220825
[08/12/2025 17:56:10 INFO]: Training loss at epoch 180: 0.43358103930950165
[08/12/2025 17:56:34 INFO]: Training loss at epoch 181: 0.4481489658355713
[08/12/2025 17:56:59 INFO]: Training loss at epoch 182: 0.4619251787662506
[08/12/2025 17:57:06 INFO]: Training loss at epoch 50: 1.1607178449630737
[08/12/2025 17:57:17 INFO]: New best epoch, val score: -0.8907138972610972
[08/12/2025 17:57:17 INFO]: Saving model to: model_best.pth
[08/12/2025 17:57:25 INFO]: Training loss at epoch 183: 0.44809235632419586
[08/12/2025 17:57:34 INFO]: Training loss at epoch 53: 0.36004742980003357
[08/12/2025 17:57:49 INFO]: Training loss at epoch 184: 0.4756912589073181
[08/12/2025 17:58:13 INFO]: Training loss at epoch 185: 0.4643588960170746
[08/12/2025 17:58:36 INFO]: Training loss at epoch 51: 0.8766402900218964
[08/12/2025 17:58:38 INFO]: Training loss at epoch 186: 0.3798159956932068
[08/12/2025 17:59:03 INFO]: Training loss at epoch 187: 0.47631336748600006
[08/12/2025 17:59:06 INFO]: Training loss at epoch 54: 0.40435458719730377
[08/12/2025 17:59:27 INFO]: Training loss at epoch 188: 0.44877535104751587
[08/12/2025 17:59:52 INFO]: Training loss at epoch 189: 0.34879378229379654
[08/12/2025 18:00:00 INFO]: Training stats: {
    "score": -0.6336005852727488,
    "rmse": 0.6336005852727488
}
[08/12/2025 18:00:00 INFO]: Val stats: {
    "score": -0.7986211573243892,
    "rmse": 0.7986211573243892
}
[08/12/2025 18:00:00 INFO]: Test stats: {
    "score": -0.7173560772718973,
    "rmse": 0.7173560772718973
}
[08/12/2025 18:00:06 INFO]: Training loss at epoch 52: 0.8109092116355896
[08/12/2025 18:00:25 INFO]: Training loss at epoch 190: 0.4101412147283554
[08/12/2025 18:00:38 INFO]: Training loss at epoch 55: 0.38681723177433014
[08/12/2025 18:00:50 INFO]: Training loss at epoch 191: 0.4845573753118515
[08/12/2025 18:01:15 INFO]: Training loss at epoch 192: 0.3953539729118347
[08/12/2025 18:01:36 INFO]: Training loss at epoch 53: 0.9295170903205872
[08/12/2025 18:01:40 INFO]: Training loss at epoch 193: 0.4527572840452194
[08/12/2025 18:02:05 INFO]: Training loss at epoch 194: 0.472132071852684
[08/12/2025 18:02:10 INFO]: Training loss at epoch 56: 0.37305502593517303
[08/12/2025 18:02:32 INFO]: Training loss at epoch 195: 0.4848892390727997
[08/12/2025 18:02:57 INFO]: Training loss at epoch 196: 0.4848635345697403
[08/12/2025 18:03:12 INFO]: Training loss at epoch 54: 0.7232789993286133
[08/12/2025 18:03:21 INFO]: Training loss at epoch 197: 0.45556640625
[08/12/2025 18:03:35 INFO]: Training loss at epoch 57: 0.32660311460494995
[08/12/2025 18:03:45 INFO]: Training loss at epoch 198: 0.5221863985061646
[08/12/2025 18:03:48 INFO]: Running Final Evaluation...
[08/12/2025 18:04:48 INFO]: Training loss at epoch 55: 0.9316678047180176
[08/12/2025 18:05:03 INFO]: Training loss at epoch 58: 0.3528873175382614
[08/12/2025 18:05:14 INFO]: New best epoch, val score: -0.7564894449199219
[08/12/2025 18:05:14 INFO]: Saving model to: model_best.pth
[08/12/2025 18:06:24 INFO]: Training loss at epoch 56: 0.8978887796401978
[08/12/2025 18:06:34 INFO]: Training loss at epoch 59: 0.34610477089881897
[08/12/2025 18:07:05 INFO]: Training stats: {
    "score": -0.563261726436006,
    "rmse": 0.563261726436006
}
[08/12/2025 18:07:05 INFO]: Val stats: {
    "score": -0.7268971092072873,
    "rmse": 0.7268971092072873
}
[08/12/2025 18:07:05 INFO]: Test stats: {
    "score": -0.7654689911724456,
    "rmse": 0.7654689911724456
}
[08/12/2025 18:07:15 INFO]: New best epoch, val score: -0.7268971092072873
[08/12/2025 18:07:15 INFO]: Saving model to: model_best.pth
[08/12/2025 18:08:00 INFO]: Training loss at epoch 57: 0.6728496700525284
[08/12/2025 18:08:11 INFO]: New best epoch, val score: -0.8791307508673033
[08/12/2025 18:08:11 INFO]: Saving model to: model_best.pth
[08/12/2025 18:08:35 INFO]: Training loss at epoch 60: 0.2930978462100029
[08/12/2025 18:08:46 INFO]: New best epoch, val score: -0.7193595197042832
[08/12/2025 18:08:46 INFO]: Saving model to: model_best.pth
[08/12/2025 18:09:36 INFO]: Training loss at epoch 58: 0.7896602749824524
[08/12/2025 18:09:47 INFO]: New best epoch, val score: -0.8581171934050411
[08/12/2025 18:09:47 INFO]: Saving model to: model_best.pth
[08/12/2025 18:10:06 INFO]: Training loss at epoch 61: 0.33324871957302094
[08/12/2025 18:11:12 INFO]: Training loss at epoch 59: 1.0110146403312683
[08/12/2025 18:11:34 INFO]: Training loss at epoch 62: 0.308877095580101
[08/12/2025 18:11:44 INFO]: Training stats: {
    "score": -0.8486744135124592,
    "rmse": 0.8486744135124592
}
[08/12/2025 18:11:44 INFO]: Val stats: {
    "score": -0.8615529089112407,
    "rmse": 0.8615529089112407
}
[08/12/2025 18:11:44 INFO]: Test stats: {
    "score": -0.8025017823994243,
    "rmse": 0.8025017823994243
}
[08/12/2025 18:13:07 INFO]: Training loss at epoch 63: 0.3360135406255722
[08/12/2025 18:13:19 INFO]: Training loss at epoch 60: 0.7286824584007263
[08/12/2025 18:14:38 INFO]: Training loss at epoch 64: 0.3623012751340866
[08/12/2025 18:14:53 INFO]: Training loss at epoch 61: 0.7736698687076569
[08/12/2025 18:16:11 INFO]: Training loss at epoch 65: 0.3318290710449219
[08/12/2025 18:16:25 INFO]: Training loss at epoch 62: 0.8405576348304749
[08/12/2025 18:17:43 INFO]: Training loss at epoch 66: 0.3053363114595413
[08/12/2025 18:17:59 INFO]: Training loss at epoch 63: 0.6651864945888519
[08/12/2025 18:19:12 INFO]: Training loss at epoch 67: 0.3481336832046509
[08/12/2025 18:19:36 INFO]: Training loss at epoch 64: 0.6449942886829376
[08/12/2025 18:19:47 INFO]: New best epoch, val score: -0.8484747004333896
[08/12/2025 18:19:47 INFO]: Saving model to: model_best.pth
[08/12/2025 18:20:43 INFO]: Training loss at epoch 68: 0.2930065169930458
[08/12/2025 18:21:07 INFO]: Training loss at epoch 65: 0.7367376387119293
[08/12/2025 18:22:14 INFO]: Training loss at epoch 69: 0.283584401011467
[08/12/2025 18:22:42 INFO]: Training loss at epoch 66: 0.57415571808815
[08/12/2025 18:22:43 INFO]: Training stats: {
    "score": -0.5436134015055359,
    "rmse": 0.5436134015055359
}
[08/12/2025 18:22:43 INFO]: Val stats: {
    "score": -0.7714782472379941,
    "rmse": 0.7714782472379941
}
[08/12/2025 18:22:43 INFO]: Test stats: {
    "score": -0.7895704710766933,
    "rmse": 0.7895704710766933
}
[08/12/2025 18:24:14 INFO]: Training loss at epoch 70: 0.3975343257188797
[08/12/2025 18:24:19 INFO]: Training loss at epoch 67: 0.7732642292976379
[08/12/2025 18:25:47 INFO]: Training loss at epoch 71: 0.2446463331580162
[08/12/2025 18:25:55 INFO]: Training loss at epoch 68: 0.580178290605545
[08/12/2025 18:27:19 INFO]: Training loss at epoch 72: 0.23211189359426498
[08/12/2025 18:27:32 INFO]: Training loss at epoch 69: 0.612705409526825
[08/12/2025 18:28:05 INFO]: Training stats: {
    "score": -0.8311508974787974,
    "rmse": 0.8311508974787974
}
[08/12/2025 18:28:05 INFO]: Val stats: {
    "score": -0.9666146668203909,
    "rmse": 0.9666146668203909
}
[08/12/2025 18:28:05 INFO]: Test stats: {
    "score": -0.904012031035859,
    "rmse": 0.904012031035859
}
[08/12/2025 18:28:52 INFO]: Training loss at epoch 73: 0.29846662282943726
[08/12/2025 18:29:42 INFO]: Training loss at epoch 70: 0.616879940032959
[08/12/2025 18:30:20 INFO]: Training loss at epoch 74: 0.31478914618492126
[08/12/2025 18:31:17 INFO]: Training loss at epoch 71: 0.6704050302505493
[08/12/2025 18:31:51 INFO]: Training loss at epoch 75: 0.2827947437763214
[08/12/2025 18:32:53 INFO]: Training loss at epoch 72: 0.7079009711742401
[08/12/2025 18:33:22 INFO]: Training loss at epoch 76: 0.3086143583059311
[08/12/2025 18:34:28 INFO]: Training loss at epoch 73: 0.6245662569999695
[08/12/2025 18:34:51 INFO]: Training loss at epoch 77: 0.2819449454545975
[08/12/2025 18:36:02 INFO]: Training loss at epoch 74: 0.6612982451915741
[08/12/2025 18:36:23 INFO]: Training loss at epoch 78: 0.23506063222885132
[08/12/2025 18:37:36 INFO]: Training loss at epoch 75: 0.7434407472610474
[08/12/2025 18:37:48 INFO]: New best epoch, val score: -0.7970977205948137
[08/12/2025 18:37:48 INFO]: Saving model to: model_best.pth
[08/12/2025 18:37:54 INFO]: Training loss at epoch 79: 0.23772969841957092
[08/12/2025 18:38:25 INFO]: Training stats: {
    "score": -0.5255537349792905,
    "rmse": 0.5255537349792905
}
[08/12/2025 18:38:25 INFO]: Val stats: {
    "score": -0.7390100186936945,
    "rmse": 0.7390100186936945
}
[08/12/2025 18:38:25 INFO]: Test stats: {
    "score": -0.8184268297809185,
    "rmse": 0.8184268297809185
}
[08/12/2025 18:39:10 INFO]: Training loss at epoch 76: 0.5884168744087219
[08/12/2025 18:39:55 INFO]: Training loss at epoch 80: 0.27025048434734344
[08/12/2025 18:40:42 INFO]: Training loss at epoch 77: 0.6640960574150085
[08/12/2025 18:41:23 INFO]: Training loss at epoch 81: 0.2217877134680748
[08/12/2025 18:42:19 INFO]: Training loss at epoch 78: 0.6797138452529907
[08/12/2025 18:42:56 INFO]: Training loss at epoch 82: 0.26939044892787933
[08/12/2025 18:43:54 INFO]: Training loss at epoch 79: 0.5918125212192535
[08/12/2025 18:44:26 INFO]: Training loss at epoch 83: 0.22890786826610565
[08/12/2025 18:44:28 INFO]: Training stats: {
    "score": -0.744646025581402,
    "rmse": 0.744646025581402
}
[08/12/2025 18:44:28 INFO]: Val stats: {
    "score": -0.7910553268404237,
    "rmse": 0.7910553268404237
}
[08/12/2025 18:44:28 INFO]: Test stats: {
    "score": -0.7930527601937993,
    "rmse": 0.7930527601937993
}
[08/12/2025 18:44:40 INFO]: New best epoch, val score: -0.7910553268404237
[08/12/2025 18:44:40 INFO]: Saving model to: model_best.pth
[08/12/2025 18:45:58 INFO]: Training loss at epoch 84: 0.2853539511561394
[08/12/2025 18:46:03 INFO]: Training loss at epoch 80: 0.5600065886974335
[08/12/2025 18:47:29 INFO]: Training loss at epoch 85: 0.2061961144208908
[08/12/2025 18:47:38 INFO]: Training loss at epoch 81: 0.5889375507831573
[08/12/2025 18:49:02 INFO]: Training loss at epoch 86: 0.285023957490921
[08/12/2025 18:49:15 INFO]: Training loss at epoch 82: 0.5830259323120117
[08/12/2025 18:50:34 INFO]: Training loss at epoch 87: 0.25199446082115173
[08/12/2025 18:50:50 INFO]: Training loss at epoch 83: 0.5507018864154816
[08/12/2025 18:52:07 INFO]: Training loss at epoch 88: 0.22881846129894257
[08/12/2025 18:52:25 INFO]: Training loss at epoch 84: 0.4291892498731613
[08/12/2025 18:53:37 INFO]: Training loss at epoch 89: 0.2692996487021446
[08/12/2025 18:54:01 INFO]: Training loss at epoch 85: 0.4536411166191101
[08/12/2025 18:54:07 INFO]: Training stats: {
    "score": -0.47560764543495243,
    "rmse": 0.47560764543495243
}
[08/12/2025 18:54:07 INFO]: Val stats: {
    "score": -0.758327312160908,
    "rmse": 0.758327312160908
}
[08/12/2025 18:54:07 INFO]: Test stats: {
    "score": -0.7777313033433149,
    "rmse": 0.7777313033433149
}
[08/12/2025 18:55:36 INFO]: Training loss at epoch 86: 0.6358064264059067
[08/12/2025 18:55:38 INFO]: Training loss at epoch 90: 0.2129632607102394
[08/12/2025 18:57:11 INFO]: Training loss at epoch 91: 0.24026888608932495
[08/12/2025 18:57:12 INFO]: Training loss at epoch 87: 0.4282955229282379
[08/12/2025 18:57:22 INFO]: Running Final Evaluation...
[08/12/2025 18:58:45 INFO]: Training loss at epoch 88: 0.5696341693401337
[08/12/2025 19:00:15 INFO]: Training loss at epoch 89: 0.5196355581283569
[08/12/2025 19:00:46 INFO]: Training stats: {
    "score": -0.7109501597481396,
    "rmse": 0.7109501597481396
}
[08/12/2025 19:00:46 INFO]: Val stats: {
    "score": -0.8058695206395736,
    "rmse": 0.8058695206395736
}
[08/12/2025 19:00:46 INFO]: Test stats: {
    "score": -0.8028362528537115,
    "rmse": 0.8028362528537115
}
[08/12/2025 19:02:16 INFO]: Training loss at epoch 90: 0.43494701385498047
[08/12/2025 19:03:48 INFO]: Training loss at epoch 91: 0.4222726970911026
[08/12/2025 19:05:18 INFO]: Training loss at epoch 92: 0.5185230523347855
[08/12/2025 19:06:49 INFO]: Training loss at epoch 93: 0.5445816516876221
[08/12/2025 19:08:20 INFO]: Training loss at epoch 94: 0.6368153840303421
[08/12/2025 19:09:51 INFO]: Training loss at epoch 95: 0.5251375734806061
[08/12/2025 19:11:21 INFO]: Training loss at epoch 96: 0.35501260310411453
[08/12/2025 19:11:33 INFO]: New best epoch, val score: -0.7772845007262754
[08/12/2025 19:11:33 INFO]: Saving model to: model_best.pth
[08/12/2025 19:12:53 INFO]: Training loss at epoch 97: 0.43712931871414185
[08/12/2025 19:14:27 INFO]: Training loss at epoch 98: 0.4669840335845947
[08/12/2025 19:15:56 INFO]: Training loss at epoch 99: 0.4701375365257263
[08/12/2025 19:16:27 INFO]: Training stats: {
    "score": -0.6716974756388696,
    "rmse": 0.6716974756388696
}
[08/12/2025 19:16:27 INFO]: Val stats: {
    "score": -0.8384397135645933,
    "rmse": 0.8384397135645933
}
[08/12/2025 19:16:27 INFO]: Test stats: {
    "score": -0.7751866443859255,
    "rmse": 0.7751866443859255
}
[08/12/2025 19:17:58 INFO]: Training loss at epoch 100: 0.485187366604805
[08/12/2025 19:19:29 INFO]: Training loss at epoch 101: 0.49143746495246887
[08/12/2025 19:21:00 INFO]: Training loss at epoch 102: 0.3664880692958832
[08/12/2025 19:22:29 INFO]: Training loss at epoch 103: 0.37655021250247955
[08/12/2025 19:24:16 INFO]: Training loss at epoch 104: 0.3953136205673218
[08/12/2025 19:26:07 INFO]: Training loss at epoch 105: 0.4280976355075836
[08/12/2025 19:27:58 INFO]: Training loss at epoch 106: 0.4755991995334625
[08/12/2025 19:29:49 INFO]: Training loss at epoch 107: 0.5216068923473358
[08/12/2025 19:31:40 INFO]: Training loss at epoch 108: 0.5179262608289719
[08/12/2025 19:33:30 INFO]: Training loss at epoch 109: 0.4708481878042221
[08/12/2025 19:34:08 INFO]: Training stats: {
    "score": -0.6687949226304781,
    "rmse": 0.6687949226304781
}
[08/12/2025 19:34:08 INFO]: Val stats: {
    "score": -0.7578570294055021,
    "rmse": 0.7578570294055021
}
[08/12/2025 19:34:08 INFO]: Test stats: {
    "score": -0.7603168365745975,
    "rmse": 0.7603168365745975
}
[08/12/2025 19:34:22 INFO]: New best epoch, val score: -0.7578570294055021
[08/12/2025 19:34:22 INFO]: Saving model to: model_best.pth
[08/12/2025 19:35:59 INFO]: Training loss at epoch 110: 0.3913554847240448
[08/12/2025 19:36:12 INFO]: New best epoch, val score: -0.754127078864875
[08/12/2025 19:36:12 INFO]: Saving model to: model_best.pth
[08/12/2025 19:37:50 INFO]: Training loss at epoch 111: 0.5627211928367615
[08/12/2025 19:39:41 INFO]: Training loss at epoch 112: 0.510398805141449
[08/12/2025 19:41:32 INFO]: Training loss at epoch 113: 0.4832407385110855
[08/12/2025 19:43:23 INFO]: Training loss at epoch 114: 0.3604668080806732
[08/12/2025 19:45:15 INFO]: Training loss at epoch 115: 0.5070283859968185
[08/12/2025 19:47:06 INFO]: Training loss at epoch 116: 0.5801467299461365
[08/12/2025 19:48:57 INFO]: Training loss at epoch 117: 0.36896440386772156
[08/12/2025 19:50:48 INFO]: Training loss at epoch 118: 0.4084034711122513
[08/12/2025 19:52:39 INFO]: Training loss at epoch 119: 0.42525342106819153
[08/12/2025 19:53:17 INFO]: Training stats: {
    "score": -0.6729165348587801,
    "rmse": 0.6729165348587801
}
[08/12/2025 19:53:17 INFO]: Val stats: {
    "score": -0.7918688623418247,
    "rmse": 0.7918688623418247
}
[08/12/2025 19:53:17 INFO]: Test stats: {
    "score": -0.788872890889186,
    "rmse": 0.788872890889186
}
[08/12/2025 19:55:08 INFO]: Training loss at epoch 120: 0.4700523018836975
[08/12/2025 19:56:59 INFO]: Training loss at epoch 121: 0.48965364694595337
[08/12/2025 19:58:49 INFO]: Training loss at epoch 122: 0.530071422457695
[08/12/2025 20:00:39 INFO]: Training loss at epoch 123: 0.37197625637054443
[08/12/2025 20:02:30 INFO]: Training loss at epoch 124: 0.39976489543914795
[08/12/2025 20:04:21 INFO]: Training loss at epoch 125: 0.4383438527584076
[08/12/2025 20:06:11 INFO]: Training loss at epoch 126: 0.4724489897489548
[08/12/2025 20:08:01 INFO]: Training loss at epoch 127: 0.34391336143016815
[08/12/2025 20:09:52 INFO]: Training loss at epoch 128: 0.4013676047325134
[08/12/2025 20:11:43 INFO]: Training loss at epoch 129: 0.34910009801387787
[08/12/2025 20:12:21 INFO]: Training stats: {
    "score": -0.6241072262975765,
    "rmse": 0.6241072262975765
}
[08/12/2025 20:12:21 INFO]: Val stats: {
    "score": -0.8131277695090195,
    "rmse": 0.8131277695090195
}
[08/12/2025 20:12:21 INFO]: Test stats: {
    "score": -0.7379021000913406,
    "rmse": 0.7379021000913406
}
[08/12/2025 20:14:12 INFO]: Training loss at epoch 130: 0.34881503880023956
[08/12/2025 20:16:02 INFO]: Training loss at epoch 131: 0.31325697898864746
[08/12/2025 20:17:53 INFO]: Training loss at epoch 132: 0.37359902262687683
[08/12/2025 20:19:44 INFO]: Training loss at epoch 133: 0.30193567276000977
[08/12/2025 20:21:35 INFO]: Training loss at epoch 134: 0.5156541764736176
[08/12/2025 20:23:25 INFO]: Training loss at epoch 135: 0.4444376826286316
[08/12/2025 20:25:16 INFO]: Training loss at epoch 136: 0.46323801577091217
[08/12/2025 20:27:07 INFO]: Training loss at epoch 137: 0.3854387700557709
[08/12/2025 20:28:58 INFO]: Training loss at epoch 138: 0.36126065254211426
[08/12/2025 20:30:48 INFO]: Training loss at epoch 139: 0.4169795364141464
[08/12/2025 20:31:27 INFO]: Training stats: {
    "score": -0.6114990395465663,
    "rmse": 0.6114990395465663
}
[08/12/2025 20:31:27 INFO]: Val stats: {
    "score": -0.8174849570957936,
    "rmse": 0.8174849570957936
}
[08/12/2025 20:31:27 INFO]: Test stats: {
    "score": -0.7241185339763765,
    "rmse": 0.7241185339763765
}
[08/12/2025 20:33:18 INFO]: Training loss at epoch 140: 0.3529791086912155
[08/12/2025 20:35:07 INFO]: Training loss at epoch 141: 0.3905086815357208
[08/12/2025 20:35:21 INFO]: Running Final Evaluation...
[08/12/2025 20:35:59 INFO]: Training accuracy: {
    "score": -0.6665243986724892,
    "rmse": 0.6665243986724892
}
[08/12/2025 20:35:59 INFO]: Val accuracy: {
    "score": -0.754127078864875,
    "rmse": 0.754127078864875
}
[08/12/2025 20:35:59 INFO]: Test accuracy: {
    "score": -0.7533046049425218,
    "rmse": 0.7533046049425218
}
[08/12/2025 20:35:59 INFO]: {
    "dataset": "ic_upstream3",
    "model_name": "ft_transformer",
    "run_id": "revolved-Rody",
    "best_epoch": 110,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.7533046049425218,
        "rmse": 0.7533046049425218
    },
    "train_stats": {
        "score": -0.6665243986724892,
        "rmse": 0.6665243986724892
    },
    "val_stats": {
        "score": -0.754127078864875,
        "rmse": 0.754127078864875
    }
}
