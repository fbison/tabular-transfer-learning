[08/27/2025 14:34:58 INFO]: Building Dataset
[08/27/2025 14:34:58 INFO]: pre normalizer.fit

[08/27/2025 14:34:58 INFO]: pos normalizer.fit

[08/27/2025 14:35:00 INFO]: Task: regression, Dataset: ic_upstream2_ImputacaoEstatistica_exp_100_1, n_numerical: 162, n_categorical: 0, n_classes: 1, n_train_samples: 282, n_val_samples: 66, n_test_samples: 87
[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 2.3789835444170646
  attention_dropout: 0.0990505706355585
  ffn_dropout: 0.0990505706355585
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 5.0889260523813867e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: -1
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_1

[08/27/2025 14:35:07 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 4
  d_ffn_factor: 1.6615563296322966
  attention_dropout: 0.23468319740506238
  ffn_dropout: 0.23468319740506238
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 9.208565483189965e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_2

[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 2.472839892458197
  attention_dropout: 0.25720686175081753
  ffn_dropout: 0.25720686175081753
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.106578965114205e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_3

[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 0.6753451065549964
  attention_dropout: 0.024371583897178528
  ffn_dropout: 0.024371583897178528
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00015887505030621167
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_4

[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 16
  d_ffn_factor: 2.626571936048099
  attention_dropout: 0.19965658628005506
  ffn_dropout: 0.19965658628005506
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0003834536080538385
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_5

[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 2.5356797473606867
  attention_dropout: 0.41397509115540887
  ffn_dropout: 0.41397509115540887
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0002066876007819504
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_6

[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 1.828895985150878
  attention_dropout: 0.2718809951975485
  ffn_dropout: 0.2718809951975485
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.9848078829408577e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_7

[08/27/2025 14:35:07 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 2.119141960190843
  attention_dropout: 0.3981926540620071
  ffn_dropout: 0.3981926540620071
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0004599342918170343
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_8

[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 1.9095853119924704
  attention_dropout: 0.31143268279517144
  ffn_dropout: 0.31143268279517144
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0009137390514429863
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_9

[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: 
_________________________________________________

[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 16
  d_ffn_factor: 0.6711320416280188
  attention_dropout: 0.26185484979708157
  ffn_dropout: 0.26185484979708157
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00010811838141844301
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_10

[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 4
  d_ffn_factor: 0.7663468684351257
  attention_dropout: 0.08755720083223029
  ffn_dropout: 0.08755720083223029
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0003541322958090609
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_11

[08/27/2025 14:35:08 INFO]: 
_________________________________________________

[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: 
_________________________________________________

[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: 
_________________________________________________

[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: 
_________________________________________________

[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: 
_________________________________________________

[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 1.5714763259785602
  attention_dropout: 0.0645176035549267
  ffn_dropout: 0.0645176035549267
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00019758852998134349
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_16

[08/27/2025 14:35:08 INFO]: 
_________________________________________________

[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: 
_________________________________________________

[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.722038343533273
  attention_dropout: 0.46816755829285084
  ffn_dropout: 0.46816755829285084
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.737333323015034e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_15

[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 2.091077065710853
  attention_dropout: 0.4587914839020757
  ffn_dropout: 0.4587914839020757
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0004693408793515337
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_20

[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 1.6888897357864412
  attention_dropout: 0.4147037657047055
  ffn_dropout: 0.4147037657047055
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00010179419522474537
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_18

[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: 
_________________________________________________

[08/27/2025 14:35:08 INFO]: 
_________________________________________________

[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 0.7885646041491953
  attention_dropout: 0.2524123322798536
  ffn_dropout: 0.2524123322798536
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00029245068689279935
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_12

[08/27/2025 14:35:08 INFO]: This ft_transformer has 0.284 million parameters.
[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 1.582420030937746
  attention_dropout: 0.14311260966182238
  ffn_dropout: 0.14311260966182238
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00011983895669884569
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_19

[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 1.4402113236000673
  attention_dropout: 0.14321471820413095
  ffn_dropout: 0.14321471820413095
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0007823017908140833
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_14

[08/27/2025 14:35:08 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.1966543739053948
  attention_dropout: 0.06523556360867544
  ffn_dropout: 0.06523556360867544
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.5701834669235224e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_17

[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 1.5923694889135258
  attention_dropout: 0.24111739628621176
  ffn_dropout: 0.24111739628621176
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.4604626411872425e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_13

[08/27/2025 14:35:08 INFO]: This ft_transformer has 1.910 million parameters.
[08/27/2025 14:35:08 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:09 INFO]: This ft_transformer has 0.067 million parameters.
[08/27/2025 14:35:09 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:09 INFO]: This ft_transformer has 0.381 million parameters.
[08/27/2025 14:35:09 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:09 INFO]: This ft_transformer has 4.079 million parameters.
[08/27/2025 14:35:09 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:10 INFO]: This ft_transformer has 2.018 million parameters.
[08/27/2025 14:35:10 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:10 INFO]: This ft_transformer has 6.251 million parameters.
[08/27/2025 14:35:10 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:10 INFO]: This ft_transformer has 10.006 million parameters.
[08/27/2025 14:35:10 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:10 INFO]: This ft_transformer has 2.028 million parameters.
[08/27/2025 14:35:10 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:10 INFO]: This ft_transformer has 14.272 million parameters.
[08/27/2025 14:35:10 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:10 INFO]: This ft_transformer has 6.106 million parameters.
[08/27/2025 14:35:10 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:10 INFO]: This ft_transformer has 4.957 million parameters.
[08/27/2025 14:35:10 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:10 INFO]: This ft_transformer has 0.277 million parameters.
[08/27/2025 14:35:10 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:11 INFO]: This ft_transformer has 0.750 million parameters.
[08/27/2025 14:35:11 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:11 INFO]: This ft_transformer has 0.312 million parameters.
[08/27/2025 14:35:11 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:11 INFO]: This ft_transformer has 0.993 million parameters.
[08/27/2025 14:35:11 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:11 INFO]: This ft_transformer has 1.181 million parameters.
[08/27/2025 14:35:11 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:11 INFO]: This ft_transformer has 9.428 million parameters.
[08/27/2025 14:35:11 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:11 INFO]: This ft_transformer has 23.157 million parameters.
[08/27/2025 14:35:11 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:11 INFO]: This ft_transformer has 8.380 million parameters.
[08/27/2025 14:35:11 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:34 INFO]: Training loss at epoch 0: 1.4856849312782288
[08/27/2025 14:35:35 INFO]: New best epoch, val score: -0.9318268088469179
[08/27/2025 14:35:35 INFO]: Saving model to: maddest-Elbert_trial_11/model_best.pth
[08/27/2025 14:35:39 INFO]: Training loss at epoch 0: 1.1608113050460815
[08/27/2025 14:35:40 INFO]: Training loss at epoch 1: 1.4548436403274536
[08/27/2025 14:35:41 INFO]: New best epoch, val score: -0.8859581449233314
[08/27/2025 14:35:41 INFO]: Saving model to: maddest-Elbert_trial_2/model_best.pth
[08/27/2025 14:35:41 INFO]: New best epoch, val score: -0.8830357801302532
[08/27/2025 14:35:41 INFO]: Saving model to: maddest-Elbert_trial_11/model_best.pth
[08/27/2025 14:35:42 INFO]: Training loss at epoch 0: 1.095881998538971
[08/27/2025 14:35:44 INFO]: New best epoch, val score: -0.8791749079426427
[08/27/2025 14:35:44 INFO]: Saving model to: maddest-Elbert_trial_20/model_best.pth
[08/27/2025 14:35:47 INFO]: Training loss at epoch 2: 1.1595349311828613
[08/27/2025 14:35:51 INFO]: Training loss at epoch 1: 1.0932008624076843
[08/27/2025 14:35:53 INFO]: Training loss at epoch 3: 0.9845678806304932
[08/27/2025 14:35:58 INFO]: Training loss at epoch 1: 1.0625574588775635
[08/27/2025 14:35:59 INFO]: Training loss at epoch 4: 1.1880643367767334
[08/27/2025 14:36:01 INFO]: Training loss at epoch 2: 0.966352254152298
[08/27/2025 14:36:04 INFO]: Training loss at epoch 0: 0.9339824318885803
[08/27/2025 14:36:05 INFO]: Training loss at epoch 5: 1.0619772672653198
[08/27/2025 14:36:05 INFO]: Training loss at epoch 0: 1.2569559812545776
[08/27/2025 14:36:06 INFO]: Training loss at epoch 0: 1.2944777607917786
[08/27/2025 14:36:08 INFO]: Training loss at epoch 0: 1.057641327381134
[08/27/2025 14:36:09 INFO]: New best epoch, val score: -0.8970419343368582
[08/27/2025 14:36:09 INFO]: Saving model to: maddest-Elbert_trial_7/model_best.pth
[08/27/2025 14:36:10 INFO]: New best epoch, val score: -1.02870706548895
[08/27/2025 14:36:10 INFO]: Saving model to: maddest-Elbert_trial_16/model_best.pth
[08/27/2025 14:36:11 INFO]: Training loss at epoch 6: 1.203512191772461
[08/27/2025 14:36:12 INFO]: New best epoch, val score: -0.8787650193261191
[08/27/2025 14:36:12 INFO]: Saving model to: maddest-Elbert_trial_4/model_best.pth
[08/27/2025 14:36:12 INFO]: Training loss at epoch 3: 1.35948246717453
[08/27/2025 14:36:13 INFO]: New best epoch, val score: -1.0469909946948972
[08/27/2025 14:36:13 INFO]: Saving model to: maddest-Elbert_trial_17/model_best.pth
[08/27/2025 14:36:14 INFO]: Training loss at epoch 2: 0.982991099357605
[08/27/2025 14:36:17 INFO]: Training loss at epoch 7: 1.1709440350532532
[08/27/2025 14:36:23 INFO]: Training loss at epoch 4: 1.0686804354190826
[08/27/2025 14:36:23 INFO]: Training loss at epoch 8: 1.0833347737789154
[08/27/2025 14:36:29 INFO]: Training loss at epoch 9: 1.09855255484581
[08/27/2025 14:36:30 INFO]: Training loss at epoch 3: 1.2336355447769165
[08/27/2025 14:36:31 INFO]: Training stats: {
    "score": -1.1011182144136131,
    "rmse": 1.1011182144136131
}
[08/27/2025 14:36:31 INFO]: Val stats: {
    "score": -1.1490318855658617,
    "rmse": 1.1490318855658617
}
[08/27/2025 14:36:31 INFO]: Test stats: {
    "score": -1.0749604217192419,
    "rmse": 1.0749604217192419
}
[08/27/2025 14:36:34 INFO]: Training loss at epoch 5: 1.2622454762458801
[08/27/2025 14:36:35 INFO]: Training loss at epoch 0: 1.1631447672843933
[08/27/2025 14:36:37 INFO]: Training loss at epoch 10: 1.1809203624725342
[08/27/2025 14:36:44 INFO]: Training loss at epoch 11: 0.9830699861049652
[08/27/2025 14:36:44 INFO]: New best epoch, val score: -0.920124675147283
[08/27/2025 14:36:44 INFO]: Saving model to: maddest-Elbert_trial_15/model_best.pth
[08/27/2025 14:36:45 INFO]: Training loss at epoch 6: 1.0249182879924774
[08/27/2025 14:36:45 INFO]: Training loss at epoch 1: 1.0279228687286377
[08/27/2025 14:36:46 INFO]: Training loss at epoch 4: 0.9687570035457611
[08/27/2025 14:36:48 INFO]: Training loss at epoch 1: 1.1256009936332703
[08/27/2025 14:36:48 INFO]: Training loss at epoch 0: 1.0697077512741089
[08/27/2025 14:36:50 INFO]: Training loss at epoch 12: 1.256722629070282
[08/27/2025 14:36:50 INFO]: New best epoch, val score: -0.8951188852517223
[08/27/2025 14:36:50 INFO]: Saving model to: maddest-Elbert_trial_7/model_best.pth
[08/27/2025 14:36:52 INFO]: Training loss at epoch 1: 1.134505808353424
[08/27/2025 14:36:54 INFO]: New best epoch, val score: -0.8667804952027927
[08/27/2025 14:36:54 INFO]: Saving model to: maddest-Elbert_trial_16/model_best.pth
[08/27/2025 14:36:55 INFO]: Training loss at epoch 1: 1.1078773736953735
[08/27/2025 14:36:57 INFO]: Training loss at epoch 7: 0.8959198296070099
[08/27/2025 14:36:57 INFO]: Training loss at epoch 0: 1.8431708812713623
[08/27/2025 14:36:57 INFO]: Training loss at epoch 13: 0.9730224609375
[08/27/2025 14:37:01 INFO]: New best epoch, val score: -0.9048305964600398
[08/27/2025 14:37:01 INFO]: Saving model to: maddest-Elbert_trial_17/model_best.pth
[08/27/2025 14:37:02 INFO]: New best epoch, val score: -0.9518937645232959
[08/27/2025 14:37:02 INFO]: Saving model to: maddest-Elbert_trial_13/model_best.pth
[08/27/2025 14:37:03 INFO]: Training loss at epoch 5: 1.0347149074077606
[08/27/2025 14:37:04 INFO]: Training loss at epoch 14: 1.0440554022789001
[08/27/2025 14:37:08 INFO]: Training loss at epoch 8: 1.186530590057373
[08/27/2025 14:37:09 INFO]: Training loss at epoch 0: 0.8710848987102509
[08/27/2025 14:37:10 INFO]: New best epoch, val score: -1.633318922081712
[08/27/2025 14:37:10 INFO]: Saving model to: maddest-Elbert_trial_18/model_best.pth
[08/27/2025 14:37:10 INFO]: Training loss at epoch 15: 0.9561543166637421
[08/27/2025 14:37:16 INFO]: Training loss at epoch 16: 1.1203667521476746
[08/27/2025 14:37:18 INFO]: Training loss at epoch 9: 1.0400704145431519
[08/27/2025 14:37:19 INFO]: Training loss at epoch 6: 1.1071839928627014
[08/27/2025 14:37:22 INFO]: Training loss at epoch 17: 0.9638145864009857
[08/27/2025 14:37:22 INFO]: Training stats: {
    "score": -1.0292753195534583,
    "rmse": 1.0292753195534583
}
[08/27/2025 14:37:22 INFO]: Val stats: {
    "score": -1.0236777714251084,
    "rmse": 1.0236777714251084
}
[08/27/2025 14:37:22 INFO]: Test stats: {
    "score": -1.0018549793283231,
    "rmse": 1.0018549793283231
}
[08/27/2025 14:37:24 INFO]: New best epoch, val score: -1.0096669039191102
[08/27/2025 14:37:24 INFO]: Saving model to: maddest-Elbert_trial_14/model_best.pth
[08/27/2025 14:37:27 INFO]: Training loss at epoch 0: 1.7567843794822693
[08/27/2025 14:37:29 INFO]: Training loss at epoch 18: 1.1010745763778687
[08/27/2025 14:37:29 INFO]: Training loss at epoch 2: 1.0074380040168762
[08/27/2025 14:37:33 INFO]: Training loss at epoch 2: 1.3821571469306946
[08/27/2025 14:37:35 INFO]: Training loss at epoch 10: 0.850780576467514
[08/27/2025 14:37:35 INFO]: New best epoch, val score: -0.8949093359549867
[08/27/2025 14:37:35 INFO]: Saving model to: maddest-Elbert_trial_7/model_best.pth
[08/27/2025 14:37:36 INFO]: Training loss at epoch 19: 0.8255588710308075
[08/27/2025 14:37:37 INFO]: Training loss at epoch 7: 0.9753566086292267
[08/27/2025 14:37:38 INFO]: Training stats: {
    "score": -1.0014146138437656,
    "rmse": 1.0014146138437656
}
[08/27/2025 14:37:38 INFO]: Val stats: {
    "score": -0.9091617186724159,
    "rmse": 0.9091617186724159
}
[08/27/2025 14:37:38 INFO]: Test stats: {
    "score": -0.9709389027408319,
    "rmse": 0.9709389027408319
}
[08/27/2025 14:37:39 INFO]: Training loss at epoch 2: 1.3898561596870422
[08/27/2025 14:37:44 INFO]: Training loss at epoch 2: 0.8074041306972504
[08/27/2025 14:37:44 INFO]: Training loss at epoch 20: 1.0319429636001587
[08/27/2025 14:37:46 INFO]: New best epoch, val score: -1.5588517590161706
[08/27/2025 14:37:46 INFO]: Saving model to: maddest-Elbert_trial_10/model_best.pth
[08/27/2025 14:37:46 INFO]: Training loss at epoch 11: 0.9282391667366028
[08/27/2025 14:37:49 INFO]: New best epoch, val score: -0.8760096685992153
[08/27/2025 14:37:49 INFO]: Saving model to: maddest-Elbert_trial_17/model_best.pth
[08/27/2025 14:37:50 INFO]: Training loss at epoch 21: 0.8184985220432281
[08/27/2025 14:37:53 INFO]: Training loss at epoch 8: 1.1816644072532654
[08/27/2025 14:37:56 INFO]: Training loss at epoch 22: 0.8460955917835236
[08/27/2025 14:37:57 INFO]: Training loss at epoch 12: 0.923500657081604
[08/27/2025 14:37:57 INFO]: Training loss at epoch 0: 1.0174176692962646
[08/27/2025 14:37:57 INFO]: Training loss at epoch 1: 1.0561509132385254
[08/27/2025 14:38:02 INFO]: Training loss at epoch 23: 0.9110175371170044
[08/27/2025 14:38:07 INFO]: New best epoch, val score: -0.8802363047789352
[08/27/2025 14:38:07 INFO]: Saving model to: maddest-Elbert_trial_15/model_best.pth
[08/27/2025 14:38:08 INFO]: Training loss at epoch 13: 0.9702128171920776
[08/27/2025 14:38:10 INFO]: Training loss at epoch 24: 0.92124143242836
[08/27/2025 14:38:11 INFO]: Training loss at epoch 9: 1.0046337246894836
[08/27/2025 14:38:13 INFO]: Training loss at epoch 3: 1.099191576242447
[08/27/2025 14:38:16 INFO]: Training loss at epoch 25: 0.9450976848602295
[08/27/2025 14:38:17 INFO]: Training stats: {
    "score": -1.010844149710607,
    "rmse": 1.010844149710607
}
[08/27/2025 14:38:17 INFO]: Val stats: {
    "score": -0.8889005790211414,
    "rmse": 0.8889005790211414
}
[08/27/2025 14:38:17 INFO]: Test stats: {
    "score": -0.9836097081362599,
    "rmse": 0.9836097081362599
}
[08/27/2025 14:38:18 INFO]: Training loss at epoch 3: 0.9217081964015961
[08/27/2025 14:38:19 INFO]: New best epoch, val score: -0.9186521080660278
[08/27/2025 14:38:19 INFO]: Saving model to: maddest-Elbert_trial_8/model_best.pth
[08/27/2025 14:38:20 INFO]: Training loss at epoch 14: 0.8468897938728333
[08/27/2025 14:38:22 INFO]: Training loss at epoch 26: 0.9987317025661469
[08/27/2025 14:38:26 INFO]: Training loss at epoch 1: 1.1900747418403625
[08/27/2025 14:38:26 INFO]: Training loss at epoch 3: 1.2958901226520538
[08/27/2025 14:38:28 INFO]: Training loss at epoch 27: 1.1028594374656677
[08/27/2025 14:38:31 INFO]: Training loss at epoch 15: 0.9971080720424652
[08/27/2025 14:38:33 INFO]: Training loss at epoch 3: 1.1053932905197144
[08/27/2025 14:38:34 INFO]: Training loss at epoch 10: 1.22406005859375
[08/27/2025 14:38:35 INFO]: Training loss at epoch 28: 0.9321175217628479
[08/27/2025 14:38:39 INFO]: New best epoch, val score: -0.9311403259144427
[08/27/2025 14:38:39 INFO]: Saving model to: maddest-Elbert_trial_13/model_best.pth
[08/27/2025 14:38:42 INFO]: Training loss at epoch 29: 1.032220482826233
[08/27/2025 14:38:43 INFO]: Training loss at epoch 1: 1.476029098033905
[08/27/2025 14:38:44 INFO]: Training loss at epoch 16: 1.1940070390701294
[08/27/2025 14:38:44 INFO]: Training stats: {
    "score": -1.0001644016990152,
    "rmse": 1.0001644016990152
}
[08/27/2025 14:38:44 INFO]: Val stats: {
    "score": -0.9132055226415579,
    "rmse": 0.9132055226415579
}
[08/27/2025 14:38:44 INFO]: Test stats: {
    "score": -0.9693428613179581,
    "rmse": 0.9693428613179581
}
[08/27/2025 14:38:50 INFO]: Training loss at epoch 30: 0.8893933594226837
[08/27/2025 14:38:51 INFO]: Training loss at epoch 11: 1.2675021290779114
[08/27/2025 14:38:54 INFO]: Training loss at epoch 17: 0.9573728144168854
[08/27/2025 14:38:56 INFO]: Training loss at epoch 4: 0.9844764173030853
[08/27/2025 14:38:56 INFO]: New best epoch, val score: -0.8813504885993805
[08/27/2025 14:38:56 INFO]: Saving model to: maddest-Elbert_trial_18/model_best.pth
[08/27/2025 14:38:57 INFO]: Training loss at epoch 31: 1.022257387638092
[08/27/2025 14:39:02 INFO]: Training loss at epoch 0: 1.2466680407524109
[08/27/2025 14:39:02 INFO]: Training loss at epoch 4: 0.9226864278316498
[08/27/2025 14:39:03 INFO]: Training loss at epoch 32: 1.2105789184570312
[08/27/2025 14:39:03 INFO]: Running Final Evaluation...
[08/27/2025 14:39:05 INFO]: Training loss at epoch 18: 1.03920179605484
[08/27/2025 14:39:06 INFO]: Training accuracy: {
    "score": -1.06343506604211,
    "rmse": 1.06343506604211
}
[08/27/2025 14:39:06 INFO]: Val accuracy: {
    "score": -0.8830357801302532,
    "rmse": 0.8830357801302532
}
[08/27/2025 14:39:06 INFO]: Test accuracy: {
    "score": -1.0317191176095473,
    "rmse": 1.0317191176095473
}
[08/27/2025 14:39:06 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_11",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0317191176095473,
        "rmse": 1.0317191176095473
    },
    "train_stats": {
        "score": -1.06343506604211,
        "rmse": 1.06343506604211
    },
    "val_stats": {
        "score": -0.8830357801302532,
        "rmse": 0.8830357801302532
    }
}
[08/27/2025 14:39:06 INFO]: Procewss finished for trial maddest-Elbert_trial_11
[08/27/2025 14:39:06 INFO]: 
_________________________________________________

[08/27/2025 14:39:06 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:39:06 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 16
  d_ffn_factor: 2.6292407256110772
  attention_dropout: 0.12174408162548056
  ffn_dropout: 0.12174408162548056
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.000276106383603318
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_21

[08/27/2025 14:39:06 INFO]: This ft_transformer has 1.266 million parameters.
[08/27/2025 14:39:06 INFO]: Training will start at epoch 0.
[08/27/2025 14:39:06 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:39:07 INFO]: Training loss at epoch 12: 1.0418239831924438
[08/27/2025 14:39:09 INFO]: Training loss at epoch 1: 1.6840440034866333
[08/27/2025 14:39:12 INFO]: Training loss at epoch 4: 0.9280783534049988
[08/27/2025 14:39:14 INFO]: Training loss at epoch 0: 1.0540151000022888
[08/27/2025 14:39:16 INFO]: Training loss at epoch 19: 0.879035621881485
[08/27/2025 14:39:19 INFO]: Training loss at epoch 2: 1.0835919380187988
[08/27/2025 14:39:20 INFO]: Training stats: {
    "score": -0.9995965107093503,
    "rmse": 0.9995965107093503
}
[08/27/2025 14:39:20 INFO]: Val stats: {
    "score": -0.9241320930861807,
    "rmse": 0.9241320930861807
}
[08/27/2025 14:39:20 INFO]: Test stats: {
    "score": -0.9706981041620334,
    "rmse": 0.9706981041620334
}
[08/27/2025 14:39:20 INFO]: Training loss at epoch 4: 0.9837367236614227
[08/27/2025 14:39:22 INFO]: Training loss at epoch 13: 1.1746589541435242
[08/27/2025 14:39:23 INFO]: New best epoch, val score: -0.884301360797299
[08/27/2025 14:39:23 INFO]: Saving model to: maddest-Elbert_trial_14/model_best.pth
[08/27/2025 14:39:30 INFO]: New best epoch, val score: -1.0271700955418201
[08/27/2025 14:39:30 INFO]: Saving model to: maddest-Elbert_trial_9/model_best.pth
[08/27/2025 14:39:31 INFO]: Training loss at epoch 20: 0.9215893149375916
[08/27/2025 14:39:37 INFO]: Training loss at epoch 5: 0.931589275598526
[08/27/2025 14:39:39 INFO]: Training loss at epoch 14: 1.1469250917434692
[08/27/2025 14:39:42 INFO]: Training loss at epoch 21: 0.9783447682857513
[08/27/2025 14:39:45 INFO]: Training loss at epoch 5: 0.9407237470149994
[08/27/2025 14:39:47 INFO]: New best epoch, val score: -0.9660443892448037
[08/27/2025 14:39:47 INFO]: Saving model to: maddest-Elbert_trial_5/model_best.pth
[08/27/2025 14:39:47 INFO]: Training loss at epoch 1: 1.5662710070610046
[08/27/2025 14:39:54 INFO]: Training loss at epoch 22: 1.1145482063293457
[08/27/2025 14:39:56 INFO]: Training loss at epoch 15: 1.0661247968673706
[08/27/2025 14:39:59 INFO]: Training loss at epoch 5: 1.053010106086731
[08/27/2025 14:40:02 INFO]: Training loss at epoch 2: 0.9225712716579437
[08/27/2025 14:40:05 INFO]: Training loss at epoch 23: 1.1786044538021088
[08/27/2025 14:40:05 INFO]: New best epoch, val score: -1.0798484905619155
[08/27/2025 14:40:05 INFO]: Saving model to: maddest-Elbert_trial_10/model_best.pth
[08/27/2025 14:40:08 INFO]: Training loss at epoch 5: 0.8269616365432739
[08/27/2025 14:40:12 INFO]: Training loss at epoch 0: 0.7638084441423416
[08/27/2025 14:40:12 INFO]: Training loss at epoch 16: 1.152259349822998
[08/27/2025 14:40:13 INFO]: New best epoch, val score: -0.9259589770121984
[08/27/2025 14:40:13 INFO]: Saving model to: maddest-Elbert_trial_13/model_best.pth
[08/27/2025 14:40:16 INFO]: Training loss at epoch 24: 1.0183075666427612
[08/27/2025 14:40:20 INFO]: Training loss at epoch 6: 0.8444770276546478
[08/27/2025 14:40:22 INFO]: Training loss at epoch 0: 0.8651031255722046
[08/27/2025 14:40:26 INFO]: Training loss at epoch 2: 1.1041529774665833
[08/27/2025 14:40:26 INFO]: Training loss at epoch 25: 1.1575549840927124
[08/27/2025 14:40:28 INFO]: Training loss at epoch 6: 1.1780973672866821
[08/27/2025 14:40:28 INFO]: Training loss at epoch 17: 0.7657723277807236
[08/27/2025 14:40:36 INFO]: Training loss at epoch 0: 1.1561589241027832
[08/27/2025 14:40:37 INFO]: Training loss at epoch 26: 0.9260362386703491
[08/27/2025 14:40:39 INFO]: Training loss at epoch 3: 0.892396867275238
[08/27/2025 14:40:45 INFO]: Training loss at epoch 6: 0.9608414173126221
[08/27/2025 14:40:46 INFO]: Training loss at epoch 18: 0.967814177274704
[08/27/2025 14:40:50 INFO]: Training loss at epoch 27: 0.9970530867576599
[08/27/2025 14:40:52 INFO]: New best epoch, val score: -0.9439914456079566
[08/27/2025 14:40:52 INFO]: Saving model to: maddest-Elbert_trial_12/model_best.pth
[08/27/2025 14:40:53 INFO]: Training loss at epoch 1: 3.145636558532715
[08/27/2025 14:40:56 INFO]: Training loss at epoch 6: 1.2185335159301758
[08/27/2025 14:41:01 INFO]: Training loss at epoch 28: 0.8169337213039398
[08/27/2025 14:41:02 INFO]: Training loss at epoch 19: 1.0119385123252869
[08/27/2025 14:41:03 INFO]: Training loss at epoch 7: 0.8185524940490723
[08/27/2025 14:41:03 INFO]: New best epoch, val score: -0.9529868614020698
[08/27/2025 14:41:03 INFO]: Saving model to: maddest-Elbert_trial_1/model_best.pth
[08/27/2025 14:41:08 INFO]: Training loss at epoch 2: 1.311747431755066
[08/27/2025 14:41:09 INFO]: Training stats: {
    "score": -0.994913813565532,
    "rmse": 0.994913813565532
}
[08/27/2025 14:41:09 INFO]: Val stats: {
    "score": -0.9471812447200645,
    "rmse": 0.9471812447200645
}
[08/27/2025 14:41:09 INFO]: Test stats: {
    "score": -0.9703269162219766,
    "rmse": 0.9703269162219766
}
[08/27/2025 14:41:13 INFO]: Training loss at epoch 29: 0.9470428824424744
[08/27/2025 14:41:14 INFO]: Training loss at epoch 7: 0.9754009246826172
[08/27/2025 14:41:17 INFO]: Training stats: {
    "score": -0.99910841380089,
    "rmse": 0.99910841380089
}
[08/27/2025 14:41:17 INFO]: Val stats: {
    "score": -0.9348914911423124,
    "rmse": 0.9348914911423124
}
[08/27/2025 14:41:17 INFO]: Test stats: {
    "score": -0.9713626608273443,
    "rmse": 0.9713626608273443
}
[08/27/2025 14:41:20 INFO]: New best epoch, val score: -1.0480035998974606
[08/27/2025 14:41:20 INFO]: Saving model to: maddest-Elbert_trial_6/model_best.pth
[08/27/2025 14:41:23 INFO]: Training loss at epoch 0: 1.058409333229065
[08/27/2025 14:41:27 INFO]: Training loss at epoch 20: 1.081792026758194
[08/27/2025 14:41:29 INFO]: Training loss at epoch 30: 1.01227405667305
[08/27/2025 14:41:34 INFO]: Training loss at epoch 7: 0.9028809070587158
[08/27/2025 14:41:39 INFO]: Training loss at epoch 3: 1.1213615536689758
[08/27/2025 14:41:41 INFO]: Training loss at epoch 31: 0.8352846503257751
[08/27/2025 14:41:42 INFO]: Running Final Evaluation...
[08/27/2025 14:41:44 INFO]: New best epoch, val score: -0.9286715753031237
[08/27/2025 14:41:44 INFO]: Saving model to: maddest-Elbert_trial_21/model_best.pth
[08/27/2025 14:41:45 INFO]: Training loss at epoch 21: 1.025657594203949
[08/27/2025 14:41:47 INFO]: Training accuracy: {
    "score": -1.0358270480956295,
    "rmse": 1.0358270480956295
}
[08/27/2025 14:41:47 INFO]: Val accuracy: {
    "score": -0.8859581449233314,
    "rmse": 0.8859581449233314
}
[08/27/2025 14:41:47 INFO]: Test accuracy: {
    "score": -1.0044707541042681,
    "rmse": 1.0044707541042681
}
[08/27/2025 14:41:47 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_2",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0044707541042681,
        "rmse": 1.0044707541042681
    },
    "train_stats": {
        "score": -1.0358270480956295,
        "rmse": 1.0358270480956295
    },
    "val_stats": {
        "score": -0.8859581449233314,
        "rmse": 0.8859581449233314
    }
}
[08/27/2025 14:41:47 INFO]: Procewss finished for trial maddest-Elbert_trial_2
[08/27/2025 14:41:47 INFO]: 
_________________________________________________

[08/27/2025 14:41:47 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:41:47 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 1.9174345084486566
  attention_dropout: 0.24124281224665822
  ffn_dropout: 0.24124281224665822
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 9.733191677722131e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_22

[08/27/2025 14:41:47 INFO]: This ft_transformer has 11.718 million parameters.
[08/27/2025 14:41:47 INFO]: Training will start at epoch 0.
[08/27/2025 14:41:47 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:41:47 INFO]: Training loss at epoch 7: 1.136760175228119
[08/27/2025 14:41:49 INFO]: Training loss at epoch 0: 1.1337844133377075
[08/27/2025 14:41:49 INFO]: Training loss at epoch 8: 1.1032652258872986
[08/27/2025 14:42:00 INFO]: Training loss at epoch 8: 1.242875576019287
[08/27/2025 14:42:01 INFO]: Training loss at epoch 22: 1.3671300113201141
[08/27/2025 14:42:05 INFO]: Training loss at epoch 4: 0.9400044679641724
[08/27/2025 14:42:07 INFO]: Training loss at epoch 2: 0.8948515951633453
[08/27/2025 14:42:11 INFO]: Training loss at epoch 3: 1.892495036125183
[08/27/2025 14:42:17 INFO]: Training loss at epoch 23: 1.098588228225708
[08/27/2025 14:42:21 INFO]: Training loss at epoch 8: 0.7919139564037323
[08/27/2025 14:42:25 INFO]: New best epoch, val score: -0.8977923271098394
[08/27/2025 14:42:25 INFO]: Saving model to: maddest-Elbert_trial_10/model_best.pth
[08/27/2025 14:42:30 INFO]: Training loss at epoch 9: 0.934862345457077
[08/27/2025 14:42:33 INFO]: Training loss at epoch 24: 0.9061556756496429
[08/27/2025 14:42:34 INFO]: Training loss at epoch 8: 0.9523933231830597
[08/27/2025 14:42:40 INFO]: New best epoch, val score: -0.8918235282365543
[08/27/2025 14:42:40 INFO]: Saving model to: maddest-Elbert_trial_3/model_best.pth
[08/27/2025 14:42:43 INFO]: Training loss at epoch 9: 0.8583098649978638
[08/27/2025 14:42:47 INFO]: Training stats: {
    "score": -1.0016278086427468,
    "rmse": 1.0016278086427468
}
[08/27/2025 14:42:47 INFO]: Val stats: {
    "score": -0.9105323968815251,
    "rmse": 0.9105323968815251
}
[08/27/2025 14:42:47 INFO]: Test stats: {
    "score": -0.9709242136236172,
    "rmse": 0.9709242136236172
}
[08/27/2025 14:42:51 INFO]: Training loss at epoch 25: 1.0022547543048859
[08/27/2025 14:42:59 INFO]: Training stats: {
    "score": -1.0107401840703925,
    "rmse": 1.0107401840703925
}
[08/27/2025 14:42:59 INFO]: Val stats: {
    "score": -0.9797773489129226,
    "rmse": 0.9797773489129226
}
[08/27/2025 14:42:59 INFO]: Test stats: {
    "score": -0.9813743620439513,
    "rmse": 0.9813743620439513
}
[08/27/2025 14:43:08 INFO]: Training loss at epoch 26: 0.9965693950653076
[08/27/2025 14:43:09 INFO]: Training loss at epoch 3: 1.0897764563560486
[08/27/2025 14:43:10 INFO]: Training loss at epoch 9: 1.0824089646339417
[08/27/2025 14:43:11 INFO]: Training loss at epoch 1: 3.868102729320526
[08/27/2025 14:43:16 INFO]: Training loss at epoch 4: 1.003918707370758
[08/27/2025 14:43:24 INFO]: Training loss at epoch 9: 0.9121643304824829
[08/27/2025 14:43:25 INFO]: Training loss at epoch 27: 1.010052353143692
[08/27/2025 14:43:26 INFO]: Training stats: {
    "score": -0.9951182417221177,
    "rmse": 0.9951182417221177
}
[08/27/2025 14:43:26 INFO]: Val stats: {
    "score": -0.9354580146632975,
    "rmse": 0.9354580146632975
}
[08/27/2025 14:43:26 INFO]: Test stats: {
    "score": -0.973005049726185,
    "rmse": 0.973005049726185
}
[08/27/2025 14:43:27 INFO]: Training loss at epoch 5: 0.9998799264431
[08/27/2025 14:43:30 INFO]: Training loss at epoch 10: 1.074065238237381
[08/27/2025 14:43:39 INFO]: Training loss at epoch 1: 1.7545281052589417
[08/27/2025 14:43:42 INFO]: Training stats: {
    "score": -1.0039423010893562,
    "rmse": 1.0039423010893562
}
[08/27/2025 14:43:42 INFO]: Val stats: {
    "score": -0.9556426690911888,
    "rmse": 0.9556426690911888
}
[08/27/2025 14:43:42 INFO]: Test stats: {
    "score": -0.9781302931334721,
    "rmse": 0.9781302931334721
}
[08/27/2025 14:43:42 INFO]: New best epoch, val score: -0.9003587698846491
[08/27/2025 14:43:42 INFO]: Saving model to: maddest-Elbert_trial_9/model_best.pth
[08/27/2025 14:43:44 INFO]: Training loss at epoch 28: 0.9440729022026062
[08/27/2025 14:43:44 INFO]: Training loss at epoch 10: 0.9925165772438049
[08/27/2025 14:43:53 INFO]: Training loss at epoch 2: 1.6951188445091248
[08/27/2025 14:43:57 INFO]: Training loss at epoch 4: 1.5175836086273193
[08/27/2025 14:44:01 INFO]: Training loss at epoch 29: 0.8597227334976196
[08/27/2025 14:44:04 INFO]: Training loss at epoch 1: 0.973966121673584
[08/27/2025 14:44:08 INFO]: Training stats: {
    "score": -0.9901573515194011,
    "rmse": 0.9901573515194011
}
[08/27/2025 14:44:08 INFO]: Val stats: {
    "score": -0.9133645067889568,
    "rmse": 0.9133645067889568
}
[08/27/2025 14:44:08 INFO]: Test stats: {
    "score": -0.9654406101755196,
    "rmse": 0.9654406101755196
}
[08/27/2025 14:44:14 INFO]: Training loss at epoch 11: 1.0489830374717712
[08/27/2025 14:44:16 INFO]: Training loss at epoch 10: 0.9058812856674194
[08/27/2025 14:44:24 INFO]: Training loss at epoch 30: 1.3987461626529694
[08/27/2025 14:44:30 INFO]: Training loss at epoch 11: 0.9248433709144592
[08/27/2025 14:44:30 INFO]: Training loss at epoch 3: 1.2258398532867432
[08/27/2025 14:44:32 INFO]: Training loss at epoch 10: 0.9783793687820435
[08/27/2025 14:44:42 INFO]: Training loss at epoch 31: 1.0536108613014221
[08/27/2025 14:44:44 INFO]: Running Final Evaluation...
[08/27/2025 14:44:49 INFO]: Training accuracy: {
    "score": -1.047072894185873,
    "rmse": 1.047072894185873
}
[08/27/2025 14:44:49 INFO]: Val accuracy: {
    "score": -0.8791749079426427,
    "rmse": 0.8791749079426427
}
[08/27/2025 14:44:49 INFO]: Test accuracy: {
    "score": -1.009916705901517,
    "rmse": 1.009916705901517
}
[08/27/2025 14:44:49 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_20",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.009916705901517,
        "rmse": 1.009916705901517
    },
    "train_stats": {
        "score": -1.047072894185873,
        "rmse": 1.047072894185873
    },
    "val_stats": {
        "score": -0.8791749079426427,
        "rmse": 0.8791749079426427
    }
}
[08/27/2025 14:44:49 INFO]: Procewss finished for trial maddest-Elbert_trial_20
[08/27/2025 14:44:49 INFO]: 
_________________________________________________

[08/27/2025 14:44:49 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:44:49 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 0.7122500844292533
  attention_dropout: 0.46979377471901246
  ffn_dropout: 0.46979377471901246
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0002939216708908848
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_23

[08/27/2025 14:44:49 INFO]: This ft_transformer has 9.845 million parameters.
[08/27/2025 14:44:49 INFO]: Training will start at epoch 0.
[08/27/2025 14:44:49 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:44:52 INFO]: Training loss at epoch 5: 0.9524603188037872
[08/27/2025 14:44:53 INFO]: Training loss at epoch 6: 1.204420030117035
[08/27/2025 14:44:57 INFO]: Training loss at epoch 12: 1.1052422523498535
[08/27/2025 14:45:04 INFO]: Training loss at epoch 11: 0.9600828588008881
[08/27/2025 14:45:10 INFO]: Training loss at epoch 4: 0.9345575869083405
[08/27/2025 14:45:14 INFO]: Training loss at epoch 12: 0.9303797483444214
[08/27/2025 14:45:15 INFO]: Training loss at epoch 0: 1.3472905158996582
[08/27/2025 14:45:20 INFO]: Training loss at epoch 11: 1.2255149483680725
[08/27/2025 14:45:41 INFO]: Training loss at epoch 13: 1.0488686561584473
[08/27/2025 14:45:44 INFO]: Training loss at epoch 5: 1.1077245473861694
[08/27/2025 14:45:48 INFO]: Training loss at epoch 1: 3.620194911956787
[08/27/2025 14:45:51 INFO]: Training loss at epoch 12: 1.0503695011138916
[08/27/2025 14:45:58 INFO]: Training loss at epoch 13: 0.9567298591136932
[08/27/2025 14:46:10 INFO]: Training loss at epoch 12: 0.9918347895145416
[08/27/2025 14:46:12 INFO]: Training loss at epoch 1: 1.1533799171447754
[08/27/2025 14:46:17 INFO]: Training loss at epoch 7: 1.04985910654068
[08/27/2025 14:46:24 INFO]: Training loss at epoch 14: 0.9485728442668915
[08/27/2025 14:46:29 INFO]: Training loss at epoch 6: 0.8894798457622528
[08/27/2025 14:46:34 INFO]: New best epoch, val score: -0.8755655494103118
[08/27/2025 14:46:34 INFO]: Saving model to: maddest-Elbert_trial_19/model_best.pth
[08/27/2025 14:46:39 INFO]: Training loss at epoch 1: 1.508355438709259
[08/27/2025 14:46:40 INFO]: Training loss at epoch 13: 1.134812980890274
[08/27/2025 14:46:44 INFO]: Training loss at epoch 14: 0.8455139696598053
[08/27/2025 14:46:45 INFO]: Training loss at epoch 2: 1.2494037747383118
[08/27/2025 14:46:52 INFO]: Training loss at epoch 4: 1.352807879447937
[08/27/2025 14:46:52 INFO]: Training loss at epoch 3: 1.3091847598552704
[08/27/2025 14:46:58 INFO]: Training loss at epoch 13: 1.136056661605835
[08/27/2025 14:47:05 INFO]: New best epoch, val score: -0.8973195186545959
[08/27/2025 14:47:05 INFO]: Saving model to: maddest-Elbert_trial_21/model_best.pth
[08/27/2025 14:47:08 INFO]: Training loss at epoch 15: 1.2423249781131744
[08/27/2025 14:47:09 INFO]: Training loss at epoch 5: 1.197974681854248
[08/27/2025 14:47:12 INFO]: New best epoch, val score: -0.8963179546620915
[08/27/2025 14:47:12 INFO]: Saving model to: maddest-Elbert_trial_10/model_best.pth
[08/27/2025 14:47:21 INFO]: New best epoch, val score: -1.0120977170330312
[08/27/2025 14:47:21 INFO]: Saving model to: maddest-Elbert_trial_6/model_best.pth
[08/27/2025 14:47:24 INFO]: Training loss at epoch 2: 1.2491961121559143
[08/27/2025 14:47:24 INFO]: Training loss at epoch 0: 1.1670659184455872
[08/27/2025 14:47:26 INFO]: Training loss at epoch 14: 1.1675119400024414
[08/27/2025 14:47:28 INFO]: Training loss at epoch 15: 1.0159237384796143
[08/27/2025 14:47:31 INFO]: Training loss at epoch 6: 1.0749502182006836
[08/27/2025 14:47:42 INFO]: Training loss at epoch 8: 1.0577434301376343
[08/27/2025 14:47:49 INFO]: Training loss at epoch 14: 0.9521577656269073
[08/27/2025 14:47:52 INFO]: Training loss at epoch 16: 0.8332131803035736
[08/27/2025 14:48:07 INFO]: Training loss at epoch 7: 1.1028913259506226
[08/27/2025 14:48:10 INFO]: Training loss at epoch 2: 1.284694492816925
[08/27/2025 14:48:10 INFO]: New best epoch, val score: -1.0472251932138361
[08/27/2025 14:48:10 INFO]: Saving model to: maddest-Elbert_trial_22/model_best.pth
[08/27/2025 14:48:15 INFO]: Training loss at epoch 16: 0.9057373106479645
[08/27/2025 14:48:15 INFO]: Training loss at epoch 15: 1.120321124792099
[08/27/2025 14:48:37 INFO]: Training loss at epoch 17: 0.8488683104515076
[08/27/2025 14:48:39 INFO]: Training loss at epoch 15: 0.9477979838848114
[08/27/2025 14:49:00 INFO]: Training loss at epoch 17: 1.1211323738098145
[08/27/2025 14:49:04 INFO]: Training loss at epoch 16: 0.9448317885398865
[08/27/2025 14:49:05 INFO]: Training loss at epoch 0: 0.919399231672287
[08/27/2025 14:49:08 INFO]: Training loss at epoch 9: 1.1547017693519592
[08/27/2025 14:49:11 INFO]: Training loss at epoch 6: 0.9481524527072906
[08/27/2025 14:49:18 INFO]: Training loss at epoch 5: 1.0323799848556519
[08/27/2025 14:49:18 INFO]: Training loss at epoch 1: 1.0258506536483765
[08/27/2025 14:49:19 INFO]: Training loss at epoch 7: 1.158864140510559
[08/27/2025 14:49:22 INFO]: Training loss at epoch 18: 0.9517165124416351
[08/27/2025 14:49:26 INFO]: Training loss at epoch 3: 0.8791639804840088
[08/27/2025 14:49:29 INFO]: Training loss at epoch 16: 1.0619725584983826
[08/27/2025 14:49:37 INFO]: New best epoch, val score: -0.8951119958166074
[08/27/2025 14:49:37 INFO]: Saving model to: maddest-Elbert_trial_10/model_best.pth
[08/27/2025 14:49:38 INFO]: Training stats: {
    "score": -1.0195888879439297,
    "rmse": 1.0195888879439297
}
[08/27/2025 14:49:38 INFO]: Val stats: {
    "score": -0.8839559128566871,
    "rmse": 0.8839559128566871
}
[08/27/2025 14:49:38 INFO]: Test stats: {
    "score": -0.992200615246428,
    "rmse": 0.992200615246428
}
[08/27/2025 14:49:41 INFO]: New best epoch, val score: -1.039742724877976
[08/27/2025 14:49:41 INFO]: Saving model to: maddest-Elbert_trial_23/model_best.pth
[08/27/2025 14:49:46 INFO]: Training loss at epoch 8: 0.9298129081726074
[08/27/2025 14:49:47 INFO]: Training loss at epoch 18: 1.2177180349826813
[08/27/2025 14:49:53 INFO]: Training loss at epoch 17: 0.9987518191337585
[08/27/2025 14:49:55 INFO]: Training loss at epoch 4: 1.138877511024475
[08/27/2025 14:50:06 INFO]: Training loss at epoch 19: 1.1263937950134277
[08/27/2025 14:50:17 INFO]: New best epoch, val score: -0.9168035200755407
[08/27/2025 14:50:17 INFO]: Saving model to: maddest-Elbert_trial_8/model_best.pth
[08/27/2025 14:50:20 INFO]: Training loss at epoch 17: 1.0274795293807983
[08/27/2025 14:50:22 INFO]: Training stats: {
    "score": -1.0014172734779154,
    "rmse": 1.0014172734779154
}
[08/27/2025 14:50:22 INFO]: Val stats: {
    "score": -0.911435782292896,
    "rmse": 0.911435782292896
}
[08/27/2025 14:50:22 INFO]: Test stats: {
    "score": -0.9708272004858767,
    "rmse": 0.9708272004858767
}
[08/27/2025 14:50:32 INFO]: Training loss at epoch 19: 0.84754478931427
[08/27/2025 14:50:41 INFO]: Training loss at epoch 18: 0.9470360279083252
[08/27/2025 14:50:49 INFO]: Training stats: {
    "score": -0.9998014264884164,
    "rmse": 0.9998014264884164
}
[08/27/2025 14:50:49 INFO]: Val stats: {
    "score": -0.9106332555293745,
    "rmse": 0.9106332555293745
}
[08/27/2025 14:50:49 INFO]: Test stats: {
    "score": -0.9706331154007053,
    "rmse": 0.9706331154007053
}
[08/27/2025 14:51:05 INFO]: Training loss at epoch 10: 0.9416435956954956
[08/27/2025 14:51:06 INFO]: Training loss at epoch 20: 1.0416435301303864
[08/27/2025 14:51:07 INFO]: Training loss at epoch 8: 0.9510755240917206
[08/27/2025 14:51:10 INFO]: Training loss at epoch 18: 1.066098153591156
[08/27/2025 14:51:13 INFO]: Training loss at epoch 7: 0.9145249724388123
[08/27/2025 14:51:23 INFO]: Training loss at epoch 9: 1.1336301565170288
[08/27/2025 14:51:30 INFO]: Training loss at epoch 19: 1.0244517922401428
[08/27/2025 14:51:32 INFO]: Training loss at epoch 2: 1.8315849304199219
[08/27/2025 14:51:34 INFO]: Training loss at epoch 20: 1.0300132036209106
[08/27/2025 14:51:41 INFO]: Training loss at epoch 3: 0.9630217850208282
[08/27/2025 14:51:43 INFO]: Training loss at epoch 6: 1.2826838493347168
[08/27/2025 14:51:47 INFO]: Training stats: {
    "score": -0.9876910362252914,
    "rmse": 0.9876910362252914
}
[08/27/2025 14:51:47 INFO]: Val stats: {
    "score": -0.9353220916510192,
    "rmse": 0.9353220916510192
}
[08/27/2025 14:51:47 INFO]: Test stats: {
    "score": -0.9709452816844767,
    "rmse": 0.9709452816844767
}
[08/27/2025 14:51:49 INFO]: Training loss at epoch 21: 1.1620559096336365
[08/27/2025 14:51:59 INFO]: Training stats: {
    "score": -1.0003814383315306,
    "rmse": 1.0003814383315306
}
[08/27/2025 14:51:59 INFO]: Val stats: {
    "score": -0.9357066428150944,
    "rmse": 0.9357066428150944
}
[08/27/2025 14:51:59 INFO]: Test stats: {
    "score": -0.9740044077465276,
    "rmse": 0.9740044077465276
}
[08/27/2025 14:51:59 INFO]: Training loss at epoch 19: 1.0620396137237549
[08/27/2025 14:52:02 INFO]: New best epoch, val score: -0.8935500779779563
[08/27/2025 14:52:02 INFO]: Saving model to: maddest-Elbert_trial_10/model_best.pth
[08/27/2025 14:52:07 INFO]: Training loss at epoch 2: 0.9213788211345673
[08/27/2025 14:52:07 INFO]: Training loss at epoch 4: 1.2281032800674438
[08/27/2025 14:52:17 INFO]: Training stats: {
    "score": -1.0000745654497645,
    "rmse": 1.0000745654497645
}
[08/27/2025 14:52:17 INFO]: Val stats: {
    "score": -0.8988756857475154,
    "rmse": 0.8988756857475154
}
[08/27/2025 14:52:17 INFO]: Test stats: {
    "score": -0.9759823156111229,
    "rmse": 0.9759823156111229
}
[08/27/2025 14:52:20 INFO]: Training loss at epoch 21: 0.9269210696220398
[08/27/2025 14:52:30 INFO]: Training loss at epoch 11: 1.1536434292793274
[08/27/2025 14:52:33 INFO]: Training loss at epoch 22: 0.9166739284992218
[08/27/2025 14:52:36 INFO]: Training loss at epoch 20: 0.8875105381011963
[08/27/2025 14:52:43 INFO]: Training loss at epoch 3: 1.659337043762207
[08/27/2025 14:52:45 INFO]: Training loss at epoch 2: 1.2825902104377747
[08/27/2025 14:52:50 INFO]: New best epoch, val score: -0.9033435887563045
[08/27/2025 14:52:50 INFO]: Saving model to: maddest-Elbert_trial_1/model_best.pth
[08/27/2025 14:52:55 INFO]: Training loss at epoch 9: 1.1557475328445435
[08/27/2025 14:52:58 INFO]: Training loss at epoch 5: 0.9583516418933868
[08/27/2025 14:53:06 INFO]: Training loss at epoch 22: 1.0831491947174072
[08/27/2025 14:53:07 INFO]: Training loss at epoch 20: 0.888413667678833
[08/27/2025 14:53:13 INFO]: Training loss at epoch 8: 1.2638159692287445
[08/27/2025 14:53:15 INFO]: New best epoch, val score: -0.890980543980873
[08/27/2025 14:53:15 INFO]: Saving model to: maddest-Elbert_trial_5/model_best.pth
[08/27/2025 14:53:16 INFO]: Training loss at epoch 23: 0.8750326633453369
[08/27/2025 14:53:19 INFO]: New best epoch, val score: -0.9140025531459705
[08/27/2025 14:53:19 INFO]: Saving model to: maddest-Elbert_trial_8/model_best.pth
[08/27/2025 14:53:24 INFO]: Training loss at epoch 21: 0.9624212682247162
[08/27/2025 14:53:33 INFO]: Training stats: {
    "score": -0.9977096800181026,
    "rmse": 0.9977096800181026
}
[08/27/2025 14:53:33 INFO]: Val stats: {
    "score": -0.9181614780017927,
    "rmse": 0.9181614780017927
}
[08/27/2025 14:53:33 INFO]: Test stats: {
    "score": -0.9718763881481348,
    "rmse": 0.9718763881481348
}
[08/27/2025 14:53:37 INFO]: Training loss at epoch 10: 0.9472533464431763
[08/27/2025 14:53:50 INFO]: New best epoch, val score: -0.9256006706052048
[08/27/2025 14:53:51 INFO]: Saving model to: maddest-Elbert_trial_13/model_best.pth
[08/27/2025 14:53:53 INFO]: Training loss at epoch 23: 1.13617143034935
[08/27/2025 14:53:55 INFO]: Training loss at epoch 1: 1.769004762172699
[08/27/2025 14:53:56 INFO]: Training loss at epoch 12: 1.14912348985672
[08/27/2025 14:53:57 INFO]: Training loss at epoch 21: 1.2501455545425415
[08/27/2025 14:53:59 INFO]: Training loss at epoch 1: 1.9093022346496582
[08/27/2025 14:54:01 INFO]: Training loss at epoch 24: 1.118370771408081
[08/27/2025 14:54:07 INFO]: Training loss at epoch 7: 1.0660306513309479
[08/27/2025 14:54:13 INFO]: Training loss at epoch 22: 1.0641287863254547
[08/27/2025 14:54:26 INFO]: New best epoch, val score: -0.8917939889374817
[08/27/2025 14:54:26 INFO]: Saving model to: maddest-Elbert_trial_10/model_best.pth
[08/27/2025 14:54:32 INFO]: New best epoch, val score: -0.9091949674214289
[08/27/2025 14:54:32 INFO]: Saving model to: maddest-Elbert_trial_23/model_best.pth
[08/27/2025 14:54:38 INFO]: Training loss at epoch 24: 1.0142765939235687
[08/27/2025 14:54:45 INFO]: Training loss at epoch 25: 1.2320339679718018
[08/27/2025 14:54:46 INFO]: Training loss at epoch 22: 1.0065075159072876
[08/27/2025 14:54:48 INFO]: Training loss at epoch 5: 0.9296281039714813
[08/27/2025 14:55:00 INFO]: Training loss at epoch 23: 1.1206496357917786
[08/27/2025 14:55:16 INFO]: Training loss at epoch 11: 1.1988455653190613
[08/27/2025 14:55:16 INFO]: Training loss at epoch 9: 1.0018090605735779
[08/27/2025 14:55:21 INFO]: Training loss at epoch 13: 0.9478828012943268
[08/27/2025 14:55:24 INFO]: Training loss at epoch 10: 1.0051019489765167
[08/27/2025 14:55:26 INFO]: Training loss at epoch 25: 1.0073901414871216
[08/27/2025 14:55:28 INFO]: New best epoch, val score: -0.9178509842717376
[08/27/2025 14:55:28 INFO]: Saving model to: maddest-Elbert_trial_13/model_best.pth
[08/27/2025 14:55:28 INFO]: Training loss at epoch 26: 1.1012723743915558
[08/27/2025 14:55:35 INFO]: Training loss at epoch 23: 0.9813056290149689
[08/27/2025 14:55:48 INFO]: Training loss at epoch 24: 1.0089015066623688
[08/27/2025 14:55:58 INFO]: Training loss at epoch 4: 1.138388991355896
[08/27/2025 14:55:58 INFO]: Training loss at epoch 6: 1.0968214869499207
[08/27/2025 14:55:59 INFO]: Training stats: {
    "score": -1.0059168274516546,
    "rmse": 1.0059168274516546
}
[08/27/2025 14:55:59 INFO]: Val stats: {
    "score": -0.8959146590994824,
    "rmse": 0.8959146590994824
}
[08/27/2025 14:55:59 INFO]: Test stats: {
    "score": -0.9776178292629893,
    "rmse": 0.9776178292629893
}
[08/27/2025 14:56:12 INFO]: Training loss at epoch 27: 0.9204060137271881
[08/27/2025 14:56:12 INFO]: Training loss at epoch 26: 1.2405683100223541
[08/27/2025 14:56:21 INFO]: New best epoch, val score: -0.9116592218908507
[08/27/2025 14:56:21 INFO]: Saving model to: maddest-Elbert_trial_8/model_best.pth
[08/27/2025 14:56:25 INFO]: Training loss at epoch 24: 1.025371015071869
[08/27/2025 14:56:32 INFO]: Training loss at epoch 8: 1.1853496432304382
[08/27/2025 14:56:37 INFO]: Training loss at epoch 25: 0.8392150104045868
[08/27/2025 14:56:43 INFO]: Training loss at epoch 1: 3.7795572876930237
[08/27/2025 14:56:48 INFO]: Training loss at epoch 14: 0.8745633661746979
[08/27/2025 14:56:51 INFO]: New best epoch, val score: -0.8899993207379356
[08/27/2025 14:56:51 INFO]: Saving model to: maddest-Elbert_trial_10/model_best.pth
[08/27/2025 14:56:52 INFO]: Training loss at epoch 2: 1.3108936548233032
[08/27/2025 14:56:55 INFO]: Training loss at epoch 12: 1.2637318670749664
[08/27/2025 14:56:56 INFO]: Training loss at epoch 28: 1.0886921882629395
[08/27/2025 14:56:58 INFO]: Training loss at epoch 27: 0.8277535438537598
[08/27/2025 14:57:09 INFO]: New best epoch, val score: -0.9146614859575044
[08/27/2025 14:57:09 INFO]: Saving model to: maddest-Elbert_trial_13/model_best.pth
[08/27/2025 14:57:12 INFO]: Training loss at epoch 11: 0.9122046232223511
[08/27/2025 14:57:14 INFO]: Training loss at epoch 4: 1.0353314578533173
[08/27/2025 14:57:15 INFO]: Training loss at epoch 25: 1.1172228157520294
[08/27/2025 14:57:16 INFO]: Training loss at epoch 3: 1.5190885663032532
[08/27/2025 14:57:25 INFO]: Training loss at epoch 26: 1.0086480677127838
[08/27/2025 14:57:29 INFO]: Training loss at epoch 6: 1.040503442287445
[08/27/2025 14:57:40 INFO]: Training loss at epoch 29: 1.193484365940094
[08/27/2025 14:57:43 INFO]: Training loss at epoch 28: 1.1503199636936188
[08/27/2025 14:57:46 INFO]: New best epoch, val score: -0.8892642672035203
[08/27/2025 14:57:46 INFO]: Saving model to: maddest-Elbert_trial_3/model_best.pth
[08/27/2025 14:57:57 INFO]: Training stats: {
    "score": -1.0019493999120979,
    "rmse": 1.0019493999120979
}
[08/27/2025 14:57:57 INFO]: Val stats: {
    "score": -0.9088090456821052,
    "rmse": 0.9088090456821052
}
[08/27/2025 14:57:57 INFO]: Test stats: {
    "score": -0.9711418493003123,
    "rmse": 0.9711418493003123
}
[08/27/2025 14:57:57 INFO]: New best epoch, val score: -0.9121088752457959
[08/27/2025 14:57:57 INFO]: Saving model to: maddest-Elbert_trial_12/model_best.pth
[08/27/2025 14:58:01 INFO]: Training loss at epoch 10: 0.8410343527793884
[08/27/2025 14:58:01 INFO]: Training loss at epoch 3: 0.9370706677436829
[08/27/2025 14:58:05 INFO]: Training loss at epoch 26: 0.9838921129703522
[08/27/2025 14:58:13 INFO]: Training loss at epoch 27: 0.9846707880496979
[08/27/2025 14:58:13 INFO]: Training loss at epoch 15: 1.1190162301063538
[08/27/2025 14:58:29 INFO]: Training loss at epoch 29: 0.8517261743545532
[08/27/2025 14:58:36 INFO]: Training loss at epoch 13: 0.9330399036407471
[08/27/2025 14:58:41 INFO]: Training loss at epoch 30: 1.152466893196106
[08/27/2025 14:58:45 INFO]: Training stats: {
    "score": -0.9981278989509023,
    "rmse": 0.9981278989509023
}
[08/27/2025 14:58:45 INFO]: Val stats: {
    "score": -0.9269619271294762,
    "rmse": 0.9269619271294762
}
[08/27/2025 14:58:45 INFO]: Test stats: {
    "score": -0.968545458149282,
    "rmse": 0.968545458149282
}
[08/27/2025 14:58:49 INFO]: New best epoch, val score: -0.9130604303513087
[08/27/2025 14:58:49 INFO]: Saving model to: maddest-Elbert_trial_13/model_best.pth
[08/27/2025 14:58:49 INFO]: Training loss at epoch 3: 0.9829952716827393
[08/27/2025 14:58:50 INFO]: Training loss at epoch 2: 1.679488182067871
[08/27/2025 14:58:55 INFO]: Training loss at epoch 27: 0.915628969669342
[08/27/2025 14:58:57 INFO]: Training loss at epoch 9: 1.0782669186592102
[08/27/2025 14:58:59 INFO]: Training loss at epoch 12: 1.1692313253879547
[08/27/2025 14:59:01 INFO]: Training loss at epoch 7: 0.7521742880344391
[08/27/2025 14:59:02 INFO]: Training loss at epoch 28: 1.0355761647224426
[08/27/2025 14:59:23 INFO]: New best epoch, val score: -0.909973956514099
[08/27/2025 14:59:23 INFO]: Saving model to: maddest-Elbert_trial_8/model_best.pth
[08/27/2025 14:59:26 INFO]: Training loss at epoch 31: 1.1205635964870453
[08/27/2025 14:59:31 INFO]: Training loss at epoch 30: 0.9570777118206024
[08/27/2025 14:59:32 INFO]: New best epoch, val score: -0.9839081473142665
[08/27/2025 14:59:32 INFO]: Saving model to: maddest-Elbert_trial_6/model_best.pth
[08/27/2025 14:59:40 INFO]: Training loss at epoch 16: 1.053813099861145
[08/27/2025 14:59:45 INFO]: Training loss at epoch 28: 0.9297944009304047
[08/27/2025 14:59:50 INFO]: Training loss at epoch 29: 1.012528896331787
[08/27/2025 14:59:52 INFO]: Training stats: {
    "score": -1.0652709193023935,
    "rmse": 1.0652709193023935
}
[08/27/2025 14:59:52 INFO]: Val stats: {
    "score": -0.8882748621644647,
    "rmse": 0.8882748621644647
}
[08/27/2025 14:59:52 INFO]: Test stats: {
    "score": -1.0409338271098876,
    "rmse": 1.0409338271098876
}
[08/27/2025 15:00:01 INFO]: Training loss at epoch 11: 1.2019230425357819
[08/27/2025 15:00:07 INFO]: Training stats: {
    "score": -0.9750813316443168,
    "rmse": 0.9750813316443168
}
[08/27/2025 15:00:07 INFO]: Val stats: {
    "score": -0.9372902253785141,
    "rmse": 0.9372902253785141
}
[08/27/2025 15:00:07 INFO]: Test stats: {
    "score": -0.9697085798727428,
    "rmse": 0.9697085798727428
}
[08/27/2025 15:00:10 INFO]: Training loss at epoch 7: 1.179356873035431
[08/27/2025 15:00:10 INFO]: Training loss at epoch 32: 1.1608431935310364
[08/27/2025 15:00:11 INFO]: New best epoch, val score: -0.8882748621644647
[08/27/2025 15:00:11 INFO]: Saving model to: maddest-Elbert_trial_10/model_best.pth
[08/27/2025 15:00:13 INFO]: Training loss at epoch 5: 1.191258430480957
[08/27/2025 15:00:16 INFO]: Training loss at epoch 14: 0.8742478489875793
[08/27/2025 15:00:17 INFO]: Training loss at epoch 31: 0.8811323344707489
[08/27/2025 15:00:23 INFO]: Training loss at epoch 2: 1.6653559803962708
[08/27/2025 15:00:28 INFO]: New best epoch, val score: -0.9111836701883923
[08/27/2025 15:00:28 INFO]: Saving model to: maddest-Elbert_trial_13/model_best.pth
[08/27/2025 15:00:35 INFO]: Training loss at epoch 29: 1.1437605321407318
[08/27/2025 15:00:47 INFO]: Training loss at epoch 13: 0.9948727488517761
[08/27/2025 15:00:52 INFO]: Training stats: {
    "score": -0.9973993827001639,
    "rmse": 0.9973993827001639
}
[08/27/2025 15:00:52 INFO]: Val stats: {
    "score": -0.9389946463227523,
    "rmse": 0.9389946463227523
}
[08/27/2025 15:00:52 INFO]: Test stats: {
    "score": -0.9743627746060383,
    "rmse": 0.9743627746060383
}
[08/27/2025 15:00:54 INFO]: Training loss at epoch 33: 1.0378589034080505
[08/27/2025 15:00:56 INFO]: Training loss at epoch 30: 0.8441612124443054
[08/27/2025 15:01:00 INFO]: Running Final Evaluation...
[08/27/2025 15:01:02 INFO]: Training loss at epoch 32: 0.9793037474155426
[08/27/2025 15:01:07 INFO]: Training loss at epoch 17: 1.0405979752540588
[08/27/2025 15:01:08 INFO]: Running Final Evaluation...
[08/27/2025 15:01:16 INFO]: Training accuracy: {
    "score": -1.0080424866649256,
    "rmse": 1.0080424866649256
}
[08/27/2025 15:01:16 INFO]: Val accuracy: {
    "score": -0.8949093359549867,
    "rmse": 0.8949093359549867
}
[08/27/2025 15:01:16 INFO]: Test accuracy: {
    "score": -0.9779415650238906,
    "rmse": 0.9779415650238906
}
[08/27/2025 15:01:16 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_7",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9779415650238906,
        "rmse": 0.9779415650238906
    },
    "train_stats": {
        "score": -1.0080424866649256,
        "rmse": 1.0080424866649256
    },
    "val_stats": {
        "score": -0.8949093359549867,
        "rmse": 0.8949093359549867
    }
}
[08/27/2025 15:01:16 INFO]: Procewss finished for trial maddest-Elbert_trial_7
[08/27/2025 15:01:16 INFO]: 
_________________________________________________

[08/27/2025 15:01:16 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:01:16 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 0.954696764273128
  attention_dropout: 0.25535816934975175
  ffn_dropout: 0.25535816934975175
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.243661857389441e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_24

[08/27/2025 15:01:16 INFO]: This ft_transformer has 4.968 million parameters.
[08/27/2025 15:01:16 INFO]: Training will start at epoch 0.
[08/27/2025 15:01:16 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:01:23 INFO]: Training accuracy: {
    "score": -1.0457635792069402,
    "rmse": 1.0457635792069402
}
[08/27/2025 15:01:23 INFO]: Val accuracy: {
    "score": -0.8667804952027927,
    "rmse": 0.8667804952027927
}
[08/27/2025 15:01:23 INFO]: Test accuracy: {
    "score": -1.0108561754823826,
    "rmse": 1.0108561754823826
}
[08/27/2025 15:01:23 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_16",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0108561754823826,
        "rmse": 1.0108561754823826
    },
    "train_stats": {
        "score": -1.0457635792069402,
        "rmse": 1.0457635792069402
    },
    "val_stats": {
        "score": -0.8667804952027927,
        "rmse": 0.8667804952027927
    }
}
[08/27/2025 15:01:23 INFO]: Procewss finished for trial maddest-Elbert_trial_16
[08/27/2025 15:01:23 INFO]: 
_________________________________________________

[08/27/2025 15:01:23 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:01:23 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 2.6653769307958
  attention_dropout: 0.08582684906857746
  ffn_dropout: 0.08582684906857746
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.095115025818955e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_25

[08/27/2025 15:01:23 INFO]: This ft_transformer has 2.569 million parameters.
[08/27/2025 15:01:23 INFO]: Training will start at epoch 0.
[08/27/2025 15:01:23 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:01:42 INFO]: Training loss at epoch 30: 0.8943905234336853
[08/27/2025 15:01:44 INFO]: Training loss at epoch 31: 0.9895515143871307
[08/27/2025 15:01:46 INFO]: Training loss at epoch 5: 1.31083744764328
[08/27/2025 15:01:50 INFO]: Running Final Evaluation...
[08/27/2025 15:01:53 INFO]: Training loss at epoch 15: 1.0468225479125977
[08/27/2025 15:02:03 INFO]: Training loss at epoch 12: 0.8888502717018127
[08/27/2025 15:02:04 INFO]: Training loss at epoch 8: 0.8671129643917084
[08/27/2025 15:02:06 INFO]: New best epoch, val score: -0.9091681302093513
[08/27/2025 15:02:06 INFO]: Saving model to: maddest-Elbert_trial_13/model_best.pth
[08/27/2025 15:02:07 INFO]: Training accuracy: {
    "score": -1.035226920284623,
    "rmse": 1.035226920284623
}
[08/27/2025 15:02:07 INFO]: Val accuracy: {
    "score": -0.8787650193261191,
    "rmse": 0.8787650193261191
}
[08/27/2025 15:02:07 INFO]: Test accuracy: {
    "score": -1.011283225845782,
    "rmse": 1.011283225845782
}
[08/27/2025 15:02:07 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_4",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.011283225845782,
        "rmse": 1.011283225845782
    },
    "train_stats": {
        "score": -1.035226920284623,
        "rmse": 1.035226920284623
    },
    "val_stats": {
        "score": -0.8787650193261191,
        "rmse": 0.8787650193261191
    }
}
[08/27/2025 15:02:07 INFO]: Procewss finished for trial maddest-Elbert_trial_4
[08/27/2025 15:02:07 INFO]: 
_________________________________________________

[08/27/2025 15:02:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:02:07 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 0.9907119506844028
  attention_dropout: 0.11879352801674514
  ffn_dropout: 0.11879352801674514
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00045175985967171525
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_26

[08/27/2025 15:02:07 INFO]: This ft_transformer has 0.222 million parameters.
[08/27/2025 15:02:07 INFO]: Training will start at epoch 0.
[08/27/2025 15:02:07 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:02:12 INFO]: Training loss at epoch 0: 1.1553294658660889
[08/27/2025 15:02:15 INFO]: Training loss at epoch 10: 0.878724217414856
[08/27/2025 15:02:18 INFO]: New best epoch, val score: -1.1718273608375798
[08/27/2025 15:02:18 INFO]: Saving model to: maddest-Elbert_trial_25/model_best.pth
[08/27/2025 15:02:25 INFO]: New best epoch, val score: -0.9092263441659632
[08/27/2025 15:02:25 INFO]: Saving model to: maddest-Elbert_trial_8/model_best.pth
[08/27/2025 15:02:31 INFO]: Training loss at epoch 31: 1.2590195834636688
[08/27/2025 15:02:32 INFO]: Training loss at epoch 18: 0.9496731162071228
[08/27/2025 15:02:33 INFO]: Training loss at epoch 14: 1.3518125414848328
[08/27/2025 15:02:34 INFO]: New best epoch, val score: -0.8825362651559039
[08/27/2025 15:02:34 INFO]: Saving model to: maddest-Elbert_trial_10/model_best.pth
[08/27/2025 15:02:44 INFO]: Training loss at epoch 0: 0.8208829760551453
[08/27/2025 15:02:48 INFO]: Training loss at epoch 8: 1.0871285796165466
[08/27/2025 15:02:50 INFO]: New best epoch, val score: -0.9048863117069535
[08/27/2025 15:02:50 INFO]: Saving model to: maddest-Elbert_trial_26/model_best.pth
[08/27/2025 15:02:59 INFO]: Training loss at epoch 4: 0.961229145526886
[08/27/2025 15:03:08 INFO]: Training loss at epoch 1: 1.019746571779251
[08/27/2025 15:03:14 INFO]: New best epoch, val score: -1.029741858732129
[08/27/2025 15:03:14 INFO]: Saving model to: maddest-Elbert_trial_25/model_best.pth
[08/27/2025 15:03:20 INFO]: Training loss at epoch 32: 0.9964636862277985
[08/27/2025 15:03:28 INFO]: Training loss at epoch 1: 0.9067227244377136
[08/27/2025 15:03:33 INFO]: Training loss at epoch 16: 1.0527348518371582
[08/27/2025 15:03:42 INFO]: Training loss at epoch 3: 1.380858600139618
[08/27/2025 15:03:45 INFO]: New best epoch, val score: -0.9067919138956444
[08/27/2025 15:03:45 INFO]: Saving model to: maddest-Elbert_trial_13/model_best.pth
[08/27/2025 15:03:56 INFO]: Training loss at epoch 4: 0.9972143173217773
[08/27/2025 15:03:56 INFO]: Training loss at epoch 19: 1.3206851482391357
[08/27/2025 15:04:03 INFO]: Training loss at epoch 2: 1.1161556839942932
[08/27/2025 15:04:06 INFO]: Training loss at epoch 13: 0.9203534126281738
[08/27/2025 15:04:07 INFO]: Training loss at epoch 0: 1.0661364793777466
[08/27/2025 15:04:10 INFO]: New best epoch, val score: -0.9088954554430073
[08/27/2025 15:04:10 INFO]: Saving model to: maddest-Elbert_trial_25/model_best.pth
[08/27/2025 15:04:12 INFO]: Training loss at epoch 33: 1.2727218866348267
[08/27/2025 15:04:12 INFO]: Training loss at epoch 2: 1.1267122626304626
[08/27/2025 15:04:18 INFO]: Running Final Evaluation...
[08/27/2025 15:04:21 INFO]: Training loss at epoch 15: 0.9346505701541901
[08/27/2025 15:04:26 INFO]: Training stats: {
    "score": -1.047841188779543,
    "rmse": 1.047841188779543
}
[08/27/2025 15:04:26 INFO]: Val stats: {
    "score": -0.8818496046447598,
    "rmse": 0.8818496046447598
}
[08/27/2025 15:04:26 INFO]: Test stats: {
    "score": -1.0221292770916879,
    "rmse": 1.0221292770916879
}
[08/27/2025 15:04:28 INFO]: Training loss at epoch 6: 1.1885786652565002
[08/27/2025 15:04:30 INFO]: New best epoch, val score: -1.0827186052310307
[08/27/2025 15:04:30 INFO]: Saving model to: maddest-Elbert_trial_24/model_best.pth
[08/27/2025 15:04:31 INFO]: Training loss at epoch 3: 1.0904763340950012
[08/27/2025 15:04:36 INFO]: Training accuracy: {
    "score": -1.0519859301207435,
    "rmse": 1.0519859301207435
}
[08/27/2025 15:04:36 INFO]: Val accuracy: {
    "score": -0.8760096685992153,
    "rmse": 0.8760096685992153
}
[08/27/2025 15:04:36 INFO]: Test accuracy: {
    "score": -1.0271215380233898,
    "rmse": 1.0271215380233898
}
[08/27/2025 15:04:36 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_17",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0271215380233898,
        "rmse": 1.0271215380233898
    },
    "train_stats": {
        "score": -1.0519859301207435,
        "rmse": 1.0519859301207435
    },
    "val_stats": {
        "score": -0.8760096685992153,
        "rmse": 0.8760096685992153
    }
}
[08/27/2025 15:04:36 INFO]: Procewss finished for trial maddest-Elbert_trial_17
[08/27/2025 15:04:36 INFO]: 
_________________________________________________

[08/27/2025 15:04:36 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:04:36 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 2.39846088038313
  attention_dropout: 0.04855271713826509
  ffn_dropout: 0.04855271713826509
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00032353315491612266
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_27

[08/27/2025 15:04:36 INFO]: This ft_transformer has 7.934 million parameters.
[08/27/2025 15:04:36 INFO]: Training will start at epoch 0.
[08/27/2025 15:04:36 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:04:39 INFO]: New best epoch, val score: -0.8966135522462225
[08/27/2025 15:04:39 INFO]: Saving model to: maddest-Elbert_trial_1/model_best.pth
[08/27/2025 15:04:41 INFO]: Training loss at epoch 11: 0.9203179776668549
[08/27/2025 15:04:55 INFO]: Training loss at epoch 4: 1.4169070720672607
[08/27/2025 15:04:56 INFO]: Training loss at epoch 3: 1.07014599442482
[08/27/2025 15:04:59 INFO]: Training loss at epoch 3: 1.1957383453845978
[08/27/2025 15:05:02 INFO]: New best epoch, val score: -0.8891340967739878
[08/27/2025 15:05:02 INFO]: Saving model to: maddest-Elbert_trial_26/model_best.pth
[08/27/2025 15:05:06 INFO]: New best epoch, val score: -0.90157628211034
[08/27/2025 15:05:06 INFO]: Saving model to: maddest-Elbert_trial_25/model_best.pth
[08/27/2025 15:05:07 INFO]: Training loss at epoch 9: 1.2774843275547028
[08/27/2025 15:05:14 INFO]: Training loss at epoch 17: 1.0684662461280823
[08/27/2025 15:05:26 INFO]: New best epoch, val score: -0.885747934753033
[08/27/2025 15:05:26 INFO]: Saving model to: maddest-Elbert_trial_3/model_best.pth
[08/27/2025 15:05:27 INFO]: New best epoch, val score: -0.9054567723775586
[08/27/2025 15:05:27 INFO]: Saving model to: maddest-Elbert_trial_13/model_best.pth
[08/27/2025 15:05:30 INFO]: Training loss at epoch 9: 0.9576300084590912
[08/27/2025 15:05:39 INFO]: Training loss at epoch 4: 0.9002558887004852
[08/27/2025 15:05:53 INFO]: Training loss at epoch 20: 0.9432953000068665
[08/27/2025 15:05:56 INFO]: Training loss at epoch 4: 1.035878986120224
[08/27/2025 15:06:08 INFO]: Training loss at epoch 14: 0.9864068925380707
[08/27/2025 15:06:09 INFO]: Training stats: {
    "score": -0.9997081093340555,
    "rmse": 0.9997081093340555
}
[08/27/2025 15:06:09 INFO]: Val stats: {
    "score": -0.9093480591695426,
    "rmse": 0.9093480591695426
}
[08/27/2025 15:06:09 INFO]: Test stats: {
    "score": -0.9726231038838029,
    "rmse": 0.9726231038838029
}
[08/27/2025 15:06:09 INFO]: Training loss at epoch 16: 1.026144027709961
[08/27/2025 15:06:20 INFO]: Training loss at epoch 6: 1.3081082701683044
[08/27/2025 15:06:21 INFO]: Training loss at epoch 5: 1.0264923572540283
[08/27/2025 15:06:28 INFO]: Training stats: {
    "score": -1.0211429607303957,
    "rmse": 1.0211429607303957
}
[08/27/2025 15:06:28 INFO]: Val stats: {
    "score": -1.0064406636853354,
    "rmse": 1.0064406636853354
}
[08/27/2025 15:06:28 INFO]: Test stats: {
    "score": -0.9948897918843284,
    "rmse": 0.9948897918843284
}
[08/27/2025 15:06:51 INFO]: Training loss at epoch 3: 1.4080703854560852
[08/27/2025 15:06:52 INFO]: Training loss at epoch 5: 1.164364516735077
[08/27/2025 15:06:52 INFO]: Training loss at epoch 18: 1.104794442653656
[08/27/2025 15:07:02 INFO]: Training loss at epoch 6: 0.856955349445343
[08/27/2025 15:07:07 INFO]: Training loss at epoch 12: 0.9679971039295197
[08/27/2025 15:07:18 INFO]: Training loss at epoch 21: 1.0658726692199707
[08/27/2025 15:07:21 INFO]: Training loss at epoch 1: 0.901255875825882
[08/27/2025 15:07:44 INFO]: Training loss at epoch 7: 1.122592031955719
[08/27/2025 15:07:44 INFO]: New best epoch, val score: -0.9364638811459977
[08/27/2025 15:07:44 INFO]: Saving model to: maddest-Elbert_trial_24/model_best.pth
[08/27/2025 15:07:47 INFO]: Training loss at epoch 6: 1.1975664496421814
[08/27/2025 15:07:57 INFO]: Training loss at epoch 17: 1.0239616632461548
[08/27/2025 15:08:09 INFO]: Training loss at epoch 15: 1.015071541070938
[08/27/2025 15:08:11 INFO]: Training loss at epoch 2: 1.7174289226531982
[08/27/2025 15:08:26 INFO]: Training loss at epoch 8: 1.1609993875026703
[08/27/2025 15:08:29 INFO]: Training loss at epoch 19: 0.9712061285972595
[08/27/2025 15:08:37 INFO]: Training loss at epoch 4: 1.088024765253067
[08/27/2025 15:08:43 INFO]: Training loss at epoch 7: 1.4192197918891907
[08/27/2025 15:08:44 INFO]: Training loss at epoch 7: 1.2617884278297424
[08/27/2025 15:08:44 INFO]: Training loss at epoch 5: 1.4132747054100037
[08/27/2025 15:08:44 INFO]: Training loss at epoch 22: 0.9226938188076019
[08/27/2025 15:09:02 INFO]: Training loss at epoch 0: 1.033738374710083
[08/27/2025 15:09:04 INFO]: Training stats: {
    "score": -1.0015486852346736,
    "rmse": 1.0015486852346736
}
[08/27/2025 15:09:04 INFO]: Val stats: {
    "score": -0.9083479579330733,
    "rmse": 0.9083479579330733
}
[08/27/2025 15:09:04 INFO]: Test stats: {
    "score": -0.9756011138793035,
    "rmse": 0.9756011138793035
}
[08/27/2025 15:09:07 INFO]: Training loss at epoch 9: 0.9339381456375122
[08/27/2025 15:09:07 INFO]: Training loss at epoch 10: 1.0803855657577515
[08/27/2025 15:09:11 INFO]: Training loss at epoch 10: 1.0353314280509949
[08/27/2025 15:09:22 INFO]: Training stats: {
    "score": -1.0046685394039683,
    "rmse": 1.0046685394039683
}
[08/27/2025 15:09:22 INFO]: Val stats: {
    "score": -0.8973586470677752,
    "rmse": 0.8973586470677752
}
[08/27/2025 15:09:22 INFO]: Test stats: {
    "score": -0.9760975154817516,
    "rmse": 0.9760975154817516
}
[08/27/2025 15:09:33 INFO]: Training loss at epoch 13: 0.8856510818004608
[08/27/2025 15:09:37 INFO]: New best epoch, val score: -0.9143698289633247
[08/27/2025 15:09:37 INFO]: Saving model to: maddest-Elbert_trial_27/model_best.pth
[08/27/2025 15:09:39 INFO]: Training loss at epoch 8: 1.0149915218353271
[08/27/2025 15:09:44 INFO]: Training loss at epoch 18: 1.1762695908546448
[08/27/2025 15:09:53 INFO]: Training loss at epoch 5: 0.9057000577449799
[08/27/2025 15:10:05 INFO]: Training loss at epoch 10: 1.129386693239212
[08/27/2025 15:10:09 INFO]: Training loss at epoch 23: 1.0826178193092346
[08/27/2025 15:10:10 INFO]: Training loss at epoch 16: 1.2053788900375366
[08/27/2025 15:10:34 INFO]: Training loss at epoch 2: 1.3591840863227844
[08/27/2025 15:10:34 INFO]: Training loss at epoch 9: 1.3901231288909912
[08/27/2025 15:10:34 INFO]: New best epoch, val score: -0.8965249151091988
[08/27/2025 15:10:34 INFO]: Saving model to: maddest-Elbert_trial_1/model_best.pth
[08/27/2025 15:10:40 INFO]: Training loss at epoch 20: 0.9310977756977081
[08/27/2025 15:10:48 INFO]: Training loss at epoch 11: 1.255359023809433
[08/27/2025 15:10:50 INFO]: Training loss at epoch 7: 1.0558544397354126
[08/27/2025 15:10:53 INFO]: Training stats: {
    "score": -1.0877247876502438,
    "rmse": 1.0877247876502438
}
[08/27/2025 15:10:53 INFO]: Val stats: {
    "score": -0.9016763643179774,
    "rmse": 0.9016763643179774
}
[08/27/2025 15:10:53 INFO]: Test stats: {
    "score": -1.0684816396817436,
    "rmse": 1.0684816396817436
}
[08/27/2025 15:10:57 INFO]: New best epoch, val score: -0.8878258623205881
[08/27/2025 15:10:57 INFO]: Saving model to: maddest-Elbert_trial_24/model_best.pth
[08/27/2025 15:11:00 INFO]: Training loss at epoch 5: 1.421840786933899
[08/27/2025 15:11:30 INFO]: Training loss at epoch 12: 0.9381781220436096
[08/27/2025 15:11:31 INFO]: Training loss at epoch 19: 1.1874775290489197
[08/27/2025 15:11:34 INFO]: Training loss at epoch 24: 1.0069307684898376
[08/27/2025 15:11:47 INFO]: Training loss at epoch 11: 1.196365088224411
[08/27/2025 15:11:49 INFO]: Training loss at epoch 10: 1.2578712105751038
[08/27/2025 15:11:55 INFO]: New best epoch, val score: -0.8905571305219377
[08/27/2025 15:11:55 INFO]: Saving model to: maddest-Elbert_trial_25/model_best.pth
[08/27/2025 15:11:57 INFO]: Training loss at epoch 14: 0.9395216107368469
[08/27/2025 15:12:04 INFO]: Training loss at epoch 4: 1.0930467247962952
[08/27/2025 15:12:07 INFO]: New best epoch, val score: -0.8880958012049212
[08/27/2025 15:12:07 INFO]: Saving model to: maddest-Elbert_trial_21/model_best.pth
[08/27/2025 15:12:09 INFO]: Training stats: {
    "score": -0.998463106819584,
    "rmse": 0.998463106819584
}
[08/27/2025 15:12:09 INFO]: Val stats: {
    "score": -0.9115832054770042,
    "rmse": 0.9115832054770042
}
[08/27/2025 15:12:09 INFO]: Test stats: {
    "score": -0.9713522833650292,
    "rmse": 0.9713522833650292
}
[08/27/2025 15:12:12 INFO]: Training loss at epoch 17: 1.0379459857940674
[08/27/2025 15:12:12 INFO]: Training loss at epoch 13: 0.9027194678783417
[08/27/2025 15:12:13 INFO]: Training loss at epoch 11: 0.8853299617767334
[08/27/2025 15:12:17 INFO]: Training loss at epoch 21: 0.9335839152336121
[08/27/2025 15:12:45 INFO]: Training loss at epoch 11: 1.2760335803031921
[08/27/2025 15:12:51 INFO]: New best epoch, val score: -0.8898995444374165
[08/27/2025 15:12:51 INFO]: Saving model to: maddest-Elbert_trial_25/model_best.pth
[08/27/2025 15:12:53 INFO]: Training loss at epoch 14: 0.8629672229290009
[08/27/2025 15:12:57 INFO]: Training loss at epoch 8: 1.0333408415317535
[08/27/2025 15:12:58 INFO]: Training loss at epoch 25: 0.9769082963466644
[08/27/2025 15:13:18 INFO]: Training loss at epoch 4: 1.479312002658844
[08/27/2025 15:13:27 INFO]: Training loss at epoch 5: 0.9833667874336243
[08/27/2025 15:13:35 INFO]: Training loss at epoch 15: 1.0047516822814941
[08/27/2025 15:13:41 INFO]: Training loss at epoch 12: 1.2036800980567932
[08/27/2025 15:13:47 INFO]: Training loss at epoch 3: 1.1575081944465637
[08/27/2025 15:13:54 INFO]: Training loss at epoch 22: 1.108858048915863
[08/27/2025 15:13:57 INFO]: Training loss at epoch 20: 1.2782001495361328
[08/27/2025 15:14:02 INFO]: Training loss at epoch 1: 2.510679066181183
[08/27/2025 15:14:04 INFO]: New best epoch, val score: -0.8855995358326071
[08/27/2025 15:14:04 INFO]: Saving model to: maddest-Elbert_trial_22/model_best.pth
[08/27/2025 15:14:12 INFO]: Training loss at epoch 18: 1.039595514535904
[08/27/2025 15:14:17 INFO]: Training loss at epoch 16: 0.9669183194637299
[08/27/2025 15:14:21 INFO]: Training loss at epoch 15: 1.0974396467208862
[08/27/2025 15:14:25 INFO]: Training loss at epoch 26: 1.0524885654449463
[08/27/2025 15:14:26 INFO]: Training loss at epoch 6: 1.0115387439727783
[08/27/2025 15:14:27 INFO]: Training loss at epoch 12: 1.0450449585914612
[08/27/2025 15:14:36 INFO]: Training loss at epoch 13: 0.9577907025814056
[08/27/2025 15:14:48 INFO]: New best epoch, val score: -0.882541487958959
[08/27/2025 15:14:48 INFO]: Saving model to: maddest-Elbert_trial_21/model_best.pth
[08/27/2025 15:14:59 INFO]: Training loss at epoch 17: 0.8914314210414886
[08/27/2025 15:15:07 INFO]: New best epoch, val score: -0.9102715769650367
[08/27/2025 15:15:07 INFO]: Saving model to: maddest-Elbert_trial_12/model_best.pth
[08/27/2025 15:15:16 INFO]: Training loss at epoch 12: 0.9989139437675476
[08/27/2025 15:15:22 INFO]: Training loss at epoch 8: 0.9383687078952789
[08/27/2025 15:15:31 INFO]: Training loss at epoch 23: 1.106730341911316
[08/27/2025 15:15:32 INFO]: Training loss at epoch 14: 0.8852353692054749
[08/27/2025 15:15:40 INFO]: Training loss at epoch 18: 1.1771020889282227
[08/27/2025 15:15:45 INFO]: Training loss at epoch 21: 1.2039649784564972
[08/27/2025 15:15:47 INFO]: Training loss at epoch 6: 0.9875122904777527
[08/27/2025 15:15:51 INFO]: Training loss at epoch 27: 1.1024273037910461
[08/27/2025 15:15:55 INFO]: New best epoch, val score: -0.8878340423876274
[08/27/2025 15:15:55 INFO]: Saving model to: maddest-Elbert_trial_5/model_best.pth
[08/27/2025 15:16:14 INFO]: Training loss at epoch 19: 1.0628444254398346
[08/27/2025 15:16:22 INFO]: Training loss at epoch 19: 1.005297064781189
[08/27/2025 15:16:28 INFO]: Training loss at epoch 15: 0.9664580523967743
[08/27/2025 15:16:28 INFO]: New best epoch, val score: -0.8963414097104664
[08/27/2025 15:16:28 INFO]: Saving model to: maddest-Elbert_trial_1/model_best.pth
[08/27/2025 15:16:37 INFO]: Training stats: {
    "score": -1.009238574406731,
    "rmse": 1.009238574406731
}
[08/27/2025 15:16:37 INFO]: Val stats: {
    "score": -0.8904150771619149,
    "rmse": 0.8904150771619149
}
[08/27/2025 15:16:37 INFO]: Test stats: {
    "score": -0.981468183397861,
    "rmse": 0.981468183397861
}
[08/27/2025 15:16:45 INFO]: Training loss at epoch 16: 1.142581820487976
[08/27/2025 15:16:58 INFO]: Training stats: {
    "score": -1.002694131190662,
    "rmse": 1.002694131190662
}
[08/27/2025 15:16:58 INFO]: Val stats: {
    "score": -0.9027526339827607,
    "rmse": 0.9027526339827607
}
[08/27/2025 15:16:58 INFO]: Test stats: {
    "score": -0.9744220277456923,
    "rmse": 0.9744220277456923
}
[08/27/2025 15:17:01 INFO]: Training loss at epoch 4: 1.2585532069206238
[08/27/2025 15:17:04 INFO]: Training loss at epoch 6: 1.3241207599639893
[08/27/2025 15:17:07 INFO]: Training loss at epoch 13: 0.9051062166690826
[08/27/2025 15:17:09 INFO]: Training loss at epoch 24: 1.0162522196769714
[08/27/2025 15:17:15 INFO]: Training loss at epoch 9: 1.3119983673095703
[08/27/2025 15:17:16 INFO]: Training loss at epoch 28: 1.13841712474823
[08/27/2025 15:17:19 INFO]: Training loss at epoch 20: 0.9809935986995697
[08/27/2025 15:17:24 INFO]: Training loss at epoch 16: 0.9103485643863678
[08/27/2025 15:17:31 INFO]: Training loss at epoch 22: 0.9268053472042084
[08/27/2025 15:18:00 INFO]: Training loss at epoch 21: 0.8197962641716003
[08/27/2025 15:18:16 INFO]: Training loss at epoch 13: 1.1561920642852783
[08/27/2025 15:18:19 INFO]: Training loss at epoch 6: 0.8344323039054871
[08/27/2025 15:18:20 INFO]: Training loss at epoch 17: 1.1847837567329407
[08/27/2025 15:18:40 INFO]: Training loss at epoch 29: 0.8864336907863617
[08/27/2025 15:18:41 INFO]: Training loss at epoch 22: 0.999345988035202
[08/27/2025 15:18:42 INFO]: Training stats: {
    "score": -1.1068120763358964,
    "rmse": 1.1068120763358964
}
[08/27/2025 15:18:42 INFO]: Val stats: {
    "score": -0.9029725066226161,
    "rmse": 0.9029725066226161
}
[08/27/2025 15:18:42 INFO]: Test stats: {
    "score": -1.0798839374263256,
    "rmse": 1.0798839374263256
}
[08/27/2025 15:18:44 INFO]: Training loss at epoch 25: 1.0102537274360657
[08/27/2025 15:18:58 INFO]: Training loss at epoch 20: 0.9129943251609802
[08/27/2025 15:19:00 INFO]: Training loss at epoch 2: 1.8552753925323486
[08/27/2025 15:19:08 INFO]: Training loss at epoch 17: 1.0663304328918457
[08/27/2025 15:19:09 INFO]: Training stats: {
    "score": -1.0016987372323187,
    "rmse": 1.0016987372323187
}
[08/27/2025 15:19:09 INFO]: Val stats: {
    "score": -0.8981058340737478,
    "rmse": 0.8981058340737478
}
[08/27/2025 15:19:09 INFO]: Test stats: {
    "score": -0.9754570811340061,
    "rmse": 0.9754570811340061
}
[08/27/2025 15:19:17 INFO]: Training loss at epoch 23: 0.9411671161651611
[08/27/2025 15:19:18 INFO]: Training loss at epoch 18: 1.2164146304130554
[08/27/2025 15:19:22 INFO]: Training loss at epoch 23: 0.8968445658683777
[08/27/2025 15:19:37 INFO]: Training loss at epoch 3: 1.2639720439910889
[08/27/2025 15:19:42 INFO]: Training loss at epoch 5: 0.845678448677063
[08/27/2025 15:19:45 INFO]: Training loss at epoch 14: 1.1056552529335022
[08/27/2025 15:19:46 INFO]: Training loss at epoch 5: 1.274017721414566
[08/27/2025 15:19:52 INFO]: Training loss at epoch 9: 1.0046771168708801
[08/27/2025 15:20:04 INFO]: Training loss at epoch 24: 1.0146217346191406
[08/27/2025 15:20:08 INFO]: Training loss at epoch 7: 1.2517420649528503
[08/27/2025 15:20:12 INFO]: Training loss at epoch 5: 1.3057767748832703
[08/27/2025 15:20:13 INFO]: Training loss at epoch 19: 0.8355978727340698
[08/27/2025 15:20:20 INFO]: Training loss at epoch 26: 1.1692014336585999
[08/27/2025 15:20:32 INFO]: New best epoch, val score: -0.884421541212074
[08/27/2025 15:20:32 INFO]: Saving model to: maddest-Elbert_trial_22/model_best.pth
[08/27/2025 15:20:32 INFO]: Training stats: {
    "score": -1.0174145464022284,
    "rmse": 1.0174145464022284
}
[08/27/2025 15:20:32 INFO]: Val stats: {
    "score": -0.998520809317759,
    "rmse": 0.998520809317759
}
[08/27/2025 15:20:32 INFO]: Test stats: {
    "score": -0.9954087154830575,
    "rmse": 0.9954087154830575
}
[08/27/2025 15:20:36 INFO]: Training loss at epoch 30: 1.0239959359169006
[08/27/2025 15:20:46 INFO]: Training loss at epoch 25: 0.87611523270607
[08/27/2025 15:20:51 INFO]: New best epoch, val score: -0.9024692360752127
[08/27/2025 15:20:51 INFO]: Saving model to: maddest-Elbert_trial_12/model_best.pth
[08/27/2025 15:21:02 INFO]: Training loss at epoch 21: 0.9923348724842072
[08/27/2025 15:21:05 INFO]: Training loss at epoch 24: 1.1088817715644836
[08/27/2025 15:21:17 INFO]: Training loss at epoch 14: 0.8440253436565399
[08/27/2025 15:21:28 INFO]: Training loss at epoch 26: 0.9131152033805847
[08/27/2025 15:21:29 INFO]: Training stats: {
    "score": -1.0611543485656119,
    "rmse": 1.0611543485656119
}
[08/27/2025 15:21:29 INFO]: Val stats: {
    "score": -0.8848242827804477,
    "rmse": 0.8848242827804477
}
[08/27/2025 15:21:29 INFO]: Test stats: {
    "score": -1.0332760079341805,
    "rmse": 1.0332760079341805
}
[08/27/2025 15:21:29 INFO]: Training loss at epoch 20: 0.8597276210784912
[08/27/2025 15:21:32 INFO]: Training loss at epoch 18: 0.8163046836853027
[08/27/2025 15:21:38 INFO]: Training loss at epoch 7: 0.7555991858243942
[08/27/2025 15:21:58 INFO]: Training loss at epoch 27: 0.8649890422821045
[08/27/2025 15:22:01 INFO]: Training loss at epoch 31: 0.9081993401050568
[08/27/2025 15:22:01 INFO]: New best epoch, val score: -0.8848242827804477
[08/27/2025 15:22:01 INFO]: Saving model to: maddest-Elbert_trial_5/model_best.pth
[08/27/2025 15:22:09 INFO]: Training loss at epoch 27: 0.901982456445694
[08/27/2025 15:22:24 INFO]: Training loss at epoch 21: 0.975090742111206
[08/27/2025 15:22:25 INFO]: Training loss at epoch 15: 0.9284549653530121
[08/27/2025 15:22:51 INFO]: Training loss at epoch 28: 1.1276068687438965
[08/27/2025 15:22:51 INFO]: Training loss at epoch 25: 0.9357475638389587
[08/27/2025 15:22:57 INFO]: Training loss at epoch 10: 1.2682524919509888
[08/27/2025 15:23:02 INFO]: Training loss at epoch 22: 1.1354919373989105
[08/27/2025 15:23:05 INFO]: Training loss at epoch 7: 1.158474624156952
[08/27/2025 15:23:09 INFO]: Training loss at epoch 7: 1.0368353724479675
[08/27/2025 15:23:19 INFO]: Training loss at epoch 22: 1.1448915600776672
[08/27/2025 15:23:25 INFO]: Training loss at epoch 6: 1.2021719813346863
[08/27/2025 15:23:25 INFO]: Training loss at epoch 32: 1.0488040447235107
[08/27/2025 15:23:33 INFO]: Training loss at epoch 29: 0.9234844446182251
[08/27/2025 15:23:34 INFO]: Training loss at epoch 28: 0.8123853504657745
[08/27/2025 15:23:36 INFO]: Running Final Evaluation...
[08/27/2025 15:23:49 INFO]: Training stats: {
    "score": -1.0105833463873424,
    "rmse": 1.0105833463873424
}
[08/27/2025 15:23:49 INFO]: Val stats: {
    "score": -0.980575902412761,
    "rmse": 0.980575902412761
}
[08/27/2025 15:23:49 INFO]: Test stats: {
    "score": -0.9840059348843145,
    "rmse": 0.9840059348843145
}
[08/27/2025 15:23:55 INFO]: Training loss at epoch 19: 1.1207800507545471
[08/27/2025 15:23:59 INFO]: Training loss at epoch 3: 1.166793406009674
[08/27/2025 15:24:06 INFO]: Training accuracy: {
    "score": -1.0481687368408743,
    "rmse": 1.0481687368408743
}
[08/27/2025 15:24:06 INFO]: Val accuracy: {
    "score": -0.8802363047789352,
    "rmse": 0.8802363047789352
}
[08/27/2025 15:24:06 INFO]: Test accuracy: {
    "score": -1.0212678309025383,
    "rmse": 1.0212678309025383
}
[08/27/2025 15:24:06 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_15",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0212678309025383,
        "rmse": 1.0212678309025383
    },
    "train_stats": {
        "score": -1.0481687368408743,
        "rmse": 1.0481687368408743
    },
    "val_stats": {
        "score": -0.8802363047789352,
        "rmse": 0.8802363047789352
    }
}
[08/27/2025 15:24:06 INFO]: Procewss finished for trial maddest-Elbert_trial_15
[08/27/2025 15:24:06 INFO]: 
_________________________________________________

[08/27/2025 15:24:06 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:24:06 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 1.5048690906507587
  attention_dropout: 0.35087130518854615
  ffn_dropout: 0.35087130518854615
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.28471882167045e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_28

[08/27/2025 15:24:06 INFO]: This ft_transformer has 0.738 million parameters.
[08/27/2025 15:24:06 INFO]: Training will start at epoch 0.
[08/27/2025 15:24:06 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:24:15 INFO]: Training loss at epoch 23: 0.9611520767211914
[08/27/2025 15:24:18 INFO]: Training loss at epoch 15: 0.9966829121112823
[08/27/2025 15:24:31 INFO]: Training loss at epoch 30: 0.895959883928299
[08/27/2025 15:24:39 INFO]: Training loss at epoch 26: 1.05959814786911
[08/27/2025 15:24:41 INFO]: New best epoch, val score: -0.9019337830578225
[08/27/2025 15:24:41 INFO]: Saving model to: maddest-Elbert_trial_8/model_best.pth
[08/27/2025 15:24:52 INFO]: Training loss at epoch 0: 1.0115308165550232
[08/27/2025 15:24:53 INFO]: Training stats: {
    "score": -1.0058625041021152,
    "rmse": 1.0058625041021152
}
[08/27/2025 15:24:53 INFO]: Val stats: {
    "score": -0.9719995388863318,
    "rmse": 0.9719995388863318
}
[08/27/2025 15:24:53 INFO]: Test stats: {
    "score": -0.9824689786248261,
    "rmse": 0.9824689786248261
}
[08/27/2025 15:24:59 INFO]: New best epoch, val score: -0.9023344144667546
[08/27/2025 15:24:59 INFO]: Saving model to: maddest-Elbert_trial_28/model_best.pth
[08/27/2025 15:25:06 INFO]: Training loss at epoch 23: 1.0142283737659454
[08/27/2025 15:25:07 INFO]: Training loss at epoch 16: 0.8740566372871399
[08/27/2025 15:25:12 INFO]: Training loss at epoch 24: 0.8913733065128326
[08/27/2025 15:25:13 INFO]: Training loss at epoch 29: 1.088245928287506
[08/27/2025 15:25:15 INFO]: Training loss at epoch 31: 0.9810982346534729
[08/27/2025 15:25:42 INFO]: Training loss at epoch 1: 1.0509763360023499
[08/27/2025 15:25:49 INFO]: Training stats: {
    "score": -1.0000930872934013,
    "rmse": 1.0000930872934013
}
[08/27/2025 15:25:49 INFO]: Val stats: {
    "score": -0.9387300107511312,
    "rmse": 0.9387300107511312
}
[08/27/2025 15:25:49 INFO]: Test stats: {
    "score": -0.9742126730803956,
    "rmse": 0.9742126730803956
}
[08/27/2025 15:25:55 INFO]: Training loss at epoch 8: 1.1691044569015503
[08/27/2025 15:25:56 INFO]: Training loss at epoch 32: 1.0493075847625732
[08/27/2025 15:26:02 INFO]: Training loss at epoch 10: 1.258444905281067
[08/27/2025 15:26:07 INFO]: Training loss at epoch 25: 1.0132509768009186
[08/27/2025 15:26:18 INFO]: Training loss at epoch 6: 1.226902723312378
[08/27/2025 15:26:29 INFO]: Training loss at epoch 27: 1.1819913983345032
[08/27/2025 15:26:32 INFO]: Training loss at epoch 2: 1.0835569500923157
[08/27/2025 15:26:35 INFO]: New best epoch, val score: -0.8948761974122565
[08/27/2025 15:26:35 INFO]: Saving model to: maddest-Elbert_trial_12/model_best.pth
[08/27/2025 15:26:37 INFO]: Training loss at epoch 33: 1.0527039766311646
[08/27/2025 15:26:40 INFO]: Training loss at epoch 7: 1.1020217835903168
[08/27/2025 15:27:03 INFO]: Training loss at epoch 26: 1.0099606812000275
[08/27/2025 15:27:03 INFO]: New best epoch, val score: -0.8840744915787672
[08/27/2025 15:27:03 INFO]: Saving model to: maddest-Elbert_trial_22/model_best.pth
[08/27/2025 15:27:06 INFO]: Training loss at epoch 24: 0.995537519454956
[08/27/2025 15:27:13 INFO]: Training loss at epoch 11: 0.8651340901851654
[08/27/2025 15:27:17 INFO]: Training loss at epoch 20: 1.199611783027649
[08/27/2025 15:27:18 INFO]: Training loss at epoch 6: 0.98507159948349
[08/27/2025 15:27:20 INFO]: Training loss at epoch 34: 1.1700224578380585
[08/27/2025 15:27:23 INFO]: Training loss at epoch 3: 1.1751580238342285
[08/27/2025 15:27:23 INFO]: Training loss at epoch 16: 0.9555786848068237
[08/27/2025 15:27:25 INFO]: Training loss at epoch 30: 1.1398643255233765
[08/27/2025 15:27:25 INFO]: Running Final Evaluation...
[08/27/2025 15:27:34 INFO]: Training loss at epoch 8: 0.9607839584350586
[08/27/2025 15:27:40 INFO]: Training accuracy: {
    "score": -1.0117861699850839,
    "rmse": 1.0117861699850839
}
[08/27/2025 15:27:40 INFO]: Val accuracy: {
    "score": -0.8891340967739878,
    "rmse": 0.8891340967739878
}
[08/27/2025 15:27:40 INFO]: Test accuracy: {
    "score": -0.9835336492206671,
    "rmse": 0.9835336492206671
}
[08/27/2025 15:27:40 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_26",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9835336492206671,
        "rmse": 0.9835336492206671
    },
    "train_stats": {
        "score": -1.0117861699850839,
        "rmse": 1.0117861699850839
    },
    "val_stats": {
        "score": -0.8891340967739878,
        "rmse": 0.8891340967739878
    }
}
[08/27/2025 15:27:40 INFO]: Procewss finished for trial maddest-Elbert_trial_26
[08/27/2025 15:27:40 INFO]: 
_________________________________________________

[08/27/2025 15:27:40 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:27:41 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 0.9690653428803887
  attention_dropout: 0.4391302314388495
  ffn_dropout: 0.4391302314388495
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 7.250091963859801e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_29

[08/27/2025 15:27:41 INFO]: This ft_transformer has 0.633 million parameters.
[08/27/2025 15:27:41 INFO]: Training will start at epoch 0.
[08/27/2025 15:27:41 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:27:43 INFO]: New best epoch, val score: -0.8873491504444999
[08/27/2025 15:27:43 INFO]: Saving model to: maddest-Elbert_trial_8/model_best.pth
[08/27/2025 15:27:46 INFO]: Training loss at epoch 17: 1.0474666953086853
[08/27/2025 15:27:57 INFO]: Training loss at epoch 27: 1.3784805238246918
[08/27/2025 15:28:01 INFO]: Training loss at epoch 8: 1.119472622871399
[08/27/2025 15:28:12 INFO]: Training loss at epoch 4: 0.9881424009799957
[08/27/2025 15:28:14 INFO]: Training loss at epoch 28: 0.9052716493606567
[08/27/2025 15:28:54 INFO]: Training loss at epoch 28: 0.8818091452121735
[08/27/2025 15:28:58 INFO]: Training loss at epoch 4: 1.3868627548217773
[08/27/2025 15:29:01 INFO]: Training loss at epoch 31: 1.0117871761322021
[08/27/2025 15:29:02 INFO]: Training loss at epoch 5: 0.9605291783809662
[08/27/2025 15:29:06 INFO]: Training loss at epoch 25: 1.0755091905593872
[08/27/2025 15:29:10 INFO]: Training loss at epoch 8: 1.4890915155410767
[08/27/2025 15:29:13 INFO]: Training loss at epoch 0: 1.1462249159812927
[08/27/2025 15:29:27 INFO]: New best epoch, val score: -0.9554606633384627
[08/27/2025 15:29:27 INFO]: Saving model to: maddest-Elbert_trial_29/model_best.pth
[08/27/2025 15:29:40 INFO]: Training loss at epoch 21: 0.9117392003536224
[08/27/2025 15:29:48 INFO]: Training loss at epoch 29: 1.045745313167572
[08/27/2025 15:29:51 INFO]: Training loss at epoch 6: 1.0058797895908356
[08/27/2025 15:29:52 INFO]: Training loss at epoch 8: 1.4117398858070374
[08/27/2025 15:30:01 INFO]: Training loss at epoch 29: 0.9665765166282654
[08/27/2025 15:30:07 INFO]: Training stats: {
    "score": -1.000548552182664,
    "rmse": 1.000548552182664
}
[08/27/2025 15:30:07 INFO]: Val stats: {
    "score": -0.9147315567363346,
    "rmse": 0.9147315567363346
}
[08/27/2025 15:30:07 INFO]: Test stats: {
    "score": -0.9802472849990744,
    "rmse": 0.9802472849990744
}
[08/27/2025 15:30:24 INFO]: Training loss at epoch 17: 0.9607507884502411
[08/27/2025 15:30:27 INFO]: Training loss at epoch 18: 0.996838241815567
[08/27/2025 15:30:32 INFO]: Training loss at epoch 11: 1.071751594543457
[08/27/2025 15:30:36 INFO]: Training loss at epoch 32: 1.2996217012405396
[08/27/2025 15:30:39 INFO]: Training stats: {
    "score": -1.005210034561623,
    "rmse": 1.005210034561623
}
[08/27/2025 15:30:39 INFO]: Val stats: {
    "score": -0.9686803965128381,
    "rmse": 0.9686803965128381
}
[08/27/2025 15:30:39 INFO]: Test stats: {
    "score": -0.9798250582764403,
    "rmse": 0.9798250582764403
}
[08/27/2025 15:30:41 INFO]: Training loss at epoch 7: 0.8148406744003296
[08/27/2025 15:31:01 INFO]: Training loss at epoch 1: 0.9364670217037201
[08/27/2025 15:31:03 INFO]: Training loss at epoch 30: 0.9313068389892578
[08/27/2025 15:31:07 INFO]: Training loss at epoch 26: 0.9369376003742218
[08/27/2025 15:31:07 INFO]: Training loss at epoch 4: 1.3521344661712646
[08/27/2025 15:31:25 INFO]: Training loss at epoch 12: 1.4461406469345093
[08/27/2025 15:31:31 INFO]: Training loss at epoch 8: 0.9711908400058746
[08/27/2025 15:31:36 INFO]: Training loss at epoch 9: 1.0585777163505554
[08/27/2025 15:31:59 INFO]: Training loss at epoch 31: 0.9103333353996277
[08/27/2025 15:32:05 INFO]: Training loss at epoch 22: 0.9462045431137085
[08/27/2025 15:32:14 INFO]: Training loss at epoch 33: 0.8913754820823669
[08/27/2025 15:32:21 INFO]: Training loss at epoch 9: 1.2190876603126526
[08/27/2025 15:32:26 INFO]: Training loss at epoch 30: 0.9268021285533905
[08/27/2025 15:32:39 INFO]: Training stats: {
    "score": -1.018632959939009,
    "rmse": 1.018632959939009
}
[08/27/2025 15:32:39 INFO]: Val stats: {
    "score": -0.9970411983751494,
    "rmse": 0.9970411983751494
}
[08/27/2025 15:32:39 INFO]: Test stats: {
    "score": -0.9878276795979806,
    "rmse": 0.9878276795979806
}
[08/27/2025 15:32:46 INFO]: Training loss at epoch 7: 1.1699577867984772
[08/27/2025 15:32:49 INFO]: Training loss at epoch 2: 0.8936404585838318
[08/27/2025 15:32:53 INFO]: Training loss at epoch 9: 0.9949785470962524
[08/27/2025 15:32:54 INFO]: Training loss at epoch 32: 1.3020554184913635
[08/27/2025 15:33:03 INFO]: New best epoch, val score: -0.947437114040741
[08/27/2025 15:33:03 INFO]: Saving model to: maddest-Elbert_trial_29/model_best.pth
[08/27/2025 15:33:06 INFO]: Training loss at epoch 19: 0.9445099532604218
[08/27/2025 15:33:07 INFO]: Training loss at epoch 27: 0.8621606230735779
[08/27/2025 15:33:08 INFO]: Training loss at epoch 9: 1.333884835243225
[08/27/2025 15:33:24 INFO]: Training loss at epoch 9: 0.7917594909667969
[08/27/2025 15:33:25 INFO]: Training loss at epoch 18: 1.0864899158477783
[08/27/2025 15:33:28 INFO]: Training loss at epoch 10: 1.0801199674606323
[08/27/2025 15:33:30 INFO]: New best epoch, val score: -0.8839596122912184
[08/27/2025 15:33:30 INFO]: Saving model to: maddest-Elbert_trial_22/model_best.pth
[08/27/2025 15:33:36 INFO]: Training stats: {
    "score": -1.0765652707805315,
    "rmse": 1.0765652707805315
}
[08/27/2025 15:33:36 INFO]: Val stats: {
    "score": -0.8884423389346224,
    "rmse": 0.8884423389346224
}
[08/27/2025 15:33:36 INFO]: Test stats: {
    "score": -1.048253564351607,
    "rmse": 1.048253564351607
}
[08/27/2025 15:33:51 INFO]: Training loss at epoch 33: 1.0791941583156586
[08/27/2025 15:33:51 INFO]: Training loss at epoch 34: 0.9453670084476471
[08/27/2025 15:33:56 INFO]: Training loss at epoch 5: 1.2557751834392548
[08/27/2025 15:34:06 INFO]: Training stats: {
    "score": -1.0014919931323811,
    "rmse": 1.0014919931323811
}
[08/27/2025 15:34:06 INFO]: Val stats: {
    "score": -0.9472199899708109,
    "rmse": 0.9472199899708109
}
[08/27/2025 15:34:06 INFO]: Test stats: {
    "score": -0.9744352779322594,
    "rmse": 0.9744352779322594
}
[08/27/2025 15:34:14 INFO]: Training loss at epoch 31: 0.9646872878074646
[08/27/2025 15:34:15 INFO]: Training stats: {
    "score": -1.117499121557056,
    "rmse": 1.117499121557056
}
[08/27/2025 15:34:15 INFO]: Val stats: {
    "score": -1.172885825141314,
    "rmse": 1.172885825141314
}
[08/27/2025 15:34:15 INFO]: Test stats: {
    "score": -1.0917054126239318,
    "rmse": 1.0917054126239318
}
[08/27/2025 15:34:17 INFO]: New best epoch, val score: -0.8884423389346224
[08/27/2025 15:34:17 INFO]: Saving model to: maddest-Elbert_trial_12/model_best.pth
[08/27/2025 15:34:19 INFO]: Training loss at epoch 11: 1.079249620437622
[08/27/2025 15:34:29 INFO]: Training loss at epoch 23: 0.9659116864204407
[08/27/2025 15:34:33 INFO]: Training stats: {
    "score": -1.0053051343830237,
    "rmse": 1.0053051343830237
}
[08/27/2025 15:34:33 INFO]: Val stats: {
    "score": -0.9674790480059484,
    "rmse": 0.9674790480059484
}
[08/27/2025 15:34:33 INFO]: Test stats: {
    "score": -0.9782139731185281,
    "rmse": 0.9782139731185281
}
[08/27/2025 15:34:36 INFO]: Training loss at epoch 3: 1.1361344456672668
[08/27/2025 15:34:46 INFO]: Training loss at epoch 34: 1.059183418750763
[08/27/2025 15:34:47 INFO]: Training loss at epoch 7: 1.1153725981712341
[08/27/2025 15:34:49 INFO]: New best epoch, val score: -0.9125559119123767
[08/27/2025 15:34:49 INFO]: Saving model to: maddest-Elbert_trial_29/model_best.pth
[08/27/2025 15:35:04 INFO]: Training loss at epoch 12: 1.2217776775360107
[08/27/2025 15:35:08 INFO]: Training loss at epoch 28: 1.0884197354316711
[08/27/2025 15:35:09 INFO]: Training loss at epoch 12: 0.9339345097541809
[08/27/2025 15:35:15 INFO]: Training loss at epoch 9: 1.4614629745483398
[08/27/2025 15:35:27 INFO]: Training stats: {
    "score": -0.9690210542395122,
    "rmse": 0.9690210542395122
}
[08/27/2025 15:35:27 INFO]: Val stats: {
    "score": -0.9030422869256306,
    "rmse": 0.9030422869256306
}
[08/27/2025 15:35:27 INFO]: Test stats: {
    "score": -0.9827902309005159,
    "rmse": 0.9827902309005159
}
[08/27/2025 15:35:28 INFO]: Training loss at epoch 35: 1.0422351360321045
[08/27/2025 15:35:38 INFO]: Training loss at epoch 13: 1.801843285560608
[08/27/2025 15:35:41 INFO]: Training loss at epoch 35: 1.0189789235591888
[08/27/2025 15:35:59 INFO]: Training loss at epoch 13: 0.761515274643898
[08/27/2025 15:36:02 INFO]: Training loss at epoch 32: 1.230314463376999
[08/27/2025 15:36:16 INFO]: Running Final Evaluation...
[08/27/2025 15:36:23 INFO]: Training loss at epoch 4: 1.0966681838035583
[08/27/2025 15:36:29 INFO]: Training loss at epoch 19: 1.0738557875156403
[08/27/2025 15:36:38 INFO]: Training loss at epoch 36: 1.0083874464035034
[08/27/2025 15:36:48 INFO]: Training loss at epoch 20: 1.1513399481773376
[08/27/2025 15:36:49 INFO]: Training loss at epoch 14: 1.2034350633621216
[08/27/2025 15:36:53 INFO]: Training loss at epoch 24: 0.9607905447483063
[08/27/2025 15:36:57 INFO]: Training accuracy: {
    "score": -1.0577066295892614,
    "rmse": 1.0577066295892614
}
[08/27/2025 15:36:57 INFO]: Val accuracy: {
    "score": -0.8813504885993805,
    "rmse": 0.8813504885993805
}
[08/27/2025 15:36:57 INFO]: Test accuracy: {
    "score": -1.0330060848201394,
    "rmse": 1.0330060848201394
}
[08/27/2025 15:36:57 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_18",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0330060848201394,
        "rmse": 1.0330060848201394
    },
    "train_stats": {
        "score": -1.0577066295892614,
        "rmse": 1.0577066295892614
    },
    "val_stats": {
        "score": -0.8813504885993805,
        "rmse": 0.8813504885993805
    }
}
[08/27/2025 15:36:57 INFO]: Procewss finished for trial maddest-Elbert_trial_18
[08/27/2025 15:36:57 INFO]: 
_________________________________________________

[08/27/2025 15:36:57 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:36:57 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 16
  d_ffn_factor: 2.6016417252633994
  attention_dropout: 0.20163582520117848
  ffn_dropout: 0.20163582520117848
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00020628407132751883
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_30

[08/27/2025 15:36:58 INFO]: This ft_transformer has 12.233 million parameters.
[08/27/2025 15:36:58 INFO]: Training will start at epoch 0.
[08/27/2025 15:36:58 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:37:05 INFO]: Training loss at epoch 36: 0.8318913578987122
[08/27/2025 15:37:09 INFO]: Training loss at epoch 29: 0.9137222766876221
[08/27/2025 15:37:18 INFO]: Training stats: {
    "score": -1.087188735499611,
    "rmse": 1.087188735499611
}
[08/27/2025 15:37:18 INFO]: Val stats: {
    "score": -1.1258865447209714,
    "rmse": 1.1258865447209714
}
[08/27/2025 15:37:18 INFO]: Test stats: {
    "score": -1.0620408433557984,
    "rmse": 1.0620408433557984
}
[08/27/2025 15:37:28 INFO]: Training loss at epoch 10: 1.083691567182541
[08/27/2025 15:37:30 INFO]: Training stats: {
    "score": -0.9961216739039309,
    "rmse": 0.9961216739039309
}
[08/27/2025 15:37:30 INFO]: Val stats: {
    "score": -0.9527169780446514,
    "rmse": 0.9527169780446514
}
[08/27/2025 15:37:30 INFO]: Test stats: {
    "score": -0.9765772849246892,
    "rmse": 0.9765772849246892
}
[08/27/2025 15:37:33 INFO]: Training loss at epoch 37: 0.8400568962097168
[08/27/2025 15:37:38 INFO]: Training loss at epoch 15: 0.8784395754337311
[08/27/2025 15:37:52 INFO]: Training stats: {
    "score": -1.0046510001753002,
    "rmse": 1.0046510001753002
}
[08/27/2025 15:37:52 INFO]: Val stats: {
    "score": -0.8980498914140627,
    "rmse": 0.8980498914140627
}
[08/27/2025 15:37:52 INFO]: Test stats: {
    "score": -0.9764813903656681,
    "rmse": 0.9764813903656681
}
[08/27/2025 15:38:10 INFO]: Training loss at epoch 5: 0.861022412776947
[08/27/2025 15:38:27 INFO]: Training loss at epoch 16: 1.0710933208465576
[08/27/2025 15:38:29 INFO]: Training loss at epoch 38: 0.8519685566425323
[08/27/2025 15:38:41 INFO]: Training loss at epoch 37: 0.8583864867687225
[08/27/2025 15:38:53 INFO]: Training loss at epoch 6: 1.0549018383026123
[08/27/2025 15:39:14 INFO]: Training loss at epoch 8: 1.0464434623718262
[08/27/2025 15:39:18 INFO]: Training loss at epoch 10: 1.0525097250938416
[08/27/2025 15:39:18 INFO]: Training loss at epoch 25: 1.163975477218628
[08/27/2025 15:39:18 INFO]: Training loss at epoch 17: 1.1351112723350525
[08/27/2025 15:39:23 INFO]: Training loss at epoch 10: 1.0889559388160706
[08/27/2025 15:39:25 INFO]: Training loss at epoch 39: 0.94210284948349
[08/27/2025 15:39:27 INFO]: Training loss at epoch 21: 1.1458089351654053
[08/27/2025 15:39:36 INFO]: Training loss at epoch 13: 0.9936538934707642
[08/27/2025 15:39:44 INFO]: Training stats: {
    "score": -0.9973125972448305,
    "rmse": 0.9973125972448305
}
[08/27/2025 15:39:44 INFO]: Val stats: {
    "score": -0.9320390832712885,
    "rmse": 0.9320390832712885
}
[08/27/2025 15:39:44 INFO]: Test stats: {
    "score": -0.9780128556308774,
    "rmse": 0.9780128556308774
}
[08/27/2025 15:39:51 INFO]: Training loss at epoch 14: 0.963660329580307
[08/27/2025 15:39:56 INFO]: Training loss at epoch 6: 1.187547743320465
[08/27/2025 15:39:56 INFO]: Training loss at epoch 30: 0.8707488179206848
[08/27/2025 15:39:58 INFO]: New best epoch, val score: -0.8857029938736597
[08/27/2025 15:39:58 INFO]: Saving model to: maddest-Elbert_trial_23/model_best.pth
[08/27/2025 15:40:00 INFO]: New best epoch, val score: -0.8838108133381241
[08/27/2025 15:40:00 INFO]: Saving model to: maddest-Elbert_trial_22/model_best.pth
[08/27/2025 15:40:08 INFO]: Training loss at epoch 18: 1.041584849357605
[08/27/2025 15:40:17 INFO]: Training loss at epoch 38: 1.024304986000061
[08/27/2025 15:40:32 INFO]: Training loss at epoch 20: 1.106468826532364
[08/27/2025 15:40:39 INFO]: Training loss at epoch 40: 1.2262301445007324
[08/27/2025 15:40:41 INFO]: Training loss at epoch 11: 0.8748191297054291
[08/27/2025 15:40:58 INFO]: Training loss at epoch 19: 1.1324244737625122
[08/27/2025 15:41:16 INFO]: Training stats: {
    "score": -1.0009167661019513,
    "rmse": 1.0009167661019513
}
[08/27/2025 15:41:16 INFO]: Val stats: {
    "score": -0.9381879995331567,
    "rmse": 0.9381879995331567
}
[08/27/2025 15:41:16 INFO]: Test stats: {
    "score": -0.9704178118642152,
    "rmse": 0.9704178118642152
}
[08/27/2025 15:41:19 INFO]: Training loss at epoch 10: 0.8070855736732483
[08/27/2025 15:41:35 INFO]: Training loss at epoch 41: 1.0124182403087616
[08/27/2025 15:41:44 INFO]: Training loss at epoch 7: 1.2582800090312958
[08/27/2025 15:41:45 INFO]: Training loss at epoch 26: 0.9996252059936523
[08/27/2025 15:41:55 INFO]: Training loss at epoch 39: 1.0945362448692322
[08/27/2025 15:42:02 INFO]: Training loss at epoch 31: 1.2306199073791504
[08/27/2025 15:42:06 INFO]: Training loss at epoch 20: 1.0988949537277222
[08/27/2025 15:42:07 INFO]: Training loss at epoch 22: 0.9312935173511505
[08/27/2025 15:42:19 INFO]: Training loss at epoch 8: 0.8648678064346313
[08/27/2025 15:42:30 INFO]: Training stats: {
    "score": -0.9990510363873747,
    "rmse": 0.9990510363873747
}
[08/27/2025 15:42:30 INFO]: Val stats: {
    "score": -0.9195789422202774,
    "rmse": 0.9195789422202774
}
[08/27/2025 15:42:30 INFO]: Test stats: {
    "score": -0.9733025740542054,
    "rmse": 0.9733025740542054
}
[08/27/2025 15:42:31 INFO]: Training loss at epoch 42: 0.9135361313819885
[08/27/2025 15:42:38 INFO]: Running Final Evaluation...
[08/27/2025 15:42:38 INFO]: Training loss at epoch 5: 0.9589870870113373
[08/27/2025 15:42:55 INFO]: Training loss at epoch 21: 0.9269459545612335
[08/27/2025 15:42:56 INFO]: Training accuracy: {
    "score": -1.0270697249362506,
    "rmse": 1.0270697249362506
}
[08/27/2025 15:42:56 INFO]: Val accuracy: {
    "score": -0.8898995444374165,
    "rmse": 0.8898995444374165
}
[08/27/2025 15:42:56 INFO]: Test accuracy: {
    "score": -1.0062298626545607,
    "rmse": 1.0062298626545607
}
[08/27/2025 15:42:56 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_25",
    "best_epoch": 11,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0062298626545607,
        "rmse": 1.0062298626545607
    },
    "train_stats": {
        "score": -1.0270697249362506,
        "rmse": 1.0270697249362506
    },
    "val_stats": {
        "score": -0.8898995444374165,
        "rmse": 0.8898995444374165
    }
}
[08/27/2025 15:42:56 INFO]: Procewss finished for trial maddest-Elbert_trial_25
[08/27/2025 15:42:56 INFO]: 
_________________________________________________

[08/27/2025 15:42:56 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:42:56 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 16
  d_ffn_factor: 1.244365652346866
  attention_dropout: 0.19614014585529083
  ffn_dropout: 0.19614014585529083
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.061923401422387e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_31

[08/27/2025 15:42:57 INFO]: This ft_transformer has 9.726 million parameters.
[08/27/2025 15:42:57 INFO]: Training will start at epoch 0.
[08/27/2025 15:42:57 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:43:22 INFO]: Training loss at epoch 10: 1.1943168640136719
[08/27/2025 15:43:30 INFO]: Training loss at epoch 8: 1.1005164980888367
[08/27/2025 15:43:35 INFO]: Training loss at epoch 21: 0.987400084733963
[08/27/2025 15:43:45 INFO]: Training loss at epoch 22: 0.9217479526996613
[08/27/2025 15:43:53 INFO]: Training loss at epoch 7: 1.0684438943862915
[08/27/2025 15:43:55 INFO]: Training loss at epoch 12: 0.8772404789924622
[08/27/2025 15:44:05 INFO]: New best epoch, val score: -0.8896622839123348
[08/27/2025 15:44:05 INFO]: Saving model to: maddest-Elbert_trial_6/model_best.pth
[08/27/2025 15:44:07 INFO]: Training loss at epoch 32: 1.0179250836372375
[08/27/2025 15:44:08 INFO]: Training loss at epoch 15: 1.0727009177207947
[08/27/2025 15:44:08 INFO]: Training loss at epoch 14: 1.1925508379936218
[08/27/2025 15:44:08 INFO]: Training loss at epoch 40: 1.2001448571681976
[08/27/2025 15:44:10 INFO]: Training loss at epoch 27: 0.9138256907463074
[08/27/2025 15:44:17 INFO]: New best epoch, val score: -0.8829077344575449
[08/27/2025 15:44:17 INFO]: Saving model to: maddest-Elbert_trial_24/model_best.pth
[08/27/2025 15:44:20 INFO]: Training loss at epoch 11: 1.1010572910308838
[08/27/2025 15:44:24 INFO]: Running Final Evaluation...
[08/27/2025 15:44:30 INFO]: Training loss at epoch 0: 1.3916133046150208
[08/27/2025 15:44:35 INFO]: Training loss at epoch 23: 0.8647228181362152
[08/27/2025 15:44:46 INFO]: Training loss at epoch 23: 0.9691474437713623
[08/27/2025 15:44:54 INFO]: New best epoch, val score: -0.8826948197553446
[08/27/2025 15:44:54 INFO]: Saving model to: maddest-Elbert_trial_23/model_best.pth
[08/27/2025 15:44:59 INFO]: Training loss at epoch 11: 1.1001102924346924
[08/27/2025 15:45:13 INFO]: Training accuracy: {
    "score": -1.0621370118842555,
    "rmse": 1.0621370118842555
}
[08/27/2025 15:45:13 INFO]: Val accuracy: {
    "score": -0.884301360797299,
    "rmse": 0.884301360797299
}
[08/27/2025 15:45:13 INFO]: Test accuracy: {
    "score": -1.036707167965922,
    "rmse": 1.036707167965922
}
[08/27/2025 15:45:13 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_14",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.036707167965922,
        "rmse": 1.036707167965922
    },
    "train_stats": {
        "score": -1.0621370118842555,
        "rmse": 1.0621370118842555
    },
    "val_stats": {
        "score": -0.884301360797299,
        "rmse": 0.884301360797299
    }
}
[08/27/2025 15:45:13 INFO]: Procewss finished for trial maddest-Elbert_trial_14
[08/27/2025 15:45:13 INFO]: 
_________________________________________________

[08/27/2025 15:45:13 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:45:13 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 1.2229959533458323
  attention_dropout: 0.001741911785101552
  ffn_dropout: 0.001741911785101552
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.3222701690988166e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_32

[08/27/2025 15:45:13 INFO]: This ft_transformer has 5.849 million parameters.
[08/27/2025 15:45:13 INFO]: Training will start at epoch 0.
[08/27/2025 15:45:13 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:45:15 INFO]: Training loss at epoch 9: 1.0101224184036255
[08/27/2025 15:45:25 INFO]: Training loss at epoch 24: 1.0416272282600403
[08/27/2025 15:45:31 INFO]: New best epoch, val score: -1.1211111570684527
[08/27/2025 15:45:31 INFO]: Saving model to: maddest-Elbert_trial_30/model_best.pth
[08/27/2025 15:45:44 INFO]: Training loss at epoch 9: 0.9949668347835541
[08/27/2025 15:45:46 INFO]: Training loss at epoch 41: 1.0501574277877808
[08/27/2025 15:45:55 INFO]: Training stats: {
    "score": -0.9977342817398657,
    "rmse": 0.9977342817398657
}
[08/27/2025 15:45:55 INFO]: Val stats: {
    "score": -0.931015496186736,
    "rmse": 0.931015496186736
}
[08/27/2025 15:45:55 INFO]: Test stats: {
    "score": -0.972556492929049,
    "rmse": 0.972556492929049
}
[08/27/2025 15:46:15 INFO]: Training loss at epoch 25: 0.8999809920787811
[08/27/2025 15:46:36 INFO]: Training loss at epoch 28: 0.9063829779624939
[08/27/2025 15:46:37 INFO]: Training loss at epoch 22: 0.8735062777996063
[08/27/2025 15:47:05 INFO]: Training loss at epoch 26: 0.8345086574554443
[08/27/2025 15:47:09 INFO]: Training loss at epoch 13: 1.1843811869621277
[08/27/2025 15:47:13 INFO]: Training loss at epoch 11: 1.0773118734359741
[08/27/2025 15:47:24 INFO]: Training loss at epoch 42: 0.8608492314815521
[08/27/2025 15:47:26 INFO]: Training loss at epoch 24: 0.9775626957416534
[08/27/2025 15:47:41 INFO]: Training loss at epoch 10: 0.96184241771698
[08/27/2025 15:47:55 INFO]: Training stats: {
    "score": -1.0202875177298767,
    "rmse": 1.0202875177298767
}
[08/27/2025 15:47:55 INFO]: Val stats: {
    "score": -0.8838697064863275,
    "rmse": 0.8838697064863275
}
[08/27/2025 15:47:55 INFO]: Test stats: {
    "score": -0.9918317502674524,
    "rmse": 0.9918317502674524
}
[08/27/2025 15:47:55 INFO]: Training loss at epoch 27: 0.9838833212852478
[08/27/2025 15:48:23 INFO]: Training loss at epoch 16: 1.1168455481529236
[08/27/2025 15:48:39 INFO]: Training loss at epoch 15: 1.3070523738861084
[08/27/2025 15:48:45 INFO]: Training loss at epoch 28: 0.9047948122024536
[08/27/2025 15:48:53 INFO]: Training loss at epoch 0: 1.0744366645812988
[08/27/2025 15:48:54 INFO]: New best epoch, val score: -0.8822781907154541
[08/27/2025 15:48:54 INFO]: Saving model to: maddest-Elbert_trial_9/model_best.pth
[08/27/2025 15:48:55 INFO]: Training loss at epoch 8: 0.9532725811004639
[08/27/2025 15:49:00 INFO]: Training loss at epoch 43: 1.0424268245697021
[08/27/2025 15:49:01 INFO]: Training loss at epoch 29: 1.0876448154449463
[08/27/2025 15:49:14 INFO]: Training loss at epoch 12: 0.9888545572757721
[08/27/2025 15:49:24 INFO]: New best epoch, val score: -0.9217231319683623
[08/27/2025 15:49:24 INFO]: Saving model to: maddest-Elbert_trial_32/model_best.pth
[08/27/2025 15:49:27 INFO]: Training loss at epoch 11: 1.0370665788650513
[08/27/2025 15:49:27 INFO]: Training loss at epoch 11: 1.2729116678237915
[08/27/2025 15:49:29 INFO]: Training loss at epoch 0: 1.2161030173301697
[08/27/2025 15:49:34 INFO]: Training loss at epoch 29: 1.0421165525913239
[08/27/2025 15:49:39 INFO]: Training loss at epoch 23: 0.8601163029670715
[08/27/2025 15:49:49 INFO]: Training loss at epoch 9: 0.8657128810882568
[08/27/2025 15:49:52 INFO]: Training stats: {
    "score": -1.0014349562023737,
    "rmse": 1.0014349562023737
}
[08/27/2025 15:49:52 INFO]: Val stats: {
    "score": -0.9433190041968849,
    "rmse": 0.9433190041968849
}
[08/27/2025 15:49:52 INFO]: Test stats: {
    "score": -0.9710072413248163,
    "rmse": 0.9710072413248163
}
[08/27/2025 15:49:55 INFO]: Training stats: {
    "score": -0.9959997895879039,
    "rmse": 0.9959997895879039
}
[08/27/2025 15:49:55 INFO]: Val stats: {
    "score": -0.9218174046561952,
    "rmse": 0.9218174046561952
}
[08/27/2025 15:49:55 INFO]: Test stats: {
    "score": -0.9730160423652358,
    "rmse": 0.9730160423652358
}
[08/27/2025 15:50:04 INFO]: Training loss at epoch 25: 0.8616544604301453
[08/27/2025 15:50:21 INFO]: Training loss at epoch 14: 1.390492022037506
[08/27/2025 15:50:24 INFO]: New best epoch, val score: -0.891924098327762
[08/27/2025 15:50:24 INFO]: Saving model to: maddest-Elbert_trial_31/model_best.pth
[08/27/2025 15:50:36 INFO]: Training loss at epoch 44: 0.8529439270496368
[08/27/2025 15:50:41 INFO]: Training loss at epoch 30: 1.2625523209571838
[08/27/2025 15:50:43 INFO]: Training loss at epoch 12: 1.2445701956748962
[08/27/2025 15:51:13 INFO]: Training loss at epoch 12: 0.9567857384681702
[08/27/2025 15:51:31 INFO]: Training loss at epoch 31: 0.7316336184740067
[08/27/2025 15:51:37 INFO]: Running Final Evaluation...
[08/27/2025 15:51:55 INFO]: Training accuracy: {
    "score": -1.0038297699041163,
    "rmse": 1.0038297699041163
}
[08/27/2025 15:51:55 INFO]: Val accuracy: {
    "score": -0.9023344144667546,
    "rmse": 0.9023344144667546
}
[08/27/2025 15:51:55 INFO]: Test accuracy: {
    "score": -0.9728434734368938,
    "rmse": 0.9728434734368938
}
[08/27/2025 15:51:55 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_28",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9728434734368938,
        "rmse": 0.9728434734368938
    },
    "train_stats": {
        "score": -1.0038297699041163,
        "rmse": 1.0038297699041163
    },
    "val_stats": {
        "score": -0.9023344144667546,
        "rmse": 0.9023344144667546
    }
}
[08/27/2025 15:51:55 INFO]: Procewss finished for trial maddest-Elbert_trial_28
[08/27/2025 15:51:55 INFO]: 
_________________________________________________

[08/27/2025 15:51:55 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:51:55 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 1.1749248101420748
  attention_dropout: 0.0020673731426293362
  ffn_dropout: 0.0020673731426293362
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.475146069452561e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_33

[08/27/2025 15:51:55 INFO]: This ft_transformer has 5.766 million parameters.
[08/27/2025 15:51:55 INFO]: Training will start at epoch 0.
[08/27/2025 15:51:55 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:52:13 INFO]: Training loss at epoch 45: 1.0844168961048126
[08/27/2025 15:52:19 INFO]: Training loss at epoch 30: 1.147909164428711
[08/27/2025 15:52:24 INFO]: Training stats: {
    "score": -1.0066249407064474,
    "rmse": 1.0066249407064474
}
[08/27/2025 15:52:24 INFO]: Val stats: {
    "score": -0.9702868176624833,
    "rmse": 0.9702868176624833
}
[08/27/2025 15:52:24 INFO]: Test stats: {
    "score": -0.9786670481579879,
    "rmse": 0.9786670481579879
}
[08/27/2025 15:52:26 INFO]: New best epoch, val score: -0.9040607367525888
[08/27/2025 15:52:26 INFO]: Saving model to: maddest-Elbert_trial_13/model_best.pth
[08/27/2025 15:52:37 INFO]: Training loss at epoch 17: 0.8450897336006165
[08/27/2025 15:52:40 INFO]: Training loss at epoch 24: 0.8392181694507599
[08/27/2025 15:52:43 INFO]: Training loss at epoch 26: 0.9143726825714111
[08/27/2025 15:52:59 INFO]: Training loss at epoch 13: 0.9633837938308716
[08/27/2025 15:53:03 INFO]: Training loss at epoch 1: 2.3581960797309875
[08/27/2025 15:53:05 INFO]: Training loss at epoch 12: 1.2775376439094543
[08/27/2025 15:53:06 INFO]: Training loss at epoch 1: 1.1328939199447632
[08/27/2025 15:53:09 INFO]: Training loss at epoch 16: 0.9487462043762207
[08/27/2025 15:53:32 INFO]: Training loss at epoch 15: 1.0121528804302216
[08/27/2025 15:53:36 INFO]: New best epoch, val score: -0.8790474215370098
[08/27/2025 15:53:36 INFO]: Saving model to: maddest-Elbert_trial_32/model_best.pth
[08/27/2025 15:53:49 INFO]: Training loss at epoch 46: 1.0184966325759888
[08/27/2025 15:53:53 INFO]: Training loss at epoch 9: 1.0358667373657227
[08/27/2025 15:54:02 INFO]: New best epoch, val score: -0.9031002369418292
[08/27/2025 15:54:02 INFO]: Saving model to: maddest-Elbert_trial_13/model_best.pth
[08/27/2025 15:54:04 INFO]: New best epoch, val score: -1.0572949913013967
[08/27/2025 15:54:04 INFO]: Saving model to: maddest-Elbert_trial_30/model_best.pth
[08/27/2025 15:54:05 INFO]: Training loss at epoch 13: 0.9271771609783173
[08/27/2025 15:54:06 INFO]: Training loss at epoch 6: 0.9366917014122009
[08/27/2025 15:54:23 INFO]: Training loss at epoch 10: 0.9525282680988312
[08/27/2025 15:54:41 INFO]: Training loss at epoch 31: 1.0407530069351196
[08/27/2025 15:54:42 INFO]: Training loss at epoch 14: 1.191996693611145
[08/27/2025 15:55:22 INFO]: Training loss at epoch 27: 0.9624443352222443
[08/27/2025 15:55:27 INFO]: Training loss at epoch 47: 1.1147511005401611
[08/27/2025 15:55:30 INFO]: Training loss at epoch 12: 1.258692741394043
[08/27/2025 15:55:34 INFO]: Training loss at epoch 0: 1.3255891799926758
[08/27/2025 15:55:36 INFO]: Training stats: {
    "score": -1.0036864437598814,
    "rmse": 1.0036864437598814
}
[08/27/2025 15:55:36 INFO]: Val stats: {
    "score": -0.9528377422689422,
    "rmse": 0.9528377422689422
}
[08/27/2025 15:55:36 INFO]: Test stats: {
    "score": -0.9757918517597189,
    "rmse": 0.9757918517597189
}
[08/27/2025 15:55:38 INFO]: New best epoch, val score: -0.9028784222237792
[08/27/2025 15:55:38 INFO]: Saving model to: maddest-Elbert_trial_13/model_best.pth
[08/27/2025 15:55:40 INFO]: Training loss at epoch 25: 0.8607660233974457
[08/27/2025 15:56:06 INFO]: New best epoch, val score: -0.8930403736012158
[08/27/2025 15:56:06 INFO]: Saving model to: maddest-Elbert_trial_33/model_best.pth
[08/27/2025 15:56:25 INFO]: Training loss at epoch 13: 1.3218836784362793
[08/27/2025 15:56:28 INFO]: Training loss at epoch 15: 1.2094372510910034
[08/27/2025 15:56:44 INFO]: Training loss at epoch 16: 0.9528522193431854
[08/27/2025 15:56:52 INFO]: Training loss at epoch 18: 1.0351914167404175
[08/27/2025 15:56:56 INFO]: Training loss at epoch 1: 1.0410853028297424
[08/27/2025 15:57:04 INFO]: Training loss at epoch 48: 0.9602037370204926
[08/27/2025 15:57:04 INFO]: Training loss at epoch 32: 0.9199028015136719
[08/27/2025 15:57:17 INFO]: Training loss at epoch 2: 1.0064803063869476
[08/27/2025 15:57:39 INFO]: Training loss at epoch 17: 0.9771339297294617
[08/27/2025 15:58:01 INFO]: Training loss at epoch 28: 1.2284250259399414
[08/27/2025 15:58:14 INFO]: Training loss at epoch 16: 0.8673616647720337
[08/27/2025 15:58:41 INFO]: Training loss at epoch 49: 0.9283271729946136
[08/27/2025 15:58:42 INFO]: Training loss at epoch 26: 0.9234330654144287
[08/27/2025 15:58:54 INFO]: Training loss at epoch 13: 0.8677038550376892
[08/27/2025 15:58:56 INFO]: Training loss at epoch 14: 0.9925258457660675
[08/27/2025 15:59:16 INFO]: Training stats: {
    "score": -1.0016801654219305,
    "rmse": 1.0016801654219305
}
[08/27/2025 15:59:16 INFO]: Val stats: {
    "score": -0.9051818492685273,
    "rmse": 0.9051818492685273
}
[08/27/2025 15:59:16 INFO]: Test stats: {
    "score": -0.975895534488445,
    "rmse": 0.975895534488445
}
[08/27/2025 15:59:26 INFO]: Training loss at epoch 33: 0.9334564208984375
[08/27/2025 15:59:45 INFO]: Training loss at epoch 1: 1.153879702091217
[08/27/2025 15:59:52 INFO]: Training loss at epoch 10: 1.1568593382835388
[08/27/2025 15:59:55 INFO]: Training loss at epoch 17: 0.886005699634552
[08/27/2025 15:59:59 INFO]: Training loss at epoch 17: 1.0680068135261536
[08/27/2025 16:00:36 INFO]: Training loss at epoch 10: 0.9875546395778656
[08/27/2025 16:00:39 INFO]: Training loss at epoch 29: 1.1184958219528198
[08/27/2025 16:00:51 INFO]: Training loss at epoch 11: 1.1498067378997803
[08/27/2025 16:00:53 INFO]: Training loss at epoch 50: 1.1291062831878662
[08/27/2025 16:01:05 INFO]: Training loss at epoch 19: 1.304006278514862
[08/27/2025 16:01:11 INFO]: New best epoch, val score: -0.8956355319016193
[08/27/2025 16:01:11 INFO]: Saving model to: maddest-Elbert_trial_27/model_best.pth
[08/27/2025 16:01:26 INFO]: Training loss at epoch 3: 0.994300365447998
[08/27/2025 16:01:33 INFO]: Training loss at epoch 13: 1.1421244442462921
[08/27/2025 16:01:34 INFO]: Training loss at epoch 2: 1.2464070916175842
[08/27/2025 16:01:37 INFO]: Training stats: {
    "score": -1.0009419183950012,
    "rmse": 1.0009419183950012
}
[08/27/2025 16:01:37 INFO]: Val stats: {
    "score": -0.9020179738342097,
    "rmse": 0.9020179738342097
}
[08/27/2025 16:01:37 INFO]: Test stats: {
    "score": -0.9741221132653465,
    "rmse": 0.9741221132653465
}
[08/27/2025 16:01:43 INFO]: Training loss at epoch 18: 1.073978304862976
[08/27/2025 16:01:43 INFO]: Training loss at epoch 27: 0.9788564443588257
[08/27/2025 16:01:49 INFO]: Training loss at epoch 34: 0.9858929514884949
[08/27/2025 16:02:03 INFO]: Training loss at epoch 14: 1.3214623928070068
[08/27/2025 16:02:09 INFO]: Training loss at epoch 18: 1.264980971813202
[08/27/2025 16:02:30 INFO]: Training loss at epoch 51: 0.9368009567260742
[08/27/2025 16:02:32 INFO]: Training stats: {
    "score": -1.0494044722913634,
    "rmse": 1.0494044722913634
}
[08/27/2025 16:02:32 INFO]: Val stats: {
    "score": -0.8822785150397656,
    "rmse": 0.8822785150397656
}
[08/27/2025 16:02:32 INFO]: Test stats: {
    "score": -1.0217750257133744,
    "rmse": 1.0217750257133744
}
[08/27/2025 16:03:06 INFO]: Training loss at epoch 18: 0.8852597773075104
[08/27/2025 16:03:29 INFO]: Training loss at epoch 19: 1.0104824900627136
[08/27/2025 16:03:48 INFO]: Training loss at epoch 15: 0.9578669667243958
[08/27/2025 16:03:54 INFO]: Training loss at epoch 2: 1.6467780470848083
[08/27/2025 16:04:07 INFO]: Training loss at epoch 52: 0.9549781680107117
[08/27/2025 16:04:08 INFO]: Training stats: {
    "score": -0.9963256715103924,
    "rmse": 0.9963256715103924
}
[08/27/2025 16:04:08 INFO]: Val stats: {
    "score": -0.9206499506093453,
    "rmse": 0.9206499506093453
}
[08/27/2025 16:04:08 INFO]: Test stats: {
    "score": -0.9711106121654481,
    "rmse": 0.9711106121654481
}
[08/27/2025 16:04:13 INFO]: Training loss at epoch 35: 0.9493595361709595
[08/27/2025 16:04:16 INFO]: Training loss at epoch 30: 1.0848848223686218
[08/27/2025 16:04:20 INFO]: Training loss at epoch 2: 1.5282062292099
[08/27/2025 16:04:46 INFO]: Training loss at epoch 14: 1.0702532827854156
[08/27/2025 16:04:46 INFO]: Training loss at epoch 28: 0.8911656141281128
[08/27/2025 16:05:29 INFO]: Training loss at epoch 7: 0.9755721092224121
[08/27/2025 16:05:33 INFO]: Training loss at epoch 11: 0.9655739367008209
[08/27/2025 16:05:34 INFO]: Training loss at epoch 4: 1.2562409043312073
[08/27/2025 16:05:47 INFO]: Training loss at epoch 53: 1.0748698711395264
[08/27/2025 16:05:53 INFO]: Training loss at epoch 20: 1.159792959690094
[08/27/2025 16:06:09 INFO]: New best epoch, val score: -0.8816422424309457
[08/27/2025 16:06:09 INFO]: Saving model to: maddest-Elbert_trial_27/model_best.pth
[08/27/2025 16:06:18 INFO]: Training loss at epoch 19: 0.8512972891330719
[08/27/2025 16:06:37 INFO]: Training loss at epoch 36: 1.0137486457824707
[08/27/2025 16:06:40 INFO]: Training loss at epoch 19: 0.9241918921470642
[08/27/2025 16:06:47 INFO]: Training loss at epoch 20: 1.0804362297058105
[08/27/2025 16:06:54 INFO]: Training loss at epoch 31: 0.9939467012882233
[08/27/2025 16:07:17 INFO]: Training loss at epoch 12: 1.2682369351387024
[08/27/2025 16:07:21 INFO]: Training loss at epoch 11: 1.2644734978675842
[08/27/2025 16:07:23 INFO]: Training stats: {
    "score": -1.0038963476971878,
    "rmse": 1.0038963476971878
}
[08/27/2025 16:07:23 INFO]: Val stats: {
    "score": -0.9614378275081267,
    "rmse": 0.9614378275081267
}
[08/27/2025 16:07:23 INFO]: Test stats: {
    "score": -0.9751113636676306,
    "rmse": 0.9751113636676306
}
[08/27/2025 16:07:27 INFO]: Training loss at epoch 54: 0.8178777694702148
[08/27/2025 16:07:35 INFO]: Training loss at epoch 14: 1.3166568279266357
[08/27/2025 16:07:39 INFO]: Training loss at epoch 21: 1.0260030627250671
[08/27/2025 16:07:45 INFO]: Training loss at epoch 15: 1.154776692390442
[08/27/2025 16:07:49 INFO]: Training loss at epoch 29: 1.011288344860077
[08/27/2025 16:07:54 INFO]: New best epoch, val score: -0.9069010813515282
[08/27/2025 16:07:54 INFO]: Saving model to: maddest-Elbert_trial_29/model_best.pth
[08/27/2025 16:08:05 INFO]: Training loss at epoch 3: 1.300038456916809
[08/27/2025 16:08:15 INFO]: Training stats: {
    "score": -1.01599109095294,
    "rmse": 1.01599109095294
}
[08/27/2025 16:08:15 INFO]: Val stats: {
    "score": -0.8861649193896671,
    "rmse": 0.8861649193896671
}
[08/27/2025 16:08:15 INFO]: Test stats: {
    "score": -0.9873465787169905,
    "rmse": 0.9873465787169905
}
[08/27/2025 16:08:19 INFO]: New best epoch, val score: -0.8811420807466014
[08/27/2025 16:08:19 INFO]: Saving model to: maddest-Elbert_trial_6/model_best.pth
[08/27/2025 16:08:36 INFO]: New best epoch, val score: -0.8825583560296766
[08/27/2025 16:08:36 INFO]: Saving model to: maddest-Elbert_trial_33/model_best.pth
[08/27/2025 16:08:39 INFO]: Training loss at epoch 16: 1.1943721771240234
[08/27/2025 16:08:51 INFO]: Training stats: {
    "score": -1.0241385008729909,
    "rmse": 1.0241385008729909
}
[08/27/2025 16:08:51 INFO]: Val stats: {
    "score": -1.0382679733938178,
    "rmse": 1.0382679733938178
}
[08/27/2025 16:08:51 INFO]: Test stats: {
    "score": -1.0111391399421668,
    "rmse": 1.0111391399421668
}
[08/27/2025 16:09:00 INFO]: Training loss at epoch 37: 1.0628116726875305
[08/27/2025 16:09:05 INFO]: Training loss at epoch 55: 1.2044449150562286
[08/27/2025 16:09:26 INFO]: Training loss at epoch 22: 1.151437520980835
[08/27/2025 16:09:34 INFO]: Training loss at epoch 32: 1.0323089063167572
[08/27/2025 16:09:41 INFO]: New best epoch, val score: -0.9025522444831586
[08/27/2025 16:09:41 INFO]: Saving model to: maddest-Elbert_trial_29/model_best.pth
[08/27/2025 16:09:44 INFO]: Training loss at epoch 5: 0.8135068565607071
[08/27/2025 16:10:05 INFO]: Training loss at epoch 3: 1.5604808628559113
[08/27/2025 16:10:33 INFO]: Training loss at epoch 12: 1.0174197256565094
[08/27/2025 16:10:36 INFO]: Training loss at epoch 20: 1.3034515976905823
[08/27/2025 16:10:40 INFO]: Training loss at epoch 15: 1.1493684649467468
[08/27/2025 16:10:44 INFO]: Training loss at epoch 56: 0.9138789772987366
[08/27/2025 16:11:03 INFO]: Training loss at epoch 21: 1.022237777709961
[08/27/2025 16:11:07 INFO]: New best epoch, val score: -0.8869085486865405
[08/27/2025 16:11:07 INFO]: Saving model to: maddest-Elbert_trial_30/model_best.pth
[08/27/2025 16:11:11 INFO]: New best epoch, val score: -0.8809944492577898
[08/27/2025 16:11:11 INFO]: Saving model to: maddest-Elbert_trial_27/model_best.pth
[08/27/2025 16:11:15 INFO]: Training loss at epoch 23: 0.9825782477855682
[08/27/2025 16:11:26 INFO]: Training loss at epoch 38: 0.7775077819824219
[08/27/2025 16:11:47 INFO]: Training loss at epoch 3: 0.922444224357605
[08/27/2025 16:11:53 INFO]: Training loss at epoch 30: 1.2852134108543396
[08/27/2025 16:12:14 INFO]: Training loss at epoch 33: 0.8838132619857788
[08/27/2025 16:12:15 INFO]: Training loss at epoch 4: 1.0362704992294312
[08/27/2025 16:12:22 INFO]: Training loss at epoch 57: 1.225717693567276
[08/27/2025 16:12:45 INFO]: Training loss at epoch 20: 1.0836776494979858
[08/27/2025 16:12:59 INFO]: Training loss at epoch 24: 1.1886437237262726
[08/27/2025 16:13:28 INFO]: Training loss at epoch 16: 1.103573590517044
[08/27/2025 16:13:33 INFO]: Training loss at epoch 17: 1.080159306526184
[08/27/2025 16:13:39 INFO]: Training loss at epoch 15: 1.0295094847679138
[08/27/2025 16:13:46 INFO]: Training loss at epoch 13: 1.0365133881568909
[08/27/2025 16:13:49 INFO]: Training loss at epoch 39: 0.8333344459533691
[08/27/2025 16:13:49 INFO]: Training loss at epoch 21: 0.9962417483329773
[08/27/2025 16:13:53 INFO]: Training loss at epoch 6: 1.1607017517089844
[08/27/2025 16:14:01 INFO]: Training loss at epoch 58: 1.0751270055770874
[08/27/2025 16:14:44 INFO]: Training stats: {
    "score": -1.0021147412334557,
    "rmse": 1.0021147412334557
}
[08/27/2025 16:14:44 INFO]: Val stats: {
    "score": -0.8977616614851635,
    "rmse": 0.8977616614851635
}
[08/27/2025 16:14:44 INFO]: Test stats: {
    "score": -0.9792771270692407,
    "rmse": 0.9792771270692407
}
[08/27/2025 16:14:44 INFO]: Training loss at epoch 25: 1.0674028992652893
[08/27/2025 16:14:52 INFO]: Training loss at epoch 12: 0.837196558713913
[08/27/2025 16:14:54 INFO]: Training loss at epoch 34: 1.0668418109416962
[08/27/2025 16:14:54 INFO]: Training loss at epoch 31: 1.1040855050086975
[08/27/2025 16:15:17 INFO]: Training loss at epoch 22: 1.0997701287269592
[08/27/2025 16:15:32 INFO]: Training loss at epoch 13: 0.9315439462661743
[08/27/2025 16:15:39 INFO]: Training loss at epoch 59: 1.2007149457931519
[08/27/2025 16:16:14 INFO]: Training stats: {
    "score": -0.998043180683776,
    "rmse": 0.998043180683776
}
[08/27/2025 16:16:14 INFO]: Val stats: {
    "score": -0.9280022119831141,
    "rmse": 0.9280022119831141
}
[08/27/2025 16:16:14 INFO]: Test stats: {
    "score": -0.9719845832360338,
    "rmse": 0.9719845832360338
}
[08/27/2025 16:16:25 INFO]: Training loss at epoch 5: 1.1894491910934448
[08/27/2025 16:16:29 INFO]: Training loss at epoch 26: 1.1171767711639404
[08/27/2025 16:16:34 INFO]: Training loss at epoch 16: 1.0842119455337524
[08/27/2025 16:16:56 INFO]: Training loss at epoch 8: 0.9533856213092804
[08/27/2025 16:17:01 INFO]: Training loss at epoch 22: 1.0148622393608093
[08/27/2025 16:17:06 INFO]: Training loss at epoch 40: 1.0288809537887573
[08/27/2025 16:17:16 INFO]: Training loss at epoch 21: 0.9835470914840698
[08/27/2025 16:17:32 INFO]: Training loss at epoch 35: 1.0822608470916748
[08/27/2025 16:17:51 INFO]: Training loss at epoch 60: 1.3373894095420837
[08/27/2025 16:17:54 INFO]: Training loss at epoch 32: 1.143233597278595
[08/27/2025 16:18:00 INFO]: Training loss at epoch 7: 1.0577396154403687
[08/27/2025 16:18:14 INFO]: Training loss at epoch 27: 0.8102777004241943
[08/27/2025 16:18:24 INFO]: Training loss at epoch 18: 1.032230794429779
[08/27/2025 16:18:37 INFO]: Training loss at epoch 4: 1.1327286958694458
[08/27/2025 16:19:06 INFO]: Training loss at epoch 17: 0.8740798830986023
[08/27/2025 16:19:13 INFO]: Training loss at epoch 4: 1.3449496626853943
[08/27/2025 16:19:29 INFO]: Training loss at epoch 61: 0.9464106559753418
[08/27/2025 16:19:29 INFO]: Training loss at epoch 41: 0.9200724065303802
[08/27/2025 16:19:30 INFO]: Training loss at epoch 23: 1.0074805617332458
[08/27/2025 16:19:41 INFO]: Training loss at epoch 16: 0.8912917971611023
[08/27/2025 16:19:46 INFO]: New best epoch, val score: -0.8820969359500009
[08/27/2025 16:19:46 INFO]: Saving model to: maddest-Elbert_trial_12/model_best.pth
[08/27/2025 16:19:47 INFO]: Running Final Evaluation...
[08/27/2025 16:19:59 INFO]: Training loss at epoch 28: 0.8769415020942688
[08/27/2025 16:20:09 INFO]: New best epoch, val score: -0.8858748428971649
[08/27/2025 16:20:09 INFO]: Saving model to: maddest-Elbert_trial_31/model_best.pth
[08/27/2025 16:20:11 INFO]: Training loss at epoch 23: 0.9429281949996948
[08/27/2025 16:20:12 INFO]: Training loss at epoch 36: 0.8374110460281372
[08/27/2025 16:20:13 INFO]: Training loss at epoch 14: 0.8928070664405823
[08/27/2025 16:20:30 INFO]: Training loss at epoch 14: 1.5427189767360687
[08/27/2025 16:20:33 INFO]: Training loss at epoch 6: 1.33386892080307
[08/27/2025 16:20:44 INFO]: Training accuracy: {
    "score": -1.0311634439316948,
    "rmse": 1.0311634439316948
}
[08/27/2025 16:20:44 INFO]: Val accuracy: {
    "score": -0.8825362651559039,
    "rmse": 0.8825362651559039
}
[08/27/2025 16:20:44 INFO]: Test accuracy: {
    "score": -1.0061854548176252,
    "rmse": 1.0061854548176252
}
[08/27/2025 16:20:44 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_10",
    "best_epoch": 10,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0061854548176252,
        "rmse": 1.0061854548176252
    },
    "train_stats": {
        "score": -1.0311634439316948,
        "rmse": 1.0311634439316948
    },
    "val_stats": {
        "score": -0.8825362651559039,
        "rmse": 0.8825362651559039
    }
}
[08/27/2025 16:20:44 INFO]: Procewss finished for trial maddest-Elbert_trial_10
[08/27/2025 16:20:44 INFO]: 
_________________________________________________

[08/27/2025 16:20:44 INFO]: train_net_for_optune.py main() running.
[08/27/2025 16:20:44 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 1.186863289214479
  attention_dropout: 0.017503525860621087
  ffn_dropout: 0.017503525860621087
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.531597318108562e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_34

[08/27/2025 16:20:44 INFO]: This ft_transformer has 5.784 million parameters.
[08/27/2025 16:20:44 INFO]: Training will start at epoch 0.
[08/27/2025 16:20:44 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 16:20:56 INFO]: Training loss at epoch 33: 1.0908673703670502
[08/27/2025 16:21:06 INFO]: Training loss at epoch 62: 1.0190095901489258
[08/27/2025 16:21:47 INFO]: Training loss at epoch 29: 0.8179542124271393
[08/27/2025 16:21:47 INFO]: Training loss at epoch 22: 0.7495314180850983
[08/27/2025 16:22:11 INFO]: Training loss at epoch 8: 1.1860755681991577
[08/27/2025 16:22:22 INFO]: Training loss at epoch 13: 1.1674980521202087
[08/27/2025 16:22:25 INFO]: Training loss at epoch 17: 0.9561720192432404
[08/27/2025 16:22:25 INFO]: Training stats: {
    "score": -0.9994992778101343,
    "rmse": 0.9994992778101343
}
[08/27/2025 16:22:25 INFO]: Val stats: {
    "score": -0.9487654244014248,
    "rmse": 0.9487654244014248
}
[08/27/2025 16:22:25 INFO]: Test stats: {
    "score": -0.9738227386207196,
    "rmse": 0.9738227386207196
}
[08/27/2025 16:22:44 INFO]: Training loss at epoch 63: 1.025239109992981
[08/27/2025 16:22:51 INFO]: Training loss at epoch 37: 1.1367996037006378
[08/27/2025 16:23:16 INFO]: New best epoch, val score: -0.884906359758318
[08/27/2025 16:23:16 INFO]: Saving model to: maddest-Elbert_trial_3/model_best.pth
[08/27/2025 16:23:17 INFO]: Training loss at epoch 19: 1.2805006504058838
[08/27/2025 16:23:23 INFO]: Training loss at epoch 24: 1.2318238615989685
[08/27/2025 16:23:43 INFO]: Training loss at epoch 24: 1.0123600363731384
[08/27/2025 16:23:58 INFO]: Training loss at epoch 34: 0.8435134589672089
[08/27/2025 16:24:11 INFO]: Training loss at epoch 30: 0.9881578683853149
[08/27/2025 16:24:21 INFO]: Training loss at epoch 64: 0.8689746260643005
[08/27/2025 16:24:22 INFO]: Training loss at epoch 0: 1.3168736100196838
[08/27/2025 16:24:43 INFO]: Training loss at epoch 7: 1.1969468593597412
[08/27/2025 16:24:43 INFO]: Training loss at epoch 18: 0.9793323576450348
[08/27/2025 16:24:54 INFO]: New best epoch, val score: -1.220117588307646
[08/27/2025 16:24:54 INFO]: Saving model to: maddest-Elbert_trial_34/model_best.pth
[08/27/2025 16:24:58 INFO]: Training stats: {
    "score": -0.9989486745691794,
    "rmse": 0.9989486745691794
}
[08/27/2025 16:24:58 INFO]: Val stats: {
    "score": -0.9029250499552961,
    "rmse": 0.9029250499552961
}
[08/27/2025 16:24:58 INFO]: Test stats: {
    "score": -0.9742902314568528,
    "rmse": 0.9742902314568528
}
[08/27/2025 16:25:29 INFO]: Training loss at epoch 38: 1.0406432151794434
[08/27/2025 16:25:29 INFO]: Training loss at epoch 15: 0.9338216185569763
[08/27/2025 16:25:47 INFO]: Training loss at epoch 17: 0.7941065579652786
[08/27/2025 16:25:55 INFO]: Training loss at epoch 31: 0.9582872986793518
[08/27/2025 16:26:00 INFO]: Training loss at epoch 65: 0.922247439622879
[08/27/2025 16:26:16 INFO]: Training loss at epoch 23: 1.066934585571289
[08/27/2025 16:26:18 INFO]: Training loss at epoch 9: 1.0838395357131958
[08/27/2025 16:26:35 INFO]: Training loss at epoch 25: 0.9800816774368286
[08/27/2025 16:26:41 INFO]: Training loss at epoch 15: 1.1133392453193665
[08/27/2025 16:26:41 INFO]: Training loss at epoch 5: 0.7831116169691086
[08/27/2025 16:26:58 INFO]: Training loss at epoch 35: 0.918750524520874
[08/27/2025 16:27:08 INFO]: Training loss at epoch 5: 1.604433000087738
[08/27/2025 16:27:37 INFO]: New best epoch, val score: -0.8838661122442986
[08/27/2025 16:27:37 INFO]: Saving model to: maddest-Elbert_trial_31/model_best.pth
[08/27/2025 16:27:38 INFO]: Training loss at epoch 66: 1.0581015348434448
[08/27/2025 16:27:42 INFO]: Training loss at epoch 32: 0.9497030079364777
[08/27/2025 16:27:46 INFO]: Training stats: {
    "score": -1.0126004853552093,
    "rmse": 1.0126004853552093
}
[08/27/2025 16:27:46 INFO]: Val stats: {
    "score": -1.0028789710016262,
    "rmse": 1.0028789710016262
}
[08/27/2025 16:27:46 INFO]: Test stats: {
    "score": -0.9955651934695783,
    "rmse": 0.9955651934695783
}
[08/27/2025 16:27:57 INFO]: Training loss at epoch 25: 0.7809399515390396
[08/27/2025 16:28:08 INFO]: Training loss at epoch 39: 1.0180540680885315
[08/27/2025 16:28:17 INFO]: Training loss at epoch 18: 0.752750501036644
[08/27/2025 16:28:21 INFO]: Training loss at epoch 9: 0.9043935835361481
[08/27/2025 16:28:32 INFO]: Training loss at epoch 1: 1.0632818341255188
[08/27/2025 16:28:53 INFO]: Training loss at epoch 8: 1.2197635173797607
[08/27/2025 16:29:04 INFO]: New best epoch, val score: -0.8894279995491664
[08/27/2025 16:29:04 INFO]: Saving model to: maddest-Elbert_trial_34/model_best.pth
[08/27/2025 16:29:06 INFO]: Training stats: {
    "score": -1.008519067959204,
    "rmse": 1.008519067959204
}
[08/27/2025 16:29:06 INFO]: Val stats: {
    "score": -0.8888635406770145,
    "rmse": 0.8888635406770145
}
[08/27/2025 16:29:06 INFO]: Test stats: {
    "score": -0.9823927492543466,
    "rmse": 0.9823927492543466
}
[08/27/2025 16:29:15 INFO]: Training loss at epoch 67: 0.9591591656208038
[08/27/2025 16:29:27 INFO]: Training loss at epoch 33: 0.797903448343277
[08/27/2025 16:29:48 INFO]: Training loss at epoch 26: 1.0333718061447144
[08/27/2025 16:29:52 INFO]: Training loss at epoch 20: 0.8421974182128906
[08/27/2025 16:29:54 INFO]: Training loss at epoch 14: 0.9664121270179749
[08/27/2025 16:30:00 INFO]: Training loss at epoch 36: 1.1393290162086487
[08/27/2025 16:30:22 INFO]: Training loss at epoch 19: 1.127726435661316
[08/27/2025 16:30:29 INFO]: Training loss at epoch 16: 0.8889622092247009
[08/27/2025 16:30:48 INFO]: Training loss at epoch 24: 0.9807871580123901
[08/27/2025 16:30:53 INFO]: Training loss at epoch 68: 1.0934849083423615
[08/27/2025 16:31:12 INFO]: Training loss at epoch 34: 1.0816808938980103
[08/27/2025 16:31:45 INFO]: Training loss at epoch 40: 0.9056874811649323
[08/27/2025 16:31:50 INFO]: Training loss at epoch 18: 0.9466907382011414
[08/27/2025 16:31:54 INFO]: Training loss at epoch 10: 1.0080823004245758
[08/27/2025 16:32:11 INFO]: Training loss at epoch 26: 0.9821079969406128
[08/27/2025 16:32:13 INFO]: Training stats: {
    "score": -1.0003644956687845,
    "rmse": 1.0003644956687845
}
[08/27/2025 16:32:13 INFO]: Val stats: {
    "score": -0.9462085844826781,
    "rmse": 0.9462085844826781
}
[08/27/2025 16:32:13 INFO]: Test stats: {
    "score": -0.9730557357570656,
    "rmse": 0.9730557357570656
}
[08/27/2025 16:32:18 INFO]: Training stats: {
    "score": -1.0776896312323845,
    "rmse": 1.0776896312323845
}
[08/27/2025 16:32:18 INFO]: Val stats: {
    "score": -0.8899472208070871,
    "rmse": 0.8899472208070871
}
[08/27/2025 16:32:18 INFO]: Test stats: {
    "score": -1.0498932297418766,
    "rmse": 1.0498932297418766
}
[08/27/2025 16:32:30 INFO]: Training loss at epoch 69: 0.8979335725307465
[08/27/2025 16:32:42 INFO]: Training loss at epoch 2: 1.158812701702118
[08/27/2025 16:32:57 INFO]: Training loss at epoch 35: 1.046774446964264
[08/27/2025 16:33:00 INFO]: Training loss at epoch 27: 1.0071351826190948
[08/27/2025 16:33:02 INFO]: Training loss at epoch 37: 1.2299152612686157
[08/27/2025 16:33:03 INFO]: Training loss at epoch 9: 0.9385272860527039
[08/27/2025 16:33:06 INFO]: Training stats: {
    "score": -0.9980409164771549,
    "rmse": 0.9980409164771549
}
[08/27/2025 16:33:06 INFO]: Val stats: {
    "score": -0.9334173270771907,
    "rmse": 0.9334173270771907
}
[08/27/2025 16:33:06 INFO]: Test stats: {
    "score": -0.972014414875174,
    "rmse": 0.972014414875174
}
[08/27/2025 16:33:10 INFO]: Training loss at epoch 16: 0.9703514277935028
[08/27/2025 16:34:08 INFO]: Training loss at epoch 19: 1.1626354455947876
[08/27/2025 16:34:09 INFO]: Training loss at epoch 6: 1.1738965511322021
[08/27/2025 16:34:23 INFO]: Training loss at epoch 41: 0.9130472242832184
[08/27/2025 16:34:30 INFO]: Training stats: {
    "score": -1.0701658595771588,
    "rmse": 1.0701658595771588
}
[08/27/2025 16:34:30 INFO]: Val stats: {
    "score": -0.8878832640698606,
    "rmse": 0.8878832640698606
}
[08/27/2025 16:34:30 INFO]: Test stats: {
    "score": -1.0466151569902842,
    "rmse": 1.0466151569902842
}
[08/27/2025 16:34:42 INFO]: Training loss at epoch 70: 0.9855417013168335
[08/27/2025 16:34:42 INFO]: Training loss at epoch 36: 1.0896223783493042
[08/27/2025 16:34:44 INFO]: Training loss at epoch 21: 1.0229093730449677
[08/27/2025 16:35:04 INFO]: New best epoch, val score: -0.8819830775976338
[08/27/2025 16:35:04 INFO]: Saving model to: maddest-Elbert_trial_31/model_best.pth
[08/27/2025 16:35:20 INFO]: Training loss at epoch 25: 0.8801716864109039
[08/27/2025 16:35:21 INFO]: New best epoch, val score: -0.8824127394478518
[08/27/2025 16:35:21 INFO]: Saving model to: maddest-Elbert_trial_23/model_best.pth
[08/27/2025 16:35:31 INFO]: Training loss at epoch 17: 1.0937765538692474
[08/27/2025 16:35:38 INFO]: Training loss at epoch 6: 1.5193832516670227
[08/27/2025 16:36:02 INFO]: Training loss at epoch 38: 0.9458873867988586
[08/27/2025 16:36:05 INFO]: Training loss at epoch 11: 1.0404993295669556
[08/27/2025 16:36:10 INFO]: Training stats: {
    "score": -1.0259138499745033,
    "rmse": 1.0259138499745033
}
[08/27/2025 16:36:10 INFO]: Val stats: {
    "score": -1.0858459163859002,
    "rmse": 1.0858459163859002
}
[08/27/2025 16:36:10 INFO]: Test stats: {
    "score": -1.0428000501728425,
    "rmse": 1.0428000501728425
}
[08/27/2025 16:36:12 INFO]: Training loss at epoch 28: 1.0141555070877075
[08/27/2025 16:36:21 INFO]: Training loss at epoch 71: 1.0378649830818176
[08/27/2025 16:36:27 INFO]: Training loss at epoch 27: 1.0397086143493652
[08/27/2025 16:36:28 INFO]: Training loss at epoch 37: 0.8980140388011932
[08/27/2025 16:36:53 INFO]: Training loss at epoch 3: 1.449629008769989
[08/27/2025 16:37:04 INFO]: Training loss at epoch 42: 0.9957809448242188
[08/27/2025 16:37:24 INFO]: New best epoch, val score: -0.8844483901264816
[08/27/2025 16:37:24 INFO]: Saving model to: maddest-Elbert_trial_34/model_best.pth
[08/27/2025 16:37:25 INFO]: Training loss at epoch 15: 1.0640010237693787
[08/27/2025 16:37:54 INFO]: Training loss at epoch 19: 1.076356053352356
[08/27/2025 16:37:55 INFO]: Training loss at epoch 20: 0.8738878667354584
[08/27/2025 16:37:59 INFO]: Training loss at epoch 72: 1.073543131351471
[08/27/2025 16:38:18 INFO]: Training loss at epoch 38: 1.0033723413944244
[08/27/2025 16:38:39 INFO]: Training loss at epoch 10: 1.1353387832641602
[08/27/2025 16:39:04 INFO]: Training loss at epoch 39: 1.1323565542697906
[08/27/2025 16:39:25 INFO]: Training loss at epoch 29: 1.0418511629104614
[08/27/2025 16:39:37 INFO]: Training loss at epoch 73: 0.9941857755184174
[08/27/2025 16:39:39 INFO]: Training loss at epoch 17: 0.979033887386322
[08/27/2025 16:39:41 INFO]: Training loss at epoch 22: 1.0971162915229797
[08/27/2025 16:39:43 INFO]: Training loss at epoch 43: 0.9320853054523468
[08/27/2025 16:39:49 INFO]: Training loss at epoch 26: 0.7882069647312164
[08/27/2025 16:39:56 INFO]: Training stats: {
    "score": -1.0420585621007779,
    "rmse": 1.0420585621007779
}
[08/27/2025 16:39:56 INFO]: Val stats: {
    "score": -1.048867656613978,
    "rmse": 1.048867656613978
}
[08/27/2025 16:39:56 INFO]: Test stats: {
    "score": -1.015846774656503,
    "rmse": 1.015846774656503
}
[08/27/2025 16:40:04 INFO]: Running Final Evaluation...
[08/27/2025 16:40:05 INFO]: Training stats: {
    "score": -1.0233997035335645,
    "rmse": 1.0233997035335645
}
[08/27/2025 16:40:05 INFO]: Val stats: {
    "score": -0.8896480986413311,
    "rmse": 0.8896480986413311
}
[08/27/2025 16:40:05 INFO]: Test stats: {
    "score": -1.0286694084742298,
    "rmse": 1.0286694084742298
}
[08/27/2025 16:40:06 INFO]: Training loss at epoch 39: 0.8737437427043915
[08/27/2025 16:40:12 INFO]: Training loss at epoch 12: 1.1268872618675232
[08/27/2025 16:40:28 INFO]: Training loss at epoch 18: 1.1184009313583374
[08/27/2025 16:40:29 INFO]: Training stats: {
    "score": -0.997770618196238,
    "rmse": 0.997770618196238
}
[08/27/2025 16:40:29 INFO]: Val stats: {
    "score": -0.912389036229961,
    "rmse": 0.912389036229961
}
[08/27/2025 16:40:29 INFO]: Test stats: {
    "score": -0.967598280952087,
    "rmse": 0.967598280952087
}
[08/27/2025 16:40:41 INFO]: Training loss at epoch 28: 0.9462421834468842
[08/27/2025 16:40:45 INFO]: Training stats: {
    "score": -0.9965092637918628,
    "rmse": 0.9965092637918628
}
[08/27/2025 16:40:45 INFO]: Val stats: {
    "score": -0.9075806193086131,
    "rmse": 0.9075806193086131
}
[08/27/2025 16:40:45 INFO]: Test stats: {
    "score": -0.9715936122522784,
    "rmse": 0.9715936122522784
}
[08/27/2025 16:41:02 INFO]: Training loss at epoch 4: 1.1753790974617004
[08/27/2025 16:41:07 INFO]: Training accuracy: {
    "score": -1.0206471878648198,
    "rmse": 1.0206471878648198
}
[08/27/2025 16:41:07 INFO]: Val accuracy: {
    "score": -0.882541487958959,
    "rmse": 0.882541487958959
}
[08/27/2025 16:41:07 INFO]: Test accuracy: {
    "score": -0.9934969536558836,
    "rmse": 0.9934969536558836
}
[08/27/2025 16:41:07 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_21",
    "best_epoch": 12,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9934969536558836,
        "rmse": 0.9934969536558836
    },
    "train_stats": {
        "score": -1.0206471878648198,
        "rmse": 1.0206471878648198
    },
    "val_stats": {
        "score": -0.882541487958959,
        "rmse": 0.882541487958959
    }
}
[08/27/2025 16:41:07 INFO]: Procewss finished for trial maddest-Elbert_trial_21
[08/27/2025 16:41:07 INFO]: 
_________________________________________________

[08/27/2025 16:41:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 16:41:07 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 8
  d_ffn_factor: 1.2222283790040582
  attention_dropout: 0.007455075969360132
  ffn_dropout: 0.007455075969360132
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.5036608525315066e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_35

[08/27/2025 16:41:07 INFO]: This ft_transformer has 6.932 million parameters.
[08/27/2025 16:41:07 INFO]: Training will start at epoch 0.
[08/27/2025 16:41:07 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 16:41:12 INFO]: Training loss at epoch 74: 0.82131028175354
[08/27/2025 16:41:39 INFO]: Training loss at epoch 7: 0.9408598244190216
[08/27/2025 16:42:01 INFO]: Training loss at epoch 20: 0.9762127697467804
[08/27/2025 16:42:31 INFO]: Training loss at epoch 40: 0.8742324709892273
[08/27/2025 16:42:34 INFO]: New best epoch, val score: -0.8807510310737372
[08/27/2025 16:42:34 INFO]: Saving model to: maddest-Elbert_trial_31/model_best.pth
[08/27/2025 16:42:48 INFO]: Training loss at epoch 11: 0.7888558804988861
[08/27/2025 16:42:49 INFO]: Training loss at epoch 75: 1.0925028324127197
[08/27/2025 16:43:05 INFO]: Training loss at epoch 40: 0.8903519213199615
[08/27/2025 16:43:31 INFO]: Training loss at epoch 21: 1.1347864270210266
[08/27/2025 16:43:39 INFO]: Training loss at epoch 30: 0.9160360991954803
[08/27/2025 16:43:41 INFO]: Training loss at epoch 10: 1.1637731790542603
[08/27/2025 16:44:08 INFO]: Training loss at epoch 7: 1.1796820163726807
[08/27/2025 16:44:12 INFO]: New best epoch, val score: -0.8815447048075988
[08/27/2025 16:44:12 INFO]: Saving model to: maddest-Elbert_trial_12/model_best.pth
[08/27/2025 16:44:17 INFO]: Training loss at epoch 41: 1.154143899679184
[08/27/2025 16:44:18 INFO]: Training loss at epoch 27: 0.9865188002586365
[08/27/2025 16:44:20 INFO]: Training loss at epoch 13: 1.1704005599021912
[08/27/2025 16:44:26 INFO]: Training loss at epoch 76: 1.141630470752716
[08/27/2025 16:44:31 INFO]: Training loss at epoch 23: 0.8763624131679535
[08/27/2025 16:44:33 INFO]: Training loss at epoch 0: 1.37051123380661
[08/27/2025 16:44:53 INFO]: Training loss at epoch 16: 0.8368876576423645
[08/27/2025 16:44:54 INFO]: Training loss at epoch 29: 1.0342288315296173
[08/27/2025 16:45:01 INFO]: New best epoch, val score: -1.2660403444666224
[08/27/2025 16:45:01 INFO]: Saving model to: maddest-Elbert_trial_35/model_best.pth
[08/27/2025 16:45:10 INFO]: Training loss at epoch 5: 1.088912010192871
[08/27/2025 16:45:25 INFO]: Training loss at epoch 19: 0.9552917182445526
[08/27/2025 16:45:56 INFO]: Training loss at epoch 20: 1.0455257892608643
[08/27/2025 16:46:03 INFO]: Training loss at epoch 77: 0.9760590493679047
[08/27/2025 16:46:03 INFO]: Training loss at epoch 18: 1.0097363591194153
[08/27/2025 16:46:04 INFO]: Training loss at epoch 42: 0.8445208072662354
[08/27/2025 16:46:06 INFO]: Training loss at epoch 41: 1.580488920211792
[08/27/2025 16:46:21 INFO]: Training stats: {
    "score": -0.9998362914437611,
    "rmse": 0.9998362914437611
}
[08/27/2025 16:46:21 INFO]: Val stats: {
    "score": -0.9182747343795122,
    "rmse": 0.9182747343795122
}
[08/27/2025 16:46:21 INFO]: Test stats: {
    "score": -0.9717139809264848,
    "rmse": 0.9717139809264848
}
[08/27/2025 16:46:50 INFO]: Training loss at epoch 31: 1.0194450914859772
[08/27/2025 16:46:55 INFO]: Training loss at epoch 12: 1.1324206292629242
[08/27/2025 16:47:08 INFO]: Training stats: {
    "score": -1.0016806998304184,
    "rmse": 1.0016806998304184
}
[08/27/2025 16:47:08 INFO]: Val stats: {
    "score": -0.9439915241512744,
    "rmse": 0.9439915241512744
}
[08/27/2025 16:47:08 INFO]: Test stats: {
    "score": -0.9737925812924982,
    "rmse": 0.9737925812924982
}
[08/27/2025 16:47:41 INFO]: Training loss at epoch 78: 0.8833937644958496
[08/27/2025 16:47:51 INFO]: Training loss at epoch 43: 0.9780780076980591
[08/27/2025 16:47:52 INFO]: Training loss at epoch 21: 0.9776372313499451
[08/27/2025 16:47:55 INFO]: Running Final Evaluation...
[08/27/2025 16:48:28 INFO]: Training loss at epoch 14: 1.133371353149414
[08/27/2025 16:48:30 INFO]: Training loss at epoch 1: 1.066557228565216
[08/27/2025 16:48:33 INFO]: Training accuracy: {
    "score": -1.0027168427028246,
    "rmse": 1.0027168427028246
}
[08/27/2025 16:48:33 INFO]: Val accuracy: {
    "score": -0.9028784222237792,
    "rmse": 0.9028784222237792
}
[08/27/2025 16:48:33 INFO]: Test accuracy: {
    "score": -0.9769770313985006,
    "rmse": 0.9769770313985006
}
[08/27/2025 16:48:33 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_13",
    "best_epoch": 47,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9769770313985006,
        "rmse": 0.9769770313985006
    },
    "train_stats": {
        "score": -1.0027168427028246,
        "rmse": 1.0027168427028246
    },
    "val_stats": {
        "score": -0.9028784222237792,
        "rmse": 0.9028784222237792
    }
}
[08/27/2025 16:48:33 INFO]: Procewss finished for trial maddest-Elbert_trial_13
[08/27/2025 16:48:34 INFO]: 
_________________________________________________

[08/27/2025 16:48:34 INFO]: train_net_for_optune.py main() running.
[08/27/2025 16:48:34 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 1.2034010692949177
  attention_dropout: 0.010480950165394798
  ffn_dropout: 0.010480950165394798
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.280526032602009e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_36

[08/27/2025 16:48:34 INFO]: This ft_transformer has 6.884 million parameters.
[08/27/2025 16:48:34 INFO]: Training will start at epoch 0.
[08/27/2025 16:48:34 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 16:48:48 INFO]: Training loss at epoch 28: 1.0223750472068787
[08/27/2025 16:48:58 INFO]: New best epoch, val score: -0.9147270528186049
[08/27/2025 16:48:58 INFO]: Saving model to: maddest-Elbert_trial_35/model_best.pth
[08/27/2025 16:49:07 INFO]: Training loss at epoch 22: 1.0928730964660645
[08/27/2025 16:49:08 INFO]: Training loss at epoch 42: 0.9691670536994934
[08/27/2025 16:49:11 INFO]: Training loss at epoch 8: 0.9470154047012329
[08/27/2025 16:49:18 INFO]: Training loss at epoch 6: 1.1638966798782349
[08/27/2025 16:49:23 INFO]: Training loss at epoch 24: 0.9610326290130615
[08/27/2025 16:49:37 INFO]: Training loss at epoch 44: 0.8297052681446075
[08/27/2025 16:50:01 INFO]: Training loss at epoch 32: 0.9440317153930664
[08/27/2025 16:50:06 INFO]: New best epoch, val score: -0.880493047605885
[08/27/2025 16:50:06 INFO]: Saving model to: maddest-Elbert_trial_31/model_best.pth
[08/27/2025 16:50:33 INFO]: Training loss at epoch 30: 0.9800700843334198
[08/27/2025 16:51:04 INFO]: Training loss at epoch 13: 1.064891278743744
[08/27/2025 16:51:23 INFO]: Training loss at epoch 45: 1.0226125121116638
[08/27/2025 16:51:44 INFO]: Training loss at epoch 0: 0.915516585111618
[08/27/2025 16:51:57 INFO]: Training loss at epoch 21: 0.9085999727249146
[08/27/2025 16:52:06 INFO]: Training loss at epoch 20: 1.0690400898456573
[08/27/2025 16:52:09 INFO]: Training loss at epoch 43: 1.1443070769309998
[08/27/2025 16:52:09 INFO]: New best epoch, val score: -0.9241100988575031
[08/27/2025 16:52:09 INFO]: Saving model to: maddest-Elbert_trial_36/model_best.pth
[08/27/2025 16:52:23 INFO]: Training loss at epoch 17: 1.0128703713417053
[08/27/2025 16:52:26 INFO]: Training loss at epoch 2: 1.460071861743927
[08/27/2025 16:52:33 INFO]: Training loss at epoch 19: 1.1276526749134064
[08/27/2025 16:52:35 INFO]: Training loss at epoch 15: 0.9972091913223267
[08/27/2025 16:52:39 INFO]: Training loss at epoch 8: 1.3397956490516663
[08/27/2025 16:53:09 INFO]: Training loss at epoch 46: 0.8952168226242065
[08/27/2025 16:53:13 INFO]: Training loss at epoch 33: 0.9656936526298523
[08/27/2025 16:53:16 INFO]: Training loss at epoch 29: 1.2615419328212738
[08/27/2025 16:53:28 INFO]: Training loss at epoch 7: 1.1233506798744202
[08/27/2025 16:53:41 INFO]: Training loss at epoch 22: 0.9731805920600891
[08/27/2025 16:54:18 INFO]: Training loss at epoch 25: 0.9792812168598175
[08/27/2025 16:54:24 INFO]: New best epoch, val score: -0.8916205106500017
[08/27/2025 16:54:24 INFO]: Saving model to: maddest-Elbert_trial_1/model_best.pth
[08/27/2025 16:54:43 INFO]: Training stats: {
    "score": -1.000867787043964,
    "rmse": 1.000867787043964
}
[08/27/2025 16:54:43 INFO]: Val stats: {
    "score": -0.9095472234431011,
    "rmse": 0.9095472234431011
}
[08/27/2025 16:54:43 INFO]: Test stats: {
    "score": -0.9729442294453068,
    "rmse": 0.9729442294453068
}
[08/27/2025 16:54:46 INFO]: Training loss at epoch 23: 0.9746799767017365
[08/27/2025 16:54:47 INFO]: Training loss at epoch 31: 0.9226167500019073
[08/27/2025 16:54:51 INFO]: Training stats: {
    "score": -1.0025777253525832,
    "rmse": 1.0025777253525832
}
[08/27/2025 16:54:51 INFO]: Val stats: {
    "score": -0.9491114953585644,
    "rmse": 0.9491114953585644
}
[08/27/2025 16:54:51 INFO]: Test stats: {
    "score": -0.974432474563366,
    "rmse": 0.974432474563366
}
[08/27/2025 16:54:55 INFO]: Training loss at epoch 47: 0.9883172810077667
[08/27/2025 16:55:12 INFO]: Training loss at epoch 44: 0.9142449498176575
[08/27/2025 16:55:14 INFO]: Training loss at epoch 11: 0.9542339742183685
[08/27/2025 16:55:14 INFO]: Training loss at epoch 14: 0.967376708984375
[08/27/2025 16:55:22 INFO]: Training loss at epoch 1: 0.8303177058696747
[08/27/2025 16:55:47 INFO]: New best epoch, val score: -0.9035568090038931
[08/27/2025 16:55:47 INFO]: Saving model to: maddest-Elbert_trial_36/model_best.pth
[08/27/2025 16:56:24 INFO]: Training loss at epoch 3: 1.3739013671875
[08/27/2025 16:56:26 INFO]: Training loss at epoch 34: 0.8499028384685516
[08/27/2025 16:56:36 INFO]: Training loss at epoch 9: 1.2336501479148865
[08/27/2025 16:56:42 INFO]: Training loss at epoch 48: 1.099732905626297
[08/27/2025 16:56:44 INFO]: Training loss at epoch 16: 1.0423922538757324
[08/27/2025 16:56:52 INFO]: New best epoch, val score: -0.9067913340196476
[08/27/2025 16:56:52 INFO]: Saving model to: maddest-Elbert_trial_35/model_best.pth
[08/27/2025 16:57:06 INFO]: Training loss at epoch 21: 1.1621050834655762
[08/27/2025 16:57:37 INFO]: Training loss at epoch 8: 0.998929888010025
[08/27/2025 16:58:01 INFO]: Training loss at epoch 22: 0.9279856383800507
[08/27/2025 16:58:12 INFO]: Training loss at epoch 45: 1.1122138500213623
[08/27/2025 16:58:28 INFO]: Training loss at epoch 49: 1.1719550490379333
[08/27/2025 16:58:58 INFO]: Training loss at epoch 2: 0.9022152423858643
[08/27/2025 16:59:00 INFO]: Training loss at epoch 32: 1.213384360074997
[08/27/2025 16:59:07 INFO]: Training stats: {
    "score": -0.9961742983627079,
    "rmse": 0.9961742983627079
}
[08/27/2025 16:59:07 INFO]: Val stats: {
    "score": -0.9066146820623598,
    "rmse": 0.9066146820623598
}
[08/27/2025 16:59:07 INFO]: Test stats: {
    "score": -0.9715117737042275,
    "rmse": 0.9715117737042275
}
[08/27/2025 16:59:10 INFO]: Training loss at epoch 26: 1.0510135889053345
[08/27/2025 16:59:11 INFO]: Training stats: {
    "score": -1.0254874377795737,
    "rmse": 1.0254874377795737
}
[08/27/2025 16:59:11 INFO]: Val stats: {
    "score": -0.8815003404393128,
    "rmse": 0.8815003404393128
}
[08/27/2025 16:59:11 INFO]: Test stats: {
    "score": -0.9984967960995942,
    "rmse": 0.9984967960995942
}
[08/27/2025 16:59:21 INFO]: Training loss at epoch 30: 0.8522604703903198
[08/27/2025 16:59:22 INFO]: Training loss at epoch 15: 1.2531859874725342
[08/27/2025 16:59:23 INFO]: New best epoch, val score: -0.8973493842297077
[08/27/2025 16:59:23 INFO]: Saving model to: maddest-Elbert_trial_36/model_best.pth
[08/27/2025 16:59:33 INFO]: Training loss at epoch 23: 1.1406536996364594
[08/27/2025 16:59:36 INFO]: Training loss at epoch 35: 0.9541719555854797
[08/27/2025 16:59:55 INFO]: Training loss at epoch 18: 1.157140076160431
[08/27/2025 17:00:14 INFO]: New best epoch, val score: -0.8906271789618415
[08/27/2025 17:00:14 INFO]: Saving model to: maddest-Elbert_trial_1/model_best.pth
[08/27/2025 17:00:20 INFO]: Training loss at epoch 4: 0.9483288526535034
[08/27/2025 17:00:21 INFO]: Training loss at epoch 24: 0.9924524426460266
[08/27/2025 17:00:51 INFO]: Training loss at epoch 17: 1.0039097368717194
[08/27/2025 17:00:52 INFO]: Training loss at epoch 50: 0.8800481855869293
[08/27/2025 17:01:08 INFO]: Training loss at epoch 9: 1.461672842502594
[08/27/2025 17:01:11 INFO]: Training loss at epoch 20: 1.0266418755054474
[08/27/2025 17:01:14 INFO]: Training loss at epoch 46: 0.8476817905902863
[08/27/2025 17:01:37 INFO]: New best epoch, val score: -0.8858084968573517
[08/27/2025 17:01:37 INFO]: Saving model to: maddest-Elbert_trial_8/model_best.pth
[08/27/2025 17:01:46 INFO]: Training loss at epoch 9: 1.1866472959518433
[08/27/2025 17:02:04 INFO]: Training loss at epoch 22: 1.126385748386383
[08/27/2025 17:02:34 INFO]: Training loss at epoch 3: 1.0876520276069641
[08/27/2025 17:02:38 INFO]: Training loss at epoch 51: 1.2035638689994812
[08/27/2025 17:02:46 INFO]: Training loss at epoch 36: 1.0226995348930359
[08/27/2025 17:03:12 INFO]: Training loss at epoch 33: 1.0322043895721436
[08/27/2025 17:03:12 INFO]: Training stats: {
    "score": -1.0324669066952405,
    "rmse": 1.0324669066952405
}
[08/27/2025 17:03:12 INFO]: Val stats: {
    "score": -1.0305484722222453,
    "rmse": 1.0305484722222453
}
[08/27/2025 17:03:12 INFO]: Test stats: {
    "score": -1.0062946205299443,
    "rmse": 1.0062946205299443
}
[08/27/2025 17:03:29 INFO]: Training loss at epoch 16: 1.054681420326233
[08/27/2025 17:03:50 INFO]: Training loss at epoch 31: 1.2407397329807281
[08/27/2025 17:04:01 INFO]: Training stats: {
    "score": -1.1283477586595845,
    "rmse": 1.1283477586595845
}
[08/27/2025 17:04:01 INFO]: Val stats: {
    "score": -0.9169211569463073,
    "rmse": 0.9169211569463073
}
[08/27/2025 17:04:01 INFO]: Test stats: {
    "score": -1.102617954051591,
    "rmse": 1.102617954051591
}
[08/27/2025 17:04:02 INFO]: Training loss at epoch 27: 1.0857125520706177
[08/27/2025 17:04:02 INFO]: Training loss at epoch 23: 1.0055908262729645
[08/27/2025 17:04:15 INFO]: Training loss at epoch 5: 1.1945849657058716
[08/27/2025 17:04:15 INFO]: Training loss at epoch 47: 1.148765742778778
[08/27/2025 17:04:22 INFO]: Training loss at epoch 52: 1.081916093826294
[08/27/2025 17:04:37 INFO]: New best epoch, val score: -0.885235924617268
[08/27/2025 17:04:37 INFO]: Saving model to: maddest-Elbert_trial_8/model_best.pth
[08/27/2025 17:04:59 INFO]: Training loss at epoch 18: 0.9217808246612549
[08/27/2025 17:05:22 INFO]: Training loss at epoch 24: 0.8606185615062714
[08/27/2025 17:05:57 INFO]: Training loss at epoch 37: 1.03097864985466
[08/27/2025 17:05:57 INFO]: Training loss at epoch 25: 1.2077887654304504
[08/27/2025 17:06:07 INFO]: Training loss at epoch 53: 1.1072118878364563
[08/27/2025 17:06:08 INFO]: Training loss at epoch 4: 1.142015129327774
[08/27/2025 17:06:21 INFO]: Running Final Evaluation...
[08/27/2025 17:06:32 INFO]: Training loss at epoch 10: 0.8280379176139832
[08/27/2025 17:06:38 INFO]: Training loss at epoch 12: 0.9028898775577545
[08/27/2025 17:06:59 INFO]: Training loss at epoch 23: 1.0367067456245422
[08/27/2025 17:07:01 INFO]: Training accuracy: {
    "score": -0.9984733491413597,
    "rmse": 0.9984733491413597
}
[08/27/2025 17:07:01 INFO]: Val accuracy: {
    "score": -0.9025522444831586,
    "rmse": 0.9025522444831586
}
[08/27/2025 17:07:01 INFO]: Test accuracy: {
    "score": -0.9728827229139941,
    "rmse": 0.9728827229139941
}
[08/27/2025 17:07:01 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_29",
    "best_epoch": 22,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9728827229139941,
        "rmse": 0.9728827229139941
    },
    "train_stats": {
        "score": -0.9984733491413597,
        "rmse": 0.9984733491413597
    },
    "val_stats": {
        "score": -0.9025522444831586,
        "rmse": 0.9025522444831586
    }
}
[08/27/2025 17:07:01 INFO]: Procewss finished for trial maddest-Elbert_trial_29
[08/27/2025 17:07:01 INFO]: 
_________________________________________________

[08/27/2025 17:07:01 INFO]: train_net_for_optune.py main() running.
[08/27/2025 17:07:01 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 1.2564332591072704
  attention_dropout: 0.030389322704794747
  ffn_dropout: 0.030389322704794747
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.348746961715476e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_37

[08/27/2025 17:07:02 INFO]: This ft_transformer has 4.723 million parameters.
[08/27/2025 17:07:02 INFO]: Training will start at epoch 0.
[08/27/2025 17:07:02 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 17:07:17 INFO]: Training loss at epoch 48: 1.0626044571399689
[08/27/2025 17:07:20 INFO]: Training loss at epoch 10: 0.9240513741970062
[08/27/2025 17:07:23 INFO]: Training loss at epoch 34: 0.9333160817623138
[08/27/2025 17:07:26 INFO]: Training loss at epoch 19: 1.0676854252815247
[08/27/2025 17:07:37 INFO]: Training loss at epoch 21: 0.8898829817771912
[08/27/2025 17:07:37 INFO]: Training loss at epoch 17: 1.1152400374412537
[08/27/2025 17:08:11 INFO]: Training loss at epoch 6: 1.135981559753418
[08/27/2025 17:08:19 INFO]: Training loss at epoch 32: 1.1223803162574768
[08/27/2025 17:08:52 INFO]: Training loss at epoch 28: 0.9174414575099945
[08/27/2025 17:09:03 INFO]: Training loss at epoch 0: 1.3688080310821533
[08/27/2025 17:09:07 INFO]: Training loss at epoch 19: 1.0300681591033936
[08/27/2025 17:09:08 INFO]: Training loss at epoch 38: 1.111100673675537
[08/27/2025 17:09:20 INFO]: New best epoch, val score: -0.8898048579631835
[08/27/2025 17:09:20 INFO]: Saving model to: maddest-Elbert_trial_37/model_best.pth
[08/27/2025 17:09:43 INFO]: Training loss at epoch 5: 0.9725633561611176
[08/27/2025 17:10:02 INFO]: Training stats: {
    "score": -1.001002296417491,
    "rmse": 1.001002296417491
}
[08/27/2025 17:10:02 INFO]: Val stats: {
    "score": -0.9069364305735556,
    "rmse": 0.9069364305735556
}
[08/27/2025 17:10:02 INFO]: Test stats: {
    "score": -0.9734106531371985,
    "rmse": 0.9734106531371985
}
[08/27/2025 17:10:04 INFO]: Training loss at epoch 24: 1.0850918889045715
[08/27/2025 17:10:19 INFO]: Training loss at epoch 49: 0.8909587860107422
[08/27/2025 17:10:35 INFO]: Training stats: {
    "score": -1.004870792045495,
    "rmse": 1.004870792045495
}
[08/27/2025 17:10:35 INFO]: Val stats: {
    "score": -0.9844394453811743,
    "rmse": 0.9844394453811743
}
[08/27/2025 17:10:35 INFO]: Test stats: {
    "score": -0.9861349419803721,
    "rmse": 0.9861349419803721
}
[08/27/2025 17:11:13 INFO]: Training loss at epoch 25: 0.8782427310943604
[08/27/2025 17:11:20 INFO]: Training loss at epoch 1: 1.0083057284355164
[08/27/2025 17:11:20 INFO]: Training stats: {
    "score": -0.9481410484569286,
    "rmse": 0.9481410484569286
}
[08/27/2025 17:11:20 INFO]: Val stats: {
    "score": -0.9595844079122756,
    "rmse": 0.9595844079122756
}
[08/27/2025 17:11:20 INFO]: Test stats: {
    "score": -0.9793379413963881,
    "rmse": 0.9793379413963881
}
[08/27/2025 17:11:29 INFO]: Training loss at epoch 11: 1.1648336052894592
[08/27/2025 17:11:36 INFO]: Training loss at epoch 26: 1.0396894216537476
[08/27/2025 17:11:37 INFO]: Training loss at epoch 35: 1.0182741284370422
[08/27/2025 17:11:46 INFO]: Training loss at epoch 18: 1.2133033275604248
[08/27/2025 17:11:59 INFO]: Training loss at epoch 24: 0.9953857362270355
[08/27/2025 17:12:06 INFO]: Training loss at epoch 7: 1.1711406111717224
[08/27/2025 17:12:20 INFO]: Training loss at epoch 39: 0.9643238484859467
[08/27/2025 17:12:30 INFO]: Training loss at epoch 10: 1.5248692035675049
[08/27/2025 17:12:50 INFO]: Training loss at epoch 33: 0.9254887104034424
[08/27/2025 17:13:20 INFO]: Training loss at epoch 6: 1.0792410373687744
[08/27/2025 17:13:24 INFO]: Training stats: {
    "score": -1.0014588920116787,
    "rmse": 1.0014588920116787
}
[08/27/2025 17:13:24 INFO]: Val stats: {
    "score": -0.9580867881094197,
    "rmse": 0.9580867881094197
}
[08/27/2025 17:13:24 INFO]: Test stats: {
    "score": -0.9735210973389969,
    "rmse": 0.9735210973389969
}
[08/27/2025 17:13:38 INFO]: Training loss at epoch 2: 1.0551015138626099
[08/27/2025 17:13:44 INFO]: Training loss at epoch 29: 1.114885151386261
[08/27/2025 17:13:56 INFO]: Training loss at epoch 11: 0.9944694340229034
[08/27/2025 17:14:06 INFO]: Training loss at epoch 22: 1.0254517793655396
[08/27/2025 17:14:23 INFO]: Training loss at epoch 50: 0.7113356292247772
[08/27/2025 17:14:45 INFO]: Training loss at epoch 20: 0.8815841972827911
[08/27/2025 17:15:23 INFO]: Training stats: {
    "score": -0.9924617616376824,
    "rmse": 0.9924617616376824
}
[08/27/2025 17:15:23 INFO]: Val stats: {
    "score": -0.9265512682489455,
    "rmse": 0.9265512682489455
}
[08/27/2025 17:15:23 INFO]: Test stats: {
    "score": -0.9703252275269403,
    "rmse": 0.9703252275269403
}
[08/27/2025 17:15:37 INFO]: Training loss at epoch 12: 0.809393584728241
[08/27/2025 17:15:49 INFO]: Training loss at epoch 36: 1.128858506679535
[08/27/2025 17:15:54 INFO]: Training loss at epoch 3: 1.2194362878799438
[08/27/2025 17:15:54 INFO]: Training loss at epoch 19: 1.0283498466014862
[08/27/2025 17:16:01 INFO]: Training loss at epoch 8: 1.0582375824451447
[08/27/2025 17:16:07 INFO]: Training loss at epoch 25: 1.014934480190277
[08/27/2025 17:16:08 INFO]: New best epoch, val score: -0.8834545791107867
[08/27/2025 17:16:08 INFO]: Saving model to: maddest-Elbert_trial_34/model_best.pth
[08/27/2025 17:16:10 INFO]: New best epoch, val score: -0.876833914373953
[08/27/2025 17:16:10 INFO]: Saving model to: maddest-Elbert_trial_37/model_best.pth
[08/27/2025 17:16:34 INFO]: Training loss at epoch 40: 0.8309857249259949
[08/27/2025 17:16:54 INFO]: Training loss at epoch 7: 1.2714876532554626
[08/27/2025 17:16:59 INFO]: Training loss at epoch 25: 0.9103794395923615
[08/27/2025 17:17:02 INFO]: Training loss at epoch 26: 1.100362479686737
[08/27/2025 17:17:11 INFO]: Training loss at epoch 27: 0.8598348200321198
[08/27/2025 17:17:17 INFO]: Training loss at epoch 34: 1.0376533269882202
[08/27/2025 17:17:22 INFO]: Training stats: {
    "score": -0.9996492424953548,
    "rmse": 0.9996492424953548
}
[08/27/2025 17:17:22 INFO]: Val stats: {
    "score": -0.901127606018028,
    "rmse": 0.901127606018028
}
[08/27/2025 17:17:22 INFO]: Test stats: {
    "score": -0.9759144837293245,
    "rmse": 0.9759144837293245
}
[08/27/2025 17:17:24 INFO]: Training loss at epoch 51: 0.9130559861660004
[08/27/2025 17:17:31 INFO]: Training loss at epoch 20: 1.1072862148284912
[08/27/2025 17:18:04 INFO]: Training loss at epoch 13: 1.0005081593990326
[08/27/2025 17:18:10 INFO]: Training loss at epoch 4: 1.1044312715530396
[08/27/2025 17:18:52 INFO]: Training loss at epoch 21: 0.9835290610790253
[08/27/2025 17:19:44 INFO]: Training loss at epoch 13: 0.9937763214111328
[08/27/2025 17:19:44 INFO]: Training loss at epoch 41: 1.0571970343589783
[08/27/2025 17:19:54 INFO]: Training loss at epoch 9: 1.0620938539505005
[08/27/2025 17:20:02 INFO]: Training loss at epoch 37: 0.8891098201274872
[08/27/2025 17:20:14 INFO]: Training loss at epoch 30: 0.8299345970153809
[08/27/2025 17:20:15 INFO]: New best epoch, val score: -0.8803470668612654
[08/27/2025 17:20:15 INFO]: Saving model to: maddest-Elbert_trial_34/model_best.pth
[08/27/2025 17:20:24 INFO]: Training loss at epoch 52: 1.090777188539505
[08/27/2025 17:20:26 INFO]: Training loss at epoch 5: 0.9181119203567505
[08/27/2025 17:20:30 INFO]: Training loss at epoch 8: 1.069100022315979
[08/27/2025 17:20:32 INFO]: Training loss at epoch 23: 0.857489675283432
[08/27/2025 17:20:57 INFO]: Training loss at epoch 11: 1.0883046984672546
[08/27/2025 17:21:15 INFO]: Training stats: {
    "score": -1.0386315681365599,
    "rmse": 1.0386315681365599
}
[08/27/2025 17:21:15 INFO]: Val stats: {
    "score": -1.0603557154843035,
    "rmse": 1.0603557154843035
}
[08/27/2025 17:21:15 INFO]: Test stats: {
    "score": -1.019852088340988,
    "rmse": 1.019852088340988
}
[08/27/2025 17:21:17 INFO]: Training loss at epoch 12: 1.241989016532898
[08/27/2025 17:21:30 INFO]: Training loss at epoch 20: 0.8814526796340942
[08/27/2025 17:21:46 INFO]: Training loss at epoch 35: 1.1268154978752136
[08/27/2025 17:21:59 INFO]: Training loss at epoch 26: 0.7981997430324554
[08/27/2025 17:22:08 INFO]: Training loss at epoch 26: 1.0029895305633545
[08/27/2025 17:22:43 INFO]: Training loss at epoch 6: 1.0386670529842377
[08/27/2025 17:22:48 INFO]: Training loss at epoch 28: 0.9424348771572113
[08/27/2025 17:22:52 INFO]: Training loss at epoch 27: 0.8896298706531525
[08/27/2025 17:22:55 INFO]: Training loss at epoch 42: 0.8866177201271057
[08/27/2025 17:23:00 INFO]: Training loss at epoch 22: 0.8870304822921753
[08/27/2025 17:23:26 INFO]: Training loss at epoch 53: 0.7999782264232635
[08/27/2025 17:23:52 INFO]: Training loss at epoch 14: 1.1203843355178833
[08/27/2025 17:24:06 INFO]: Training loss at epoch 9: 1.198596864938736
[08/27/2025 17:24:15 INFO]: Training loss at epoch 38: 1.0295013785362244
[08/27/2025 17:24:24 INFO]: New best epoch, val score: -0.8803461766329237
[08/27/2025 17:24:24 INFO]: Saving model to: maddest-Elbert_trial_34/model_best.pth
[08/27/2025 17:25:00 INFO]: Training loss at epoch 7: 1.0230988264083862
[08/27/2025 17:25:02 INFO]: Training loss at epoch 21: 0.862969309091568
[08/27/2025 17:25:09 INFO]: Training loss at epoch 31: 1.199452430009842
[08/27/2025 17:25:11 INFO]: Training loss at epoch 10: 1.1246290802955627
[08/27/2025 17:25:20 INFO]: Training stats: {
    "score": -0.9730938321916411,
    "rmse": 0.9730938321916411
}
[08/27/2025 17:25:20 INFO]: Val stats: {
    "score": -0.9128080070850367,
    "rmse": 0.9128080070850367
}
[08/27/2025 17:25:20 INFO]: Test stats: {
    "score": -0.9880805710182836,
    "rmse": 0.9880805710182836
}
[08/27/2025 17:25:40 INFO]: Training loss at epoch 21: 1.0926586389541626
[08/27/2025 17:26:08 INFO]: Training loss at epoch 43: 1.2487483024597168
[08/27/2025 17:26:14 INFO]: Training loss at epoch 36: 1.0532939732074738
[08/27/2025 17:26:29 INFO]: Running Final Evaluation...
[08/27/2025 17:26:30 INFO]: Training loss at epoch 54: 0.9972845315933228
[08/27/2025 17:26:59 INFO]: Training loss at epoch 24: 1.1377328932285309
[08/27/2025 17:27:00 INFO]: Training loss at epoch 27: 0.9167343080043793
[08/27/2025 17:27:13 INFO]: Training loss at epoch 23: 1.0728617906570435
[08/27/2025 17:27:16 INFO]: Training loss at epoch 8: 1.0541456937789917
[08/27/2025 17:27:36 INFO]: Training accuracy: {
    "score": -1.0378739201417198,
    "rmse": 1.0378739201417198
}
[08/27/2025 17:27:36 INFO]: Val accuracy: {
    "score": -0.8829077344575449,
    "rmse": 0.8829077344575449
}
[08/27/2025 17:27:36 INFO]: Test accuracy: {
    "score": -1.0056694825863144,
    "rmse": 1.0056694825863144
}
[08/27/2025 17:27:36 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_24",
    "best_epoch": 12,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0056694825863144,
        "rmse": 1.0056694825863144
    },
    "train_stats": {
        "score": -1.0378739201417198,
        "rmse": 1.0378739201417198
    },
    "val_stats": {
        "score": -0.8829077344575449,
        "rmse": 0.8829077344575449
    }
}
[08/27/2025 17:27:36 INFO]: Procewss finished for trial maddest-Elbert_trial_24
[08/27/2025 17:27:37 INFO]: 
_________________________________________________

[08/27/2025 17:27:37 INFO]: train_net_for_optune.py main() running.
[08/27/2025 17:27:37 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 1.3553503480033562
  attention_dropout: 0.006591550748905814
  ffn_dropout: 0.006591550748905814
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.7436494739315707e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_38

[08/27/2025 17:27:37 INFO]: This ft_transformer has 4.899 million parameters.
[08/27/2025 17:27:37 INFO]: Training will start at epoch 0.
[08/27/2025 17:27:37 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 17:28:02 INFO]: Training loss at epoch 15: 1.0797373056411743
[08/27/2025 17:28:11 INFO]: Training loss at epoch 27: 1.0126494765281677
[08/27/2025 17:28:24 INFO]: Training loss at epoch 29: 1.0023082196712494
[08/27/2025 17:28:28 INFO]: Training loss at epoch 39: 1.0653546750545502
[08/27/2025 17:28:38 INFO]: Training loss at epoch 13: 1.1707237362861633
[08/27/2025 17:28:41 INFO]: Training loss at epoch 28: 0.7590721845626831
[08/27/2025 17:28:54 INFO]: Training loss at epoch 10: 1.4237593710422516
[08/27/2025 17:29:05 INFO]: Training loss at epoch 11: 0.8905845880508423
[08/27/2025 17:29:26 INFO]: Training loss at epoch 12: 1.2545884251594543
[08/27/2025 17:29:29 INFO]: Training loss at epoch 14: 0.8343806862831116
[08/27/2025 17:29:31 INFO]: Training loss at epoch 55: 0.8564257323741913
[08/27/2025 17:29:32 INFO]: Training loss at epoch 9: 0.837069183588028
[08/27/2025 17:29:33 INFO]: New best epoch, val score: -0.8998284411830546
[08/27/2025 17:29:33 INFO]: Saving model to: maddest-Elbert_trial_35/model_best.pth
[08/27/2025 17:29:48 INFO]: Training loss at epoch 0: 1.081261307001114
[08/27/2025 17:29:48 INFO]: Training loss at epoch 22: 1.111122488975525
[08/27/2025 17:29:58 INFO]: Training stats: {
    "score": -1.0002133430770872,
    "rmse": 1.0002133430770872
}
[08/27/2025 17:29:58 INFO]: Val stats: {
    "score": -0.9336322523679073,
    "rmse": 0.9336322523679073
}
[08/27/2025 17:29:58 INFO]: Test stats: {
    "score": -0.9721901833025808,
    "rmse": 0.9721901833025808
}
[08/27/2025 17:30:02 INFO]: Training loss at epoch 32: 0.92188361287117
[08/27/2025 17:30:05 INFO]: New best epoch, val score: -0.8993425138813733
[08/27/2025 17:30:05 INFO]: Saving model to: maddest-Elbert_trial_38/model_best.pth
[08/27/2025 17:30:18 INFO]: Training stats: {
    "score": -0.97920937965436,
    "rmse": 0.97920937965436
}
[08/27/2025 17:30:18 INFO]: Val stats: {
    "score": -0.9103912410532227,
    "rmse": 0.9103912410532227
}
[08/27/2025 17:30:18 INFO]: Test stats: {
    "score": -0.9758060495689056,
    "rmse": 0.9758060495689056
}
[08/27/2025 17:30:23 INFO]: Training stats: {
    "score": -1.0000262026554776,
    "rmse": 1.0000262026554776
}
[08/27/2025 17:30:23 INFO]: Val stats: {
    "score": -0.9176531750075743,
    "rmse": 0.9176531750075743
}
[08/27/2025 17:30:23 INFO]: Test stats: {
    "score": -0.9715457287097223,
    "rmse": 0.9715457287097223
}
[08/27/2025 17:30:42 INFO]: Training loss at epoch 37: 0.9451408982276917
[08/27/2025 17:31:23 INFO]: Training loss at epoch 24: 0.9384172558784485
[08/27/2025 17:32:02 INFO]: Training loss at epoch 28: 1.0517150163650513
[08/27/2025 17:32:10 INFO]: Training loss at epoch 16: 0.8609326183795929
[08/27/2025 17:32:20 INFO]: Training loss at epoch 1: 1.0551223158836365
[08/27/2025 17:32:31 INFO]: Training loss at epoch 11: 0.9945298135280609
[08/27/2025 17:32:32 INFO]: Training loss at epoch 22: 0.8614530265331268
[08/27/2025 17:32:33 INFO]: Training loss at epoch 56: 0.9261435568332672
[08/27/2025 17:32:35 INFO]: Training loss at epoch 10: 0.9187877178192139
[08/27/2025 17:33:02 INFO]: Training loss at epoch 12: 0.8242102861404419
[08/27/2025 17:33:29 INFO]: Training loss at epoch 25: 0.959631085395813
[08/27/2025 17:33:31 INFO]: New best epoch, val score: -0.8831866127350881
[08/27/2025 17:33:31 INFO]: Saving model to: maddest-Elbert_trial_35/model_best.pth
[08/27/2025 17:33:58 INFO]: Training loss at epoch 23: 0.9570735692977905
[08/27/2025 17:34:14 INFO]: Training loss at epoch 40: 0.9947435557842255
[08/27/2025 17:34:15 INFO]: Training loss at epoch 28: 0.9494037926197052
[08/27/2025 17:34:34 INFO]: Training loss at epoch 29: 0.8314876556396484
[08/27/2025 17:34:51 INFO]: Training loss at epoch 2: 1.144185870885849
[08/27/2025 17:34:53 INFO]: Training loss at epoch 11: 0.926151305437088
[08/27/2025 17:34:57 INFO]: Training loss at epoch 33: 0.9024763405323029
[08/27/2025 17:35:12 INFO]: Training loss at epoch 38: 0.9337680041790009
[08/27/2025 17:35:34 INFO]: Training loss at epoch 25: 0.9379012286663055
[08/27/2025 17:35:37 INFO]: Training loss at epoch 57: 0.8594453632831573
[08/27/2025 17:36:04 INFO]: Training loss at epoch 14: 1.0092629492282867
[08/27/2025 17:36:06 INFO]: Training loss at epoch 30: 0.9965683817863464
[08/27/2025 17:36:09 INFO]: Training loss at epoch 12: 1.3756403923034668
[08/27/2025 17:36:22 INFO]: Training loss at epoch 17: 1.1109686195850372
[08/27/2025 17:36:35 INFO]: Training stats: {
    "score": -0.9291087936229364,
    "rmse": 0.9291087936229364
}
[08/27/2025 17:36:35 INFO]: Val stats: {
    "score": -0.9225351070726191,
    "rmse": 0.9225351070726191
}
[08/27/2025 17:36:35 INFO]: Test stats: {
    "score": -0.972222858748788,
    "rmse": 0.972222858748788
}
[08/27/2025 17:36:59 INFO]: Training loss at epoch 13: 1.5444748997688293
[08/27/2025 17:37:03 INFO]: Training loss at epoch 29: 1.0191491842269897
[08/27/2025 17:37:09 INFO]: Training loss at epoch 12: 0.7996407449245453
[08/27/2025 17:37:22 INFO]: Training loss at epoch 3: 1.2695882320404053
[08/27/2025 17:37:27 INFO]: New best epoch, val score: -0.8825968902269575
[08/27/2025 17:37:27 INFO]: Saving model to: maddest-Elbert_trial_35/model_best.pth
[08/27/2025 17:37:59 INFO]: Training loss at epoch 13: 1.3171334862709045
[08/27/2025 17:38:07 INFO]: Training loss at epoch 24: 1.0402381420135498
[08/27/2025 17:38:28 INFO]: Training loss at epoch 41: 1.0344363152980804
[08/27/2025 17:38:39 INFO]: Training loss at epoch 58: 0.9261826872825623
[08/27/2025 17:38:47 INFO]: Training stats: {
    "score": -0.9996546118113459,
    "rmse": 0.9996546118113459
}
[08/27/2025 17:38:47 INFO]: Val stats: {
    "score": -0.9298068954113954,
    "rmse": 0.9298068954113954
}
[08/27/2025 17:38:47 INFO]: Test stats: {
    "score": -0.9716784554201517,
    "rmse": 0.9716784554201517
}
[08/27/2025 17:39:25 INFO]: Training loss at epoch 13: 1.068244218826294
[08/27/2025 17:39:41 INFO]: Training loss at epoch 39: 0.981454998254776
[08/27/2025 17:39:42 INFO]: Training loss at epoch 26: 0.9109824597835541
[08/27/2025 17:39:44 INFO]: Training loss at epoch 13: 0.8088115006685257
[08/27/2025 17:39:49 INFO]: Training loss at epoch 34: 0.9457830786705017
[08/27/2025 17:39:52 INFO]: Training loss at epoch 4: 0.9262453615665436
[08/27/2025 17:39:59 INFO]: Training loss at epoch 26: 0.908433586359024
[08/27/2025 17:40:06 INFO]: Training loss at epoch 23: 1.0118622183799744
[08/27/2025 17:40:10 INFO]: New best epoch, val score: -0.8937702502960426
[08/27/2025 17:40:10 INFO]: Saving model to: maddest-Elbert_trial_38/model_best.pth
[08/27/2025 17:40:18 INFO]: Training loss at epoch 29: 1.041150450706482
[08/27/2025 17:40:31 INFO]: Training loss at epoch 18: 0.9923824369907379
[08/27/2025 17:40:54 INFO]: Training loss at epoch 14: 0.9959258437156677
[08/27/2025 17:40:59 INFO]: Training loss at epoch 15: 1.1165385246276855
[08/27/2025 17:41:16 INFO]: Training stats: {
    "score": -1.0046626402558037,
    "rmse": 1.0046626402558037
}
[08/27/2025 17:41:16 INFO]: Val stats: {
    "score": -0.9586796724965849,
    "rmse": 0.9586796724965849
}
[08/27/2025 17:41:16 INFO]: Test stats: {
    "score": -0.9768316315646699,
    "rmse": 0.9768316315646699
}
[08/27/2025 17:41:40 INFO]: Training loss at epoch 59: 0.8981390595436096
[08/27/2025 17:41:42 INFO]: Training loss at epoch 14: 0.8475169539451599
[08/27/2025 17:41:46 INFO]: Training loss at epoch 31: 1.2525015771389008
[08/27/2025 17:42:16 INFO]: Training loss at epoch 25: 1.1080763936042786
[08/27/2025 17:42:21 INFO]: Training stats: {
    "score": -1.0031805502507114,
    "rmse": 1.0031805502507114
}
[08/27/2025 17:42:21 INFO]: Val stats: {
    "score": -0.952049137899185,
    "rmse": 0.952049137899185
}
[08/27/2025 17:42:21 INFO]: Test stats: {
    "score": -0.9752638123298659,
    "rmse": 0.9752638123298659
}
[08/27/2025 17:42:24 INFO]: Training loss at epoch 5: 1.1079949140548706
[08/27/2025 17:42:24 INFO]: Training loss at epoch 30: 0.8459661304950714
[08/27/2025 17:42:42 INFO]: Training stats: {
    "score": -0.8636900979470241,
    "rmse": 0.8636900979470241
}
[08/27/2025 17:42:42 INFO]: Val stats: {
    "score": -0.9930798456196631,
    "rmse": 0.9930798456196631
}
[08/27/2025 17:42:42 INFO]: Test stats: {
    "score": -1.094108417412982,
    "rmse": 1.094108417412982
}
[08/27/2025 17:42:42 INFO]: Training loss at epoch 42: 1.1232546865940094
[08/27/2025 17:42:42 INFO]: New best epoch, val score: -0.8929578027774133
[08/27/2025 17:42:42 INFO]: Saving model to: maddest-Elbert_trial_38/model_best.pth
[08/27/2025 17:43:21 INFO]: Training loss at epoch 14: 0.8475030958652496
[08/27/2025 17:43:26 INFO]: Training loss at epoch 15: 1.1175042986869812
[08/27/2025 17:43:46 INFO]: Training loss at epoch 30: 1.005556881427765
[08/27/2025 17:43:51 INFO]: Training loss at epoch 27: 0.9345667064189911
[08/27/2025 17:43:59 INFO]: Training loss at epoch 15: 0.8746984004974365
[08/27/2025 17:44:40 INFO]: Training loss at epoch 19: 1.0271139740943909
[08/27/2025 17:44:41 INFO]: Training loss at epoch 35: 0.8363819122314453
[08/27/2025 17:44:49 INFO]: Training loss at epoch 15: 1.3747512698173523
[08/27/2025 17:44:55 INFO]: Training loss at epoch 6: 0.896744430065155
[08/27/2025 17:45:13 INFO]: New best epoch, val score: -0.8904958930739576
[08/27/2025 17:45:13 INFO]: Saving model to: maddest-Elbert_trial_38/model_best.pth
[08/27/2025 17:45:44 INFO]: Training loss at epoch 60: 0.6645074188709259
[08/27/2025 17:45:44 INFO]: Training loss at epoch 40: 0.9773508012294769
[08/27/2025 17:46:09 INFO]: Training stats: {
    "score": -1.0002862258657215,
    "rmse": 1.0002862258657215
}
[08/27/2025 17:46:09 INFO]: Val stats: {
    "score": -0.9448542690362824,
    "rmse": 0.9448542690362824
}
[08/27/2025 17:46:09 INFO]: Test stats: {
    "score": -0.9729062477983519,
    "rmse": 0.9729062477983519
}
[08/27/2025 17:46:16 INFO]: Training loss at epoch 16: 0.8748322129249573
[08/27/2025 17:46:17 INFO]: Running Final Evaluation...
[08/27/2025 17:46:25 INFO]: Training loss at epoch 26: 0.9377726316452026
[08/27/2025 17:46:30 INFO]: Training loss at epoch 14: 1.4323830604553223
[08/27/2025 17:46:30 INFO]: Training loss at epoch 27: 1.0183902978897095
[08/27/2025 17:46:57 INFO]: Training loss at epoch 43: 1.0277459025382996
[08/27/2025 17:46:58 INFO]: Training loss at epoch 15: 0.8505128026008606
[08/27/2025 17:47:23 INFO]: New best epoch, val score: -0.8945013059093021
[08/27/2025 17:47:23 INFO]: Saving model to: maddest-Elbert_trial_36/model_best.pth
[08/27/2025 17:47:24 INFO]: Training loss at epoch 32: 0.9364646673202515
[08/27/2025 17:47:26 INFO]: Training loss at epoch 7: 1.2226668000221252
[08/27/2025 17:47:38 INFO]: Training loss at epoch 24: 1.0529010593891144
[08/27/2025 17:47:44 INFO]: New best epoch, val score: -0.887292536916662
[08/27/2025 17:47:44 INFO]: Saving model to: maddest-Elbert_trial_38/model_best.pth
[08/27/2025 17:48:00 INFO]: Training loss at epoch 28: 0.8127856254577637
[08/27/2025 17:48:07 INFO]: Training accuracy: {
    "score": -1.0611543487096842,
    "rmse": 1.0611543487096842
}
[08/27/2025 17:48:07 INFO]: Val accuracy: {
    "score": -0.8848242827804477,
    "rmse": 0.8848242827804477
}
[08/27/2025 17:48:07 INFO]: Test accuracy: {
    "score": -1.0332760079341805,
    "rmse": 1.0332760079341805
}
[08/27/2025 17:48:08 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_5",
    "best_epoch": 9,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0332760079341805,
        "rmse": 1.0332760079341805
    },
    "train_stats": {
        "score": -1.0611543487096842,
        "rmse": 1.0611543487096842
    },
    "val_stats": {
        "score": -0.8848242827804477,
        "rmse": 0.8848242827804477
    }
}
[08/27/2025 17:48:08 INFO]: Procewss finished for trial maddest-Elbert_trial_5
[08/27/2025 17:48:08 INFO]: 
_________________________________________________

[08/27/2025 17:48:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 17:48:08 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 1.3125837846612798
  attention_dropout: 0.012477661272281435
  ffn_dropout: 0.012477661272281435
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.83846725389259e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_39

[08/27/2025 17:48:08 INFO]: This ft_transformer has 4.825 million parameters.
[08/27/2025 17:48:08 INFO]: Training will start at epoch 0.
[08/27/2025 17:48:08 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 17:48:18 INFO]: Training loss at epoch 31: 0.8635479211807251
[08/27/2025 17:48:24 INFO]: Training loss at epoch 30: 1.1058711111545563
[08/27/2025 17:48:33 INFO]: Training loss at epoch 17: 0.9711419939994812
[08/27/2025 17:48:45 INFO]: Training loss at epoch 61: 0.641672283411026
[08/27/2025 17:48:46 INFO]: Training loss at epoch 16: 0.9634893238544464
[08/27/2025 17:48:47 INFO]: Training loss at epoch 31: 1.0345322489738464
[08/27/2025 17:49:33 INFO]: Training loss at epoch 36: 0.9163522720336914
[08/27/2025 17:49:56 INFO]: Training loss at epoch 8: 1.0800954103469849
[08/27/2025 17:50:14 INFO]: New best epoch, val score: -0.8846690901055644
[08/27/2025 17:50:14 INFO]: Saving model to: maddest-Elbert_trial_38/model_best.pth
[08/27/2025 17:50:15 INFO]: Training loss at epoch 20: 1.21376633644104
[08/27/2025 17:50:18 INFO]: Training loss at epoch 0: 1.2134589552879333
[08/27/2025 17:50:34 INFO]: Training loss at epoch 27: 0.9357938170433044
[08/27/2025 17:50:34 INFO]: Training loss at epoch 16: 1.1047737896442413
[08/27/2025 17:50:37 INFO]: New best epoch, val score: -1.1638500814784332
[08/27/2025 17:50:37 INFO]: Saving model to: maddest-Elbert_trial_39/model_best.pth
[08/27/2025 17:50:50 INFO]: Training loss at epoch 16: 1.0285476446151733
[08/27/2025 17:50:54 INFO]: Training loss at epoch 18: 1.1150444746017456
[08/27/2025 17:51:12 INFO]: Training loss at epoch 44: 1.242107331752777
[08/27/2025 17:51:48 INFO]: Training loss at epoch 62: 0.6067440062761307
[08/27/2025 17:52:10 INFO]: Training loss at epoch 29: 0.8934175670146942
[08/27/2025 17:52:26 INFO]: Training loss at epoch 16: 1.0854549705982208
[08/27/2025 17:52:27 INFO]: Training loss at epoch 9: 1.0636854767799377
[08/27/2025 17:52:43 INFO]: Training loss at epoch 17: 1.1395753026008606
[08/27/2025 17:52:51 INFO]: Training loss at epoch 1: 0.947795182466507
[08/27/2025 17:52:56 INFO]: Training loss at epoch 28: 1.2808384597301483
[08/27/2025 17:53:02 INFO]: Training loss at epoch 33: 0.8531012833118439
[08/27/2025 17:53:09 INFO]: New best epoch, val score: -0.8806504134280243
[08/27/2025 17:53:09 INFO]: Saving model to: maddest-Elbert_trial_39/model_best.pth
[08/27/2025 17:53:11 INFO]: Training loss at epoch 19: 0.9386619925498962
[08/27/2025 17:53:18 INFO]: Training stats: {
    "score": -1.0268629510996474,
    "rmse": 1.0268629510996474
}
[08/27/2025 17:53:18 INFO]: Val stats: {
    "score": -0.8834771727733436,
    "rmse": 0.8834771727733436
}
[08/27/2025 17:53:18 INFO]: Test stats: {
    "score": -1.007317205644866,
    "rmse": 1.007317205644866
}
[08/27/2025 17:53:37 INFO]: New best epoch, val score: -0.8834771727733436
[08/27/2025 17:53:37 INFO]: Saving model to: maddest-Elbert_trial_38/model_best.pth
[08/27/2025 17:53:38 INFO]: Training stats: {
    "score": -0.993440103399915,
    "rmse": 0.993440103399915
}
[08/27/2025 17:53:38 INFO]: Val stats: {
    "score": -0.9550510426015469,
    "rmse": 0.9550510426015469
}
[08/27/2025 17:53:38 INFO]: Test stats: {
    "score": -0.9773825706774404,
    "rmse": 0.9773825706774404
}
[08/27/2025 17:53:51 INFO]: Training loss at epoch 32: 0.8862578272819519
[08/27/2025 17:53:58 INFO]: Training stats: {
    "score": -0.9616924878780103,
    "rmse": 0.9616924878780103
}
[08/27/2025 17:53:58 INFO]: Val stats: {
    "score": -0.9215195279491831,
    "rmse": 0.9215195279491831
}
[08/27/2025 17:53:58 INFO]: Test stats: {
    "score": -0.9762726732087951,
    "rmse": 0.9762726732087951
}
[08/27/2025 17:54:11 INFO]: Training loss at epoch 17: 0.9747148752212524
[08/27/2025 17:54:11 INFO]: Training loss at epoch 32: 0.8314037621021271
[08/27/2025 17:54:24 INFO]: Training loss at epoch 21: 1.0321093797683716
[08/27/2025 17:54:27 INFO]: Training loss at epoch 37: 0.9801889359951019
[08/27/2025 17:54:28 INFO]: Training loss at epoch 31: 0.8971973359584808
[08/27/2025 17:54:45 INFO]: Training loss at epoch 28: 0.8867039084434509
[08/27/2025 17:54:50 INFO]: Training loss at epoch 63: 0.6077558100223541
[08/27/2025 17:54:58 INFO]: Training loss at epoch 15: 1.0630162060260773
[08/27/2025 17:55:09 INFO]: Training loss at epoch 25: 1.0294567942619324
[08/27/2025 17:55:24 INFO]: Training loss at epoch 2: 1.4473098516464233
[08/27/2025 17:55:32 INFO]: Training loss at epoch 45: 1.1821836233139038
[08/27/2025 17:55:50 INFO]: Training loss at epoch 10: 0.8951718807220459
[08/27/2025 17:56:15 INFO]: Training loss at epoch 20: 0.9075070321559906
[08/27/2025 17:56:40 INFO]: Training loss at epoch 18: 0.9599786698818207
[08/27/2025 17:57:48 INFO]: Training loss at epoch 18: 1.0703457593917847
[08/27/2025 17:57:51 INFO]: Training loss at epoch 30: 0.9883791506290436
[08/27/2025 17:57:52 INFO]: Training loss at epoch 64: 0.8199032843112946
[08/27/2025 17:57:54 INFO]: Training loss at epoch 3: 1.061714768409729
[08/27/2025 17:58:15 INFO]: Training loss at epoch 17: 0.8537407219409943
[08/27/2025 17:58:21 INFO]: Training loss at epoch 11: 1.0606233179569244
[08/27/2025 17:58:32 INFO]: Training loss at epoch 22: 1.0008221864700317
[08/27/2025 17:58:33 INFO]: Training loss at epoch 21: 0.8548597097396851
[08/27/2025 17:58:41 INFO]: Training loss at epoch 34: 1.0249257683753967
[08/27/2025 17:58:50 INFO]: Training loss at epoch 33: 1.050509512424469
[08/27/2025 17:58:56 INFO]: Training loss at epoch 29: 0.9481557607650757
[08/27/2025 17:59:19 INFO]: Training loss at epoch 38: 1.0879420042037964
[08/27/2025 17:59:26 INFO]: Training loss at epoch 29: 0.9272567629814148
[08/27/2025 17:59:46 INFO]: Training loss at epoch 46: 0.840604692697525
[08/27/2025 18:00:02 INFO]: Training loss at epoch 33: 0.8732903003692627
[08/27/2025 18:00:22 INFO]: Training loss at epoch 4: 0.7972134351730347
[08/27/2025 18:00:24 INFO]: Training stats: {
    "score": -0.9947544157898575,
    "rmse": 0.9947544157898575
}
[08/27/2025 18:00:24 INFO]: Val stats: {
    "score": -0.9375658970405351,
    "rmse": 0.9375658970405351
}
[08/27/2025 18:00:24 INFO]: Test stats: {
    "score": -0.9745306331481296,
    "rmse": 0.9745306331481296
}
[08/27/2025 18:00:31 INFO]: Training loss at epoch 32: 0.9061994254589081
[08/27/2025 18:00:35 INFO]: Training loss at epoch 19: 0.8716341257095337
[08/27/2025 18:00:50 INFO]: Training loss at epoch 22: 0.9853694438934326
[08/27/2025 18:00:51 INFO]: Training loss at epoch 12: 0.9780694544315338
[08/27/2025 18:00:53 INFO]: Training loss at epoch 65: 0.5960171967744827
[08/27/2025 18:01:25 INFO]: Training loss at epoch 19: 0.9405330121517181
[08/27/2025 18:01:36 INFO]: Training stats: {
    "score": -1.0016238518396163,
    "rmse": 1.0016238518396163
}
[08/27/2025 18:01:36 INFO]: Val stats: {
    "score": -0.9443139437838718,
    "rmse": 0.9443139437838718
}
[08/27/2025 18:01:36 INFO]: Test stats: {
    "score": -0.9742643769802927,
    "rmse": 0.9742643769802927
}
[08/27/2025 18:01:57 INFO]: Training stats: {
    "score": -0.9950751563023963,
    "rmse": 0.9950751563023963
}
[08/27/2025 18:01:57 INFO]: Val stats: {
    "score": -0.9823805484712911,
    "rmse": 0.9823805484712911
}
[08/27/2025 18:01:57 INFO]: Test stats: {
    "score": -0.9801337089972082,
    "rmse": 0.9801337089972082
}
[08/27/2025 18:02:00 INFO]: Training loss at epoch 31: 0.8660675585269928
[08/27/2025 18:02:38 INFO]: Training stats: {
    "score": -0.9503686709270973,
    "rmse": 0.9503686709270973
}
[08/27/2025 18:02:38 INFO]: Val stats: {
    "score": -0.9395140189658805,
    "rmse": 0.9395140189658805
}
[08/27/2025 18:02:38 INFO]: Test stats: {
    "score": -0.9695312092176819,
    "rmse": 0.9695312092176819
}
[08/27/2025 18:02:40 INFO]: Training loss at epoch 23: 1.1525961756706238
[08/27/2025 18:02:41 INFO]: Training loss at epoch 26: 1.1121680736541748
[08/27/2025 18:02:52 INFO]: Training loss at epoch 5: 1.082430362701416
[08/27/2025 18:03:06 INFO]: Training loss at epoch 23: 0.7730778753757477
[08/27/2025 18:03:22 INFO]: Training loss at epoch 13: 1.3046356439590454
[08/27/2025 18:03:29 INFO]: Training loss at epoch 16: 1.099521815776825
[08/27/2025 18:03:51 INFO]: Training loss at epoch 34: 0.8895818293094635
[08/27/2025 18:03:55 INFO]: Training loss at epoch 66: 0.7407952547073364
[08/27/2025 18:03:58 INFO]: Training loss at epoch 17: 0.9109828174114227
[08/27/2025 18:04:01 INFO]: Training loss at epoch 47: 0.8200002014636993
[08/27/2025 18:04:14 INFO]: Training loss at epoch 39: 1.0201664566993713
[08/27/2025 18:04:19 INFO]: Training loss at epoch 35: 0.9315932393074036
[08/27/2025 18:04:32 INFO]: Running Final Evaluation...
[08/27/2025 18:04:35 INFO]: Training loss at epoch 30: 0.8813160955905914
[08/27/2025 18:05:21 INFO]: Training loss at epoch 6: 0.8510428369045258
[08/27/2025 18:05:23 INFO]: Training loss at epoch 24: 0.9897918701171875
[08/27/2025 18:05:38 INFO]: Training loss at epoch 18: 0.9705809652805328
[08/27/2025 18:05:52 INFO]: Training loss at epoch 20: 0.960370659828186
[08/27/2025 18:05:53 INFO]: Training loss at epoch 14: 0.8562949299812317
[08/27/2025 18:05:54 INFO]: Training stats: {
    "score": -0.9888950634647018,
    "rmse": 0.9888950634647018
}
[08/27/2025 18:05:54 INFO]: Val stats: {
    "score": -0.9069228137562595,
    "rmse": 0.9069228137562595
}
[08/27/2025 18:05:54 INFO]: Test stats: {
    "score": -0.9699343642111136,
    "rmse": 0.9699343642111136
}
[08/27/2025 18:05:54 INFO]: Training loss at epoch 34: 0.9814423322677612
[08/27/2025 18:06:02 INFO]: Training accuracy: {
    "score": -1.0262950080437023,
    "rmse": 1.0262950080437023
}
[08/27/2025 18:06:02 INFO]: Val accuracy: {
    "score": -0.8822781907154541,
    "rmse": 0.8822781907154541
}
[08/27/2025 18:06:02 INFO]: Test accuracy: {
    "score": -0.9981833971714715,
    "rmse": 0.9981833971714715
}
[08/27/2025 18:06:02 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_9",
    "best_epoch": 16,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9981833971714715,
        "rmse": 0.9981833971714715
    },
    "train_stats": {
        "score": -1.0262950080437023,
        "rmse": 1.0262950080437023
    },
    "val_stats": {
        "score": -0.8822781907154541,
        "rmse": 0.8822781907154541
    }
}
[08/27/2025 18:06:02 INFO]: Procewss finished for trial maddest-Elbert_trial_9
[08/27/2025 18:06:03 INFO]: 
_________________________________________________

[08/27/2025 18:06:03 INFO]: train_net_for_optune.py main() running.
[08/27/2025 18:06:03 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 1.2756264188248212
  attention_dropout: 0.009431249875515746
  ffn_dropout: 0.009431249875515746
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.068982733143936e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_40

[08/27/2025 18:06:03 INFO]: This ft_transformer has 4.756 million parameters.
[08/27/2025 18:06:03 INFO]: Training will start at epoch 0.
[08/27/2025 18:06:03 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 18:06:11 INFO]: Training loss at epoch 32: 1.2137374877929688
[08/27/2025 18:06:14 INFO]: Training loss at epoch 20: 0.920669674873352
[08/27/2025 18:06:35 INFO]: Training loss at epoch 33: 0.9100898206233978
[08/27/2025 18:06:44 INFO]: Running Final Evaluation...
[08/27/2025 18:06:48 INFO]: Training loss at epoch 24: 1.0625762939453125
[08/27/2025 18:06:57 INFO]: Training loss at epoch 67: 0.607918381690979
[08/27/2025 18:07:40 INFO]: Training loss at epoch 25: 1.0595648884773254
[08/27/2025 18:07:50 INFO]: Training loss at epoch 7: 1.0815180540084839
[08/27/2025 18:08:05 INFO]: Training loss at epoch 30: 0.9259308874607086
[08/27/2025 18:08:14 INFO]: Training loss at epoch 0: 1.4011048078536987
[08/27/2025 18:08:16 INFO]: Training accuracy: {
    "score": -1.033680831068798,
    "rmse": 1.033680831068798
}
[08/27/2025 18:08:16 INFO]: Val accuracy: {
    "score": -0.8790474215370098,
    "rmse": 0.8790474215370098
}
[08/27/2025 18:08:16 INFO]: Test accuracy: {
    "score": -1.0103717803366346,
    "rmse": 1.0103717803366346
}
[08/27/2025 18:08:16 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_32",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0103717803366346,
        "rmse": 1.0103717803366346
    },
    "train_stats": {
        "score": -1.033680831068798,
        "rmse": 1.033680831068798
    },
    "val_stats": {
        "score": -0.8790474215370098,
        "rmse": 0.8790474215370098
    }
}
[08/27/2025 18:08:16 INFO]: Procewss finished for trial maddest-Elbert_trial_32
[08/27/2025 18:08:16 INFO]: 
_________________________________________________

[08/27/2025 18:08:16 INFO]: train_net_for_optune.py main() running.
[08/27/2025 18:08:16 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 1.2676542543661609
  attention_dropout: 0.020986424805889456
  ffn_dropout: 0.020986424805889456
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00019599876384289772
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_41

[08/27/2025 18:08:16 INFO]: This ft_transformer has 4.742 million parameters.
[08/27/2025 18:08:16 INFO]: Training will start at epoch 0.
[08/27/2025 18:08:16 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 18:08:23 INFO]: Training loss at epoch 15: 1.1658543944358826
[08/27/2025 18:08:33 INFO]: New best epoch, val score: -0.8920555930503219
[08/27/2025 18:08:33 INFO]: Saving model to: maddest-Elbert_trial_40/model_best.pth
[08/27/2025 18:08:43 INFO]: Training loss at epoch 31: 1.124837726354599
[08/27/2025 18:08:51 INFO]: Training loss at epoch 35: 1.198524534702301
[08/27/2025 18:09:51 INFO]: Training loss at epoch 21: 0.9381446540355682
[08/27/2025 18:09:53 INFO]: Training loss at epoch 21: 0.9405196905136108
[08/27/2025 18:09:56 INFO]: Training loss at epoch 26: 0.8670738935470581
[08/27/2025 18:09:58 INFO]: Training loss at epoch 36: 0.8862532675266266
[08/27/2025 18:09:59 INFO]: Training loss at epoch 68: 0.6012682318687439
[08/27/2025 18:10:13 INFO]: Training loss at epoch 27: 0.8934252560138702
[08/27/2025 18:10:20 INFO]: Training loss at epoch 8: 1.0328212976455688
[08/27/2025 18:10:25 INFO]: Training loss at epoch 0: 2.716860294342041
[08/27/2025 18:10:43 INFO]: New best epoch, val score: -1.0677006656991905
[08/27/2025 18:10:43 INFO]: Saving model to: maddest-Elbert_trial_41/model_best.pth
[08/27/2025 18:10:44 INFO]: Training loss at epoch 1: 1.1054278016090393
[08/27/2025 18:10:46 INFO]: Training loss at epoch 40: 0.8641612529754639
[08/27/2025 18:10:54 INFO]: Training loss at epoch 16: 0.94510617852211
[08/27/2025 18:10:56 INFO]: Training loss at epoch 25: 1.2253205180168152
[08/27/2025 18:11:46 INFO]: Training loss at epoch 35: 0.8364841639995575
[08/27/2025 18:11:57 INFO]: Training loss at epoch 17: 1.0292481482028961
[08/27/2025 18:12:13 INFO]: Training loss at epoch 27: 0.896754115819931
[08/27/2025 18:12:38 INFO]: Training loss at epoch 34: 0.9030498564243317
[08/27/2025 18:12:50 INFO]: Training loss at epoch 9: 0.9493566453456879
[08/27/2025 18:12:52 INFO]: Training loss at epoch 32: 1.0742343664169312
[08/27/2025 18:12:52 INFO]: Training loss at epoch 1: 2.4472920894622803
[08/27/2025 18:13:00 INFO]: Training loss at epoch 69: 0.6319638788700104
[08/27/2025 18:13:00 INFO]: Training loss at epoch 19: 0.8306757211685181
[08/27/2025 18:13:11 INFO]: Training loss at epoch 2: 1.019749641418457
[08/27/2025 18:13:25 INFO]: Training loss at epoch 17: 1.034486711025238
[08/27/2025 18:13:27 INFO]: Training loss at epoch 22: 1.0219157934188843
[08/27/2025 18:13:29 INFO]: New best epoch, val score: -0.8647111483206955
[08/27/2025 18:13:29 INFO]: Saving model to: maddest-Elbert_trial_40/model_best.pth
[08/27/2025 18:13:40 INFO]: Training stats: {
    "score": -1.010228541063669,
    "rmse": 1.010228541063669
}
[08/27/2025 18:13:40 INFO]: Val stats: {
    "score": -1.0071622720905586,
    "rmse": 1.0071622720905586
}
[08/27/2025 18:13:40 INFO]: Test stats: {
    "score": -0.9926692020743871,
    "rmse": 0.9926692020743871
}
[08/27/2025 18:13:47 INFO]: Training loss at epoch 22: 0.9937619268894196
[08/27/2025 18:13:49 INFO]: Training loss at epoch 36: 1.0342500805854797
[08/27/2025 18:14:01 INFO]: Training stats: {
    "score": -0.918421549094816,
    "rmse": 0.918421549094816
}
[08/27/2025 18:14:01 INFO]: Val stats: {
    "score": -1.3950521826664375,
    "rmse": 1.3950521826664375
}
[08/27/2025 18:14:01 INFO]: Test stats: {
    "score": -1.3545806985158373,
    "rmse": 1.3545806985158373
}
[08/27/2025 18:14:29 INFO]: Training loss at epoch 28: 0.8251520991325378
[08/27/2025 18:14:31 INFO]: Training loss at epoch 31: 0.876371830701828
[08/27/2025 18:15:03 INFO]: Training loss at epoch 26: 0.8232056200504303
[08/27/2025 18:15:18 INFO]: Training loss at epoch 2: 1.9361679553985596
[08/27/2025 18:15:25 INFO]: Training loss at epoch 18: 1.046192318201065
[08/27/2025 18:15:36 INFO]: Training loss at epoch 37: 0.9076327085494995
[08/27/2025 18:15:36 INFO]: New best epoch, val score: -0.8782297529903855
[08/27/2025 18:15:36 INFO]: Saving model to: maddest-Elbert_trial_41/model_best.pth
[08/27/2025 18:15:39 INFO]: Training stats: {
    "score": -1.0148383022512972,
    "rmse": 1.0148383022512972
}
[08/27/2025 18:15:39 INFO]: Val stats: {
    "score": -0.882560670885135,
    "rmse": 0.882560670885135
}
[08/27/2025 18:15:39 INFO]: Test stats: {
    "score": -0.9894073010979522,
    "rmse": 0.9894073010979522
}
[08/27/2025 18:15:40 INFO]: Training loss at epoch 41: 0.9198744297027588
[08/27/2025 18:15:42 INFO]: Training loss at epoch 3: 1.2446597814559937
[08/27/2025 18:15:57 INFO]: Training loss at epoch 18: 0.8897220194339752
[08/27/2025 18:16:12 INFO]: Training loss at epoch 10: 0.8270783424377441
[08/27/2025 18:16:50 INFO]: Training loss at epoch 29: 0.9053065478801727
[08/27/2025 18:17:01 INFO]: Training loss at epoch 33: 0.951753556728363
[08/27/2025 18:17:05 INFO]: Training loss at epoch 23: 0.8628196716308594
[08/27/2025 18:17:05 INFO]: Training loss at epoch 70: 0.5849987268447876
[08/27/2025 18:17:37 INFO]: Training stats: {
    "score": -0.9505340847878707,
    "rmse": 0.9505340847878707
}
[08/27/2025 18:17:37 INFO]: Val stats: {
    "score": -0.971190068713373,
    "rmse": 0.971190068713373
}
[08/27/2025 18:17:37 INFO]: Test stats: {
    "score": -0.9894022675551759,
    "rmse": 0.9894022675551759
}
[08/27/2025 18:17:37 INFO]: Training loss at epoch 36: 0.814870297908783
[08/27/2025 18:17:43 INFO]: Training loss at epoch 23: 0.9649529457092285
[08/27/2025 18:17:44 INFO]: Training loss at epoch 28: 0.9467982053756714
[08/27/2025 18:17:47 INFO]: Training loss at epoch 3: 1.12076735496521
[08/27/2025 18:18:10 INFO]: Training loss at epoch 4: 0.9974648058414459
[08/27/2025 18:18:27 INFO]: Training loss at epoch 19: 1.0814157128334045
[08/27/2025 18:18:43 INFO]: Training loss at epoch 11: 0.9407411515712738
[08/27/2025 18:18:43 INFO]: Training loss at epoch 35: 0.7726282328367233
[08/27/2025 18:18:51 INFO]: Training loss at epoch 37: 0.8549852967262268
[08/27/2025 18:19:12 INFO]: Training loss at epoch 27: 0.8728207051753998
[08/27/2025 18:19:19 INFO]: Training stats: {
    "score": -0.9798756895293317,
    "rmse": 0.9798756895293317
}
[08/27/2025 18:19:19 INFO]: Val stats: {
    "score": -0.9060294731698911,
    "rmse": 0.9060294731698911
}
[08/27/2025 18:19:19 INFO]: Test stats: {
    "score": -0.9689171139697754,
    "rmse": 0.9689171139697754
}
[08/27/2025 18:19:54 INFO]: Training loss at epoch 30: 1.1096813380718231
[08/27/2025 18:20:07 INFO]: Training loss at epoch 71: 0.6703070402145386
[08/27/2025 18:20:14 INFO]: Training loss at epoch 4: 1.4656920433044434
[08/27/2025 18:20:24 INFO]: Training loss at epoch 18: 1.1237865686416626
[08/27/2025 18:20:32 INFO]: Training loss at epoch 42: 0.8670258224010468
[08/27/2025 18:20:38 INFO]: Training loss at epoch 5: 0.9517591893672943
[08/27/2025 18:20:42 INFO]: Training loss at epoch 24: 0.7665833830833435
[08/27/2025 18:21:03 INFO]: Training loss at epoch 32: 1.048993468284607
[08/27/2025 18:21:10 INFO]: Training loss at epoch 34: 1.1852408647537231
[08/27/2025 18:21:13 INFO]: Training loss at epoch 12: 0.9708386063575745
[08/27/2025 18:21:16 INFO]: Training loss at epoch 38: 0.9858386218547821
[08/27/2025 18:21:27 INFO]: New best epoch, val score: -0.8806096596200622
[08/27/2025 18:21:27 INFO]: Saving model to: maddest-Elbert_trial_30/model_best.pth
[08/27/2025 18:21:40 INFO]: Training loss at epoch 24: 0.8616864383220673
[08/27/2025 18:21:42 INFO]: Running Final Evaluation...
[08/27/2025 18:21:49 INFO]: Training loss at epoch 20: 0.8395080268383026
[08/27/2025 18:22:10 INFO]: Training loss at epoch 31: 0.8156011402606964
[08/27/2025 18:22:41 INFO]: Training loss at epoch 5: 0.8175775408744812
[08/27/2025 18:23:02 INFO]: Training loss at epoch 20: 1.03575998544693
[08/27/2025 18:23:06 INFO]: Training loss at epoch 6: 0.8904741704463959
[08/27/2025 18:23:10 INFO]: Training loss at epoch 72: 0.7605409324169159
[08/27/2025 18:23:13 INFO]: Training accuracy: {
    "score": -1.0219552242362595,
    "rmse": 1.0219552242362595
}
[08/27/2025 18:23:13 INFO]: Val accuracy: {
    "score": -0.8825583560296766,
    "rmse": 0.8825583560296766
}
[08/27/2025 18:23:13 INFO]: Test accuracy: {
    "score": -0.9977781306152651,
    "rmse": 0.9977781306152651
}
[08/27/2025 18:23:13 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_33",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9977781306152651,
        "rmse": 0.9977781306152651
    },
    "train_stats": {
        "score": -1.0219552242362595,
        "rmse": 1.0219552242362595
    },
    "val_stats": {
        "score": -0.8825583560296766,
        "rmse": 0.8825583560296766
    }
}
[08/27/2025 18:23:13 INFO]: Procewss finished for trial maddest-Elbert_trial_33
[08/27/2025 18:23:13 INFO]: 
_________________________________________________

[08/27/2025 18:23:13 INFO]: train_net_for_optune.py main() running.
[08/27/2025 18:23:13 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 1.3148539129698915
  attention_dropout: 0.04514277258301269
  ffn_dropout: 0.04514277258301269
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0002172111403684544
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_42

[08/27/2025 18:23:13 INFO]: This ft_transformer has 4.825 million parameters.
[08/27/2025 18:23:13 INFO]: Training will start at epoch 0.
[08/27/2025 18:23:13 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 18:23:21 INFO]: Training loss at epoch 28: 1.2394904494285583
[08/27/2025 18:23:30 INFO]: Training loss at epoch 37: 0.8954043388366699
[08/27/2025 18:23:44 INFO]: Training loss at epoch 13: 0.9564358592033386
[08/27/2025 18:23:52 INFO]: Training loss at epoch 38: 0.8883078098297119
[08/27/2025 18:24:19 INFO]: Training loss at epoch 25: 0.9560387134552002
[08/27/2025 18:24:20 INFO]: Training loss at epoch 21: 0.9904537796974182
[08/27/2025 18:24:28 INFO]: Training loss at epoch 32: 0.8146482408046722
[08/27/2025 18:24:46 INFO]: Training loss at epoch 36: 1.1620051860809326
[08/27/2025 18:25:08 INFO]: Training loss at epoch 6: 1.023783951997757
[08/27/2025 18:25:16 INFO]: Training loss at epoch 29: 0.8531809151172638
[08/27/2025 18:25:25 INFO]: Training loss at epoch 0: 1.2571780681610107
[08/27/2025 18:25:26 INFO]: Training loss at epoch 43: 1.2322470843791962
[08/27/2025 18:25:34 INFO]: Training loss at epoch 25: 0.9215160012245178
[08/27/2025 18:25:34 INFO]: Training loss at epoch 7: 0.8168746829032898
[08/27/2025 18:25:42 INFO]: New best epoch, val score: -0.9125699447492525
[08/27/2025 18:25:42 INFO]: Saving model to: maddest-Elbert_trial_42/model_best.pth
[08/27/2025 18:26:12 INFO]: Training loss at epoch 73: 0.6901676952838898
[08/27/2025 18:26:13 INFO]: Training loss at epoch 14: 0.9503601491451263
[08/27/2025 18:26:45 INFO]: Training loss at epoch 33: 0.827785074710846
[08/27/2025 18:26:53 INFO]: Training loss at epoch 22: 0.8778034746646881
[08/27/2025 18:26:55 INFO]: Training loss at epoch 39: 1.0289658308029175
[08/27/2025 18:27:01 INFO]: Training loss at epoch 19: 1.0010376274585724
[08/27/2025 18:27:29 INFO]: Training loss at epoch 29: 0.9137259423732758
[08/27/2025 18:27:35 INFO]: Training loss at epoch 33: 1.0885798931121826
[08/27/2025 18:27:35 INFO]: Training loss at epoch 7: 0.8537698984146118
[08/27/2025 18:27:51 INFO]: Training stats: {
    "score": -1.0002446360113297,
    "rmse": 1.0002446360113297
}
[08/27/2025 18:27:51 INFO]: Val stats: {
    "score": -0.9514257923888412,
    "rmse": 0.9514257923888412
}
[08/27/2025 18:27:51 INFO]: Test stats: {
    "score": -0.9741069962694077,
    "rmse": 0.9741069962694077
}
[08/27/2025 18:27:54 INFO]: Training loss at epoch 1: 2.9975892305374146
[08/27/2025 18:27:56 INFO]: Training loss at epoch 26: 0.9433148205280304
[08/27/2025 18:28:04 INFO]: Training loss at epoch 8: 1.0651578307151794
[08/27/2025 18:28:41 INFO]: Training loss at epoch 15: 0.8842935562133789
[08/27/2025 18:28:53 INFO]: Training stats: {
    "score": -1.0009480404214064,
    "rmse": 1.0009480404214064
}
[08/27/2025 18:28:53 INFO]: Val stats: {
    "score": -0.9390426375275136,
    "rmse": 0.9390426375275136
}
[08/27/2025 18:28:53 INFO]: Test stats: {
    "score": -0.9727677674021341,
    "rmse": 0.9727677674021341
}
[08/27/2025 18:28:54 INFO]: Training loss at epoch 39: 0.9041032791137695
[08/27/2025 18:28:56 INFO]: Training stats: {
    "score": -1.002873745717942,
    "rmse": 1.002873745717942
}
[08/27/2025 18:28:56 INFO]: Val stats: {
    "score": -0.8920909131531118,
    "rmse": 0.8920909131531118
}
[08/27/2025 18:28:56 INFO]: Test stats: {
    "score": -0.9761483009375376,
    "rmse": 0.9761483009375376
}
[08/27/2025 18:28:58 INFO]: Training loss at epoch 19: 1.0907774567604065
[08/27/2025 18:29:04 INFO]: Training loss at epoch 34: 1.101183295249939
[08/27/2025 18:29:14 INFO]: Training loss at epoch 74: 0.6057160794734955
[08/27/2025 18:29:21 INFO]: Running Final Evaluation...
[08/27/2025 18:29:22 INFO]: Training loss at epoch 38: 0.777044028043747
[08/27/2025 18:29:24 INFO]: Training loss at epoch 23: 0.9755820333957672
[08/27/2025 18:29:29 INFO]: Training loss at epoch 26: 1.0702042877674103
[08/27/2025 18:29:59 INFO]: Training loss at epoch 8: 0.9372616708278656
[08/27/2025 18:30:16 INFO]: Training loss at epoch 44: 1.1761810183525085
[08/27/2025 18:30:22 INFO]: Training loss at epoch 2: 1.725232183933258
[08/27/2025 18:30:25 INFO]: Training accuracy: {
    "score": -1.014351038019063,
    "rmse": 1.014351038019063
}
[08/27/2025 18:30:25 INFO]: Val accuracy: {
    "score": -0.876833914373953,
    "rmse": 0.876833914373953
}
[08/27/2025 18:30:25 INFO]: Test accuracy: {
    "score": -1.0063006908581063,
    "rmse": 1.0063006908581063
}
[08/27/2025 18:30:25 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_37",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0063006908581063,
        "rmse": 1.0063006908581063
    },
    "train_stats": {
        "score": -1.014351038019063,
        "rmse": 1.014351038019063
    },
    "val_stats": {
        "score": -0.876833914373953,
        "rmse": 0.876833914373953
    }
}
[08/27/2025 18:30:25 INFO]: Procewss finished for trial maddest-Elbert_trial_37
[08/27/2025 18:30:25 INFO]: Training loss at epoch 21: 1.129546344280243
[08/27/2025 18:30:26 INFO]: 
_________________________________________________

[08/27/2025 18:30:26 INFO]: train_net_for_optune.py main() running.
[08/27/2025 18:30:26 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 1.3571571284374115
  attention_dropout: 0.05781492635324172
  ffn_dropout: 0.05781492635324172
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.155385413809635e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_43

[08/27/2025 18:30:26 INFO]: This ft_transformer has 4.903 million parameters.
[08/27/2025 18:30:26 INFO]: Training will start at epoch 0.
[08/27/2025 18:30:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 18:30:30 INFO]: Training loss at epoch 9: 1.1980824768543243
[08/27/2025 18:30:37 INFO]: Training stats: {
    "score": -0.9993841151983284,
    "rmse": 0.9993841151983284
}
[08/27/2025 18:30:37 INFO]: Val stats: {
    "score": -0.9284256248077895,
    "rmse": 0.9284256248077895
}
[08/27/2025 18:30:37 INFO]: Test stats: {
    "score": -0.9715132210575219,
    "rmse": 0.9715132210575219
}
[08/27/2025 18:30:49 INFO]: Training loss at epoch 37: 1.1478975117206573
[08/27/2025 18:30:52 INFO]: Training stats: {
    "score": -1.0087927239305683,
    "rmse": 1.0087927239305683
}
[08/27/2025 18:30:52 INFO]: Val stats: {
    "score": -0.9844537247236044,
    "rmse": 0.9844537247236044
}
[08/27/2025 18:30:52 INFO]: Test stats: {
    "score": -0.9837337978794323,
    "rmse": 0.9837337978794323
}
[08/27/2025 18:31:10 INFO]: Training loss at epoch 16: 0.9895046055316925
[08/27/2025 18:31:21 INFO]: Training stats: {
    "score": -0.9967077756558377,
    "rmse": 0.9967077756558377
}
[08/27/2025 18:31:21 INFO]: Val stats: {
    "score": -0.9698268513799543,
    "rmse": 0.9698268513799543
}
[08/27/2025 18:31:21 INFO]: Test stats: {
    "score": -0.992778756792061,
    "rmse": 0.992778756792061
}
[08/27/2025 18:31:30 INFO]: Training loss at epoch 27: 0.7995763421058655
[08/27/2025 18:31:53 INFO]: Training loss at epoch 24: 0.8343716263771057
[08/27/2025 18:31:54 INFO]: Training stats: {
    "score": -1.0590976143257793,
    "rmse": 1.0590976143257793
}
[08/27/2025 18:31:54 INFO]: Val stats: {
    "score": -0.883921635779506,
    "rmse": 0.883921635779506
}
[08/27/2025 18:31:54 INFO]: Test stats: {
    "score": -1.0318777404373858,
    "rmse": 1.0318777404373858
}
[08/27/2025 18:32:15 INFO]: Training loss at epoch 75: 0.5378649830818176
[08/27/2025 18:32:27 INFO]: Training loss at epoch 9: 0.8636122345924377
[08/27/2025 18:32:30 INFO]: Training loss at epoch 0: 0.9064509570598602
[08/27/2025 18:32:47 INFO]: New best epoch, val score: -1.0194874876093065
[08/27/2025 18:32:47 INFO]: Saving model to: maddest-Elbert_trial_43/model_best.pth
[08/27/2025 18:32:53 INFO]: Training loss at epoch 3: 1.6997831463813782
[08/27/2025 18:33:02 INFO]: Training loss at epoch 30: 1.0768034756183624
[08/27/2025 18:33:09 INFO]: New best epoch, val score: -0.8836210074254034
[08/27/2025 18:33:09 INFO]: Saving model to: maddest-Elbert_trial_42/model_best.pth
[08/27/2025 18:33:17 INFO]: Training stats: {
    "score": -1.0179617280517372,
    "rmse": 1.0179617280517372
}
[08/27/2025 18:33:17 INFO]: Val stats: {
    "score": -1.0017092245259704,
    "rmse": 1.0017092245259704
}
[08/27/2025 18:33:17 INFO]: Test stats: {
    "score": -0.9922883362577435,
    "rmse": 0.9922883362577435
}
[08/27/2025 18:33:24 INFO]: Training loss at epoch 27: 1.1874595880508423
[08/27/2025 18:33:40 INFO]: Training loss at epoch 17: 0.9438549876213074
[08/27/2025 18:33:49 INFO]: Training loss at epoch 10: 1.1780089437961578
[08/27/2025 18:34:05 INFO]: Training loss at epoch 34: 0.9922770261764526
[08/27/2025 18:34:24 INFO]: Training loss at epoch 25: 1.3156185448169708
[08/27/2025 18:34:30 INFO]: Training loss at epoch 40: 0.9908711314201355
[08/27/2025 18:34:53 INFO]: Training loss at epoch 1: 1.1041876077651978
[08/27/2025 18:35:07 INFO]: Training loss at epoch 28: 0.8790872991085052
[08/27/2025 18:35:09 INFO]: New best epoch, val score: -0.8803826899698395
[08/27/2025 18:35:09 INFO]: Saving model to: maddest-Elbert_trial_43/model_best.pth
[08/27/2025 18:35:10 INFO]: Training loss at epoch 45: 1.1659972071647644
[08/27/2025 18:35:12 INFO]: Training loss at epoch 39: 0.8187656998634338
[08/27/2025 18:35:19 INFO]: Training loss at epoch 76: 0.5024570375680923
[08/27/2025 18:35:22 INFO]: Training loss at epoch 30: 0.9947904646396637
[08/27/2025 18:35:22 INFO]: Training loss at epoch 4: 0.8740907609462738
[08/27/2025 18:35:38 INFO]: Training loss at epoch 40: 0.8699737191200256
[08/27/2025 18:35:45 INFO]: Training loss at epoch 10: 1.1449485421180725
[08/27/2025 18:36:10 INFO]: Training loss at epoch 18: 1.0129331052303314
[08/27/2025 18:36:18 INFO]: Training loss at epoch 11: 0.9523346424102783
[08/27/2025 18:36:53 INFO]: Training loss at epoch 38: 1.1606458127498627
[08/27/2025 18:36:55 INFO]: Training loss at epoch 26: 0.862486332654953
[08/27/2025 18:37:12 INFO]: Training loss at epoch 31: 1.1187878549098969
[08/27/2025 18:37:12 INFO]: Training stats: {
    "score": -0.8859888389588199,
    "rmse": 0.8859888389588199
}
[08/27/2025 18:37:12 INFO]: Val stats: {
    "score": -0.9060132191823561,
    "rmse": 0.9060132191823561
}
[08/27/2025 18:37:12 INFO]: Test stats: {
    "score": -1.0195287952055851,
    "rmse": 1.0195287952055851
}
[08/27/2025 18:37:14 INFO]: Training loss at epoch 2: 0.9854137301445007
[08/27/2025 18:37:19 INFO]: Training loss at epoch 28: 1.1152772903442383
[08/27/2025 18:37:31 INFO]: New best epoch, val score: -0.8780256419982834
[08/27/2025 18:37:31 INFO]: Saving model to: maddest-Elbert_trial_43/model_best.pth
[08/27/2025 18:37:51 INFO]: Training loss at epoch 22: 1.0063929557800293
[08/27/2025 18:37:52 INFO]: Training loss at epoch 5: 1.4544521570205688
[08/27/2025 18:38:10 INFO]: Training loss at epoch 11: 1.3038641810417175
[08/27/2025 18:38:21 INFO]: Training loss at epoch 77: 0.6231732368469238
[08/27/2025 18:38:39 INFO]: Training loss at epoch 19: 0.8544888198375702
[08/27/2025 18:38:43 INFO]: Training loss at epoch 29: 0.7429032921791077
[08/27/2025 18:38:47 INFO]: Training loss at epoch 12: 1.1038997173309326
[08/27/2025 18:39:25 INFO]: Training loss at epoch 27: 0.8451276123523712
[08/27/2025 18:39:29 INFO]: Training stats: {
    "score": -0.983138262557984,
    "rmse": 0.983138262557984
}
[08/27/2025 18:39:29 INFO]: Val stats: {
    "score": -0.942597731610491,
    "rmse": 0.942597731610491
}
[08/27/2025 18:39:29 INFO]: Test stats: {
    "score": -0.9691468558680264,
    "rmse": 0.9691468558680264
}
[08/27/2025 18:39:35 INFO]: Training loss at epoch 3: 1.1953096389770508
[08/27/2025 18:39:54 INFO]: Training stats: {
    "score": -0.9300553967983298,
    "rmse": 0.9300553967983298
}
[08/27/2025 18:39:54 INFO]: Val stats: {
    "score": -0.9723233309439868,
    "rmse": 0.9723233309439868
}
[08/27/2025 18:39:54 INFO]: Test stats: {
    "score": -0.9692917895883726,
    "rmse": 0.9692917895883726
}
[08/27/2025 18:40:03 INFO]: Training loss at epoch 46: 1.0871756076812744
[08/27/2025 18:40:07 INFO]: Training loss at epoch 41: 1.1690319776535034
[08/27/2025 18:40:19 INFO]: Training loss at epoch 6: 1.094014286994934
[08/27/2025 18:40:22 INFO]: Training loss at epoch 20: 1.1753459572792053
[08/27/2025 18:40:35 INFO]: Training loss at epoch 35: 0.9488800764083862
[08/27/2025 18:40:37 INFO]: Training loss at epoch 41: 0.9439487755298615
[08/27/2025 18:40:37 INFO]: Training loss at epoch 12: 1.0049154162406921
[08/27/2025 18:41:14 INFO]: Training loss at epoch 13: 1.2413782477378845
[08/27/2025 18:41:15 INFO]: Training loss at epoch 29: 0.7990069389343262
[08/27/2025 18:41:20 INFO]: Training loss at epoch 32: 1.2196131348609924
[08/27/2025 18:41:23 INFO]: Training loss at epoch 78: 0.5892481207847595
[08/27/2025 18:41:43 INFO]: Running Final Evaluation...
[08/27/2025 18:41:56 INFO]: Training loss at epoch 28: 1.0595832765102386
[08/27/2025 18:41:57 INFO]: Training loss at epoch 4: 0.8018132448196411
[08/27/2025 18:41:59 INFO]: Training loss at epoch 20: 0.8438642621040344
[08/27/2025 18:42:22 INFO]: Training loss at epoch 20: 1.2793248891830444
[08/27/2025 18:42:34 INFO]: Training stats: {
    "score": -0.9770789023706243,
    "rmse": 0.9770789023706243
}
[08/27/2025 18:42:34 INFO]: Val stats: {
    "score": -0.9052070784560189,
    "rmse": 0.9052070784560189
}
[08/27/2025 18:42:34 INFO]: Test stats: {
    "score": -0.9709744467711148,
    "rmse": 0.9709744467711148
}
[08/27/2025 18:42:48 INFO]: Training accuracy: {
    "score": -0.9808575152782341,
    "rmse": 0.9808575152782341
}
[08/27/2025 18:42:48 INFO]: Val accuracy: {
    "score": -0.885235924617268,
    "rmse": 0.885235924617268
}
[08/27/2025 18:42:48 INFO]: Test accuracy: {
    "score": -1.001197027198553,
    "rmse": 1.001197027198553
}
[08/27/2025 18:42:48 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_8",
    "best_epoch": 47,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.001197027198553,
        "rmse": 1.001197027198553
    },
    "train_stats": {
        "score": -0.9808575152782341,
        "rmse": 0.9808575152782341
    },
    "val_stats": {
        "score": -0.885235924617268,
        "rmse": 0.885235924617268
    }
}
[08/27/2025 18:42:48 INFO]: Procewss finished for trial maddest-Elbert_trial_8
[08/27/2025 18:42:48 INFO]: 
_________________________________________________

[08/27/2025 18:42:48 INFO]: train_net_for_optune.py main() running.
[08/27/2025 18:42:48 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 1.3411783931135706
  attention_dropout: 0.052067629390900476
  ffn_dropout: 0.052067629390900476
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.6106263137034267e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_44

[08/27/2025 18:42:48 INFO]: This ft_transformer has 4.876 million parameters.
[08/27/2025 18:42:48 INFO]: Training will start at epoch 0.
[08/27/2025 18:42:48 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 18:42:49 INFO]: Training loss at epoch 7: 1.4300530552864075
[08/27/2025 18:42:54 INFO]: Training loss at epoch 31: 1.169862151145935
[08/27/2025 18:42:56 INFO]: Training loss at epoch 39: 0.9304045736789703
[08/27/2025 18:43:02 INFO]: Training loss at epoch 40: 0.8057909607887268
[08/27/2025 18:43:03 INFO]: Training loss at epoch 13: 0.9429194629192352
[08/27/2025 18:43:30 INFO]: Training loss at epoch 30: 0.9072189927101135
[08/27/2025 18:43:42 INFO]: Training loss at epoch 14: 1.1272314488887787
[08/27/2025 18:44:18 INFO]: Training loss at epoch 5: 0.9928295612335205
[08/27/2025 18:44:26 INFO]: Training loss at epoch 29: 0.9262740314006805
[08/27/2025 18:44:29 INFO]: Training loss at epoch 21: 1.1934754550457
[08/27/2025 18:44:52 INFO]: Training loss at epoch 0: 1.6867179870605469
[08/27/2025 18:44:57 INFO]: Training loss at epoch 47: 0.9376298189163208
[08/27/2025 18:44:59 INFO]: Training stats: {
    "score": -1.0007623016071752,
    "rmse": 1.0007623016071752
}
[08/27/2025 18:44:59 INFO]: Val stats: {
    "score": -0.9402613757986937,
    "rmse": 0.9402613757986937
}
[08/27/2025 18:44:59 INFO]: Test stats: {
    "score": -0.9729586702549068,
    "rmse": 0.9729586702549068
}
[08/27/2025 18:45:08 INFO]: New best epoch, val score: -1.2740562630390873
[08/27/2025 18:45:08 INFO]: Saving model to: maddest-Elbert_trial_44/model_best.pth
[08/27/2025 18:45:15 INFO]: Training loss at epoch 23: 0.8581036627292633
[08/27/2025 18:45:17 INFO]: Training stats: {
    "score": -0.9672300016415458,
    "rmse": 0.9672300016415458
}
[08/27/2025 18:45:17 INFO]: Val stats: {
    "score": -0.9201302551595788,
    "rmse": 0.9201302551595788
}
[08/27/2025 18:45:17 INFO]: Test stats: {
    "score": -0.9696720724130697,
    "rmse": 0.9696720724130697
}
[08/27/2025 18:45:18 INFO]: Training loss at epoch 8: 1.2527397871017456
[08/27/2025 18:45:27 INFO]: Training loss at epoch 33: 0.9751514196395874
[08/27/2025 18:45:29 INFO]: Training loss at epoch 14: 0.9237226843833923
[08/27/2025 18:45:36 INFO]: Training loss at epoch 42: 0.9661847949028015
[08/27/2025 18:45:45 INFO]: Training loss at epoch 42: 1.2016098499298096
[08/27/2025 18:46:11 INFO]: Training loss at epoch 15: 0.9287014007568359
[08/27/2025 18:46:31 INFO]: Training loss at epoch 30: 0.9386898279190063
[08/27/2025 18:46:39 INFO]: Training loss at epoch 6: 0.8447467088699341
[08/27/2025 18:46:58 INFO]: Training loss at epoch 22: 0.8996441662311554
[08/27/2025 18:47:04 INFO]: Training loss at epoch 36: 0.9660983383655548
[08/27/2025 18:47:07 INFO]: Training loss at epoch 31: 0.7638834714889526
[08/27/2025 18:47:12 INFO]: Training loss at epoch 1: 1.2043685615062714
[08/27/2025 18:47:30 INFO]: New best epoch, val score: -0.8955708026810983
[08/27/2025 18:47:30 INFO]: Saving model to: maddest-Elbert_trial_44/model_best.pth
[08/27/2025 18:47:48 INFO]: Training loss at epoch 9: 1.1493061780929565
[08/27/2025 18:47:49 INFO]: Training loss at epoch 30: 0.9230089485645294
[08/27/2025 18:47:59 INFO]: Training loss at epoch 15: 1.2823398113250732
[08/27/2025 18:48:39 INFO]: Training stats: {
    "score": -1.1254315735512148,
    "rmse": 1.1254315735512148
}
[08/27/2025 18:48:39 INFO]: Val stats: {
    "score": -1.1854444899821186,
    "rmse": 1.1854444899821186
}
[08/27/2025 18:48:39 INFO]: Test stats: {
    "score": -1.1026357878922,
    "rmse": 1.1026357878922
}
[08/27/2025 18:48:40 INFO]: Training loss at epoch 16: 0.7482869327068329
[08/27/2025 18:48:51 INFO]: Training loss at epoch 21: 1.13002210855484
[08/27/2025 18:48:52 INFO]: Training loss at epoch 41: 0.6840691268444061
[08/27/2025 18:49:01 INFO]: Training loss at epoch 7: 0.9815168082714081
[08/27/2025 18:49:26 INFO]: Training loss at epoch 23: 0.8161098957061768
[08/27/2025 18:49:33 INFO]: Training loss at epoch 2: 1.2274301052093506
[08/27/2025 18:49:34 INFO]: Training loss at epoch 34: 1.001253753900528
[08/27/2025 18:49:50 INFO]: Training loss at epoch 48: 0.8491886556148529
[08/27/2025 18:50:19 INFO]: Training loss at epoch 31: 1.0445736050605774
[08/27/2025 18:50:24 INFO]: Training loss at epoch 16: 0.8908396363258362
[08/27/2025 18:50:24 INFO]: Training loss at epoch 31: 1.121301144361496
[08/27/2025 18:50:24 INFO]: Training loss at epoch 32: 1.0419790744781494
[08/27/2025 18:50:35 INFO]: Training loss at epoch 43: 0.991379976272583
[08/27/2025 18:50:42 INFO]: Training loss at epoch 32: 0.9495157599449158
[08/27/2025 18:51:04 INFO]: Training loss at epoch 40: 0.894749641418457
[08/27/2025 18:51:06 INFO]: Training loss at epoch 17: 0.8830581903457642
[08/27/2025 18:51:07 INFO]: Training loss at epoch 10: 1.5851531028747559
[08/27/2025 18:51:11 INFO]: Running Final Evaluation...
[08/27/2025 18:51:20 INFO]: Training loss at epoch 8: 0.926709920167923
[08/27/2025 18:51:21 INFO]: Training loss at epoch 43: 1.0651548504829407
[08/27/2025 18:51:49 INFO]: Training loss at epoch 3: 1.5089913606643677
[08/27/2025 18:51:50 INFO]: Training loss at epoch 24: 1.093624860048294
[08/27/2025 18:52:34 INFO]: Training loss at epoch 24: 1.1978447437286377
[08/27/2025 18:52:46 INFO]: Training loss at epoch 32: 0.9145398437976837
[08/27/2025 18:52:48 INFO]: Training loss at epoch 17: 0.8093224167823792
[08/27/2025 18:53:28 INFO]: Training loss at epoch 37: 1.11687433719635
[08/27/2025 18:53:29 INFO]: Training loss at epoch 18: 0.76743945479393
[08/27/2025 18:53:31 INFO]: Training accuracy: {
    "score": -1.0398396451981766,
    "rmse": 1.0398396451981766
}
[08/27/2025 18:53:31 INFO]: Val accuracy: {
    "score": -0.8809944492577898,
    "rmse": 0.8809944492577898
}
[08/27/2025 18:53:31 INFO]: Test accuracy: {
    "score": -1.011913602587956,
    "rmse": 1.011913602587956
}
[08/27/2025 18:53:32 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_27",
    "best_epoch": 12,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.011913602587956,
        "rmse": 1.011913602587956
    },
    "train_stats": {
        "score": -1.0398396451981766,
        "rmse": 1.0398396451981766
    },
    "val_stats": {
        "score": -0.8809944492577898,
        "rmse": 0.8809944492577898
    }
}
[08/27/2025 18:53:32 INFO]: Procewss finished for trial maddest-Elbert_trial_27
[08/27/2025 18:53:32 INFO]: 
_________________________________________________

[08/27/2025 18:53:32 INFO]: train_net_for_optune.py main() running.
[08/27/2025 18:53:32 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 1.3149881468478961
  attention_dropout: 0.17372914295288078
  ffn_dropout: 0.17372914295288078
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.1389978344373723e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_45

[08/27/2025 18:53:32 INFO]: This ft_transformer has 8.520 million parameters.
[08/27/2025 18:53:32 INFO]: Training will start at epoch 0.
[08/27/2025 18:53:32 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 18:53:35 INFO]: Training loss at epoch 11: 0.9760755896568298
[08/27/2025 18:53:38 INFO]: Training loss at epoch 9: 1.0303320288658142
[08/27/2025 18:53:39 INFO]: Training loss at epoch 35: 0.9526745080947876
[08/27/2025 18:53:46 INFO]: Training loss at epoch 21: 0.8611372113227844
[08/27/2025 18:54:11 INFO]: Training loss at epoch 4: 1.2441416382789612
[08/27/2025 18:54:14 INFO]: Training loss at epoch 33: 0.7333549559116364
[08/27/2025 18:54:16 INFO]: Training loss at epoch 32: 1.0442563891410828
[08/27/2025 18:54:20 INFO]: Training loss at epoch 25: 0.8573667109012604
[08/27/2025 18:54:26 INFO]: Training stats: {
    "score": -0.9895022140832633,
    "rmse": 0.9895022140832633
}
[08/27/2025 18:54:26 INFO]: Val stats: {
    "score": -0.9535110961023493,
    "rmse": 0.9535110961023493
}
[08/27/2025 18:54:26 INFO]: Test stats: {
    "score": -0.9756282535878217,
    "rmse": 0.9756282535878217
}
[08/27/2025 18:54:38 INFO]: Training loss at epoch 49: 1.1956284046173096
[08/27/2025 18:54:38 INFO]: Training loss at epoch 42: 0.7260855436325073
[08/27/2025 18:55:14 INFO]: Training loss at epoch 18: 1.0490184128284454
[08/27/2025 18:55:17 INFO]: Training loss at epoch 33: 1.006436139345169
[08/27/2025 18:55:58 INFO]: Training loss at epoch 19: 1.0770027339458466
[08/27/2025 18:56:06 INFO]: Training loss at epoch 12: 1.1995791792869568
[08/27/2025 18:56:18 INFO]: Training stats: {
    "score": -0.9793493783734741,
    "rmse": 0.9793493783734741
}
[08/27/2025 18:56:18 INFO]: Val stats: {
    "score": -0.9408878727996164,
    "rmse": 0.9408878727996164
}
[08/27/2025 18:56:18 INFO]: Test stats: {
    "score": -0.9683376166784433,
    "rmse": 0.9683376166784433
}
[08/27/2025 18:56:33 INFO]: Training loss at epoch 5: 0.8017596006393433
[08/27/2025 18:56:48 INFO]: Training loss at epoch 10: 1.0295020639896393
[08/27/2025 18:56:49 INFO]: Training stats: {
    "score": -0.9629521932756975,
    "rmse": 0.9629521932756975
}
[08/27/2025 18:56:49 INFO]: Val stats: {
    "score": -0.8998241771770533,
    "rmse": 0.8998241771770533
}
[08/27/2025 18:56:49 INFO]: Test stats: {
    "score": -0.9718801705254317,
    "rmse": 0.9718801705254317
}
[08/27/2025 18:56:49 INFO]: Training loss at epoch 26: 0.9361220896244049
[08/27/2025 18:56:49 INFO]: Training loss at epoch 0: 1.0338598787784576
[08/27/2025 18:56:56 INFO]: Training loss at epoch 44: 0.9520356953144073
[08/27/2025 18:57:08 INFO]: Training loss at epoch 41: 0.963125079870224
[08/27/2025 18:57:15 INFO]: Training loss at epoch 22: 1.0732438564300537
[08/27/2025 18:57:16 INFO]: New best epoch, val score: -1.0002657133109325
[08/27/2025 18:57:16 INFO]: Saving model to: maddest-Elbert_trial_45/model_best.pth
[08/27/2025 18:57:41 INFO]: Training loss at epoch 19: 0.9749228358268738
[08/27/2025 18:57:47 INFO]: Training loss at epoch 34: 0.8755126893520355
[08/27/2025 18:57:48 INFO]: Training loss at epoch 36: 0.9316730499267578
[08/27/2025 18:57:50 INFO]: Training loss at epoch 34: 0.9179655909538269
[08/27/2025 18:57:54 INFO]: Training loss at epoch 33: 1.0092434883117676
[08/27/2025 18:58:12 INFO]: Training loss at epoch 33: 0.9727562665939331
[08/27/2025 18:58:31 INFO]: Training stats: {
    "score": -1.00654960421607,
    "rmse": 1.00654960421607
}
[08/27/2025 18:58:31 INFO]: Val stats: {
    "score": -0.9764720985382518,
    "rmse": 0.9764720985382518
}
[08/27/2025 18:58:31 INFO]: Test stats: {
    "score": -0.9819367242178322,
    "rmse": 0.9819367242178322
}
[08/27/2025 18:58:35 INFO]: Training loss at epoch 13: 1.4790316820144653
[08/27/2025 18:58:53 INFO]: Training loss at epoch 6: 1.1068570613861084
[08/27/2025 18:59:10 INFO]: Training loss at epoch 11: 0.8557913899421692
[08/27/2025 18:59:18 INFO]: Training loss at epoch 20: 1.068848967552185
[08/27/2025 18:59:20 INFO]: Training loss at epoch 27: 0.9898169636726379
[08/27/2025 18:59:59 INFO]: Training loss at epoch 38: 0.8446373045444489
[08/27/2025 19:00:04 INFO]: Training loss at epoch 25: 0.8978528678417206
[08/27/2025 19:00:18 INFO]: Training loss at epoch 35: 1.35884228348732
[08/27/2025 19:00:30 INFO]: Training loss at epoch 43: 0.8376215994358063
[08/27/2025 19:00:34 INFO]: Training loss at epoch 1: 0.9384907484054565
[08/27/2025 19:00:58 INFO]: Training loss at epoch 20: 0.9677081108093262
[08/27/2025 19:01:00 INFO]: New best epoch, val score: -0.9703342310218648
[08/27/2025 19:01:00 INFO]: Saving model to: maddest-Elbert_trial_45/model_best.pth
[08/27/2025 19:01:03 INFO]: Training loss at epoch 14: 1.3219005465507507
[08/27/2025 19:01:10 INFO]: Training loss at epoch 50: 1.007091075181961
[08/27/2025 19:01:14 INFO]: Training loss at epoch 7: 1.0145303010940552
[08/27/2025 19:01:26 INFO]: Training loss at epoch 35: 0.7268395125865936
[08/27/2025 19:01:30 INFO]: Training loss at epoch 12: 1.0314851701259613
[08/27/2025 19:01:46 INFO]: Training loss at epoch 21: 0.9109576046466827
[08/27/2025 19:01:52 INFO]: Training loss at epoch 28: 0.8077824711799622
[08/27/2025 19:01:56 INFO]: Training loss at epoch 37: 1.0335545539855957
[08/27/2025 19:02:07 INFO]: Training loss at epoch 34: 1.1866873800754547
[08/27/2025 19:02:35 INFO]: Training loss at epoch 45: 1.0616844594478607
[08/27/2025 19:02:47 INFO]: Training loss at epoch 36: 0.8435827195644379
[08/27/2025 19:03:12 INFO]: Training loss at epoch 42: 0.9014652967453003
[08/27/2025 19:03:23 INFO]: Training loss at epoch 21: 1.1417783498764038
[08/27/2025 19:03:32 INFO]: Training loss at epoch 15: 1.0763450860977173
[08/27/2025 19:03:34 INFO]: Training loss at epoch 8: 0.9492661654949188
[08/27/2025 19:03:50 INFO]: New best epoch, val score: -0.8817473366682378
[08/27/2025 19:03:50 INFO]: Saving model to: maddest-Elbert_trial_42/model_best.pth
[08/27/2025 19:03:51 INFO]: Training loss at epoch 13: 0.9759772717952728
[08/27/2025 19:04:12 INFO]: Training loss at epoch 22: 0.8968358039855957
[08/27/2025 19:04:17 INFO]: Training loss at epoch 2: 1.0701371729373932
[08/27/2025 19:04:21 INFO]: Training loss at epoch 29: 0.8690861761569977
[08/27/2025 19:05:01 INFO]: Training loss at epoch 36: 0.8276331424713135
[08/27/2025 19:05:12 INFO]: Training stats: {
    "score": -0.9776345455401094,
    "rmse": 0.9776345455401094
}
[08/27/2025 19:05:12 INFO]: Val stats: {
    "score": -0.8967978959647814,
    "rmse": 0.8967978959647814
}
[08/27/2025 19:05:12 INFO]: Test stats: {
    "score": -0.9725961752448374,
    "rmse": 0.9725961752448374
}
[08/27/2025 19:05:14 INFO]: Training loss at epoch 22: 1.2246547937393188
[08/27/2025 19:05:18 INFO]: Training loss at epoch 37: 0.8488388359546661
[08/27/2025 19:05:25 INFO]: Training loss at epoch 34: 1.352329820394516
[08/27/2025 19:05:43 INFO]: Training loss at epoch 23: 0.9266160130500793
[08/27/2025 19:05:50 INFO]: Training loss at epoch 22: 0.9487724006175995
[08/27/2025 19:05:54 INFO]: Training loss at epoch 9: 0.9647527039051056
[08/27/2025 19:06:01 INFO]: Training loss at epoch 35: 0.7909814417362213
[08/27/2025 19:06:02 INFO]: Training loss at epoch 16: 1.022836148738861
[08/27/2025 19:06:02 INFO]: Training loss at epoch 51: 0.9376835227012634
[08/27/2025 19:06:06 INFO]: Training loss at epoch 38: 0.949613630771637
[08/27/2025 19:06:12 INFO]: Training loss at epoch 14: 1.1947234869003296
[08/27/2025 19:06:19 INFO]: Training loss at epoch 44: 0.7744668424129486
[08/27/2025 19:06:27 INFO]: Training loss at epoch 39: 0.9045918881893158
[08/27/2025 19:06:41 INFO]: Training loss at epoch 23: 0.9624597430229187
[08/27/2025 19:06:42 INFO]: Training stats: {
    "score": -0.9943944348425916,
    "rmse": 0.9943944348425916
}
[08/27/2025 19:06:42 INFO]: Val stats: {
    "score": -0.9425464226225134,
    "rmse": 0.9425464226225134
}
[08/27/2025 19:06:42 INFO]: Test stats: {
    "score": -0.974241217918671,
    "rmse": 0.974241217918671
}
[08/27/2025 19:07:27 INFO]: Training loss at epoch 26: 0.9470970332622528
[08/27/2025 19:07:41 INFO]: Training loss at epoch 30: 0.8379641771316528
[08/27/2025 19:07:49 INFO]: Training loss at epoch 38: 0.9002406895160675
[08/27/2025 19:08:00 INFO]: Training loss at epoch 3: 0.9169419407844543
[08/27/2025 19:08:12 INFO]: Training loss at epoch 46: 0.95726078748703
[08/27/2025 19:08:17 INFO]: Training loss at epoch 23: 1.01087686419487
[08/27/2025 19:08:30 INFO]: Training loss at epoch 17: 1.1737309694290161
[08/27/2025 19:08:33 INFO]: Training loss at epoch 15: 0.7426515221595764
[08/27/2025 19:08:37 INFO]: Training loss at epoch 37: 0.9471748471260071
[08/27/2025 19:08:37 INFO]: Training stats: {
    "score": -1.0118531180763537,
    "rmse": 1.0118531180763537
}
[08/27/2025 19:08:37 INFO]: Val stats: {
    "score": -0.888494425151309,
    "rmse": 0.888494425151309
}
[08/27/2025 19:08:37 INFO]: Test stats: {
    "score": -0.9839965354297803,
    "rmse": 0.9839965354297803
}
[08/27/2025 19:09:03 INFO]: Training loss at epoch 10: 0.9748484790325165
[08/27/2025 19:09:11 INFO]: Training loss at epoch 24: 0.9092161953449249
[08/27/2025 19:09:15 INFO]: Training loss at epoch 43: 1.0414540767669678
[08/27/2025 19:09:23 INFO]: Running Final Evaluation...
[08/27/2025 19:09:57 INFO]: Training loss at epoch 36: 0.9805132448673248
[08/27/2025 19:10:11 INFO]: Training loss at epoch 31: 0.9119438529014587
[08/27/2025 19:10:15 INFO]: Training loss at epoch 39: 0.9505340158939362
[08/27/2025 19:10:21 INFO]: Training loss at epoch 39: 0.9066711366176605
[08/27/2025 19:10:43 INFO]: Training loss at epoch 24: 0.8249395489692688
[08/27/2025 19:10:55 INFO]: Training loss at epoch 16: 0.81558558344841
[08/27/2025 19:10:56 INFO]: Training loss at epoch 52: 1.0282509922981262
[08/27/2025 19:11:00 INFO]: Training loss at epoch 18: 1.1685492992401123
[08/27/2025 19:11:13 INFO]: Training stats: {
    "score": -0.9592756663975909,
    "rmse": 0.9592756663975909
}
[08/27/2025 19:11:13 INFO]: Val stats: {
    "score": -0.9638559491981055,
    "rmse": 0.9638559491981055
}
[08/27/2025 19:11:13 INFO]: Test stats: {
    "score": -0.9773495875109145,
    "rmse": 0.9773495875109145
}
[08/27/2025 19:11:24 INFO]: Training loss at epoch 11: 1.1640372276306152
[08/27/2025 19:11:30 INFO]: Running Final Evaluation...
[08/27/2025 19:11:38 INFO]: Training accuracy: {
    "score": -1.0204277982606669,
    "rmse": 1.0204277982606669
}
[08/27/2025 19:11:38 INFO]: Val accuracy: {
    "score": -0.8838108133381241,
    "rmse": 0.8838108133381241
}
[08/27/2025 19:11:38 INFO]: Test accuracy: {
    "score": -0.9919727204530909,
    "rmse": 0.9919727204530909
}
[08/27/2025 19:11:38 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_22",
    "best_epoch": 8,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9919727204530909,
        "rmse": 0.9919727204530909
    },
    "train_stats": {
        "score": -1.0204277982606669,
        "rmse": 1.0204277982606669
    },
    "val_stats": {
        "score": -0.8838108133381241,
        "rmse": 0.8838108133381241
    }
}
[08/27/2025 19:11:38 INFO]: Procewss finished for trial maddest-Elbert_trial_22
[08/27/2025 19:11:38 INFO]: 
_________________________________________________

[08/27/2025 19:11:38 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:11:38 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 1.2986364779819914
  attention_dropout: 0.16893677546509808
  ffn_dropout: 0.16893677546509808
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.160819270959262e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_46

[08/27/2025 19:11:39 INFO]: This ft_transformer has 8.465 million parameters.
[08/27/2025 19:11:39 INFO]: Training will start at epoch 0.
[08/27/2025 19:11:39 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:11:40 INFO]: Training loss at epoch 25: 1.1003339290618896
[08/27/2025 19:11:44 INFO]: Training loss at epoch 4: 0.9364980459213257
[08/27/2025 19:11:44 INFO]: Training stats: {
    "score": -0.994709059765523,
    "rmse": 0.994709059765523
}
[08/27/2025 19:11:44 INFO]: Val stats: {
    "score": -0.9152263739784173,
    "rmse": 0.9152263739784173
}
[08/27/2025 19:11:44 INFO]: Test stats: {
    "score": -0.9691651134816625,
    "rmse": 0.9691651134816625
}
[08/27/2025 19:12:10 INFO]: New best epoch, val score: -0.9097567161503882
[08/27/2025 19:12:10 INFO]: Saving model to: maddest-Elbert_trial_45/model_best.pth
[08/27/2025 19:12:13 INFO]: Training loss at epoch 38: 0.8640652894973755
[08/27/2025 19:12:14 INFO]: Training loss at epoch 45: 1.0519838333129883
[08/27/2025 19:12:40 INFO]: Training loss at epoch 32: 1.086986482143402
[08/27/2025 19:12:57 INFO]: Training loss at epoch 35: 0.90707728266716
[08/27/2025 19:12:58 INFO]: Running Final Evaluation...
[08/27/2025 19:13:09 INFO]: Training loss at epoch 25: 1.3069826662540436
[08/27/2025 19:13:16 INFO]: Training loss at epoch 17: 1.1302100121974945
[08/27/2025 19:13:24 INFO]: Training accuracy: {
    "score": -1.0332886191562753,
    "rmse": 1.0332886191562753
}
[08/27/2025 19:13:24 INFO]: Val accuracy: {
    "score": -0.8824127394478518,
    "rmse": 0.8824127394478518
}
[08/27/2025 19:13:24 INFO]: Test accuracy: {
    "score": -1.0088196852517035,
    "rmse": 1.0088196852517035
}
[08/27/2025 19:13:24 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_23",
    "best_epoch": 21,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0088196852517035,
        "rmse": 1.0088196852517035
    },
    "train_stats": {
        "score": -1.0332886191562753,
        "rmse": 1.0332886191562753
    },
    "val_stats": {
        "score": -0.8824127394478518,
        "rmse": 0.8824127394478518
    }
}
[08/27/2025 19:13:24 INFO]: Procewss finished for trial maddest-Elbert_trial_23
[08/27/2025 19:13:24 INFO]: 
_________________________________________________

[08/27/2025 19:13:24 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:13:24 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 1.3504067567668718
  attention_dropout: 0.060308825044204276
  ffn_dropout: 0.060308825044204276
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.2935656410227327e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_47

[08/27/2025 19:13:24 INFO]: This ft_transformer has 0.155 million parameters.
[08/27/2025 19:13:24 INFO]: Training will start at epoch 0.
[08/27/2025 19:13:24 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:13:29 INFO]: Training loss at epoch 19: 1.0519028902053833
[08/27/2025 19:13:40 INFO]: Training loss at epoch 0: 1.3141982555389404
[08/27/2025 19:13:43 INFO]: New best epoch, val score: -1.0037298310645881
[08/27/2025 19:13:43 INFO]: Saving model to: maddest-Elbert_trial_47/model_best.pth
[08/27/2025 19:13:44 INFO]: Training loss at epoch 40: 0.6554974317550659
[08/27/2025 19:13:45 INFO]: Training loss at epoch 12: 1.0381243526935577
[08/27/2025 19:13:51 INFO]: Training accuracy: {
    "score": -1.0538013172026441,
    "rmse": 1.0538013172026441
}
[08/27/2025 19:13:51 INFO]: Val accuracy: {
    "score": -0.8806504134280243,
    "rmse": 0.8806504134280243
}
[08/27/2025 19:13:51 INFO]: Test accuracy: {
    "score": -1.022250307597806,
    "rmse": 1.022250307597806
}
[08/27/2025 19:13:51 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_39",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.022250307597806,
        "rmse": 1.022250307597806
    },
    "train_stats": {
        "score": -1.0538013172026441,
        "rmse": 1.0538013172026441
    },
    "val_stats": {
        "score": -0.8806504134280243,
        "rmse": 0.8806504134280243
    }
}
[08/27/2025 19:13:51 INFO]: Procewss finished for trial maddest-Elbert_trial_39
[08/27/2025 19:13:51 INFO]: 
_________________________________________________

[08/27/2025 19:13:51 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:13:51 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 1.1269171586742392
  attention_dropout: 0.17513521313346023
  ffn_dropout: 0.17513521313346023
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00016502310826929458
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_48

[08/27/2025 19:13:51 INFO]: This ft_transformer has 0.145 million parameters.
[08/27/2025 19:13:51 INFO]: Training will start at epoch 0.
[08/27/2025 19:13:51 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:13:51 INFO]: Training loss at epoch 47: 0.9277994632720947
[08/27/2025 19:13:51 INFO]: Training loss at epoch 37: 1.0374445617198944
[08/27/2025 19:13:59 INFO]: Training loss at epoch 1: 0.9517333805561066
[08/27/2025 19:14:02 INFO]: New best epoch, val score: -0.970232420304399
[08/27/2025 19:14:02 INFO]: Saving model to: maddest-Elbert_trial_47/model_best.pth
[08/27/2025 19:14:02 INFO]: Running Final Evaluation...
[08/27/2025 19:14:07 INFO]: Training loss at epoch 0: 1.0375924110412598
[08/27/2025 19:14:07 INFO]: Training loss at epoch 26: 0.9205796122550964
[08/27/2025 19:14:10 INFO]: New best epoch, val score: -0.9110027880973599
[08/27/2025 19:14:10 INFO]: Saving model to: maddest-Elbert_trial_48/model_best.pth
[08/27/2025 19:14:12 INFO]: Training loss at epoch 24: 0.8818472027778625
[08/27/2025 19:14:19 INFO]: Training loss at epoch 2: 1.3644078373908997
[08/27/2025 19:14:20 INFO]: Training stats: {
    "score": -1.0440707988247246,
    "rmse": 1.0440707988247246
}
[08/27/2025 19:14:20 INFO]: Val stats: {
    "score": -1.0574696263634156,
    "rmse": 1.0574696263634156
}
[08/27/2025 19:14:20 INFO]: Test stats: {
    "score": -1.0201180056471473,
    "rmse": 1.0201180056471473
}
[08/27/2025 19:14:21 INFO]: New best epoch, val score: -0.9329695175620564
[08/27/2025 19:14:21 INFO]: Saving model to: maddest-Elbert_trial_47/model_best.pth
[08/27/2025 19:14:25 INFO]: Training loss at epoch 1: 0.9606304466724396
[08/27/2025 19:14:28 INFO]: New best epoch, val score: -0.9057513310613475
[08/27/2025 19:14:28 INFO]: Saving model to: maddest-Elbert_trial_48/model_best.pth
[08/27/2025 19:14:38 INFO]: Training loss at epoch 3: 0.9767220914363861
[08/27/2025 19:14:41 INFO]: New best epoch, val score: -0.9049555708921095
[08/27/2025 19:14:41 INFO]: Saving model to: maddest-Elbert_trial_47/model_best.pth
[08/27/2025 19:14:45 INFO]: Training loss at epoch 2: 1.0439656972885132
[08/27/2025 19:14:51 INFO]: Training loss at epoch 27: 1.0443934202194214
[08/27/2025 19:14:52 INFO]: Training loss at epoch 0: 1.1648237705230713
[08/27/2025 19:14:54 INFO]: Training accuracy: {
    "score": -1.0268629513996803,
    "rmse": 1.0268629513996803
}
[08/27/2025 19:14:54 INFO]: Val accuracy: {
    "score": -0.8834771727733436,
    "rmse": 0.8834771727733436
}
[08/27/2025 19:14:54 INFO]: Test accuracy: {
    "score": -1.007317205644866,
    "rmse": 1.007317205644866
}
[08/27/2025 19:14:54 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_38",
    "best_epoch": 9,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.007317205644866,
        "rmse": 1.007317205644866
    },
    "train_stats": {
        "score": -1.0268629513996803,
        "rmse": 1.0268629513996803
    },
    "val_stats": {
        "score": -0.8834771727733436,
        "rmse": 0.8834771727733436
    }
}
[08/27/2025 19:14:54 INFO]: Procewss finished for trial maddest-Elbert_trial_38
[08/27/2025 19:14:54 INFO]: 
_________________________________________________

[08/27/2025 19:14:54 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:14:54 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 1.1203536626595123
  attention_dropout: 0.17661408127494377
  ffn_dropout: 0.17661408127494377
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00015095222332191474
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_49

[08/27/2025 19:14:54 INFO]: This ft_transformer has 0.144 million parameters.
[08/27/2025 19:14:54 INFO]: Training will start at epoch 0.
[08/27/2025 19:14:54 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:14:57 INFO]: Training loss at epoch 4: 0.9855850040912628
[08/27/2025 19:14:59 INFO]: New best epoch, val score: -0.8873090666542608
[08/27/2025 19:14:59 INFO]: Saving model to: maddest-Elbert_trial_47/model_best.pth
[08/27/2025 19:15:03 INFO]: Training loss at epoch 3: 1.0858446061611176
[08/27/2025 19:15:11 INFO]: Training loss at epoch 0: 1.5915610790252686
[08/27/2025 19:15:14 INFO]: New best epoch, val score: -1.138293545040655
[08/27/2025 19:15:14 INFO]: Saving model to: maddest-Elbert_trial_49/model_best.pth
[08/27/2025 19:15:16 INFO]: Training loss at epoch 5: 0.9045279622077942
[08/27/2025 19:15:18 INFO]: New best epoch, val score: -1.0946815585288134
[08/27/2025 19:15:18 INFO]: Saving model to: maddest-Elbert_trial_46/model_best.pth
[08/27/2025 19:15:19 INFO]: New best epoch, val score: -0.8863637475216688
[08/27/2025 19:15:19 INFO]: Saving model to: maddest-Elbert_trial_47/model_best.pth
[08/27/2025 19:15:20 INFO]: Training loss at epoch 44: 0.9201138913631439
[08/27/2025 19:15:22 INFO]: Training loss at epoch 4: 0.9434622228145599
[08/27/2025 19:15:26 INFO]: Training loss at epoch 5: 1.0335392653942108
[08/27/2025 19:15:30 INFO]: Training loss at epoch 1: 1.1758974194526672
[08/27/2025 19:15:32 INFO]: New best epoch, val score: -1.0143407347600741
[08/27/2025 19:15:32 INFO]: Saving model to: maddest-Elbert_trial_49/model_best.pth
[08/27/2025 19:15:35 INFO]: Training loss at epoch 26: 0.9513538479804993
[08/27/2025 19:15:36 INFO]: Training loss at epoch 6: 0.9649977087974548
[08/27/2025 19:15:36 INFO]: Training loss at epoch 18: 0.887347549200058
[08/27/2025 19:15:38 INFO]: New best epoch, val score: -0.8855651480209618
[08/27/2025 19:15:38 INFO]: Saving model to: maddest-Elbert_trial_47/model_best.pth
[08/27/2025 19:15:41 INFO]: Training loss at epoch 5: 1.0270505547523499
[08/27/2025 19:15:48 INFO]: Training loss at epoch 39: 0.8204106092453003
[08/27/2025 19:15:49 INFO]: Training loss at epoch 2: 0.9494813680648804
[08/27/2025 19:15:51 INFO]: Training loss at epoch 40: 0.9577756524085999
[08/27/2025 19:15:51 INFO]: New best epoch, val score: -0.9281953095512075
[08/27/2025 19:15:51 INFO]: Saving model to: maddest-Elbert_trial_49/model_best.pth
[08/27/2025 19:15:52 INFO]: New best epoch, val score: -0.9096843135186067
[08/27/2025 19:15:52 INFO]: Saving model to: maddest-Elbert_trial_45/model_best.pth
[08/27/2025 19:15:55 INFO]: Training loss at epoch 7: 1.1565037369728088
[08/27/2025 19:15:57 INFO]: New best epoch, val score: -0.8851346295024144
[08/27/2025 19:15:57 INFO]: Saving model to: maddest-Elbert_trial_47/model_best.pth
[08/27/2025 19:16:00 INFO]: Training loss at epoch 6: 0.9129689931869507
[08/27/2025 19:16:04 INFO]: Training loss at epoch 13: 0.8880979418754578
[08/27/2025 19:16:08 INFO]: Training loss at epoch 3: 0.9212629199028015
[08/27/2025 19:16:11 INFO]: New best epoch, val score: -0.8948987492801603
[08/27/2025 19:16:11 INFO]: Saving model to: maddest-Elbert_trial_49/model_best.pth
[08/27/2025 19:16:15 INFO]: Training loss at epoch 8: 1.2393506467342377
[08/27/2025 19:16:17 INFO]: New best epoch, val score: -0.8851110957555017
[08/27/2025 19:16:17 INFO]: Saving model to: maddest-Elbert_trial_47/model_best.pth
[08/27/2025 19:16:19 INFO]: Training loss at epoch 7: 1.0579046607017517
[08/27/2025 19:16:27 INFO]: Training loss at epoch 4: 0.9765416085720062
[08/27/2025 19:16:30 INFO]: New best epoch, val score: -0.886780365443234
[08/27/2025 19:16:30 INFO]: Saving model to: maddest-Elbert_trial_49/model_best.pth
[08/27/2025 19:16:34 INFO]: Training loss at epoch 9: 0.9554602205753326
[08/27/2025 19:16:35 INFO]: Training loss at epoch 27: 0.7229421436786652
[08/27/2025 19:16:38 INFO]: Training loss at epoch 8: 1.2156162858009338
[08/27/2025 19:16:41 INFO]: Training stats: {
    "score": -1.0066892479807554,
    "rmse": 1.0066892479807554
}
[08/27/2025 19:16:41 INFO]: Val stats: {
    "score": -0.8852499545997675,
    "rmse": 0.8852499545997675
}
[08/27/2025 19:16:41 INFO]: Test stats: {
    "score": -0.9821158304464036,
    "rmse": 0.9821158304464036
}
[08/27/2025 19:16:45 INFO]: Training loss at epoch 23: 0.8210679292678833
[08/27/2025 19:16:46 INFO]: Training loss at epoch 5: 0.9792170822620392
[08/27/2025 19:16:49 INFO]: Training loss at epoch 20: 0.993298351764679
[08/27/2025 19:16:57 INFO]: Training loss at epoch 9: 1.1177639365196228
[08/27/2025 19:17:01 INFO]: Training loss at epoch 10: 0.7674935758113861
[08/27/2025 19:17:02 INFO]: Training stats: {
    "score": -0.9151856725290548,
    "rmse": 0.9151856725290548
}
[08/27/2025 19:17:02 INFO]: Val stats: {
    "score": -0.9063163663422108,
    "rmse": 0.9063163663422108
}
[08/27/2025 19:17:02 INFO]: Test stats: {
    "score": -1.0225988149400274,
    "rmse": 1.0225988149400274
}
[08/27/2025 19:17:04 INFO]: Training stats: {
    "score": -1.0251867642561578,
    "rmse": 1.0251867642561578
}
[08/27/2025 19:17:04 INFO]: Val stats: {
    "score": -1.0108269975920812,
    "rmse": 1.0108269975920812
}
[08/27/2025 19:17:04 INFO]: Test stats: {
    "score": -0.9999759699063511,
    "rmse": 0.9999759699063511
}
[08/27/2025 19:17:05 INFO]: Training loss at epoch 6: 1.0008354485034943
[08/27/2025 19:17:20 INFO]: Training loss at epoch 11: 0.9858843386173248
[08/27/2025 19:17:23 INFO]: Training loss at epoch 10: 0.9660652577877045
[08/27/2025 19:17:24 INFO]: Training loss at epoch 7: 1.253451406955719
[08/27/2025 19:17:39 INFO]: Training loss at epoch 12: 0.9680706560611725
[08/27/2025 19:17:41 INFO]: Training loss at epoch 11: 1.5248180031776428
[08/27/2025 19:17:43 INFO]: Training loss at epoch 8: 1.0980790853500366
[08/27/2025 19:17:46 INFO]: Training loss at epoch 38: 0.7769011855125427
[08/27/2025 19:17:57 INFO]: Training loss at epoch 19: 0.8433382511138916
[08/27/2025 19:17:58 INFO]: Training loss at epoch 13: 0.9015042781829834
[08/27/2025 19:18:00 INFO]: Training loss at epoch 12: 1.1433421075344086
[08/27/2025 19:18:01 INFO]: Training loss at epoch 9: 1.0200954675674438
[08/27/2025 19:18:02 INFO]: Training loss at epoch 27: 0.9033805429935455
[08/27/2025 19:18:03 INFO]: Training loss at epoch 46: 0.6984801292419434
[08/27/2025 19:18:09 INFO]: Training stats: {
    "score": -1.0194454790502159,
    "rmse": 1.0194454790502159
}
[08/27/2025 19:18:09 INFO]: Val stats: {
    "score": -0.8880575948180616,
    "rmse": 0.8880575948180616
}
[08/27/2025 19:18:09 INFO]: Test stats: {
    "score": -0.991117429064382,
    "rmse": 0.991117429064382
}
[08/27/2025 19:18:18 INFO]: Training loss at epoch 14: 1.0521591305732727
[08/27/2025 19:18:20 INFO]: Training loss at epoch 13: 1.0520227253437042
[08/27/2025 19:18:22 INFO]: New best epoch, val score: -0.8987727091769184
[08/27/2025 19:18:22 INFO]: Saving model to: maddest-Elbert_trial_48/model_best.pth
[08/27/2025 19:18:24 INFO]: Training loss at epoch 14: 1.0317952334880829
[08/27/2025 19:18:27 INFO]: Training loss at epoch 10: 0.9102228581905365
[08/27/2025 19:18:33 INFO]: Training loss at epoch 1: 1.0759885609149933
[08/27/2025 19:18:38 INFO]: Training loss at epoch 15: 0.8788444995880127
[08/27/2025 19:18:39 INFO]: Training loss at epoch 14: 0.8621361255645752
[08/27/2025 19:18:42 INFO]: New best epoch, val score: -0.8884873912485093
[08/27/2025 19:18:42 INFO]: Saving model to: maddest-Elbert_trial_48/model_best.pth
[08/27/2025 19:18:46 INFO]: Training stats: {
    "score": -0.9804923870130458,
    "rmse": 0.9804923870130458
}
[08/27/2025 19:18:46 INFO]: Val stats: {
    "score": -0.9636407236047335,
    "rmse": 0.9636407236047335
}
[08/27/2025 19:18:46 INFO]: Test stats: {
    "score": -0.9788552040331512,
    "rmse": 0.9788552040331512
}
[08/27/2025 19:18:46 INFO]: Training loss at epoch 11: 1.2764447331428528
[08/27/2025 19:18:58 INFO]: Training loss at epoch 16: 0.9292033016681671
[08/27/2025 19:18:58 INFO]: Training loss at epoch 15: 1.1311323046684265
[08/27/2025 19:18:58 INFO]: New best epoch, val score: -0.8764524983660992
[08/27/2025 19:18:58 INFO]: Saving model to: maddest-Elbert_trial_46/model_best.pth
[08/27/2025 19:19:01 INFO]: New best epoch, val score: -0.8833804887760435
[08/27/2025 19:19:01 INFO]: Saving model to: maddest-Elbert_trial_48/model_best.pth
[08/27/2025 19:19:05 INFO]: Training loss at epoch 28: 0.9090852439403534
[08/27/2025 19:19:05 INFO]: Training loss at epoch 12: 1.1751859784126282
[08/27/2025 19:19:12 INFO]: Training loss at epoch 6: 1.1084411144256592
[08/27/2025 19:19:17 INFO]: Training loss at epoch 17: 1.1841726303100586
[08/27/2025 19:19:17 INFO]: Training loss at epoch 16: 1.054762840270996
[08/27/2025 19:19:18 INFO]: Training loss at epoch 21: 1.0382292568683624
[08/27/2025 19:19:19 INFO]: New best epoch, val score: -0.8820252989842269
[08/27/2025 19:19:19 INFO]: Saving model to: maddest-Elbert_trial_48/model_best.pth
[08/27/2025 19:19:26 INFO]: Training loss at epoch 13: 0.8564995229244232
[08/27/2025 19:19:30 INFO]: Training loss at epoch 48: 1.1036000847816467
[08/27/2025 19:19:36 INFO]: Training loss at epoch 17: 0.9492945969104767
[08/27/2025 19:19:37 INFO]: Training loss at epoch 18: 1.0399896502494812
[08/27/2025 19:19:45 INFO]: Training loss at epoch 14: 0.9314335286617279
[08/27/2025 19:19:55 INFO]: Training loss at epoch 18: 1.0804229378700256
[08/27/2025 19:19:56 INFO]: Training loss at epoch 19: 0.9796303510665894
[08/27/2025 19:19:59 INFO]: Training loss at epoch 41: 1.188856989145279
[08/27/2025 19:20:03 INFO]: Training stats: {
    "score": -1.0039439652394946,
    "rmse": 1.0039439652394946
}
[08/27/2025 19:20:03 INFO]: Val stats: {
    "score": -0.8982159229560783,
    "rmse": 0.8982159229560783
}
[08/27/2025 19:20:03 INFO]: Test stats: {
    "score": -0.9792643258472534,
    "rmse": 0.9792643258472534
}
[08/27/2025 19:20:04 INFO]: Training loss at epoch 15: 0.9368893206119537
[08/27/2025 19:20:13 INFO]: Training loss at epoch 19: 1.1044787168502808
[08/27/2025 19:20:20 INFO]: Training stats: {
    "score": -1.0059209258134818,
    "rmse": 1.0059209258134818
}
[08/27/2025 19:20:20 INFO]: Val stats: {
    "score": -0.8910627055040468,
    "rmse": 0.8910627055040468
}
[08/27/2025 19:20:20 INFO]: Test stats: {
    "score": -0.979774254816182,
    "rmse": 0.979774254816182
}
[08/27/2025 19:20:22 INFO]: Training loss at epoch 20: 1.0588025450706482
[08/27/2025 19:20:24 INFO]: Training loss at epoch 16: 1.3165860176086426
[08/27/2025 19:20:28 INFO]: Training loss at epoch 36: 1.123220443725586
[08/27/2025 19:20:29 INFO]: Training loss at epoch 28: 1.1586155593395233
[08/27/2025 19:20:39 INFO]: Training loss at epoch 20: 0.807762861251831
[08/27/2025 19:20:40 INFO]: Training loss at epoch 21: 0.977537214756012
[08/27/2025 19:20:41 INFO]: Training loss at epoch 40: 0.6215418428182602
[08/27/2025 19:20:45 INFO]: Training loss at epoch 15: 0.8683367967605591
[08/27/2025 19:20:46 INFO]: Training loss at epoch 17: 0.8555877804756165
[08/27/2025 19:20:58 INFO]: Training loss at epoch 21: 1.0323211550712585
[08/27/2025 19:21:00 INFO]: Training loss at epoch 22: 0.8719770908355713
[08/27/2025 19:21:04 INFO]: Training loss at epoch 18: 1.213113248348236
[08/27/2025 19:21:07 INFO]: Training loss at epoch 20: 0.9575774669647217
[08/27/2025 19:21:18 INFO]: Training loss at epoch 22: 1.039812982082367
[08/27/2025 19:21:20 INFO]: Training loss at epoch 23: 1.0112262070178986
[08/27/2025 19:21:24 INFO]: Training loss at epoch 19: 1.0858121514320374
[08/27/2025 19:21:24 INFO]: Training loss at epoch 45: 1.0900172591209412
[08/27/2025 19:21:31 INFO]: Training stats: {
    "score": -0.9995893847767402,
    "rmse": 0.9995893847767402
}
[08/27/2025 19:21:31 INFO]: Val stats: {
    "score": -0.9431861842079045,
    "rmse": 0.9431861842079045
}
[08/27/2025 19:21:31 INFO]: Test stats: {
    "score": -0.9728790781732656,
    "rmse": 0.9728790781732656
}
[08/27/2025 19:21:33 INFO]: Training loss at epoch 29: 1.0312087535858154
[08/27/2025 19:21:36 INFO]: Training loss at epoch 23: 1.2169369459152222
[08/27/2025 19:21:39 INFO]: Training loss at epoch 24: 1.1021353006362915
[08/27/2025 19:21:41 INFO]: Training loss at epoch 39: 1.1314648389816284
[08/27/2025 19:21:48 INFO]: Training loss at epoch 22: 0.9061713814735413
[08/27/2025 19:21:50 INFO]: Training loss at epoch 20: 1.0555151104927063
[08/27/2025 19:21:55 INFO]: Training loss at epoch 24: 1.0017767548561096
[08/27/2025 19:21:58 INFO]: Training loss at epoch 25: 1.0897748172283173
[08/27/2025 19:22:08 INFO]: Running Final Evaluation...
[08/27/2025 19:22:09 INFO]: Training loss at epoch 21: 0.9773177206516266
[08/27/2025 19:22:13 INFO]: Training loss at epoch 2: 0.9302194714546204
[08/27/2025 19:22:14 INFO]: Training loss at epoch 25: 0.898335188627243
[08/27/2025 19:22:15 INFO]: Training loss at epoch 28: 1.1165566444396973
[08/27/2025 19:22:17 INFO]: Training loss at epoch 26: 0.9511987268924713
[08/27/2025 19:22:23 INFO]: Training stats: {
    "score": -0.9427124923247809,
    "rmse": 0.9427124923247809
}
[08/27/2025 19:22:23 INFO]: Val stats: {
    "score": -0.8789852200604025,
    "rmse": 0.8789852200604025
}
[08/27/2025 19:22:23 INFO]: Test stats: {
    "score": -0.9704147174376155,
    "rmse": 0.9704147174376155
}
[08/27/2025 19:22:27 INFO]: Training loss at epoch 22: 1.0112980604171753
[08/27/2025 19:22:33 INFO]: Training loss at epoch 26: 1.2484119832515717
[08/27/2025 19:22:36 INFO]: Training loss at epoch 27: 1.0384540557861328
[08/27/2025 19:22:41 INFO]: Training loss at epoch 25: 0.9210957884788513
[08/27/2025 19:22:47 INFO]: Training loss at epoch 23: 0.9130438566207886
[08/27/2025 19:22:51 INFO]: Training loss at epoch 27: 1.096105307340622
[08/27/2025 19:22:55 INFO]: Training loss at epoch 7: 1.2682639956474304
[08/27/2025 19:22:55 INFO]: Training loss at epoch 28: 1.0622857511043549
[08/27/2025 19:22:55 INFO]: Training loss at epoch 29: 0.9248429536819458
[08/27/2025 19:23:01 INFO]: Training stats: {
    "score": -0.9566387105507682,
    "rmse": 0.9566387105507682
}
[08/27/2025 19:23:01 INFO]: Val stats: {
    "score": -0.9357989253854053,
    "rmse": 0.9357989253854053
}
[08/27/2025 19:23:01 INFO]: Test stats: {
    "score": -0.9705626709095558,
    "rmse": 0.9705626709095558
}
[08/27/2025 19:23:06 INFO]: Training loss at epoch 16: 0.8302555680274963
[08/27/2025 19:23:06 INFO]: Training loss at epoch 24: 0.9818103909492493
[08/27/2025 19:23:10 INFO]: Training loss at epoch 28: 0.9523539245128632
[08/27/2025 19:23:14 INFO]: Training loss at epoch 29: 0.8490334153175354
[08/27/2025 19:23:21 INFO]: Training stats: {
    "score": -1.0032398173042074,
    "rmse": 1.0032398173042074
}
[08/27/2025 19:23:21 INFO]: Val stats: {
    "score": -0.9043786456574237,
    "rmse": 0.9043786456574237
}
[08/27/2025 19:23:21 INFO]: Test stats: {
    "score": -0.9783305331853548,
    "rmse": 0.9783305331853548
}
[08/27/2025 19:23:26 INFO]: Training loss at epoch 25: 1.1850337088108063
[08/27/2025 19:23:29 INFO]: Training loss at epoch 21: 0.9346995055675507
[08/27/2025 19:23:29 INFO]: Training loss at epoch 29: 0.8736666440963745
[08/27/2025 19:23:36 INFO]: Training stats: {
    "score": -0.9980674904540519,
    "rmse": 0.9980674904540519
}
[08/27/2025 19:23:36 INFO]: Val stats: {
    "score": -0.9230639274951009,
    "rmse": 0.9230639274951009
}
[08/27/2025 19:23:36 INFO]: Test stats: {
    "score": -0.9731173774612358,
    "rmse": 0.9731173774612358
}
[08/27/2025 19:23:40 INFO]: Training loss at epoch 30: 1.0723170340061188
[08/27/2025 19:23:45 INFO]: Training loss at epoch 26: 0.9846923649311066
[08/27/2025 19:23:45 INFO]: Training stats: {
    "score": -1.0127995457619863,
    "rmse": 1.0127995457619863
}
[08/27/2025 19:23:45 INFO]: Val stats: {
    "score": -0.9976827165793195,
    "rmse": 0.9976827165793195
}
[08/27/2025 19:23:45 INFO]: Test stats: {
    "score": -0.9909498070978475,
    "rmse": 0.9909498070978475
}
[08/27/2025 19:23:54 INFO]: Training loss at epoch 47: 0.696284681558609
[08/27/2025 19:23:55 INFO]: Training loss at epoch 30: 0.812634140253067
[08/27/2025 19:24:00 INFO]: Training loss at epoch 31: 1.0749692916870117
[08/27/2025 19:24:04 INFO]: Training loss at epoch 27: 1.143114686012268
[08/27/2025 19:24:08 INFO]: Training loss at epoch 42: 0.9647486805915833
[08/27/2025 19:24:14 INFO]: Training loss at epoch 31: 0.9328340590000153
[08/27/2025 19:24:18 INFO]: Training loss at epoch 23: 0.8637244403362274
[08/27/2025 19:24:19 INFO]: Training loss at epoch 41: 0.6709656417369843
[08/27/2025 19:24:19 INFO]: Training loss at epoch 32: 0.9819598197937012
[08/27/2025 19:24:24 INFO]: Training loss at epoch 28: 1.023228257894516
[08/27/2025 19:24:29 INFO]: Training accuracy: {
    "score": -1.0389449042059804,
    "rmse": 1.0389449042059804
}
[08/27/2025 19:24:29 INFO]: Val accuracy: {
    "score": -0.8811420807466014,
    "rmse": 0.8811420807466014
}
[08/27/2025 19:24:29 INFO]: Test accuracy: {
    "score": -1.0109028180778454,
    "rmse": 1.0109028180778454
}
[08/27/2025 19:24:30 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_6",
    "best_epoch": 14,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0109028180778454,
        "rmse": 1.0109028180778454
    },
    "train_stats": {
        "score": -1.0389449042059804,
        "rmse": 1.0389449042059804
    },
    "val_stats": {
        "score": -0.8811420807466014,
        "rmse": 0.8811420807466014
    }
}
[08/27/2025 19:24:30 INFO]: Procewss finished for trial maddest-Elbert_trial_6
[08/27/2025 19:24:30 INFO]: 
_________________________________________________

[08/27/2025 19:24:30 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:24:30 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 1.1291480785895636
  attention_dropout: 0.19609655255592412
  ffn_dropout: 0.19609655255592412
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 7.105955459549767e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_50

[08/27/2025 19:24:30 INFO]: This ft_transformer has 0.145 million parameters.
[08/27/2025 19:24:30 INFO]: Training will start at epoch 0.
[08/27/2025 19:24:30 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:24:32 INFO]: Training loss at epoch 32: 0.8892517983913422
[08/27/2025 19:24:38 INFO]: Training loss at epoch 33: 1.043915331363678
[08/27/2025 19:24:42 INFO]: Training loss at epoch 29: 1.2603766918182373
[08/27/2025 19:24:48 INFO]: Training loss at epoch 0: 1.3481903076171875
[08/27/2025 19:24:49 INFO]: Training stats: {
    "score": -0.9979453283134871,
    "rmse": 0.9979453283134871
}
[08/27/2025 19:24:49 INFO]: Val stats: {
    "score": -0.9119857689234306,
    "rmse": 0.9119857689234306
}
[08/27/2025 19:24:49 INFO]: Test stats: {
    "score": -0.9712920222359688,
    "rmse": 0.9712920222359688
}
[08/27/2025 19:24:50 INFO]: New best epoch, val score: -0.900694988063131
[08/27/2025 19:24:50 INFO]: Saving model to: maddest-Elbert_trial_50/model_best.pth
[08/27/2025 19:24:51 INFO]: Training loss at epoch 33: 0.9439725279808044
[08/27/2025 19:24:53 INFO]: Training loss at epoch 30: 0.9497002959251404
[08/27/2025 19:24:57 INFO]: Training loss at epoch 34: 0.8906025588512421
[08/27/2025 19:25:07 INFO]: Training loss at epoch 1: 0.7832539081573486
[08/27/2025 19:25:08 INFO]: Training loss at epoch 30: 0.9317939579486847
[08/27/2025 19:25:10 INFO]: Training loss at epoch 34: 0.8236379325389862
[08/27/2025 19:25:11 INFO]: Training loss at epoch 49: 0.9491809606552124
[08/27/2025 19:25:16 INFO]: Training loss at epoch 35: 1.0355027318000793
[08/27/2025 19:25:26 INFO]: Training loss at epoch 2: 0.9898275434970856
[08/27/2025 19:25:26 INFO]: Training loss at epoch 17: 0.9768608212471008
[08/27/2025 19:25:27 INFO]: Training loss at epoch 31: 0.9417657256126404
[08/27/2025 19:25:29 INFO]: Training loss at epoch 35: 1.1463633179664612
[08/27/2025 19:25:36 INFO]: Training loss at epoch 36: 1.0637233257293701
[08/27/2025 19:25:45 INFO]: Training loss at epoch 3: 0.7766796052455902
[08/27/2025 19:25:45 INFO]: Training loss at epoch 32: 0.8767342567443848
[08/27/2025 19:25:48 INFO]: Training loss at epoch 36: 0.858724057674408
[08/27/2025 19:25:50 INFO]: Training loss at epoch 22: 0.9167127907276154
[08/27/2025 19:25:54 INFO]: Training loss at epoch 3: 1.0194457471370697
[08/27/2025 19:25:55 INFO]: Training loss at epoch 37: 0.9840631783008575
[08/27/2025 19:26:04 INFO]: Training loss at epoch 4: 0.9721784293651581
[08/27/2025 19:26:04 INFO]: Training loss at epoch 33: 1.0139077305793762
[08/27/2025 19:26:07 INFO]: Training loss at epoch 37: 1.1775085031986237
[08/27/2025 19:26:12 INFO]: Training loss at epoch 30: 1.142824411392212
[08/27/2025 19:26:14 INFO]: Training loss at epoch 38: 1.0217538475990295
[08/27/2025 19:26:20 INFO]: New best epoch, val score: -0.8743224376714034
[08/27/2025 19:26:20 INFO]: Saving model to: maddest-Elbert_trial_46/model_best.pth
[08/27/2025 19:26:24 INFO]: Training loss at epoch 5: 1.0355908274650574
[08/27/2025 19:26:24 INFO]: Training loss at epoch 34: 1.01356041431427
[08/27/2025 19:26:27 INFO]: Training loss at epoch 38: 0.9318699240684509
[08/27/2025 19:26:34 INFO]: Training loss at epoch 39: 1.0572176575660706
[08/27/2025 19:26:39 INFO]: Training loss at epoch 8: 0.9861677885055542
[08/27/2025 19:26:41 INFO]: Training stats: {
    "score": -1.0027044067775228,
    "rmse": 1.0027044067775228
}
[08/27/2025 19:26:41 INFO]: Val stats: {
    "score": -0.9004615172925984,
    "rmse": 0.9004615172925984
}
[08/27/2025 19:26:41 INFO]: Test stats: {
    "score": -0.977567202461868,
    "rmse": 0.977567202461868
}
[08/27/2025 19:26:43 INFO]: Training loss at epoch 6: 1.0401482582092285
[08/27/2025 19:26:43 INFO]: Training loss at epoch 35: 0.9829594790935516
[08/27/2025 19:26:44 INFO]: Running Final Evaluation...
[08/27/2025 19:26:46 INFO]: Running Final Evaluation...
[08/27/2025 19:26:47 INFO]: Training loss at epoch 39: 0.9741122126579285
[08/27/2025 19:26:48 INFO]: Training loss at epoch 24: 0.8025404810905457
[08/27/2025 19:26:51 INFO]: Training accuracy: {
    "score": -1.0067339544233649,
    "rmse": 1.0067339544233649
}
[08/27/2025 19:26:51 INFO]: Val accuracy: {
    "score": -0.8851110957555017,
    "rmse": 0.8851110957555017
}
[08/27/2025 19:26:51 INFO]: Test accuracy: {
    "score": -0.98216832722352,
    "rmse": 0.98216832722352
}
[08/27/2025 19:26:51 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_47",
    "best_epoch": 8,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.98216832722352,
        "rmse": 0.98216832722352
    },
    "train_stats": {
        "score": -1.0067339544233649,
        "rmse": 1.0067339544233649
    },
    "val_stats": {
        "score": -0.8851110957555017,
        "rmse": 0.8851110957555017
    }
}
[08/27/2025 19:26:51 INFO]: Procewss finished for trial maddest-Elbert_trial_47
[08/27/2025 19:26:51 INFO]: 
_________________________________________________

[08/27/2025 19:26:51 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:26:51 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 4
  d_ffn_factor: 1.1212238018819152
  attention_dropout: 0.17936980123777807
  ffn_dropout: 0.17936980123777807
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00014465481317239734
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_51

[08/27/2025 19:26:51 INFO]: This ft_transformer has 2.304 million parameters.
[08/27/2025 19:26:51 INFO]: Training will start at epoch 0.
[08/27/2025 19:26:51 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:26:53 INFO]: Training accuracy: {
    "score": -1.0232051608903572,
    "rmse": 1.0232051608903572
}
[08/27/2025 19:26:53 INFO]: Val accuracy: {
    "score": -0.886780365443234,
    "rmse": 0.886780365443234
}
[08/27/2025 19:26:53 INFO]: Test accuracy: {
    "score": -0.9949583853294921,
    "rmse": 0.9949583853294921
}
[08/27/2025 19:26:53 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_49",
    "best_epoch": 4,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9949583853294921,
        "rmse": 0.9949583853294921
    },
    "train_stats": {
        "score": -1.0232051608903572,
        "rmse": 1.0232051608903572
    },
    "val_stats": {
        "score": -0.886780365443234,
        "rmse": 0.886780365443234
    }
}
[08/27/2025 19:26:53 INFO]: Procewss finished for trial maddest-Elbert_trial_49
[08/27/2025 19:26:53 INFO]: 
_________________________________________________

[08/27/2025 19:26:53 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:26:53 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 0.8383433989380973
  attention_dropout: 0.04974449185777041
  ffn_dropout: 0.04974449185777041
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 6.580796522597022e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_52

[08/27/2025 19:26:53 INFO]: This ft_transformer has 2.053 million parameters.
[08/27/2025 19:26:53 INFO]: Training will start at epoch 0.
[08/27/2025 19:26:53 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:26:54 INFO]: Training stats: {
    "score": -0.99727938653283,
    "rmse": 0.99727938653283
}
[08/27/2025 19:26:54 INFO]: Val stats: {
    "score": -0.9080740354863006,
    "rmse": 0.9080740354863006
}
[08/27/2025 19:26:54 INFO]: Test stats: {
    "score": -0.9724210743100726,
    "rmse": 0.9724210743100726
}
[08/27/2025 19:26:57 INFO]: Training loss at epoch 40: 1.0864574313163757
[08/27/2025 19:27:02 INFO]: Training loss at epoch 7: 0.9147949516773224
[08/27/2025 19:27:09 INFO]: Training stats: {
    "score": -1.0002097952627458,
    "rmse": 1.0002097952627458
}
[08/27/2025 19:27:09 INFO]: Val stats: {
    "score": -0.934251212852229,
    "rmse": 0.934251212852229
}
[08/27/2025 19:27:09 INFO]: Test stats: {
    "score": -0.9719729621883675,
    "rmse": 0.9719729621883675
}
[08/27/2025 19:27:14 INFO]: Training loss at epoch 40: 0.9735760390758514
[08/27/2025 19:27:21 INFO]: Training loss at epoch 8: 1.0831108689308167
[08/27/2025 19:27:22 INFO]: Training loss at epoch 31: 0.8473173677921295
[08/27/2025 19:27:32 INFO]: Training loss at epoch 41: 0.9129577279090881
[08/27/2025 19:27:35 INFO]: Training loss at epoch 0: 1.209730863571167
[08/27/2025 19:27:37 INFO]: Training loss at epoch 0: 2.012169897556305
[08/27/2025 19:27:40 INFO]: Training loss at epoch 9: 0.7869842052459717
[08/27/2025 19:27:41 INFO]: New best epoch, val score: -0.887358315980555
[08/27/2025 19:27:41 INFO]: Saving model to: maddest-Elbert_trial_51/model_best.pth
[08/27/2025 19:27:43 INFO]: New best epoch, val score: -1.6288701704335276
[08/27/2025 19:27:43 INFO]: Saving model to: maddest-Elbert_trial_52/model_best.pth
[08/27/2025 19:27:47 INFO]: Training stats: {
    "score": -1.00090904293604,
    "rmse": 1.00090904293604
}
[08/27/2025 19:27:47 INFO]: Val stats: {
    "score": -0.904329345687136,
    "rmse": 0.904329345687136
}
[08/27/2025 19:27:47 INFO]: Test stats: {
    "score": -0.9755575427260034,
    "rmse": 0.9755575427260034
}
[08/27/2025 19:27:48 INFO]: Training loss at epoch 18: 0.9736380577087402
[08/27/2025 19:27:54 INFO]: Training loss at epoch 42: 0.90716353058815
[08/27/2025 19:27:57 INFO]: Training loss at epoch 42: 0.7425853312015533
[08/27/2025 19:28:02 INFO]: Training loss at epoch 37: 1.09311181306839
[08/27/2025 19:28:06 INFO]: Training loss at epoch 10: 0.8973370790481567
[08/27/2025 19:28:13 INFO]: Training loss at epoch 43: 0.8217937350273132
[08/27/2025 19:28:13 INFO]: Training loss at epoch 23: 0.8209090232849121
[08/27/2025 19:28:17 INFO]: Training loss at epoch 43: 0.8732675909996033
[08/27/2025 19:28:19 INFO]: Training loss at epoch 24: 1.0723482370376587
[08/27/2025 19:28:24 INFO]: Training loss at epoch 11: 1.2023385167121887
[08/27/2025 19:28:24 INFO]: Training loss at epoch 1: 1.1365408301353455
[08/27/2025 19:28:27 INFO]: Training loss at epoch 1: 1.4984224438667297
[08/27/2025 19:28:31 INFO]: Training loss at epoch 44: 0.9139637649059296
[08/27/2025 19:28:34 INFO]: New best epoch, val score: -1.0309100103685505
[08/27/2025 19:28:34 INFO]: Saving model to: maddest-Elbert_trial_52/model_best.pth
[08/27/2025 19:28:41 INFO]: Training loss at epoch 31: 0.9069540500640869
[08/27/2025 19:28:43 INFO]: Training loss at epoch 12: 1.1721172332763672
[08/27/2025 19:28:50 INFO]: Training loss at epoch 45: 1.1614544689655304
[08/27/2025 19:29:02 INFO]: Training loss at epoch 13: 1.1711483299732208
[08/27/2025 19:29:04 INFO]: New best epoch, val score: -0.8996796652604144
[08/27/2025 19:29:04 INFO]: Saving model to: maddest-Elbert_trial_50/model_best.pth
[08/27/2025 19:29:09 INFO]: Training loss at epoch 46: 1.1313782334327698
[08/27/2025 19:29:14 INFO]: Training loss at epoch 2: 1.5965580344200134
[08/27/2025 19:29:17 INFO]: Training loss at epoch 25: 0.9480786323547363
[08/27/2025 19:29:18 INFO]: Training loss at epoch 2: 1.1070644855499268
[08/27/2025 19:29:20 INFO]: Training loss at epoch 14: 1.0185921788215637
[08/27/2025 19:29:23 INFO]: New best epoch, val score: -0.8973989105699468
[08/27/2025 19:29:23 INFO]: Saving model to: maddest-Elbert_trial_50/model_best.pth
[08/27/2025 19:29:24 INFO]: New best epoch, val score: -0.9461890494461896
[08/27/2025 19:29:24 INFO]: Saving model to: maddest-Elbert_trial_52/model_best.pth
[08/27/2025 19:29:28 INFO]: Training loss at epoch 47: 0.898459792137146
[08/27/2025 19:29:30 INFO]: Running Final Evaluation...
[08/27/2025 19:29:36 INFO]: Training loss at epoch 4: 0.9137801826000214
[08/27/2025 19:29:37 INFO]: Training accuracy: {
    "score": -1.0149179951489697,
    "rmse": 1.0149179951489697
}
[08/27/2025 19:29:37 INFO]: Val accuracy: {
    "score": -0.8820252989842269,
    "rmse": 0.8820252989842269
}
[08/27/2025 19:29:37 INFO]: Test accuracy: {
    "score": -0.9886904714681379,
    "rmse": 0.9886904714681379
}
[08/27/2025 19:29:37 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_48",
    "best_epoch": 16,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9886904714681379,
        "rmse": 0.9886904714681379
    },
    "train_stats": {
        "score": -1.0149179951489697,
        "rmse": 1.0149179951489697
    },
    "val_stats": {
        "score": -0.8820252989842269,
        "rmse": 0.8820252989842269
    }
}
[08/27/2025 19:29:37 INFO]: Procewss finished for trial maddest-Elbert_trial_48
[08/27/2025 19:29:37 INFO]: 
_________________________________________________

[08/27/2025 19:29:37 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:29:37 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 0.8650279419049144
  attention_dropout: 0.04318535254540913
  ffn_dropout: 0.04318535254540913
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 6.177191796630216e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_53

[08/27/2025 19:29:37 INFO]: This ft_transformer has 2.078 million parameters.
[08/27/2025 19:29:37 INFO]: Training will start at epoch 0.
[08/27/2025 19:29:37 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:29:39 INFO]: Training loss at epoch 15: 1.187004268169403
[08/27/2025 19:29:40 INFO]: Training loss at epoch 29: 0.9722839593887329
[08/27/2025 19:29:41 INFO]: New best epoch, val score: -0.8948644239610344
[08/27/2025 19:29:41 INFO]: Saving model to: maddest-Elbert_trial_50/model_best.pth
[08/27/2025 19:29:44 INFO]: Training loss at epoch 48: 0.7471920251846313
[08/27/2025 19:29:51 INFO]: Training loss at epoch 32: 0.8632235527038574
[08/27/2025 19:29:58 INFO]: Training loss at epoch 16: 1.1567827463150024
[08/27/2025 19:30:00 INFO]: New best epoch, val score: -0.8931876205242288
[08/27/2025 19:30:00 INFO]: Saving model to: maddest-Elbert_trial_50/model_best.pth
[08/27/2025 19:30:05 INFO]: Training loss at epoch 3: 1.1889020800590515
[08/27/2025 19:30:08 INFO]: Training loss at epoch 19: 0.8433649837970734
[08/27/2025 19:30:08 INFO]: Training loss at epoch 3: 1.4028889536857605
[08/27/2025 19:30:17 INFO]: Training loss at epoch 17: 0.9850139021873474
[08/27/2025 19:30:19 INFO]: New best epoch, val score: -0.8926639189849637
[08/27/2025 19:30:19 INFO]: Saving model to: maddest-Elbert_trial_50/model_best.pth
[08/27/2025 19:30:21 INFO]: Training loss at epoch 0: 1.330381691455841
[08/27/2025 19:30:24 INFO]: Training loss at epoch 9: 0.9530222713947296
[08/27/2025 19:30:27 INFO]: New best epoch, val score: -1.2139734958454034
[08/27/2025 19:30:27 INFO]: Saving model to: maddest-Elbert_trial_53/model_best.pth
[08/27/2025 19:30:33 INFO]: Training loss at epoch 24: 0.9224591255187988
[08/27/2025 19:30:36 INFO]: Training loss at epoch 18: 1.054570198059082
[08/27/2025 19:30:38 INFO]: New best epoch, val score: -0.8924301294238455
[08/27/2025 19:30:38 INFO]: Saving model to: maddest-Elbert_trial_50/model_best.pth
[08/27/2025 19:30:52 INFO]: Training loss at epoch 41: 0.9506303369998932
[08/27/2025 19:30:54 INFO]: Training loss at epoch 19: 1.217792570590973
[08/27/2025 19:30:55 INFO]: Training loss at epoch 4: 1.1032037138938904
[08/27/2025 19:30:56 INFO]: Training stats: {
    "score": -0.984750833058129,
    "rmse": 0.984750833058129
}
[08/27/2025 19:30:56 INFO]: Val stats: {
    "score": -0.919540489161803,
    "rmse": 0.919540489161803
}
[08/27/2025 19:30:56 INFO]: Test stats: {
    "score": -0.9720083298019604,
    "rmse": 0.9720083298019604
}
[08/27/2025 19:30:58 INFO]: Training loss at epoch 4: 1.4202906489372253
[08/27/2025 19:31:01 INFO]: Training stats: {
    "score": -1.00451250217358,
    "rmse": 1.00451250217358
}
[08/27/2025 19:31:01 INFO]: Val stats: {
    "score": -0.8918180707587379,
    "rmse": 0.8918180707587379
}
[08/27/2025 19:31:01 INFO]: Test stats: {
    "score": -0.9791413089766596,
    "rmse": 0.9791413089766596
}
[08/27/2025 19:31:04 INFO]: New best epoch, val score: -0.8918180707587379
[08/27/2025 19:31:04 INFO]: Saving model to: maddest-Elbert_trial_50/model_best.pth
[08/27/2025 19:31:05 INFO]: New best epoch, val score: -0.9037440270255274
[08/27/2025 19:31:05 INFO]: Saving model to: maddest-Elbert_trial_52/model_best.pth
[08/27/2025 19:31:07 INFO]: Training loss at epoch 32: 1.0590706765651703
[08/27/2025 19:31:12 INFO]: Training loss at epoch 1: 1.2124615907669067
[08/27/2025 19:31:14 INFO]: Training loss at epoch 26: 1.1070924997329712
[08/27/2025 19:31:18 INFO]: New best epoch, val score: -0.8819608956215819
[08/27/2025 19:31:18 INFO]: Saving model to: maddest-Elbert_trial_53/model_best.pth
[08/27/2025 19:31:21 INFO]: Training loss at epoch 20: 0.9251621663570404
[08/27/2025 19:31:23 INFO]: New best epoch, val score: -0.8912402983895822
[08/27/2025 19:31:23 INFO]: Saving model to: maddest-Elbert_trial_50/model_best.pth
[08/27/2025 19:31:33 INFO]: Training loss at epoch 43: 0.7481976449489594
[08/27/2025 19:31:39 INFO]: Training stats: {
    "score": -0.9884609383896807,
    "rmse": 0.9884609383896807
}
[08/27/2025 19:31:39 INFO]: Val stats: {
    "score": -0.9537259867145089,
    "rmse": 0.9537259867145089
}
[08/27/2025 19:31:39 INFO]: Test stats: {
    "score": -0.9838383355908483,
    "rmse": 0.9838383355908483
}
[08/27/2025 19:31:41 INFO]: Training loss at epoch 21: 0.959259033203125
[08/27/2025 19:31:43 INFO]: New best epoch, val score: -0.8902711288955757
[08/27/2025 19:31:43 INFO]: Saving model to: maddest-Elbert_trial_50/model_best.pth
[08/27/2025 19:31:45 INFO]: Training loss at epoch 5: 1.1030309200286865
[08/27/2025 19:31:47 INFO]: Training loss at epoch 26: 1.018764317035675
[08/27/2025 19:31:49 INFO]: Training loss at epoch 5: 0.9526775777339935
[08/27/2025 19:31:55 INFO]: New best epoch, val score: -0.8994395090324057
[08/27/2025 19:31:55 INFO]: Saving model to: maddest-Elbert_trial_52/model_best.pth
[08/27/2025 19:31:59 INFO]: Training loss at epoch 22: 1.117283195257187
[08/27/2025 19:32:02 INFO]: New best epoch, val score: -0.8889534511990382
[08/27/2025 19:32:02 INFO]: Saving model to: maddest-Elbert_trial_50/model_best.pth
[08/27/2025 19:32:05 INFO]: Training loss at epoch 2: 1.1719986200332642
[08/27/2025 19:32:18 INFO]: Training loss at epoch 23: 0.8802079558372498
[08/27/2025 19:32:19 INFO]: Training stats: {
    "score": -0.9992573558295946,
    "rmse": 0.9992573558295946
}
[08/27/2025 19:32:19 INFO]: Val stats: {
    "score": -0.8980030528174063,
    "rmse": 0.8980030528174063
}
[08/27/2025 19:32:19 INFO]: Test stats: {
    "score": -0.9735659479467165,
    "rmse": 0.9735659479467165
}
[08/27/2025 19:32:20 INFO]: Training loss at epoch 33: 0.7778112292289734
[08/27/2025 19:32:21 INFO]: New best epoch, val score: -0.8878643632999984
[08/27/2025 19:32:21 INFO]: Saving model to: maddest-Elbert_trial_50/model_best.pth
[08/27/2025 19:32:25 INFO]: Training loss at epoch 44: 0.9337611794471741
[08/27/2025 19:32:35 INFO]: Training loss at epoch 6: 1.2463207244873047
[08/27/2025 19:32:37 INFO]: Training loss at epoch 24: 1.1869510412216187
[08/27/2025 19:32:38 INFO]: Running Final Evaluation...
[08/27/2025 19:32:39 INFO]: Training loss at epoch 6: 1.4745219945907593
[08/27/2025 19:32:39 INFO]: New best epoch, val score: -0.8874114081146603
[08/27/2025 19:32:39 INFO]: Saving model to: maddest-Elbert_trial_50/model_best.pth
[08/27/2025 19:32:45 INFO]: New best epoch, val score: -0.8946241482725222
[08/27/2025 19:32:45 INFO]: Saving model to: maddest-Elbert_trial_52/model_best.pth
[08/27/2025 19:32:48 INFO]: Training loss at epoch 50: 1.0079035758972168
[08/27/2025 19:32:53 INFO]: Training loss at epoch 25: 1.0053875744342804
[08/27/2025 19:32:54 INFO]: Training loss at epoch 3: 1.2856734991073608
[08/27/2025 19:32:55 INFO]: Training loss at epoch 25: 1.009969174861908
[08/27/2025 19:33:15 INFO]: Training loss at epoch 26: 1.0641902089118958
[08/27/2025 19:33:16 INFO]: Training loss at epoch 5: 1.3180583715438843
[08/27/2025 19:33:16 INFO]: Training loss at epoch 20: 0.9116436541080475
[08/27/2025 19:33:24 INFO]: Training loss at epoch 7: 1.2855669856071472
[08/27/2025 19:33:29 INFO]: Training loss at epoch 7: 1.1915667057037354
[08/27/2025 19:33:34 INFO]: Training loss at epoch 33: 0.7559339255094528
[08/27/2025 19:33:35 INFO]: New best epoch, val score: -0.8900164208472489
[08/27/2025 19:33:35 INFO]: Saving model to: maddest-Elbert_trial_52/model_best.pth
[08/27/2025 19:33:36 INFO]: Training loss at epoch 27: 0.9970980882644653
[08/27/2025 19:33:43 INFO]: Training accuracy: {
    "score": -1.0305931102241757,
    "rmse": 1.0305931102241757
}
[08/27/2025 19:33:43 INFO]: Val accuracy: {
    "score": -0.8647111483206955,
    "rmse": 0.8647111483206955
}
[08/27/2025 19:33:43 INFO]: Test accuracy: {
    "score": -1.004509655567716,
    "rmse": 1.004509655567716
}
[08/27/2025 19:33:43 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_40",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.004509655567716,
        "rmse": 1.004509655567716
    },
    "train_stats": {
        "score": -1.0305931102241757,
        "rmse": 1.0305931102241757
    },
    "val_stats": {
        "score": -0.8647111483206955,
        "rmse": 0.8647111483206955
    }
}
[08/27/2025 19:33:43 INFO]: Procewss finished for trial maddest-Elbert_trial_40
[08/27/2025 19:33:43 INFO]: 
_________________________________________________

[08/27/2025 19:33:43 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:33:43 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.2416880647826458
  attention_dropout: 0.04305031782853963
  ffn_dropout: 0.04305031782853963
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 6.165388719988059e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_54

[08/27/2025 19:33:43 INFO]: This ft_transformer has 2.410 million parameters.
[08/27/2025 19:33:43 INFO]: Training will start at epoch 0.
[08/27/2025 19:33:43 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:33:45 INFO]: Training loss at epoch 4: 0.9168387353420258
[08/27/2025 19:33:52 INFO]: Running Final Evaluation...
[08/27/2025 19:33:55 INFO]: Training loss at epoch 28: 1.007749617099762
[08/27/2025 19:34:14 INFO]: Training loss at epoch 8: 1.1419773697853088
[08/27/2025 19:34:14 INFO]: Training loss at epoch 29: 1.0838852226734161
[08/27/2025 19:34:15 INFO]: Training loss at epoch 27: 1.0287305116653442
[08/27/2025 19:34:20 INFO]: Training loss at epoch 8: 0.9901914298534393
[08/27/2025 19:34:20 INFO]: New best epoch, val score: -0.8860079908438003
[08/27/2025 19:34:20 INFO]: Saving model to: maddest-Elbert_trial_51/model_best.pth
[08/27/2025 19:34:21 INFO]: Training stats: {
    "score": -1.0028233448113872,
    "rmse": 1.0028233448113872
}
[08/27/2025 19:34:21 INFO]: Val stats: {
    "score": -0.8936945900302413,
    "rmse": 0.8936945900302413
}
[08/27/2025 19:34:21 INFO]: Test stats: {
    "score": -0.977884087594785,
    "rmse": 0.977884087594785
}
[08/27/2025 19:34:26 INFO]: New best epoch, val score: -0.8862338223576826
[08/27/2025 19:34:26 INFO]: Saving model to: maddest-Elbert_trial_52/model_best.pth
[08/27/2025 19:34:32 INFO]: Training loss at epoch 0: 1.0228197574615479
[08/27/2025 19:34:36 INFO]: Training loss at epoch 5: 1.0623960196971893
[08/27/2025 19:34:39 INFO]: New best epoch, val score: -0.9253739042209133
[08/27/2025 19:34:39 INFO]: Saving model to: maddest-Elbert_trial_54/model_best.pth
[08/27/2025 19:34:41 INFO]: Training loss at epoch 30: 1.0814087986946106
[08/27/2025 19:34:46 INFO]: Training accuracy: {
    "score": -1.0430256895107008,
    "rmse": 1.0430256895107008
}
[08/27/2025 19:34:46 INFO]: Val accuracy: {
    "score": -0.8782297529903855,
    "rmse": 0.8782297529903855
}
[08/27/2025 19:34:46 INFO]: Test accuracy: {
    "score": -1.012314105089028,
    "rmse": 1.012314105089028
}
[08/27/2025 19:34:46 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_41",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.012314105089028,
        "rmse": 1.012314105089028
    },
    "train_stats": {
        "score": -1.0430256895107008,
        "rmse": 1.0430256895107008
    },
    "val_stats": {
        "score": -0.8782297529903855,
        "rmse": 0.8782297529903855
    }
}
[08/27/2025 19:34:46 INFO]: Procewss finished for trial maddest-Elbert_trial_41
[08/27/2025 19:34:46 INFO]: 
_________________________________________________

[08/27/2025 19:34:46 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:34:46 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 4
  d_ffn_factor: 1.4855765360810615
  attention_dropout: 0.05403020260350725
  ffn_dropout: 0.05403020260350725
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 6.106539472543673e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_55

[08/27/2025 19:34:46 INFO]: This ft_transformer has 1.198 million parameters.
[08/27/2025 19:34:46 INFO]: Training will start at epoch 0.
[08/27/2025 19:34:46 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:34:48 INFO]: Training loss at epoch 42: 0.9003633260726929
[08/27/2025 19:34:59 INFO]: Training loss at epoch 31: 1.117950677871704
[08/27/2025 19:35:04 INFO]: Training loss at epoch 9: 0.9534580409526825
[08/27/2025 19:35:10 INFO]: Training loss at epoch 9: 0.9226012229919434
[08/27/2025 19:35:11 INFO]: Training loss at epoch 44: 0.8000310361385345
[08/27/2025 19:35:12 INFO]: Training loss at epoch 0: 1.2885907292366028
[08/27/2025 19:35:15 INFO]: New best epoch, val score: -0.8903675241909023
[08/27/2025 19:35:15 INFO]: Saving model to: maddest-Elbert_trial_55/model_best.pth
[08/27/2025 19:35:17 INFO]: Training loss at epoch 26: 0.915315181016922
[08/27/2025 19:35:19 INFO]: Training loss at epoch 32: 1.1355677247047424
[08/27/2025 19:35:21 INFO]: Training stats: {
    "score": -1.0354274876436538,
    "rmse": 1.0354274876436538
}
[08/27/2025 19:35:21 INFO]: Val stats: {
    "score": -0.8847859069932192,
    "rmse": 0.8847859069932192
}
[08/27/2025 19:35:21 INFO]: Test stats: {
    "score": -1.014231844646081,
    "rmse": 1.014231844646081
}
[08/27/2025 19:35:22 INFO]: Training loss at epoch 10: 1.0531782507896423
[08/27/2025 19:35:27 INFO]: Training loss at epoch 6: 1.1973363161087036
[08/27/2025 19:35:27 INFO]: New best epoch, val score: -0.8847859069932192
[08/27/2025 19:35:27 INFO]: Saving model to: maddest-Elbert_trial_51/model_best.pth
[08/27/2025 19:35:27 INFO]: Training stats: {
    "score": -1.0469830536467837,
    "rmse": 1.0469830536467837
}
[08/27/2025 19:35:27 INFO]: Val stats: {
    "score": -0.8837734279288,
    "rmse": 0.8837734279288
}
[08/27/2025 19:35:27 INFO]: Test stats: {
    "score": -1.0174170128539042,
    "rmse": 1.0174170128539042
}
[08/27/2025 19:35:29 INFO]: Training loss at epoch 1: 0.9961379170417786
[08/27/2025 19:35:33 INFO]: Training loss at epoch 38: 1.07159423828125
[08/27/2025 19:35:34 INFO]: New best epoch, val score: -0.8837734279288
[08/27/2025 19:35:34 INFO]: Saving model to: maddest-Elbert_trial_52/model_best.pth
[08/27/2025 19:35:35 INFO]: Training loss at epoch 49: 0.6963605284690857
[08/27/2025 19:35:36 INFO]: New best epoch, val score: -0.9193519636304941
[08/27/2025 19:35:36 INFO]: Saving model to: maddest-Elbert_trial_54/model_best.pth
[08/27/2025 19:35:38 INFO]: Training loss at epoch 21: 1.020054280757904
[08/27/2025 19:35:39 INFO]: Training loss at epoch 33: 0.8290868103504181
[08/27/2025 19:35:41 INFO]: Training loss at epoch 1: 1.0096630454063416
[08/27/2025 19:35:58 INFO]: Training loss at epoch 34: 0.8482312262058258
[08/27/2025 19:36:09 INFO]: Training loss at epoch 2: 1.0008503794670105
[08/27/2025 19:36:12 INFO]: Training loss at epoch 10: 0.9380633234977722
[08/27/2025 19:36:16 INFO]: Training loss at epoch 35: 0.8623690009117126
[08/27/2025 19:36:18 INFO]: Training loss at epoch 7: 0.9872575104236603
[08/27/2025 19:36:18 INFO]: Training loss at epoch 10: 1.1069068312644958
[08/27/2025 19:36:25 INFO]: Training loss at epoch 2: 0.9717124104499817
[08/27/2025 19:36:32 INFO]: Training loss at epoch 45: 1.189162790775299
[08/27/2025 19:36:35 INFO]: Training loss at epoch 36: 0.9465801119804382
[08/27/2025 19:36:38 INFO]: Training loss at epoch 3: 1.0986574292182922
[08/27/2025 19:36:45 INFO]: Training loss at epoch 28: 1.0943334698677063
[08/27/2025 19:36:54 INFO]: Training loss at epoch 37: 1.0359171032905579
[08/27/2025 19:36:59 INFO]: Training loss at epoch 6: 0.990685373544693
[08/27/2025 19:37:02 INFO]: Training loss at epoch 11: 0.9949385225772858
[08/27/2025 19:37:03 INFO]: Running Final Evaluation...
[08/27/2025 19:37:06 INFO]: Training loss at epoch 4: 1.1974194645881653
[08/27/2025 19:37:07 INFO]: Training loss at epoch 11: 1.0661435723304749
[08/27/2025 19:37:08 INFO]: Training loss at epoch 8: 0.9013445973396301
[08/27/2025 19:37:11 INFO]: Training loss at epoch 38: 1.203026443719864
[08/27/2025 19:37:17 INFO]: Training loss at epoch 3: 1.0704191327095032
[08/27/2025 19:37:24 INFO]: New best epoch, val score: -0.9028523343388279
[08/27/2025 19:37:24 INFO]: Saving model to: maddest-Elbert_trial_54/model_best.pth
[08/27/2025 19:37:30 INFO]: Training loss at epoch 39: 0.8783071637153625
[08/27/2025 19:37:34 INFO]: Training stats: {
    "score": -0.8047492201992476,
    "rmse": 0.8047492201992476
}
[08/27/2025 19:37:34 INFO]: Val stats: {
    "score": -0.9591686831278503,
    "rmse": 0.9591686831278503
}
[08/27/2025 19:37:34 INFO]: Test stats: {
    "score": -1.0972614406487728,
    "rmse": 1.0972614406487728
}
[08/27/2025 19:37:34 INFO]: Training loss at epoch 5: 0.8371178507804871
[08/27/2025 19:37:37 INFO]: Training loss at epoch 27: 0.8509231209754944
[08/27/2025 19:37:37 INFO]: Training stats: {
    "score": -1.0004928677588905,
    "rmse": 1.0004928677588905
}
[08/27/2025 19:37:37 INFO]: Val stats: {
    "score": -0.8995302082543699,
    "rmse": 0.8995302082543699
}
[08/27/2025 19:37:37 INFO]: Test stats: {
    "score": -0.9759997747272855,
    "rmse": 0.9759997747272855
}
[08/27/2025 19:37:37 INFO]: New best epoch, val score: -0.889767983970792
[08/27/2025 19:37:37 INFO]: Saving model to: maddest-Elbert_trial_55/model_best.pth
[08/27/2025 19:37:50 INFO]: Training loss at epoch 12: 1.054283320903778
[08/27/2025 19:37:55 INFO]: Training loss at epoch 40: 1.0042527616024017
[08/27/2025 19:37:56 INFO]: Training loss at epoch 22: 0.9981816112995148
[08/27/2025 19:37:56 INFO]: Training loss at epoch 12: 0.9039661288261414
[08/27/2025 19:37:57 INFO]: Training loss at epoch 9: 0.8932171761989594
[08/27/2025 19:38:03 INFO]: Training loss at epoch 6: 0.8853175938129425
[08/27/2025 19:38:06 INFO]: New best epoch, val score: -0.8896147273041787
[08/27/2025 19:38:06 INFO]: Saving model to: maddest-Elbert_trial_55/model_best.pth
[08/27/2025 19:38:12 INFO]: Training loss at epoch 4: 1.021267592906952
[08/27/2025 19:38:14 INFO]: Training loss at epoch 41: 1.1675961017608643
[08/27/2025 19:38:14 INFO]: Training stats: {
    "score": -0.9963984232834097,
    "rmse": 0.9963984232834097
}
[08/27/2025 19:38:14 INFO]: Val stats: {
    "score": -0.9208908283502972,
    "rmse": 0.9208908283502972
}
[08/27/2025 19:38:14 INFO]: Test stats: {
    "score": -0.9722846019254505,
    "rmse": 0.9722846019254505
}
[08/27/2025 19:38:26 INFO]: Training loss at epoch 51: 0.9247430860996246
[08/27/2025 19:38:31 INFO]: Training loss at epoch 7: 1.097077488899231
[08/27/2025 19:38:33 INFO]: Training loss at epoch 42: 0.8626123070716858
[08/27/2025 19:38:34 INFO]: New best epoch, val score: -0.889460696614029
[08/27/2025 19:38:34 INFO]: Saving model to: maddest-Elbert_trial_55/model_best.pth
[08/27/2025 19:38:40 INFO]: Training loss at epoch 13: 1.0628254413604736
[08/27/2025 19:38:41 INFO]: Training loss at epoch 43: 0.7738185226917267
[08/27/2025 19:38:43 INFO]: Training accuracy: {
    "score": -1.0286381893895742,
    "rmse": 1.0286381893895742
}
[08/27/2025 19:38:43 INFO]: Val accuracy: {
    "score": -0.8803461766329237,
    "rmse": 0.8803461766329237
}
[08/27/2025 19:38:43 INFO]: Test accuracy: {
    "score": -0.9997255719813135,
    "rmse": 0.9997255719813135
}
[08/27/2025 19:38:43 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_34",
    "best_epoch": 14,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9997255719813135,
        "rmse": 0.9997255719813135
    },
    "train_stats": {
        "score": -1.0286381893895742,
        "rmse": 1.0286381893895742
    },
    "val_stats": {
        "score": -0.8803461766329237,
        "rmse": 0.8803461766329237
    }
}
[08/27/2025 19:38:43 INFO]: Procewss finished for trial maddest-Elbert_trial_34
[08/27/2025 19:38:43 INFO]: 
_________________________________________________

[08/27/2025 19:38:43 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:38:43 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.5071376944965225
  attention_dropout: 0.052637786379061635
  ffn_dropout: 0.052637786379061635
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 6.595137440846906e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_56

[08/27/2025 19:38:43 INFO]: This ft_transformer has 1.205 million parameters.
[08/27/2025 19:38:43 INFO]: Training will start at epoch 0.
[08/27/2025 19:38:43 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:38:46 INFO]: Training loss at epoch 13: 1.051315426826477
[08/27/2025 19:38:46 INFO]: Training loss at epoch 45: 0.7097762525081635
[08/27/2025 19:38:51 INFO]: Training loss at epoch 43: 0.9970517158508301
[08/27/2025 19:39:00 INFO]: Training loss at epoch 8: 0.9348165988922119
[08/27/2025 19:39:03 INFO]: New best epoch, val score: -0.8891822322299572
[08/27/2025 19:39:03 INFO]: Saving model to: maddest-Elbert_trial_55/model_best.pth
[08/27/2025 19:39:04 INFO]: Training loss at epoch 11: 1.0852026343345642
[08/27/2025 19:39:04 INFO]: Training loss at epoch 10: 0.9539006948471069
[08/27/2025 19:39:07 INFO]: Training loss at epoch 5: 0.9286797344684601
[08/27/2025 19:39:10 INFO]: Training loss at epoch 44: 1.322459101676941
[08/27/2025 19:39:12 INFO]: Training loss at epoch 0: 1.1758432984352112
[08/27/2025 19:39:13 INFO]: Training loss at epoch 29: 0.9497636556625366
[08/27/2025 19:39:16 INFO]: New best epoch, val score: -1.0762094583430653
[08/27/2025 19:39:16 INFO]: Saving model to: maddest-Elbert_trial_56/model_best.pth
[08/27/2025 19:39:28 INFO]: Training loss at epoch 45: 1.396151453256607
[08/27/2025 19:39:28 INFO]: Training loss at epoch 9: 0.8881990313529968
[08/27/2025 19:39:29 INFO]: Training loss at epoch 14: 1.1498858332633972
[08/27/2025 19:39:36 INFO]: Training loss at epoch 14: 0.9544109106063843
[08/27/2025 19:39:39 INFO]: Training stats: {
    "score": -1.0050676621069103,
    "rmse": 1.0050676621069103
}
[08/27/2025 19:39:39 INFO]: Val stats: {
    "score": -0.8894187948086815,
    "rmse": 0.8894187948086815
}
[08/27/2025 19:39:39 INFO]: Test stats: {
    "score": -0.9837694664774637,
    "rmse": 0.9837694664774637
}
[08/27/2025 19:39:42 INFO]: Training loss at epoch 30: 1.035718321800232
[08/27/2025 19:39:42 INFO]: Training loss at epoch 27: 1.0618206560611725
[08/27/2025 19:39:46 INFO]: Training loss at epoch 1: 0.8832947015762329
[08/27/2025 19:39:47 INFO]: Training loss at epoch 46: 1.0671301186084747
[08/27/2025 19:39:49 INFO]: Training loss at epoch 25: 0.9481104016304016
[08/27/2025 19:39:50 INFO]: New best epoch, val score: -0.8780330886037558
[08/27/2025 19:39:50 INFO]: Saving model to: maddest-Elbert_trial_56/model_best.pth
[08/27/2025 19:39:55 INFO]: Training loss at epoch 11: 0.9765563011169434
[08/27/2025 19:39:57 INFO]: Training loss at epoch 28: 0.7216354161500931
[08/27/2025 19:40:04 INFO]: Training loss at epoch 6: 0.9636642634868622
[08/27/2025 19:40:04 INFO]: Training stats: {
    "score": -0.9996078106959015,
    "rmse": 0.9996078106959015
}
[08/27/2025 19:40:04 INFO]: Val stats: {
    "score": -0.903380438334017,
    "rmse": 0.903380438334017
}
[08/27/2025 19:40:04 INFO]: Test stats: {
    "score": -0.9749080025101228,
    "rmse": 0.9749080025101228
}
[08/27/2025 19:40:06 INFO]: Training loss at epoch 47: 0.8644152581691742
[08/27/2025 19:40:08 INFO]: Training loss at epoch 10: 1.1051061749458313
[08/27/2025 19:40:17 INFO]: Training loss at epoch 23: 0.871778130531311
[08/27/2025 19:40:19 INFO]: Training loss at epoch 2: 1.0209750533103943
[08/27/2025 19:40:19 INFO]: Training loss at epoch 15: 1.009970337152481
[08/27/2025 19:40:25 INFO]: Training loss at epoch 48: 0.9887612760066986
[08/27/2025 19:40:26 INFO]: Training loss at epoch 15: 0.9909079372882843
[08/27/2025 19:40:37 INFO]: Training loss at epoch 11: 1.1578187942504883
[08/27/2025 19:40:38 INFO]: Training loss at epoch 7: 1.0671767592430115
[08/27/2025 19:40:45 INFO]: Training loss at epoch 49: 1.0716004073619843
[08/27/2025 19:40:46 INFO]: Training loss at epoch 12: 0.7991341054439545
[08/27/2025 19:40:53 INFO]: Training loss at epoch 3: 1.2667429447174072
[08/27/2025 19:40:53 INFO]: Training stats: {
    "score": -0.9990442306580076,
    "rmse": 0.9990442306580076
}
[08/27/2025 19:40:53 INFO]: Val stats: {
    "score": -0.9051424602638796,
    "rmse": 0.9051424602638796
}
[08/27/2025 19:40:53 INFO]: Test stats: {
    "score": -0.9747802294855227,
    "rmse": 0.9747802294855227
}
[08/27/2025 19:40:57 INFO]: New best epoch, val score: -0.876946587309548
[08/27/2025 19:40:57 INFO]: Saving model to: maddest-Elbert_trial_56/model_best.pth
[08/27/2025 19:40:59 INFO]: Training loss at epoch 7: 1.234294056892395
[08/27/2025 19:41:05 INFO]: Training loss at epoch 12: 0.8876831829547882
[08/27/2025 19:41:09 INFO]: Training loss at epoch 16: 1.001496434211731
[08/27/2025 19:41:12 INFO]: Training loss at epoch 50: 1.0184340476989746
[08/27/2025 19:41:19 INFO]: Training loss at epoch 16: 1.1670179963111877
[08/27/2025 19:41:26 INFO]: Training loss at epoch 4: 1.0504170656204224
[08/27/2025 19:41:31 INFO]: Training loss at epoch 51: 1.0660293698310852
[08/27/2025 19:41:34 INFO]: Training loss at epoch 13: 0.9541364014148712
[08/27/2025 19:41:37 INFO]: Training loss at epoch 13: 1.0227513313293457
[08/27/2025 19:41:50 INFO]: Training loss at epoch 52: 1.1574692726135254
[08/27/2025 19:41:55 INFO]: Training loss at epoch 8: 1.0856541097164154
[08/27/2025 19:41:58 INFO]: Training loss at epoch 5: 0.9303916096687317
[08/27/2025 19:42:00 INFO]: Training loss at epoch 17: 1.1622275710105896
[08/27/2025 19:42:03 INFO]: Training loss at epoch 14: 0.8645978569984436
[08/27/2025 19:42:08 INFO]: Training loss at epoch 53: 1.1114080846309662
[08/27/2025 19:42:10 INFO]: Training loss at epoch 17: 1.0832951068878174
[08/27/2025 19:42:20 INFO]: Training loss at epoch 29: 0.9451849460601807
[08/27/2025 19:42:23 INFO]: Training loss at epoch 46: 0.628144308924675
[08/27/2025 19:42:26 INFO]: Training loss at epoch 54: 0.9180383384227753
[08/27/2025 19:42:27 INFO]: Training loss at epoch 14: 0.8479061424732208
[08/27/2025 19:42:31 INFO]: Training loss at epoch 15: 0.9998630881309509
[08/27/2025 19:42:32 INFO]: Training loss at epoch 6: 1.12772935628891
[08/27/2025 19:42:33 INFO]: Training loss at epoch 30: 0.9770193099975586
[08/27/2025 19:42:39 INFO]: Training loss at epoch 44: 0.8633120059967041
[08/27/2025 19:42:40 INFO]: Training loss at epoch 24: 1.063794195652008
[08/27/2025 19:42:46 INFO]: Training loss at epoch 55: 1.1425397992134094
[08/27/2025 19:42:48 INFO]: Training loss at epoch 12: 1.219666063785553
[08/27/2025 19:42:48 INFO]: Running Final Evaluation...
[08/27/2025 19:42:48 INFO]: Running Final Evaluation...
[08/27/2025 19:42:50 INFO]: Training loss at epoch 18: 1.0680774450302124
[08/27/2025 19:42:52 INFO]: Training loss at epoch 9: 0.9164687991142273
[08/27/2025 19:42:54 INFO]: Training accuracy: {
    "score": -1.0065948475389606,
    "rmse": 1.0065948475389606
}
[08/27/2025 19:42:54 INFO]: Val accuracy: {
    "score": -0.8874114081146603,
    "rmse": 0.8874114081146603
}
[08/27/2025 19:42:54 INFO]: Test accuracy: {
    "score": -0.9814662744058613,
    "rmse": 0.9814662744058613
}
[08/27/2025 19:42:54 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_50",
    "best_epoch": 24,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9814662744058613,
        "rmse": 0.9814662744058613
    },
    "train_stats": {
        "score": -1.0065948475389606,
        "rmse": 1.0065948475389606
    },
    "val_stats": {
        "score": -0.8874114081146603,
        "rmse": 0.8874114081146603
    }
}
[08/27/2025 19:42:54 INFO]: Procewss finished for trial maddest-Elbert_trial_50
[08/27/2025 19:42:54 INFO]: 
_________________________________________________

[08/27/2025 19:42:54 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:42:54 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.4300532777210861
  attention_dropout: 0.05565962420756437
  ffn_dropout: 0.05565962420756437
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.7878646888798645e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_57

[08/27/2025 19:42:54 INFO]: This ft_transformer has 1.176 million parameters.
[08/27/2025 19:42:54 INFO]: Training will start at epoch 0.
[08/27/2025 19:42:54 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:42:58 INFO]: Training loss at epoch 18: 0.8918605148792267
[08/27/2025 19:42:59 INFO]: Training loss at epoch 16: 1.010030597448349
[08/27/2025 19:43:02 INFO]: Training loss at epoch 39: 1.1349998712539673
[08/27/2025 19:43:03 INFO]: Training loss at epoch 7: 1.0217545628547668
[08/27/2025 19:43:05 INFO]: Running Final Evaluation...
[08/27/2025 19:43:05 INFO]: Training stats: {
    "score": -0.9606651999097632,
    "rmse": 0.9606651999097632
}
[08/27/2025 19:43:05 INFO]: Val stats: {
    "score": -0.9352590926927029,
    "rmse": 0.9352590926927029
}
[08/27/2025 19:43:05 INFO]: Test stats: {
    "score": -0.9740555962502672,
    "rmse": 0.9740555962502672
}
[08/27/2025 19:43:08 INFO]: Training stats: {
    "score": -0.9790469449804794,
    "rmse": 0.9790469449804794
}
[08/27/2025 19:43:08 INFO]: Val stats: {
    "score": -0.9469142920247915,
    "rmse": 0.9469142920247915
}
[08/27/2025 19:43:08 INFO]: Test stats: {
    "score": -0.9787172743394844,
    "rmse": 0.9787172743394844
}
[08/27/2025 19:43:16 INFO]: Training loss at epoch 15: 1.1771043241024017
[08/27/2025 19:43:22 INFO]: Training loss at epoch 50: 0.8386781513690948
[08/27/2025 19:43:22 INFO]: Training loss at epoch 0: 1.3074508309364319
[08/27/2025 19:43:26 INFO]: New best epoch, val score: -0.8925712971342498
[08/27/2025 19:43:26 INFO]: Saving model to: maddest-Elbert_trial_57/model_best.pth
[08/27/2025 19:43:27 INFO]: Training loss at epoch 17: 1.1849470734596252
[08/27/2025 19:43:36 INFO]: Training loss at epoch 8: 0.8971026241779327
[08/27/2025 19:43:38 INFO]: Training loss at epoch 19: 0.9541318714618683
[08/27/2025 19:43:47 INFO]: Training loss at epoch 19: 0.9609799981117249
[08/27/2025 19:43:54 INFO]: Training loss at epoch 1: 1.123816192150116
[08/27/2025 19:43:54 INFO]: Training stats: {
    "score": -0.994017557223608,
    "rmse": 0.994017557223608
}
[08/27/2025 19:43:54 INFO]: Val stats: {
    "score": -0.9043608865188479,
    "rmse": 0.9043608865188479
}
[08/27/2025 19:43:54 INFO]: Test stats: {
    "score": -0.9766592881944904,
    "rmse": 0.9766592881944904
}
[08/27/2025 19:43:55 INFO]: Training loss at epoch 18: 0.7800310850143433
[08/27/2025 19:43:58 INFO]: New best epoch, val score: -0.8892123747344469
[08/27/2025 19:43:58 INFO]: Saving model to: maddest-Elbert_trial_57/model_best.pth
[08/27/2025 19:44:03 INFO]: Training loss at epoch 52: 0.9068739414215088
[08/27/2025 19:44:04 INFO]: Training stats: {
    "score": -0.9994263282638496,
    "rmse": 0.9994263282638496
}
[08/27/2025 19:44:04 INFO]: Val stats: {
    "score": -0.948899965496121,
    "rmse": 0.948899965496121
}
[08/27/2025 19:44:04 INFO]: Test stats: {
    "score": -0.9707728106468685,
    "rmse": 0.9707728106468685
}
[08/27/2025 19:44:05 INFO]: Training loss at epoch 10: 0.9393441379070282
[08/27/2025 19:44:06 INFO]: Training loss at epoch 16: 0.9412350952625275
[08/27/2025 19:44:10 INFO]: Training loss at epoch 9: 1.0624825954437256
[08/27/2025 19:44:11 INFO]: New best epoch, val score: -0.8882029353837969
[08/27/2025 19:44:11 INFO]: Saving model to: maddest-Elbert_trial_54/model_best.pth
[08/27/2025 19:44:17 INFO]: Training loss at epoch 8: 1.1508975625038147
[08/27/2025 19:44:18 INFO]: Training accuracy: {
    "score": -0.9979697356261258,
    "rmse": 0.9979697356261258
}
[08/27/2025 19:44:18 INFO]: Val accuracy: {
    "score": -0.8945013059093021,
    "rmse": 0.8945013059093021
}
[08/27/2025 19:44:18 INFO]: Test accuracy: {
    "score": -1.0178288443727759,
    "rmse": 1.0178288443727759
}
[08/27/2025 19:44:18 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_36",
    "best_epoch": 15,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0178288443727759,
        "rmse": 1.0178288443727759
    },
    "train_stats": {
        "score": -0.9979697356261258,
        "rmse": 0.9979697356261258
    },
    "val_stats": {
        "score": -0.8945013059093021,
        "rmse": 0.8945013059093021
    }
}
[08/27/2025 19:44:18 INFO]: Procewss finished for trial maddest-Elbert_trial_36
[08/27/2025 19:44:18 INFO]: 
_________________________________________________

[08/27/2025 19:44:18 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:44:18 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.4970330397798852
  attention_dropout: 0.06568448236534993
  ffn_dropout: 0.06568448236534993
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.6707718557324943e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_58

[08/27/2025 19:44:18 INFO]: This ft_transformer has 2.637 million parameters.
[08/27/2025 19:44:18 INFO]: Training will start at epoch 0.
[08/27/2025 19:44:18 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:44:20 INFO]: Training stats: {
    "score": -1.0139533961005174,
    "rmse": 1.0139533961005174
}
[08/27/2025 19:44:20 INFO]: Val stats: {
    "score": -0.9856452614924757,
    "rmse": 0.9856452614924757
}
[08/27/2025 19:44:20 INFO]: Test stats: {
    "score": -0.9838697818552196,
    "rmse": 0.9838697818552196
}
[08/27/2025 19:44:25 INFO]: Training loss at epoch 19: 1.0957094430923462
[08/27/2025 19:44:28 INFO]: Training loss at epoch 2: 1.2740774750709534
[08/27/2025 19:44:28 INFO]: Training accuracy: {
    "score": -1.036298204724057,
    "rmse": 1.036298204724057
}
[08/27/2025 19:44:28 INFO]: Val accuracy: {
    "score": -0.8825968902269575,
    "rmse": 0.8825968902269575
}
[08/27/2025 19:44:28 INFO]: Test accuracy: {
    "score": -1.0119172546756967,
    "rmse": 1.0119172546756967
}
[08/27/2025 19:44:28 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_35",
    "best_epoch": 13,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0119172546756967,
        "rmse": 1.0119172546756967
    },
    "train_stats": {
        "score": -1.036298204724057,
        "rmse": 1.036298204724057
    },
    "val_stats": {
        "score": -0.8825968902269575,
        "rmse": 0.8825968902269575
    }
}
[08/27/2025 19:44:28 INFO]: Procewss finished for trial maddest-Elbert_trial_35
[08/27/2025 19:44:28 INFO]: 
_________________________________________________

[08/27/2025 19:44:28 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:44:28 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.5685676359265166
  attention_dropout: 0.05168181287424334
  ffn_dropout: 0.05168181287424334
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.90484681667215e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_59

[08/27/2025 19:44:28 INFO]: This ft_transformer has 1.230 million parameters.
[08/27/2025 19:44:28 INFO]: Training will start at epoch 0.
[08/27/2025 19:44:28 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:44:35 INFO]: Training stats: {
    "score": -0.9931430243652785,
    "rmse": 0.9931430243652785
}
[08/27/2025 19:44:35 INFO]: Val stats: {
    "score": -0.905058729109238,
    "rmse": 0.905058729109238
}
[08/27/2025 19:44:35 INFO]: Test stats: {
    "score": -0.973945753111286,
    "rmse": 0.973945753111286
}
[08/27/2025 19:44:44 INFO]: Running Final Evaluation...
[08/27/2025 19:44:45 INFO]: Training loss at epoch 20: 1.147099792957306
[08/27/2025 19:44:52 INFO]: Training loss at epoch 10: 1.202355980873108
[08/27/2025 19:44:53 INFO]: Training loss at epoch 20: 1.0276483297348022
[08/27/2025 19:44:56 INFO]: Training loss at epoch 17: 0.865842878818512
[08/27/2025 19:44:56 INFO]: Training loss at epoch 0: 1.160895824432373
[08/27/2025 19:44:57 INFO]: Training loss at epoch 25: 1.478288173675537
[08/27/2025 19:44:58 INFO]: Training loss at epoch 3: 1.004003882408142
[08/27/2025 19:44:58 INFO]: Training loss at epoch 11: 1.2546174824237823
[08/27/2025 19:45:00 INFO]: New best epoch, val score: -0.9117684898220628
[08/27/2025 19:45:00 INFO]: Saving model to: maddest-Elbert_trial_59/model_best.pth
[08/27/2025 19:45:00 INFO]: Training loss at epoch 31: 1.1599465012550354
[08/27/2025 19:45:01 INFO]: Training loss at epoch 20: 0.9190335273742676
[08/27/2025 19:45:04 INFO]: New best epoch, val score: -0.883742134455171
[08/27/2025 19:45:04 INFO]: Saving model to: maddest-Elbert_trial_54/model_best.pth
[08/27/2025 19:45:08 INFO]: Training loss at epoch 0: 1.1190687417984009
[08/27/2025 19:45:15 INFO]: New best epoch, val score: -0.8952071578200186
[08/27/2025 19:45:15 INFO]: Saving model to: maddest-Elbert_trial_58/model_best.pth
[08/27/2025 19:45:24 INFO]: Training loss at epoch 11: 0.9096417427062988
[08/27/2025 19:45:25 INFO]: Training loss at epoch 30: 0.8535800278186798
[08/27/2025 19:45:29 INFO]: Training loss at epoch 1: 1.1524580717086792
[08/27/2025 19:45:31 INFO]: Training loss at epoch 21: 0.8128689825534821
[08/27/2025 19:45:32 INFO]: Training loss at epoch 4: 0.8696600794792175
[08/27/2025 19:45:33 INFO]: Training loss at epoch 21: 0.8833065629005432
[08/27/2025 19:45:36 INFO]: Training stats: {
    "score": -0.997986443119567,
    "rmse": 0.997986443119567
}
[08/27/2025 19:45:36 INFO]: Val stats: {
    "score": -0.9413439541830013,
    "rmse": 0.9413439541830013
}
[08/27/2025 19:45:36 INFO]: Test stats: {
    "score": -0.9720150616005969,
    "rmse": 0.9720150616005969
}
[08/27/2025 19:45:43 INFO]: Training loss at epoch 21: 0.9564288854598999
[08/27/2025 19:45:46 INFO]: Training loss at epoch 18: 1.041516661643982
[08/27/2025 19:45:54 INFO]: Training loss at epoch 12: 1.0209510922431946
[08/27/2025 19:45:58 INFO]: Training loss at epoch 12: 1.1124443411827087
[08/27/2025 19:46:01 INFO]: Training loss at epoch 22: 0.933546394109726
[08/27/2025 19:46:03 INFO]: Training loss at epoch 2: 1.2229482531547546
[08/27/2025 19:46:06 INFO]: Training loss at epoch 5: 1.2557628154754639
[08/27/2025 19:46:07 INFO]: Training loss at epoch 1: 1.0397946536540985
[08/27/2025 19:46:24 INFO]: Training loss at epoch 22: 1.0135200321674347
[08/27/2025 19:46:29 INFO]: Training loss at epoch 13: 1.154986321926117
[08/27/2025 19:46:30 INFO]: Training loss at epoch 23: 1.2680644094944
[08/27/2025 19:46:31 INFO]: Training loss at epoch 13: 1.0401560068130493
[08/27/2025 19:46:33 INFO]: Training loss at epoch 22: 1.0570953786373138
[08/27/2025 19:46:36 INFO]: Training loss at epoch 3: 1.3309125900268555
[08/27/2025 19:46:38 INFO]: Training loss at epoch 19: 0.8398782908916473
[08/27/2025 19:46:38 INFO]: Training loss at epoch 6: 1.0378692746162415
[08/27/2025 19:46:50 INFO]: Training loss at epoch 13: 0.921893984079361
[08/27/2025 19:46:54 INFO]: Training accuracy: {
    "score": -1.046570957697471,
    "rmse": 1.046570957697471
}
[08/27/2025 19:46:54 INFO]: Val accuracy: {
    "score": -0.8815447048075988,
    "rmse": 0.8815447048075988
}
[08/27/2025 19:46:54 INFO]: Test accuracy: {
    "score": -1.0183791161106606,
    "rmse": 1.0183791161106606
}
[08/27/2025 19:46:54 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_12",
    "best_epoch": 21,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0183791161106606,
        "rmse": 1.0183791161106606
    },
    "train_stats": {
        "score": -1.046570957697471,
        "rmse": 1.046570957697471
    },
    "val_stats": {
        "score": -0.8815447048075988,
        "rmse": 0.8815447048075988
    }
}
[08/27/2025 19:46:54 INFO]: Procewss finished for trial maddest-Elbert_trial_12
[08/27/2025 19:46:55 INFO]: 
_________________________________________________

[08/27/2025 19:46:55 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:46:55 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.47843197847888
  attention_dropout: 0.05682553440507036
  ffn_dropout: 0.05682553440507036
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.8231470034413046e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_60

[08/27/2025 19:46:55 INFO]: This ft_transformer has 1.194 million parameters.
[08/27/2025 19:46:55 INFO]: Training will start at epoch 0.
[08/27/2025 19:46:55 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:46:56 INFO]: Training stats: {
    "score": -0.9929100066201896,
    "rmse": 0.9929100066201896
}
[08/27/2025 19:46:56 INFO]: Val stats: {
    "score": -0.9202459092198141,
    "rmse": 0.9202459092198141
}
[08/27/2025 19:46:56 INFO]: Test stats: {
    "score": -0.9714872253338402,
    "rmse": 0.9714872253338402
}
[08/27/2025 19:46:59 INFO]: Training loss at epoch 24: 0.9303812980651855
[08/27/2025 19:47:03 INFO]: Training loss at epoch 14: 0.8684476315975189
[08/27/2025 19:47:04 INFO]: Training loss at epoch 2: 0.9729424118995667
[08/27/2025 19:47:05 INFO]: Training loss at epoch 31: 0.9171494245529175
[08/27/2025 19:47:08 INFO]: Training loss at epoch 4: 0.9257741272449493
[08/27/2025 19:47:10 INFO]: Training loss at epoch 7: 1.1276454329490662
[08/27/2025 19:47:13 INFO]: New best epoch, val score: -0.8937804161772084
[08/27/2025 19:47:13 INFO]: Saving model to: maddest-Elbert_trial_59/model_best.pth
[08/27/2025 19:47:13 INFO]: Training loss at epoch 23: 0.8787862062454224
[08/27/2025 19:47:20 INFO]: Training loss at epoch 26: 0.8298190832138062
[08/27/2025 19:47:23 INFO]: Training loss at epoch 23: 1.0610370337963104
[08/27/2025 19:47:23 INFO]: Training loss at epoch 0: 0.9779129922389984
[08/27/2025 19:47:27 INFO]: New best epoch, val score: -0.9224243670261604
[08/27/2025 19:47:27 INFO]: Saving model to: maddest-Elbert_trial_60/model_best.pth
[08/27/2025 19:47:27 INFO]: Training loss at epoch 25: 1.099556803703308
[08/27/2025 19:47:30 INFO]: Training loss at epoch 32: 0.9224246740341187
[08/27/2025 19:47:36 INFO]: Training loss at epoch 15: 1.0702536702156067
[08/27/2025 19:47:42 INFO]: Training loss at epoch 5: 0.8937360644340515
[08/27/2025 19:47:43 INFO]: Training loss at epoch 8: 1.2634086608886719
[08/27/2025 19:47:45 INFO]: Training loss at epoch 14: 1.1069814562797546
[08/27/2025 19:47:46 INFO]: Training loss at epoch 20: 1.0059532225131989
[08/27/2025 19:47:47 INFO]: New best epoch, val score: -0.8923939153461091
[08/27/2025 19:47:47 INFO]: Saving model to: maddest-Elbert_trial_59/model_best.pth
[08/27/2025 19:47:47 INFO]: Training loss at epoch 31: 0.8263095319271088
[08/27/2025 19:47:56 INFO]: Training loss at epoch 1: 1.1780992150306702
[08/27/2025 19:47:56 INFO]: Training loss at epoch 9: 1.1224534511566162
[08/27/2025 19:47:56 INFO]: Training loss at epoch 26: 0.9445450901985168
[08/27/2025 19:48:02 INFO]: Training loss at epoch 3: 1.1113927364349365
[08/27/2025 19:48:03 INFO]: Training loss at epoch 24: 0.9667540490627289
[08/27/2025 19:48:09 INFO]: Training loss at epoch 28: 1.028322458267212
[08/27/2025 19:48:09 INFO]: Training loss at epoch 16: 1.1537256836891174
[08/27/2025 19:48:13 INFO]: Training loss at epoch 24: 0.9907499849796295
[08/27/2025 19:48:15 INFO]: Training loss at epoch 9: 0.9989647567272186
[08/27/2025 19:48:16 INFO]: Training loss at epoch 6: 0.8042058050632477
[08/27/2025 19:48:20 INFO]: New best epoch, val score: -0.8919587790061556
[08/27/2025 19:48:20 INFO]: Saving model to: maddest-Elbert_trial_59/model_best.pth
[08/27/2025 19:48:25 INFO]: Training loss at epoch 27: 0.8589674830436707
[08/27/2025 19:48:25 INFO]: Training stats: {
    "score": -1.0124380480356878,
    "rmse": 1.0124380480356878
}
[08/27/2025 19:48:25 INFO]: Val stats: {
    "score": -0.9867142769440538,
    "rmse": 0.9867142769440538
}
[08/27/2025 19:48:25 INFO]: Test stats: {
    "score": -0.9871724457883888,
    "rmse": 0.9871724457883888
}
[08/27/2025 19:48:28 INFO]: Training loss at epoch 2: 1.1267533898353577
[08/27/2025 19:48:36 INFO]: Training loss at epoch 21: 0.9668273329734802
[08/27/2025 19:48:40 INFO]: Training loss at epoch 15: 0.9050622582435608
[08/27/2025 19:48:42 INFO]: Training loss at epoch 17: 1.0614181756973267
[08/27/2025 19:48:49 INFO]: Training loss at epoch 7: 0.9331389963626862
[08/27/2025 19:48:52 INFO]: Training loss at epoch 25: 0.9250051379203796
[08/27/2025 19:48:53 INFO]: Training loss at epoch 28: 1.010007232427597
[08/27/2025 19:48:58 INFO]: Training loss at epoch 10: 0.8401808738708496
[08/27/2025 19:49:00 INFO]: Training loss at epoch 4: 1.2094901204109192
[08/27/2025 19:49:01 INFO]: Training loss at epoch 3: 1.3508546650409698
[08/27/2025 19:49:02 INFO]: Training loss at epoch 25: 1.0343440771102905
[08/27/2025 19:49:11 INFO]: Training stats: {
    "score": -0.9915761152645958,
    "rmse": 0.9915761152645958
}
[08/27/2025 19:49:11 INFO]: Val stats: {
    "score": -0.9358134642796367,
    "rmse": 0.9358134642796367
}
[08/27/2025 19:49:11 INFO]: Test stats: {
    "score": -0.972230536654191,
    "rmse": 0.972230536654191
}
[08/27/2025 19:49:11 INFO]: Training loss at epoch 51: 0.805112212896347
[08/27/2025 19:49:15 INFO]: Training loss at epoch 18: 0.94554403424263
[08/27/2025 19:49:22 INFO]: Training loss at epoch 8: 0.9327986836433411
[08/27/2025 19:49:22 INFO]: Training loss at epoch 29: 0.9202220141887665
[08/27/2025 19:49:27 INFO]: Training loss at epoch 22: 0.7801627218723297
[08/27/2025 19:49:30 INFO]: Training loss at epoch 11: 1.2825847268104553
[08/27/2025 19:49:32 INFO]: Training stats: {
    "score": -0.9898662751976625,
    "rmse": 0.9898662751976625
}
[08/27/2025 19:49:32 INFO]: Val stats: {
    "score": -0.9450252846326636,
    "rmse": 0.9450252846326636
}
[08/27/2025 19:49:32 INFO]: Test stats: {
    "score": -0.9754622055782652,
    "rmse": 0.9754622055782652
}
[08/27/2025 19:49:34 INFO]: Training loss at epoch 4: 0.8726999461650848
[08/27/2025 19:49:35 INFO]: Training loss at epoch 16: 1.0432894825935364
[08/27/2025 19:49:38 INFO]: New best epoch, val score: -0.9163404766869979
[08/27/2025 19:49:38 INFO]: Saving model to: maddest-Elbert_trial_60/model_best.pth
[08/27/2025 19:49:40 INFO]: Training loss at epoch 27: 1.0053272545337677
[08/27/2025 19:49:42 INFO]: Training loss at epoch 26: 0.9344947338104248
[08/27/2025 19:49:48 INFO]: Training loss at epoch 19: 1.135773092508316
[08/27/2025 19:49:52 INFO]: Training loss at epoch 26: 1.0460002422332764
[08/27/2025 19:49:56 INFO]: Training loss at epoch 9: 1.1411870121955872
[08/27/2025 19:49:58 INFO]: Training loss at epoch 5: 0.8694797158241272
[08/27/2025 19:49:59 INFO]: Training loss at epoch 33: 0.9773365259170532
[08/27/2025 19:49:59 INFO]: Training stats: {
    "score": -0.9954095234502568,
    "rmse": 0.9954095234502568
}
[08/27/2025 19:49:59 INFO]: Val stats: {
    "score": -0.9333067448549309,
    "rmse": 0.9333067448549309
}
[08/27/2025 19:49:59 INFO]: Test stats: {
    "score": -0.968633329842037,
    "rmse": 0.968633329842037
}
[08/27/2025 19:50:01 INFO]: Training loss at epoch 30: 0.9640922546386719
[08/27/2025 19:50:03 INFO]: Training loss at epoch 12: 0.8882435858249664
[08/27/2025 19:50:07 INFO]: Training loss at epoch 5: 1.1776284277439117
[08/27/2025 19:50:07 INFO]: Training stats: {
    "score": -1.0046465190791039,
    "rmse": 1.0046465190791039
}
[08/27/2025 19:50:07 INFO]: Val stats: {
    "score": -0.8932624545110144,
    "rmse": 0.8932624545110144
}
[08/27/2025 19:50:07 INFO]: Test stats: {
    "score": -0.979982644288386,
    "rmse": 0.979982644288386
}
[08/27/2025 19:50:08 INFO]: Training loss at epoch 32: 1.1381619572639465
[08/27/2025 19:50:10 INFO]: Training loss at epoch 14: 0.8541935086250305
[08/27/2025 19:50:17 INFO]: Training loss at epoch 23: 1.0732210576534271
[08/27/2025 19:50:29 INFO]: Training loss at epoch 31: 0.9886535406112671
[08/27/2025 19:50:30 INFO]: Training loss at epoch 17: 0.9307301938533783
[08/27/2025 19:50:32 INFO]: Training loss at epoch 20: 0.9694784283638
[08/27/2025 19:50:32 INFO]: Training loss at epoch 27: 1.2360668182373047
[08/27/2025 19:50:34 INFO]: Training loss at epoch 13: 1.0905816555023193
[08/27/2025 19:50:39 INFO]: Training loss at epoch 6: 1.2889269292354584
[08/27/2025 19:50:40 INFO]: Training loss at epoch 10: 1.0428078770637512
[08/27/2025 19:50:42 INFO]: Training loss at epoch 27: 1.0041418671607971
[08/27/2025 19:50:56 INFO]: Training loss at epoch 6: 1.1798619627952576
[08/27/2025 19:50:58 INFO]: Training loss at epoch 32: 0.9869235157966614
[08/27/2025 19:51:05 INFO]: Training loss at epoch 21: 1.1092601418495178
[08/27/2025 19:51:07 INFO]: Training loss at epoch 14: 1.0299452543258667
[08/27/2025 19:51:08 INFO]: Training loss at epoch 24: 0.8642384111881256
[08/27/2025 19:51:13 INFO]: Training loss at epoch 7: 1.1222362220287323
[08/27/2025 19:51:14 INFO]: Training loss at epoch 11: 1.0902950763702393
[08/27/2025 19:51:16 INFO]: Training loss at epoch 26: 1.0017970204353333
[08/27/2025 19:51:22 INFO]: Training loss at epoch 28: 0.9566255807876587
[08/27/2025 19:51:25 INFO]: Training loss at epoch 18: 0.9600311517715454
[08/27/2025 19:51:27 INFO]: Training loss at epoch 33: 0.9683219790458679
[08/27/2025 19:51:32 INFO]: Training loss at epoch 28: 0.9496541023254395
[08/27/2025 19:51:38 INFO]: Training loss at epoch 22: 1.1141290664672852
[08/27/2025 19:51:40 INFO]: Training loss at epoch 15: 0.7803866863250732
[08/27/2025 19:51:45 INFO]: Training loss at epoch 8: 1.318590521812439
[08/27/2025 19:51:47 INFO]: Training loss at epoch 12: 1.1219820976257324
[08/27/2025 19:51:55 INFO]: Training loss at epoch 7: 0.8844572901725769
[08/27/2025 19:51:56 INFO]: Training loss at epoch 34: 1.0746040642261505
[08/27/2025 19:51:59 INFO]: Training loss at epoch 25: 0.9103838801383972
[08/27/2025 19:52:01 INFO]: Training loss at epoch 28: 0.9248192608356476
[08/27/2025 19:52:12 INFO]: Training loss at epoch 16: 1.2447715401649475
[08/27/2025 19:52:12 INFO]: Training loss at epoch 29: 1.171755075454712
[08/27/2025 19:52:12 INFO]: Training loss at epoch 23: 1.0749132633209229
[08/27/2025 19:52:19 INFO]: Training loss at epoch 9: 1.0647109746932983
[08/27/2025 19:52:20 INFO]: Training loss at epoch 13: 0.896913468837738
[08/27/2025 19:52:22 INFO]: Training loss at epoch 19: 0.9976437389850616
[08/27/2025 19:52:22 INFO]: Training loss at epoch 29: 0.8999565243721008
[08/27/2025 19:52:25 INFO]: Training loss at epoch 35: 0.9609774649143219
[08/27/2025 19:52:29 INFO]: Training loss at epoch 34: 0.8338984847068787
[08/27/2025 19:52:29 INFO]: Training stats: {
    "score": -0.9848927901753777,
    "rmse": 0.9848927901753777
}
[08/27/2025 19:52:29 INFO]: Val stats: {
    "score": -0.9194393597582143,
    "rmse": 0.9194393597582143
}
[08/27/2025 19:52:29 INFO]: Test stats: {
    "score": -0.973809519246672,
    "rmse": 0.973809519246672
}
[08/27/2025 19:52:30 INFO]: Training stats: {
    "score": -1.0018553360741969,
    "rmse": 1.0018553360741969
}
[08/27/2025 19:52:30 INFO]: Val stats: {
    "score": -0.9218434920120774,
    "rmse": 0.9218434920120774
}
[08/27/2025 19:52:30 INFO]: Test stats: {
    "score": -0.9758055523824511,
    "rmse": 0.9758055523824511
}
[08/27/2025 19:52:30 INFO]: Training loss at epoch 33: 0.8275293409824371
[08/27/2025 19:52:38 INFO]: Training stats: {
    "score": -0.9944319715769595,
    "rmse": 0.9944319715769595
}
[08/27/2025 19:52:38 INFO]: Val stats: {
    "score": -0.9243179705631058,
    "rmse": 0.9243179705631058
}
[08/27/2025 19:52:38 INFO]: Test stats: {
    "score": -0.9678046640167267,
    "rmse": 0.9678046640167267
}
[08/27/2025 19:52:40 INFO]: Training stats: {
    "score": -0.9752173617033947,
    "rmse": 0.9752173617033947
}
[08/27/2025 19:52:40 INFO]: Val stats: {
    "score": -0.8959797981959162,
    "rmse": 0.8959797981959162
}
[08/27/2025 19:52:40 INFO]: Test stats: {
    "score": -0.9863271842911967,
    "rmse": 0.9863271842911967
}
[08/27/2025 19:52:44 INFO]: Training loss at epoch 17: 1.0013254284858704
[08/27/2025 19:52:45 INFO]: Training loss at epoch 24: 1.0967431664466858
[08/27/2025 19:52:47 INFO]: Running Final Evaluation...
[08/27/2025 19:52:49 INFO]: Training loss at epoch 26: 0.9061282575130463
[08/27/2025 19:52:53 INFO]: Training loss at epoch 10: 1.029266357421875
[08/27/2025 19:52:53 INFO]: Training loss at epoch 8: 0.9713715314865112
[08/27/2025 19:52:54 INFO]: Training loss at epoch 14: 0.9274555742740631
[08/27/2025 19:52:54 INFO]: Training loss at epoch 36: 0.7960197925567627
[08/27/2025 19:53:03 INFO]: Training loss at epoch 10: 1.1332864165306091
[08/27/2025 19:53:10 INFO]: Training loss at epoch 40: 1.0777021646499634
[08/27/2025 19:53:17 INFO]: Training loss at epoch 18: 0.8985485434532166
[08/27/2025 19:53:18 INFO]: Training loss at epoch 25: 0.963871568441391
[08/27/2025 19:53:19 INFO]: Training loss at epoch 30: 0.7868326604366302
[08/27/2025 19:53:23 INFO]: Training loss at epoch 37: 0.9023166298866272
[08/27/2025 19:53:27 INFO]: Training loss at epoch 15: 0.9504064321517944
[08/27/2025 19:53:28 INFO]: Training loss at epoch 30: 1.007629007101059
[08/27/2025 19:53:35 INFO]: Training loss at epoch 20: 1.032242625951767
[08/27/2025 19:53:36 INFO]: Training loss at epoch 11: 1.1223735809326172
[08/27/2025 19:53:39 INFO]: Training loss at epoch 27: 0.8447248935699463
[08/27/2025 19:53:43 INFO]: Training accuracy: {
    "score": -1.0453907753429612,
    "rmse": 1.0453907753429612
}
[08/27/2025 19:53:43 INFO]: Val accuracy: {
    "score": -0.8780256419982834,
    "rmse": 0.8780256419982834
}
[08/27/2025 19:53:43 INFO]: Test accuracy: {
    "score": -1.022475383800257,
    "rmse": 1.022475383800257
}
[08/27/2025 19:53:43 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_43",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.022475383800257,
        "rmse": 1.022475383800257
    },
    "train_stats": {
        "score": -1.0453907753429612,
        "rmse": 1.0453907753429612
    },
    "val_stats": {
        "score": -0.8780256419982834,
        "rmse": 0.8780256419982834
    }
}
[08/27/2025 19:53:43 INFO]: Procewss finished for trial maddest-Elbert_trial_43
[08/27/2025 19:53:43 INFO]: 
_________________________________________________

[08/27/2025 19:53:43 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:53:43 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 4
  d_ffn_factor: 1.4698922560396677
  attention_dropout: 0.0517702458686129
  ffn_dropout: 0.0517702458686129
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.8200941066563602e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_61

[08/27/2025 19:53:43 INFO]: This ft_transformer has 1.191 million parameters.
[08/27/2025 19:53:43 INFO]: Training will start at epoch 0.
[08/27/2025 19:53:43 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:53:49 INFO]: Training loss at epoch 19: 1.1215739250183105
[08/27/2025 19:53:51 INFO]: Training loss at epoch 26: 0.9945079386234283
[08/27/2025 19:53:51 INFO]: Training loss at epoch 9: 1.038642406463623
[08/27/2025 19:53:51 INFO]: Training loss at epoch 38: 1.0953364372253418
[08/27/2025 19:53:55 INFO]: Training loss at epoch 15: 0.7182900756597519
[08/27/2025 19:53:59 INFO]: Training loss at epoch 16: 1.030781328678131
[08/27/2025 19:54:00 INFO]: Training stats: {
    "score": -0.9969743573381972,
    "rmse": 0.9969743573381972
}
[08/27/2025 19:54:00 INFO]: Val stats: {
    "score": -0.9098700586392917,
    "rmse": 0.9098700586392917
}
[08/27/2025 19:54:00 INFO]: Test stats: {
    "score": -0.9713770086105811,
    "rmse": 0.9713770086105811
}
[08/27/2025 19:54:08 INFO]: Training loss at epoch 0: 1.0835633873939514
[08/27/2025 19:54:08 INFO]: Training loss at epoch 12: 0.8496781587600708
[08/27/2025 19:54:09 INFO]: Training loss at epoch 31: 0.8940059542655945
[08/27/2025 19:54:11 INFO]: Training stats: {
    "score": -1.023224700645848,
    "rmse": 1.023224700645848
}
[08/27/2025 19:54:11 INFO]: Val stats: {
    "score": -1.026492645890424,
    "rmse": 1.026492645890424
}
[08/27/2025 19:54:11 INFO]: Test stats: {
    "score": -1.001536992244229,
    "rmse": 1.001536992244229
}
[08/27/2025 19:54:12 INFO]: New best epoch, val score: -1.1229893998686293
[08/27/2025 19:54:12 INFO]: Saving model to: maddest-Elbert_trial_61/model_best.pth
[08/27/2025 19:54:18 INFO]: Training loss at epoch 31: 1.0495625436306
[08/27/2025 19:54:20 INFO]: Training loss at epoch 39: 0.8456745743751526
[08/27/2025 19:54:22 INFO]: Training loss at epoch 29: 1.0579289197921753
[08/27/2025 19:54:24 INFO]: Training loss at epoch 27: 0.8996430933475494
[08/27/2025 19:54:29 INFO]: Training loss at epoch 28: 1.1826953291893005
[08/27/2025 19:54:30 INFO]: Training stats: {
    "score": -0.9830258465711714,
    "rmse": 0.9830258465711714
}
[08/27/2025 19:54:30 INFO]: Val stats: {
    "score": -0.9308050065189026,
    "rmse": 0.9308050065189026
}
[08/27/2025 19:54:30 INFO]: Test stats: {
    "score": -0.9742001077906951,
    "rmse": 0.9742001077906951
}
[08/27/2025 19:54:30 INFO]: Training loss at epoch 21: 1.0401673316955566
[08/27/2025 19:54:30 INFO]: Training loss at epoch 32: 1.0126412510871887
[08/27/2025 19:54:33 INFO]: Training loss at epoch 20: 0.8367432355880737
[08/27/2025 19:54:33 INFO]: Training loss at epoch 17: 1.0123844146728516
[08/27/2025 19:54:34 INFO]: Running Final Evaluation...
[08/27/2025 19:54:38 INFO]: Training loss at epoch 1: 1.1416925191879272
[08/27/2025 19:54:42 INFO]: New best epoch, val score: -1.0014013194070173
[08/27/2025 19:54:42 INFO]: Saving model to: maddest-Elbert_trial_61/model_best.pth
[08/27/2025 19:54:42 INFO]: Training loss at epoch 13: 0.8324392437934875
[08/27/2025 19:54:43 INFO]: Training accuracy: {
    "score": -1.005320465844786,
    "rmse": 1.005320465844786
}
[08/27/2025 19:54:43 INFO]: Val accuracy: {
    "score": -0.8891822322299572,
    "rmse": 0.8891822322299572
}
[08/27/2025 19:54:43 INFO]: Test accuracy: {
    "score": -0.984020510988367,
    "rmse": 0.984020510988367
}
[08/27/2025 19:54:43 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_55",
    "best_epoch": 8,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.984020510988367,
        "rmse": 0.984020510988367
    },
    "train_stats": {
        "score": -1.005320465844786,
        "rmse": 1.005320465844786
    },
    "val_stats": {
        "score": -0.8891822322299572,
        "rmse": 0.8891822322299572
    }
}
[08/27/2025 19:54:43 INFO]: Procewss finished for trial maddest-Elbert_trial_55
[08/27/2025 19:54:44 INFO]: 
_________________________________________________

[08/27/2025 19:54:44 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:54:44 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 4
  d_ffn_factor: 1.5767154839073643
  attention_dropout: 0.08039877223413172
  ffn_dropout: 0.08039877223413172
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.941312300889982e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_62

[08/27/2025 19:54:44 INFO]: This ft_transformer has 2.708 million parameters.
[08/27/2025 19:54:44 INFO]: Training will start at epoch 0.
[08/27/2025 19:54:44 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:54:58 INFO]: Training loss at epoch 28: 0.8754312694072723
[08/27/2025 19:54:58 INFO]: Training loss at epoch 35: 0.7437276542186737
[08/27/2025 19:54:59 INFO]: Training loss at epoch 32: 0.9309873282909393
[08/27/2025 19:55:03 INFO]: Training loss at epoch 52: 0.5819744616746902
[08/27/2025 19:55:05 INFO]: Training loss at epoch 21: 1.0713936686515808
[08/27/2025 19:55:07 INFO]: Training loss at epoch 18: 0.8876791596412659
[08/27/2025 19:55:08 INFO]: Training loss at epoch 2: 0.914947897195816
[08/27/2025 19:55:09 INFO]: Training loss at epoch 32: 1.364701747894287
[08/27/2025 19:55:10 INFO]: Training loss at epoch 10: 1.0447142720222473
[08/27/2025 19:55:10 INFO]: Training stats: {
    "score": -0.975758441064515,
    "rmse": 0.975758441064515
}
[08/27/2025 19:55:10 INFO]: Val stats: {
    "score": -0.9222611048298013,
    "rmse": 0.9222611048298013
}
[08/27/2025 19:55:10 INFO]: Test stats: {
    "score": -0.9709751113549324,
    "rmse": 0.9709751113549324
}
[08/27/2025 19:55:11 INFO]: New best epoch, val score: -0.8973641627638296
[08/27/2025 19:55:11 INFO]: Saving model to: maddest-Elbert_trial_61/model_best.pth
[08/27/2025 19:55:14 INFO]: Training loss at epoch 14: 0.9891388118267059
[08/27/2025 19:55:20 INFO]: Training loss at epoch 29: 1.0349362194538116
[08/27/2025 19:55:27 INFO]: Training loss at epoch 22: 0.8405980169773102
[08/27/2025 19:55:31 INFO]: Training loss at epoch 29: 1.1676258742809296
[08/27/2025 19:55:34 INFO]: Training loss at epoch 0: 1.0305794775485992
[08/27/2025 19:55:37 INFO]: Training stats: {
    "score": -0.9914796869158602,
    "rmse": 0.9914796869158602
}
[08/27/2025 19:55:37 INFO]: Val stats: {
    "score": -0.9460276352726271,
    "rmse": 0.9460276352726271
}
[08/27/2025 19:55:37 INFO]: Test stats: {
    "score": -0.9732140117658072,
    "rmse": 0.9732140117658072
}
[08/27/2025 19:55:37 INFO]: Training loss at epoch 22: 1.0140400528907776
[08/27/2025 19:55:37 INFO]: Training loss at epoch 3: 1.183050274848938
[08/27/2025 19:55:39 INFO]: Training loss at epoch 19: 0.903694212436676
[08/27/2025 19:55:40 INFO]: New best epoch, val score: -1.000484296419045
[08/27/2025 19:55:40 INFO]: Saving model to: maddest-Elbert_trial_62/model_best.pth
[08/27/2025 19:55:41 INFO]: New best epoch, val score: -0.8869916835347681
[08/27/2025 19:55:41 INFO]: Saving model to: maddest-Elbert_trial_61/model_best.pth
[08/27/2025 19:55:41 INFO]: Training stats: {
    "score": -0.9985603557623086,
    "rmse": 0.9985603557623086
}
[08/27/2025 19:55:41 INFO]: Val stats: {
    "score": -0.8903730119732193,
    "rmse": 0.8903730119732193
}
[08/27/2025 19:55:41 INFO]: Test stats: {
    "score": -0.9744246309019863,
    "rmse": 0.9744246309019863
}
[08/27/2025 19:55:48 INFO]: Training loss at epoch 15: 0.8397987484931946
[08/27/2025 19:55:49 INFO]: Training loss at epoch 33: 1.1094539165496826
[08/27/2025 19:55:52 INFO]: Training stats: {
    "score": -0.9912962855426247,
    "rmse": 0.9912962855426247
}
[08/27/2025 19:55:52 INFO]: Val stats: {
    "score": -0.9129707092246586,
    "rmse": 0.9129707092246586
}
[08/27/2025 19:55:52 INFO]: Test stats: {
    "score": -0.9691787092000417,
    "rmse": 0.9691787092000417
}
[08/27/2025 19:56:00 INFO]: Training loss at epoch 33: 1.13987797498703
[08/27/2025 19:56:07 INFO]: Training loss at epoch 4: 1.1575517654418945
[08/27/2025 19:56:08 INFO]: Training loss at epoch 11: 0.9829937815666199
[08/27/2025 19:56:10 INFO]: Training loss at epoch 23: 1.0293684303760529
[08/27/2025 19:56:11 INFO]: New best epoch, val score: -0.884634644256521
[08/27/2025 19:56:11 INFO]: Saving model to: maddest-Elbert_trial_61/model_best.pth
[08/27/2025 19:56:15 INFO]: Training loss at epoch 30: 0.9223329722881317
[08/27/2025 19:56:21 INFO]: Training loss at epoch 16: 1.0389769673347473
[08/27/2025 19:56:22 INFO]: Training loss at epoch 23: 1.0267285704612732
[08/27/2025 19:56:25 INFO]: Training loss at epoch 20: 0.8950466513633728
[08/27/2025 19:56:27 INFO]: Training loss at epoch 30: 0.9751737713813782
[08/27/2025 19:56:30 INFO]: Training loss at epoch 1: 1.128003180027008
[08/27/2025 19:56:34 INFO]: Training loss at epoch 11: 1.0663140416145325
[08/27/2025 19:56:37 INFO]: Training loss at epoch 5: 1.0058341920375824
[08/27/2025 19:56:37 INFO]: New best epoch, val score: -0.8865120675428257
[08/27/2025 19:56:37 INFO]: Saving model to: maddest-Elbert_trial_62/model_best.pth
[08/27/2025 19:56:39 INFO]: Training loss at epoch 34: 1.0866250693798065
[08/27/2025 19:56:40 INFO]: Training loss at epoch 29: 0.9422690570354462
[08/27/2025 19:56:41 INFO]: New best epoch, val score: -0.8844357001845157
[08/27/2025 19:56:41 INFO]: Saving model to: maddest-Elbert_trial_61/model_best.pth
[08/27/2025 19:56:43 INFO]: Training loss at epoch 24: 1.2326771914958954
[08/27/2025 19:56:48 INFO]: Training loss at epoch 31: 1.0432959496974945
[08/27/2025 19:56:50 INFO]: Training loss at epoch 34: 0.8905268013477325
[08/27/2025 19:56:53 INFO]: Training loss at epoch 17: 1.1157768964767456
[08/27/2025 19:56:59 INFO]: Training loss at epoch 21: 0.8644920587539673
[08/27/2025 19:57:07 INFO]: Training loss at epoch 6: 1.0600380301475525
[08/27/2025 19:57:08 INFO]: Training loss at epoch 12: 1.1338200867176056
[08/27/2025 19:57:10 INFO]: New best epoch, val score: -0.8844067449426172
[08/27/2025 19:57:10 INFO]: Saving model to: maddest-Elbert_trial_61/model_best.pth
[08/27/2025 19:57:15 INFO]: Training loss at epoch 25: 1.0475492477416992
[08/27/2025 19:57:17 INFO]: Training loss at epoch 24: 0.8456967771053314
[08/27/2025 19:57:19 INFO]: Training loss at epoch 31: 1.0157663226127625
[08/27/2025 19:57:20 INFO]: Training loss at epoch 32: 0.9801293015480042
[08/27/2025 19:57:26 INFO]: Training loss at epoch 18: 1.0098764300346375
[08/27/2025 19:57:28 INFO]: Training loss at epoch 36: 1.114268034696579
[08/27/2025 19:57:28 INFO]: Training loss at epoch 2: 1.072528600692749
[08/27/2025 19:57:30 INFO]: Training loss at epoch 35: 0.9801214933395386
[08/27/2025 19:57:32 INFO]: Training loss at epoch 22: 1.1360238790512085
[08/27/2025 19:57:33 INFO]: Training loss at epoch 30: 0.9472756385803223
[08/27/2025 19:57:37 INFO]: Training loss at epoch 7: 1.02653169631958
[08/27/2025 19:57:39 INFO]: Training loss at epoch 16: 1.102254331111908
[08/27/2025 19:57:41 INFO]: Training loss at epoch 35: 1.0175487697124481
[08/27/2025 19:57:47 INFO]: Training loss at epoch 26: 0.9748190343379974
[08/27/2025 19:57:54 INFO]: Training loss at epoch 33: 1.0997794270515442
[08/27/2025 19:57:59 INFO]: Training loss at epoch 19: 1.210064321756363
[08/27/2025 19:58:05 INFO]: Training loss at epoch 23: 1.037817358970642
[08/27/2025 19:58:06 INFO]: Training loss at epoch 8: 1.079158365726471
[08/27/2025 19:58:07 INFO]: Training loss at epoch 13: 0.9702416360378265
[08/27/2025 19:58:10 INFO]: Training stats: {
    "score": -0.9996230439353126,
    "rmse": 0.9996230439353126
}
[08/27/2025 19:58:10 INFO]: Val stats: {
    "score": -0.9231988677185273,
    "rmse": 0.9231988677185273
}
[08/27/2025 19:58:10 INFO]: Test stats: {
    "score": -0.9758115731649473,
    "rmse": 0.9758115731649473
}
[08/27/2025 19:58:10 INFO]: Training loss at epoch 32: 1.0123918056488037
[08/27/2025 19:58:14 INFO]: Training loss at epoch 25: 0.9684523940086365
[08/27/2025 19:58:17 INFO]: Running Final Evaluation...
[08/27/2025 19:58:20 INFO]: Training loss at epoch 36: 1.1500513553619385
[08/27/2025 19:58:21 INFO]: Training loss at epoch 27: 0.8468222916126251
[08/27/2025 19:58:25 INFO]: Training loss at epoch 3: 1.3320273756980896
[08/27/2025 19:58:27 INFO]: Training loss at epoch 34: 1.008469820022583
[08/27/2025 19:58:30 INFO]: Running Final Evaluation...
[08/27/2025 19:58:31 INFO]: New best epoch, val score: -0.8822747431224891
[08/27/2025 19:58:31 INFO]: Saving model to: maddest-Elbert_trial_62/model_best.pth
[08/27/2025 19:58:31 INFO]: Training loss at epoch 36: 0.9006631374359131
[08/27/2025 19:58:33 INFO]: Training accuracy: {
    "score": -1.0170248553638506,
    "rmse": 1.0170248553638506
}
[08/27/2025 19:58:33 INFO]: Val accuracy: {
    "score": -0.8819608956215819,
    "rmse": 0.8819608956215819
}
[08/27/2025 19:58:33 INFO]: Test accuracy: {
    "score": -0.9899174355493903,
    "rmse": 0.9899174355493903
}
[08/27/2025 19:58:33 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_53",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9899174355493903,
        "rmse": 0.9899174355493903
    },
    "train_stats": {
        "score": -1.0170248553638506,
        "rmse": 1.0170248553638506
    },
    "val_stats": {
        "score": -0.8819608956215819,
        "rmse": 0.8819608956215819
    }
}
[08/27/2025 19:58:33 INFO]: Procewss finished for trial maddest-Elbert_trial_53
[08/27/2025 19:58:33 INFO]: 
_________________________________________________

[08/27/2025 19:58:33 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:58:33 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 1.606451254069723
  attention_dropout: 0.08640678404947054
  ffn_dropout: 0.08640678404947054
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.8342818466747317e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_63

[08/27/2025 19:58:33 INFO]: This ft_transformer has 5.342 million parameters.
[08/27/2025 19:58:33 INFO]: Training will start at epoch 0.
[08/27/2025 19:58:33 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:58:35 INFO]: Training loss at epoch 9: 1.2711652517318726
[08/27/2025 19:58:38 INFO]: Training loss at epoch 24: 1.041896402835846
[08/27/2025 19:58:41 INFO]: Training accuracy: {
    "score": -1.0405241603068058,
    "rmse": 1.0405241603068058
}
[08/27/2025 19:58:41 INFO]: Val accuracy: {
    "score": -0.876946587309548,
    "rmse": 0.876946587309548
}
[08/27/2025 19:58:41 INFO]: Test accuracy: {
    "score": -1.0095219779172198,
    "rmse": 1.0095219779172198
}
[08/27/2025 19:58:41 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_56",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0095219779172198,
        "rmse": 1.0095219779172198
    },
    "train_stats": {
        "score": -1.0405241603068058,
        "rmse": 1.0405241603068058
    },
    "val_stats": {
        "score": -0.876946587309548,
        "rmse": 0.876946587309548
    }
}
[08/27/2025 19:58:41 INFO]: Procewss finished for trial maddest-Elbert_trial_56
[08/27/2025 19:58:41 INFO]: 
_________________________________________________

[08/27/2025 19:58:41 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:58:41 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.6111874989456403
  attention_dropout: 0.09069991555007846
  ffn_dropout: 0.09069991555007846
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.6543524906909195e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_64

[08/27/2025 19:58:41 INFO]: This ft_transformer has 1.247 million parameters.
[08/27/2025 19:58:41 INFO]: Training will start at epoch 0.
[08/27/2025 19:58:41 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:58:42 INFO]: Training loss at epoch 20: 1.072292983531952
[08/27/2025 19:58:45 INFO]: Training stats: {
    "score": -1.0219094756076605,
    "rmse": 1.0219094756076605
}
[08/27/2025 19:58:45 INFO]: Val stats: {
    "score": -0.8864372458421981,
    "rmse": 0.8864372458421981
}
[08/27/2025 19:58:45 INFO]: Test stats: {
    "score": -0.9931634801967689,
    "rmse": 0.9931634801967689
}
[08/27/2025 19:58:53 INFO]: Training loss at epoch 28: 1.0443978309631348
[08/27/2025 19:59:06 INFO]: Training loss at epoch 14: 1.0356969833374023
[08/27/2025 19:59:09 INFO]: Training loss at epoch 26: 0.9735098481178284
[08/27/2025 19:59:09 INFO]: Training loss at epoch 37: 0.9661537110805511
[08/27/2025 19:59:11 INFO]: Training loss at epoch 0: 1.9411684274673462
[08/27/2025 19:59:12 INFO]: Training loss at epoch 25: 1.0622280836105347
[08/27/2025 19:59:13 INFO]: New best epoch, val score: -0.8940114353249348
[08/27/2025 19:59:13 INFO]: Saving model to: maddest-Elbert_trial_58/model_best.pth
[08/27/2025 19:59:15 INFO]: Training loss at epoch 10: 1.1249643564224243
[08/27/2025 19:59:16 INFO]: New best epoch, val score: -1.0974054411843688
[08/27/2025 19:59:16 INFO]: Saving model to: maddest-Elbert_trial_64/model_best.pth
[08/27/2025 19:59:16 INFO]: Training loss at epoch 21: 1.2351366877555847
[08/27/2025 19:59:21 INFO]: Training loss at epoch 37: 0.9460361897945404
[08/27/2025 19:59:21 INFO]: Training loss at epoch 4: 1.1345171928405762
[08/27/2025 19:59:25 INFO]: Training loss at epoch 29: 0.9008356928825378
[08/27/2025 19:59:36 INFO]: Training stats: {
    "score": -1.0111864454539532,
    "rmse": 1.0111864454539532
}
[08/27/2025 19:59:36 INFO]: Val stats: {
    "score": -0.9819989110520154,
    "rmse": 0.9819989110520154
}
[08/27/2025 19:59:36 INFO]: Test stats: {
    "score": -0.9841185560662492,
    "rmse": 0.9841185560662492
}
[08/27/2025 19:59:36 INFO]: Training stats: {
    "score": -1.000786807476687,
    "rmse": 1.000786807476687
}
[08/27/2025 19:59:36 INFO]: Val stats: {
    "score": -0.9661328142986626,
    "rmse": 0.9661328142986626
}
[08/27/2025 19:59:36 INFO]: Test stats: {
    "score": -0.9799501177541853,
    "rmse": 0.9799501177541853
}
[08/27/2025 19:59:45 INFO]: Training loss at epoch 11: 1.1257882416248322
[08/27/2025 19:59:45 INFO]: Training loss at epoch 26: 1.1684690415859222
[08/27/2025 19:59:46 INFO]: Training loss at epoch 1: 1.5301916003227234
[08/27/2025 19:59:49 INFO]: Training loss at epoch 22: 0.9310487508773804
[08/27/2025 19:59:50 INFO]: New best epoch, val score: -0.9915718893436187
[08/27/2025 19:59:50 INFO]: Saving model to: maddest-Elbert_trial_64/model_best.pth
[08/27/2025 19:59:54 INFO]: Training loss at epoch 31: 1.030624508857727
[08/27/2025 19:59:57 INFO]: Training loss at epoch 37: 0.9954271614551544
[08/27/2025 19:59:58 INFO]: Training loss at epoch 38: 1.0017504692077637
[08/27/2025 20:00:04 INFO]: Training loss at epoch 27: 0.8899002075195312
[08/27/2025 20:00:04 INFO]: Training loss at epoch 15: 0.8827945590019226
[08/27/2025 20:00:09 INFO]: Training loss at epoch 30: 0.9871078729629517
[08/27/2025 20:00:10 INFO]: Training loss at epoch 38: 1.0910634100437164
[08/27/2025 20:00:12 INFO]: New best epoch, val score: -0.8917106268434106
[08/27/2025 20:00:12 INFO]: Saving model to: maddest-Elbert_trial_58/model_best.pth
[08/27/2025 20:00:14 INFO]: Training loss at epoch 12: 1.0975671708583832
[08/27/2025 20:00:15 INFO]: Training loss at epoch 12: 0.951964944601059
[08/27/2025 20:00:16 INFO]: Training loss at epoch 5: 0.8727055490016937
[08/27/2025 20:00:18 INFO]: Training loss at epoch 27: 0.8394838571548462
[08/27/2025 20:00:20 INFO]: Training loss at epoch 2: 1.4872128367424011
[08/27/2025 20:00:22 INFO]: Training loss at epoch 23: 0.9628775417804718
[08/27/2025 20:00:24 INFO]: New best epoch, val score: -0.9003464286538148
[08/27/2025 20:00:24 INFO]: Saving model to: maddest-Elbert_trial_64/model_best.pth
[08/27/2025 20:00:41 INFO]: Training loss at epoch 31: 0.8215888738632202
[08/27/2025 20:00:43 INFO]: Training loss at epoch 13: 0.8668018579483032
[08/27/2025 20:00:44 INFO]: Training loss at epoch 41: 1.0098011195659637
[08/27/2025 20:00:46 INFO]: Training loss at epoch 0: 1.1596325039863586
[08/27/2025 20:00:49 INFO]: Training loss at epoch 39: 0.9365762770175934
[08/27/2025 20:00:51 INFO]: Training loss at epoch 28: 0.8754046559333801
[08/27/2025 20:00:53 INFO]: Training loss at epoch 53: 0.748883992433548
[08/27/2025 20:00:55 INFO]: Training loss at epoch 24: 0.9599567949771881
[08/27/2025 20:00:55 INFO]: Training loss at epoch 3: 1.1322245001792908
[08/27/2025 20:01:00 INFO]: Training loss at epoch 28: 0.8930146396160126
[08/27/2025 20:01:00 INFO]: Training loss at epoch 39: 0.8464860916137695
[08/27/2025 20:01:03 INFO]: Training loss at epoch 16: 1.026711106300354
[08/27/2025 20:01:05 INFO]: New best epoch, val score: -0.9671839631816407
[08/27/2025 20:01:05 INFO]: Saving model to: maddest-Elbert_trial_63/model_best.pth
[08/27/2025 20:01:06 INFO]: Training stats: {
    "score": -0.9867833021976665,
    "rmse": 0.9867833021976665
}
[08/27/2025 20:01:06 INFO]: Val stats: {
    "score": -0.891692640379017,
    "rmse": 0.891692640379017
}
[08/27/2025 20:01:06 INFO]: Test stats: {
    "score": -0.9834474573242241,
    "rmse": 0.9834474573242241
}
[08/27/2025 20:01:12 INFO]: Training loss at epoch 6: 1.384995847940445
[08/27/2025 20:01:13 INFO]: Training loss at epoch 14: 1.0983059704303741
[08/27/2025 20:01:13 INFO]: Training loss at epoch 32: 1.1174507141113281
[08/27/2025 20:01:17 INFO]: Training stats: {
    "score": -0.9918834382056594,
    "rmse": 0.9918834382056594
}
[08/27/2025 20:01:17 INFO]: Val stats: {
    "score": -0.9300998036502611,
    "rmse": 0.9300998036502611
}
[08/27/2025 20:01:17 INFO]: Test stats: {
    "score": -0.9674806550701643,
    "rmse": 0.9674806550701643
}
[08/27/2025 20:01:17 INFO]: Running Final Evaluation...
[08/27/2025 20:01:23 INFO]: Training loss at epoch 17: 1.2116613984107971
[08/27/2025 20:01:25 INFO]: Training loss at epoch 29: 1.1792850196361542
[08/27/2025 20:01:30 INFO]: Training loss at epoch 25: 0.9737422466278076
[08/27/2025 20:01:30 INFO]: Training loss at epoch 4: 0.8906067609786987
[08/27/2025 20:01:31 INFO]: Training accuracy: {
    "score": -1.0126334974983195,
    "rmse": 1.0126334974983195
}
[08/27/2025 20:01:31 INFO]: Val accuracy: {
    "score": -0.8892123747344469,
    "rmse": 0.8892123747344469
}
[08/27/2025 20:01:31 INFO]: Test accuracy: {
    "score": -0.984196533793898,
    "rmse": 0.984196533793898
}
[08/27/2025 20:01:31 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_57",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.984196533793898,
        "rmse": 0.984196533793898
    },
    "train_stats": {
        "score": -1.0126334974983195,
        "rmse": 1.0126334974983195
    },
    "val_stats": {
        "score": -0.8892123747344469,
        "rmse": 0.8892123747344469
    }
}
[08/27/2025 20:01:31 INFO]: Procewss finished for trial maddest-Elbert_trial_57
[08/27/2025 20:01:31 INFO]: 
_________________________________________________

[08/27/2025 20:01:31 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:01:31 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.6105424698185553
  attention_dropout: 0.08808476010485994
  ffn_dropout: 0.08808476010485994
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00012643855796507902
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_65

[08/27/2025 20:01:31 INFO]: This ft_transformer has 1.247 million parameters.
[08/27/2025 20:01:31 INFO]: Training will start at epoch 0.
[08/27/2025 20:01:31 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:01:37 INFO]: Training stats: {
    "score": -0.9861814817134362,
    "rmse": 0.9861814817134362
}
[08/27/2025 20:01:37 INFO]: Val stats: {
    "score": -0.9299339365106939,
    "rmse": 0.9299339365106939
}
[08/27/2025 20:01:37 INFO]: Test stats: {
    "score": -0.9675420174080065,
    "rmse": 0.9675420174080065
}
[08/27/2025 20:01:42 INFO]: Training loss at epoch 15: 1.0573428869247437
[08/27/2025 20:01:55 INFO]: Training loss at epoch 29: 0.7725920975208282
[08/27/2025 20:01:56 INFO]: Training loss at epoch 40: 0.929672509431839
[08/27/2025 20:01:59 INFO]: Training loss at epoch 33: 1.014648050069809
[08/27/2025 20:02:01 INFO]: Training loss at epoch 0: 0.9344652891159058
[08/27/2025 20:02:01 INFO]: Training loss at epoch 17: 0.9880859553813934
[08/27/2025 20:02:03 INFO]: Training loss at epoch 26: 1.0499367713928223
[08/27/2025 20:02:03 INFO]: Running Final Evaluation...
[08/27/2025 20:02:04 INFO]: New best epoch, val score: -1.005051539274666
[08/27/2025 20:02:04 INFO]: Saving model to: maddest-Elbert_trial_65/model_best.pth
[08/27/2025 20:02:04 INFO]: Training loss at epoch 5: 1.1464564204216003
[08/27/2025 20:02:07 INFO]: Training loss at epoch 40: 1.3126820623874664
[08/27/2025 20:02:08 INFO]: Training loss at epoch 7: 0.9128972291946411
[08/27/2025 20:02:10 INFO]: Training loss at epoch 30: 1.1019174754619598
[08/27/2025 20:02:11 INFO]: Training loss at epoch 16: 1.222286581993103
[08/27/2025 20:02:13 INFO]: Running Final Evaluation...
[08/27/2025 20:02:13 INFO]: Training stats: {
    "score": -0.9604584359251569,
    "rmse": 0.9604584359251569
}
[08/27/2025 20:02:13 INFO]: Val stats: {
    "score": -0.8971843153610611,
    "rmse": 0.8971843153610611
}
[08/27/2025 20:02:13 INFO]: Test stats: {
    "score": -0.9870043691809236,
    "rmse": 0.9870043691809236
}
[08/27/2025 20:02:16 INFO]: Training loss at epoch 32: 0.749258354306221
[08/27/2025 20:02:20 INFO]: Training accuracy: {
    "score": -1.0354274878300938,
    "rmse": 1.0354274878300938
}
[08/27/2025 20:02:20 INFO]: Val accuracy: {
    "score": -0.8847859069932192,
    "rmse": 0.8847859069932192
}
[08/27/2025 20:02:20 INFO]: Test accuracy: {
    "score": -1.014231844646081,
    "rmse": 1.014231844646081
}
[08/27/2025 20:02:20 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_51",
    "best_epoch": 9,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.014231844646081,
        "rmse": 1.014231844646081
    },
    "train_stats": {
        "score": -1.0354274878300938,
        "rmse": 1.0354274878300938
    },
    "val_stats": {
        "score": -0.8847859069932192,
        "rmse": 0.8847859069932192
    }
}
[08/27/2025 20:02:20 INFO]: Procewss finished for trial maddest-Elbert_trial_51
[08/27/2025 20:02:20 INFO]: 
_________________________________________________

[08/27/2025 20:02:20 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:02:20 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.5536538628284025
  attention_dropout: 0.0902981871438111
  ffn_dropout: 0.0902981871438111
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.6110915972965482e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_66

[08/27/2025 20:02:20 INFO]: This ft_transformer has 1.224 million parameters.
[08/27/2025 20:02:20 INFO]: Training will start at epoch 0.
[08/27/2025 20:02:20 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:02:27 INFO]: Training loss at epoch 38: 0.9671447575092316
[08/27/2025 20:02:30 INFO]: Training accuracy: {
    "score": -1.046983053799612,
    "rmse": 1.046983053799612
}
[08/27/2025 20:02:30 INFO]: Val accuracy: {
    "score": -0.8837734279288,
    "rmse": 0.8837734279288
}
[08/27/2025 20:02:30 INFO]: Test accuracy: {
    "score": -1.0174170128539042,
    "rmse": 1.0174170128539042
}
[08/27/2025 20:02:30 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_52",
    "best_epoch": 9,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0174170128539042,
        "rmse": 1.0174170128539042
    },
    "train_stats": {
        "score": -1.046983053799612,
        "rmse": 1.046983053799612
    },
    "val_stats": {
        "score": -0.8837734279288,
        "rmse": 0.8837734279288
    }
}
[08/27/2025 20:02:30 INFO]: Procewss finished for trial maddest-Elbert_trial_52
[08/27/2025 20:02:30 INFO]: 
_________________________________________________

[08/27/2025 20:02:30 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:02:30 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.8347689739216169
  attention_dropout: 0.07656284297432084
  ffn_dropout: 0.07656284297432084
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.609752067725294e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_67

[08/27/2025 20:02:30 INFO]: This ft_transformer has 1.335 million parameters.
[08/27/2025 20:02:30 INFO]: Training will start at epoch 0.
[08/27/2025 20:02:30 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:02:33 INFO]: Running Final Evaluation...
[08/27/2025 20:02:34 INFO]: Training loss at epoch 1: 0.9982217848300934
[08/27/2025 20:02:35 INFO]: Training loss at epoch 27: 0.9705567359924316
[08/27/2025 20:02:37 INFO]: New best epoch, val score: -0.9072312742270571
[08/27/2025 20:02:37 INFO]: Saving model to: maddest-Elbert_trial_65/model_best.pth
[08/27/2025 20:02:38 INFO]: Training loss at epoch 6: 1.0232615768909454
[08/27/2025 20:02:40 INFO]: Training loss at epoch 17: 1.0412660837173462
[08/27/2025 20:02:43 INFO]: Training loss at epoch 31: 0.9127556383609772
[08/27/2025 20:02:43 INFO]: Training loss at epoch 27: 1.103614330291748
[08/27/2025 20:02:47 INFO]: Training loss at epoch 0: 1.1306644082069397
[08/27/2025 20:02:52 INFO]: New best epoch, val score: -1.0594817010279844
[08/27/2025 20:02:52 INFO]: Saving model to: maddest-Elbert_trial_66/model_best.pth
[08/27/2025 20:02:58 INFO]: Training loss at epoch 18: 0.9942479729652405
[08/27/2025 20:03:00 INFO]: Training loss at epoch 0: 1.0261613130569458
[08/27/2025 20:03:04 INFO]: Training loss at epoch 8: 1.0991310477256775
[08/27/2025 20:03:04 INFO]: New best epoch, val score: -0.9553032043518187
[08/27/2025 20:03:04 INFO]: Saving model to: maddest-Elbert_trial_67/model_best.pth
[08/27/2025 20:03:07 INFO]: Training loss at epoch 2: 1.2806637287139893
[08/27/2025 20:03:07 INFO]: Training loss at epoch 30: 1.1210412979125977
[08/27/2025 20:03:07 INFO]: Training loss at epoch 28: 0.9416788816452026
[08/27/2025 20:03:11 INFO]: Training loss at epoch 7: 1.142354965209961
[08/27/2025 20:03:11 INFO]: Training loss at epoch 18: 0.9893420040607452
[08/27/2025 20:03:14 INFO]: Training loss at epoch 1: 1.0505894422531128
[08/27/2025 20:03:16 INFO]: Training loss at epoch 32: 0.9343230128288269
[08/27/2025 20:03:21 INFO]: Training loss at epoch 1: 1.2306057810783386
[08/27/2025 20:03:25 INFO]: New best epoch, val score: -0.9661845352337399
[08/27/2025 20:03:25 INFO]: Saving model to: maddest-Elbert_trial_66/model_best.pth
[08/27/2025 20:03:26 INFO]: Training accuracy: {
    "score": -1.0204460762825422,
    "rmse": 1.0204460762825422
}
[08/27/2025 20:03:26 INFO]: Val accuracy: {
    "score": -0.8955708026810983,
    "rmse": 0.8955708026810983
}
[08/27/2025 20:03:26 INFO]: Test accuracy: {
    "score": -0.9978797070004457,
    "rmse": 0.9978797070004457
}
[08/27/2025 20:03:26 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_44",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9978797070004457,
        "rmse": 0.9978797070004457
    },
    "train_stats": {
        "score": -1.0204460762825422,
        "rmse": 1.0204460762825422
    },
    "val_stats": {
        "score": -0.8955708026810983,
        "rmse": 0.8955708026810983
    }
}
[08/27/2025 20:03:26 INFO]: Procewss finished for trial maddest-Elbert_trial_44
[08/27/2025 20:03:26 INFO]: 
_________________________________________________

[08/27/2025 20:03:26 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:03:26 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.5885271149798894
  attention_dropout: 0.08527040419558957
  ffn_dropout: 0.08527040419558957
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.50481215123573e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_68

[08/27/2025 20:03:26 INFO]: This ft_transformer has 1.029 million parameters.
[08/27/2025 20:03:26 INFO]: Training will start at epoch 0.
[08/27/2025 20:03:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:03:35 INFO]: Training loss at epoch 1: 1.1713836789131165
[08/27/2025 20:03:38 INFO]: New best epoch, val score: -0.9153554969041642
[08/27/2025 20:03:38 INFO]: Saving model to: maddest-Elbert_trial_67/model_best.pth
[08/27/2025 20:03:40 INFO]: Training loss at epoch 19: 1.0933823585510254
[08/27/2025 20:03:41 INFO]: Training loss at epoch 29: 1.0773332715034485
[08/27/2025 20:03:41 INFO]: Training loss at epoch 3: 1.1718323528766632
[08/27/2025 20:03:45 INFO]: Training loss at epoch 8: 1.2550572752952576
[08/27/2025 20:03:50 INFO]: Training loss at epoch 33: 1.0380488634109497
[08/27/2025 20:03:51 INFO]: Training stats: {
    "score": -1.0027058000397624,
    "rmse": 1.0027058000397624
}
[08/27/2025 20:03:51 INFO]: Val stats: {
    "score": -0.9641594491593375,
    "rmse": 0.9641594491593375
}
[08/27/2025 20:03:51 INFO]: Test stats: {
    "score": -0.9767930874119753,
    "rmse": 0.9767930874119753
}
[08/27/2025 20:03:52 INFO]: Training stats: {
    "score": -0.9988813072651698,
    "rmse": 0.9988813072651698
}
[08/27/2025 20:03:52 INFO]: Val stats: {
    "score": -0.9527350356897667,
    "rmse": 0.9527350356897667
}
[08/27/2025 20:03:52 INFO]: Test stats: {
    "score": -0.9780054429444826,
    "rmse": 0.9780054429444826
}
[08/27/2025 20:03:53 INFO]: Training loss at epoch 0: 1.4618691802024841
[08/27/2025 20:03:54 INFO]: Training loss at epoch 13: 1.1533846855163574
[08/27/2025 20:03:54 INFO]: Training loss at epoch 2: 1.166783481836319
[08/27/2025 20:03:56 INFO]: Training loss at epoch 19: 0.864802747964859
[08/27/2025 20:03:57 INFO]: New best epoch, val score: -1.323986145406669
[08/27/2025 20:03:57 INFO]: Saving model to: maddest-Elbert_trial_68/model_best.pth
[08/27/2025 20:03:58 INFO]: New best epoch, val score: -0.8999615014450503
[08/27/2025 20:03:58 INFO]: Saving model to: maddest-Elbert_trial_66/model_best.pth
[08/27/2025 20:03:59 INFO]: Training loss at epoch 9: 0.8808537423610687
[08/27/2025 20:04:02 INFO]: Training loss at epoch 31: 1.174210637807846
[08/27/2025 20:04:10 INFO]: Training loss at epoch 2: 1.0226819515228271
[08/27/2025 20:04:15 INFO]: New best epoch, val score: -0.9063921285956277
[08/27/2025 20:04:15 INFO]: Saving model to: maddest-Elbert_trial_67/model_best.pth
[08/27/2025 20:04:17 INFO]: Training loss at epoch 4: 1.161594569683075
[08/27/2025 20:04:18 INFO]: Training stats: {
    "score": -0.9946029312984107,
    "rmse": 0.9946029312984107
}
[08/27/2025 20:04:18 INFO]: Val stats: {
    "score": -0.9054403548810585,
    "rmse": 0.9054403548810585
}
[08/27/2025 20:04:18 INFO]: Test stats: {
    "score": -0.9742856405938604,
    "rmse": 0.9742856405938604
}
[08/27/2025 20:04:19 INFO]: Training loss at epoch 9: 1.1075397729873657
[08/27/2025 20:04:19 INFO]: Training stats: {
    "score": -1.003489932924396,
    "rmse": 1.003489932924396
}
[08/27/2025 20:04:19 INFO]: Val stats: {
    "score": -0.9581843237322812,
    "rmse": 0.9581843237322812
}
[08/27/2025 20:04:19 INFO]: Test stats: {
    "score": -0.9705942042516462,
    "rmse": 0.9705942042516462
}
[08/27/2025 20:04:22 INFO]: Training loss at epoch 20: 0.9369641542434692
[08/27/2025 20:04:24 INFO]: Training loss at epoch 34: 0.7659305930137634
[08/27/2025 20:04:25 INFO]: Training loss at epoch 1: 1.5146396160125732
[08/27/2025 20:04:26 INFO]: Training loss at epoch 30: 1.0155403017997742
[08/27/2025 20:04:28 INFO]: Training loss at epoch 3: 1.038506269454956
[08/27/2025 20:04:29 INFO]: New best epoch, val score: -1.1732736456302755
[08/27/2025 20:04:29 INFO]: Saving model to: maddest-Elbert_trial_68/model_best.pth
[08/27/2025 20:04:30 INFO]: Training stats: {
    "score": -1.0586017912281518,
    "rmse": 1.0586017912281518
}
[08/27/2025 20:04:30 INFO]: Val stats: {
    "score": -1.084415545594235,
    "rmse": 1.084415545594235
}
[08/27/2025 20:04:30 INFO]: Test stats: {
    "score": -1.042516808881202,
    "rmse": 1.042516808881202
}
[08/27/2025 20:04:49 INFO]: Training loss at epoch 3: 0.8418298661708832
[08/27/2025 20:04:50 INFO]: Training loss at epoch 5: 0.9321915209293365
[08/27/2025 20:04:52 INFO]: Training loss at epoch 21: 1.1614660024642944
[08/27/2025 20:04:56 INFO]: Training loss at epoch 2: 1.1220454275608063
[08/27/2025 20:04:57 INFO]: Training loss at epoch 39: 0.929977148771286
[08/27/2025 20:04:57 INFO]: Training loss at epoch 35: 0.8020878434181213
[08/27/2025 20:04:58 INFO]: Training loss at epoch 32: 0.7699810266494751
[08/27/2025 20:04:59 INFO]: Training loss at epoch 31: 0.8957896530628204
[08/27/2025 20:05:00 INFO]: New best epoch, val score: -0.9947103283140568
[08/27/2025 20:05:00 INFO]: Saving model to: maddest-Elbert_trial_68/model_best.pth
[08/27/2025 20:05:02 INFO]: Training loss at epoch 4: 1.272461473941803
[08/27/2025 20:05:03 INFO]: Training loss at epoch 10: 1.189428687095642
[08/27/2025 20:05:04 INFO]: Training loss at epoch 18: 0.9817730784416199
[08/27/2025 20:05:15 INFO]: Training loss at epoch 20: 0.9763199985027313
[08/27/2025 20:05:15 INFO]: Training loss at epoch 10: 1.1116858124732971
[08/27/2025 20:05:21 INFO]: Training loss at epoch 22: 0.9642650783061981
[08/27/2025 20:05:23 INFO]: Training loss at epoch 6: 1.0704630017280579
[08/27/2025 20:05:24 INFO]: Training loss at epoch 4: 0.9028831422328949
[08/27/2025 20:05:28 INFO]: Training loss at epoch 3: 0.96916863322258
[08/27/2025 20:05:31 INFO]: Training loss at epoch 36: 0.9964860677719116
[08/27/2025 20:05:32 INFO]: New best epoch, val score: -0.8872111727135897
[08/27/2025 20:05:32 INFO]: Saving model to: maddest-Elbert_trial_68/model_best.pth
[08/27/2025 20:05:33 INFO]: Training loss at epoch 32: 0.9279484152793884
[08/27/2025 20:05:36 INFO]: Training loss at epoch 5: 1.0951001644134521
[08/27/2025 20:05:37 INFO]: Training loss at epoch 11: 0.9009687900543213
[08/27/2025 20:05:46 INFO]: Training loss at epoch 2: 1.183879166841507
[08/27/2025 20:05:47 INFO]: Training stats: {
    "score": -0.9943274491240937,
    "rmse": 0.9943274491240937
}
[08/27/2025 20:05:47 INFO]: Val stats: {
    "score": -0.9278494948445057,
    "rmse": 0.9278494948445057
}
[08/27/2025 20:05:47 INFO]: Test stats: {
    "score": -0.9714358665223656,
    "rmse": 0.9714358665223656
}
[08/27/2025 20:05:50 INFO]: Training loss at epoch 23: 0.7816793024539948
[08/27/2025 20:05:53 INFO]: Training loss at epoch 33: 0.9304839670658112
[08/27/2025 20:05:57 INFO]: Training loss at epoch 7: 1.1096478700637817
[08/27/2025 20:05:59 INFO]: Training loss at epoch 4: 1.181778907775879
[08/27/2025 20:05:59 INFO]: Training loss at epoch 5: 0.8910926580429077
[08/27/2025 20:06:04 INFO]: New best epoch, val score: -0.8925739448858822
[08/27/2025 20:06:04 INFO]: Saving model to: maddest-Elbert_trial_63/model_best.pth
[08/27/2025 20:06:05 INFO]: Training loss at epoch 37: 1.1271363496780396
[08/27/2025 20:06:05 INFO]: Training loss at epoch 33: 0.8666799664497375
[08/27/2025 20:06:08 INFO]: Training loss at epoch 6: 1.0361221730709076
[08/27/2025 20:06:10 INFO]: Training loss at epoch 12: 0.9726341366767883
[08/27/2025 20:06:10 INFO]: Running Final Evaluation...
[08/27/2025 20:06:11 INFO]: Training loss at epoch 11: 0.9809155464172363
[08/27/2025 20:06:13 INFO]: Training loss at epoch 21: 0.9791708588600159
[08/27/2025 20:06:19 INFO]: Training loss at epoch 24: 1.0577395260334015
[08/27/2025 20:06:21 INFO]: Training accuracy: {
    "score": -1.0060809017997676,
    "rmse": 1.0060809017997676
}
[08/27/2025 20:06:21 INFO]: Val accuracy: {
    "score": -0.8919587790061556,
    "rmse": 0.8919587790061556
}
[08/27/2025 20:06:21 INFO]: Test accuracy: {
    "score": -0.9814254136934666,
    "rmse": 0.9814254136934666
}
[08/27/2025 20:06:21 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_59",
    "best_epoch": 6,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9814254136934666,
        "rmse": 0.9814254136934666
    },
    "train_stats": {
        "score": -1.0060809017997676,
        "rmse": 1.0060809017997676
    },
    "val_stats": {
        "score": -0.8919587790061556,
        "rmse": 0.8919587790061556
    }
}
[08/27/2025 20:06:21 INFO]: Procewss finished for trial maddest-Elbert_trial_59
[08/27/2025 20:06:21 INFO]: 
_________________________________________________

[08/27/2025 20:06:21 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:06:21 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.7479554495650658
  attention_dropout: 0.09987030636453384
  ffn_dropout: 0.09987030636453384
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 5.2316903571124584e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_69

[08/27/2025 20:06:21 INFO]: This ft_transformer has 4.106 million parameters.
[08/27/2025 20:06:21 INFO]: Training will start at epoch 0.
[08/27/2025 20:06:21 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:06:30 INFO]: Training loss at epoch 5: 0.9874556660652161
[08/27/2025 20:06:30 INFO]: Training loss at epoch 8: 0.9633933007717133
[08/27/2025 20:06:34 INFO]: Training loss at epoch 6: 1.25779128074646
[08/27/2025 20:06:39 INFO]: Training loss at epoch 34: 0.9838153719902039
[08/27/2025 20:06:41 INFO]: Training loss at epoch 7: 1.201696753501892
[08/27/2025 20:06:43 INFO]: Training loss at epoch 13: 1.0889204740524292
[08/27/2025 20:06:45 INFO]: Training loss at epoch 54: 0.621746301651001
[08/27/2025 20:06:45 INFO]: New best epoch, val score: -0.89978358072632
[08/27/2025 20:06:45 INFO]: Saving model to: maddest-Elbert_trial_66/model_best.pth
[08/27/2025 20:06:49 INFO]: Training loss at epoch 25: 1.131360650062561
[08/27/2025 20:06:49 INFO]: Training loss at epoch 34: 0.8370779752731323
[08/27/2025 20:07:01 INFO]: Training loss at epoch 6: 1.1767049431800842
[08/27/2025 20:07:04 INFO]: Training loss at epoch 9: 0.9668995440006256
[08/27/2025 20:07:06 INFO]: Training loss at epoch 12: 0.8362578451633453
[08/27/2025 20:07:08 INFO]: Training loss at epoch 7: 0.9444249868392944
[08/27/2025 20:07:10 INFO]: Training loss at epoch 22: 1.2866260409355164
[08/27/2025 20:07:11 INFO]: Training loss at epoch 35: 1.294683426618576
[08/27/2025 20:07:14 INFO]: Training loss at epoch 8: 1.0915007591247559
[08/27/2025 20:07:14 INFO]: Training stats: {
    "score": -1.019974969960447,
    "rmse": 1.019974969960447
}
[08/27/2025 20:07:14 INFO]: Val stats: {
    "score": -1.0218194083394971,
    "rmse": 1.0218194083394971
}
[08/27/2025 20:07:14 INFO]: Test stats: {
    "score": -1.0082664872785139,
    "rmse": 1.0082664872785139
}
[08/27/2025 20:07:15 INFO]: Running Final Evaluation...
[08/27/2025 20:07:16 INFO]: Training loss at epoch 14: 0.9653779864311218
[08/27/2025 20:07:18 INFO]: Training loss at epoch 26: 0.9278774857521057
[08/27/2025 20:07:18 INFO]: New best epoch, val score: -0.8970894772718826
[08/27/2025 20:07:18 INFO]: Saving model to: maddest-Elbert_trial_66/model_best.pth
[08/27/2025 20:07:27 INFO]: Training accuracy: {
    "score": -1.0031961675297167,
    "rmse": 1.0031961675297167
}
[08/27/2025 20:07:27 INFO]: Val accuracy: {
    "score": -0.9163404766869979,
    "rmse": 0.9163404766869979
}
[08/27/2025 20:07:27 INFO]: Test accuracy: {
    "score": -0.9769480387375903,
    "rmse": 0.9769480387375903
}
[08/27/2025 20:07:27 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_60",
    "best_epoch": 4,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9769480387375903,
        "rmse": 0.9769480387375903
    },
    "train_stats": {
        "score": -1.0031961675297167,
        "rmse": 1.0031961675297167
    },
    "val_stats": {
        "score": -0.9163404766869979,
        "rmse": 0.9163404766869979
    }
}
[08/27/2025 20:07:27 INFO]: Procewss finished for trial maddest-Elbert_trial_60
[08/27/2025 20:07:27 INFO]: 
_________________________________________________

[08/27/2025 20:07:27 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:07:27 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.7938243080909448
  attention_dropout: 0.09470226282987482
  ffn_dropout: 0.09470226282987482
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 8.756883243470455e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_70

[08/27/2025 20:07:28 INFO]: This ft_transformer has 4.155 million parameters.
[08/27/2025 20:07:28 INFO]: Training will start at epoch 0.
[08/27/2025 20:07:28 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:07:28 INFO]: Running Final Evaluation...
[08/27/2025 20:07:32 INFO]: Training loss at epoch 7: 1.080712229013443
[08/27/2025 20:07:35 INFO]: Training loss at epoch 14: 0.9592459797859192
[08/27/2025 20:07:36 INFO]: Training loss at epoch 0: 1.077671468257904
[08/27/2025 20:07:44 INFO]: Training loss at epoch 8: 1.028077781200409
[08/27/2025 20:07:45 INFO]: Training loss at epoch 35: 0.9458041787147522
[08/27/2025 20:07:47 INFO]: New best epoch, val score: -0.8896249562721603
[08/27/2025 20:07:47 INFO]: Saving model to: maddest-Elbert_trial_69/model_best.pth
[08/27/2025 20:07:48 INFO]: Training loss at epoch 9: 1.0107406675815582
[08/27/2025 20:07:48 INFO]: Training loss at epoch 27: 1.187831699848175
[08/27/2025 20:07:49 INFO]: Training loss at epoch 10: 1.1104127764701843
[08/27/2025 20:07:50 INFO]: Training loss at epoch 15: 0.8759962618350983
[08/27/2025 20:07:59 INFO]: Training stats: {
    "score": -1.0475653588948997,
    "rmse": 1.0475653588948997
}
[08/27/2025 20:07:59 INFO]: Val stats: {
    "score": -0.8953421505746055,
    "rmse": 0.8953421505746055
}
[08/27/2025 20:07:59 INFO]: Test stats: {
    "score": -1.0308643693424668,
    "rmse": 1.0308643693424668
}
[08/27/2025 20:08:01 INFO]: Training loss at epoch 13: 1.018894374370575
[08/27/2025 20:08:01 INFO]: New best epoch, val score: -0.8737762998751186
[08/27/2025 20:08:01 INFO]: Saving model to: maddest-Elbert_trial_46/model_best.pth
[08/27/2025 20:08:03 INFO]: Training loss at epoch 8: 1.0621317327022552
[08/27/2025 20:08:03 INFO]: New best epoch, val score: -0.8953421505746055
[08/27/2025 20:08:03 INFO]: Saving model to: maddest-Elbert_trial_66/model_best.pth
[08/27/2025 20:08:05 INFO]: Training loss at epoch 30: 1.2929561734199524
[08/27/2025 20:08:09 INFO]: Training loss at epoch 23: 0.8943634629249573
[08/27/2025 20:08:16 INFO]: Training loss at epoch 42: 1.0509361624717712
[08/27/2025 20:08:16 INFO]: Training loss at epoch 3: 0.9867542386054993
[08/27/2025 20:08:17 INFO]: Training loss at epoch 40: 0.7722629904747009
[08/27/2025 20:08:18 INFO]: Training loss at epoch 28: 0.8131747245788574
[08/27/2025 20:08:19 INFO]: Training loss at epoch 9: 0.9278425574302673
[08/27/2025 20:08:22 INFO]: Training loss at epoch 11: 1.0054031014442444
[08/27/2025 20:08:23 INFO]: Training loss at epoch 16: 1.0103854835033417
[08/27/2025 20:08:26 INFO]: New best epoch, val score: -0.891090337352496
[08/27/2025 20:08:26 INFO]: Saving model to: maddest-Elbert_trial_65/model_best.pth
[08/27/2025 20:08:30 INFO]: Training stats: {
    "score": -1.00275942938406,
    "rmse": 1.00275942938406
}
[08/27/2025 20:08:30 INFO]: Val stats: {
    "score": -0.9548091483539847,
    "rmse": 0.9548091483539847
}
[08/27/2025 20:08:30 INFO]: Test stats: {
    "score": -0.9688062432906083,
    "rmse": 0.9688062432906083
}
[08/27/2025 20:08:32 INFO]: Training loss at epoch 10: 1.2888534665107727
[08/27/2025 20:08:34 INFO]: Training loss at epoch 9: 1.0768855810165405
[08/27/2025 20:08:40 INFO]: Training loss at epoch 36: 0.8550701141357422
[08/27/2025 20:08:44 INFO]: Training loss at epoch 0: 0.8405532240867615
[08/27/2025 20:08:45 INFO]: Training stats: {
    "score": -1.0727535608676921,
    "rmse": 1.0727535608676921
}
[08/27/2025 20:08:45 INFO]: Val stats: {
    "score": -0.8875693444942659,
    "rmse": 0.8875693444942659
}
[08/27/2025 20:08:45 INFO]: Test stats: {
    "score": -1.0522289398085398,
    "rmse": 1.0522289398085398
}
[08/27/2025 20:08:47 INFO]: Training loss at epoch 19: 0.872156172990799
[08/27/2025 20:08:47 INFO]: Training loss at epoch 29: 0.9766325354576111
[08/27/2025 20:08:54 INFO]: New best epoch, val score: -0.9478359888138732
[08/27/2025 20:08:54 INFO]: Saving model to: maddest-Elbert_trial_70/model_best.pth
[08/27/2025 20:08:55 INFO]: Training loss at epoch 12: 0.7923288345336914
[08/27/2025 20:08:56 INFO]: Training loss at epoch 17: 0.832884669303894
[08/27/2025 20:08:57 INFO]: Training stats: {
    "score": -0.9942514924096499,
    "rmse": 0.9942514924096499
}
[08/27/2025 20:08:57 INFO]: Val stats: {
    "score": -0.9237472460235889,
    "rmse": 0.9237472460235889
}
[08/27/2025 20:08:57 INFO]: Test stats: {
    "score": -0.9697138153059782,
    "rmse": 0.9697138153059782
}
[08/27/2025 20:08:57 INFO]: Training loss at epoch 14: 0.8125210404396057
[08/27/2025 20:09:00 INFO]: New best epoch, val score: -0.8865121432314821
[08/27/2025 20:09:00 INFO]: Saving model to: maddest-Elbert_trial_65/model_best.pth
[08/27/2025 20:09:03 INFO]: Training loss at epoch 1: 1.0697627067565918
[08/27/2025 20:09:06 INFO]: Training loss at epoch 10: 1.0654875040054321
[08/27/2025 20:09:06 INFO]: Training loss at epoch 11: 1.079817533493042
[08/27/2025 20:09:07 INFO]: Training loss at epoch 24: 1.2065986394882202
[08/27/2025 20:09:15 INFO]: Training loss at epoch 10: 0.961675763130188
[08/27/2025 20:09:19 INFO]: New best epoch, val score: -0.8814455662377565
[08/27/2025 20:09:19 INFO]: Saving model to: maddest-Elbert_trial_68/model_best.pth
[08/27/2025 20:09:24 INFO]: Training loss at epoch 34: 1.0156777799129486
[08/27/2025 20:09:27 INFO]: Training loss at epoch 30: 0.891168475151062
[08/27/2025 20:09:30 INFO]: Training loss at epoch 13: 0.8731440603733063
[08/27/2025 20:09:30 INFO]: Training loss at epoch 18: 1.1360024809837341
[08/27/2025 20:09:33 INFO]: Training accuracy: {
    "score": -0.9673110195242963,
    "rmse": 0.9673110195242963
}
[08/27/2025 20:09:33 INFO]: Val accuracy: {
    "score": -0.8906271789618415,
    "rmse": 0.8906271789618415
}
[08/27/2025 20:09:33 INFO]: Test accuracy: {
    "score": -0.9935822620043068,
    "rmse": 0.9935822620043068
}
[08/27/2025 20:09:33 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_1",
    "best_epoch": 23,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9935822620043068,
        "rmse": 0.9935822620043068
    },
    "train_stats": {
        "score": -0.9673110195242963,
        "rmse": 0.9673110195242963
    },
    "val_stats": {
        "score": -0.8906271789618415,
        "rmse": 0.8906271789618415
    }
}
[08/27/2025 20:09:33 INFO]: Procewss finished for trial maddest-Elbert_trial_1
[08/27/2025 20:09:33 INFO]: 
_________________________________________________

[08/27/2025 20:09:33 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:09:33 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 1.6428043380766377
  attention_dropout: 0.07767374200261362
  ffn_dropout: 0.07767374200261362
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.2734730806039476e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_71

[08/27/2025 20:09:33 INFO]: This ft_transformer has 9.553 million parameters.
[08/27/2025 20:09:33 INFO]: Training will start at epoch 0.
[08/27/2025 20:09:33 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:09:36 INFO]: Training loss at epoch 37: 0.8213415741920471
[08/27/2025 20:09:39 INFO]: Training loss at epoch 12: 0.9114477634429932
[08/27/2025 20:09:41 INFO]: Training loss at epoch 11: 0.9992794394493103
[08/27/2025 20:09:47 INFO]: Training loss at epoch 11: 0.991655558347702
[08/27/2025 20:09:51 INFO]: New best epoch, val score: -0.8789458223800595
[08/27/2025 20:09:51 INFO]: Saving model to: maddest-Elbert_trial_68/model_best.pth
[08/27/2025 20:09:53 INFO]: Training loss at epoch 15: 0.9116478860378265
[08/27/2025 20:09:56 INFO]: Training loss at epoch 31: 0.851232647895813
[08/27/2025 20:10:02 INFO]: Training loss at epoch 14: 0.8230825364589691
[08/27/2025 20:10:03 INFO]: Training stats: {
    "score": -0.9782974029017937,
    "rmse": 0.9782974029017937
}
[08/27/2025 20:10:03 INFO]: Val stats: {
    "score": -0.9626913740647276,
    "rmse": 0.9626913740647276
}
[08/27/2025 20:10:03 INFO]: Test stats: {
    "score": -0.9820788003962675,
    "rmse": 0.9820788003962675
}
[08/27/2025 20:10:03 INFO]: Training loss at epoch 19: 0.9334042370319366
[08/27/2025 20:10:05 INFO]: Training loss at epoch 25: 1.1870728135108948
[08/27/2025 20:10:11 INFO]: Training loss at epoch 1: 1.2902679443359375
[08/27/2025 20:10:12 INFO]: Training loss at epoch 13: 1.032038688659668
[08/27/2025 20:10:15 INFO]: Training stats: {
    "score": -0.9970619692453274,
    "rmse": 0.9970619692453274
}
[08/27/2025 20:10:15 INFO]: Val stats: {
    "score": -0.9170328520698354,
    "rmse": 0.9170328520698354
}
[08/27/2025 20:10:15 INFO]: Test stats: {
    "score": -0.9788613443577224,
    "rmse": 0.9788613443577224
}
[08/27/2025 20:10:16 INFO]: Training loss at epoch 12: 1.0415951013565063
[08/27/2025 20:10:17 INFO]: Training loss at epoch 12: 1.1126642227172852
[08/27/2025 20:10:22 INFO]: New best epoch, val score: -0.9083092703747608
[08/27/2025 20:10:22 INFO]: Saving model to: maddest-Elbert_trial_70/model_best.pth
[08/27/2025 20:10:25 INFO]: Training loss at epoch 32: 0.889487087726593
[08/27/2025 20:10:29 INFO]: Training loss at epoch 2: 1.1570789217948914
[08/27/2025 20:10:31 INFO]: Training loss at epoch 38: 1.0189299285411835
[08/27/2025 20:10:36 INFO]: Training loss at epoch 15: 0.7429301738739014
[08/27/2025 20:10:46 INFO]: Training loss at epoch 14: 1.1662302613258362
[08/27/2025 20:10:46 INFO]: Training loss at epoch 4: 1.0947611629962921
[08/27/2025 20:10:47 INFO]: Training loss at epoch 41: 0.8228894174098969
[08/27/2025 20:10:48 INFO]: Training loss at epoch 13: 1.0968550443649292
[08/27/2025 20:10:49 INFO]: Training loss at epoch 20: 1.0040191113948822
[08/27/2025 20:10:49 INFO]: Training loss at epoch 16: 0.904578447341919
[08/27/2025 20:10:52 INFO]: Training loss at epoch 13: 1.2069693505764008
[08/27/2025 20:10:55 INFO]: Training loss at epoch 33: 1.013637900352478
[08/27/2025 20:11:03 INFO]: Training loss at epoch 26: 0.958467036485672
[08/27/2025 20:11:10 INFO]: Training loss at epoch 16: 0.929824560880661
[08/27/2025 20:11:17 INFO]: Training loss at epoch 15: 0.8976685106754303
[08/27/2025 20:11:19 INFO]: Training loss at epoch 15: 1.1276941299438477
[08/27/2025 20:11:19 INFO]: Training loss at epoch 14: 0.9874540865421295
[08/27/2025 20:11:21 INFO]: Training loss at epoch 21: 0.9374508559703827
[08/27/2025 20:11:24 INFO]: Training loss at epoch 34: 0.9975852072238922
[08/27/2025 20:11:27 INFO]: Training loss at epoch 39: 1.0058948397636414
[08/27/2025 20:11:28 INFO]: Training loss at epoch 14: 1.0023063123226166
[08/27/2025 20:11:39 INFO]: Training loss at epoch 2: 1.1088702082633972
[08/27/2025 20:11:43 INFO]: Training loss at epoch 17: 1.0957994163036346
[08/27/2025 20:11:44 INFO]: Training loss at epoch 17: 0.9424175918102264
[08/27/2025 20:11:46 INFO]: Training stats: {
    "score": -0.9318206769857644,
    "rmse": 0.9318206769857644
}
[08/27/2025 20:11:46 INFO]: Val stats: {
    "score": -0.9293542706860384,
    "rmse": 0.9293542706860384
}
[08/27/2025 20:11:46 INFO]: Test stats: {
    "score": -0.9820096612157254,
    "rmse": 0.9820096612157254
}
[08/27/2025 20:11:49 INFO]: Training loss at epoch 15: 0.9734563529491425
[08/27/2025 20:11:52 INFO]: Training loss at epoch 16: 1.0279015898704529
[08/27/2025 20:11:54 INFO]: Training loss at epoch 35: 1.0436226725578308
[08/27/2025 20:11:55 INFO]: Training loss at epoch 22: 0.9147051572799683
[08/27/2025 20:11:55 INFO]: Training loss at epoch 3: 1.1658708453178406
[08/27/2025 20:12:03 INFO]: Training loss at epoch 27: 1.1062103509902954
[08/27/2025 20:12:03 INFO]: Training loss at epoch 15: 0.7914932668209076
[08/27/2025 20:12:05 INFO]: New best epoch, val score: -0.8858791618753425
[08/27/2025 20:12:05 INFO]: Saving model to: maddest-Elbert_trial_69/model_best.pth
[08/27/2025 20:12:18 INFO]: Training loss at epoch 18: 1.0789761543273926
[08/27/2025 20:12:21 INFO]: Training loss at epoch 16: 1.009818971157074
[08/27/2025 20:12:25 INFO]: Training loss at epoch 36: 1.089923620223999
[08/27/2025 20:12:28 INFO]: Training loss at epoch 17: 1.1262357234954834
[08/27/2025 20:12:30 INFO]: Training loss at epoch 23: 1.2104752361774445
[08/27/2025 20:12:39 INFO]: Training loss at epoch 16: 1.1463537216186523
[08/27/2025 20:12:41 INFO]: Training loss at epoch 18: 0.9426402151584625
[08/27/2025 20:12:42 INFO]: Training loss at epoch 40: 0.8143580257892609
[08/27/2025 20:12:52 INFO]: Training loss at epoch 19: 0.8374338746070862
[08/27/2025 20:12:52 INFO]: Training loss at epoch 17: 1.017731785774231
[08/27/2025 20:12:54 INFO]: Training loss at epoch 37: 0.849114716053009
[08/27/2025 20:12:57 INFO]: Running Final Evaluation...
[08/27/2025 20:13:00 INFO]: Training loss at epoch 18: 0.8739928305149078
[08/27/2025 20:13:01 INFO]: Training loss at epoch 28: 1.0677967071533203
[08/27/2025 20:13:02 INFO]: Training loss at epoch 24: 1.0236187279224396
[08/27/2025 20:13:03 INFO]: Training stats: {
    "score": -0.9812326588441624,
    "rmse": 0.9812326588441624
}
[08/27/2025 20:13:03 INFO]: Val stats: {
    "score": -0.9378927352137001,
    "rmse": 0.9378927352137001
}
[08/27/2025 20:13:03 INFO]: Test stats: {
    "score": -0.9750706794374138,
    "rmse": 0.9750706794374138
}
[08/27/2025 20:13:05 INFO]: Training loss at epoch 3: 1.2620511054992676
[08/27/2025 20:13:07 INFO]: Training accuracy: {
    "score": -1.035012491273957,
    "rmse": 1.035012491273957
}
[08/27/2025 20:13:07 INFO]: Val accuracy: {
    "score": -0.8844067449426172,
    "rmse": 0.8844067449426172
}
[08/27/2025 20:13:07 INFO]: Test accuracy: {
    "score": -1.0063307395916898,
    "rmse": 1.0063307395916898
}
[08/27/2025 20:13:07 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_61",
    "best_epoch": 6,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0063307395916898,
        "rmse": 1.0063307395916898
    },
    "train_stats": {
        "score": -1.035012491273957,
        "rmse": 1.035012491273957
    },
    "val_stats": {
        "score": -0.8844067449426172,
        "rmse": 0.8844067449426172
    }
}
[08/27/2025 20:13:07 INFO]: Procewss finished for trial maddest-Elbert_trial_61
[08/27/2025 20:13:07 INFO]: 
_________________________________________________

[08/27/2025 20:13:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:13:07 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 1.7710062804050626
  attention_dropout: 0.09686978038201369
  ffn_dropout: 0.09686978038201369
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.2370812436176864e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_72

[08/27/2025 20:13:08 INFO]: This ft_transformer has 9.953 million parameters.
[08/27/2025 20:13:08 INFO]: Training will start at epoch 0.
[08/27/2025 20:13:08 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:13:09 INFO]: Training loss at epoch 0: 1.5267691016197205
[08/27/2025 20:13:15 INFO]: Training loss at epoch 17: 1.006137728691101
[08/27/2025 20:13:16 INFO]: New best epoch, val score: -0.8895100851435446
[08/27/2025 20:13:16 INFO]: Saving model to: maddest-Elbert_trial_70/model_best.pth
[08/27/2025 20:13:17 INFO]: Training loss at epoch 5: 1.0841869115829468
[08/27/2025 20:13:17 INFO]: Training loss at epoch 42: 0.8160183727741241
[08/27/2025 20:13:22 INFO]: Training loss at epoch 4: 1.0284982919692993
[08/27/2025 20:13:23 INFO]: Training loss at epoch 18: 0.9018923342227936
[08/27/2025 20:13:33 INFO]: Training loss at epoch 19: 1.0464100241661072
[08/27/2025 20:13:36 INFO]: Training loss at epoch 25: 0.9559729397296906
[08/27/2025 20:13:36 INFO]: Training loss at epoch 20: 0.9475394189357758
[08/27/2025 20:13:37 INFO]: Training loss at epoch 19: 1.1435110569000244
[08/27/2025 20:13:37 INFO]: Training loss at epoch 41: 0.7938133478164673
[08/27/2025 20:13:38 INFO]: New best epoch, val score: -0.8611116039906399
[08/27/2025 20:13:38 INFO]: Saving model to: maddest-Elbert_trial_71/model_best.pth
[08/27/2025 20:13:45 INFO]: Training stats: {
    "score": -1.0102146163385928,
    "rmse": 1.0102146163385928
}
[08/27/2025 20:13:45 INFO]: Val stats: {
    "score": -0.9959198461248557,
    "rmse": 0.9959198461248557
}
[08/27/2025 20:13:45 INFO]: Test stats: {
    "score": -0.9887563886469563,
    "rmse": 0.9887563886469563
}
[08/27/2025 20:13:46 INFO]: Training loss at epoch 20: 1.088561236858368
[08/27/2025 20:13:49 INFO]: Training loss at epoch 18: 1.162559986114502
[08/27/2025 20:13:53 INFO]: Training loss at epoch 19: 1.1068289279937744
[08/27/2025 20:13:55 INFO]: Training stats: {
    "score": -0.9968169964316264,
    "rmse": 0.9968169964316264
}
[08/27/2025 20:13:55 INFO]: Val stats: {
    "score": -0.9033900508344367,
    "rmse": 0.9033900508344367
}
[08/27/2025 20:13:55 INFO]: Test stats: {
    "score": -0.9665007063397572,
    "rmse": 0.9665007063397572
}
[08/27/2025 20:14:00 INFO]: Training loss at epoch 29: 0.9016120135784149
[08/27/2025 20:14:05 INFO]: Training stats: {
    "score": -0.9982026496132002,
    "rmse": 0.9982026496132002
}
[08/27/2025 20:14:05 INFO]: Val stats: {
    "score": -0.9252023634895332,
    "rmse": 0.9252023634895332
}
[08/27/2025 20:14:05 INFO]: Test stats: {
    "score": -0.9763322210702333,
    "rmse": 0.9763322210702333
}
[08/27/2025 20:14:10 INFO]: Training loss at epoch 26: 1.0539670586585999
[08/27/2025 20:14:10 INFO]: Training loss at epoch 21: 0.9527300596237183
[08/27/2025 20:14:15 INFO]: Training loss at epoch 28: 0.9264103174209595
[08/27/2025 20:14:18 INFO]: Training loss at epoch 20: 0.8166947066783905
[08/27/2025 20:14:19 INFO]: Training stats: {
    "score": -0.9885865736767754,
    "rmse": 0.9885865736767754
}
[08/27/2025 20:14:19 INFO]: Val stats: {
    "score": -0.94063235040221,
    "rmse": 0.94063235040221
}
[08/27/2025 20:14:19 INFO]: Test stats: {
    "score": -0.9692661952756263,
    "rmse": 0.9692661952756263
}
[08/27/2025 20:14:24 INFO]: Training loss at epoch 19: 1.1281906068325043
[08/27/2025 20:14:32 INFO]: Training loss at epoch 42: 1.013894408941269
[08/27/2025 20:14:33 INFO]: Training loss at epoch 4: 0.9399906992912292
[08/27/2025 20:14:35 INFO]: Training loss at epoch 20: 1.0210468769073486
[08/27/2025 20:14:36 INFO]: Training stats: {
    "score": -0.998227018341174,
    "rmse": 0.998227018341174
}
[08/27/2025 20:14:36 INFO]: Val stats: {
    "score": -0.9406019504597382,
    "rmse": 0.9406019504597382
}
[08/27/2025 20:14:36 INFO]: Test stats: {
    "score": -0.9658482720533507,
    "rmse": 0.9658482720533507
}
[08/27/2025 20:14:39 INFO]: Running Final Evaluation...
[08/27/2025 20:14:43 INFO]: Training loss at epoch 27: 0.8740575015544891
[08/27/2025 20:14:43 INFO]: Training loss at epoch 22: 0.9099225699901581
[08/27/2025 20:14:47 INFO]: Training loss at epoch 5: 1.0529404878616333
[08/27/2025 20:14:51 INFO]: Training loss at epoch 20: 0.8557720482349396
[08/27/2025 20:14:51 INFO]: Training loss at epoch 21: 1.1928951144218445
[08/27/2025 20:14:58 INFO]: Training accuracy: {
    "score": -1.0109730747842165,
    "rmse": 1.0109730747842165
}
[08/27/2025 20:14:58 INFO]: Val accuracy: {
    "score": -0.883742134455171,
    "rmse": 0.883742134455171
}
[08/27/2025 20:14:58 INFO]: Test accuracy: {
    "score": -1.0114132047689133,
    "rmse": 1.0114132047689133
}
[08/27/2025 20:14:58 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_54",
    "best_epoch": 11,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0114132047689133,
        "rmse": 1.0114132047689133
    },
    "train_stats": {
        "score": -1.0109730747842165,
        "rmse": 1.0109730747842165
    },
    "val_stats": {
        "score": -0.883742134455171,
        "rmse": 0.883742134455171
    }
}
[08/27/2025 20:14:58 INFO]: Procewss finished for trial maddest-Elbert_trial_54
[08/27/2025 20:14:58 INFO]: 
_________________________________________________

[08/27/2025 20:14:58 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:14:58 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 1.608146845744697
  attention_dropout: 0.09720263904090742
  ffn_dropout: 0.09720263904090742
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.9779564540377776e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_73

[08/27/2025 20:14:58 INFO]: This ft_transformer has 7.754 million parameters.
[08/27/2025 20:14:58 INFO]: Training will start at epoch 0.
[08/27/2025 20:14:58 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:14:59 INFO]: Training loss at epoch 16: 0.8146758675575256
[08/27/2025 20:15:06 INFO]: Training loss at epoch 21: 1.067230463027954
[08/27/2025 20:15:11 INFO]: Training loss at epoch 20: 0.9127679467201233
[08/27/2025 20:15:16 INFO]: Training loss at epoch 28: 1.0602360963821411
[08/27/2025 20:15:17 INFO]: Training loss at epoch 23: 0.8940597176551819
[08/27/2025 20:15:19 INFO]: Training loss at epoch 30: 0.8941965699195862
[08/27/2025 20:15:25 INFO]: Training loss at epoch 22: 0.8524800539016724
[08/27/2025 20:15:36 INFO]: Training loss at epoch 22: 1.081739604473114
[08/27/2025 20:15:46 INFO]: Training loss at epoch 6: 1.095140814781189
[08/27/2025 20:15:47 INFO]: Training loss at epoch 21: 1.0279769003391266
[08/27/2025 20:15:47 INFO]: Training loss at epoch 43: 0.9959980845451355
[08/27/2025 20:15:49 INFO]: Training loss at epoch 43: 1.13205948472023
[08/27/2025 20:15:50 INFO]: Training loss at epoch 21: 0.9020056128501892
[08/27/2025 20:15:51 INFO]: Training loss at epoch 24: 1.112489640712738
[08/27/2025 20:15:52 INFO]: Training loss at epoch 29: 1.3571773767471313
[08/27/2025 20:15:57 INFO]: Training loss at epoch 23: 1.1734706163406372
[08/27/2025 20:15:58 INFO]: Training loss at epoch 5: 1.418389916419983
[08/27/2025 20:16:03 INFO]: Training stats: {
    "score": -0.9963902431593059,
    "rmse": 0.9963902431593059
}
[08/27/2025 20:16:03 INFO]: Val stats: {
    "score": -0.9450835289340216,
    "rmse": 0.9450835289340216
}
[08/27/2025 20:16:03 INFO]: Test stats: {
    "score": -0.9797167953125344,
    "rmse": 0.9797167953125344
}
[08/27/2025 20:16:07 INFO]: Training loss at epoch 23: 0.9515501260757446
[08/27/2025 20:16:14 INFO]: Training loss at epoch 6: 0.978508472442627
[08/27/2025 20:16:18 INFO]: Training loss at epoch 31: 0.9343617856502533
[08/27/2025 20:16:24 INFO]: Training loss at epoch 25: 0.996992826461792
[08/27/2025 20:16:24 INFO]: Training loss at epoch 22: 0.9255399405956268
[08/27/2025 20:16:31 INFO]: Training loss at epoch 24: 1.2431102097034454
[08/27/2025 20:16:38 INFO]: Training loss at epoch 31: 0.9921028017997742
[08/27/2025 20:16:38 INFO]: Training loss at epoch 30: 0.8437594473361969
[08/27/2025 20:16:38 INFO]: Training loss at epoch 24: 1.1321951150894165
[08/27/2025 20:16:42 INFO]: Training loss at epoch 22: 0.9573296308517456
[08/27/2025 20:16:48 INFO]: Training loss at epoch 35: 1.2675749063491821
[08/27/2025 20:16:58 INFO]: Training loss at epoch 26: 1.0794785022735596
[08/27/2025 20:17:00 INFO]: Training loss at epoch 23: 1.1840781271457672
[08/27/2025 20:17:01 INFO]: Training loss at epoch 0: 1.1390126943588257
[08/27/2025 20:17:04 INFO]: Training loss at epoch 25: 1.052785575389862
[08/27/2025 20:17:09 INFO]: Training loss at epoch 25: 1.0896745324134827
[08/27/2025 20:17:11 INFO]: Training loss at epoch 31: 1.0477387309074402
[08/27/2025 20:17:14 INFO]: Training loss at epoch 1: 0.8985949754714966
[08/27/2025 20:17:16 INFO]: Training loss at epoch 32: 1.0160841643810272
[08/27/2025 20:17:26 INFO]: Training loss at epoch 6: 1.502844750881195
[08/27/2025 20:17:29 INFO]: Training loss at epoch 21: 0.9783784449100494
[08/27/2025 20:17:31 INFO]: Training loss at epoch 27: 0.8329283893108368
[08/27/2025 20:17:32 INFO]: New best epoch, val score: -1.0075127348352364
[08/27/2025 20:17:32 INFO]: Saving model to: maddest-Elbert_trial_72/model_best.pth
[08/27/2025 20:17:35 INFO]: Training loss at epoch 24: 0.9258530139923096
[08/27/2025 20:17:37 INFO]: Training loss at epoch 23: 1.0181593894958496
[08/27/2025 20:17:38 INFO]: Training loss at epoch 26: 1.0204261541366577
[08/27/2025 20:17:39 INFO]: Training loss at epoch 7: 1.3761019110679626
[08/27/2025 20:17:41 INFO]: Training loss at epoch 26: 0.9635222256183624
[08/27/2025 20:17:44 INFO]: Training loss at epoch 32: 1.0516166388988495
[08/27/2025 20:18:01 INFO]: Training loss at epoch 0: 0.8703346848487854
[08/27/2025 20:18:04 INFO]: Training loss at epoch 28: 0.7044518887996674
[08/27/2025 20:18:09 INFO]: Training loss at epoch 25: 1.162065327167511
[08/27/2025 20:18:10 INFO]: Training loss at epoch 27: 0.9483376443386078
[08/27/2025 20:18:11 INFO]: Training loss at epoch 27: 0.9813031554222107
[08/27/2025 20:18:15 INFO]: Training loss at epoch 33: 1.022293359041214
[08/27/2025 20:18:15 INFO]: Training loss at epoch 7: 1.08982914686203
[08/27/2025 20:18:16 INFO]: Training loss at epoch 44: 1.085044652223587
[08/27/2025 20:18:17 INFO]: Training loss at epoch 33: 1.0015230774879456
[08/27/2025 20:18:21 INFO]: Running Final Evaluation...
[08/27/2025 20:18:25 INFO]: New best epoch, val score: -0.9332010792512586
[08/27/2025 20:18:25 INFO]: Saving model to: maddest-Elbert_trial_73/model_best.pth
[08/27/2025 20:18:32 INFO]: Training loss at epoch 24: 0.9118413925170898
[08/27/2025 20:18:33 INFO]: Training accuracy: {
    "score": -1.0764762110297705,
    "rmse": 1.0764762110297705
}
[08/27/2025 20:18:33 INFO]: Val accuracy: {
    "score": -0.9003464286538148,
    "rmse": 0.9003464286538148
}
[08/27/2025 20:18:33 INFO]: Test accuracy: {
    "score": -1.0580600814630254,
    "rmse": 1.0580600814630254
}
[08/27/2025 20:18:33 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_64",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0580600814630254,
        "rmse": 1.0580600814630254
    },
    "train_stats": {
        "score": -1.0764762110297705,
        "rmse": 1.0764762110297705
    },
    "val_stats": {
        "score": -0.9003464286538148,
        "rmse": 0.9003464286538148
    }
}
[08/27/2025 20:18:33 INFO]: Procewss finished for trial maddest-Elbert_trial_64
[08/27/2025 20:18:33 INFO]: 
_________________________________________________

[08/27/2025 20:18:33 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:18:33 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 1.4076751560396208
  attention_dropout: 0.07526862703765852
  ffn_dropout: 0.07526862703765852
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.276320150992345e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_74

[08/27/2025 20:18:33 INFO]: This ft_transformer has 7.332 million parameters.
[08/27/2025 20:18:33 INFO]: Training will start at epoch 0.
[08/27/2025 20:18:33 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:18:38 INFO]: Training loss at epoch 29: 1.1682702004909515
[08/27/2025 20:18:38 INFO]: Training loss at epoch 17: 0.987315833568573
[08/27/2025 20:18:43 INFO]: Training loss at epoch 28: 1.2228332161903381
[08/27/2025 20:18:44 INFO]: Training loss at epoch 28: 0.9217786490917206
[08/27/2025 20:18:44 INFO]: Training loss at epoch 26: 0.9435043036937714
[08/27/2025 20:18:49 INFO]: Training stats: {
    "score": -0.9681836474127611,
    "rmse": 0.9681836474127611
}
[08/27/2025 20:18:49 INFO]: Val stats: {
    "score": -0.9415726266538322,
    "rmse": 0.9415726266538322
}
[08/27/2025 20:18:49 INFO]: Test stats: {
    "score": -0.973362013285776,
    "rmse": 0.973362013285776
}
[08/27/2025 20:18:53 INFO]: Training loss at epoch 7: 1.2346641421318054
[08/27/2025 20:19:03 INFO]: Training loss at epoch 8: 0.8554555475711823
[08/27/2025 20:19:15 INFO]: Training loss at epoch 34: 0.9123812913894653
[08/27/2025 20:19:15 INFO]: Training loss at epoch 29: 1.1613373160362244
[08/27/2025 20:19:19 INFO]: Training loss at epoch 29: 1.170631617307663
[08/27/2025 20:19:20 INFO]: Training loss at epoch 27: 1.2531298100948334
[08/27/2025 20:19:23 INFO]: Training loss at epoch 30: 0.7147526144981384
[08/27/2025 20:19:25 INFO]: Training stats: {
    "score": -0.9988286921089775,
    "rmse": 0.9988286921089775
}
[08/27/2025 20:19:25 INFO]: Val stats: {
    "score": -0.9059912069604168,
    "rmse": 0.9059912069604168
}
[08/27/2025 20:19:25 INFO]: Test stats: {
    "score": -0.9773257518957053,
    "rmse": 0.9773257518957053
}
[08/27/2025 20:19:27 INFO]: Training loss at epoch 25: 1.2098143100738525
[08/27/2025 20:19:30 INFO]: Training stats: {
    "score": -0.996922639375739,
    "rmse": 0.996922639375739
}
[08/27/2025 20:19:30 INFO]: Val stats: {
    "score": -0.9252589250908579,
    "rmse": 0.9252589250908579
}
[08/27/2025 20:19:30 INFO]: Test stats: {
    "score": -0.9786917255606581,
    "rmse": 0.9786917255606581
}
[08/27/2025 20:19:54 INFO]: Training loss at epoch 28: 1.0505344569683075
[08/27/2025 20:19:56 INFO]: Training loss at epoch 31: 1.134191244840622
[08/27/2025 20:19:56 INFO]: Training loss at epoch 30: 1.0170516967773438
[08/27/2025 20:20:04 INFO]: Training loss at epoch 30: 0.9869948923587799
[08/27/2025 20:20:13 INFO]: Training loss at epoch 35: 0.9915226101875305
[08/27/2025 20:20:19 INFO]: Training loss at epoch 8: 0.9370307624340057
[08/27/2025 20:20:22 INFO]: Training loss at epoch 26: 1.3535407781600952
[08/27/2025 20:20:27 INFO]: Training loss at epoch 31: 1.005597561597824
[08/27/2025 20:20:29 INFO]: Training loss at epoch 32: 0.7494202554225922
[08/27/2025 20:20:29 INFO]: Training loss at epoch 29: 0.8064534664154053
[08/27/2025 20:20:30 INFO]: Training loss at epoch 9: 1.0283874571323395
[08/27/2025 20:20:30 INFO]: New best epoch, val score: -0.8877618863234947
[08/27/2025 20:20:30 INFO]: Saving model to: maddest-Elbert_trial_70/model_best.pth
[08/27/2025 20:20:37 INFO]: Training loss at epoch 31: 0.96317258477211
[08/27/2025 20:20:41 INFO]: Training stats: {
    "score": -0.9996451226500391,
    "rmse": 0.9996451226500391
}
[08/27/2025 20:20:41 INFO]: Val stats: {
    "score": -0.9564938592319597,
    "rmse": 0.9564938592319597
}
[08/27/2025 20:20:41 INFO]: Test stats: {
    "score": -0.9691215297616181,
    "rmse": 0.9691215297616181
}
[08/27/2025 20:20:45 INFO]: Training loss at epoch 8: 0.8621816635131836
[08/27/2025 20:20:45 INFO]: Training loss at epoch 45: 1.0526983737945557
[08/27/2025 20:20:58 INFO]: Training loss at epoch 32: 1.2426866590976715
[08/27/2025 20:20:59 INFO]: Training stats: {
    "score": -1.0114552236327132,
    "rmse": 1.0114552236327132
}
[08/27/2025 20:20:59 INFO]: Val stats: {
    "score": -0.9902653342294316,
    "rmse": 0.9902653342294316
}
[08/27/2025 20:20:59 INFO]: Test stats: {
    "score": -0.9867198791255609,
    "rmse": 0.9867198791255609
}
[08/27/2025 20:21:03 INFO]: Training loss at epoch 33: 1.1892141699790955
[08/27/2025 20:21:10 INFO]: Training loss at epoch 32: 0.9346500933170319
[08/27/2025 20:21:11 INFO]: Training loss at epoch 36: 0.9266830682754517
[08/27/2025 20:21:12 INFO]: Training loss at epoch 22: 1.0694931149482727
[08/27/2025 20:21:15 INFO]: Training loss at epoch 30: 0.9818742871284485
[08/27/2025 20:21:18 INFO]: Training loss at epoch 27: 0.8972174227237701
[08/27/2025 20:21:20 INFO]: Training loss at epoch 2: 1.2039208710193634
[08/27/2025 20:21:25 INFO]: Training loss at epoch 1: 1.0638612508773804
[08/27/2025 20:21:28 INFO]: Training loss at epoch 0: 1.2110189497470856
[08/27/2025 20:21:28 INFO]: Training loss at epoch 1: 1.397318571805954
[08/27/2025 20:21:28 INFO]: Training loss at epoch 33: 0.8980644643306732
[08/27/2025 20:21:35 INFO]: Training loss at epoch 34: 0.9165017902851105
[08/27/2025 20:21:43 INFO]: Training loss at epoch 33: 0.8343216478824615
[08/27/2025 20:21:46 INFO]: Training loss at epoch 9: 1.0273402333259583
[08/27/2025 20:21:51 INFO]: Training loss at epoch 31: 0.9390326142311096
[08/27/2025 20:21:52 INFO]: New best epoch, val score: -0.9306812715006114
[08/27/2025 20:21:52 INFO]: Saving model to: maddest-Elbert_trial_74/model_best.pth
[08/27/2025 20:21:53 INFO]: New best epoch, val score: -0.894707736305578
[08/27/2025 20:21:53 INFO]: Saving model to: maddest-Elbert_trial_73/model_best.pth
[08/27/2025 20:21:57 INFO]: New best epoch, val score: -0.912236840986961
[08/27/2025 20:21:57 INFO]: Saving model to: maddest-Elbert_trial_72/model_best.pth
[08/27/2025 20:22:00 INFO]: Training loss at epoch 34: 0.9628654420375824
[08/27/2025 20:22:08 INFO]: Training loss at epoch 35: 0.8614723682403564
[08/27/2025 20:22:08 INFO]: Training loss at epoch 37: 1.0750115513801575
[08/27/2025 20:22:13 INFO]: Training loss at epoch 28: 0.7842285335063934
[08/27/2025 20:22:15 INFO]: Training stats: {
    "score": -1.0426808133408347,
    "rmse": 1.0426808133408347
}
[08/27/2025 20:22:15 INFO]: Val stats: {
    "score": -0.8839526174593102,
    "rmse": 0.8839526174593102
}
[08/27/2025 20:22:15 INFO]: Test stats: {
    "score": -1.0205981455609197,
    "rmse": 1.0205981455609197
}
[08/27/2025 20:22:16 INFO]: Training loss at epoch 34: 0.972459614276886
[08/27/2025 20:22:18 INFO]: Training loss at epoch 18: 1.05226331949234
[08/27/2025 20:22:24 INFO]: Training loss at epoch 10: 0.9744287729263306
[08/27/2025 20:22:25 INFO]: Training loss at epoch 32: 1.1914618611335754
[08/27/2025 20:22:26 INFO]: New best epoch, val score: -0.8839526174593102
[08/27/2025 20:22:26 INFO]: Saving model to: maddest-Elbert_trial_70/model_best.pth
[08/27/2025 20:22:30 INFO]: Training loss at epoch 35: 1.0428854823112488
[08/27/2025 20:22:41 INFO]: Training loss at epoch 36: 0.899267166852951
[08/27/2025 20:22:49 INFO]: Training loss at epoch 35: 1.2695780396461487
[08/27/2025 20:23:00 INFO]: Training loss at epoch 33: 0.77662593126297
[08/27/2025 20:23:00 INFO]: Training loss at epoch 36: 1.0421525537967682
[08/27/2025 20:23:04 INFO]: Running Final Evaluation...
[08/27/2025 20:23:06 INFO]: Training loss at epoch 38: 0.9888678789138794
[08/27/2025 20:23:08 INFO]: Training loss at epoch 29: 0.8136176466941833
[08/27/2025 20:23:14 INFO]: Training loss at epoch 9: 0.9319917857646942
[08/27/2025 20:23:14 INFO]: Training loss at epoch 46: 1.0796060264110565
[08/27/2025 20:23:15 INFO]: Training loss at epoch 37: 0.9084538817405701
[08/27/2025 20:23:16 INFO]: Training accuracy: {
    "score": -1.0057975076496082,
    "rmse": 1.0057975076496082
}
[08/27/2025 20:23:16 INFO]: Val accuracy: {
    "score": -0.9063921285956277,
    "rmse": 0.9063921285956277
}
[08/27/2025 20:23:16 INFO]: Test accuracy: {
    "score": -0.9686081478490371,
    "rmse": 0.9686081478490371
}
[08/27/2025 20:23:16 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_67",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9686081478490371,
        "rmse": 0.9686081478490371
    },
    "train_stats": {
        "score": -1.0057975076496082,
        "rmse": 1.0057975076496082
    },
    "val_stats": {
        "score": -0.9063921285956277,
        "rmse": 0.9063921285956277
    }
}
[08/27/2025 20:23:16 INFO]: Procewss finished for trial maddest-Elbert_trial_67
[08/27/2025 20:23:16 INFO]: 
_________________________________________________

[08/27/2025 20:23:16 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:23:16 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 1.015154322709111
  attention_dropout: 0.10616574038673313
  ffn_dropout: 0.10616574038673313
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.25493116339064e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_75

[08/27/2025 20:23:16 INFO]: This ft_transformer has 6.508 million parameters.
[08/27/2025 20:23:16 INFO]: Training will start at epoch 0.
[08/27/2025 20:23:16 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:23:21 INFO]: Training loss at epoch 44: 1.0251852869987488
[08/27/2025 20:23:22 INFO]: Training loss at epoch 36: 1.0896024405956268
[08/27/2025 20:23:26 INFO]: Training stats: {
    "score": -0.9910785878928906,
    "rmse": 0.9910785878928906
}
[08/27/2025 20:23:26 INFO]: Val stats: {
    "score": -0.930415609115698,
    "rmse": 0.930415609115698
}
[08/27/2025 20:23:26 INFO]: Test stats: {
    "score": -0.9646794315557669,
    "rmse": 0.9646794315557669
}
[08/27/2025 20:23:30 INFO]: Training loss at epoch 37: 1.0958213210105896
[08/27/2025 20:23:31 INFO]: Running Final Evaluation...
[08/27/2025 20:23:42 INFO]: Training loss at epoch 10: 1.1976138949394226
[08/27/2025 20:23:48 INFO]: Training loss at epoch 38: 0.7717625200748444
[08/27/2025 20:23:48 INFO]: Training loss at epoch 11: 0.9430104792118073
[08/27/2025 20:23:54 INFO]: Training loss at epoch 37: 1.0737927556037903
[08/27/2025 20:24:01 INFO]: Training loss at epoch 38: 1.0543802976608276
[08/27/2025 20:24:04 INFO]: Training stats: {
    "score": -0.9941444910409994,
    "rmse": 0.9941444910409994
}
[08/27/2025 20:24:04 INFO]: Val stats: {
    "score": -0.9314588732335453,
    "rmse": 0.9314588732335453
}
[08/27/2025 20:24:04 INFO]: Test stats: {
    "score": -0.9864814986430637,
    "rmse": 0.9864814986430637
}
[08/27/2025 20:24:04 INFO]: Training loss at epoch 39: 0.9583446085453033
[08/27/2025 20:24:13 INFO]: Training loss at epoch 36: 1.068348616361618
[08/27/2025 20:24:17 INFO]: Running Final Evaluation...
[08/27/2025 20:24:21 INFO]: Training loss at epoch 39: 1.2262232601642609
[08/27/2025 20:24:21 INFO]: Training loss at epoch 30: 1.0430723428726196
[08/27/2025 20:24:23 INFO]: Training stats: {
    "score": -0.9856013541252963,
    "rmse": 0.9856013541252963
}
[08/27/2025 20:24:23 INFO]: Val stats: {
    "score": -0.9451183585226375,
    "rmse": 0.9451183585226375
}
[08/27/2025 20:24:23 INFO]: Test stats: {
    "score": -0.9690186761709485,
    "rmse": 0.9690186761709485
}
[08/27/2025 20:24:23 INFO]: Training accuracy: {
    "score": -1.0393987893332206,
    "rmse": 1.0393987893332206
}
[08/27/2025 20:24:23 INFO]: Val accuracy: {
    "score": -0.8817473366682378,
    "rmse": 0.8817473366682378
}
[08/27/2025 20:24:23 INFO]: Test accuracy: {
    "score": -1.0124087733214318,
    "rmse": 1.0124087733214318
}
[08/27/2025 20:24:23 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_42",
    "best_epoch": 15,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0124087733214318,
        "rmse": 1.0124087733214318
    },
    "train_stats": {
        "score": -1.0393987893332206,
        "rmse": 1.0393987893332206
    },
    "val_stats": {
        "score": -0.8817473366682378,
        "rmse": 0.8817473366682378
    }
}
[08/27/2025 20:24:23 INFO]: Procewss finished for trial maddest-Elbert_trial_42
[08/27/2025 20:24:23 INFO]: 
_________________________________________________

[08/27/2025 20:24:23 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:24:23 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 1.029743704226295
  attention_dropout: 0.11555596385075559
  ffn_dropout: 0.11555596385075559
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.2078498217417288e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_76

[08/27/2025 20:24:24 INFO]: This ft_transformer has 6.540 million parameters.
[08/27/2025 20:24:24 INFO]: Training will start at epoch 0.
[08/27/2025 20:24:24 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:24:26 INFO]: Training loss at epoch 38: 0.8822937309741974
[08/27/2025 20:24:31 INFO]: Training loss at epoch 39: 1.0122052431106567
[08/27/2025 20:24:31 INFO]: Training stats: {
    "score": -0.9530499601079864,
    "rmse": 0.9530499601079864
}
[08/27/2025 20:24:31 INFO]: Val stats: {
    "score": -0.9081306072084431,
    "rmse": 0.9081306072084431
}
[08/27/2025 20:24:31 INFO]: Test stats: {
    "score": -0.9778339210472419,
    "rmse": 0.9778339210472419
}
[08/27/2025 20:24:41 INFO]: Training stats: {
    "score": -1.001804752821566,
    "rmse": 1.001804752821566
}
[08/27/2025 20:24:41 INFO]: Val stats: {
    "score": -0.8958717608100002,
    "rmse": 0.8958717608100002
}
[08/27/2025 20:24:41 INFO]: Test stats: {
    "score": -0.9807788909498323,
    "rmse": 0.9807788909498323
}
[08/27/2025 20:24:41 INFO]: Training loss at epoch 1: 1.0282231569290161
[08/27/2025 20:24:50 INFO]: Training loss at epoch 23: 1.036841094493866
[08/27/2025 20:24:50 INFO]: Training loss at epoch 2: 1.2267602682113647
[08/27/2025 20:24:57 INFO]: Training loss at epoch 39: 0.8989056944847107
[08/27/2025 20:25:03 INFO]: Training loss at epoch 32: 1.0010179281234741
[08/27/2025 20:25:04 INFO]: Training loss at epoch 40: 0.9840967655181885
[08/27/2025 20:25:05 INFO]: New best epoch, val score: -0.8966748785220254
[08/27/2025 20:25:05 INFO]: Saving model to: maddest-Elbert_trial_74/model_best.pth
[08/27/2025 20:25:05 INFO]: Training loss at epoch 11: 1.0917753279209137
[08/27/2025 20:25:08 INFO]: Training stats: {
    "score": -0.9964295398198643,
    "rmse": 0.9964295398198643
}
[08/27/2025 20:25:08 INFO]: Val stats: {
    "score": -0.9181957286103919,
    "rmse": 0.9181957286103919
}
[08/27/2025 20:25:08 INFO]: Test stats: {
    "score": -0.9802036696162321,
    "rmse": 0.9802036696162321
}
[08/27/2025 20:25:10 INFO]: Training loss at epoch 12: 1.1203590035438538
[08/27/2025 20:25:11 INFO]: Training loss at epoch 40: 1.016938328742981
[08/27/2025 20:25:13 INFO]: Training loss at epoch 31: 1.0629996061325073
[08/27/2025 20:25:18 INFO]: Training loss at epoch 40: 0.8992020487785339
[08/27/2025 20:25:20 INFO]: Training loss at epoch 3: 1.3376688361167908
[08/27/2025 20:25:38 INFO]: Training loss at epoch 41: 1.0290855467319489
[08/27/2025 20:25:40 INFO]: Training loss at epoch 29: 0.99146968126297
[08/27/2025 20:25:41 INFO]: Training loss at epoch 40: 1.0541446208953857
[08/27/2025 20:25:43 INFO]: Training loss at epoch 41: 1.0365949869155884
[08/27/2025 20:25:46 INFO]: Running Final Evaluation...
[08/27/2025 20:25:46 INFO]: Training loss at epoch 2: 1.1582394540309906
[08/27/2025 20:25:53 INFO]: Training loss at epoch 0: 1.716880202293396
[08/27/2025 20:25:54 INFO]: Training loss at epoch 19: 0.8472298979759216
[08/27/2025 20:25:57 INFO]: Training accuracy: {
    "score": -1.0475653584521638,
    "rmse": 1.0475653584521638
}
[08/27/2025 20:25:57 INFO]: Val accuracy: {
    "score": -0.8953421505746055,
    "rmse": 0.8953421505746055
}
[08/27/2025 20:25:57 INFO]: Test accuracy: {
    "score": -1.0308643693424668,
    "rmse": 1.0308643693424668
}
[08/27/2025 20:25:57 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_66",
    "best_epoch": 9,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0308643693424668,
        "rmse": 1.0308643693424668
    },
    "train_stats": {
        "score": -1.0475653584521638,
        "rmse": 1.0475653584521638
    },
    "val_stats": {
        "score": -0.8953421505746055,
        "rmse": 0.8953421505746055
    }
}
[08/27/2025 20:25:57 INFO]: Procewss finished for trial maddest-Elbert_trial_66
[08/27/2025 20:25:57 INFO]: 
_________________________________________________

[08/27/2025 20:25:57 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:25:57 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 1.7537951202523232
  attention_dropout: 0.11452093946740748
  ffn_dropout: 0.11452093946740748
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.2181138003985775e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_77

[08/27/2025 20:25:57 INFO]: This ft_transformer has 8.057 million parameters.
[08/27/2025 20:25:57 INFO]: Training will start at epoch 0.
[08/27/2025 20:25:57 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:26:10 INFO]: Training loss at epoch 32: 0.9658989310264587
[08/27/2025 20:26:12 INFO]: Training loss at epoch 42: 0.9293123185634613
[08/27/2025 20:26:14 INFO]: Training loss at epoch 42: 0.9926982223987579
[08/27/2025 20:26:14 INFO]: New best epoch, val score: -0.9198276883030254
[08/27/2025 20:26:14 INFO]: Saving model to: maddest-Elbert_trial_75/model_best.pth
[08/27/2025 20:26:17 INFO]: Training loss at epoch 41: 1.0236349403858185
[08/27/2025 20:26:18 INFO]: Running Final Evaluation...
[08/27/2025 20:26:28 INFO]: Training accuracy: {
    "score": -1.0378139832093005,
    "rmse": 1.0378139832093005
}
[08/27/2025 20:26:28 INFO]: Val accuracy: {
    "score": -0.8789458223800595,
    "rmse": 0.8789458223800595
}
[08/27/2025 20:26:28 INFO]: Test accuracy: {
    "score": -1.0164362689079858,
    "rmse": 1.0164362689079858
}
[08/27/2025 20:26:28 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_68",
    "best_epoch": 11,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0164362689079858,
        "rmse": 1.0164362689079858
    },
    "train_stats": {
        "score": -1.0378139832093005,
        "rmse": 1.0378139832093005
    },
    "val_stats": {
        "score": -0.8789458223800595,
        "rmse": 0.8789458223800595
    }
}
[08/27/2025 20:26:28 INFO]: Procewss finished for trial maddest-Elbert_trial_68
[08/27/2025 20:26:28 INFO]: 
_________________________________________________

[08/27/2025 20:26:28 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:26:28 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 1.0535102217003136
  attention_dropout: 0.11891311919773265
  ffn_dropout: 0.11891311919773265
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.266936986774587e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_78

[08/27/2025 20:26:28 INFO]: This ft_transformer has 6.590 million parameters.
[08/27/2025 20:26:28 INFO]: Training will start at epoch 0.
[08/27/2025 20:26:28 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:26:31 INFO]: Training loss at epoch 10: 1.050106167793274
[08/27/2025 20:26:32 INFO]: Training loss at epoch 12: 1.1700913906097412
[08/27/2025 20:26:37 INFO]: Training loss at epoch 13: 0.918703019618988
[08/27/2025 20:26:45 INFO]: Training loss at epoch 43: 0.9669166803359985
[08/27/2025 20:26:50 INFO]: Running Final Evaluation...
[08/27/2025 20:27:02 INFO]: Training accuracy: {
    "score": -1.0031305095204206,
    "rmse": 1.0031305095204206
}
[08/27/2025 20:27:02 INFO]: Val accuracy: {
    "score": -0.8865121432314821,
    "rmse": 0.8865121432314821
}
[08/27/2025 20:27:02 INFO]: Test accuracy: {
    "score": -0.9906041848208157,
    "rmse": 0.9906041848208157
}
[08/27/2025 20:27:02 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_65",
    "best_epoch": 12,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9906041848208157,
        "rmse": 0.9906041848208157
    },
    "train_stats": {
        "score": -1.0031305095204206,
        "rmse": 1.0031305095204206
    },
    "val_stats": {
        "score": -0.8865121432314821,
        "rmse": 0.8865121432314821
    }
}
[08/27/2025 20:27:02 INFO]: Procewss finished for trial maddest-Elbert_trial_65
[08/27/2025 20:27:02 INFO]: 
_________________________________________________

[08/27/2025 20:27:02 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:27:02 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 1.0286423430855403
  attention_dropout: 0.1272757854475714
  ffn_dropout: 0.1272757854475714
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.692228675711238e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_79

[08/27/2025 20:27:02 INFO]: This ft_transformer has 12.906 million parameters.
[08/27/2025 20:27:02 INFO]: Training will start at epoch 0.
[08/27/2025 20:27:02 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:27:03 INFO]: Training loss at epoch 0: 1.0529756844043732
[08/27/2025 20:27:06 INFO]: Training loss at epoch 33: 0.8765427470207214
[08/27/2025 20:27:09 INFO]: Training stats: {
    "score": -0.988898419177657,
    "rmse": 0.988898419177657
}
[08/27/2025 20:27:09 INFO]: Val stats: {
    "score": -0.9586551755109792,
    "rmse": 0.9586551755109792
}
[08/27/2025 20:27:09 INFO]: Test stats: {
    "score": -0.9773660804887241,
    "rmse": 0.9773660804887241
}
[08/27/2025 20:27:14 INFO]: Training loss at epoch 42: 1.0291156768798828
[08/27/2025 20:27:23 INFO]: Training accuracy: {
    "score": -1.0304486870879757,
    "rmse": 1.0304486870879757
}
[08/27/2025 20:27:23 INFO]: Val accuracy: {
    "score": -0.884906359758318,
    "rmse": 0.884906359758318
}
[08/27/2025 20:27:23 INFO]: Test accuracy: {
    "score": -1.0032133410378938,
    "rmse": 1.0032133410378938
}
[08/27/2025 20:27:23 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_3",
    "best_epoch": 13,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0032133410378938,
        "rmse": 1.0032133410378938
    },
    "train_stats": {
        "score": -1.0304486870879757,
        "rmse": 1.0304486870879757
    },
    "val_stats": {
        "score": -0.884906359758318,
        "rmse": 0.884906359758318
    }
}
[08/27/2025 20:27:23 INFO]: Procewss finished for trial maddest-Elbert_trial_3
[08/27/2025 20:27:23 INFO]: 
_________________________________________________

[08/27/2025 20:27:23 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:27:23 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 1.7158770186761476
  attention_dropout: 0.11433448930806289
  ffn_dropout: 0.11433448930806289
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 8.379248625758399e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_80

[08/27/2025 20:27:24 INFO]: New best epoch, val score: -1.094316096166119
[08/27/2025 20:27:24 INFO]: Saving model to: maddest-Elbert_trial_76/model_best.pth
[08/27/2025 20:27:24 INFO]: This ft_transformer has 15.792 million parameters.
[08/27/2025 20:27:24 INFO]: Training will start at epoch 0.
[08/27/2025 20:27:24 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:28:00 INFO]: Training loss at epoch 13: 1.3007490634918213
[08/27/2025 20:28:00 INFO]: Training loss at epoch 2: 1.0583717823028564
[08/27/2025 20:28:02 INFO]: Training loss at epoch 14: 1.4768542051315308
[08/27/2025 20:28:02 INFO]: Training loss at epoch 34: 1.2980627715587616
[08/27/2025 20:28:08 INFO]: Running Final Evaluation...
[08/27/2025 20:28:11 INFO]: New best epoch, val score: -0.8835031359374267
[08/27/2025 20:28:11 INFO]: Saving model to: maddest-Elbert_trial_69/model_best.pth
[08/27/2025 20:28:12 INFO]: Training loss at epoch 43: 1.0506943464279175
[08/27/2025 20:28:19 INFO]: Training loss at epoch 3: 1.1281986832618713
[08/27/2025 20:28:28 INFO]: Training accuracy: {
    "score": -1.0278815380362876,
    "rmse": 1.0278815380362876
}
[08/27/2025 20:28:28 INFO]: Val accuracy: {
    "score": -0.8822747431224891,
    "rmse": 0.8822747431224891
}
[08/27/2025 20:28:28 INFO]: Test accuracy: {
    "score": -0.9935995290181818,
    "rmse": 0.9935995290181818
}
[08/27/2025 20:28:28 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_62",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9935995290181818,
        "rmse": 0.9935995290181818
    },
    "train_stats": {
        "score": -1.0278815380362876,
        "rmse": 1.0278815380362876
    },
    "val_stats": {
        "score": -0.8822747431224891,
        "rmse": 0.8822747431224891
    }
}
[08/27/2025 20:28:28 INFO]: Procewss finished for trial maddest-Elbert_trial_62
[08/27/2025 20:28:28 INFO]: 
_________________________________________________

[08/27/2025 20:28:28 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:28:28 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 1.0624976473399492
  attention_dropout: 0.12079431579048591
  ffn_dropout: 0.12079431579048591
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.2755917433624897e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_81

[08/27/2025 20:28:28 INFO]: This ft_transformer has 6.606 million parameters.
[08/27/2025 20:28:28 INFO]: Training will start at epoch 0.
[08/27/2025 20:28:28 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:28:33 INFO]: Training loss at epoch 24: 0.8300811052322388
[08/27/2025 20:28:44 INFO]: New best epoch, val score: -0.883075920111307
[08/27/2025 20:28:44 INFO]: Saving model to: maddest-Elbert_trial_73/model_best.pth
[08/27/2025 20:28:54 INFO]: Training loss at epoch 1: 1.1365221738815308
[08/27/2025 20:29:00 INFO]: Training loss at epoch 11: 0.8168068826198578
[08/27/2025 20:29:05 INFO]: Training loss at epoch 0: 1.0609988570213318
[08/27/2025 20:29:11 INFO]: Training loss at epoch 44: 1.0335321426391602
[08/27/2025 20:29:11 INFO]: Training loss at epoch 0: 0.9807490110397339
[08/27/2025 20:29:24 INFO]: Training loss at epoch 4: 0.8888092041015625
[08/27/2025 20:29:26 INFO]: Training loss at epoch 14: 1.4192683696746826
[08/27/2025 20:29:27 INFO]: Training loss at epoch 15: 1.0524733662605286
[08/27/2025 20:29:31 INFO]: New best epoch, val score: -1.133068851447823
[08/27/2025 20:29:31 INFO]: Saving model to: maddest-Elbert_trial_77/model_best.pth
[08/27/2025 20:29:34 INFO]: Training stats: {
    "score": -1.0016980963237596,
    "rmse": 1.0016980963237596
}
[08/27/2025 20:29:34 INFO]: Val stats: {
    "score": -0.972802057628488,
    "rmse": 0.972802057628488
}
[08/27/2025 20:29:34 INFO]: Test stats: {
    "score": -0.9800689173660126,
    "rmse": 0.9800689173660126
}
[08/27/2025 20:29:35 INFO]: New best epoch, val score: -0.9690607573294595
[08/27/2025 20:29:35 INFO]: Saving model to: maddest-Elbert_trial_78/model_best.pth
[08/27/2025 20:30:06 INFO]: Training loss at epoch 1: 1.1925466656684875
[08/27/2025 20:30:09 INFO]: Training loss at epoch 45: 0.9013451039791107
[08/27/2025 20:30:10 INFO]: Training loss at epoch 3: 1.0217740535736084
[08/27/2025 20:30:28 INFO]: New best epoch, val score: -0.9073546094646747
[08/27/2025 20:30:28 INFO]: Saving model to: maddest-Elbert_trial_76/model_best.pth
[08/27/2025 20:30:50 INFO]: Training loss at epoch 20: 0.923941433429718
[08/27/2025 20:30:55 INFO]: Training loss at epoch 16: 0.9706598520278931
[08/27/2025 20:30:55 INFO]: Training loss at epoch 15: 1.1605262160301208
[08/27/2025 20:31:07 INFO]: Training loss at epoch 46: 0.8967013359069824
[08/27/2025 20:31:11 INFO]: Training loss at epoch 0: 0.9956823289394379
[08/27/2025 20:31:15 INFO]: Running Final Evaluation...
[08/27/2025 20:31:19 INFO]: Training loss at epoch 3: 0.8434518277645111
[08/27/2025 20:31:31 INFO]: Training loss at epoch 12: 1.0637752413749695
[08/27/2025 20:31:31 INFO]: New best epoch, val score: -0.9119476003563103
[08/27/2025 20:31:31 INFO]: Saving model to: maddest-Elbert_trial_81/model_best.pth
[08/27/2025 20:31:34 INFO]: Training accuracy: {
    "score": -1.0077736537804711,
    "rmse": 1.0077736537804711
}
[08/27/2025 20:31:34 INFO]: Val accuracy: {
    "score": -0.8917106268434106,
    "rmse": 0.8917106268434106
}
[08/27/2025 20:31:34 INFO]: Test accuracy: {
    "score": -0.9873347463314813,
    "rmse": 0.9873347463314813
}
[08/27/2025 20:31:34 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_58",
    "best_epoch": 15,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9873347463314813,
        "rmse": 0.9873347463314813
    },
    "train_stats": {
        "score": -1.0077736537804711,
        "rmse": 1.0077736537804711
    },
    "val_stats": {
        "score": -0.8917106268434106,
        "rmse": 0.8917106268434106
    }
}
[08/27/2025 20:31:34 INFO]: Procewss finished for trial maddest-Elbert_trial_58
[08/27/2025 20:31:34 INFO]: 
_________________________________________________

[08/27/2025 20:31:34 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:31:34 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 1.0273592246395422
  attention_dropout: 0.1150603910996494
  ffn_dropout: 0.1150603910996494
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00021713294402921616
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_82

[08/27/2025 20:31:34 INFO]: This ft_transformer has 6.536 million parameters.
[08/27/2025 20:31:34 INFO]: Training will start at epoch 0.
[08/27/2025 20:31:34 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:31:37 INFO]: Training loss at epoch 37: 0.9769582450389862
[08/27/2025 20:31:48 INFO]: Training loss at epoch 4: 1.575257271528244
[08/27/2025 20:31:56 INFO]: Training loss at epoch 2: 1.1096646189689636
[08/27/2025 20:32:16 INFO]: Training loss at epoch 25: 0.8368332087993622
[08/27/2025 20:32:17 INFO]: Training loss at epoch 1: 0.9063510894775391
[08/27/2025 20:32:19 INFO]: Training loss at epoch 17: 0.9527128636837006
[08/27/2025 20:32:21 INFO]: Training loss at epoch 16: 1.0891961455345154
[08/27/2025 20:32:39 INFO]: New best epoch, val score: -0.9680447193608318
[08/27/2025 20:32:39 INFO]: Saving model to: maddest-Elbert_trial_78/model_best.pth
[08/27/2025 20:32:40 INFO]: Training loss at epoch 1: 1.1538583636283875
[08/27/2025 20:32:59 INFO]: Training loss at epoch 0: 0.7591260820627213
[08/27/2025 20:33:06 INFO]: New best epoch, val score: -0.9604262811039243
[08/27/2025 20:33:06 INFO]: Saving model to: maddest-Elbert_trial_77/model_best.pth
[08/27/2025 20:33:08 INFO]: Training loss at epoch 2: 1.4338796138763428
[08/27/2025 20:33:29 INFO]: Training loss at epoch 5: 1.4613234996795654
[08/27/2025 20:33:32 INFO]: Training loss at epoch 33: 0.944699227809906
[08/27/2025 20:33:45 INFO]: Training loss at epoch 18: 1.2121784687042236
[08/27/2025 20:33:46 INFO]: New best epoch, val score: -0.9108890205790563
[08/27/2025 20:33:46 INFO]: Saving model to: maddest-Elbert_trial_79/model_best.pth
[08/27/2025 20:33:48 INFO]: Training loss at epoch 17: 1.0983684062957764
[08/27/2025 20:34:02 INFO]: Training loss at epoch 13: 1.0820608735084534
[08/27/2025 20:34:13 INFO]: Training loss at epoch 1: 0.9273851811885834
[08/27/2025 20:34:14 INFO]: Training loss at epoch 0: 1.1232416033744812
[08/27/2025 20:34:23 INFO]: Training loss at epoch 0: 1.1309889554977417
[08/27/2025 20:34:28 INFO]: Training loss at epoch 21: 1.0776653587818146
[08/27/2025 20:34:33 INFO]: Training loss at epoch 4: 1.0482833981513977
[08/27/2025 20:34:34 INFO]: New best epoch, val score: -0.8916430305594316
[08/27/2025 20:34:34 INFO]: Saving model to: maddest-Elbert_trial_81/model_best.pth
[08/27/2025 20:34:36 INFO]: New best epoch, val score: -0.8988664234148154
[08/27/2025 20:34:36 INFO]: Saving model to: maddest-Elbert_trial_82/model_best.pth
[08/27/2025 20:34:36 INFO]: Training loss at epoch 4: 1.135574996471405
[08/27/2025 20:34:56 INFO]: Training loss at epoch 3: 1.5811229944229126
[08/27/2025 20:35:12 INFO]: Training loss at epoch 19: 1.0352507829666138
[08/27/2025 20:35:15 INFO]: Training loss at epoch 18: 1.1029630899429321
[08/27/2025 20:35:15 INFO]: Training loss at epoch 5: 1.042355239391327
[08/27/2025 20:35:18 INFO]: New best epoch, val score: -0.9724351813457167
[08/27/2025 20:35:18 INFO]: Saving model to: maddest-Elbert_trial_80/model_best.pth
[08/27/2025 20:35:21 INFO]: Training loss at epoch 2: 1.2161182761192322
[08/27/2025 20:35:41 INFO]: Training stats: {
    "score": -1.0323479184554047,
    "rmse": 1.0323479184554047
}
[08/27/2025 20:35:41 INFO]: Val stats: {
    "score": -1.041079313343785,
    "rmse": 1.041079313343785
}
[08/27/2025 20:35:41 INFO]: Test stats: {
    "score": -1.0107647075733999,
    "rmse": 1.0107647075733999
}
[08/27/2025 20:35:44 INFO]: New best epoch, val score: -0.9244026789112705
[08/27/2025 20:35:44 INFO]: Saving model to: maddest-Elbert_trial_78/model_best.pth
[08/27/2025 20:35:59 INFO]: Training loss at epoch 26: 0.8663927316665649
[08/27/2025 20:36:11 INFO]: Training loss at epoch 3: 1.0394958555698395
[08/27/2025 20:36:13 INFO]: Training loss at epoch 2: 1.4174049496650696
[08/27/2025 20:36:31 INFO]: Training loss at epoch 14: 1.0472835302352905
[08/27/2025 20:36:41 INFO]: Training loss at epoch 19: 1.0569575428962708
[08/27/2025 20:37:05 INFO]: Training loss at epoch 20: 1.1085017323493958
[08/27/2025 20:37:09 INFO]: Training stats: {
    "score": -1.0237214153011727,
    "rmse": 1.0237214153011727
}
[08/27/2025 20:37:09 INFO]: Val stats: {
    "score": -0.8842747030581526,
    "rmse": 0.8842747030581526
}
[08/27/2025 20:37:09 INFO]: Test stats: {
    "score": -1.0021783841179526,
    "rmse": 1.0021783841179526
}
[08/27/2025 20:37:16 INFO]: Training loss at epoch 2: 1.1120420098304749
[08/27/2025 20:37:16 INFO]: Training loss at epoch 1: 3.723666787147522
[08/27/2025 20:37:32 INFO]: Training loss at epoch 6: 1.2698840498924255
[08/27/2025 20:37:54 INFO]: Training loss at epoch 5: 0.8063817620277405
[08/27/2025 20:37:57 INFO]: Training loss at epoch 4: 1.4324339628219604
[08/27/2025 20:38:06 INFO]: Training loss at epoch 22: 1.074436515569687
[08/27/2025 20:38:25 INFO]: Training loss at epoch 3: 1.1267064213752747
[08/27/2025 20:38:29 INFO]: Training loss at epoch 21: 1.1621280908584595
[08/27/2025 20:38:35 INFO]: Training loss at epoch 20: 1.2083635032176971
[08/27/2025 20:38:41 INFO]: Training loss at epoch 6: 1.1404487490653992
[08/27/2025 20:38:56 INFO]: Training loss at epoch 5: 0.9043544828891754
[08/27/2025 20:38:58 INFO]: Training loss at epoch 15: 0.9630028903484344
[08/27/2025 20:39:00 INFO]: Training loss at epoch 38: 1.1002895534038544
[08/27/2025 20:39:12 INFO]: Training loss at epoch 4: 0.8688324093818665
[08/27/2025 20:39:39 INFO]: Training loss at epoch 27: 0.8903480470180511
[08/27/2025 20:39:39 INFO]: Training loss at epoch 1: 1.0708986520767212
[08/27/2025 20:39:45 INFO]: Training loss at epoch 3: 1.555727243423462
[08/27/2025 20:39:53 INFO]: Training loss at epoch 22: 1.1080297827720642
[08/27/2025 20:40:01 INFO]: Training loss at epoch 21: 1.0480252802371979
[08/27/2025 20:40:10 INFO]: New best epoch, val score: -0.9291044997461534
[08/27/2025 20:40:10 INFO]: Saving model to: maddest-Elbert_trial_77/model_best.pth
[08/27/2025 20:40:14 INFO]: Training loss at epoch 2: 1.6511181592941284
[08/27/2025 20:40:15 INFO]: Training loss at epoch 3: 1.1731637716293335
[08/27/2025 20:40:36 INFO]: New best epoch, val score: -0.8795009196086623
[08/27/2025 20:40:36 INFO]: Saving model to: maddest-Elbert_trial_81/model_best.pth
[08/27/2025 20:40:54 INFO]: Training loss at epoch 5: 1.1493410170078278
[08/27/2025 20:40:57 INFO]: Training loss at epoch 30: 1.2640363276004791
[08/27/2025 20:41:09 INFO]: Training loss at epoch 6: 1.0394605994224548
[08/27/2025 20:41:16 INFO]: New best epoch, val score: -0.917839930957676
[08/27/2025 20:41:16 INFO]: Saving model to: maddest-Elbert_trial_75/model_best.pth
[08/27/2025 20:41:18 INFO]: Training loss at epoch 23: 0.9308953285217285
[08/27/2025 20:41:26 INFO]: Training loss at epoch 16: 0.9659174084663391
[08/27/2025 20:41:28 INFO]: Training loss at epoch 22: 0.8295256197452545
[08/27/2025 20:41:28 INFO]: Training loss at epoch 4: 1.1647896766662598
[08/27/2025 20:41:34 INFO]: Training loss at epoch 7: 1.3736833333969116
[08/27/2025 20:41:45 INFO]: Training loss at epoch 23: 0.9998589754104614
[08/27/2025 20:41:57 INFO]: Training loss at epoch 34: 1.1065248548984528
[08/27/2025 20:42:06 INFO]: Training loss at epoch 7: 1.0374295115470886
[08/27/2025 20:42:11 INFO]: Training loss at epoch 5: 1.2332881093025208
[08/27/2025 20:42:13 INFO]: Training loss at epoch 1: 1.0812570452690125
[08/27/2025 20:42:44 INFO]: Training loss at epoch 24: 1.0784380435943604
[08/27/2025 20:42:54 INFO]: Training loss at epoch 23: 1.439045399427414
[08/27/2025 20:43:08 INFO]: New best epoch, val score: -0.8847714972549346
[08/27/2025 20:43:08 INFO]: Saving model to: maddest-Elbert_trial_80/model_best.pth
[08/27/2025 20:43:16 INFO]: Training loss at epoch 3: 2.22845059633255
[08/27/2025 20:43:18 INFO]: Training loss at epoch 6: 1.070491373538971
[08/27/2025 20:43:18 INFO]: Training loss at epoch 4: 1.2508374452590942
[08/27/2025 20:43:20 INFO]: Training loss at epoch 28: 1.0797603726387024
[08/27/2025 20:43:20 INFO]: Training loss at epoch 4: 1.139518916606903
[08/27/2025 20:43:55 INFO]: Training loss at epoch 6: 1.0352907478809357
[08/27/2025 20:43:55 INFO]: Training loss at epoch 17: 0.9195803701877594
[08/27/2025 20:44:08 INFO]: Training loss at epoch 25: 1.0958560705184937
[08/27/2025 20:44:16 INFO]: New best epoch, val score: -0.9105991353963049
[08/27/2025 20:44:16 INFO]: Saving model to: maddest-Elbert_trial_75/model_best.pth
[08/27/2025 20:44:20 INFO]: Training loss at epoch 24: 0.8150208592414856
[08/27/2025 20:44:24 INFO]: Training loss at epoch 7: 0.7951889634132385
[08/27/2025 20:44:31 INFO]: Training loss at epoch 5: 1.1861645579338074
[08/27/2025 20:45:13 INFO]: Training loss at epoch 6: 1.5005698204040527
[08/27/2025 20:45:24 INFO]: Training loss at epoch 24: 1.2136314511299133
[08/27/2025 20:45:33 INFO]: Training loss at epoch 8: 0.9708423912525177
[08/27/2025 20:45:33 INFO]: Training loss at epoch 26: 0.9611511826515198
[08/27/2025 20:45:36 INFO]: Training loss at epoch 8: 1.1370711624622345
[08/27/2025 20:45:47 INFO]: Training loss at epoch 25: 0.8978016376495361
[08/27/2025 20:46:15 INFO]: Training loss at epoch 4: 1.2524603605270386
[08/27/2025 20:46:19 INFO]: Training loss at epoch 5: 0.918635219335556
[08/27/2025 20:46:19 INFO]: Training loss at epoch 2: 1.1385523080825806
[08/27/2025 20:46:23 INFO]: Training loss at epoch 39: 0.8942139744758606
[08/27/2025 20:46:24 INFO]: Training loss at epoch 18: 1.296358585357666
[08/27/2025 20:46:52 INFO]: Training loss at epoch 5: 1.1093510389328003
[08/27/2025 20:46:55 INFO]: Training loss at epoch 7: 1.0295826196670532
[08/27/2025 20:46:57 INFO]: Training loss at epoch 27: 0.987867683172226
[08/27/2025 20:47:00 INFO]: Training loss at epoch 29: 0.8875819444656372
[08/27/2025 20:47:12 INFO]: Training loss at epoch 26: 1.0763205587863922
[08/27/2025 20:47:17 INFO]: New best epoch, val score: -0.9058459941652934
[08/27/2025 20:47:17 INFO]: Saving model to: maddest-Elbert_trial_75/model_best.pth
[08/27/2025 20:47:34 INFO]: Training loss at epoch 6: 1.4018185138702393
[08/27/2025 20:47:41 INFO]: Training loss at epoch 7: 1.0381710529327393
[08/27/2025 20:47:41 INFO]: Training loss at epoch 8: 1.0289740562438965
[08/27/2025 20:48:14 INFO]: Training loss at epoch 7: 1.322826325893402
[08/27/2025 20:48:16 INFO]: Training stats: {
    "score": -0.9640683534728326,
    "rmse": 0.9640683534728326
}
[08/27/2025 20:48:16 INFO]: Val stats: {
    "score": -0.9526465987798102,
    "rmse": 0.9526465987798102
}
[08/27/2025 20:48:16 INFO]: Test stats: {
    "score": -0.9777276685590224,
    "rmse": 0.9777276685590224
}
[08/27/2025 20:48:21 INFO]: Training loss at epoch 28: 1.1003663539886475
[08/27/2025 20:48:38 INFO]: Training loss at epoch 27: 0.8082775771617889
[08/27/2025 20:48:51 INFO]: Training loss at epoch 19: 1.047692358493805
[08/27/2025 20:48:59 INFO]: Training loss at epoch 9: 0.9657821357250214
[08/27/2025 20:49:00 INFO]: Training stats: {
    "score": -0.9947652400807256,
    "rmse": 0.9947652400807256
}
[08/27/2025 20:49:00 INFO]: Val stats: {
    "score": -0.9372523822938009,
    "rmse": 0.9372523822938009
}
[08/27/2025 20:49:00 INFO]: Test stats: {
    "score": -0.9692784390399478,
    "rmse": 0.9692784390399478
}
[08/27/2025 20:49:03 INFO]: Training loss at epoch 25: 0.8994931876659393
[08/27/2025 20:49:15 INFO]: Training loss at epoch 5: 1.3318209648132324
[08/27/2025 20:49:19 INFO]: Training loss at epoch 6: 1.0192563831806183
[08/27/2025 20:49:38 INFO]: Training loss at epoch 9: 1.1352980136871338
[08/27/2025 20:49:41 INFO]: Training stats: {
    "score": -0.984379785963681,
    "rmse": 0.984379785963681
}
[08/27/2025 20:49:41 INFO]: Val stats: {
    "score": -0.9122390238769615,
    "rmse": 0.9122390238769615
}
[08/27/2025 20:49:41 INFO]: Test stats: {
    "score": -0.9876605829289046,
    "rmse": 0.9876605829289046
}
[08/27/2025 20:49:46 INFO]: Training loss at epoch 29: 0.8598609268665314
[08/27/2025 20:49:55 INFO]: Running Final Evaluation...
[08/27/2025 20:49:55 INFO]: Training loss at epoch 8: 1.0124995708465576
[08/27/2025 20:50:04 INFO]: Training loss at epoch 2: 1.4105929136276245
[08/27/2025 20:50:04 INFO]: Training loss at epoch 28: 0.9985504448413849
[08/27/2025 20:50:09 INFO]: Training stats: {
    "score": -1.0105721896109818,
    "rmse": 1.0105721896109818
}
[08/27/2025 20:50:09 INFO]: Val stats: {
    "score": -0.8885941060122899,
    "rmse": 0.8885941060122899
}
[08/27/2025 20:50:09 INFO]: Test stats: {
    "score": -0.9860405624994982,
    "rmse": 0.9860405624994982
}
[08/27/2025 20:50:14 INFO]: Training stats: {
    "score": -1.0140541848669735,
    "rmse": 1.0140541848669735
}
[08/27/2025 20:50:14 INFO]: Val stats: {
    "score": -1.007805029643136,
    "rmse": 1.007805029643136
}
[08/27/2025 20:50:14 INFO]: Test stats: {
    "score": -0.9939693284232684,
    "rmse": 0.9939693284232684
}
[08/27/2025 20:50:15 INFO]: New best epoch, val score: -0.9031328279433624
[08/27/2025 20:50:15 INFO]: Saving model to: maddest-Elbert_trial_75/model_best.pth
[08/27/2025 20:50:21 INFO]: Training loss at epoch 35: 0.7584317028522491
[08/27/2025 20:50:22 INFO]: Training loss at epoch 6: 1.2564473748207092
[08/27/2025 20:50:36 INFO]: Training loss at epoch 7: 1.078202724456787
[08/27/2025 20:50:55 INFO]: Training loss at epoch 9: 0.9661865234375
[08/27/2025 20:50:58 INFO]: Training stats: {
    "score": -1.0913932941521909,
    "rmse": 1.0913932941521909
}
[08/27/2025 20:50:58 INFO]: Val stats: {
    "score": -0.8989586078208475,
    "rmse": 0.8989586078208475
}
[08/27/2025 20:50:58 INFO]: Test stats: {
    "score": -1.0675788888349034,
    "rmse": 1.0675788888349034
}
[08/27/2025 20:51:12 INFO]: Training loss at epoch 8: 1.2464396357536316
[08/27/2025 20:51:27 INFO]: Training loss at epoch 29: 1.16579270362854
[08/27/2025 20:51:38 INFO]: Training loss at epoch 30: 0.9340995252132416
[08/27/2025 20:51:53 INFO]: Training loss at epoch 30: 1.1353342235088348
[08/27/2025 20:51:56 INFO]: Training stats: {
    "score": -1.0022082438892082,
    "rmse": 1.0022082438892082
}
[08/27/2025 20:51:56 INFO]: Val stats: {
    "score": -0.8940857543677624,
    "rmse": 0.8940857543677624
}
[08/27/2025 20:51:56 INFO]: Test stats: {
    "score": -0.9819303438688124,
    "rmse": 0.9819303438688124
}
[08/27/2025 20:51:59 INFO]: Training loss at epoch 8: 1.1087695360183716
[08/27/2025 20:52:00 INFO]: Training stats: {
    "score": -0.9947088841168978,
    "rmse": 0.9947088841168978
}
[08/27/2025 20:52:00 INFO]: Val stats: {
    "score": -0.9155516269075115,
    "rmse": 0.9155516269075115
}
[08/27/2025 20:52:00 INFO]: Test stats: {
    "score": -0.9655879221226694,
    "rmse": 0.9655879221226694
}
[08/27/2025 20:52:07 INFO]: Training loss at epoch 20: 1.0050006210803986
[08/27/2025 20:52:15 INFO]: Training loss at epoch 6: 1.366463541984558
[08/27/2025 20:52:18 INFO]: Training loss at epoch 7: 1.0048029720783234
[08/27/2025 20:52:19 INFO]: Training loss at epoch 31: 0.8590944111347198
[08/27/2025 20:52:38 INFO]: Training loss at epoch 26: 0.9164772033691406
[08/27/2025 20:52:47 INFO]: Training accuracy: {
    "score": -1.0338427580329717,
    "rmse": 1.0338427580329717
}
[08/27/2025 20:52:47 INFO]: Val accuracy: {
    "score": -0.880493047605885,
    "rmse": 0.880493047605885
}
[08/27/2025 20:52:47 INFO]: Test accuracy: {
    "score": -1.0070023959171277,
    "rmse": 1.0070023959171277
}
[08/27/2025 20:52:47 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_31",
    "best_epoch": 8,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0070023959171277,
        "rmse": 1.0070023959171277
    },
    "train_stats": {
        "score": -1.0338427580329717,
        "rmse": 1.0338427580329717
    },
    "val_stats": {
        "score": -0.880493047605885,
        "rmse": 0.880493047605885
    }
}
[08/27/2025 20:52:47 INFO]: Procewss finished for trial maddest-Elbert_trial_31
[08/27/2025 20:52:47 INFO]: 
_________________________________________________

[08/27/2025 20:52:47 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:52:47 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 1.0380594010478439
  attention_dropout: 0.1228879292222064
  ffn_dropout: 0.1228879292222064
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0002040878489624135
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_83

[08/27/2025 20:52:47 INFO]: This ft_transformer has 2.605 million parameters.
[08/27/2025 20:52:47 INFO]: Training will start at epoch 0.
[08/27/2025 20:52:47 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:52:52 INFO]: Training loss at epoch 9: 1.1354703307151794
[08/27/2025 20:52:57 INFO]: Training loss at epoch 3: 1.0164071023464203
[08/27/2025 20:53:03 INFO]: Training loss at epoch 31: 1.083447277545929
[08/27/2025 20:53:23 INFO]: Training loss at epoch 30: 1.141756385564804
[08/27/2025 20:53:32 INFO]: Training loss at epoch 10: 1.0743355751037598
[08/27/2025 20:53:37 INFO]: Training loss at epoch 8: 1.2447593212127686
[08/27/2025 20:53:39 INFO]: Running Final Evaluation...
[08/27/2025 20:53:43 INFO]: New best epoch, val score: -0.8842082190453863
[08/27/2025 20:53:43 INFO]: Saving model to: maddest-Elbert_trial_79/model_best.pth
[08/27/2025 20:53:53 INFO]: Training stats: {
    "score": -1.0011298773594777,
    "rmse": 1.0011298773594777
}
[08/27/2025 20:53:53 INFO]: Val stats: {
    "score": -0.9029402665365026,
    "rmse": 0.9029402665365026
}
[08/27/2025 20:53:53 INFO]: Test stats: {
    "score": -0.9744504655635077,
    "rmse": 0.9744504655635077
}
[08/27/2025 20:53:53 INFO]: Training loss at epoch 7: 1.2676768898963928
[08/27/2025 20:54:03 INFO]: Training loss at epoch 0: 1.0205232501029968
[08/27/2025 20:54:13 INFO]: Training loss at epoch 9: 0.9235217571258545
[08/27/2025 20:54:13 INFO]: New best epoch, val score: -1.0372927531099831
[08/27/2025 20:54:13 INFO]: Saving model to: maddest-Elbert_trial_83/model_best.pth
[08/27/2025 20:54:14 INFO]: New best epoch, val score: -0.9029402665365026
[08/27/2025 20:54:14 INFO]: Saving model to: maddest-Elbert_trial_75/model_best.pth
[08/27/2025 20:54:27 INFO]: Training loss at epoch 32: 1.1496045589447021
[08/27/2025 20:54:34 INFO]: Training loss at epoch 21: 0.9662267565727234
[08/27/2025 20:54:49 INFO]: Training loss at epoch 31: 0.7853862047195435
[08/27/2025 20:54:59 INFO]: Training loss at epoch 10: 1.0400814414024353
[08/27/2025 20:55:15 INFO]: Training loss at epoch 7: 1.0615940392017365
[08/27/2025 20:55:17 INFO]: Training loss at epoch 10: 1.2629241049289703
[08/27/2025 20:55:17 INFO]: Training stats: {
    "score": -1.0570687643134202,
    "rmse": 1.0570687643134202
}
[08/27/2025 20:55:17 INFO]: Val stats: {
    "score": -1.0760690325527782,
    "rmse": 1.0760690325527782
}
[08/27/2025 20:55:17 INFO]: Test stats: {
    "score": -1.0354016423063006,
    "rmse": 1.0354016423063006
}
[08/27/2025 20:55:19 INFO]: Training loss at epoch 8: 0.9862605631351471
[08/27/2025 20:55:29 INFO]: Training loss at epoch 1: 1.3866950273513794
[08/27/2025 20:55:33 INFO]: Training loss at epoch 31: 0.9036297500133514
[08/27/2025 20:55:40 INFO]: New best epoch, val score: -0.9834568988921657
[08/27/2025 20:55:40 INFO]: Saving model to: maddest-Elbert_trial_83/model_best.pth
[08/27/2025 20:55:52 INFO]: Training loss at epoch 33: 1.1555370092391968
[08/27/2025 20:56:14 INFO]: Training loss at epoch 32: 0.7762238383293152
[08/27/2025 20:56:16 INFO]: Training loss at epoch 27: 1.0924508571624756
[08/27/2025 20:56:22 INFO]: Training loss at epoch 9: 0.9305560886859894
[08/27/2025 20:56:41 INFO]: Training loss at epoch 9: 1.2365863919258118
[08/27/2025 20:56:52 INFO]: Training loss at epoch 10: 1.0499881505966187
[08/27/2025 20:56:56 INFO]: Training loss at epoch 2: 1.4229995608329773
[08/27/2025 20:56:57 INFO]: Training loss at epoch 11: 1.163681834936142
[08/27/2025 20:57:03 INFO]: Training loss at epoch 22: 1.0370869040489197
[08/27/2025 20:57:16 INFO]: Training loss at epoch 34: 1.021373987197876
[08/27/2025 20:57:26 INFO]: Training loss at epoch 8: 1.5406684875488281
[08/27/2025 20:57:37 INFO]: Training accuracy: {
    "score": -1.07116973834232,
    "rmse": 1.07116973834232
}
[08/27/2025 20:57:37 INFO]: Val accuracy: {
    "score": -0.8755655494103118,
    "rmse": 0.8755655494103118
}
[08/27/2025 20:57:37 INFO]: Test accuracy: {
    "score": -1.0387847985738738,
    "rmse": 1.0387847985738738
}
[08/27/2025 20:57:37 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_19",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0387847985738738,
        "rmse": 1.0387847985738738
    },
    "train_stats": {
        "score": -1.07116973834232,
        "rmse": 1.07116973834232
    },
    "val_stats": {
        "score": -0.8755655494103118,
        "rmse": 0.8755655494103118
    }
}
[08/27/2025 20:57:37 INFO]: Procewss finished for trial maddest-Elbert_trial_19
[08/27/2025 20:57:37 INFO]: 
_________________________________________________

[08/27/2025 20:57:37 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:57:37 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 1.6985774092910841
  attention_dropout: 0.11387512544177564
  ffn_dropout: 0.11387512544177564
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 8.025728772832779e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_84

[08/27/2025 20:57:37 INFO]: This ft_transformer has 19.606 million parameters.
[08/27/2025 20:57:37 INFO]: Training will start at epoch 0.
[08/27/2025 20:57:37 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:57:39 INFO]: Training loss at epoch 33: 0.9425523281097412
[08/27/2025 20:57:43 INFO]: Training stats: {
    "score": -1.0448423872154122,
    "rmse": 1.0448423872154122
}
[08/27/2025 20:57:43 INFO]: Val stats: {
    "score": -1.0643408334048905,
    "rmse": 1.0643408334048905
}
[08/27/2025 20:57:43 INFO]: Test stats: {
    "score": -1.0245889503415344,
    "rmse": 1.0245889503415344
}
[08/27/2025 20:57:51 INFO]: Training loss at epoch 3: 1.2099366784095764
[08/27/2025 20:57:51 INFO]: Training stats: {
    "score": -0.9594197571849294,
    "rmse": 0.9594197571849294
}
[08/27/2025 20:57:51 INFO]: Val stats: {
    "score": -0.9233711971084015,
    "rmse": 0.9233711971084015
}
[08/27/2025 20:57:51 INFO]: Test stats: {
    "score": -0.9769648398748888,
    "rmse": 0.9769648398748888
}
[08/27/2025 20:58:14 INFO]: Training loss at epoch 8: 1.3733623623847961
[08/27/2025 20:58:17 INFO]: Training loss at epoch 10: 0.9509734809398651
[08/27/2025 20:58:20 INFO]: Training loss at epoch 9: 0.9871181845664978
[08/27/2025 20:58:22 INFO]: Training loss at epoch 3: 1.183816909790039
[08/27/2025 20:58:32 INFO]: Training loss at epoch 11: 0.9632855951786041
[08/27/2025 20:58:40 INFO]: Training loss at epoch 35: 0.8950412571430206
[08/27/2025 20:58:43 INFO]: Training loss at epoch 36: 1.0647683143615723
[08/27/2025 20:58:55 INFO]: New best epoch, val score: -0.8932584818497721
[08/27/2025 20:58:55 INFO]: Saving model to: maddest-Elbert_trial_74/model_best.pth
[08/27/2025 20:59:01 INFO]: Training loss at epoch 11: 0.988215833902359
[08/27/2025 20:59:06 INFO]: Training loss at epoch 34: 1.0730321407318115
[08/27/2025 20:59:15 INFO]: Training loss at epoch 32: 0.8154188692569733
[08/27/2025 20:59:22 INFO]: Training stats: {
    "score": -1.0041388680908443,
    "rmse": 1.0041388680908443
}
[08/27/2025 20:59:22 INFO]: Val stats: {
    "score": -0.9712189418726864,
    "rmse": 0.9712189418726864
}
[08/27/2025 20:59:22 INFO]: Test stats: {
    "score": -0.9770754084983577,
    "rmse": 0.9770754084983577
}
[08/27/2025 20:59:30 INFO]: Training loss at epoch 23: 1.3589809238910675
[08/27/2025 20:59:37 INFO]: Training loss at epoch 4: 0.8348974585533142
[08/27/2025 20:59:48 INFO]: Training loss at epoch 4: 1.2840295732021332
[08/27/2025 20:59:52 INFO]: Training loss at epoch 11: 0.9235602915287018
[08/27/2025 20:59:54 INFO]: Training loss at epoch 28: 1.016588032245636
[08/27/2025 20:59:58 INFO]: New best epoch, val score: -0.9263324972849841
[08/27/2025 20:59:58 INFO]: Saving model to: maddest-Elbert_trial_83/model_best.pth
[08/27/2025 21:00:06 INFO]: Training loss at epoch 36: 0.9422228336334229
[08/27/2025 21:00:23 INFO]: Training loss at epoch 12: 1.357903152704239
[08/27/2025 21:00:33 INFO]: Training loss at epoch 35: 0.7879422903060913
[08/27/2025 21:00:47 INFO]: Training loss at epoch 10: 0.9751365482807159
[08/27/2025 21:01:00 INFO]: Training loss at epoch 9: 1.3179811835289001
[08/27/2025 21:01:14 INFO]: Training loss at epoch 9: 1.4190926551818848
[08/27/2025 21:01:14 INFO]: Training loss at epoch 5: 1.4297733306884766
[08/27/2025 21:01:20 INFO]: Training loss at epoch 11: 0.9562998414039612
[08/27/2025 21:01:25 INFO]: New best epoch, val score: -0.9242164874819436
[08/27/2025 21:01:25 INFO]: Saving model to: maddest-Elbert_trial_83/model_best.pth
[08/27/2025 21:01:31 INFO]: Training loss at epoch 37: 0.9928094446659088
[08/27/2025 21:01:40 INFO]: New best epoch, val score: -0.8885256679923289
[08/27/2025 21:01:40 INFO]: Saving model to: maddest-Elbert_trial_76/model_best.pth
[08/27/2025 21:01:48 INFO]: Training loss at epoch 12: 1.03496915102005
[08/27/2025 21:01:58 INFO]: Training loss at epoch 36: 1.0762943923473358
[08/27/2025 21:02:00 INFO]: Training loss at epoch 24: 0.9343564808368683
[08/27/2025 21:02:10 INFO]: Training stats: {
    "score": -1.0952088153110182,
    "rmse": 1.0952088153110182
}
[08/27/2025 21:02:10 INFO]: Val stats: {
    "score": -1.146202547681959,
    "rmse": 1.146202547681959
}
[08/27/2025 21:02:10 INFO]: Test stats: {
    "score": -1.0698959659338814,
    "rmse": 1.0698959659338814
}
[08/27/2025 21:02:10 INFO]: New best epoch, val score: -0.8912129048681428
[08/27/2025 21:02:11 INFO]: Saving model to: maddest-Elbert_trial_74/model_best.pth
[08/27/2025 21:02:13 INFO]: Training loss at epoch 10: 0.929254800081253
[08/27/2025 21:02:15 INFO]: Training stats: {
    "score": -1.1478036260023037,
    "rmse": 1.1478036260023037
}
[08/27/2025 21:02:15 INFO]: Val stats: {
    "score": -1.2162133631962069,
    "rmse": 1.2162133631962069
}
[08/27/2025 21:02:15 INFO]: Test stats: {
    "score": -1.1247482392477104,
    "rmse": 1.1247482392477104
}
[08/27/2025 21:02:22 INFO]: Training loss at epoch 10: 0.8732069134712219
[08/27/2025 21:02:39 INFO]: Training loss at epoch 6: 1.133042186498642
[08/27/2025 21:02:51 INFO]: New best epoch, val score: -0.9182456698671394
[08/27/2025 21:02:51 INFO]: Saving model to: maddest-Elbert_trial_83/model_best.pth
[08/27/2025 21:02:52 INFO]: Training loss at epoch 12: 1.019466519355774
[08/27/2025 21:02:54 INFO]: Training loss at epoch 33: 1.0854217410087585
[08/27/2025 21:02:55 INFO]: Training loss at epoch 38: 0.9067336618900299
[08/27/2025 21:03:03 INFO]: Training loss at epoch 12: 1.123159110546112
[08/27/2025 21:03:24 INFO]: Training loss at epoch 37: 0.9905554056167603
[08/27/2025 21:03:33 INFO]: Training loss at epoch 29: 1.0766422748565674
[08/27/2025 21:03:49 INFO]: Training loss at epoch 13: 1.0507749319076538
[08/27/2025 21:03:50 INFO]: Training loss at epoch 11: 1.1453965306282043
[08/27/2025 21:04:07 INFO]: Training loss at epoch 7: 1.3001482486724854
[08/27/2025 21:04:12 INFO]: New best epoch, val score: -0.8910510783074337
[08/27/2025 21:04:12 INFO]: Saving model to: maddest-Elbert_trial_78/model_best.pth
[08/27/2025 21:04:18 INFO]: New best epoch, val score: -0.911214722652878
[08/27/2025 21:04:18 INFO]: Saving model to: maddest-Elbert_trial_83/model_best.pth
[08/27/2025 21:04:20 INFO]: Training loss at epoch 39: 1.1097337305545807
[08/27/2025 21:04:20 INFO]: Training loss at epoch 12: 0.7462048679590225
[08/27/2025 21:04:28 INFO]: Training loss at epoch 25: 0.8180385231971741
[08/27/2025 21:04:42 INFO]: New best epoch, val score: -0.8829150108493121
[08/27/2025 21:04:42 INFO]: Saving model to: maddest-Elbert_trial_76/model_best.pth
[08/27/2025 21:04:47 INFO]: Training stats: {
    "score": -0.972976993471374,
    "rmse": 0.972976993471374
}
[08/27/2025 21:04:47 INFO]: Val stats: {
    "score": -0.8972769347977838,
    "rmse": 0.8972769347977838
}
[08/27/2025 21:04:47 INFO]: Test stats: {
    "score": -0.9613106724510817,
    "rmse": 0.9613106724510817
}
[08/27/2025 21:04:49 INFO]: Training stats: {
    "score": -0.9914777740599623,
    "rmse": 0.9914777740599623
}
[08/27/2025 21:04:49 INFO]: Val stats: {
    "score": -0.9068978171791928,
    "rmse": 0.9068978171791928
}
[08/27/2025 21:04:49 INFO]: Test stats: {
    "score": -0.9717621431129991,
    "rmse": 0.9717621431129991
}
[08/27/2025 21:04:50 INFO]: Training loss at epoch 38: 0.9111510813236237
[08/27/2025 21:05:06 INFO]: Training loss at epoch 13: 1.075792908668518
[08/27/2025 21:05:15 INFO]: Training loss at epoch 10: 1.2423722743988037
[08/27/2025 21:05:25 INFO]: Training loss at epoch 11: 0.9875321984291077
[08/27/2025 21:05:34 INFO]: Training loss at epoch 8: 1.1071738302707672
[08/27/2025 21:05:41 INFO]: Training loss at epoch 4: 1.3515763878822327
[08/27/2025 21:05:43 INFO]: Training loss at epoch 10: 1.0905998349189758
[08/27/2025 21:05:43 INFO]: New best epoch, val score: -0.9039306168113447
[08/27/2025 21:05:43 INFO]: Saving model to: maddest-Elbert_trial_83/model_best.pth
[08/27/2025 21:05:45 INFO]: New best epoch, val score: -0.87907427777109
[08/27/2025 21:05:45 INFO]: Saving model to: maddest-Elbert_trial_81/model_best.pth
[08/27/2025 21:05:51 INFO]: Training loss at epoch 13: 1.0301368236541748
[08/27/2025 21:06:13 INFO]: Training loss at epoch 40: 0.8214377462863922
[08/27/2025 21:06:15 INFO]: Training loss at epoch 39: 0.9415078163146973
[08/27/2025 21:06:15 INFO]: Training loss at epoch 5: 0.8583315908908844
[08/27/2025 21:06:26 INFO]: Training loss at epoch 0: 1.3556768894195557
[08/27/2025 21:06:35 INFO]: Training loss at epoch 11: 1.1197660565376282
[08/27/2025 21:06:36 INFO]: Training loss at epoch 34: 0.9302437603473663
[08/27/2025 21:06:45 INFO]: Training stats: {
    "score": -1.0071667457624405,
    "rmse": 1.0071667457624405
}
[08/27/2025 21:06:45 INFO]: Val stats: {
    "score": -0.8880073970679425,
    "rmse": 0.8880073970679425
}
[08/27/2025 21:06:45 INFO]: Test stats: {
    "score": -0.9880430743711589,
    "rmse": 0.9880430743711589
}
[08/27/2025 21:06:54 INFO]: Training loss at epoch 12: 1.0742903351783752
[08/27/2025 21:06:57 INFO]: Training loss at epoch 26: 1.0971759557724
[08/27/2025 21:07:00 INFO]: Training loss at epoch 9: 1.1896380186080933
[08/27/2025 21:07:05 INFO]: Training loss at epoch 13: 1.225619912147522
[08/27/2025 21:07:06 INFO]: New best epoch, val score: -0.908688764362144
[08/27/2025 21:07:06 INFO]: Saving model to: maddest-Elbert_trial_72/model_best.pth
[08/27/2025 21:07:06 INFO]: Training loss at epoch 37: 0.9678549766540527
[08/27/2025 21:07:13 INFO]: Training loss at epoch 14: 0.8351091742515564
[08/27/2025 21:07:20 INFO]: Training loss at epoch 13: 1.1839102506637573
[08/27/2025 21:07:29 INFO]: Training stats: {
    "score": -1.0908049636669148,
    "rmse": 1.0908049636669148
}
[08/27/2025 21:07:29 INFO]: Val stats: {
    "score": -0.8969344672685668,
    "rmse": 0.8969344672685668
}
[08/27/2025 21:07:29 INFO]: Test stats: {
    "score": -1.066058555850791,
    "rmse": 1.066058555850791
}
[08/27/2025 21:07:36 INFO]: New best epoch, val score: -0.8827819343698151
[08/27/2025 21:07:36 INFO]: Saving model to: maddest-Elbert_trial_84/model_best.pth
[08/27/2025 21:07:38 INFO]: Training loss at epoch 41: 0.9772593677043915
[08/27/2025 21:07:39 INFO]: New best epoch, val score: -0.8969344672685668
[08/27/2025 21:07:39 INFO]: Saving model to: maddest-Elbert_trial_83/model_best.pth
[08/27/2025 21:08:09 INFO]: Training loss at epoch 40: 1.0427364110946655
[08/27/2025 21:08:13 INFO]: Training loss at epoch 11: 0.8692298233509064
[08/27/2025 21:08:20 INFO]: Running Final Evaluation...
[08/27/2025 21:08:22 INFO]: Training loss at epoch 14: 0.7599068135023117
[08/27/2025 21:08:24 INFO]: Training loss at epoch 30: 0.9969054162502289
[08/27/2025 21:08:24 INFO]: Training loss at epoch 12: 1.0503299236297607
[08/27/2025 21:08:35 INFO]: New best epoch, val score: -0.886735306916427
[08/27/2025 21:08:35 INFO]: Saving model to: maddest-Elbert_trial_82/model_best.pth
[08/27/2025 21:08:46 INFO]: New best epoch, val score: -0.8772187706455459
[08/27/2025 21:08:46 INFO]: Saving model to: maddest-Elbert_trial_81/model_best.pth
[08/27/2025 21:08:50 INFO]: Training accuracy: {
    "score": -1.0426808135085628,
    "rmse": 1.0426808135085628
}
[08/27/2025 21:08:50 INFO]: Val accuracy: {
    "score": -0.8839526174593102,
    "rmse": 0.8839526174593102
}
[08/27/2025 21:08:50 INFO]: Test accuracy: {
    "score": -1.0205981455609197,
    "rmse": 1.0205981455609197
}
[08/27/2025 21:08:50 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_70",
    "best_epoch": 9,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0205981455609197,
        "rmse": 1.0205981455609197
    },
    "train_stats": {
        "score": -1.0426808135085628,
        "rmse": 1.0426808135085628
    },
    "val_stats": {
        "score": -0.8839526174593102,
        "rmse": 0.8839526174593102
    }
}
[08/27/2025 21:08:50 INFO]: Procewss finished for trial maddest-Elbert_trial_70
[08/27/2025 21:08:50 INFO]: 
_________________________________________________

[08/27/2025 21:08:50 INFO]: train_net_for_optune.py main() running.
[08/27/2025 21:08:50 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 1.0326776447107553
  attention_dropout: 0.11134179196473752
  ffn_dropout: 0.11134179196473752
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 5.1321321068360584e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_85

[08/27/2025 21:08:50 INFO]: This ft_transformer has 18.819 million parameters.
[08/27/2025 21:08:50 INFO]: Training will start at epoch 0.
[08/27/2025 21:08:50 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 21:08:51 INFO]: Training loss at epoch 14: 0.8354687988758087
[08/27/2025 21:08:56 INFO]: Training loss at epoch 10: 0.9653295874595642
[08/27/2025 21:09:02 INFO]: Training loss at epoch 42: 0.8979975879192352
[08/27/2025 21:09:15 INFO]: Training loss at epoch 11: 1.0001324117183685
[08/27/2025 21:09:23 INFO]: Training loss at epoch 27: 1.0325408577919006
[08/27/2025 21:09:40 INFO]: New best epoch, val score: -0.8962824375140327
[08/27/2025 21:09:40 INFO]: Saving model to: maddest-Elbert_trial_77/model_best.pth
[08/27/2025 21:09:55 INFO]: Training loss at epoch 13: 1.0901129245758057
[08/27/2025 21:10:16 INFO]: Training loss at epoch 35: 0.9542725682258606
[08/27/2025 21:10:21 INFO]: Training loss at epoch 14: 1.3073793053627014
[08/27/2025 21:10:21 INFO]: Training loss at epoch 11: 0.8468747437000275
[08/27/2025 21:10:26 INFO]: Training loss at epoch 43: 1.0094043016433716
[08/27/2025 21:10:40 INFO]: Training loss at epoch 15: 0.8471423983573914
[08/27/2025 21:10:56 INFO]: Training loss at epoch 12: 0.8346503973007202
[08/27/2025 21:11:07 INFO]: Training loss at epoch 14: 1.2047271132469177
[08/27/2025 21:11:14 INFO]: Training loss at epoch 12: 1.4578185677528381
[08/27/2025 21:11:26 INFO]: Training loss at epoch 13: 0.9176108837127686
[08/27/2025 21:11:27 INFO]: New best epoch, val score: -0.8965270235783865
[08/27/2025 21:11:27 INFO]: Saving model to: maddest-Elbert_trial_72/model_best.pth
[08/27/2025 21:11:40 INFO]: Training loss at epoch 15: 0.9299325346946716
[08/27/2025 21:11:48 INFO]: Training loss at epoch 12: 1.0840728282928467
[08/27/2025 21:11:50 INFO]: Training loss at epoch 15: 0.9308798611164093
[08/27/2025 21:11:51 INFO]: Training loss at epoch 28: 1.138167291879654
[08/27/2025 21:11:51 INFO]: Training loss at epoch 44: 1.0134166479110718
[08/27/2025 21:12:02 INFO]: Training loss at epoch 31: 1.1650883257389069
[08/27/2025 21:12:48 INFO]: Training loss at epoch 12: 0.9541952908039093
[08/27/2025 21:12:57 INFO]: Training loss at epoch 14: 0.9373310208320618
[08/27/2025 21:12:58 INFO]: Training loss at epoch 6: 0.9809089601039886
[08/27/2025 21:13:11 INFO]: New best epoch, val score: -0.8787817234246532
[08/27/2025 21:13:11 INFO]: Saving model to: maddest-Elbert_trial_77/model_best.pth
[08/27/2025 21:13:15 INFO]: Training loss at epoch 13: 1.3985660076141357
[08/27/2025 21:13:16 INFO]: Training loss at epoch 45: 0.9630979597568512
[08/27/2025 21:13:23 INFO]: Training loss at epoch 15: 1.1035838723182678
[08/27/2025 21:13:26 INFO]: Running Final Evaluation...
[08/27/2025 21:13:31 INFO]: Training loss at epoch 5: 1.073660135269165
[08/27/2025 21:13:55 INFO]: Training accuracy: {
    "score": -1.0348066057513037,
    "rmse": 1.0348066057513037
}
[08/27/2025 21:13:55 INFO]: Val accuracy: {
    "score": -0.8835031359374267,
    "rmse": 0.8835031359374267
}
[08/27/2025 21:13:55 INFO]: Test accuracy: {
    "score": -1.0112961936194724,
    "rmse": 1.0112961936194724
}
[08/27/2025 21:13:55 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_69",
    "best_epoch": 14,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0112961936194724,
        "rmse": 1.0112961936194724
    },
    "train_stats": {
        "score": -1.0348066057513037,
        "rmse": 1.0348066057513037
    },
    "val_stats": {
        "score": -0.8835031359374267,
        "rmse": 0.8835031359374267
    }
}
[08/27/2025 21:13:55 INFO]: Procewss finished for trial maddest-Elbert_trial_69
[08/27/2025 21:13:55 INFO]: 
_________________________________________________

[08/27/2025 21:13:55 INFO]: train_net_for_optune.py main() running.
[08/27/2025 21:13:55 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 1.4101144534648875
  attention_dropout: 0.12817157056878103
  ffn_dropout: 0.12817157056878103
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.4905071449960717e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_86

[08/27/2025 21:13:56 INFO]: This ft_transformer has 21.788 million parameters.
[08/27/2025 21:13:56 INFO]: Training will start at epoch 0.
[08/27/2025 21:13:56 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 21:13:56 INFO]: Training loss at epoch 36: 0.9061245918273926
[08/27/2025 21:14:04 INFO]: Training loss at epoch 16: 1.1307118237018585
[08/27/2025 21:14:14 INFO]: Training loss at epoch 13: 1.305334746837616
[08/27/2025 21:14:19 INFO]: Training loss at epoch 29: 1.3637806475162506
[08/27/2025 21:14:25 INFO]: Running Final Evaluation...
[08/27/2025 21:14:27 INFO]: Training loss at epoch 14: 0.9383978247642517
[08/27/2025 21:14:40 INFO]: Training loss at epoch 14: 1.220989167690277
[08/27/2025 21:14:49 INFO]: Training loss at epoch 16: 1.0621088743209839
[08/27/2025 21:14:55 INFO]: Training loss at epoch 16: 1.0615183413028717
[08/27/2025 21:15:08 INFO]: Training loss at epoch 15: 1.1271692514419556
[08/27/2025 21:15:09 INFO]: Training stats: {
    "score": -0.9752042264124561,
    "rmse": 0.9752042264124561
}
[08/27/2025 21:15:09 INFO]: Val stats: {
    "score": -0.9407338335512623,
    "rmse": 0.9407338335512623
}
[08/27/2025 21:15:09 INFO]: Test stats: {
    "score": -0.9882182252841811,
    "rmse": 0.9882182252841811
}
[08/27/2025 21:15:18 INFO]: Training loss at epoch 13: 1.29421728849411
[08/27/2025 21:15:28 INFO]: Training loss at epoch 38: 0.8218735158443451
[08/27/2025 21:15:40 INFO]: Training loss at epoch 32: 0.9305050373077393
[08/27/2025 21:15:49 INFO]: New best epoch, val score: -0.891537534947955
[08/27/2025 21:15:49 INFO]: Saving model to: maddest-Elbert_trial_72/model_best.pth
[08/27/2025 21:15:50 INFO]: Training accuracy: {
    "score": -0.989998258792347,
    "rmse": 0.989998258792347
}
[08/27/2025 21:15:50 INFO]: Val accuracy: {
    "score": -0.9096843135186067,
    "rmse": 0.9096843135186067
}
[08/27/2025 21:15:50 INFO]: Test accuracy: {
    "score": -0.9844796681626393,
    "rmse": 0.9844796681626393
}
[08/27/2025 21:15:50 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_45",
    "best_epoch": 5,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9844796681626393,
        "rmse": 0.9844796681626393
    },
    "train_stats": {
        "score": -0.989998258792347,
        "rmse": 0.989998258792347
    },
    "val_stats": {
        "score": -0.9096843135186067,
        "rmse": 0.9096843135186067
    }
}
[08/27/2025 21:15:50 INFO]: Procewss finished for trial maddest-Elbert_trial_45
[08/27/2025 21:15:50 INFO]: 
_________________________________________________

[08/27/2025 21:15:50 INFO]: train_net_for_optune.py main() running.
[08/27/2025 21:15:50 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 1.4156965935413925
  attention_dropout: 0.12126330429198899
  ffn_dropout: 0.12126330429198899
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00011422725284521147
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_87

[08/27/2025 21:15:50 INFO]: This ft_transformer has 8.587 million parameters.
[08/27/2025 21:15:50 INFO]: Training will start at epoch 0.
[08/27/2025 21:15:50 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 21:15:59 INFO]: Training loss at epoch 15: 1.337810218334198
[08/27/2025 21:16:07 INFO]: Training loss at epoch 15: 1.0646737813949585
[08/27/2025 21:16:18 INFO]: Training loss at epoch 13: 1.0831363201141357
[08/27/2025 21:16:23 INFO]: Training loss at epoch 16: 1.0400187969207764
[08/27/2025 21:16:25 INFO]: Training loss at epoch 1: 2.966130316257477
[08/27/2025 21:16:45 INFO]: New best epoch, val score: -0.8801911217518886
[08/27/2025 21:16:45 INFO]: Saving model to: maddest-Elbert_trial_76/model_best.pth
[08/27/2025 21:17:13 INFO]: Training loss at epoch 14: 1.6166687607765198
[08/27/2025 21:17:18 INFO]: Training loss at epoch 0: 1.0207344591617584
[08/27/2025 21:17:29 INFO]: Training loss at epoch 15: 1.0672328770160675
[08/27/2025 21:17:30 INFO]: Training loss at epoch 17: 1.0593668818473816
[08/27/2025 21:17:33 INFO]: Training loss at epoch 16: 1.2681787014007568
[08/27/2025 21:17:37 INFO]: Training loss at epoch 30: 1.100291669368744
[08/27/2025 21:17:49 INFO]: Training loss at epoch 17: 1.168115347623825
[08/27/2025 21:18:12 INFO]: Training loss at epoch 17: 0.9433748126029968
[08/27/2025 21:18:26 INFO]: New best epoch, val score: -1.0349793692599845
[08/27/2025 21:18:26 INFO]: Saving model to: maddest-Elbert_trial_85/model_best.pth
[08/27/2025 21:19:02 INFO]: Training loss at epoch 17: 1.1205130815505981
[08/27/2025 21:19:02 INFO]: Training loss at epoch 16: 1.1540716290473938
[08/27/2025 21:19:09 INFO]: Training loss at epoch 16: 1.2760030627250671
[08/27/2025 21:19:12 INFO]: New best epoch, val score: -0.8926156809653232
[08/27/2025 21:19:12 INFO]: Saving model to: maddest-Elbert_trial_83/model_best.pth
[08/27/2025 21:19:21 INFO]: Training loss at epoch 33: 1.0096195936203003
[08/27/2025 21:19:24 INFO]: Training loss at epoch 17: 0.9641551971435547
[08/27/2025 21:19:36 INFO]: Training loss at epoch 7: 1.0248441100120544
[08/27/2025 21:19:39 INFO]: Training loss at epoch 14: 1.1065604090690613
[08/27/2025 21:19:54 INFO]: Training loss at epoch 14: 1.3592718839645386
[08/27/2025 21:20:08 INFO]: Training loss at epoch 31: 1.0007384419441223
[08/27/2025 21:20:15 INFO]: Training loss at epoch 15: 1.0791308283805847
[08/27/2025 21:20:22 INFO]: Training loss at epoch 0: 1.0212847590446472
[08/27/2025 21:20:28 INFO]: Training loss at epoch 18: 0.9108328521251678
[08/27/2025 21:20:33 INFO]: Training loss at epoch 16: 0.7400462329387665
[08/27/2025 21:20:37 INFO]: New best epoch, val score: -0.8809220036725088
[08/27/2025 21:20:37 INFO]: Saving model to: maddest-Elbert_trial_82/model_best.pth
[08/27/2025 21:20:39 INFO]: New best epoch, val score: -0.8841903565393292
[08/27/2025 21:20:39 INFO]: Saving model to: maddest-Elbert_trial_83/model_best.pth
[08/27/2025 21:20:51 INFO]: Training loss at epoch 18: 1.1884105205535889
[08/27/2025 21:20:59 INFO]: Training loss at epoch 18: 1.000182956457138
[08/27/2025 21:20:59 INFO]: New best epoch, val score: -0.9253795257089941
[08/27/2025 21:20:59 INFO]: Saving model to: maddest-Elbert_trial_87/model_best.pth
[08/27/2025 21:21:26 INFO]: Training loss at epoch 6: 0.9565171599388123
[08/27/2025 21:21:31 INFO]: Training loss at epoch 18: 0.9263135194778442
[08/27/2025 21:21:55 INFO]: Training loss at epoch 19: 0.9027982950210571
[08/27/2025 21:22:05 INFO]: Training loss at epoch 17: 0.9472114741802216
[08/27/2025 21:22:24 INFO]: Training stats: {
    "score": -1.024379097400894,
    "rmse": 1.024379097400894
}
[08/27/2025 21:22:24 INFO]: Val stats: {
    "score": -0.882482316715138,
    "rmse": 0.882482316715138
}
[08/27/2025 21:22:24 INFO]: Test stats: {
    "score": -0.9972927814235171,
    "rmse": 0.9972927814235171
}
[08/27/2025 21:22:26 INFO]: Training loss at epoch 18: 1.0208268761634827
[08/27/2025 21:22:27 INFO]: New best epoch, val score: -0.8888840609206549
[08/27/2025 21:22:27 INFO]: Saving model to: maddest-Elbert_trial_78/model_best.pth
[08/27/2025 21:22:34 INFO]: New best epoch, val score: -0.882482316715138
[08/27/2025 21:22:34 INFO]: Saving model to: maddest-Elbert_trial_83/model_best.pth
[08/27/2025 21:22:36 INFO]: Training loss at epoch 32: 0.7572775781154633
[08/27/2025 21:23:01 INFO]: Training loss at epoch 34: 0.934128075838089
[08/27/2025 21:23:11 INFO]: Training loss at epoch 17: 0.9067078530788422
[08/27/2025 21:23:15 INFO]: Training loss at epoch 16: 0.8658563196659088
[08/27/2025 21:23:27 INFO]: Training loss at epoch 15: 1.1107736825942993
[08/27/2025 21:23:28 INFO]: Training loss at epoch 0: 1.1853322386741638
[08/27/2025 21:23:33 INFO]: Training loss at epoch 17: 1.0423192977905273
[08/27/2025 21:23:50 INFO]: Training loss at epoch 20: 1.033832609653473
[08/27/2025 21:23:50 INFO]: Training loss at epoch 19: 1.0416667461395264
[08/27/2025 21:23:54 INFO]: Training loss at epoch 39: 1.0705156326293945
[08/27/2025 21:24:02 INFO]: Training loss at epoch 15: 0.9448934197425842
[08/27/2025 21:24:24 INFO]: Training loss at epoch 19: 0.8030683696269989
[08/27/2025 21:24:43 INFO]: New best epoch, val score: -0.9500965520698599
[08/27/2025 21:24:43 INFO]: Saving model to: maddest-Elbert_trial_86/model_best.pth
[08/27/2025 21:24:48 INFO]: Training loss at epoch 19: 0.9994838833808899
[08/27/2025 21:24:51 INFO]: Training stats: {
    "score": -1.0032481431926072,
    "rmse": 1.0032481431926072
}
[08/27/2025 21:24:51 INFO]: Val stats: {
    "score": -0.956574083819625,
    "rmse": 0.956574083819625
}
[08/27/2025 21:24:51 INFO]: Test stats: {
    "score": -0.9789114044972591,
    "rmse": 0.9789114044972591
}
[08/27/2025 21:25:02 INFO]: Training loss at epoch 33: 1.0031724870204926
[08/27/2025 21:25:05 INFO]: Training loss at epoch 18: 1.06711345911026
[08/27/2025 21:25:17 INFO]: Training loss at epoch 21: 1.0096078515052795
[08/27/2025 21:25:20 INFO]: Running Final Evaluation...
[08/27/2025 21:25:25 INFO]: Training loss at epoch 19: 1.1320197582244873
[08/27/2025 21:25:28 INFO]: Training loss at epoch 1: 1.127964735031128
[08/27/2025 21:25:32 INFO]: Training stats: {
    "score": -1.001885346045756,
    "rmse": 1.001885346045756
}
[08/27/2025 21:25:32 INFO]: Val stats: {
    "score": -0.8948048561120896,
    "rmse": 0.8948048561120896
}
[08/27/2025 21:25:32 INFO]: Test stats: {
    "score": -0.979170746450243,
    "rmse": 0.979170746450243
}
[08/27/2025 21:25:54 INFO]: Training stats: {
    "score": -0.9907003002977491,
    "rmse": 0.9907003002977491
}
[08/27/2025 21:25:54 INFO]: Val stats: {
    "score": -0.9120531058127006,
    "rmse": 0.9120531058127006
}
[08/27/2025 21:25:54 INFO]: Test stats: {
    "score": -0.9660289603553024,
    "rmse": 0.9660289603553024
}
[08/27/2025 21:26:12 INFO]: Training accuracy: {
    "score": -1.026541083328689,
    "rmse": 1.026541083328689
}
[08/27/2025 21:26:12 INFO]: Val accuracy: {
    "score": -0.8925739448858822,
    "rmse": 0.8925739448858822
}
[08/27/2025 21:26:12 INFO]: Test accuracy: {
    "score": -1.0135809100027187,
    "rmse": 1.0135809100027187
}
[08/27/2025 21:26:12 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_63",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0135809100027187,
        "rmse": 1.0135809100027187
    },
    "train_stats": {
        "score": -1.026541083328689,
        "rmse": 1.026541083328689
    },
    "val_stats": {
        "score": -0.8925739448858822,
        "rmse": 0.8925739448858822
    }
}
[08/27/2025 21:26:12 INFO]: Procewss finished for trial maddest-Elbert_trial_63
[08/27/2025 21:26:12 INFO]: 
_________________________________________________

[08/27/2025 21:26:12 INFO]: train_net_for_optune.py main() running.
[08/27/2025 21:26:12 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 1.4107655426641212
  attention_dropout: 0.12034725605730384
  ffn_dropout: 0.12034725605730384
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 8.578879648931661e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_88

[08/27/2025 21:26:13 INFO]: This ft_transformer has 7.120 million parameters.
[08/27/2025 21:26:13 INFO]: Training will start at epoch 0.
[08/27/2025 21:26:13 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 21:26:14 INFO]: Training loss at epoch 8: 1.0397641062736511
[08/27/2025 21:26:15 INFO]: Training loss at epoch 17: 0.9780943691730499
[08/27/2025 21:26:25 INFO]: Training loss at epoch 2: 1.9033598899841309
[08/27/2025 21:26:26 INFO]: Training stats: {
    "score": -0.9977095592006386,
    "rmse": 0.9977095592006386
}
[08/27/2025 21:26:26 INFO]: Val stats: {
    "score": -0.9236282621436708,
    "rmse": 0.9236282621436708
}
[08/27/2025 21:26:26 INFO]: Test stats: {
    "score": -0.9744899296499959,
    "rmse": 0.9744899296499959
}
[08/27/2025 21:26:34 INFO]: Training loss at epoch 18: 0.952093780040741
[08/27/2025 21:26:40 INFO]: Training loss at epoch 35: 1.2149925827980042
[08/27/2025 21:26:44 INFO]: Training loss at epoch 22: 1.2324082851409912
[08/27/2025 21:26:46 INFO]: Training stats: {
    "score": -0.9984420484378737,
    "rmse": 0.9984420484378737
}
[08/27/2025 21:26:46 INFO]: Val stats: {
    "score": -0.9314778022569755,
    "rmse": 0.9314778022569755
}
[08/27/2025 21:26:46 INFO]: Test stats: {
    "score": -0.9710267066487391,
    "rmse": 0.9710267066487391
}
[08/27/2025 21:26:54 INFO]: Training loss at epoch 1: 1.434597373008728
[08/27/2025 21:26:59 INFO]: Training loss at epoch 16: 0.9377136826515198
[08/27/2025 21:27:12 INFO]: Training loss at epoch 18: 0.9519765675067902
[08/27/2025 21:27:51 INFO]: Training loss at epoch 20: 1.219317376613617
[08/27/2025 21:28:09 INFO]: Training loss at epoch 19: 1.0913761854171753
[08/27/2025 21:28:10 INFO]: Training loss at epoch 23: 1.1331298053264618
[08/27/2025 21:28:23 INFO]: Training loss at epoch 16: 0.7862253189086914
[08/27/2025 21:29:01 INFO]: Training loss at epoch 20: 0.9528571367263794
[08/27/2025 21:29:09 INFO]: Training loss at epoch 20: 1.1781276166439056
[08/27/2025 21:29:09 INFO]: Training stats: {
    "score": -0.9943742794135935,
    "rmse": 0.9943742794135935
}
[08/27/2025 21:29:09 INFO]: Val stats: {
    "score": -0.9254254652514833,
    "rmse": 0.9254254652514833
}
[08/27/2025 21:29:09 INFO]: Test stats: {
    "score": -0.9714001663971912,
    "rmse": 0.9714001663971912
}
[08/27/2025 21:29:12 INFO]: Training loss at epoch 7: 0.8683266937732697
[08/27/2025 21:29:12 INFO]: Training loss at epoch 18: 0.9369752407073975
[08/27/2025 21:29:27 INFO]: Training loss at epoch 20: 1.037297010421753
[08/27/2025 21:29:33 INFO]: New best epoch, val score: -0.8889427935159373
[08/27/2025 21:29:33 INFO]: Saving model to: maddest-Elbert_trial_74/model_best.pth
[08/27/2025 21:29:34 INFO]: Training loss at epoch 19: 0.9370631277561188
[08/27/2025 21:29:35 INFO]: Training loss at epoch 24: 0.8515012562274933
[08/27/2025 21:30:05 INFO]: Training loss at epoch 0: 1.0916160345077515
[08/27/2025 21:30:20 INFO]: Training loss at epoch 36: 0.8972134292125702
[08/27/2025 21:30:30 INFO]: Training loss at epoch 17: 0.9626980721950531
[08/27/2025 21:30:31 INFO]: Training loss at epoch 2: 0.9227090775966644
[08/27/2025 21:30:36 INFO]: Training stats: {
    "score": -0.9899630602759679,
    "rmse": 0.9899630602759679
}
[08/27/2025 21:30:36 INFO]: Val stats: {
    "score": -0.9306237295856391,
    "rmse": 0.9306237295856391
}
[08/27/2025 21:30:36 INFO]: Test stats: {
    "score": -0.9658768888016039,
    "rmse": 0.9658768888016039
}
[08/27/2025 21:30:36 INFO]: New best epoch, val score: -0.8831853527982384
[08/27/2025 21:30:36 INFO]: Saving model to: maddest-Elbert_trial_88/model_best.pth
[08/27/2025 21:30:49 INFO]: Training loss at epoch 21: 0.9846160411834717
[08/27/2025 21:31:00 INFO]: Training loss at epoch 25: 0.782724604010582
[08/27/2025 21:31:07 INFO]: New best epoch, val score: -0.8987093878446974
[08/27/2025 21:31:07 INFO]: Saving model to: maddest-Elbert_trial_87/model_best.pth
[08/27/2025 21:31:13 INFO]: Training loss at epoch 19: 1.1995665729045868
[08/27/2025 21:32:11 INFO]: Training loss at epoch 20: 0.9202655851840973
[08/27/2025 21:32:12 INFO]: Training loss at epoch 19: 0.8031282722949982
[08/27/2025 21:32:25 INFO]: Training loss at epoch 21: 1.0027451515197754
[08/27/2025 21:32:25 INFO]: Training loss at epoch 26: 1.0511781573295593
[08/27/2025 21:32:26 INFO]: Training loss at epoch 21: 1.119280844926834
[08/27/2025 21:32:27 INFO]: Training loss at epoch 21: 1.0262815356254578
[08/27/2025 21:32:33 INFO]: Training stats: {
    "score": -0.9837059345913911,
    "rmse": 0.9837059345913911
}
[08/27/2025 21:32:33 INFO]: Val stats: {
    "score": -0.9105330279788367,
    "rmse": 0.9105330279788367
}
[08/27/2025 21:32:33 INFO]: Test stats: {
    "score": -0.9683622495835328,
    "rmse": 0.9683622495835328
}
[08/27/2025 21:32:42 INFO]: Training loss at epoch 17: 0.9853382110595703
[08/27/2025 21:32:48 INFO]: New best epoch, val score: -0.886433200808729
[08/27/2025 21:32:48 INFO]: Saving model to: maddest-Elbert_trial_74/model_best.pth
[08/27/2025 21:32:52 INFO]: Training loss at epoch 9: 0.9566340148448944
[08/27/2025 21:33:14 INFO]: Training stats: {
    "score": -1.0209599791784834,
    "rmse": 1.0209599791784834
}
[08/27/2025 21:33:14 INFO]: Val stats: {
    "score": -1.0062830490234969,
    "rmse": 1.0062830490234969
}
[08/27/2025 21:33:14 INFO]: Test stats: {
    "score": -0.994374747274314,
    "rmse": 0.994374747274314
}
[08/27/2025 21:33:37 INFO]: Training loss at epoch 20: 1.0242060720920563
[08/27/2025 21:33:47 INFO]: Training loss at epoch 22: 1.0398805439472198
[08/27/2025 21:33:51 INFO]: Training loss at epoch 27: 1.1899151504039764
[08/27/2025 21:33:59 INFO]: Training loss at epoch 37: 0.8588772416114807
[08/27/2025 21:34:02 INFO]: Training loss at epoch 18: 1.1011996269226074
[08/27/2025 21:34:13 INFO]: Training loss at epoch 1: 1.2409309148788452
[08/27/2025 21:34:30 INFO]: Training loss at epoch 1: 1.1490708589553833
[08/27/2025 21:35:09 INFO]: Training loss at epoch 40: 0.8837488889694214
[08/27/2025 21:35:10 INFO]: Training stats: {
    "score": -1.0086784068576926,
    "rmse": 1.0086784068576926
}
[08/27/2025 21:35:10 INFO]: Val stats: {
    "score": -0.9844950992135791,
    "rmse": 0.9844950992135791
}
[08/27/2025 21:35:10 INFO]: Test stats: {
    "score": -0.9858239329375961,
    "rmse": 0.9858239329375961
}
[08/27/2025 21:35:15 INFO]: Training loss at epoch 21: 0.9394745528697968
[08/27/2025 21:35:18 INFO]: Training loss at epoch 28: 1.0753422975540161
[08/27/2025 21:35:28 INFO]: Training loss at epoch 22: 1.1534857749938965
[08/27/2025 21:35:30 INFO]: New best epoch, val score: -0.8981751567955839
[08/27/2025 21:35:30 INFO]: Saving model to: maddest-Elbert_trial_86/model_best.pth
[08/27/2025 21:35:37 INFO]: Training loss at epoch 3: 1.1747972965240479
[08/27/2025 21:35:43 INFO]: Training loss at epoch 22: 0.9198906719684601
[08/27/2025 21:35:52 INFO]: Training loss at epoch 22: 0.9279071688652039
[08/27/2025 21:36:13 INFO]: New best epoch, val score: -0.8897481812144883
[08/27/2025 21:36:13 INFO]: Saving model to: maddest-Elbert_trial_87/model_best.pth
[08/27/2025 21:36:15 INFO]: Training loss at epoch 20: 0.9732016623020172
[08/27/2025 21:36:23 INFO]: Training loss at epoch 3: 1.1429352462291718
[08/27/2025 21:36:28 INFO]: Training loss at epoch 2: 1.3972875475883484
[08/27/2025 21:36:36 INFO]: Training loss at epoch 20: 0.9267909228801727
[08/27/2025 21:36:39 INFO]: Training loss at epoch 21: 0.9429421126842499
[08/27/2025 21:36:44 INFO]: Training loss at epoch 29: 1.080655038356781
[08/27/2025 21:36:48 INFO]: Training loss at epoch 23: 0.9656179249286652
[08/27/2025 21:37:04 INFO]: Training loss at epoch 8: 1.1150639057159424
[08/27/2025 21:37:05 INFO]: Training loss at epoch 18: 0.9538905024528503
[08/27/2025 21:37:13 INFO]: Training stats: {
    "score": -0.997658070946942,
    "rmse": 0.997658070946942
}
[08/27/2025 21:37:13 INFO]: Val stats: {
    "score": -0.9155959283076824,
    "rmse": 0.9155959283076824
}
[08/27/2025 21:37:13 INFO]: Test stats: {
    "score": -0.9710504786025907,
    "rmse": 0.9710504786025907
}
[08/27/2025 21:37:35 INFO]: Training loss at epoch 19: 1.1405325531959534
[08/27/2025 21:37:38 INFO]: Training loss at epoch 38: 0.9799871444702148
[08/27/2025 21:38:17 INFO]: Training loss at epoch 22: 1.1274908185005188
[08/27/2025 21:38:30 INFO]: Training loss at epoch 23: 0.9212502241134644
[08/27/2025 21:38:38 INFO]: Training loss at epoch 30: 0.9979530870914459
[08/27/2025 21:38:47 INFO]: Training stats: {
    "score": -0.9961916613623182,
    "rmse": 0.9961916613623182
}
[08/27/2025 21:38:47 INFO]: Val stats: {
    "score": -0.9309736768648238,
    "rmse": 0.9309736768648238
}
[08/27/2025 21:38:47 INFO]: Test stats: {
    "score": -0.9687122070422306,
    "rmse": 0.9687122070422306
}
[08/27/2025 21:38:56 INFO]: Training loss at epoch 2: 1.2221072316169739
[08/27/2025 21:38:59 INFO]: Training loss at epoch 23: 1.0093731880187988
[08/27/2025 21:39:15 INFO]: Training loss at epoch 21: 1.0259138345718384
[08/27/2025 21:39:19 INFO]: Training loss at epoch 23: 0.9949029386043549
[08/27/2025 21:39:40 INFO]: Training loss at epoch 22: 0.8414857387542725
[08/27/2025 21:39:48 INFO]: Training loss at epoch 24: 1.1988121271133423
[08/27/2025 21:40:05 INFO]: Training loss at epoch 31: 1.1314928531646729
[08/27/2025 21:40:38 INFO]: Training loss at epoch 21: 0.7838082909584045
[08/27/2025 21:40:42 INFO]: Training loss at epoch 4: 1.10398268699646
[08/27/2025 21:41:16 INFO]: Training loss at epoch 39: 0.9180268049240112
[08/27/2025 21:41:17 INFO]: Training loss at epoch 23: 0.9874748587608337
[08/27/2025 21:41:25 INFO]: Training loss at epoch 19: 0.958391547203064
[08/27/2025 21:41:30 INFO]: Training loss at epoch 24: 1.086411476135254
[08/27/2025 21:41:31 INFO]: Training loss at epoch 32: 1.1307693123817444
[08/27/2025 21:41:48 INFO]: Training loss at epoch 10: 1.0673081278800964
[08/27/2025 21:42:13 INFO]: Training loss at epoch 22: 0.9122086763381958
[08/27/2025 21:42:14 INFO]: Training loss at epoch 24: 0.9976165592670441
[08/27/2025 21:42:17 INFO]: Training loss at epoch 20: 1.1087279319763184
[08/27/2025 21:42:30 INFO]: Training stats: {
    "score": -0.9643388427192264,
    "rmse": 0.9643388427192264
}
[08/27/2025 21:42:30 INFO]: Val stats: {
    "score": -0.8933392432606003,
    "rmse": 0.8933392432606003
}
[08/27/2025 21:42:30 INFO]: Test stats: {
    "score": -0.9592717934155831,
    "rmse": 0.9592717934155831
}
[08/27/2025 21:42:40 INFO]: Training loss at epoch 23: 1.0637585520744324
[08/27/2025 21:42:43 INFO]: Training loss at epoch 24: 0.8534030616283417
[08/27/2025 21:42:45 INFO]: Training loss at epoch 25: 1.1641656160354614
[08/27/2025 21:42:54 INFO]: Training stats: {
    "score": -0.937690418176612,
    "rmse": 0.937690418176612
}
[08/27/2025 21:42:54 INFO]: Val stats: {
    "score": -0.9012677362468716,
    "rmse": 0.9012677362468716
}
[08/27/2025 21:42:54 INFO]: Test stats: {
    "score": -0.9859349233944411,
    "rmse": 0.9859349233944411
}
[08/27/2025 21:42:56 INFO]: Training loss at epoch 33: 1.0193155407905579
[08/27/2025 21:43:22 INFO]: Training loss at epoch 3: 1.0421184301376343
[08/27/2025 21:43:31 INFO]: Training loss at epoch 41: 1.5305302143096924
[08/27/2025 21:44:19 INFO]: Training loss at epoch 24: 0.9977830648422241
[08/27/2025 21:44:21 INFO]: Training loss at epoch 34: 0.9956809282302856
[08/27/2025 21:44:29 INFO]: Training loss at epoch 25: 1.2113668024539948
[08/27/2025 21:44:37 INFO]: Training loss at epoch 22: 1.0460970997810364
[08/27/2025 21:44:53 INFO]: Training loss at epoch 9: 1.25229412317276
[08/27/2025 21:45:00 INFO]: Training loss at epoch 2: 1.0650483965873718
[08/27/2025 21:45:13 INFO]: Training loss at epoch 23: 1.1177543103694916
[08/27/2025 21:45:28 INFO]: Training loss at epoch 25: 0.9280786216259003
[08/27/2025 21:45:41 INFO]: Training loss at epoch 24: 1.0723442137241364
[08/27/2025 21:45:45 INFO]: Training loss at epoch 5: 1.037123203277588
[08/27/2025 21:45:45 INFO]: Training loss at epoch 26: 0.8383180499076843
[08/27/2025 21:45:47 INFO]: Training loss at epoch 35: 1.1411630809307098
[08/27/2025 21:45:50 INFO]: Training loss at epoch 21: 1.1348151564598083
[08/27/2025 21:45:59 INFO]: Training loss at epoch 3: 1.1932781338691711
[08/27/2025 21:46:10 INFO]: Training loss at epoch 25: 1.163480669260025
[08/27/2025 21:46:11 INFO]: Training loss at epoch 40: 0.84775111079216
[08/27/2025 21:46:20 INFO]: Training loss at epoch 4: 1.2042048871517181
[08/27/2025 21:47:07 INFO]: New best epoch, val score: -0.990496349109022
[08/27/2025 21:47:07 INFO]: Saving model to: maddest-Elbert_trial_85/model_best.pth
[08/27/2025 21:47:13 INFO]: Training loss at epoch 36: 0.9262977838516235
[08/27/2025 21:47:15 INFO]: Training loss at epoch 20: 0.9747853577136993
[08/27/2025 21:47:22 INFO]: Training loss at epoch 25: 0.98395836353302
[08/27/2025 21:47:31 INFO]: Training loss at epoch 26: 0.9894657731056213
[08/27/2025 21:47:33 INFO]: Training stats: {
    "score": -1.0418085885959256,
    "rmse": 1.0418085885959256
}
[08/27/2025 21:47:33 INFO]: Val stats: {
    "score": -1.0514418031410666,
    "rmse": 1.0514418031410666
}
[08/27/2025 21:47:33 INFO]: Test stats: {
    "score": -1.0176795824730962,
    "rmse": 1.0176795824730962
}
[08/27/2025 21:47:48 INFO]: Training loss at epoch 4: 1.3558815717697144
[08/27/2025 21:48:14 INFO]: Training loss at epoch 24: 1.1024017930030823
[08/27/2025 21:48:30 INFO]: Training loss at epoch 11: 1.0975869297981262
[08/27/2025 21:48:39 INFO]: Training loss at epoch 37: 1.167101502418518
[08/27/2025 21:48:42 INFO]: Training loss at epoch 23: 1.059357762336731
[08/27/2025 21:48:43 INFO]: Training loss at epoch 25: 0.9072343707084656
[08/27/2025 21:48:46 INFO]: Training loss at epoch 26: 1.2480082511901855
[08/27/2025 21:48:47 INFO]: Training loss at epoch 27: 1.1478860974311829
[08/27/2025 21:49:18 INFO]: New best epoch, val score: -0.877863603724616
[08/27/2025 21:49:18 INFO]: Saving model to: maddest-Elbert_trial_79/model_best.pth
[08/27/2025 21:49:22 INFO]: Training loss at epoch 22: 1.2720598876476288
[08/27/2025 21:49:36 INFO]: Training loss at epoch 26: 0.9286471903324127
[08/27/2025 21:49:50 INFO]: Training loss at epoch 41: 0.9490641355514526
[08/27/2025 21:50:06 INFO]: Training loss at epoch 38: 0.8576011061668396
[08/27/2025 21:50:26 INFO]: Training loss at epoch 26: 0.9426376521587372
[08/27/2025 21:50:32 INFO]: Training loss at epoch 27: 1.4880089461803436
[08/27/2025 21:50:50 INFO]: Training loss at epoch 6: 0.9929103851318359
[08/27/2025 21:51:15 INFO]: Training loss at epoch 25: 1.0179946422576904
[08/27/2025 21:51:32 INFO]: Training loss at epoch 39: 1.0677948892116547
[08/27/2025 21:51:38 INFO]: Training loss at epoch 21: 1.065505862236023
[08/27/2025 21:51:45 INFO]: Training loss at epoch 26: 1.1653693616390228
[08/27/2025 21:51:48 INFO]: Training loss at epoch 28: 1.1933874189853668
[08/27/2025 21:51:55 INFO]: Training loss at epoch 42: 1.063284009695053
[08/27/2025 21:52:01 INFO]: Training stats: {
    "score": -1.0023349498577871,
    "rmse": 1.0023349498577871
}
[08/27/2025 21:52:01 INFO]: Val stats: {
    "score": -0.9603765210338063,
    "rmse": 0.9603765210338063
}
[08/27/2025 21:52:01 INFO]: Test stats: {
    "score": -0.9765936191274858,
    "rmse": 0.9765936191274858
}
[08/27/2025 21:52:04 INFO]: Training loss at epoch 27: 0.9206340610980988
[08/27/2025 21:52:16 INFO]: Training loss at epoch 5: 1.4324799180030823
[08/27/2025 21:52:46 INFO]: Training loss at epoch 24: 0.9936152100563049
[08/27/2025 21:52:53 INFO]: Training loss at epoch 23: 1.1089643836021423
[08/27/2025 21:53:01 INFO]: Training loss at epoch 27: 0.9605857133865356
[08/27/2025 21:53:27 INFO]: Training loss at epoch 42: 1.0079650580883026
[08/27/2025 21:53:27 INFO]: Training loss at epoch 40: 0.8623029291629791
[08/27/2025 21:53:28 INFO]: Training loss at epoch 27: 0.9597777128219604
[08/27/2025 21:53:30 INFO]: Training loss at epoch 28: 0.922274261713028
[08/27/2025 21:54:14 INFO]: Training loss at epoch 26: 0.9773238599300385
[08/27/2025 21:54:45 INFO]: Training loss at epoch 27: 0.8288803696632385
[08/27/2025 21:54:46 INFO]: Training loss at epoch 29: 0.9522460997104645
[08/27/2025 21:54:54 INFO]: Training loss at epoch 41: 0.8747024536132812
[08/27/2025 21:55:13 INFO]: Training loss at epoch 12: 1.016414850950241
[08/27/2025 21:55:19 INFO]: Training loss at epoch 28: 0.8358902931213379
[08/27/2025 21:55:23 INFO]: Training loss at epoch 10: 1.1282513737678528
[08/27/2025 21:55:34 INFO]: Training loss at epoch 4: 1.088973581790924
[08/27/2025 21:55:46 INFO]: Training stats: {
    "score": -0.9960111735164495,
    "rmse": 0.9960111735164495
}
[08/27/2025 21:55:46 INFO]: Val stats: {
    "score": -0.9295436304636226,
    "rmse": 0.9295436304636226
}
[08/27/2025 21:55:46 INFO]: Test stats: {
    "score": -0.9712940769311944,
    "rmse": 0.9712940769311944
}
[08/27/2025 21:55:47 INFO]: Training loss at epoch 3: 0.9796606600284576
[08/27/2025 21:55:52 INFO]: Training loss at epoch 7: 1.0896925926208496
[08/27/2025 21:55:59 INFO]: Training loss at epoch 22: 0.920307993888855
[08/27/2025 21:56:17 INFO]: Training loss at epoch 5: 0.7794825285673141
[08/27/2025 21:56:19 INFO]: Training loss at epoch 42: 1.144132912158966
[08/27/2025 21:56:25 INFO]: Training loss at epoch 24: 0.8455778360366821
[08/27/2025 21:56:27 INFO]: Training loss at epoch 28: 0.9440053701400757
[08/27/2025 21:56:30 INFO]: Training loss at epoch 28: 1.3876467645168304
[08/27/2025 21:56:32 INFO]: Training loss at epoch 29: 0.9938944578170776
[08/27/2025 21:56:39 INFO]: Training loss at epoch 6: 0.8792474567890167
[08/27/2025 21:56:41 INFO]: New best epoch, val score: -0.9633363020918938
[08/27/2025 21:56:41 INFO]: Saving model to: maddest-Elbert_trial_85/model_best.pth
[08/27/2025 21:56:51 INFO]: Training loss at epoch 25: 1.0624107420444489
[08/27/2025 21:57:06 INFO]: Training loss at epoch 43: 0.9206806421279907
[08/27/2025 21:57:15 INFO]: Training loss at epoch 27: 1.0436695516109467
[08/27/2025 21:57:34 INFO]: Training stats: {
    "score": -0.9956865337348585,
    "rmse": 0.9956865337348585
}
[08/27/2025 21:57:34 INFO]: Val stats: {
    "score": -0.9152347758399447,
    "rmse": 0.9152347758399447
}
[08/27/2025 21:57:34 INFO]: Test stats: {
    "score": -0.9728638965019704,
    "rmse": 0.9728638965019704
}
[08/27/2025 21:57:46 INFO]: Training loss at epoch 43: 1.0750903487205505
[08/27/2025 21:57:47 INFO]: Training loss at epoch 28: 0.8585801720619202
[08/27/2025 21:58:37 INFO]: Training loss at epoch 29: 0.8418598175048828
[08/27/2025 21:58:47 INFO]: Training loss at epoch 30: 1.166774332523346
[08/27/2025 21:59:12 INFO]: Training loss at epoch 44: 0.9080122709274292
[08/27/2025 21:59:33 INFO]: Training loss at epoch 29: 1.1923494040966034
[08/27/2025 21:59:42 INFO]: Training stats: {
    "score": -1.0009024220119769,
    "rmse": 1.0009024220119769
}
[08/27/2025 21:59:42 INFO]: Val stats: {
    "score": -0.8906922261479006,
    "rmse": 0.8906922261479006
}
[08/27/2025 21:59:42 INFO]: Test stats: {
    "score": -0.9811293155569498,
    "rmse": 0.9811293155569498
}
[08/27/2025 21:59:53 INFO]: Training loss at epoch 29: 0.9721670150756836
[08/27/2025 21:59:57 INFO]: Training loss at epoch 25: 0.8275499641895294
[08/27/2025 22:00:13 INFO]: Training loss at epoch 28: 0.9863097071647644
[08/27/2025 22:00:16 INFO]: Training loss at epoch 43: 1.0057260394096375
[08/27/2025 22:00:20 INFO]: Training loss at epoch 23: 0.8299337327480316
[08/27/2025 22:00:32 INFO]: Training loss at epoch 30: 1.2709266543388367
[08/27/2025 22:00:34 INFO]: Training stats: {
    "score": -0.9990974272473303,
    "rmse": 0.9990974272473303
}
[08/27/2025 22:00:34 INFO]: Val stats: {
    "score": -0.9024393628951346,
    "rmse": 0.9024393628951346
}
[08/27/2025 22:00:34 INFO]: Test stats: {
    "score": -0.977087708838882,
    "rmse": 0.977087708838882
}
[08/27/2025 22:00:36 INFO]: Training loss at epoch 45: 0.8734122812747955
[08/27/2025 22:00:43 INFO]: Training loss at epoch 44: 0.9195644855499268
[08/27/2025 22:00:45 INFO]: Training loss at epoch 29: 0.8388265669345856
[08/27/2025 22:00:50 INFO]: Training loss at epoch 26: 1.1793024241924286
[08/27/2025 22:00:55 INFO]: Training loss at epoch 8: 0.9923344850540161
[08/27/2025 22:01:02 INFO]: Training stats: {
    "score": -0.9913682839371372,
    "rmse": 0.9913682839371372
}
[08/27/2025 22:01:02 INFO]: Val stats: {
    "score": -0.9275770177647499,
    "rmse": 0.9275770177647499
}
[08/27/2025 22:01:02 INFO]: Test stats: {
    "score": -0.9698438526249724,
    "rmse": 0.9698438526249724
}
[08/27/2025 22:01:04 INFO]: Training loss at epoch 7: 1.0861571431159973
[08/27/2025 22:01:46 INFO]: Training loss at epoch 31: 1.0032880902290344
[08/27/2025 22:01:48 INFO]: Training stats: {
    "score": -0.9879546793685586,
    "rmse": 0.9879546793685586
}
[08/27/2025 22:01:48 INFO]: Val stats: {
    "score": -0.9388484790819192,
    "rmse": 0.9388484790819192
}
[08/27/2025 22:01:48 INFO]: Test stats: {
    "score": -0.967721628757765,
    "rmse": 0.967721628757765
}
[08/27/2025 22:01:54 INFO]: Training loss at epoch 13: 1.0215848684310913
[08/27/2025 22:02:04 INFO]: Training loss at epoch 46: 1.0891713500022888
[08/27/2025 22:02:57 INFO]: Training loss at epoch 30: 1.1365410685539246
[08/27/2025 22:03:12 INFO]: Training loss at epoch 11: 1.0042528212070465
[08/27/2025 22:03:13 INFO]: Training loss at epoch 29: 0.8744322955608368
[08/27/2025 22:03:27 INFO]: Training loss at epoch 26: 1.0589264631271362
[08/27/2025 22:03:30 INFO]: Training loss at epoch 47: 1.0326876044273376
[08/27/2025 22:03:34 INFO]: Training loss at epoch 31: 0.9723690748214722
[08/27/2025 22:03:37 INFO]: Training loss at epoch 30: 0.9180262088775635
[08/27/2025 22:04:14 INFO]: Training stats: {
    "score": -0.9991008442846449,
    "rmse": 0.9991008442846449
}
[08/27/2025 22:04:14 INFO]: Val stats: {
    "score": -0.9117953477200744,
    "rmse": 0.9117953477200744
}
[08/27/2025 22:04:14 INFO]: Test stats: {
    "score": -0.9717260522974615,
    "rmse": 0.9717260522974615
}
[08/27/2025 22:04:22 INFO]: Training loss at epoch 45: 1.2109246850013733
[08/27/2025 22:04:29 INFO]: Training loss at epoch 30: 0.9822812974452972
[08/27/2025 22:04:42 INFO]: Training loss at epoch 24: 0.8517246544361115
[08/27/2025 22:04:45 INFO]: Training loss at epoch 32: 0.8338067829608917
[08/27/2025 22:04:47 INFO]: Running Final Evaluation...
[08/27/2025 22:04:49 INFO]: Training loss at epoch 30: 1.0870976150035858
[08/27/2025 22:04:54 INFO]: Training loss at epoch 27: 0.8310728073120117
[08/27/2025 22:04:57 INFO]: Training loss at epoch 48: 0.9742027521133423
[08/27/2025 22:05:07 INFO]: Training loss at epoch 5: 1.5055675506591797
[08/27/2025 22:05:30 INFO]: Training loss at epoch 8: 1.0048587918281555
[08/27/2025 22:06:00 INFO]: Training loss at epoch 9: 0.8564610481262207
[08/27/2025 22:06:03 INFO]: Training accuracy: {
    "score": -1.026184994391781,
    "rmse": 1.026184994391781
}
[08/27/2025 22:06:03 INFO]: Val accuracy: {
    "score": -0.8737762998751186,
    "rmse": 0.8737762998751186
}
[08/27/2025 22:06:03 INFO]: Test accuracy: {
    "score": -1.0011556330229041,
    "rmse": 1.0011556330229041
}
[08/27/2025 22:06:03 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_46",
    "best_epoch": 14,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0011556330229041,
        "rmse": 1.0011556330229041
    },
    "train_stats": {
        "score": -1.026184994391781,
        "rmse": 1.026184994391781
    },
    "val_stats": {
        "score": -0.8737762998751186,
        "rmse": 0.8737762998751186
    }
}
[08/27/2025 22:06:03 INFO]: Procewss finished for trial maddest-Elbert_trial_46
[08/27/2025 22:06:03 INFO]: 
_________________________________________________

[08/27/2025 22:06:03 INFO]: train_net_for_optune.py main() running.
[08/27/2025 22:06:03 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 1.4244341483923004
  attention_dropout: 0.13292513752440538
  ffn_dropout: 0.13292513752440538
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 5.058734260050631e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_89

[08/27/2025 22:06:03 INFO]: This ft_transformer has 18.171 million parameters.
[08/27/2025 22:06:03 INFO]: Training will start at epoch 0.
[08/27/2025 22:06:03 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 22:06:14 INFO]: Training loss at epoch 31: 0.9757696688175201
[08/27/2025 22:06:15 INFO]: New best epoch, val score: -0.955863328447671
[08/27/2025 22:06:15 INFO]: Saving model to: maddest-Elbert_trial_85/model_best.pth
[08/27/2025 22:06:16 INFO]: Training loss at epoch 6: 1.0105664432048798
[08/27/2025 22:06:25 INFO]: Training loss at epoch 49: 0.7381108105182648
[08/27/2025 22:06:34 INFO]: Training loss at epoch 32: 0.8909013867378235
[08/27/2025 22:06:34 INFO]: Training loss at epoch 4: 1.0021613240242004
[08/27/2025 22:06:40 INFO]: Training loss at epoch 31: 1.1312995254993439
[08/27/2025 22:06:54 INFO]: Training stats: {
    "score": -0.9956879591782498,
    "rmse": 0.9956879591782498
}
[08/27/2025 22:06:54 INFO]: Val stats: {
    "score": -0.9162958006631585,
    "rmse": 0.9162958006631585
}
[08/27/2025 22:06:54 INFO]: Test stats: {
    "score": -0.9701392137185414,
    "rmse": 0.9701392137185414
}
[08/27/2025 22:07:00 INFO]: Training loss at epoch 27: 1.010416030883789
[08/27/2025 22:07:14 INFO]: Training loss at epoch 30: 1.1228686571121216
[08/27/2025 22:07:43 INFO]: Training stats: {
    "score": -1.0052012978713978,
    "rmse": 1.0052012978713978
}
[08/27/2025 22:07:43 INFO]: Val stats: {
    "score": -1.0188533086642124,
    "rmse": 1.0188533086642124
}
[08/27/2025 22:07:43 INFO]: Test stats: {
    "score": -1.0021926560056615,
    "rmse": 1.0021926560056615
}
[08/27/2025 22:07:48 INFO]: Training loss at epoch 33: 0.9195731580257416
[08/27/2025 22:07:50 INFO]: Training loss at epoch 31: 1.2123644351959229
[08/27/2025 22:07:51 INFO]: New best epoch, val score: -0.8945481881769255
[08/27/2025 22:07:51 INFO]: Saving model to: maddest-Elbert_trial_86/model_best.pth
[08/27/2025 22:07:54 INFO]: Training loss at epoch 31: 0.9011446833610535
[08/27/2025 22:08:21 INFO]: Training loss at epoch 50: 0.9403847754001617
[08/27/2025 22:08:32 INFO]: Running Final Evaluation...
[08/27/2025 22:08:34 INFO]: Training loss at epoch 14: 1.0781549513339996
[08/27/2025 22:08:40 INFO]: Training loss at epoch 44: 1.1563482284545898
[08/27/2025 22:08:54 INFO]: Training loss at epoch 28: 1.0429800152778625
[08/27/2025 22:09:00 INFO]: Training accuracy: {
    "score": -1.0243790973202964,
    "rmse": 1.0243790973202964
}
[08/27/2025 22:09:00 INFO]: Val accuracy: {
    "score": -0.882482316715138,
    "rmse": 0.882482316715138
}
[08/27/2025 22:09:00 INFO]: Test accuracy: {
    "score": -0.9972927814235171,
    "rmse": 0.9972927814235171
}
[08/27/2025 22:09:00 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_83",
    "best_epoch": 19,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9972927814235171,
        "rmse": 0.9972927814235171
    },
    "train_stats": {
        "score": -1.0243790973202964,
        "rmse": 1.0243790973202964
    },
    "val_stats": {
        "score": -0.882482316715138,
        "rmse": 0.882482316715138
    }
}
[08/27/2025 22:09:00 INFO]: Procewss finished for trial maddest-Elbert_trial_83
[08/27/2025 22:09:00 INFO]: 
_________________________________________________

[08/27/2025 22:09:00 INFO]: train_net_for_optune.py main() running.
[08/27/2025 22:09:00 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 1.41703983658608
  attention_dropout: 0.14937570627531727
  ffn_dropout: 0.14937570627531727
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.2561027080026722e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_90

[08/27/2025 22:09:01 INFO]: This ft_transformer has 18.130 million parameters.
[08/27/2025 22:09:01 INFO]: Training will start at epoch 0.
[08/27/2025 22:09:01 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 22:09:05 INFO]: Training loss at epoch 25: 0.8284238874912262
[08/27/2025 22:09:32 INFO]: Training loss at epoch 32: 1.1680902540683746
[08/27/2025 22:09:35 INFO]: Training loss at epoch 33: 1.141465574502945
[08/27/2025 22:09:43 INFO]: Training loss at epoch 32: 0.8839302062988281
[08/27/2025 22:09:56 INFO]: Training loss at epoch 9: 1.0308822095394135
[08/27/2025 22:10:13 INFO]: Training loss at epoch 31: 1.1266536116600037
[08/27/2025 22:10:32 INFO]: Training loss at epoch 28: 0.8481165766716003
[08/27/2025 22:10:47 INFO]: Training loss at epoch 34: 0.9584793746471405
[08/27/2025 22:10:54 INFO]: Training loss at epoch 32: 1.1918076276779175
[08/27/2025 22:11:03 INFO]: Training loss at epoch 12: 1.2679722309112549
[08/27/2025 22:11:20 INFO]: Training loss at epoch 32: 0.9343146085739136
[08/27/2025 22:11:25 INFO]: Training stats: {
    "score": -1.024538047728871,
    "rmse": 1.024538047728871
}
[08/27/2025 22:11:25 INFO]: Val stats: {
    "score": -1.0138156505605018,
    "rmse": 1.0138156505605018
}
[08/27/2025 22:11:25 INFO]: Test stats: {
    "score": -0.9986454864450304,
    "rmse": 0.9986454864450304
}
[08/27/2025 22:12:34 INFO]: Training loss at epoch 34: 1.0294235050678253
[08/27/2025 22:12:45 INFO]: Training loss at epoch 33: 0.7987509369850159
[08/27/2025 22:12:45 INFO]: Training loss at epoch 10: 0.8995434939861298
[08/27/2025 22:12:46 INFO]: Training loss at epoch 33: 0.8323113024234772
[08/27/2025 22:12:57 INFO]: Training loss at epoch 29: 1.0418307781219482
[08/27/2025 22:13:12 INFO]: Training loss at epoch 32: 0.9460731148719788
[08/27/2025 22:13:25 INFO]: Training loss at epoch 26: 1.0880834460258484
[08/27/2025 22:13:44 INFO]: Training loss at epoch 35: 0.9956219494342804
[08/27/2025 22:13:54 INFO]: Training loss at epoch 33: 0.9978496730327606
[08/27/2025 22:14:02 INFO]: Training loss at epoch 29: 1.0788954496383667
[08/27/2025 22:14:12 INFO]: Training loss at epoch 0: 1.4885348081588745
[08/27/2025 22:14:20 INFO]: Training stats: {
    "score": -1.0150080154003673,
    "rmse": 1.0150080154003673
}
[08/27/2025 22:14:20 INFO]: Val stats: {
    "score": -1.032268254818111,
    "rmse": 1.032268254818111
}
[08/27/2025 22:14:20 INFO]: Test stats: {
    "score": -1.0097774169814167,
    "rmse": 1.0097774169814167
}
[08/27/2025 22:14:41 INFO]: Training loss at epoch 6: 1.3206303715705872
[08/27/2025 22:14:44 INFO]: Training loss at epoch 33: 1.1395825743675232
[08/27/2025 22:15:11 INFO]: Training loss at epoch 15: 0.8135579228401184
[08/27/2025 22:15:14 INFO]: Training stats: {
    "score": -0.9948468447153214,
    "rmse": 0.9948468447153214
}
[08/27/2025 22:15:14 INFO]: Val stats: {
    "score": -0.9265953586721529,
    "rmse": 0.9265953586721529
}
[08/27/2025 22:15:14 INFO]: Test stats: {
    "score": -0.9690583473292917,
    "rmse": 0.9690583473292917
}
[08/27/2025 22:15:17 INFO]: New best epoch, val score: -1.2222476661389043
[08/27/2025 22:15:17 INFO]: Saving model to: maddest-Elbert_trial_89/model_best.pth
[08/27/2025 22:15:35 INFO]: Training loss at epoch 35: 1.280825525522232
[08/27/2025 22:15:46 INFO]: Training loss at epoch 34: 0.9874719977378845
[08/27/2025 22:15:47 INFO]: Training loss at epoch 10: 1.095029592514038
[08/27/2025 22:15:48 INFO]: New best epoch, val score: -0.9412629960683635
[08/27/2025 22:15:48 INFO]: Saving model to: maddest-Elbert_trial_85/model_best.pth
[08/27/2025 22:16:03 INFO]: Training loss at epoch 34: 0.9813164174556732
[08/27/2025 22:16:11 INFO]: Training loss at epoch 33: 1.0482523441314697
[08/27/2025 22:16:12 INFO]: Training loss at epoch 7: 1.2116206586360931
[08/27/2025 22:16:44 INFO]: Training loss at epoch 36: 1.0054997205734253
[08/27/2025 22:16:56 INFO]: Training loss at epoch 34: 0.8619339764118195
[08/27/2025 22:17:00 INFO]: Training loss at epoch 45: 0.9496825933456421
[08/27/2025 22:17:08 INFO]: Training loss at epoch 0: 1.0062412023544312
[08/27/2025 22:17:19 INFO]: Training loss at epoch 5: 0.9123973846435547
[08/27/2025 22:17:44 INFO]: Training loss at epoch 27: 0.7257120907306671
[08/27/2025 22:17:50 INFO]: Training loss at epoch 11: 1.1997018456459045
[08/27/2025 22:18:09 INFO]: Training loss at epoch 34: 0.8963738679885864
[08/27/2025 22:18:13 INFO]: New best epoch, val score: -0.9233637485499442
[08/27/2025 22:18:13 INFO]: Saving model to: maddest-Elbert_trial_90/model_best.pth
[08/27/2025 22:18:22 INFO]: Training loss at epoch 30: 0.8895520269870758
[08/27/2025 22:18:34 INFO]: Running Final Evaluation...
[08/27/2025 22:18:35 INFO]: New best epoch, val score: -0.893616491053515
[08/27/2025 22:18:35 INFO]: Saving model to: maddest-Elbert_trial_86/model_best.pth
[08/27/2025 22:18:36 INFO]: Training loss at epoch 36: 0.9912209510803223
[08/27/2025 22:18:47 INFO]: Training loss at epoch 30: 0.9013396799564362
[08/27/2025 22:18:49 INFO]: Training loss at epoch 35: 1.0638024508953094
[08/27/2025 22:18:51 INFO]: Training loss at epoch 13: 1.205493986606598
[08/27/2025 22:19:12 INFO]: Training loss at epoch 34: 1.057964026927948
[08/27/2025 22:19:17 INFO]: Training loss at epoch 35: 1.2226107120513916
[08/27/2025 22:19:45 INFO]: Training loss at epoch 37: 0.9641149938106537
[08/27/2025 22:19:46 INFO]: New best epoch, val score: -0.8820103897057348
[08/27/2025 22:19:46 INFO]: Saving model to: maddest-Elbert_trial_80/model_best.pth
[08/27/2025 22:19:49 INFO]: Training accuracy: {
    "score": -1.0296874138403587,
    "rmse": 1.0296874138403587
}
[08/27/2025 22:19:49 INFO]: Val accuracy: {
    "score": -0.883075920111307,
    "rmse": 0.883075920111307
}
[08/27/2025 22:19:49 INFO]: Test accuracy: {
    "score": -1.0051304933936762,
    "rmse": 1.0051304933936762
}
[08/27/2025 22:19:49 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_73",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0051304933936762,
        "rmse": 1.0051304933936762
    },
    "train_stats": {
        "score": -1.0296874138403587,
        "rmse": 1.0296874138403587
    },
    "val_stats": {
        "score": -0.883075920111307,
        "rmse": 0.883075920111307
    }
}
[08/27/2025 22:19:49 INFO]: Procewss finished for trial maddest-Elbert_trial_73
[08/27/2025 22:19:49 INFO]: 
_________________________________________________

[08/27/2025 22:19:49 INFO]: train_net_for_optune.py main() running.
[08/27/2025 22:19:49 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 1.3972852370804423
  attention_dropout: 0.03386841848941944
  ffn_dropout: 0.03386841848941944
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.4571132522952287e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_91

[08/27/2025 22:19:49 INFO]: This ft_transformer has 17.390 million parameters.
[08/27/2025 22:19:49 INFO]: Training will start at epoch 0.
[08/27/2025 22:19:49 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 22:19:58 INFO]: Training loss at epoch 35: 0.7732511758804321
[08/27/2025 22:20:15 INFO]: Training loss at epoch 11: 1.0668143033981323
[08/27/2025 22:21:36 INFO]: Training loss at epoch 37: 0.8118588924407959
[08/27/2025 22:21:49 INFO]: Training loss at epoch 36: 1.0796091258525848
[08/27/2025 22:21:50 INFO]: Training loss at epoch 16: 0.8903579711914062
[08/27/2025 22:22:05 INFO]: Training loss at epoch 28: 0.8354507088661194
[08/27/2025 22:22:11 INFO]: Training loss at epoch 35: 1.0928251147270203
[08/27/2025 22:22:16 INFO]: Training loss at epoch 31: 1.130566954612732
[08/27/2025 22:22:22 INFO]: Training loss at epoch 31: 0.9711190164089203
[08/27/2025 22:22:31 INFO]: Training loss at epoch 36: 0.8406037092208862
[08/27/2025 22:22:43 INFO]: Training loss at epoch 38: 0.9229363799095154
[08/27/2025 22:22:50 INFO]: Running Final Evaluation...
[08/27/2025 22:22:53 INFO]: Training loss at epoch 12: 1.2842677235603333
[08/27/2025 22:22:56 INFO]: Training loss at epoch 36: 0.8412887454032898
[08/27/2025 22:23:27 INFO]: Training loss at epoch 1: 1.6243542432785034
[08/27/2025 22:23:29 INFO]: New best epoch, val score: -0.8893242876371227
[08/27/2025 22:23:29 INFO]: Saving model to: maddest-Elbert_trial_87/model_best.pth
[08/27/2025 22:24:11 INFO]: Training loss at epoch 7: 1.6464570760726929
[08/27/2025 22:24:16 INFO]: Training accuracy: {
    "score": -1.0602758260671266,
    "rmse": 1.0602758260671266
}
[08/27/2025 22:24:16 INFO]: Val accuracy: {
    "score": -0.8611116039906399,
    "rmse": 0.8611116039906399
}
[08/27/2025 22:24:16 INFO]: Test accuracy: {
    "score": -1.0209634013953675,
    "rmse": 1.0209634013953675
}
[08/27/2025 22:24:16 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_71",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0209634013953675,
        "rmse": 1.0209634013953675
    },
    "train_stats": {
        "score": -1.0602758260671266,
        "rmse": 1.0602758260671266
    },
    "val_stats": {
        "score": -0.8611116039906399,
        "rmse": 0.8611116039906399
    }
}
[08/27/2025 22:24:16 INFO]: Procewss finished for trial maddest-Elbert_trial_71
[08/27/2025 22:24:16 INFO]: 
_________________________________________________

[08/27/2025 22:24:16 INFO]: train_net_for_optune.py main() running.
[08/27/2025 22:24:16 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 1.405051459435128
  attention_dropout: 0.14692509049037827
  ffn_dropout: 0.14692509049037827
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.9733204763218078e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_92

[08/27/2025 22:24:17 INFO]: This ft_transformer has 21.757 million parameters.
[08/27/2025 22:24:17 INFO]: Training will start at epoch 0.
[08/27/2025 22:24:17 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 22:24:33 INFO]: New best epoch, val score: -1.15922892841538
[08/27/2025 22:24:33 INFO]: Saving model to: maddest-Elbert_trial_89/model_best.pth
[08/27/2025 22:24:36 INFO]: Training loss at epoch 38: 1.1497263014316559
[08/27/2025 22:24:39 INFO]: Training loss at epoch 12: 0.9500119090080261
[08/27/2025 22:24:51 INFO]: Training loss at epoch 37: 0.9149720668792725
[08/27/2025 22:25:09 INFO]: New best epoch, val score: -0.8829499014247689
[08/27/2025 22:25:09 INFO]: Saving model to: maddest-Elbert_trial_88/model_best.pth
[08/27/2025 22:25:10 INFO]: Training loss at epoch 36: 1.0893884301185608
[08/27/2025 22:25:19 INFO]: New best epoch, val score: -0.9240238343421452
[08/27/2025 22:25:19 INFO]: Saving model to: maddest-Elbert_trial_85/model_best.pth
[08/27/2025 22:25:22 INFO]: Training loss at epoch 46: 0.8854981958866119
[08/27/2025 22:25:41 INFO]: Training loss at epoch 39: 1.0117467641830444
[08/27/2025 22:25:47 INFO]: Training loss at epoch 37: 0.9780269861221313
[08/27/2025 22:25:48 INFO]: Training loss at epoch 32: 0.780385434627533
[08/27/2025 22:25:58 INFO]: Training loss at epoch 37: 0.9249125421047211
[08/27/2025 22:26:08 INFO]: Training loss at epoch 8: 0.892762690782547
[08/27/2025 22:26:22 INFO]: Training loss at epoch 1: 1.1365225613117218
[08/27/2025 22:26:27 INFO]: Training loss at epoch 29: 0.7630024254322052
[08/27/2025 22:26:42 INFO]: Training loss at epoch 14: 0.8794037997722626
[08/27/2025 22:26:43 INFO]: Training stats: {
    "score": -0.9963599017837765,
    "rmse": 0.9963599017837765
}
[08/27/2025 22:26:43 INFO]: Val stats: {
    "score": -0.9420538184639973,
    "rmse": 0.9420538184639973
}
[08/27/2025 22:26:43 INFO]: Test stats: {
    "score": -0.9716190191126085,
    "rmse": 0.9716190191126085
}
[08/27/2025 22:27:14 INFO]: Training loss at epoch 0: 0.9059807062149048
[08/27/2025 22:27:29 INFO]: New best epoch, val score: -0.9071556411978733
[08/27/2025 22:27:29 INFO]: Saving model to: maddest-Elbert_trial_90/model_best.pth
[08/27/2025 22:27:37 INFO]: Training loss at epoch 39: 0.8969288766384125
[08/27/2025 22:27:57 INFO]: Training loss at epoch 38: 1.0060449242591858
[08/27/2025 22:27:58 INFO]: Training stats: {
    "score": -0.9116097671163251,
    "rmse": 0.9116097671163251
}
[08/27/2025 22:27:58 INFO]: Val stats: {
    "score": -0.9463700679959157,
    "rmse": 0.9463700679959157
}
[08/27/2025 22:27:58 INFO]: Test stats: {
    "score": -0.9980789159160468,
    "rmse": 0.9980789159160468
}
[08/27/2025 22:28:00 INFO]: Training loss at epoch 13: 1.1285225749015808
[08/27/2025 22:28:06 INFO]: Training loss at epoch 6: 1.022481918334961
[08/27/2025 22:28:13 INFO]: Training loss at epoch 37: 1.2183766961097717
[08/27/2025 22:28:15 INFO]: New best epoch, val score: -0.8887649675842283
[08/27/2025 22:28:15 INFO]: Saving model to: maddest-Elbert_trial_91/model_best.pth
[08/27/2025 22:28:32 INFO]: Training loss at epoch 17: 1.0774857997894287
[08/27/2025 22:28:40 INFO]: Training stats: {
    "score": -0.9936166770132241,
    "rmse": 0.9936166770132241
}
[08/27/2025 22:28:40 INFO]: Val stats: {
    "score": -0.9140592902416892,
    "rmse": 0.9140592902416892
}
[08/27/2025 22:28:40 INFO]: Test stats: {
    "score": -0.9720377308788561,
    "rmse": 0.9720377308788561
}
[08/27/2025 22:29:01 INFO]: Training loss at epoch 38: 1.1562671065330505
[08/27/2025 22:29:05 INFO]: Training loss at epoch 38: 0.8124814331531525
[08/27/2025 22:29:06 INFO]: Training loss at epoch 13: 1.2185896039009094
[08/27/2025 22:29:21 INFO]: Training loss at epoch 33: 0.977117270231247
[08/27/2025 22:29:23 INFO]: New best epoch, val score: -0.8931006394421335
[08/27/2025 22:29:23 INFO]: Saving model to: maddest-Elbert_trial_86/model_best.pth
[08/27/2025 22:29:38 INFO]: New best epoch, val score: -0.8823323049997727
[08/27/2025 22:29:38 INFO]: Saving model to: maddest-Elbert_trial_88/model_best.pth
[08/27/2025 22:29:45 INFO]: Training loss at epoch 40: 1.0492658615112305
[08/27/2025 22:30:07 INFO]: Running Final Evaluation...
[08/27/2025 22:31:00 INFO]: Training loss at epoch 39: 1.1171956658363342
[08/27/2025 22:31:11 INFO]: Training accuracy: {
    "score": -1.0011298772238078,
    "rmse": 1.0011298772238078
}
[08/27/2025 22:31:11 INFO]: Val accuracy: {
    "score": -0.9029402665365026,
    "rmse": 0.9029402665365026
}
[08/27/2025 22:31:11 INFO]: Test accuracy: {
    "score": -0.9744504655635077,
    "rmse": 0.9744504655635077
}
[08/27/2025 22:31:11 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_75",
    "best_epoch": 9,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9744504655635077,
        "rmse": 0.9744504655635077
    },
    "train_stats": {
        "score": -1.0011298772238078,
        "rmse": 1.0011298772238078
    },
    "val_stats": {
        "score": -0.9029402665365026,
        "rmse": 0.9029402665365026
    }
}
[08/27/2025 22:31:11 INFO]: Procewss finished for trial maddest-Elbert_trial_75
[08/27/2025 22:31:11 INFO]: 
_________________________________________________

[08/27/2025 22:31:11 INFO]: train_net_for_optune.py main() running.
[08/27/2025 22:31:11 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 1.4064859077996412
  attention_dropout: 0.2861584654967364
  ffn_dropout: 0.2861584654967364
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.8475901864729013e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_93

[08/27/2025 22:31:11 INFO]: This ft_transformer has 21.772 million parameters.
[08/27/2025 22:31:11 INFO]: Training will start at epoch 0.
[08/27/2025 22:31:11 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 22:31:13 INFO]: Training loss at epoch 38: 1.1781745851039886
[08/27/2025 22:31:40 INFO]: Training loss at epoch 40: 1.1333029866218567
[08/27/2025 22:32:02 INFO]: Training stats: {
    "score": -0.9949038832483992,
    "rmse": 0.9949038832483992
}
[08/27/2025 22:32:02 INFO]: Val stats: {
    "score": -0.958142863553047,
    "rmse": 0.958142863553047
}
[08/27/2025 22:32:02 INFO]: Test stats: {
    "score": -0.9763878664700835,
    "rmse": 0.9763878664700835
}
[08/27/2025 22:32:02 INFO]: Training loss at epoch 39: 1.0307329893112183
[08/27/2025 22:32:19 INFO]: Training loss at epoch 30: 0.8141961097717285
[08/27/2025 22:32:20 INFO]: Training loss at epoch 39: 0.7487957179546356
[08/27/2025 22:32:45 INFO]: Training loss at epoch 2: 1.6197603344917297
[08/27/2025 22:32:50 INFO]: Training loss at epoch 34: 0.9691242873668671
[08/27/2025 22:33:03 INFO]: Training stats: {
    "score": -0.9822686342253752,
    "rmse": 0.9822686342253752
}
[08/27/2025 22:33:03 INFO]: Val stats: {
    "score": -0.911508700781679,
    "rmse": 0.911508700781679
}
[08/27/2025 22:33:03 INFO]: Test stats: {
    "score": -0.9634979231263784,
    "rmse": 0.9634979231263784
}
[08/27/2025 22:33:04 INFO]: Training loss at epoch 14: 1.2150572836399078
[08/27/2025 22:33:26 INFO]: Training stats: {
    "score": -0.9853430198381046,
    "rmse": 0.9853430198381046
}
[08/27/2025 22:33:26 INFO]: Val stats: {
    "score": -0.9387555654528588,
    "rmse": 0.9387555654528588
}
[08/27/2025 22:33:26 INFO]: Test stats: {
    "score": -0.968152783482567,
    "rmse": 0.968152783482567
}
[08/27/2025 22:33:32 INFO]: Training loss at epoch 14: 1.1136455535888672
[08/27/2025 22:33:45 INFO]: Training loss at epoch 47: 0.9507269561290741
[08/27/2025 22:33:48 INFO]: Training loss at epoch 8: 1.422027587890625
[08/27/2025 22:33:51 INFO]: Training loss at epoch 0: 1.0205114483833313
[08/27/2025 22:33:52 INFO]: New best epoch, val score: -0.9039756213803717
[08/27/2025 22:33:52 INFO]: Saving model to: maddest-Elbert_trial_89/model_best.pth
[08/27/2025 22:34:10 INFO]: Training loss at epoch 39: 0.7989982664585114
[08/27/2025 22:34:32 INFO]: Training loss at epoch 15: 1.0388423800468445
[08/27/2025 22:34:39 INFO]: Training loss at epoch 41: 1.036329299211502
[08/27/2025 22:34:56 INFO]: New best epoch, val score: -0.9068954105121414
[08/27/2025 22:34:56 INFO]: Saving model to: maddest-Elbert_trial_85/model_best.pth
[08/27/2025 22:35:03 INFO]: Training loss at epoch 40: 0.856857180595398
[08/27/2025 22:35:07 INFO]: New best epoch, val score: -0.8889203044419492
[08/27/2025 22:35:07 INFO]: Saving model to: maddest-Elbert_trial_92/model_best.pth
[08/27/2025 22:35:11 INFO]: Training loss at epoch 18: 1.0606268346309662
[08/27/2025 22:35:14 INFO]: Training stats: {
    "score": -1.0002131783402717,
    "rmse": 1.0002131783402717
}
[08/27/2025 22:35:14 INFO]: Val stats: {
    "score": -0.9045466997889872,
    "rmse": 0.9045466997889872
}
[08/27/2025 22:35:14 INFO]: Test stats: {
    "score": -0.9736128619841513,
    "rmse": 0.9736128619841513
}
[08/27/2025 22:35:40 INFO]: Training loss at epoch 2: 1.0624995827674866
[08/27/2025 22:35:41 INFO]: Training loss at epoch 1: 1.0657306909561157
[08/27/2025 22:36:05 INFO]: Training loss at epoch 40: 0.9737626314163208
[08/27/2025 22:36:06 INFO]: Training loss at epoch 9: 1.0483669638633728
[08/27/2025 22:36:24 INFO]: Training loss at epoch 35: 0.788398265838623
[08/27/2025 22:36:41 INFO]: Training loss at epoch 31: 0.9226954579353333
[08/27/2025 22:36:41 INFO]: Training loss at epoch 40: 0.9557274580001831
[08/27/2025 22:37:42 INFO]: Training loss at epoch 42: 1.0020687282085419
[08/27/2025 22:38:01 INFO]: Training loss at epoch 15: 1.0723230838775635
[08/27/2025 22:38:09 INFO]: Training loss at epoch 15: 1.0132288336753845
[08/27/2025 22:38:10 INFO]: Training loss at epoch 41: 1.0375056862831116
[08/27/2025 22:38:14 INFO]: Training loss at epoch 40: 1.051387071609497
[08/27/2025 22:38:54 INFO]: Training loss at epoch 7: 0.8716855645179749
[08/27/2025 22:39:06 INFO]: Training loss at epoch 41: 0.7958858609199524
[08/27/2025 22:39:31 INFO]: Training stats: {
    "score": -1.0158007720260882,
    "rmse": 1.0158007720260882
}
[08/27/2025 22:39:31 INFO]: Val stats: {
    "score": -0.9914722875236583,
    "rmse": 0.9914722875236583
}
[08/27/2025 22:39:31 INFO]: Test stats: {
    "score": -0.9881277206327451,
    "rmse": 0.9881277206327451
}
[08/27/2025 22:39:56 INFO]: Training loss at epoch 36: 0.9508954286575317
[08/27/2025 22:39:58 INFO]: Training loss at epoch 41: 0.9153330624103546
[08/27/2025 22:40:41 INFO]: Training loss at epoch 0: 0.8698898255825043
[08/27/2025 22:40:42 INFO]: Training loss at epoch 43: 1.1388942897319794
[08/27/2025 22:41:02 INFO]: Training loss at epoch 32: 0.7836418151855469
[08/27/2025 22:41:10 INFO]: Training loss at epoch 42: 1.0371408462524414
[08/27/2025 22:41:12 INFO]: Training loss at epoch 41: 0.908528745174408
[08/27/2025 22:41:49 INFO]: Training loss at epoch 19: 0.8753923177719116
[08/27/2025 22:41:56 INFO]: New best epoch, val score: -0.9550620737366418
[08/27/2025 22:41:56 INFO]: Saving model to: maddest-Elbert_trial_93/model_best.pth
[08/27/2025 22:42:04 INFO]: Training loss at epoch 3: 1.2225131690502167
[08/27/2025 22:42:06 INFO]: Training loss at epoch 42: 1.0014603734016418
[08/27/2025 22:42:08 INFO]: Training loss at epoch 48: 0.8890400230884552
[08/27/2025 22:42:22 INFO]: Training loss at epoch 16: 1.244837760925293
[08/27/2025 22:42:25 INFO]: Training loss at epoch 16: 0.8188892900943756
[08/27/2025 22:43:11 INFO]: Training loss at epoch 42: 0.9964097440242767
[08/27/2025 22:43:12 INFO]: Training loss at epoch 16: 0.8476959764957428
[08/27/2025 22:43:22 INFO]: Training loss at epoch 9: 1.1017771661281586
[08/27/2025 22:43:25 INFO]: Training loss at epoch 37: 1.1511602997779846
[08/27/2025 22:43:42 INFO]: Training loss at epoch 44: 0.9404878616333008
[08/27/2025 22:44:06 INFO]: Training loss at epoch 2: 1.1890390515327454
[08/27/2025 22:44:09 INFO]: Training stats: {
    "score": -0.9917128149378813,
    "rmse": 0.9917128149378813
}
[08/27/2025 22:44:09 INFO]: Val stats: {
    "score": -0.9123074102025975,
    "rmse": 0.9123074102025975
}
[08/27/2025 22:44:09 INFO]: Test stats: {
    "score": -0.9704514125654886,
    "rmse": 0.9704514125654886
}
[08/27/2025 22:44:12 INFO]: Training loss at epoch 42: 0.9957091510295868
[08/27/2025 22:44:14 INFO]: Training loss at epoch 43: 1.0475127696990967
[08/27/2025 22:44:40 INFO]: Training loss at epoch 1: 1.7021886110305786
[08/27/2025 22:44:55 INFO]: Training loss at epoch 3: 0.9375855922698975
[08/27/2025 22:45:10 INFO]: Training loss at epoch 43: 0.9184300303459167
[08/27/2025 22:45:26 INFO]: Training loss at epoch 33: 0.884147047996521
[08/27/2025 22:45:32 INFO]: Running Final Evaluation...
[08/27/2025 22:46:01 INFO]: New best epoch, val score: -0.8979903564192607
[08/27/2025 22:46:01 INFO]: Saving model to: maddest-Elbert_trial_90/model_best.pth
[08/27/2025 22:46:29 INFO]: Training loss at epoch 43: 1.0240457952022552
[08/27/2025 22:46:37 INFO]: Training stats: {
    "score": -1.0869813568507738,
    "rmse": 1.0869813568507738
}
[08/27/2025 22:46:37 INFO]: Val stats: {
    "score": -0.8924718555792793,
    "rmse": 0.8924718555792793
}
[08/27/2025 22:46:37 INFO]: Test stats: {
    "score": -1.0607753696420092,
    "rmse": 1.0607753696420092
}
[08/27/2025 22:46:37 INFO]: Training accuracy: {
    "score": -1.0139648546625022,
    "rmse": 1.0139648546625022
}
[08/27/2025 22:46:37 INFO]: Val accuracy: {
    "score": -0.8772187706455459,
    "rmse": 0.8772187706455459
}
[08/27/2025 22:46:37 INFO]: Test accuracy: {
    "score": -0.9890063564282499,
    "rmse": 0.9890063564282499
}
[08/27/2025 22:46:37 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_81",
    "best_epoch": 12,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9890063564282499,
        "rmse": 0.9890063564282499
    },
    "train_stats": {
        "score": -1.0139648546625022,
        "rmse": 1.0139648546625022
    },
    "val_stats": {
        "score": -0.8772187706455459,
        "rmse": 0.8772187706455459
    }
}
[08/27/2025 22:46:37 INFO]: Procewss finished for trial maddest-Elbert_trial_81
[08/27/2025 22:46:37 INFO]: 
_________________________________________________

[08/27/2025 22:46:37 INFO]: train_net_for_optune.py main() running.
[08/27/2025 22:46:37 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 1.4045297600761608
  attention_dropout: 0.03567486656791579
  ffn_dropout: 0.03567486656791579
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 8.459355170991425e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_94

[08/27/2025 22:46:38 INFO]: This ft_transformer has 21.757 million parameters.
[08/27/2025 22:46:38 INFO]: Training will start at epoch 0.
[08/27/2025 22:46:38 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 22:46:42 INFO]: Training loss at epoch 45: 1.0213828086853027
[08/27/2025 22:46:52 INFO]: Training loss at epoch 17: 1.0933091640472412
[08/27/2025 22:46:57 INFO]: Training loss at epoch 38: 0.9707838296890259
[08/27/2025 22:47:12 INFO]: Training loss at epoch 43: 1.109933227300644
[08/27/2025 22:47:16 INFO]: Training loss at epoch 44: 0.8909085094928741
[08/27/2025 22:47:45 INFO]: New best epoch, val score: -0.8924718555792793
[08/27/2025 22:47:45 INFO]: Saving model to: maddest-Elbert_trial_85/model_best.pth
[08/27/2025 22:48:18 INFO]: Training loss at epoch 17: 1.0006890892982483
[08/27/2025 22:49:25 INFO]: Training loss at epoch 10: 1.136483907699585
[08/27/2025 22:49:40 INFO]: Training loss at epoch 8: 1.0006557703018188
[08/27/2025 22:49:42 INFO]: Training loss at epoch 46: 0.7983543574810028
[08/27/2025 22:49:43 INFO]: Training loss at epoch 44: 1.120069980621338
[08/27/2025 22:49:47 INFO]: Training loss at epoch 34: 0.7254314124584198
[08/27/2025 22:50:11 INFO]: Training loss at epoch 44: 1.0408894419670105
[08/27/2025 22:50:11 INFO]: Training loss at epoch 17: 1.1381748914718628
[08/27/2025 22:50:17 INFO]: Training loss at epoch 45: 0.8985136449337006
[08/27/2025 22:50:27 INFO]: Training loss at epoch 39: 1.0787927508354187
[08/27/2025 22:50:31 INFO]: Training loss at epoch 49: 1.0801503658294678
[08/27/2025 22:50:48 INFO]: Training loss at epoch 20: 0.9109688401222229
[08/27/2025 22:51:16 INFO]: Training loss at epoch 18: 1.0470482110977173
[08/27/2025 22:51:18 INFO]: Training loss at epoch 4: 1.2637968063354492
[08/27/2025 22:51:27 INFO]: Training loss at epoch 1: 1.1559155583381653
[08/27/2025 22:51:39 INFO]: Training stats: {
    "score": -1.0139601225956179,
    "rmse": 1.0139601225956179
}
[08/27/2025 22:51:39 INFO]: Val stats: {
    "score": -1.0030884120882382,
    "rmse": 1.0030884120882382
}
[08/27/2025 22:51:39 INFO]: Test stats: {
    "score": -0.99029903987311,
    "rmse": 0.99029903987311
}
[08/27/2025 22:52:30 INFO]: Training loss at epoch 3: 0.9976539611816406
[08/27/2025 22:52:40 INFO]: Training loss at epoch 47: 1.0351920127868652
[08/27/2025 22:52:59 INFO]: Training loss at epoch 45: 0.8890582323074341
[08/27/2025 22:53:02 INFO]: Running Final Evaluation...
[08/27/2025 22:53:09 INFO]: Training loss at epoch 45: 1.1489604711532593
[08/27/2025 22:53:20 INFO]: Training loss at epoch 46: 0.8915491700172424
[08/27/2025 22:53:20 INFO]: Training loss at epoch 18: 1.0887131690979004
[08/27/2025 22:53:24 INFO]: Training stats: {
    "score": -1.0001466065410194,
    "rmse": 1.0001466065410194
}
[08/27/2025 22:53:24 INFO]: Val stats: {
    "score": -0.9039065891693852,
    "rmse": 0.9039065891693852
}
[08/27/2025 22:53:24 INFO]: Test stats: {
    "score": -0.9729330550830063,
    "rmse": 0.9729330550830063
}
[08/27/2025 22:54:05 INFO]: Training accuracy: {
    "score": -1.0365723471280275,
    "rmse": 1.0365723471280275
}
[08/27/2025 22:54:05 INFO]: Val accuracy: {
    "score": -0.8801911217518886,
    "rmse": 0.8801911217518886
}
[08/27/2025 22:54:05 INFO]: Test accuracy: {
    "score": -1.012665026580252,
    "rmse": 1.012665026580252
}
[08/27/2025 22:54:05 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_76",
    "best_epoch": 16,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.012665026580252,
        "rmse": 1.012665026580252
    },
    "train_stats": {
        "score": -1.0365723471280275,
        "rmse": 1.0365723471280275
    },
    "val_stats": {
        "score": -0.8801911217518886,
        "rmse": 0.8801911217518886
    }
}
[08/27/2025 22:54:05 INFO]: Procewss finished for trial maddest-Elbert_trial_76
[08/27/2025 22:54:05 INFO]: 
_________________________________________________

[08/27/2025 22:54:05 INFO]: train_net_for_optune.py main() running.
[08/27/2025 22:54:05 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 1.3961280161486624
  attention_dropout: 0.2835086694688968
  ffn_dropout: 0.2835086694688968
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.0558138646676735e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_95

[08/27/2025 22:54:06 INFO]: This ft_transformer has 21.680 million parameters.
[08/27/2025 22:54:06 INFO]: Training will start at epoch 0.
[08/27/2025 22:54:06 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 22:54:11 INFO]: Training loss at epoch 35: 0.7930127382278442
[08/27/2025 22:54:11 INFO]: Training loss at epoch 4: 0.9980351328849792
[08/27/2025 22:54:25 INFO]: Running Final Evaluation...
[08/27/2025 22:55:10 INFO]: Training loss at epoch 40: 1.0176632404327393
[08/27/2025 22:55:25 INFO]: Training loss at epoch 2: 1.101692259311676
[08/27/2025 22:55:40 INFO]: Training loss at epoch 19: 0.9578039050102234
[08/27/2025 22:56:08 INFO]: Training loss at epoch 0: 1.0910468101501465
[08/27/2025 22:56:09 INFO]: Training loss at epoch 10: 1.375491201877594
[08/27/2025 22:56:09 INFO]: Training loss at epoch 46: 1.0544903576374054
[08/27/2025 22:56:14 INFO]: Training loss at epoch 46: 0.8219456076622009
[08/27/2025 22:56:21 INFO]: Training loss at epoch 47: 0.9406790435314178
[08/27/2025 22:56:31 INFO]: Running Final Evaluation...
[08/27/2025 22:57:09 INFO]: Training stats: {
    "score": -1.000884745824071,
    "rmse": 1.000884745824071
}
[08/27/2025 22:57:09 INFO]: Val stats: {
    "score": -0.945721485498186,
    "rmse": 0.945721485498186
}
[08/27/2025 22:57:09 INFO]: Test stats: {
    "score": -0.9747133033337372,
    "rmse": 0.9747133033337372
}
[08/27/2025 22:57:24 INFO]: New best epoch, val score: -0.9140720503222466
[08/27/2025 22:57:24 INFO]: Saving model to: maddest-Elbert_trial_94/model_best.pth
[08/27/2025 22:57:26 INFO]: Training accuracy: {
    "score": -1.0383905314597544,
    "rmse": 1.0383905314597544
}
[08/27/2025 22:57:26 INFO]: Val accuracy: {
    "score": -0.8806096596200622,
    "rmse": 0.8806096596200622
}
[08/27/2025 22:57:26 INFO]: Test accuracy: {
    "score": -1.0108532381042659,
    "rmse": 1.0108532381042659
}
[08/27/2025 22:57:26 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_30",
    "best_epoch": 18,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0108532381042659,
        "rmse": 1.0108532381042659
    },
    "train_stats": {
        "score": -1.0383905314597544,
        "rmse": 1.0383905314597544
    },
    "val_stats": {
        "score": -0.8806096596200622,
        "rmse": 0.8806096596200622
    }
}
[08/27/2025 22:57:26 INFO]: Procewss finished for trial maddest-Elbert_trial_30
[08/27/2025 22:57:26 INFO]: 
_________________________________________________

[08/27/2025 22:57:26 INFO]: train_net_for_optune.py main() running.
[08/27/2025 22:57:26 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 1.7012042670861904
  attention_dropout: 0.02829701300952381
  ffn_dropout: 0.02829701300952381
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.052987009799752e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_96

[08/27/2025 22:57:27 INFO]: This ft_transformer has 15.735 million parameters.
[08/27/2025 22:57:27 INFO]: Training will start at epoch 0.
[08/27/2025 22:57:27 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 22:57:28 INFO]: Training loss at epoch 21: 1.0859330594539642
[08/27/2025 22:57:34 INFO]: Training accuracy: {
    "score": -1.0352052458618546,
    "rmse": 1.0352052458618546
}
[08/27/2025 22:57:34 INFO]: Val accuracy: {
    "score": -0.8809220036725088,
    "rmse": 0.8809220036725088
}
[08/27/2025 22:57:34 INFO]: Test accuracy: {
    "score": -1.0081636000199432,
    "rmse": 1.0081636000199432
}
[08/27/2025 22:57:34 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_82",
    "best_epoch": 15,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0081636000199432,
        "rmse": 1.0081636000199432
    },
    "train_stats": {
        "score": -1.0352052458618546,
        "rmse": 1.0352052458618546
    },
    "val_stats": {
        "score": -0.8809220036725088,
        "rmse": 0.8809220036725088
    }
}
[08/27/2025 22:57:34 INFO]: Procewss finished for trial maddest-Elbert_trial_82
[08/27/2025 22:57:34 INFO]: 
_________________________________________________

[08/27/2025 22:57:34 INFO]: train_net_for_optune.py main() running.
[08/27/2025 22:57:34 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 1.6843130081505782
  attention_dropout: 0.14894789837042693
  ffn_dropout: 0.14894789837042693
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.0448997623222415e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_97

[08/27/2025 22:57:34 INFO]: This ft_transformer has 15.661 million parameters.
[08/27/2025 22:57:34 INFO]: Training will start at epoch 0.
[08/27/2025 22:57:34 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 22:58:04 INFO]: Training loss at epoch 18: 1.0491618514060974
[08/27/2025 22:58:23 INFO]: Training loss at epoch 19: 0.9854691624641418
[08/27/2025 22:58:32 INFO]: Training loss at epoch 36: 0.9376705288887024
[08/27/2025 22:58:41 INFO]: Training loss at epoch 41: 1.010418713092804
[08/27/2025 22:59:24 INFO]: Training loss at epoch 48: 0.9233770668506622
[08/27/2025 22:59:25 INFO]: Training loss at epoch 11: 0.9278963804244995
[08/27/2025 22:59:28 INFO]: Training loss at epoch 47: 0.9245007038116455
[08/27/2025 22:59:46 INFO]: Running Final Evaluation...
[08/27/2025 23:00:03 INFO]: Training stats: {
    "score": -0.988791523547601,
    "rmse": 0.988791523547601
}
[08/27/2025 23:00:03 INFO]: Val stats: {
    "score": -0.8926728067365484,
    "rmse": 0.8926728067365484
}
[08/27/2025 23:00:03 INFO]: Test stats: {
    "score": -0.9832087658572722,
    "rmse": 0.9832087658572722
}
[08/27/2025 23:00:25 INFO]: Training loss at epoch 9: 0.8388738930225372
[08/27/2025 23:00:34 INFO]: Training loss at epoch 5: 1.134419709444046
[08/27/2025 23:00:51 INFO]: Training accuracy: {
    "score": -1.0338753954927618,
    "rmse": 1.0338753954927618
}
[08/27/2025 23:00:51 INFO]: Val accuracy: {
    "score": -0.8888840609206549,
    "rmse": 0.8888840609206549
}
[08/27/2025 23:00:51 INFO]: Test accuracy: {
    "score": -1.0088653908959022,
    "rmse": 1.0088653908959022
}
[08/27/2025 23:00:51 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_78",
    "best_epoch": 17,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0088653908959022,
        "rmse": 1.0088653908959022
    },
    "train_stats": {
        "score": -1.0338753954927618,
        "rmse": 1.0338753954927618
    },
    "val_stats": {
        "score": -0.8888840609206549,
        "rmse": 0.8888840609206549
    }
}
[08/27/2025 23:00:51 INFO]: Procewss finished for trial maddest-Elbert_trial_78
[08/27/2025 23:00:51 INFO]: 
_________________________________________________

[08/27/2025 23:00:51 INFO]: train_net_for_optune.py main() running.
[08/27/2025 23:00:51 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 1.9460871559573558
  attention_dropout: 0.1436280365247074
  ffn_dropout: 0.1436280365247074
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.2694489935653556e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_98

[08/27/2025 23:00:52 INFO]: This ft_transformer has 16.760 million parameters.
[08/27/2025 23:00:52 INFO]: Training will start at epoch 0.
[08/27/2025 23:00:52 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 23:00:58 INFO]: Training loss at epoch 4: 1.3524158596992493
[08/27/2025 23:01:32 INFO]: Training loss at epoch 20: 0.9616749286651611
[08/27/2025 23:02:10 INFO]: Training loss at epoch 2: 1.4304402470588684
[08/27/2025 23:02:12 INFO]: Training loss at epoch 42: 0.9310873448848724
[08/27/2025 23:02:43 INFO]: Training loss at epoch 48: 0.9836862683296204
[08/27/2025 23:02:54 INFO]: Training loss at epoch 37: 0.9431281089782715
[08/27/2025 23:03:22 INFO]: Training loss at epoch 5: 0.9182110130786896
[08/27/2025 23:03:24 INFO]: New best epoch, val score: -0.8871779397246747
[08/27/2025 23:03:24 INFO]: Saving model to: maddest-Elbert_trial_93/model_best.pth
[08/27/2025 23:03:35 INFO]: Training loss at epoch 0: 0.8774904012680054
[08/27/2025 23:04:01 INFO]: Training stats: {
    "score": -0.9820993857893815,
    "rmse": 0.9820993857893815
}
[08/27/2025 23:04:01 INFO]: Val stats: {
    "score": -0.9009830747899807,
    "rmse": 0.9009830747899807
}
[08/27/2025 23:04:01 INFO]: Test stats: {
    "score": -0.9907478802179206,
    "rmse": 0.9907478802179206
}
[08/27/2025 23:04:04 INFO]: Training loss at epoch 22: 0.9805072844028473
[08/27/2025 23:04:11 INFO]: Training loss at epoch 0: 0.9150029718875885
[08/27/2025 23:04:17 INFO]: Training loss at epoch 0: 1.0301004648208618
[08/27/2025 23:04:51 INFO]: New best epoch, val score: -0.9144434971933427
[08/27/2025 23:04:51 INFO]: Saving model to: maddest-Elbert_trial_95/model_best.pth
[08/27/2025 23:05:04 INFO]: Training loss at epoch 20: 0.9345533847808838
[08/27/2025 23:05:04 INFO]: New best epoch, val score: -0.9808306163499311
[08/27/2025 23:05:04 INFO]: Saving model to: maddest-Elbert_trial_96/model_best.pth
[08/27/2025 23:05:11 INFO]: New best epoch, val score: -0.8978143151706183
[08/27/2025 23:05:11 INFO]: Saving model to: maddest-Elbert_trial_97/model_best.pth
[08/27/2025 23:05:39 INFO]: Training loss at epoch 11: 1.002297192811966
[08/27/2025 23:05:42 INFO]: Training loss at epoch 43: 1.0007018744945526
[08/27/2025 23:05:50 INFO]: Training loss at epoch 19: 0.9440975189208984
[08/27/2025 23:05:56 INFO]: Training loss at epoch 21: 0.9514169692993164
[08/27/2025 23:05:57 INFO]: Training loss at epoch 49: 0.8167131543159485
[08/27/2025 23:06:07 INFO]: Running Final Evaluation...
[08/27/2025 23:06:11 INFO]: Training loss at epoch 3: 1.3856703042984009
[08/27/2025 23:06:52 INFO]: Training loss at epoch 1: 1.6168380975723267
[08/27/2025 23:07:05 INFO]: Training stats: {
    "score": -0.9802738768564413,
    "rmse": 0.9802738768564413
}
[08/27/2025 23:07:05 INFO]: Val stats: {
    "score": -0.9129001314294657,
    "rmse": 0.9129001314294657
}
[08/27/2025 23:07:05 INFO]: Test stats: {
    "score": -0.9653478043332039,
    "rmse": 0.9653478043332039
}
[08/27/2025 23:07:16 INFO]: Training loss at epoch 38: 0.7585230767726898
[08/27/2025 23:07:22 INFO]: Training accuracy: {
    "score": -1.038121364070993,
    "rmse": 1.038121364070993
}
[08/27/2025 23:07:22 INFO]: Val accuracy: {
    "score": -0.8787817234246532,
    "rmse": 0.8787817234246532
}
[08/27/2025 23:07:22 INFO]: Test accuracy: {
    "score": -1.0100866376142295,
    "rmse": 1.0100866376142295
}
[08/27/2025 23:07:22 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_77",
    "best_epoch": 12,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0100866376142295,
        "rmse": 1.0100866376142295
    },
    "train_stats": {
        "score": -1.038121364070993,
        "rmse": 1.038121364070993
    },
    "val_stats": {
        "score": -0.8787817234246532,
        "rmse": 0.8787817234246532
    }
}
[08/27/2025 23:07:22 INFO]: Procewss finished for trial maddest-Elbert_trial_77
[08/27/2025 23:07:22 INFO]: 
_________________________________________________

[08/27/2025 23:07:22 INFO]: train_net_for_optune.py main() running.
[08/27/2025 23:07:22 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 1.3956895762567947
  attention_dropout: 0.1402377703076599
  ffn_dropout: 0.1402377703076599
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 8.48854290050041e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_99

[08/27/2025 23:07:22 INFO]: This ft_transformer has 0.248 million parameters.
[08/27/2025 23:07:22 INFO]: Training will start at epoch 0.
[08/27/2025 23:07:22 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 23:07:27 INFO]: New best epoch, val score: -0.8761732243988861
[08/27/2025 23:07:27 INFO]: Saving model to: maddest-Elbert_trial_92/model_best.pth
[08/27/2025 23:07:59 INFO]: Training loss at epoch 0: 1.0636250972747803
[08/27/2025 23:08:04 INFO]: Training loss at epoch 0: 1.0460332036018372
[08/27/2025 23:08:04 INFO]: New best epoch, val score: -0.9063029217118871
[08/27/2025 23:08:04 INFO]: Saving model to: maddest-Elbert_trial_99/model_best.pth
[08/27/2025 23:08:07 INFO]: New best epoch, val score: -0.9136314318758603
[08/27/2025 23:08:07 INFO]: Saving model to: maddest-Elbert_trial_94/model_best.pth
[08/27/2025 23:08:30 INFO]: Training stats: {
    "score": -0.9986194568873088,
    "rmse": 0.9986194568873088
}
[08/27/2025 23:08:30 INFO]: Val stats: {
    "score": -0.9070786248706156,
    "rmse": 0.9070786248706156
}
[08/27/2025 23:08:30 INFO]: Test stats: {
    "score": -0.9724259411255665,
    "rmse": 0.9724259411255665
}
[08/27/2025 23:08:42 INFO]: Training loss at epoch 1: 1.0675501823425293
[08/27/2025 23:09:02 INFO]: New best epoch, val score: -0.910331033494398
[08/27/2025 23:09:02 INFO]: Saving model to: maddest-Elbert_trial_98/model_best.pth
[08/27/2025 23:09:21 INFO]: Training loss at epoch 5: 0.9226210713386536
[08/27/2025 23:09:24 INFO]: Training loss at epoch 12: 1.0059840083122253
[08/27/2025 23:09:24 INFO]: Training loss at epoch 2: 0.829038679599762
[08/27/2025 23:09:50 INFO]: Training loss at epoch 6: 1.0787944197654724
[08/27/2025 23:10:05 INFO]: Training loss at epoch 3: 1.1684920191764832
[08/27/2025 23:10:11 INFO]: Training loss at epoch 21: 1.004283457994461
[08/27/2025 23:10:21 INFO]: Training loss at epoch 50: 1.0369753241539001
[08/27/2025 23:10:21 INFO]: Training loss at epoch 22: 1.022067666053772
[08/27/2025 23:10:45 INFO]: Training loss at epoch 23: 0.9343105256557465
[08/27/2025 23:10:47 INFO]: Training loss at epoch 4: 1.2226217985153198
[08/27/2025 23:11:29 INFO]: Training loss at epoch 5: 1.0074733793735504
[08/27/2025 23:11:38 INFO]: Training loss at epoch 39: 0.7031470537185669
[08/27/2025 23:11:57 INFO]: Training loss at epoch 1: 1.0074161887168884
[08/27/2025 23:11:59 INFO]: Training loss at epoch 1: 1.2899839878082275
[08/27/2025 23:12:10 INFO]: Training loss at epoch 6: 1.0232768654823303
[08/27/2025 23:12:37 INFO]: Training loss at epoch 6: 1.0858712196350098
[08/27/2025 23:12:52 INFO]: New best epoch, val score: -0.8997907048227418
[08/27/2025 23:12:52 INFO]: Saving model to: maddest-Elbert_trial_96/model_best.pth
[08/27/2025 23:12:53 INFO]: Training loss at epoch 7: 0.9776811599731445
[08/27/2025 23:12:54 INFO]: Training loss at epoch 3: 1.2769263982772827
[08/27/2025 23:13:07 INFO]: Training stats: {
    "score": -0.8896527716070535,
    "rmse": 0.8896527716070535
}
[08/27/2025 23:13:07 INFO]: Val stats: {
    "score": -0.9247291672273656,
    "rmse": 0.9247291672273656
}
[08/27/2025 23:13:07 INFO]: Test stats: {
    "score": -1.0111900480150857,
    "rmse": 1.0111900480150857
}
[08/27/2025 23:13:34 INFO]: Training loss at epoch 8: 0.9056027829647064
[08/27/2025 23:13:35 INFO]: Training loss at epoch 51: 1.0050877630710602
[08/27/2025 23:14:16 INFO]: Training loss at epoch 9: 1.1269623935222626
[08/27/2025 23:14:19 INFO]: Training loss at epoch 1: 1.1939079761505127
[08/27/2025 23:14:32 INFO]: Training stats: {
    "score": -1.0068400032557197,
    "rmse": 1.0068400032557197
}
[08/27/2025 23:14:32 INFO]: Val stats: {
    "score": -0.9669473530516994,
    "rmse": 0.9669473530516994
}
[08/27/2025 23:14:32 INFO]: Test stats: {
    "score": -0.983549156078894,
    "rmse": 0.983549156078894
}
[08/27/2025 23:14:45 INFO]: Training loss at epoch 23: 1.2083470523357391
[08/27/2025 23:14:46 INFO]: Training loss at epoch 10: 0.9798239469528198
[08/27/2025 23:15:09 INFO]: Training loss at epoch 12: 1.2387079000473022
[08/27/2025 23:15:14 INFO]: Training loss at epoch 10: 0.8629093766212463
[08/27/2025 23:15:16 INFO]: Training loss at epoch 22: 1.1446778774261475
[08/27/2025 23:15:56 INFO]: Training loss at epoch 11: 1.150390386581421
[08/27/2025 23:16:13 INFO]: Training loss at epoch 1: 1.1219156980514526
[08/27/2025 23:16:15 INFO]: Training loss at epoch 20: 1.0815498232841492
[08/27/2025 23:16:38 INFO]: Training loss at epoch 12: 1.098302274942398
[08/27/2025 23:16:49 INFO]: Training loss at epoch 52: 1.0860580801963806
[08/27/2025 23:16:57 INFO]: Training loss at epoch 4: 1.2165838181972504
[08/27/2025 23:17:17 INFO]: Running Final Evaluation...
[08/27/2025 23:17:19 INFO]: Training loss at epoch 13: 0.9116309583187103
[08/27/2025 23:17:21 INFO]: Training loss at epoch 24: 0.9118935465812683
[08/27/2025 23:17:24 INFO]: Training loss at epoch 40: 0.8522208631038666
[08/27/2025 23:17:35 INFO]: Training loss at epoch 2: 1.6940736770629883
[08/27/2025 23:17:45 INFO]: Training loss at epoch 6: 0.8887160420417786
[08/27/2025 23:18:12 INFO]: Training loss at epoch 14: 0.9852871894836426
[08/27/2025 23:18:58 INFO]: New best epoch, val score: -0.8892350212157272
[08/27/2025 23:18:58 INFO]: Saving model to: maddest-Elbert_trial_94/model_best.pth
[08/27/2025 23:18:59 INFO]: Training loss at epoch 7: 0.9789300560951233
[08/27/2025 23:19:04 INFO]: Training loss at epoch 15: 1.1505718231201172
[08/27/2025 23:19:05 INFO]: Training accuracy: {
    "score": -1.0195892828431226,
    "rmse": 1.0195892828431226
}
[08/27/2025 23:19:05 INFO]: Val accuracy: {
    "score": -0.886433200808729,
    "rmse": 0.886433200808729
}
[08/27/2025 23:19:05 INFO]: Test accuracy: {
    "score": -0.9978461460923428,
    "rmse": 0.9978461460923428
}
[08/27/2025 23:19:05 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_74",
    "best_epoch": 21,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9978461460923428,
        "rmse": 0.9978461460923428
    },
    "train_stats": {
        "score": -1.0195892828431226,
        "rmse": 1.0195892828431226
    },
    "val_stats": {
        "score": -0.886433200808729,
        "rmse": 0.886433200808729
    }
}
[08/27/2025 23:19:05 INFO]: Procewss finished for trial maddest-Elbert_trial_74
[08/27/2025 23:19:06 INFO]: 
_________________________________________________

[08/27/2025 23:19:06 INFO]: train_net_for_optune.py main() running.
[08/27/2025 23:19:06 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 1.6739452934464287
  attention_dropout: 0.14231391656883713
  ffn_dropout: 0.14231391656883713
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.2550506365051814e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_100

[08/27/2025 23:19:06 INFO]: This ft_transformer has 0.267 million parameters.
[08/27/2025 23:19:06 INFO]: Training will start at epoch 0.
[08/27/2025 23:19:06 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 23:19:17 INFO]: Training loss at epoch 24: 0.8556784987449646
[08/27/2025 23:19:17 INFO]: Training loss at epoch 13: 1.02908855676651
[08/27/2025 23:19:34 INFO]: Training loss at epoch 2: 1.0865342617034912
[08/27/2025 23:19:40 INFO]: Training loss at epoch 2: 0.9162649214267731
[08/27/2025 23:19:44 INFO]: Training loss at epoch 0: 1.1534645855426788
[08/27/2025 23:19:48 INFO]: Training loss at epoch 16: 0.8834454119205475
[08/27/2025 23:19:50 INFO]: New best epoch, val score: -0.9239908069747647
[08/27/2025 23:19:50 INFO]: Saving model to: maddest-Elbert_trial_100/model_best.pth
[08/27/2025 23:20:19 INFO]: Training loss at epoch 23: 0.8911491632461548
[08/27/2025 23:20:28 INFO]: Training loss at epoch 1: 1.1073307394981384
[08/27/2025 23:20:29 INFO]: New best epoch, val score: -0.8874332243771996
[08/27/2025 23:20:29 INFO]: Saving model to: maddest-Elbert_trial_97/model_best.pth
[08/27/2025 23:20:31 INFO]: Training loss at epoch 17: 1.0389530658721924
[08/27/2025 23:20:34 INFO]: New best epoch, val score: -0.9201453263660124
[08/27/2025 23:20:34 INFO]: Saving model to: maddest-Elbert_trial_100/model_best.pth
[08/27/2025 23:21:11 INFO]: Training loss at epoch 2: 1.3074251413345337
[08/27/2025 23:21:12 INFO]: Training loss at epoch 18: 1.0557509660720825
[08/27/2025 23:21:26 INFO]: New best epoch, val score: -0.9105338557256203
[08/27/2025 23:21:26 INFO]: Saving model to: maddest-Elbert_trial_100/model_best.pth
[08/27/2025 23:21:44 INFO]: Training loss at epoch 7: 1.0099557638168335
[08/27/2025 23:21:46 INFO]: Training loss at epoch 41: 0.9610642492771149
[08/27/2025 23:22:03 INFO]: Training loss at epoch 19: 0.8931895196437836
[08/27/2025 23:22:04 INFO]: Training loss at epoch 3: 0.8644345700740814
[08/27/2025 23:22:11 INFO]: New best epoch, val score: -0.9034518947030348
[08/27/2025 23:22:11 INFO]: Saving model to: maddest-Elbert_trial_100/model_best.pth
[08/27/2025 23:22:21 INFO]: Training stats: {
    "score": -1.0000290759619421,
    "rmse": 1.0000290759619421
}
[08/27/2025 23:22:21 INFO]: Val stats: {
    "score": -0.9230542370338954,
    "rmse": 0.9230542370338954
}
[08/27/2025 23:22:21 INFO]: Test stats: {
    "score": -0.9760803984861418,
    "rmse": 0.9760803984861418
}
[08/27/2025 23:22:49 INFO]: Training loss at epoch 4: 0.9616058170795441
[08/27/2025 23:23:00 INFO]: New best epoch, val score: -0.9017595373465622
[08/27/2025 23:23:00 INFO]: Saving model to: maddest-Elbert_trial_100/model_best.pth
[08/27/2025 23:23:02 INFO]: Training loss at epoch 20: 0.8598207235336304
[08/27/2025 23:23:37 INFO]: Training loss at epoch 5: 1.2934115529060364
[08/27/2025 23:23:40 INFO]: Training loss at epoch 4: 1.053418755531311
[08/27/2025 23:23:43 INFO]: New best epoch, val score: -0.9015755775709936
[08/27/2025 23:23:43 INFO]: Saving model to: maddest-Elbert_trial_100/model_best.pth
[08/27/2025 23:23:43 INFO]: Training loss at epoch 25: 0.9754793345928192
[08/27/2025 23:23:45 INFO]: Training loss at epoch 21: 0.9516116082668304
[08/27/2025 23:23:59 INFO]: Training loss at epoch 21: 1.0397511720657349
[08/27/2025 23:23:59 INFO]: Training loss at epoch 25: 1.0807043313980103
[08/27/2025 23:24:20 INFO]: Training loss at epoch 6: 0.9654136002063751
[08/27/2025 23:24:30 INFO]: Training loss at epoch 2: 1.0892261266708374
[08/27/2025 23:24:31 INFO]: Training loss at epoch 22: 1.1038406491279602
[08/27/2025 23:24:35 INFO]: New best epoch, val score: -0.9014681384003826
[08/27/2025 23:24:35 INFO]: Saving model to: maddest-Elbert_trial_100/model_best.pth
[08/27/2025 23:24:43 INFO]: Training loss at epoch 13: 1.5456472635269165
[08/27/2025 23:24:46 INFO]: New best epoch, val score: -0.8771411878613287
[08/27/2025 23:24:46 INFO]: Saving model to: maddest-Elbert_trial_79/model_best.pth
[08/27/2025 23:25:12 INFO]: Training loss at epoch 2: 1.1507814526557922
[08/27/2025 23:25:16 INFO]: Training loss at epoch 23: 0.8917050361633301
[08/27/2025 23:25:16 INFO]: Training loss at epoch 7: 1.01410710811615
[08/27/2025 23:25:26 INFO]: Training loss at epoch 24: 1.0611096918582916
[08/27/2025 23:25:29 INFO]: New best epoch, val score: -0.8767994363296527
[08/27/2025 23:25:29 INFO]: Saving model to: maddest-Elbert_trial_98/model_best.pth
[08/27/2025 23:25:31 INFO]: Training loss at epoch 11: 0.8353349566459656
[08/27/2025 23:25:58 INFO]: Training loss at epoch 24: 1.4150694608688354
[08/27/2025 23:26:01 INFO]: Training loss at epoch 8: 1.182880312204361
[08/27/2025 23:26:10 INFO]: Training loss at epoch 42: 0.7427986264228821
[08/27/2025 23:26:27 INFO]: Training loss at epoch 7: 0.793927937746048
[08/27/2025 23:26:43 INFO]: Training loss at epoch 25: 0.9349517226219177
[08/27/2025 23:26:48 INFO]: Training loss at epoch 9: 1.1734047830104828
[08/27/2025 23:27:04 INFO]: Training stats: {
    "score": -1.0040646045080293,
    "rmse": 1.0040646045080293
}
[08/27/2025 23:27:04 INFO]: Val stats: {
    "score": -0.9034139583930749,
    "rmse": 0.9034139583930749
}
[08/27/2025 23:27:04 INFO]: Test stats: {
    "score": -0.9748469229428313,
    "rmse": 0.9748469229428313
}
[08/27/2025 23:27:19 INFO]: Training loss at epoch 3: 1.4530524015426636
[08/27/2025 23:27:25 INFO]: Training loss at epoch 3: 1.0011569559574127
[08/27/2025 23:27:26 INFO]: Training loss at epoch 26: 1.09959015250206
[08/27/2025 23:27:51 INFO]: Training loss at epoch 10: 0.8145547211170197
[08/27/2025 23:27:54 INFO]: Training loss at epoch 5: 1.3413555026054382
[08/27/2025 23:28:08 INFO]: Training loss at epoch 27: 1.1624056994915009
[08/27/2025 23:28:12 INFO]: Training loss at epoch 26: 0.8573561906814575
[08/27/2025 23:28:18 INFO]: Training loss at epoch 8: 0.9688236117362976
[08/27/2025 23:28:31 INFO]: Training loss at epoch 3: 0.939901739358902
[08/27/2025 23:28:33 INFO]: Training loss at epoch 11: 1.161121904850006
[08/27/2025 23:28:51 INFO]: Training loss at epoch 28: 1.0780519843101501
[08/27/2025 23:29:15 INFO]: Training loss at epoch 14: 1.1177640557289124
[08/27/2025 23:29:17 INFO]: Training loss at epoch 12: 0.9475498497486115
[08/27/2025 23:29:33 INFO]: Training loss at epoch 29: 0.9293660223484039
[08/27/2025 23:29:49 INFO]: Training stats: {
    "score": -1.0060577528768864,
    "rmse": 1.0060577528768864
}
[08/27/2025 23:29:49 INFO]: Val stats: {
    "score": -0.965319511576256,
    "rmse": 0.965319511576256
}
[08/27/2025 23:29:49 INFO]: Test stats: {
    "score": -0.9828346026383129,
    "rmse": 0.9828346026383129
}
[08/27/2025 23:29:58 INFO]: Training loss at epoch 13: 0.9510276317596436
[08/27/2025 23:30:32 INFO]: Training loss at epoch 30: 0.8170230090618134
[08/27/2025 23:30:33 INFO]: Training loss at epoch 43: 0.9199266135692596
[08/27/2025 23:30:35 INFO]: Training loss at epoch 25: 1.3735109567642212
[08/27/2025 23:30:41 INFO]: Training loss at epoch 14: 0.8267413377761841
[08/27/2025 23:30:56 INFO]: Training loss at epoch 26: 1.1703954339027405
[08/27/2025 23:31:11 INFO]: Training loss at epoch 8: 1.0069049000740051
[08/27/2025 23:31:13 INFO]: Training loss at epoch 31: 0.7849805951118469
[08/27/2025 23:31:18 INFO]: Running Final Evaluation...
[08/27/2025 23:31:33 INFO]: Training accuracy: {
    "score": -1.0048895368080542,
    "rmse": 1.0048895368080542
}
[08/27/2025 23:31:33 INFO]: Val accuracy: {
    "score": -0.9063029217118871,
    "rmse": 0.9063029217118871
}
[08/27/2025 23:31:33 INFO]: Test accuracy: {
    "score": -0.97913389908279,
    "rmse": 0.97913389908279
}
[08/27/2025 23:31:33 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_99",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.97913389908279,
        "rmse": 0.97913389908279
    },
    "train_stats": {
        "score": -1.0048895368080542,
        "rmse": 1.0048895368080542
    },
    "val_stats": {
        "score": -0.9063029217118871,
        "rmse": 0.9063029217118871
    }
}
[08/27/2025 23:31:33 INFO]: Procewss finished for trial maddest-Elbert_trial_99
[08/27/2025 23:31:34 INFO]: 
_________________________________________________

[08/27/2025 23:31:34 INFO]: train_net_for_optune.py main() running.
[08/27/2025 23:31:34 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 1.6770471556710593
  attention_dropout: 0.02965329578610165
  ffn_dropout: 0.02965329578610165
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.0489503546405096e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_101

[08/27/2025 23:31:34 INFO]: This ft_transformer has 23.895 million parameters.
[08/27/2025 23:31:34 INFO]: Training will start at epoch 0.
[08/27/2025 23:31:34 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 23:31:35 INFO]: Training loss at epoch 15: 1.14339280128479
[08/27/2025 23:31:57 INFO]: Training loss at epoch 22: 0.9897059202194214
[08/27/2025 23:32:18 INFO]: Training loss at epoch 16: 0.9233234524726868
[08/27/2025 23:32:37 INFO]: Training loss at epoch 27: 0.9511028528213501
[08/27/2025 23:32:43 INFO]: Training loss at epoch 3: 1.3443411588668823
[08/27/2025 23:33:07 INFO]: Training loss at epoch 17: 0.8293151259422302
[08/27/2025 23:33:48 INFO]: Training loss at epoch 18: 1.057101309299469
[08/27/2025 23:34:18 INFO]: Training loss at epoch 14: 1.2772828936576843
[08/27/2025 23:34:24 INFO]: Training loss at epoch 5: 1.0829012989997864
[08/27/2025 23:34:30 INFO]: Training loss at epoch 19: 1.1187189817428589
[08/27/2025 23:34:46 INFO]: Training stats: {
    "score": -1.0016211068991618,
    "rmse": 1.0016211068991618
}
[08/27/2025 23:34:46 INFO]: Val stats: {
    "score": -0.910155350710727,
    "rmse": 0.910155350710727
}
[08/27/2025 23:34:46 INFO]: Test stats: {
    "score": -0.9724885855190981,
    "rmse": 0.9724885855190981
}
[08/27/2025 23:34:50 INFO]: Training loss at epoch 8: 1.2878965735435486
[08/27/2025 23:34:58 INFO]: Training loss at epoch 4: 1.0826579332351685
[08/27/2025 23:35:01 INFO]: Training loss at epoch 44: 0.7646009624004364
[08/27/2025 23:35:10 INFO]: Training loss at epoch 4: 1.297340840101242
[08/27/2025 23:35:28 INFO]: Training loss at epoch 20: 0.9621008634567261
[08/27/2025 23:35:40 INFO]: Running Final Evaluation...
[08/27/2025 23:35:42 INFO]: Training loss at epoch 26: 0.7505779266357422
[08/27/2025 23:35:54 INFO]: Training loss at epoch 3: 1.4725397229194641
[08/27/2025 23:36:15 INFO]: New best epoch, val score: -0.8867812345380545
[08/27/2025 23:36:15 INFO]: Saving model to: maddest-Elbert_trial_87/model_best.pth
[08/27/2025 23:36:21 INFO]: Training loss at epoch 12: 1.0704493522644043
[08/27/2025 23:36:22 INFO]: Training loss at epoch 21: 0.9088632762432098
[08/27/2025 23:37:01 INFO]: Training loss at epoch 28: 0.8994744420051575
[08/27/2025 23:37:06 INFO]: Training loss at epoch 22: 0.9035064876079559
[08/27/2025 23:37:14 INFO]: New best epoch, val score: -0.9042823844278447
[08/27/2025 23:37:14 INFO]: Saving model to: maddest-Elbert_trial_95/model_best.pth
[08/27/2025 23:37:32 INFO]: Training loss at epoch 27: 1.0258422493934631
[08/27/2025 23:37:39 INFO]: Training loss at epoch 9: 1.0663038492202759
[08/27/2025 23:37:43 INFO]: Training accuracy: {
    "score": -0.9607056777822162,
    "rmse": 0.9607056777822162
}
[08/27/2025 23:37:43 INFO]: Val accuracy: {
    "score": -0.891537534947955,
    "rmse": 0.891537534947955
}
[08/27/2025 23:37:43 INFO]: Test accuracy: {
    "score": -0.9860090876548433,
    "rmse": 0.9860090876548433
}
[08/27/2025 23:37:49 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_72",
    "best_epoch": 13,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9860090876548433,
        "rmse": 0.9860090876548433
    },
    "train_stats": {
        "score": -0.9607056777822162,
        "rmse": 0.9607056777822162
    },
    "val_stats": {
        "score": -0.891537534947955,
        "rmse": 0.891537534947955
    }
}
[08/27/2025 23:37:49 INFO]: Procewss finished for trial maddest-Elbert_trial_72
[08/27/2025 23:37:50 INFO]: 
_________________________________________________

[08/27/2025 23:37:50 INFO]: train_net_for_optune.py main() running.
[08/27/2025 23:37:50 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 1.6648106278122565
  attention_dropout: 0.033642558719053944
  ffn_dropout: 0.033642558719053944
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.1042705271989964e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_102

[08/27/2025 23:37:50 INFO]: This ft_transformer has 23.803 million parameters.
[08/27/2025 23:37:50 INFO]: Training will start at epoch 0.
[08/27/2025 23:37:50 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 23:37:51 INFO]: Training loss at epoch 23: 0.784645676612854
[08/27/2025 23:38:34 INFO]: Training loss at epoch 24: 0.9727322459220886
[08/27/2025 23:38:36 INFO]: Training loss at epoch 6: 1.0934650301933289
[08/27/2025 23:39:06 INFO]: Training loss at epoch 15: 1.0169775485992432
[08/27/2025 23:39:15 INFO]: Training loss at epoch 4: 1.29886794090271
[08/27/2025 23:39:16 INFO]: Training loss at epoch 25: 1.11758691072464
[08/27/2025 23:39:46 INFO]: Training loss at epoch 23: 0.9069723784923553
[08/27/2025 23:40:02 INFO]: Training loss at epoch 26: 1.1926454305648804
[08/27/2025 23:40:23 INFO]: Training loss at epoch 9: 1.1022164225578308
[08/27/2025 23:40:30 INFO]: New best epoch, val score: -0.88349610380649
[08/27/2025 23:40:30 INFO]: Saving model to: maddest-Elbert_trial_94/model_best.pth
[08/27/2025 23:40:47 INFO]: Training loss at epoch 27: 0.9119159579277039
[08/27/2025 23:40:49 INFO]: Training stats: {
    "score": -0.9992649387956836,
    "rmse": 0.9992649387956836
}
[08/27/2025 23:40:49 INFO]: Val stats: {
    "score": -0.9182329838835,
    "rmse": 0.9182329838835
}
[08/27/2025 23:40:49 INFO]: Test stats: {
    "score": -0.9716001554830276,
    "rmse": 0.9716001554830276
}
[08/27/2025 23:40:52 INFO]: Training loss at epoch 27: 0.9201310575008392
[08/27/2025 23:40:56 INFO]: Training loss at epoch 4: 1.2022728323936462
[08/27/2025 23:41:28 INFO]: Training loss at epoch 29: 0.8816699385643005
[08/27/2025 23:41:29 INFO]: New best epoch, val score: -0.8811043821905892
[08/27/2025 23:41:29 INFO]: Saving model to: maddest-Elbert_trial_87/model_best.pth
[08/27/2025 23:41:34 INFO]: Training loss at epoch 28: 0.8150248527526855
[08/27/2025 23:41:53 INFO]: Training loss at epoch 0: 1.2369098663330078
[08/27/2025 23:42:15 INFO]: Training loss at epoch 29: 1.0061170756816864
[08/27/2025 23:42:31 INFO]: Training stats: {
    "score": -1.00067617851152,
    "rmse": 1.00067617851152
}
[08/27/2025 23:42:31 INFO]: Val stats: {
    "score": -0.9138754165947711,
    "rmse": 0.9138754165947711
}
[08/27/2025 23:42:31 INFO]: Test stats: {
    "score": -0.9716509273132685,
    "rmse": 0.9716509273132685
}
[08/27/2025 23:42:43 INFO]: Training loss at epoch 5: 1.2943063378334045
[08/27/2025 23:43:00 INFO]: Training stats: {
    "score": -1.0010480752460862,
    "rmse": 1.0010480752460862
}
[08/27/2025 23:43:00 INFO]: Val stats: {
    "score": -0.948992039642956,
    "rmse": 0.948992039642956
}
[08/27/2025 23:43:00 INFO]: Test stats: {
    "score": -0.9750032878672447,
    "rmse": 0.9750032878672447
}
[08/27/2025 23:43:06 INFO]: Training loss at epoch 5: 1.1492913365364075
[08/27/2025 23:43:13 INFO]: Training loss at epoch 30: 1.2078988552093506
[08/27/2025 23:43:14 INFO]: New best epoch, val score: -1.1787459727328617
[08/27/2025 23:43:14 INFO]: Saving model to: maddest-Elbert_trial_101/model_best.pth
[08/27/2025 23:43:39 INFO]: Training loss at epoch 9: 0.9107073545455933
[08/27/2025 23:43:53 INFO]: Training stats: {
    "score": -1.0087417060650723,
    "rmse": 1.0087417060650723
}
[08/27/2025 23:43:53 INFO]: Val stats: {
    "score": -0.9766272969983163,
    "rmse": 0.9766272969983163
}
[08/27/2025 23:43:53 INFO]: Test stats: {
    "score": -0.982132348790428,
    "rmse": 0.982132348790428
}
[08/27/2025 23:44:05 INFO]: Training loss at epoch 15: 1.3513431549072266
[08/27/2025 23:44:14 INFO]: Training loss at epoch 31: 1.049266755580902
[08/27/2025 23:44:26 INFO]: Training loss at epoch 28: 1.2040464282035828
[08/27/2025 23:44:56 INFO]: Training loss at epoch 32: 1.1574492156505585
[08/27/2025 23:45:20 INFO]: Training loss at epoch 6: 1.2849702537059784
[08/27/2025 23:45:38 INFO]: Training loss at epoch 33: 0.902521014213562
[08/27/2025 23:46:11 INFO]: Training loss at epoch 28: 1.1833036243915558
[08/27/2025 23:46:19 INFO]: Training loss at epoch 34: 1.0225289463996887
[08/27/2025 23:46:25 INFO]: Training stats: {
    "score": -0.9716402868839975,
    "rmse": 0.9716402868839975
}
[08/27/2025 23:46:25 INFO]: Val stats: {
    "score": -0.9168260793677984,
    "rmse": 0.9168260793677984
}
[08/27/2025 23:46:25 INFO]: Test stats: {
    "score": -0.9749389635708585,
    "rmse": 0.9749389635708585
}
[08/27/2025 23:46:53 INFO]: Training loss at epoch 4: 1.135864794254303
[08/27/2025 23:47:00 INFO]: Training loss at epoch 35: 1.0123856663703918
[08/27/2025 23:47:20 INFO]: Training loss at epoch 13: 1.1061568856239319
[08/27/2025 23:47:35 INFO]: Training loss at epoch 30: 0.9154011309146881
[08/27/2025 23:47:42 INFO]: Training loss at epoch 36: 0.986994206905365
[08/27/2025 23:47:53 INFO]: Training loss at epoch 24: 1.1024371981620789
[08/27/2025 23:48:09 INFO]: Training loss at epoch 0: 1.0620761513710022
[08/27/2025 23:48:29 INFO]: Training loss at epoch 37: 1.2287978529930115
[08/27/2025 23:48:35 INFO]: Running Final Evaluation...
[08/27/2025 23:48:52 INFO]: Training accuracy: {
    "score": -1.0050364340858782,
    "rmse": 1.0050364340858782
}
[08/27/2025 23:48:52 INFO]: Val accuracy: {
    "score": -0.9014681384003826,
    "rmse": 0.9014681384003826
}
[08/27/2025 23:48:52 INFO]: Test accuracy: {
    "score": -0.9758324720656106,
    "rmse": 0.9758324720656106
}
[08/27/2025 23:48:52 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_100",
    "best_epoch": 6,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9758324720656106,
        "rmse": 0.9758324720656106
    },
    "train_stats": {
        "score": -1.0050364340858782,
        "rmse": 1.0050364340858782
    },
    "val_stats": {
        "score": -0.9014681384003826,
        "rmse": 0.9014681384003826
    }
}
[08/27/2025 23:48:52 INFO]: Procewss finished for trial maddest-Elbert_trial_100
[08/27/2025 23:48:52 INFO]: 
_________________________________________________

[08/27/2025 23:48:52 INFO]: train_net_for_optune.py main() running.
[08/27/2025 23:48:52 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 1.9248920073299711
  attention_dropout: 0.033987600739045054
  ffn_dropout: 0.033987600739045054
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.0237689904434134e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_103

[08/27/2025 23:48:53 INFO]: This ft_transformer has 25.848 million parameters.
[08/27/2025 23:48:53 INFO]: Training will start at epoch 0.
[08/27/2025 23:48:53 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 23:49:17 INFO]: Training loss at epoch 16: 1.3125395774841309
[08/27/2025 23:49:22 INFO]: Training loss at epoch 5: 0.9999039471149445
[08/27/2025 23:49:35 INFO]: New best epoch, val score: -0.9174972347856417
[08/27/2025 23:49:35 INFO]: Saving model to: maddest-Elbert_trial_102/model_best.pth
[08/27/2025 23:49:36 INFO]: Training loss at epoch 7: 1.0216700732707977
[08/27/2025 23:50:17 INFO]: Training loss at epoch 10: 0.8130426108837128
[08/27/2025 23:50:17 INFO]: Training loss at epoch 5: 1.165183424949646
[08/27/2025 23:50:33 INFO]: Training loss at epoch 6: 1.2095046639442444
[08/27/2025 23:51:00 INFO]: Training loss at epoch 6: 1.0585001707077026
[08/27/2025 23:51:06 INFO]: Training loss at epoch 29: 1.1949067115783691
[08/27/2025 23:51:23 INFO]: Training loss at epoch 29: 0.8873080015182495
[08/27/2025 23:51:33 INFO]: New best epoch, val score: -0.8833578475884306
[08/27/2025 23:51:33 INFO]: Saving model to: maddest-Elbert_trial_94/model_best.pth
[08/27/2025 23:52:07 INFO]: Training loss at epoch 31: 1.243387520313263
[08/27/2025 23:53:03 INFO]: Training loss at epoch 10: 1.0315135717391968
[08/27/2025 23:53:09 INFO]: Training stats: {
    "score": -0.956030877141601,
    "rmse": 0.956030877141601
}
[08/27/2025 23:53:09 INFO]: Val stats: {
    "score": -0.935697462256124,
    "rmse": 0.935697462256124
}
[08/27/2025 23:53:09 INFO]: Test stats: {
    "score": -0.9743838348245623,
    "rmse": 0.9743838348245623
}
[08/27/2025 23:53:25 INFO]: Training stats: {
    "score": -1.0400151070997146,
    "rmse": 1.0400151070997146
}
[08/27/2025 23:53:25 INFO]: Val stats: {
    "score": -1.0620252949884033,
    "rmse": 1.0620252949884033
}
[08/27/2025 23:53:25 INFO]: Test stats: {
    "score": -1.023290002341945,
    "rmse": 1.023290002341945
}
[08/27/2025 23:53:39 INFO]: Training loss at epoch 16: 1.2025483846664429
[08/27/2025 23:53:41 INFO]: Training loss at epoch 1: 1.4519326090812683
[08/27/2025 23:54:59 INFO]: Training loss at epoch 10: 0.8843287825584412
[08/27/2025 23:55:03 INFO]: New best epoch, val score: -1.0202896065160847
[08/27/2025 23:55:03 INFO]: Saving model to: maddest-Elbert_trial_101/model_best.pth
[08/27/2025 23:55:43 INFO]: Training loss at epoch 25: 0.9518190026283264
[08/27/2025 23:56:08 INFO]: Training loss at epoch 7: 0.881623238325119
[08/27/2025 23:56:27 INFO]: Training loss at epoch 32: 0.9713431298732758
[08/27/2025 23:57:34 INFO]: Training loss at epoch 6: 1.3529448509216309
[08/27/2025 23:57:36 INFO]: Training loss at epoch 5: 1.4595906734466553
[08/27/2025 23:58:01 INFO]: Training loss at epoch 14: 0.9350296258926392
[08/27/2025 23:58:13 INFO]: Training loss at epoch 7: 1.0805829167366028
[08/27/2025 23:58:20 INFO]: Training loss at epoch 30: 1.2290114164352417
[08/27/2025 23:58:43 INFO]: Training loss at epoch 7: 0.9244038760662079
[08/27/2025 23:59:09 INFO]: Training loss at epoch 17: 1.1245784163475037
[08/27/2025 23:59:34 INFO]: Training loss at epoch 11: 0.7833855450153351
[08/27/2025 23:59:47 INFO]: Training loss at epoch 0: 1.1246227622032166
[08/27/2025 23:59:52 INFO]: Training loss at epoch 1: 1.4146873950958252
[08/27/2025 23:59:58 INFO]: Training loss at epoch 30: 1.0961388945579529
[08/28/2025 00:00:18 INFO]: Training loss at epoch 8: 0.8441324234008789
[08/28/2025 00:00:39 INFO]: New best epoch, val score: -0.9036256311645512
[08/28/2025 00:00:39 INFO]: Saving model to: maddest-Elbert_trial_89/model_best.pth
[08/28/2025 00:00:53 INFO]: Training loss at epoch 33: 1.1149734556674957
[08/28/2025 00:01:00 INFO]: Training loss at epoch 6: 1.1220592260360718
[08/28/2025 00:01:15 INFO]: New best epoch, val score: -0.9468667596581786
[08/28/2025 00:01:15 INFO]: Saving model to: maddest-Elbert_trial_103/model_best.pth
[08/28/2025 00:02:12 INFO]: Training loss at epoch 11: 0.9558115601539612
[08/28/2025 00:02:16 INFO]: New best epoch, val score: -0.8831811107264494
[08/28/2025 00:02:16 INFO]: Saving model to: maddest-Elbert_trial_94/model_best.pth
[08/28/2025 00:03:20 INFO]: Training loss at epoch 11: 1.0503725111484528
[08/28/2025 00:03:21 INFO]: Training loss at epoch 17: 0.8043443262577057
[08/28/2025 00:03:21 INFO]: New best epoch, val score: -0.8957767298022583
[08/28/2025 00:03:21 INFO]: Saving model to: maddest-Elbert_trial_90/model_best.pth
[08/28/2025 00:03:28 INFO]: Training loss at epoch 31: 0.8756193816661835
[08/28/2025 00:03:30 INFO]: Training loss at epoch 26: 0.96148881316185
[08/28/2025 00:05:16 INFO]: Training loss at epoch 2: 1.5858709216117859
[08/28/2025 00:05:20 INFO]: Training loss at epoch 34: 1.3492555916309357
[08/28/2025 00:05:43 INFO]: Training loss at epoch 7: 1.1928250193595886
[08/28/2025 00:05:56 INFO]: Training loss at epoch 8: 1.275668203830719
[08/28/2025 00:06:29 INFO]: Training loss at epoch 8: 1.0106742978096008
[08/28/2025 00:06:37 INFO]: New best epoch, val score: -0.9611910438015207
[08/28/2025 00:06:37 INFO]: Saving model to: maddest-Elbert_trial_101/model_best.pth
[08/28/2025 00:06:47 INFO]: Training loss at epoch 31: 0.9088211357593536
[08/28/2025 00:06:56 INFO]: Training loss at epoch 8: 1.1763963997364044
[08/28/2025 00:08:16 INFO]: Training loss at epoch 6: 1.107443243265152
[08/28/2025 00:08:32 INFO]: Training loss at epoch 32: 0.9741109907627106
[08/28/2025 00:08:44 INFO]: Training loss at epoch 15: 1.1270400285720825
[08/28/2025 00:08:49 INFO]: Training loss at epoch 12: 0.9994463920593262
[08/28/2025 00:09:08 INFO]: Training loss at epoch 18: 1.072173833847046
[08/28/2025 00:09:44 INFO]: Training loss at epoch 35: 0.9072211980819702
[08/28/2025 00:09:53 INFO]: New best epoch, val score: -0.897989399796535
[08/28/2025 00:09:53 INFO]: Saving model to: maddest-Elbert_trial_89/model_best.pth
[08/28/2025 00:11:07 INFO]: Training loss at epoch 9: 1.0690013766288757
[08/28/2025 00:11:18 INFO]: Training loss at epoch 27: 1.0311623215675354
[08/28/2025 00:11:25 INFO]: Training loss at epoch 2: 0.9767035841941833
[08/28/2025 00:11:28 INFO]: Training loss at epoch 12: 0.9950049519538879
[08/28/2025 00:11:44 INFO]: Training loss at epoch 12: 1.1625240445137024
[08/28/2025 00:11:49 INFO]: Training loss at epoch 7: 0.8745553493499756
[08/28/2025 00:12:12 INFO]: Training loss at epoch 1: 1.0169494152069092
[08/28/2025 00:12:33 INFO]: New best epoch, val score: -0.8872596164313764
[08/28/2025 00:12:33 INFO]: Saving model to: maddest-Elbert_trial_90/model_best.pth
[08/28/2025 00:12:49 INFO]: Training loss at epoch 18: 0.890163391828537
[08/28/2025 00:13:22 INFO]: Training loss at epoch 32: 1.065224826335907
[08/28/2025 00:13:32 INFO]: Training loss at epoch 33: 0.8488019406795502
[08/28/2025 00:13:36 INFO]: Training loss at epoch 9: 1.2565426230430603
[08/28/2025 00:13:37 INFO]: New best epoch, val score: -0.8870189510884742
[08/28/2025 00:13:37 INFO]: Saving model to: maddest-Elbert_trial_103/model_best.pth
[08/28/2025 00:13:54 INFO]: Training loss at epoch 8: 1.0556093454360962
[08/28/2025 00:13:57 INFO]: New best epoch, val score: -0.8799404312000327
[08/28/2025 00:13:57 INFO]: Saving model to: maddest-Elbert_trial_85/model_best.pth
[08/28/2025 00:14:07 INFO]: Training loss at epoch 36: 0.9728339314460754
[08/28/2025 00:14:10 INFO]: New best epoch, val score: -0.8765237083737718
[08/28/2025 00:14:10 INFO]: Saving model to: maddest-Elbert_trial_79/model_best.pth
[08/28/2025 00:14:13 INFO]: Training loss at epoch 9: 1.1182921528816223
[08/28/2025 00:14:45 INFO]: Training stats: {
    "score": -1.0099934554621215,
    "rmse": 1.0099934554621215
}
[08/28/2025 00:14:45 INFO]: Val stats: {
    "score": -0.9957743183219152,
    "rmse": 0.9957743183219152
}
[08/28/2025 00:14:45 INFO]: Test stats: {
    "score": -1.007784404811448,
    "rmse": 1.007784404811448
}
[08/28/2025 00:16:13 INFO]: Training stats: {
    "score": -1.051941552087544,
    "rmse": 1.051941552087544
}
[08/28/2025 00:16:13 INFO]: Val stats: {
    "score": -1.067865273625144,
    "rmse": 1.067865273625144
}
[08/28/2025 00:16:13 INFO]: Test stats: {
    "score": -1.029325828095927,
    "rmse": 1.029325828095927
}
[08/28/2025 00:16:50 INFO]: Training stats: {
    "score": -1.0049531877481759,
    "rmse": 1.0049531877481759
}
[08/28/2025 00:16:50 INFO]: Val stats: {
    "score": -0.9824640642812205,
    "rmse": 0.9824640642812205
}
[08/28/2025 00:16:50 INFO]: Test stats: {
    "score": -0.9833610831745749,
    "rmse": 0.9833610831745749
}
[08/28/2025 00:16:58 INFO]: Training loss at epoch 3: 1.285489797592163
[08/28/2025 00:17:36 INFO]: Training loss at epoch 9: 0.806840717792511
[08/28/2025 00:18:02 INFO]: Training loss at epoch 13: 1.2638774812221527
[08/28/2025 00:18:31 INFO]: Training loss at epoch 37: 0.9231690764427185
[08/28/2025 00:18:35 INFO]: Training loss at epoch 34: 1.1704577803611755
[08/28/2025 00:19:00 INFO]: Training loss at epoch 7: 1.3215370178222656
[08/28/2025 00:19:03 INFO]: Training loss at epoch 28: 0.9695143699645996
[08/28/2025 00:19:06 INFO]: Training loss at epoch 19: 1.0716643929481506
[08/28/2025 00:19:07 INFO]: New best epoch, val score: -0.8973279669287526
[08/28/2025 00:19:07 INFO]: Saving model to: maddest-Elbert_trial_89/model_best.pth
[08/28/2025 00:19:28 INFO]: Training loss at epoch 16: 0.8771255016326904
[08/28/2025 00:20:01 INFO]: Training loss at epoch 33: 1.0619529485702515
[08/28/2025 00:20:07 INFO]: Training loss at epoch 13: 1.1078625321388245
[08/28/2025 00:20:42 INFO]: Training loss at epoch 13: 0.929499089717865
[08/28/2025 00:21:14 INFO]: Training stats: {
    "score": -1.0090099638814567,
    "rmse": 1.0090099638814567
}
[08/28/2025 00:21:14 INFO]: Val stats: {
    "score": -0.9996888451989159,
    "rmse": 0.9996888451989159
}
[08/28/2025 00:21:14 INFO]: Test stats: {
    "score": -1.0029899420924782,
    "rmse": 1.0029899420924782
}
[08/28/2025 00:21:46 INFO]: New best epoch, val score: -0.8866964104438184
[08/28/2025 00:21:46 INFO]: Saving model to: maddest-Elbert_trial_90/model_best.pth
[08/28/2025 00:22:04 INFO]: Training loss at epoch 9: 1.2068536579608917
[08/28/2025 00:22:20 INFO]: Training loss at epoch 19: 1.061790645122528
[08/28/2025 00:22:29 INFO]: Training stats: {
    "score": -1.0139449230360722,
    "rmse": 1.0139449230360722
}
[08/28/2025 00:22:29 INFO]: Val stats: {
    "score": -0.9880195175538216,
    "rmse": 0.9880195175538216
}
[08/28/2025 00:22:29 INFO]: Test stats: {
    "score": -0.9864367991623222,
    "rmse": 0.9864367991623222
}
[08/28/2025 00:22:30 INFO]: Training loss at epoch 8: 1.0146430432796478
[08/28/2025 00:22:54 INFO]: Training loss at epoch 38: 1.0092182457447052
[08/28/2025 00:22:59 INFO]: Training loss at epoch 3: 1.201825112104416
[08/28/2025 00:23:37 INFO]: Training loss at epoch 35: 0.8271965980529785
[08/28/2025 00:23:51 INFO]: Training loss at epoch 10: 1.3135365843772888
[08/28/2025 00:24:33 INFO]: Training loss at epoch 10: 0.8689301013946533
[08/28/2025 00:24:35 INFO]: Training loss at epoch 2: 1.333531141281128
[08/28/2025 00:24:48 INFO]: Training stats: {
    "score": -1.0059287455717358,
    "rmse": 1.0059287455717358
}
[08/28/2025 00:24:48 INFO]: Val stats: {
    "score": -0.96531348566153,
    "rmse": 0.96531348566153
}
[08/28/2025 00:24:48 INFO]: Test stats: {
    "score": -0.9791363971896635,
    "rmse": 0.9791363971896635
}
[08/28/2025 00:25:28 INFO]: New best epoch, val score: -0.8891831559465307
[08/28/2025 00:25:28 INFO]: Saving model to: maddest-Elbert_trial_96/model_best.pth
[08/28/2025 00:25:28 INFO]: Training loss at epoch 10: 0.9960357844829559
[08/28/2025 00:25:34 INFO]: Training stats: {
    "score": -1.0599214896572975,
    "rmse": 1.0599214896572975
}
[08/28/2025 00:25:34 INFO]: Val stats: {
    "score": -0.8851640324913628,
    "rmse": 0.8851640324913628
}
[08/28/2025 00:25:34 INFO]: Test stats: {
    "score": -1.036843918744861,
    "rmse": 1.036843918744861
}
[08/28/2025 00:26:37 INFO]: Training loss at epoch 34: 0.9781689047813416
[08/28/2025 00:26:49 INFO]: Training loss at epoch 29: 0.9850593209266663
[08/28/2025 00:27:14 INFO]: Training loss at epoch 14: 1.0071008801460266
[08/28/2025 00:27:14 INFO]: Training loss at epoch 39: 0.846191018819809
[08/28/2025 00:28:26 INFO]: Training loss at epoch 14: 0.8965338468551636
[08/28/2025 00:28:32 INFO]: Training loss at epoch 4: 1.0956326723098755
[08/28/2025 00:28:37 INFO]: Training loss at epoch 36: 0.7731707096099854
[08/28/2025 00:28:42 INFO]: Training stats: {
    "score": -1.0038069659455693,
    "rmse": 1.0038069659455693
}
[08/28/2025 00:28:42 INFO]: Val stats: {
    "score": -0.9629166462144161,
    "rmse": 0.9629166462144161
}
[08/28/2025 00:28:42 INFO]: Test stats: {
    "score": -0.9785683555877684,
    "rmse": 0.9785683555877684
}
[08/28/2025 00:29:27 INFO]: Training stats: {
    "score": -0.9977706545457075,
    "rmse": 0.9977706545457075
}
[08/28/2025 00:29:27 INFO]: Val stats: {
    "score": -0.9066253873785636,
    "rmse": 0.9066253873785636
}
[08/28/2025 00:29:27 INFO]: Test stats: {
    "score": -0.9721301166508292,
    "rmse": 0.9721301166508292
}
[08/28/2025 00:29:41 INFO]: Training loss at epoch 8: 0.9586341679096222
[08/28/2025 00:29:50 INFO]: Training loss at epoch 14: 0.91734179854393
[08/28/2025 00:29:51 INFO]: New best epoch, val score: -0.888483671219045
[08/28/2025 00:29:51 INFO]: Saving model to: maddest-Elbert_trial_101/model_best.pth
[08/28/2025 00:30:09 INFO]: Training loss at epoch 17: 0.9178903102874756
[08/28/2025 00:31:26 INFO]: Training loss at epoch 11: 1.0751502513885498
[08/28/2025 00:31:50 INFO]: Training loss at epoch 10: 1.1389853060245514
[08/28/2025 00:32:14 INFO]: Training loss at epoch 11: 1.3461580872535706
[08/28/2025 00:32:20 INFO]: New best epoch, val score: -0.8815755583621478
[08/28/2025 00:32:20 INFO]: Saving model to: maddest-Elbert_trial_97/model_best.pth
[08/28/2025 00:32:21 INFO]: Training loss at epoch 20: 0.9574106633663177
[08/28/2025 00:32:53 INFO]: Training loss at epoch 10: 0.8534448146820068
[08/28/2025 00:33:04 INFO]: Training loss at epoch 40: 1.1150233447551727
[08/28/2025 00:33:07 INFO]: Training loss at epoch 9: 0.9684629142284393
[08/28/2025 00:33:12 INFO]: Training loss at epoch 35: 0.9113480150699615
[08/28/2025 00:33:38 INFO]: Training loss at epoch 37: 0.9497854709625244
[08/28/2025 00:34:28 INFO]: Training loss at epoch 4: 1.3979563415050507
[08/28/2025 00:35:00 INFO]: Training loss at epoch 20: 1.1459391117095947
[08/28/2025 00:36:06 INFO]: Training loss at epoch 11: 0.9237520694732666
[08/28/2025 00:36:23 INFO]: Training loss at epoch 15: 1.202975571155548
[08/28/2025 00:36:41 INFO]: Training stats: {
    "score": -1.0132687347127927,
    "rmse": 1.0132687347127927
}
[08/28/2025 00:36:41 INFO]: Val stats: {
    "score": -0.8873225189624955,
    "rmse": 0.8873225189624955
}
[08/28/2025 00:36:41 INFO]: Test stats: {
    "score": -0.9881948863753286,
    "rmse": 0.9881948863753286
}
[08/28/2025 00:36:45 INFO]: Training loss at epoch 15: 0.9586826264858246
[08/28/2025 00:36:50 INFO]: Training loss at epoch 3: 1.1016490459442139
[08/28/2025 00:37:10 INFO]: Training loss at epoch 30: 1.1571593880653381
[08/28/2025 00:37:21 INFO]: New best epoch, val score: -0.8686398024129424
[08/28/2025 00:37:21 INFO]: Saving model to: maddest-Elbert_trial_92/model_best.pth
[08/28/2025 00:37:24 INFO]: Training loss at epoch 41: 0.9550784230232239
[08/28/2025 00:38:36 INFO]: Training loss at epoch 38: 0.6886121332645416
[08/28/2025 00:38:58 INFO]: Training loss at epoch 15: 1.0786751806735992
[08/28/2025 00:39:03 INFO]: Training loss at epoch 12: 1.3525730967521667
[08/28/2025 00:39:47 INFO]: Training loss at epoch 36: 1.0995806455612183
[08/28/2025 00:39:53 INFO]: Training loss at epoch 12: 1.321773886680603
[08/28/2025 00:40:02 INFO]: Training loss at epoch 5: 1.2700299620628357
[08/28/2025 00:40:18 INFO]: Training loss at epoch 9: 1.1391506791114807
[08/28/2025 00:40:48 INFO]: Training loss at epoch 18: 1.0987382531166077
[08/28/2025 00:40:49 INFO]: New best epoch, val score: -0.8842771902154359
[08/28/2025 00:40:49 INFO]: Saving model to: maddest-Elbert_trial_96/model_best.pth
[08/28/2025 00:40:58 INFO]: Training loss at epoch 11: 1.164213240146637
[08/28/2025 00:41:47 INFO]: Training loss at epoch 42: 1.0508893728256226
[08/28/2025 00:42:12 INFO]: Training loss at epoch 21: 0.9933605194091797
[08/28/2025 00:42:28 INFO]: Training loss at epoch 11: 0.9813072979450226
[08/28/2025 00:43:36 INFO]: Training loss at epoch 39: 0.7086226344108582
[08/28/2025 00:43:53 INFO]: Training stats: {
    "score": -1.0192023819282146,
    "rmse": 1.0192023819282146
}
[08/28/2025 00:43:53 INFO]: Val stats: {
    "score": -1.0393455984418163,
    "rmse": 1.0393455984418163
}
[08/28/2025 00:43:53 INFO]: Test stats: {
    "score": -1.014597096890958,
    "rmse": 1.014597096890958
}
[08/28/2025 00:44:24 INFO]: Training loss at epoch 21: 0.9084304571151733
[08/28/2025 00:44:53 INFO]: Training loss at epoch 31: 0.9425502121448517
[08/28/2025 00:45:05 INFO]: Training loss at epoch 16: 0.8905256986618042
[08/28/2025 00:45:16 INFO]: Training stats: {
    "score": -0.9205089259085028,
    "rmse": 0.9205089259085028
}
[08/28/2025 00:45:16 INFO]: Val stats: {
    "score": -0.8924047791629482,
    "rmse": 0.8924047791629482
}
[08/28/2025 00:45:16 INFO]: Test stats: {
    "score": -0.9984889334150723,
    "rmse": 0.9984889334150723
}
[08/28/2025 00:45:32 INFO]: Training loss at epoch 16: 1.0686144530773163
[08/28/2025 00:45:54 INFO]: Training loss at epoch 5: 1.0229662656784058
[08/28/2025 00:46:05 INFO]: Training loss at epoch 43: 1.2372620105743408
[08/28/2025 00:46:21 INFO]: Training loss at epoch 37: 1.1702080368995667
[08/28/2025 00:46:40 INFO]: Training loss at epoch 13: 1.1403143405914307
[08/28/2025 00:46:46 INFO]: Training loss at epoch 12: 1.2377134561538696
[08/28/2025 00:47:21 INFO]: Training loss at epoch 10: 0.8262985646724701
[08/28/2025 00:47:34 INFO]: Training loss at epoch 13: 0.8662117421627045
[08/28/2025 00:48:09 INFO]: Training loss at epoch 16: 1.1334016025066376
[08/28/2025 00:49:03 INFO]: Training loss at epoch 12: 1.1428985595703125
[08/28/2025 00:49:12 INFO]: Training loss at epoch 4: 1.0117283463478088
[08/28/2025 00:50:18 INFO]: Training loss at epoch 40: 0.9173331558704376
[08/28/2025 00:50:27 INFO]: Training loss at epoch 44: 0.9434435665607452
[08/28/2025 00:50:58 INFO]: Running Final Evaluation...
[08/28/2025 00:51:27 INFO]: Training loss at epoch 19: 1.0928033888339996
[08/28/2025 00:51:34 INFO]: Training loss at epoch 6: 0.9811519086360931
[08/28/2025 00:52:02 INFO]: Training loss at epoch 22: 1.1018012762069702
[08/28/2025 00:52:28 INFO]: Training accuracy: {
    "score": -1.039652954525763,
    "rmse": 1.039652954525763
}
[08/28/2025 00:52:28 INFO]: Val accuracy: {
    "score": -0.8823323049997727,
    "rmse": 0.8823323049997727
}
[08/28/2025 00:52:28 INFO]: Test accuracy: {
    "score": -1.0129463100237177,
    "rmse": 1.0129463100237177
}
[08/28/2025 00:52:28 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_88",
    "best_epoch": 13,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0129463100237177,
        "rmse": 1.0129463100237177
    },
    "train_stats": {
        "score": -1.039652954525763,
        "rmse": 1.039652954525763
    },
    "val_stats": {
        "score": -0.8823323049997727,
        "rmse": 0.8823323049997727
    }
}
[08/28/2025 00:52:28 INFO]: Procewss finished for trial maddest-Elbert_trial_88
[08/28/2025 00:52:28 INFO]: 
_________________________________________________

[08/28/2025 00:52:28 INFO]: train_net_for_optune.py main() running.
[08/28/2025 00:52:28 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 1.2756343776692574
  attention_dropout: 0.03070174199506248
  ffn_dropout: 0.03070174199506248
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.9967770603527803e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_104

[08/28/2025 00:52:28 INFO]: This ft_transformer has 20.742 million parameters.
[08/28/2025 00:52:28 INFO]: Training will start at epoch 0.
[08/28/2025 00:52:28 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 00:52:39 INFO]: Training loss at epoch 32: 0.7654381841421127
[08/28/2025 00:52:55 INFO]: New best epoch, val score: -0.8884598084505851
[08/28/2025 00:52:55 INFO]: Saving model to: maddest-Elbert_trial_101/model_best.pth
[08/28/2025 00:52:58 INFO]: Training loss at epoch 38: 1.0717911124229431
[08/28/2025 00:53:06 INFO]: Training loss at epoch 12: 0.9925637245178223
[08/28/2025 00:53:13 INFO]: New best epoch, val score: -0.8809559429907319
[08/28/2025 00:53:13 INFO]: Saving model to: maddest-Elbert_trial_84/model_best.pth
[08/28/2025 00:53:27 INFO]: Training loss at epoch 17: 0.8688235580921173
[08/28/2025 00:53:51 INFO]: Training loss at epoch 22: 0.8881075084209442
[08/28/2025 00:54:18 INFO]: Training loss at epoch 14: 1.263861358165741
[08/28/2025 00:54:32 INFO]: Training loss at epoch 10: 1.0338948965072632
[08/28/2025 00:54:40 INFO]: Training loss at epoch 17: 1.003283679485321
[08/28/2025 00:55:03 INFO]: Training stats: {
    "score": -0.9542275920393672,
    "rmse": 0.9542275920393672
}
[08/28/2025 00:55:03 INFO]: Val stats: {
    "score": -0.9194270217783458,
    "rmse": 0.9194270217783458
}
[08/28/2025 00:55:03 INFO]: Test stats: {
    "score": -0.9863522486591537,
    "rmse": 0.9863522486591537
}
[08/28/2025 00:55:15 INFO]: Training loss at epoch 14: 1.0119624137878418
[08/28/2025 00:55:17 INFO]: Training loss at epoch 41: 0.854615181684494
[08/28/2025 00:55:48 INFO]: New best epoch, val score: -0.8915107396763099
[08/28/2025 00:55:48 INFO]: Saving model to: maddest-Elbert_trial_95/model_best.pth
[08/28/2025 00:57:07 INFO]: Training loss at epoch 13: 1.0634913444519043
[08/28/2025 00:57:18 INFO]: Training loss at epoch 17: 0.9540864527225494
[08/28/2025 00:57:24 INFO]: Training loss at epoch 6: 0.8361315727233887
[08/28/2025 00:57:27 INFO]: Training loss at epoch 13: 1.1734935641288757
[08/28/2025 00:57:59 INFO]: Training loss at epoch 11: 1.135345697402954
[08/28/2025 00:58:42 INFO]: New best epoch, val score: -0.8680231416094331
[08/28/2025 00:58:42 INFO]: Saving model to: maddest-Elbert_trial_92/model_best.pth
[08/28/2025 00:59:33 INFO]: Training loss at epoch 39: 1.217352032661438
[08/28/2025 01:00:15 INFO]: Training loss at epoch 42: 0.7122996151447296
[08/28/2025 01:00:20 INFO]: Training loss at epoch 33: 0.9519659280776978
[08/28/2025 01:01:30 INFO]: Training loss at epoch 5: 1.049617886543274
[08/28/2025 01:01:31 INFO]: Training loss at epoch 0: 0.8870505690574646
[08/28/2025 01:01:44 INFO]: Training loss at epoch 18: 1.0597822070121765
[08/28/2025 01:01:47 INFO]: Training stats: {
    "score": -0.9872076908193622,
    "rmse": 0.9872076908193622
}
[08/28/2025 01:01:47 INFO]: Val stats: {
    "score": -0.9111748551951608,
    "rmse": 0.9111748551951608
}
[08/28/2025 01:01:47 INFO]: Test stats: {
    "score": -0.968999349963063,
    "rmse": 0.968999349963063
}
[08/28/2025 01:01:51 INFO]: Training loss at epoch 15: 1.1775094866752625
[08/28/2025 01:01:52 INFO]: Training loss at epoch 23: 1.2209377884864807
[08/28/2025 01:02:42 INFO]: New best epoch, val score: -0.9407773632770342
[08/28/2025 01:02:42 INFO]: Saving model to: maddest-Elbert_trial_104/model_best.pth
[08/28/2025 01:02:53 INFO]: Training loss at epoch 15: 1.1253400444984436
[08/28/2025 01:03:05 INFO]: Training loss at epoch 7: 1.0154556334018707
[08/28/2025 01:03:16 INFO]: Training loss at epoch 23: 0.7934433817863464
[08/28/2025 01:03:45 INFO]: Training loss at epoch 13: 0.9940382838249207
[08/28/2025 01:03:50 INFO]: Training loss at epoch 18: 0.8220824897289276
[08/28/2025 01:04:27 INFO]: New best epoch, val score: -0.8880062825604238
[08/28/2025 01:04:27 INFO]: Saving model to: maddest-Elbert_trial_101/model_best.pth
[08/28/2025 01:05:08 INFO]: Training loss at epoch 14: 1.0037913024425507
[08/28/2025 01:05:09 INFO]: Training loss at epoch 11: 1.0508702993392944
[08/28/2025 01:05:14 INFO]: Training loss at epoch 43: 0.7186497747898102
[08/28/2025 01:05:41 INFO]: Training loss at epoch 20: 0.8588019609451294
[08/28/2025 01:06:24 INFO]: Training loss at epoch 18: 0.8520476222038269
[08/28/2025 01:08:06 INFO]: Training loss at epoch 14: 0.9205217063426971
[08/28/2025 01:08:06 INFO]: Training loss at epoch 34: 1.1634611189365387
[08/28/2025 01:08:23 INFO]: Training loss at epoch 40: 1.0432283878326416
[08/28/2025 01:08:38 INFO]: Training loss at epoch 12: 1.0588720440864563
[08/28/2025 01:08:51 INFO]: Training loss at epoch 7: 1.2759574055671692
[08/28/2025 01:09:30 INFO]: Training loss at epoch 16: 0.9925504624843597
[08/28/2025 01:10:03 INFO]: Training loss at epoch 19: 0.8826643526554108
[08/28/2025 01:10:15 INFO]: Training loss at epoch 44: 0.8648156225681305
[08/28/2025 01:10:34 INFO]: Training loss at epoch 16: 1.0817304253578186
[08/28/2025 01:11:42 INFO]: Training loss at epoch 24: 0.9830839037895203
[08/28/2025 01:11:46 INFO]: Training loss at epoch 1: 1.2829134166240692
[08/28/2025 01:12:42 INFO]: Training loss at epoch 24: 1.0263091921806335
[08/28/2025 01:12:51 INFO]: Training stats: {
    "score": -0.948873549992058,
    "rmse": 0.948873549992058
}
[08/28/2025 01:12:51 INFO]: Val stats: {
    "score": -0.9939696674216687,
    "rmse": 0.9939696674216687
}
[08/28/2025 01:12:51 INFO]: Test stats: {
    "score": -0.9913747130527406,
    "rmse": 0.9913747130527406
}
[08/28/2025 01:12:53 INFO]: New best epoch, val score: -0.880656577482466
[08/28/2025 01:12:53 INFO]: Saving model to: maddest-Elbert_trial_84/model_best.pth
[08/28/2025 01:12:59 INFO]: New best epoch, val score: -0.9020367525667009
[08/28/2025 01:12:59 INFO]: Saving model to: maddest-Elbert_trial_104/model_best.pth
[08/28/2025 01:13:01 INFO]: Training loss at epoch 19: 1.0754241645336151
[08/28/2025 01:13:14 INFO]: Training loss at epoch 15: 1.2930343747138977
[08/28/2025 01:13:47 INFO]: Training loss at epoch 6: 0.9298536479473114
[08/28/2025 01:14:22 INFO]: Training loss at epoch 14: 0.9634518027305603
[08/28/2025 01:14:37 INFO]: Training loss at epoch 8: 0.8827157318592072
[08/28/2025 01:14:57 INFO]: Training loss at epoch 41: 0.9899629950523376
[08/28/2025 01:15:16 INFO]: Training loss at epoch 45: 0.7310933470726013
[08/28/2025 01:15:33 INFO]: Training loss at epoch 19: 1.196421593427658
[08/28/2025 01:15:48 INFO]: Training loss at epoch 12: 1.1360714435577393
[08/28/2025 01:15:51 INFO]: Training loss at epoch 35: 1.055891752243042
[08/28/2025 01:15:59 INFO]: New best epoch, val score: -0.888003132861385
[08/28/2025 01:15:59 INFO]: Saving model to: maddest-Elbert_trial_101/model_best.pth
[08/28/2025 01:16:09 INFO]: Training stats: {
    "score": -0.9999465265827591,
    "rmse": 0.9999465265827591
}
[08/28/2025 01:16:09 INFO]: Val stats: {
    "score": -0.9086680696585603,
    "rmse": 0.9086680696585603
}
[08/28/2025 01:16:09 INFO]: Test stats: {
    "score": -0.973047990668187,
    "rmse": 0.973047990668187
}
[08/28/2025 01:16:22 INFO]: Training loss at epoch 21: 0.997318297624588
[08/28/2025 01:17:07 INFO]: Training loss at epoch 17: 1.0029133558273315
[08/28/2025 01:18:17 INFO]: Training loss at epoch 17: 0.8845619261264801
[08/28/2025 01:18:42 INFO]: Training stats: {
    "score": -0.9994853131356506,
    "rmse": 0.9994853131356506
}
[08/28/2025 01:18:42 INFO]: Val stats: {
    "score": -0.9441701584204936,
    "rmse": 0.9441701584204936
}
[08/28/2025 01:18:42 INFO]: Test stats: {
    "score": -0.9738728562895188,
    "rmse": 0.9738728562895188
}
[08/28/2025 01:18:48 INFO]: Training loss at epoch 15: 0.9409383535385132
[08/28/2025 01:19:19 INFO]: Training loss at epoch 13: 0.8623984158039093
[08/28/2025 01:20:18 INFO]: Training loss at epoch 46: 0.8462934494018555
[08/28/2025 01:20:23 INFO]: Training loss at epoch 8: 1.1611339449882507
[08/28/2025 01:21:16 INFO]: Training loss at epoch 20: 0.752960205078125
[08/28/2025 01:21:22 INFO]: Training loss at epoch 16: 1.1975346207618713
[08/28/2025 01:21:34 INFO]: Training loss at epoch 42: 0.8824001550674438
[08/28/2025 01:21:38 INFO]: Training loss at epoch 25: 0.9614341557025909
[08/28/2025 01:22:06 INFO]: Training loss at epoch 2: 1.2948757410049438
[08/28/2025 01:22:13 INFO]: Training loss at epoch 25: 1.1493175625801086
[08/28/2025 01:23:38 INFO]: Training loss at epoch 36: 0.9827403426170349
[08/28/2025 01:24:45 INFO]: Training loss at epoch 18: 1.064465582370758
[08/28/2025 01:25:02 INFO]: Training loss at epoch 15: 1.026437133550644
[08/28/2025 01:25:18 INFO]: Training loss at epoch 47: 0.6903190016746521
[08/28/2025 01:25:20 INFO]: Training loss at epoch 20: 0.915492832660675
[08/28/2025 01:25:59 INFO]: Training loss at epoch 18: 0.9413273930549622
[08/28/2025 01:26:10 INFO]: Training loss at epoch 7: 1.0831453800201416
[08/28/2025 01:26:20 INFO]: Training loss at epoch 9: 0.8841443359851837
[08/28/2025 01:26:29 INFO]: Training loss at epoch 13: 1.4145394563674927
[08/28/2025 01:27:04 INFO]: Training loss at epoch 22: 0.7480660974979401
[08/28/2025 01:27:53 INFO]: Training loss at epoch 20: 1.028239369392395
[08/28/2025 01:28:10 INFO]: Training loss at epoch 43: 0.8963436484336853
[08/28/2025 01:29:27 INFO]: Training loss at epoch 17: 0.9086294770240784
[08/28/2025 01:29:29 INFO]: Training loss at epoch 16: 0.9483314752578735
[08/28/2025 01:29:37 INFO]: Training loss at epoch 21: 0.9381751418113708
[08/28/2025 01:29:58 INFO]: Training loss at epoch 14: 1.070544958114624
[08/28/2025 01:30:14 INFO]: Training stats: {
    "score": -1.0146183659760817,
    "rmse": 1.0146183659760817
}
[08/28/2025 01:30:14 INFO]: Val stats: {
    "score": -0.8889411740214228,
    "rmse": 0.8889411740214228
}
[08/28/2025 01:30:14 INFO]: Test stats: {
    "score": -1.0037961430677758,
    "rmse": 1.0037961430677758
}
[08/28/2025 01:30:19 INFO]: Training loss at epoch 48: 0.6894345581531525
[08/28/2025 01:31:24 INFO]: Training loss at epoch 37: 0.9325969517230988
[08/28/2025 01:31:29 INFO]: Training loss at epoch 26: 0.9833937883377075
[08/28/2025 01:31:41 INFO]: Training loss at epoch 26: 0.8449923098087311
[08/28/2025 01:31:53 INFO]: Training loss at epoch 9: 1.2416912317276
[08/28/2025 01:32:23 INFO]: Training loss at epoch 3: 1.0212979316711426
[08/28/2025 01:32:26 INFO]: Training loss at epoch 19: 1.0191659033298492
[08/28/2025 01:33:37 INFO]: New best epoch, val score: -0.8892402284551469
[08/28/2025 01:33:37 INFO]: Saving model to: maddest-Elbert_trial_104/model_best.pth
[08/28/2025 01:33:45 INFO]: Training loss at epoch 19: 1.0484123826026917
[08/28/2025 01:34:31 INFO]: Training loss at epoch 21: 0.788423091173172
[08/28/2025 01:34:47 INFO]: Training loss at epoch 44: 1.0559404790401459
[08/28/2025 01:35:01 INFO]: Training stats: {
    "score": -1.0633810893489668,
    "rmse": 1.0633810893489668
}
[08/28/2025 01:35:01 INFO]: Val stats: {
    "score": -1.0878411587051948,
    "rmse": 1.0878411587051948
}
[08/28/2025 01:35:01 INFO]: Test stats: {
    "score": -1.039841005196984,
    "rmse": 1.039841005196984
}
[08/28/2025 01:35:21 INFO]: Training loss at epoch 49: 0.8207184672355652
[08/28/2025 01:35:43 INFO]: Training loss at epoch 16: 0.7883647680282593
[08/28/2025 01:35:46 INFO]: Training stats: {
    "score": -0.978483700316573,
    "rmse": 0.978483700316573
}
[08/28/2025 01:35:46 INFO]: Val stats: {
    "score": -0.9485542926059333,
    "rmse": 0.9485542926059333
}
[08/28/2025 01:35:46 INFO]: Test stats: {
    "score": -0.971999675136952,
    "rmse": 0.971999675136952
}
[08/28/2025 01:36:21 INFO]: Training stats: {
    "score": -0.9971312346960897,
    "rmse": 0.9971312346960897
}
[08/28/2025 01:36:21 INFO]: Val stats: {
    "score": -0.9038455782327572,
    "rmse": 0.9038455782327572
}
[08/28/2025 01:36:21 INFO]: Test stats: {
    "score": -0.9731870829492489,
    "rmse": 0.9731870829492489
}
[08/28/2025 01:37:03 INFO]: Training stats: {
    "score": -0.8361353514670187,
    "rmse": 0.8361353514670187
}
[08/28/2025 01:37:03 INFO]: Val stats: {
    "score": -1.0074091545343065,
    "rmse": 1.0074091545343065
}
[08/28/2025 01:37:03 INFO]: Test stats: {
    "score": -1.071977050037201,
    "rmse": 1.071977050037201
}
[08/28/2025 01:37:05 INFO]: Training loss at epoch 21: 0.9986354410648346
[08/28/2025 01:37:09 INFO]: Training loss at epoch 14: 1.2616034746170044
[08/28/2025 01:37:35 INFO]: Training loss at epoch 18: 1.0420358180999756
[08/28/2025 01:37:47 INFO]: Training loss at epoch 23: 0.9113262593746185
[08/28/2025 01:38:02 INFO]: Training loss at epoch 22: 0.8558247685432434
[08/28/2025 01:38:26 INFO]: New best epoch, val score: -0.890025282385561
[08/28/2025 01:38:26 INFO]: Saving model to: maddest-Elbert_trial_95/model_best.pth
[08/28/2025 01:38:32 INFO]: Training loss at epoch 8: 1.220367431640625
[08/28/2025 01:39:11 INFO]: Training loss at epoch 38: 0.8754294216632843
[08/28/2025 01:40:14 INFO]: Training loss at epoch 17: 0.8257478773593903
[08/28/2025 01:40:42 INFO]: Training loss at epoch 15: 1.071266382932663
[08/28/2025 01:41:09 INFO]: Training loss at epoch 27: 1.0401883721351624
[08/28/2025 01:41:23 INFO]: Training loss at epoch 27: 0.9485397934913635
[08/28/2025 01:41:24 INFO]: Training loss at epoch 45: 1.014965981245041
[08/28/2025 01:41:48 INFO]: Training loss at epoch 10: 0.9406554698944092
[08/28/2025 01:42:06 INFO]: Training loss at epoch 50: 0.6378633975982666
[08/28/2025 01:42:42 INFO]: Training loss at epoch 20: 1.0714168548583984
[08/28/2025 01:42:43 INFO]: Training loss at epoch 4: 1.0458934307098389
[08/28/2025 01:43:46 INFO]: Training loss at epoch 22: 1.0648112297058105
[08/28/2025 01:44:06 INFO]: Training loss at epoch 20: 1.211937040090561
[08/28/2025 01:45:42 INFO]: Training loss at epoch 19: 1.1928110122680664
[08/28/2025 01:46:17 INFO]: Training loss at epoch 22: 1.1715014278888702
[08/28/2025 01:46:25 INFO]: Training loss at epoch 17: 0.930013120174408
[08/28/2025 01:46:26 INFO]: Training loss at epoch 23: 0.8767592906951904
[08/28/2025 01:46:58 INFO]: Training loss at epoch 39: 0.9478743970394135
[08/28/2025 01:47:06 INFO]: Training loss at epoch 51: 0.6365686357021332
[08/28/2025 01:47:18 INFO]: Training loss at epoch 10: 0.9707211256027222
[08/28/2025 01:47:53 INFO]: Training loss at epoch 15: 0.8157286643981934
[08/28/2025 01:48:00 INFO]: Training loss at epoch 46: 1.0372695326805115
[08/28/2025 01:48:26 INFO]: Training stats: {
    "score": -1.0169188976012462,
    "rmse": 1.0169188976012462
}
[08/28/2025 01:48:26 INFO]: Val stats: {
    "score": -0.9959944269594664,
    "rmse": 0.9959944269594664
}
[08/28/2025 01:48:26 INFO]: Test stats: {
    "score": -0.9915967722784049,
    "rmse": 0.9915967722784049
}
[08/28/2025 01:48:31 INFO]: Training loss at epoch 24: 1.1397749781608582
[08/28/2025 01:48:39 INFO]: New best epoch, val score: -0.8931544371908344
[08/28/2025 01:48:39 INFO]: Saving model to: maddest-Elbert_trial_102/model_best.pth
[08/28/2025 01:49:34 INFO]: Training stats: {
    "score": -0.9987923865548842,
    "rmse": 0.9987923865548842
}
[08/28/2025 01:49:34 INFO]: Val stats: {
    "score": -0.8979674273417527,
    "rmse": 0.8979674273417527
}
[08/28/2025 01:49:34 INFO]: Test stats: {
    "score": -0.9746573358450256,
    "rmse": 0.9746573358450256
}
[08/28/2025 01:50:20 INFO]: Training loss at epoch 21: 1.0446454584598541
[08/28/2025 01:50:35 INFO]: Training loss at epoch 28: 1.0909367203712463
[08/28/2025 01:50:54 INFO]: Training loss at epoch 9: 1.0805511474609375
[08/28/2025 01:50:55 INFO]: Training loss at epoch 18: 1.0514829754829407
[08/28/2025 01:51:17 INFO]: Training loss at epoch 28: 1.1475608944892883
[08/28/2025 01:51:22 INFO]: Training loss at epoch 16: 1.0896834433078766
[08/28/2025 01:51:47 INFO]: Training loss at epoch 21: 0.9885781705379486
[08/28/2025 01:52:08 INFO]: Training loss at epoch 52: 0.5992117673158646
[08/28/2025 01:52:58 INFO]: Training loss at epoch 23: 1.25078284740448
[08/28/2025 01:53:03 INFO]: Training loss at epoch 5: 1.3581323027610779
[08/28/2025 01:53:22 INFO]: Training loss at epoch 11: 1.0737180709838867
[08/28/2025 01:54:03 INFO]: New best epoch, val score: -0.8950849614161828
[08/28/2025 01:54:03 INFO]: Saving model to: maddest-Elbert_trial_89/model_best.pth
[08/28/2025 01:54:37 INFO]: Training loss at epoch 47: 0.8740517199039459
[08/28/2025 01:54:48 INFO]: Training loss at epoch 24: 0.7339235246181488
[08/28/2025 01:55:05 INFO]: Training stats: {
    "score": -0.9981932927189688,
    "rmse": 0.9981932927189688
}
[08/28/2025 01:55:05 INFO]: Val stats: {
    "score": -1.0235332760496612,
    "rmse": 1.0235332760496612
}
[08/28/2025 01:55:05 INFO]: Test stats: {
    "score": -1.0236891540638406,
    "rmse": 1.0236891540638406
}
[08/28/2025 01:55:28 INFO]: Training loss at epoch 23: 0.9538334310054779
[08/28/2025 01:56:33 INFO]: Training loss at epoch 20: 0.944182425737381
[08/28/2025 01:57:06 INFO]: Training loss at epoch 18: 1.0342130959033966
[08/28/2025 01:57:10 INFO]: Training loss at epoch 53: 0.6948451697826385
[08/28/2025 01:57:23 INFO]: Training loss at epoch 40: 1.0078380107879639
[08/28/2025 01:57:59 INFO]: Training loss at epoch 22: 0.8330203294754028
[08/28/2025 01:58:32 INFO]: Training loss at epoch 16: 1.1542154252529144
[08/28/2025 01:58:50 INFO]: Training loss at epoch 11: 1.1638357937335968
[08/28/2025 01:59:15 INFO]: Training loss at epoch 25: 1.1778047680854797
[08/28/2025 01:59:29 INFO]: Training loss at epoch 22: 0.8495482504367828
[08/28/2025 02:00:05 INFO]: Training loss at epoch 29: 0.7433569878339767
[08/28/2025 02:00:12 INFO]: New best epoch, val score: -0.8818873036808373
[08/28/2025 02:00:12 INFO]: Saving model to: maddest-Elbert_trial_102/model_best.pth
[08/28/2025 02:01:09 INFO]: Training loss at epoch 29: 1.106224000453949
[08/28/2025 02:01:13 INFO]: Training loss at epoch 48: 0.8596103191375732
[08/28/2025 02:01:38 INFO]: Training loss at epoch 19: 1.0172403454780579
[08/28/2025 02:02:03 INFO]: Training loss at epoch 17: 0.7994373738765717
[08/28/2025 02:02:10 INFO]: Training loss at epoch 54: 0.6092945039272308
[08/28/2025 02:02:11 INFO]: Training loss at epoch 24: 1.0364408493041992
[08/28/2025 02:03:14 INFO]: Training loss at epoch 25: 0.9305255711078644
[08/28/2025 02:03:17 INFO]: New best epoch, val score: -0.8919558080294512
[08/28/2025 02:03:17 INFO]: Saving model to: maddest-Elbert_trial_89/model_best.pth
[08/28/2025 02:03:20 INFO]: Training stats: {
    "score": -0.9900928088932965,
    "rmse": 0.9900928088932965
}
[08/28/2025 02:03:20 INFO]: Val stats: {
    "score": -0.9215402623845977,
    "rmse": 0.9215402623845977
}
[08/28/2025 02:03:20 INFO]: Test stats: {
    "score": -0.9687800045673254,
    "rmse": 0.9687800045673254
}
[08/28/2025 02:03:23 INFO]: Training loss at epoch 6: 0.9941520988941193
[08/28/2025 02:04:32 INFO]: Training stats: {
    "score": -1.0174597106103471,
    "rmse": 1.0174597106103471
}
[08/28/2025 02:04:32 INFO]: Val stats: {
    "score": -0.9977723623821058,
    "rmse": 0.9977723623821058
}
[08/28/2025 02:04:32 INFO]: Test stats: {
    "score": -0.9910798841088252,
    "rmse": 0.9910798841088252
}
[08/28/2025 02:04:38 INFO]: Training loss at epoch 24: 0.9052303731441498
[08/28/2025 02:04:39 INFO]: Training loss at epoch 21: 0.9079860746860504
[08/28/2025 02:04:55 INFO]: Training loss at epoch 12: 0.8732033967971802
[08/28/2025 02:05:07 INFO]: Training loss at epoch 41: 1.0461678504943848
[08/28/2025 02:05:17 INFO]: Training stats: {
    "score": -0.987826279513856,
    "rmse": 0.987826279513856
}
[08/28/2025 02:05:17 INFO]: Val stats: {
    "score": -0.9691189759864102,
    "rmse": 0.9691189759864102
}
[08/28/2025 02:05:17 INFO]: Test stats: {
    "score": -0.9896909398066421,
    "rmse": 0.9896909398066421
}
[08/28/2025 02:05:38 INFO]: Training loss at epoch 23: 0.9757384359836578
[08/28/2025 02:07:10 INFO]: Training loss at epoch 23: 1.055591344833374
[08/28/2025 02:07:10 INFO]: Training loss at epoch 55: 0.5363316237926483
[08/28/2025 02:07:27 INFO]: Training loss at epoch 10: 1.0159175395965576
[08/28/2025 02:07:46 INFO]: Training loss at epoch 19: 0.9968776106834412
[08/28/2025 02:07:50 INFO]: Training loss at epoch 49: 0.8002192676067352
[08/28/2025 02:08:54 INFO]: New best epoch, val score: -0.8754133653158028
[08/28/2025 02:08:54 INFO]: Saving model to: maddest-Elbert_trial_103/model_best.pth
[08/28/2025 02:09:14 INFO]: Training loss at epoch 17: 1.132426679134369
[08/28/2025 02:09:59 INFO]: Training loss at epoch 26: 0.8433946967124939
[08/28/2025 02:10:05 INFO]: Training stats: {
    "score": -0.9810678573678969,
    "rmse": 0.9810678573678969
}
[08/28/2025 02:10:05 INFO]: Val stats: {
    "score": -0.9229908642307818,
    "rmse": 0.9229908642307818
}
[08/28/2025 02:10:05 INFO]: Test stats: {
    "score": -0.9677532939494276,
    "rmse": 0.9677532939494276
}
[08/28/2025 02:10:22 INFO]: Training loss at epoch 12: 0.9280560612678528
[08/28/2025 02:11:22 INFO]: Training stats: {
    "score": -1.0032840988461773,
    "rmse": 1.0032840988461773
}
[08/28/2025 02:11:22 INFO]: Val stats: {
    "score": -1.0094501380483325,
    "rmse": 1.0094501380483325
}
[08/28/2025 02:11:22 INFO]: Test stats: {
    "score": -1.005012687912749,
    "rmse": 1.005012687912749
}
[08/28/2025 02:11:22 INFO]: Training loss at epoch 25: 1.0025435388088226
[08/28/2025 02:11:35 INFO]: Training loss at epoch 26: 0.9135600328445435
[08/28/2025 02:11:41 INFO]: New best epoch, val score: -0.8814027174080291
[08/28/2025 02:11:41 INFO]: Saving model to: maddest-Elbert_trial_102/model_best.pth
[08/28/2025 02:12:10 INFO]: Training loss at epoch 56: 0.6730155646800995
[08/28/2025 02:12:43 INFO]: Training loss at epoch 18: 0.8696687519550323
[08/28/2025 02:12:45 INFO]: Training loss at epoch 22: 1.021783173084259
[08/28/2025 02:12:46 INFO]: Training loss at epoch 30: 1.0226725041866302
[08/28/2025 02:12:52 INFO]: Training loss at epoch 42: 0.9842597842216492
[08/28/2025 02:13:17 INFO]: Training loss at epoch 24: 0.8783559501171112
[08/28/2025 02:13:40 INFO]: Training loss at epoch 7: 1.1846311688423157
[08/28/2025 02:13:50 INFO]: Training loss at epoch 25: 0.8945315480232239
[08/28/2025 02:14:24 INFO]: Training loss at epoch 30: 1.0658674240112305
[08/28/2025 02:14:52 INFO]: Training loss at epoch 24: 0.8410587906837463
[08/28/2025 02:16:00 INFO]: Training loss at epoch 20: 1.217312753200531
[08/28/2025 02:16:30 INFO]: Training loss at epoch 13: 1.0429863929748535
[08/28/2025 02:16:43 INFO]: Training loss at epoch 50: 0.819176971912384
[08/28/2025 02:17:13 INFO]: Training loss at epoch 57: 0.5857600569725037
[08/28/2025 02:19:47 INFO]: Training loss at epoch 11: 1.0530030131340027
[08/28/2025 02:19:50 INFO]: Training loss at epoch 18: 1.1943122744560242
[08/28/2025 02:19:56 INFO]: Training loss at epoch 27: 0.7950628399848938
[08/28/2025 02:20:33 INFO]: Training loss at epoch 26: 0.8702219724655151
[08/28/2025 02:20:39 INFO]: Training loss at epoch 27: 0.9313717186450958
[08/28/2025 02:20:42 INFO]: Training loss at epoch 43: 0.8863354325294495
[08/28/2025 02:20:50 INFO]: Training loss at epoch 23: 0.9397622048854828
[08/28/2025 02:20:57 INFO]: Training loss at epoch 25: 1.1638839542865753
[08/28/2025 02:21:53 INFO]: Training loss at epoch 13: 0.85139200091362
[08/28/2025 02:22:02 INFO]: Training loss at epoch 20: 0.963128536939621
[08/28/2025 02:22:13 INFO]: Training loss at epoch 58: 0.5928303301334381
[08/28/2025 02:22:18 INFO]: Training loss at epoch 31: 1.1635154783725739
[08/28/2025 02:22:35 INFO]: Training loss at epoch 25: 0.9036238491535187
[08/28/2025 02:22:52 INFO]: Running Final Evaluation...
[08/28/2025 02:23:01 INFO]: Training loss at epoch 26: 1.1031494140625
[08/28/2025 02:23:15 INFO]: Training loss at epoch 51: 0.9748305976390839
[08/28/2025 02:23:23 INFO]: Training loss at epoch 19: 1.1383361518383026
[08/28/2025 02:23:55 INFO]: Training loss at epoch 8: 1.1685065627098083
[08/28/2025 02:24:14 INFO]: Training loss at epoch 31: 1.0302976965904236
[08/28/2025 02:24:45 INFO]: Training accuracy: {
    "score": -0.9919941856905701,
    "rmse": 0.9919941856905701
}
[08/28/2025 02:24:45 INFO]: Val accuracy: {
    "score": -0.8811043821905892,
    "rmse": 0.8811043821905892
}
[08/28/2025 02:24:45 INFO]: Test accuracy: {
    "score": -1.0042751921681345,
    "rmse": 1.0042751921681345
}
[08/28/2025 02:24:45 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_87",
    "best_epoch": 27,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0042751921681345,
        "rmse": 1.0042751921681345
    },
    "train_stats": {
        "score": -0.9919941856905701,
        "rmse": 0.9919941856905701
    },
    "val_stats": {
        "score": -0.8811043821905892,
        "rmse": 0.8811043821905892
    }
}
[08/28/2025 02:24:45 INFO]: Procewss finished for trial maddest-Elbert_trial_87
[08/28/2025 02:24:45 INFO]: 
_________________________________________________

[08/28/2025 02:24:45 INFO]: train_net_for_optune.py main() running.
[08/28/2025 02:24:45 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 0.9159994235031731
  attention_dropout: 0.029513717420327615
  ffn_dropout: 0.029513717420327615
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.983843119584312e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_105

[08/28/2025 02:24:45 INFO]: This ft_transformer has 10.805 million parameters.
[08/28/2025 02:24:45 INFO]: Training will start at epoch 0.
[08/28/2025 02:24:45 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 02:26:43 INFO]: Training loss at epoch 21: 1.005739986896515
[08/28/2025 02:26:59 INFO]: Training stats: {
    "score": -0.991034665816244,
    "rmse": 0.991034665816244
}
[08/28/2025 02:26:59 INFO]: Val stats: {
    "score": -0.9078329741498843,
    "rmse": 0.9078329741498843
}
[08/28/2025 02:26:59 INFO]: Test stats: {
    "score": -0.970498909796746,
    "rmse": 0.970498909796746
}
[08/28/2025 02:27:58 INFO]: Training loss at epoch 14: 1.1949337720870972
[08/28/2025 02:28:17 INFO]: Training loss at epoch 28: 1.027094841003418
[08/28/2025 02:28:28 INFO]: Training loss at epoch 44: 0.865884393453598
[08/28/2025 02:28:35 INFO]: Training loss at epoch 26: 0.9665019810199738
[08/28/2025 02:28:53 INFO]: Training loss at epoch 24: 0.96372851729393
[08/28/2025 02:29:18 INFO]: Training loss at epoch 0: 1.1374508142471313
[08/28/2025 02:29:23 INFO]: Running Final Evaluation...
[08/28/2025 02:29:43 INFO]: Training loss at epoch 27: 0.998219907283783
[08/28/2025 02:29:50 INFO]: Training loss at epoch 52: 1.0723722875118256
[08/28/2025 02:29:53 INFO]: New best epoch, val score: -0.9495488287043533
[08/28/2025 02:29:53 INFO]: Saving model to: maddest-Elbert_trial_105/model_best.pth
[08/28/2025 02:30:21 INFO]: Training loss at epoch 26: 0.9931195378303528
[08/28/2025 02:30:30 INFO]: Training loss at epoch 19: 1.1243232488632202
[08/28/2025 02:31:19 INFO]: Training loss at epoch 28: 0.711402490735054
[08/28/2025 02:31:49 INFO]: Training loss at epoch 32: 0.9637086689472198
[08/28/2025 02:32:08 INFO]: Training accuracy: {
    "score": -1.049052567422471,
    "rmse": 1.049052567422471
}
[08/28/2025 02:32:08 INFO]: Val accuracy: {
    "score": -0.8820103897057348,
    "rmse": 0.8820103897057348
}
[08/28/2025 02:32:08 INFO]: Test accuracy: {
    "score": -1.022314038358338,
    "rmse": 1.022314038358338
}
[08/28/2025 02:32:08 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_80",
    "best_epoch": 13,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.022314038358338,
        "rmse": 1.022314038358338
    },
    "train_stats": {
        "score": -1.049052567422471,
        "rmse": 1.049052567422471
    },
    "val_stats": {
        "score": -0.8820103897057348,
        "rmse": 0.8820103897057348
    }
}
[08/28/2025 02:32:08 INFO]: Procewss finished for trial maddest-Elbert_trial_80
[08/28/2025 02:32:09 INFO]: 
_________________________________________________

[08/28/2025 02:32:09 INFO]: train_net_for_optune.py main() running.
[08/28/2025 02:32:09 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 0.9205400896109197
  attention_dropout: 0.029150951792730428
  ffn_dropout: 0.029150951792730428
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.0043184715273246e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_106

[08/28/2025 02:32:09 INFO]: This ft_transformer has 10.832 million parameters.
[08/28/2025 02:32:09 INFO]: Training will start at epoch 0.
[08/28/2025 02:32:09 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 02:32:11 INFO]: Training loss at epoch 12: 1.2275012731552124
[08/28/2025 02:32:15 INFO]: Training loss at epoch 27: 0.9889282882213593
[08/28/2025 02:32:47 INFO]: Training loss at epoch 21: 1.216903030872345
[08/28/2025 02:33:31 INFO]: Training loss at epoch 14: 0.8797815442085266
[08/28/2025 02:34:03 INFO]: Training loss at epoch 32: 1.0470620095729828
[08/28/2025 02:34:05 INFO]: Training stats: {
    "score": -1.009634699401154,
    "rmse": 1.009634699401154
}
[08/28/2025 02:34:05 INFO]: Val stats: {
    "score": -1.0293762684865972,
    "rmse": 1.0293762684865972
}
[08/28/2025 02:34:05 INFO]: Test stats: {
    "score": -1.002656614139958,
    "rmse": 1.002656614139958
}
[08/28/2025 02:34:10 INFO]: Training loss at epoch 9: 1.2087604999542236
[08/28/2025 02:34:28 INFO]: Training loss at epoch 1: 1.0755402445793152
[08/28/2025 02:36:10 INFO]: Training loss at epoch 27: 1.0771133303642273
[08/28/2025 02:36:26 INFO]: Training loss at epoch 53: 0.952450305223465
[08/28/2025 02:36:36 INFO]: Training loss at epoch 29: 0.7983554899692535
[08/28/2025 02:36:40 INFO]: Training loss at epoch 0: 0.9836861193180084
[08/28/2025 02:36:56 INFO]: Training loss at epoch 25: 1.128197968006134
[08/28/2025 02:37:18 INFO]: New best epoch, val score: -0.9139575739775503
[08/28/2025 02:37:18 INFO]: Saving model to: maddest-Elbert_trial_106/model_best.pth
[08/28/2025 02:37:22 INFO]: Training loss at epoch 22: 0.9413754343986511
[08/28/2025 02:37:37 INFO]: Training loss at epoch 20: 1.0625252425670624
[08/28/2025 02:37:38 INFO]: Training stats: {
    "score": -1.0616452040992659,
    "rmse": 1.0616452040992659
}
[08/28/2025 02:37:38 INFO]: Val stats: {
    "score": -1.1026517852147206,
    "rmse": 1.1026517852147206
}
[08/28/2025 02:37:38 INFO]: Test stats: {
    "score": -1.063347109692219,
    "rmse": 1.063347109692219
}
[08/28/2025 02:38:01 INFO]: Training loss at epoch 27: 1.112519085407257
[08/28/2025 02:38:55 INFO]: Training loss at epoch 28: 0.8095288872718811
[08/28/2025 02:39:27 INFO]: Training stats: {
    "score": -0.890895770874095,
    "rmse": 0.890895770874095
}
[08/28/2025 02:39:27 INFO]: Val stats: {
    "score": -0.9716302380816093,
    "rmse": 0.9716302380816093
}
[08/28/2025 02:39:27 INFO]: Test stats: {
    "score": -1.0025828020707133,
    "rmse": 1.0025828020707133
}
[08/28/2025 02:39:28 INFO]: Training loss at epoch 15: 0.9064059257507324
[08/28/2025 02:39:39 INFO]: Training loss at epoch 2: 0.8635427057743073
[08/28/2025 02:41:14 INFO]: Training loss at epoch 33: 0.9388025403022766
[08/28/2025 02:41:23 INFO]: Training loss at epoch 28: 0.8954181969165802
[08/28/2025 02:41:51 INFO]: Training loss at epoch 1: 1.1842941045761108
[08/28/2025 02:42:02 INFO]: Training loss at epoch 29: 0.9250841438770294
[08/28/2025 02:43:02 INFO]: Training loss at epoch 54: 0.9717256128787994
[08/28/2025 02:43:30 INFO]: Training loss at epoch 22: 0.9529935717582703
[08/28/2025 02:43:49 INFO]: Training loss at epoch 28: 0.9808607995510101
[08/28/2025 02:43:55 INFO]: Training loss at epoch 33: 0.9771842658519745
[08/28/2025 02:44:31 INFO]: Training loss at epoch 13: 1.1594921946525574
[08/28/2025 02:44:44 INFO]: Training loss at epoch 20: 1.1572150886058807
[08/28/2025 02:44:47 INFO]: Training loss at epoch 3: 0.8843599855899811
[08/28/2025 02:45:02 INFO]: Training loss at epoch 15: 0.9421782493591309
[08/28/2025 02:45:02 INFO]: Training loss at epoch 26: 1.3446078598499298
[08/28/2025 02:45:24 INFO]: New best epoch, val score: -0.8983953450954021
[08/28/2025 02:45:24 INFO]: Saving model to: maddest-Elbert_trial_105/model_best.pth
[08/28/2025 02:45:40 INFO]: Training stats: {
    "score": -0.9224285402722345,
    "rmse": 0.9224285402722345
}
[08/28/2025 02:45:40 INFO]: Val stats: {
    "score": -0.9288108365723929,
    "rmse": 0.9288108365723929
}
[08/28/2025 02:45:40 INFO]: Test stats: {
    "score": -1.0020600448252455,
    "rmse": 1.0020600448252455
}
[08/28/2025 02:45:45 INFO]: Training loss at epoch 28: 1.046835720539093
[08/28/2025 02:47:03 INFO]: Training loss at epoch 2: 1.002281814813614
[08/28/2025 02:47:49 INFO]: Training loss at epoch 30: 0.8381765186786652
[08/28/2025 02:47:54 INFO]: Training loss at epoch 10: 0.8979892730712891
[08/28/2025 02:48:05 INFO]: Training loss at epoch 29: 1.093365103006363
[08/28/2025 02:48:06 INFO]: Training loss at epoch 23: 0.8499649167060852
[08/28/2025 02:48:15 INFO]: Training loss at epoch 21: 0.9585565030574799
[08/28/2025 02:49:07 INFO]: New best epoch, val score: -0.8858745908436941
[08/28/2025 02:49:07 INFO]: Saving model to: maddest-Elbert_trial_104/model_best.pth
[08/28/2025 02:49:40 INFO]: Training loss at epoch 55: 0.9688348770141602
[08/28/2025 02:49:59 INFO]: Training loss at epoch 4: 0.9499272406101227
[08/28/2025 02:50:36 INFO]: Training loss at epoch 29: 0.9031893014907837
[08/28/2025 02:50:44 INFO]: Training loss at epoch 34: 1.1846784055233002
[08/28/2025 02:51:03 INFO]: Training loss at epoch 16: 0.9909866750240326
[08/28/2025 02:51:17 INFO]: Training stats: {
    "score": -0.9978738382397001,
    "rmse": 0.9978738382397001
}
[08/28/2025 02:51:17 INFO]: Val stats: {
    "score": -0.9337436622306036,
    "rmse": 0.9337436622306036
}
[08/28/2025 02:51:17 INFO]: Test stats: {
    "score": -0.9718653105321419,
    "rmse": 0.9718653105321419
}
[08/28/2025 02:51:30 INFO]: Training loss at epoch 29: 1.0501235127449036
[08/28/2025 02:52:16 INFO]: Training loss at epoch 3: 1.1969037055969238
[08/28/2025 02:52:52 INFO]: New best epoch, val score: -0.8869088051796449
[08/28/2025 02:52:52 INFO]: Saving model to: maddest-Elbert_trial_106/model_best.pth
[08/28/2025 02:53:11 INFO]: Training loss at epoch 27: 1.1352494955062866
[08/28/2025 02:53:27 INFO]: Training loss at epoch 29: 1.0226725041866302
[08/28/2025 02:53:42 INFO]: Training stats: {
    "score": -1.0010371930313156,
    "rmse": 1.0010371930313156
}
[08/28/2025 02:53:42 INFO]: Val stats: {
    "score": -0.9520311711412589,
    "rmse": 0.9520311711412589
}
[08/28/2025 02:53:42 INFO]: Test stats: {
    "score": -0.9761972237830092,
    "rmse": 0.9761972237830092
}
[08/28/2025 02:53:50 INFO]: Training loss at epoch 34: 1.0121379792690277
[08/28/2025 02:54:04 INFO]: Training stats: {
    "score": -0.9974458294288717,
    "rmse": 0.9974458294288717
}
[08/28/2025 02:54:04 INFO]: Val stats: {
    "score": -0.9113571159760945,
    "rmse": 0.9113571159760945
}
[08/28/2025 02:54:04 INFO]: Test stats: {
    "score": -0.9721774449788352,
    "rmse": 0.9721774449788352
}
[08/28/2025 02:54:08 INFO]: Training loss at epoch 23: 0.9627507328987122
[08/28/2025 02:55:08 INFO]: Training loss at epoch 5: 1.0121937096118927
[08/28/2025 02:55:24 INFO]: Training loss at epoch 21: 0.9731128513813019
[08/28/2025 02:56:03 INFO]: Training stats: {
    "score": -1.0013419671376758,
    "rmse": 1.0013419671376758
}
[08/28/2025 02:56:03 INFO]: Val stats: {
    "score": -0.9761521457697974,
    "rmse": 0.9761521457697974
}
[08/28/2025 02:56:03 INFO]: Test stats: {
    "score": -0.9811292091064397,
    "rmse": 0.9811292091064397
}
[08/28/2025 02:56:12 INFO]: Training loss at epoch 31: 0.8932503163814545
[08/28/2025 02:56:16 INFO]: Training loss at epoch 56: 0.9453577697277069
[08/28/2025 02:56:21 INFO]: Training loss at epoch 30: 0.8557567894458771
[08/28/2025 02:56:35 INFO]: Training loss at epoch 16: 0.9794944524765015
[08/28/2025 02:56:54 INFO]: Training loss at epoch 14: 1.0204631686210632
[08/28/2025 02:57:12 INFO]: Running Final Evaluation...
[08/28/2025 02:57:25 INFO]: Training loss at epoch 4: 0.9304729700088501
[08/28/2025 02:58:14 INFO]: Training loss at epoch 11: 1.1014609038829803
[08/28/2025 02:58:51 INFO]: Training loss at epoch 24: 1.1476326286792755
[08/28/2025 02:58:59 INFO]: Training loss at epoch 22: 1.1247576773166656
[08/28/2025 03:00:12 INFO]: Training accuracy: {
    "score": -1.0092541474596084,
    "rmse": 1.0092541474596084
}
[08/28/2025 03:00:12 INFO]: Val accuracy: {
    "score": -0.8887649675842283,
    "rmse": 0.8887649675842283
}
[08/28/2025 03:00:12 INFO]: Test accuracy: {
    "score": -0.99232863206339,
    "rmse": 0.99232863206339
}
[08/28/2025 03:00:12 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_91",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.99232863206339,
        "rmse": 0.99232863206339
    },
    "train_stats": {
        "score": -1.0092541474596084,
        "rmse": 1.0092541474596084
    },
    "val_stats": {
        "score": -0.8887649675842283,
        "rmse": 0.8887649675842283
    }
}
[08/28/2025 03:00:12 INFO]: Procewss finished for trial maddest-Elbert_trial_91
[08/28/2025 03:00:12 INFO]: 
_________________________________________________

[08/28/2025 03:00:12 INFO]: train_net_for_optune.py main() running.
[08/28/2025 03:00:12 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 1.9895602887703048
  attention_dropout: 0.02885848787368863
  ffn_dropout: 0.02885848787368863
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.0331042648215446e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_107

[08/28/2025 03:00:12 INFO]: This ft_transformer has 1.030 million parameters.
[08/28/2025 03:00:12 INFO]: Training will start at epoch 0.
[08/28/2025 03:00:12 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 03:00:13 INFO]: Training loss at epoch 35: 0.8452317416667938
[08/28/2025 03:00:20 INFO]: Training loss at epoch 6: 0.7986638247966766
[08/28/2025 03:00:29 INFO]: Training loss at epoch 30: 1.3222971260547638
[08/28/2025 03:01:03 INFO]: Training loss at epoch 0: 1.9383133053779602
[08/28/2025 03:01:09 INFO]: New best epoch, val score: -1.4445914813931473
[08/28/2025 03:01:09 INFO]: Saving model to: maddest-Elbert_trial_107/model_best.pth
[08/28/2025 03:01:18 INFO]: Training loss at epoch 28: 0.9814216494560242
[08/28/2025 03:01:45 INFO]: Training loss at epoch 30: 0.7899053692817688
[08/28/2025 03:02:00 INFO]: Training loss at epoch 1: 1.6351785063743591
[08/28/2025 03:02:08 INFO]: New best epoch, val score: -1.2254579454613308
[08/28/2025 03:02:08 INFO]: Saving model to: maddest-Elbert_trial_107/model_best.pth
[08/28/2025 03:02:38 INFO]: Training loss at epoch 5: 0.9277136325836182
[08/28/2025 03:02:38 INFO]: Training loss at epoch 17: 0.7385718375444412
[08/28/2025 03:02:54 INFO]: Training loss at epoch 57: 1.0050641894340515
[08/28/2025 03:02:55 INFO]: Training loss at epoch 30: 1.0227249264717102
[08/28/2025 03:03:01 INFO]: Training loss at epoch 2: 1.405800998210907
[08/28/2025 03:03:08 INFO]: New best epoch, val score: -0.9886463634163499
[08/28/2025 03:03:08 INFO]: Saving model to: maddest-Elbert_trial_107/model_best.pth
[08/28/2025 03:03:43 INFO]: Training loss at epoch 35: 0.9818094670772552
[08/28/2025 03:03:49 INFO]: Training loss at epoch 30: 1.0241933166980743
[08/28/2025 03:04:00 INFO]: Training loss at epoch 3: 1.1215453743934631
[08/28/2025 03:04:06 INFO]: New best epoch, val score: -0.8863316965253524
[08/28/2025 03:04:06 INFO]: Saving model to: maddest-Elbert_trial_107/model_best.pth
[08/28/2025 03:04:51 INFO]: Training loss at epoch 24: 1.1679101288318634
[08/28/2025 03:04:58 INFO]: Training loss at epoch 4: 1.1886830925941467
[08/28/2025 03:05:31 INFO]: Training loss at epoch 7: 0.7918299436569214
[08/28/2025 03:05:55 INFO]: Training loss at epoch 5: 1.5338216423988342
[08/28/2025 03:06:08 INFO]: Training loss at epoch 22: 1.0085700154304504
[08/28/2025 03:06:53 INFO]: Training loss at epoch 6: 1.1351211071014404
[08/28/2025 03:07:05 INFO]: Training loss at epoch 31: 0.8912472128868103
[08/28/2025 03:07:49 INFO]: Training loss at epoch 6: 1.0844357013702393
[08/28/2025 03:07:50 INFO]: Training loss at epoch 7: 1.2792500853538513
[08/28/2025 03:08:11 INFO]: Training loss at epoch 17: 0.8677496314048767
[08/28/2025 03:08:35 INFO]: Training loss at epoch 12: 1.5678837895393372
[08/28/2025 03:08:47 INFO]: Training loss at epoch 8: 1.4156189560890198
[08/28/2025 03:09:20 INFO]: Training loss at epoch 15: 0.8567674458026886
[08/28/2025 03:09:26 INFO]: Training loss at epoch 29: 1.1301485002040863
[08/28/2025 03:09:26 INFO]: Training loss at epoch 31: 1.1894207000732422
[08/28/2025 03:09:33 INFO]: Training loss at epoch 58: 1.043919563293457
[08/28/2025 03:09:38 INFO]: Training loss at epoch 25: 0.9679976105690002
[08/28/2025 03:09:42 INFO]: Training loss at epoch 23: 1.0133804380893707
[08/28/2025 03:09:43 INFO]: Training loss at epoch 31: 1.098225176334381
[08/28/2025 03:09:46 INFO]: Training loss at epoch 36: 0.7898618876934052
[08/28/2025 03:09:48 INFO]: Training loss at epoch 9: 0.987316220998764
[08/28/2025 03:10:08 INFO]: Training stats: {
    "score": -1.0808653646148956,
    "rmse": 1.0808653646148956
}
[08/28/2025 03:10:08 INFO]: Val stats: {
    "score": -0.8969703427876936,
    "rmse": 0.8969703427876936
}
[08/28/2025 03:10:08 INFO]: Test stats: {
    "score": -1.060453873120816,
    "rmse": 1.060453873120816
}
[08/28/2025 03:10:43 INFO]: Training loss at epoch 8: 0.9069459736347198
[08/28/2025 03:11:08 INFO]: Training loss at epoch 10: 1.406982660293579
[08/28/2025 03:11:33 INFO]: Training loss at epoch 31: 0.937253475189209
[08/28/2025 03:12:08 INFO]: Training loss at epoch 11: 0.9632423222064972
[08/28/2025 03:12:10 INFO]: Training loss at epoch 31: 1.1147043704986572
[08/28/2025 03:12:12 INFO]: Training stats: {
    "score": -1.0040600263236301,
    "rmse": 1.0040600263236301
}
[08/28/2025 03:12:12 INFO]: Val stats: {
    "score": -0.9620411317953749,
    "rmse": 0.9620411317953749
}
[08/28/2025 03:12:12 INFO]: Test stats: {
    "score": -0.9788398554016379,
    "rmse": 0.9788398554016379
}
[08/28/2025 03:12:14 INFO]: New best epoch, val score: -0.8849887410264591
[08/28/2025 03:12:14 INFO]: Saving model to: maddest-Elbert_trial_107/model_best.pth
[08/28/2025 03:13:03 INFO]: Training loss at epoch 7: 1.1767778396606445
[08/28/2025 03:13:06 INFO]: Training loss at epoch 12: 1.1148937940597534
[08/28/2025 03:13:40 INFO]: Training loss at epoch 36: 1.0244646668434143
[08/28/2025 03:14:04 INFO]: Training loss at epoch 13: 1.133017361164093
[08/28/2025 03:14:16 INFO]: Training loss at epoch 18: 0.9515411853790283
[08/28/2025 03:15:02 INFO]: Training loss at epoch 14: 0.7706504762172699
[08/28/2025 03:15:32 INFO]: Training loss at epoch 25: 1.049313336610794
[08/28/2025 03:15:53 INFO]: Training loss at epoch 9: 1.092713624238968
[08/28/2025 03:15:59 INFO]: Training loss at epoch 15: 1.1324535310268402
[08/28/2025 03:16:11 INFO]: Training loss at epoch 59: 1.392983078956604
[08/28/2025 03:16:51 INFO]: Training loss at epoch 23: 0.8087061047554016
[08/28/2025 03:16:56 INFO]: Training loss at epoch 16: 1.0933321714401245
[08/28/2025 03:17:07 INFO]: Training loss at epoch 32: 1.113903820514679
[08/28/2025 03:17:36 INFO]: Training stats: {
    "score": -0.9715628082570541,
    "rmse": 0.9715628082570541
}
[08/28/2025 03:17:36 INFO]: Val stats: {
    "score": -0.9266107598348386,
    "rmse": 0.9266107598348386
}
[08/28/2025 03:17:36 INFO]: Test stats: {
    "score": -0.9804334784423455,
    "rmse": 0.9804334784423455
}
[08/28/2025 03:17:47 INFO]: Training loss at epoch 32: 0.835669994354248
[08/28/2025 03:17:53 INFO]: Training loss at epoch 17: 1.0707599222660065
[08/28/2025 03:18:12 INFO]: Training loss at epoch 8: 0.9268660247325897
[08/28/2025 03:18:28 INFO]: Training stats: {
    "score": -0.9757484048994698,
    "rmse": 0.9757484048994698
}
[08/28/2025 03:18:28 INFO]: Val stats: {
    "score": -0.9028779877871743,
    "rmse": 0.9028779877871743
}
[08/28/2025 03:18:28 INFO]: Test stats: {
    "score": -0.9698884266488814,
    "rmse": 0.9698884266488814
}
[08/28/2025 03:18:50 INFO]: Training loss at epoch 18: 0.9587030708789825
[08/28/2025 03:18:52 INFO]: Training loss at epoch 13: 1.6676642298698425
[08/28/2025 03:18:57 INFO]: Training loss at epoch 32: 0.8730243444442749
[08/28/2025 03:19:16 INFO]: Training loss at epoch 37: 0.9156651794910431
[08/28/2025 03:19:16 INFO]: Training loss at epoch 32: 1.2440272867679596
[08/28/2025 03:19:45 INFO]: Training loss at epoch 18: 1.0196773707866669
[08/28/2025 03:19:48 INFO]: Training loss at epoch 19: 0.9470674991607666
[08/28/2025 03:20:11 INFO]: Training stats: {
    "score": -1.0060072391328605,
    "rmse": 1.0060072391328605
}
[08/28/2025 03:20:11 INFO]: Val stats: {
    "score": -0.9705977451022211,
    "rmse": 0.9705977451022211
}
[08/28/2025 03:20:11 INFO]: Test stats: {
    "score": -0.9831415376390529,
    "rmse": 0.9831415376390529
}
[08/28/2025 03:20:20 INFO]: Training loss at epoch 30: 1.0064514577388763
[08/28/2025 03:20:26 INFO]: Training loss at epoch 26: 1.000750720500946
[08/28/2025 03:20:27 INFO]: Training loss at epoch 24: 1.0348930060863495
[08/28/2025 03:21:09 INFO]: Training loss at epoch 20: 0.918987900018692
[08/28/2025 03:21:24 INFO]: Training loss at epoch 32: 0.9254184663295746
[08/28/2025 03:21:47 INFO]: Training loss at epoch 16: 1.169472098350525
[08/28/2025 03:22:08 INFO]: Training loss at epoch 21: 1.112722933292389
[08/28/2025 03:22:51 INFO]: Training loss at epoch 10: 1.0132873058319092
[08/28/2025 03:23:05 INFO]: Training loss at epoch 22: 1.2396727204322815
[08/28/2025 03:23:30 INFO]: Training loss at epoch 9: 1.059506356716156
[08/28/2025 03:23:36 INFO]: Training loss at epoch 37: 0.9073233604431152
[08/28/2025 03:24:03 INFO]: Training loss at epoch 23: 0.9036661982536316
[08/28/2025 03:24:52 INFO]: Training loss at epoch 33: 1.0173936486244202
[08/28/2025 03:25:00 INFO]: Training loss at epoch 24: 1.1011914610862732
[08/28/2025 03:25:12 INFO]: Training loss at epoch 60: 1.1158238053321838
[08/28/2025 03:25:17 INFO]: Training stats: {
    "score": -0.9937006499846568,
    "rmse": 0.9937006499846568
}
[08/28/2025 03:25:17 INFO]: Val stats: {
    "score": -0.978994633313788,
    "rmse": 0.978994633313788
}
[08/28/2025 03:25:17 INFO]: Test stats: {
    "score": -0.9772901851709207,
    "rmse": 0.9772901851709207
}
[08/28/2025 03:25:52 INFO]: Training loss at epoch 19: 0.9394699037075043
[08/28/2025 03:25:58 INFO]: Training loss at epoch 25: 0.9791744947433472
[08/28/2025 03:26:16 INFO]: Training loss at epoch 26: 0.995218813419342
[08/28/2025 03:26:55 INFO]: Training loss at epoch 26: 1.04609614610672
[08/28/2025 03:27:04 INFO]: Training loss at epoch 33: 1.0421986877918243
[08/28/2025 03:27:36 INFO]: Training loss at epoch 24: 0.8862619400024414
[08/28/2025 03:27:53 INFO]: Training loss at epoch 27: 1.0142629146575928
[08/28/2025 03:28:02 INFO]: Training loss at epoch 11: 1.3138422667980194
[08/28/2025 03:28:18 INFO]: Training loss at epoch 33: 0.9622953534126282
[08/28/2025 03:28:28 INFO]: Training loss at epoch 31: 1.2161510586738586
[08/28/2025 03:28:34 INFO]: Training loss at epoch 33: 0.7260287702083588
[08/28/2025 03:28:50 INFO]: Training loss at epoch 38: 0.828881561756134
[08/28/2025 03:28:50 INFO]: Training loss at epoch 28: 1.2165738344192505
[08/28/2025 03:29:15 INFO]: Training loss at epoch 14: 1.0849619507789612
[08/28/2025 03:29:44 INFO]: Training stats: {
    "score": -0.9846503848193239,
    "rmse": 0.9846503848193239
}
[08/28/2025 03:29:44 INFO]: Val stats: {
    "score": -0.8922574420738458,
    "rmse": 0.8922574420738458
}
[08/28/2025 03:29:44 INFO]: Test stats: {
    "score": -0.9851758912954094,
    "rmse": 0.9851758912954094
}
[08/28/2025 03:29:48 INFO]: Training loss at epoch 29: 0.8224458992481232
[08/28/2025 03:30:08 INFO]: Training stats: {
    "score": -0.9959768789749217,
    "rmse": 0.9959768789749217
}
[08/28/2025 03:30:08 INFO]: Val stats: {
    "score": -0.923201767486712,
    "rmse": 0.923201767486712
}
[08/28/2025 03:30:08 INFO]: Test stats: {
    "score": -0.9753407052454265,
    "rmse": 0.9753407052454265
}
[08/28/2025 03:30:28 INFO]: Training loss at epoch 10: 1.0488084554672241
[08/28/2025 03:30:29 INFO]: New best epoch, val score: -0.8780302216131838
[08/28/2025 03:30:29 INFO]: Saving model to: maddest-Elbert_trial_104/model_best.pth
[08/28/2025 03:30:39 INFO]: Training loss at epoch 33: 1.0457804799079895
[08/28/2025 03:31:06 INFO]: New best epoch, val score: -0.8829636116894992
[08/28/2025 03:31:06 INFO]: Saving model to: maddest-Elbert_trial_106/model_best.pth
[08/28/2025 03:31:07 INFO]: Training loss at epoch 30: 0.9660966992378235
[08/28/2025 03:31:12 INFO]: Training loss at epoch 25: 0.9109508395195007
[08/28/2025 03:31:14 INFO]: Training loss at epoch 27: 1.0369524955749512
[08/28/2025 03:31:24 INFO]: Training loss at epoch 19: 0.8813732266426086
[08/28/2025 03:31:50 INFO]: Training loss at epoch 61: 0.9066343605518341
[08/28/2025 03:32:06 INFO]: Training loss at epoch 31: 0.9894934892654419
[08/28/2025 03:32:34 INFO]: Training loss at epoch 34: 1.05836221575737
[08/28/2025 03:33:04 INFO]: Training loss at epoch 32: 1.1139990091323853
[08/28/2025 03:33:14 INFO]: Training loss at epoch 12: 0.7523580193519592
[08/28/2025 03:33:32 INFO]: Training loss at epoch 38: 1.094334214925766
[08/28/2025 03:33:50 INFO]: New best epoch, val score: -0.8940859553501879
[08/28/2025 03:33:50 INFO]: Saving model to: maddest-Elbert_trial_105/model_best.pth
[08/28/2025 03:34:00 INFO]: Training loss at epoch 33: 1.1817754805088043
[08/28/2025 03:34:13 INFO]: Training loss at epoch 17: 1.238174855709076
[08/28/2025 03:34:48 INFO]: Training loss at epoch 34: 0.9590338170528412
[08/28/2025 03:34:58 INFO]: Training loss at epoch 34: 0.9109475612640381
[08/28/2025 03:35:17 INFO]: Training stats: {
    "score": -0.9412518024481936,
    "rmse": 0.9412518024481936
}
[08/28/2025 03:35:17 INFO]: Val stats: {
    "score": -0.9052071738196478,
    "rmse": 0.9052071738196478
}
[08/28/2025 03:35:17 INFO]: Test stats: {
    "score": -0.9730614179573222,
    "rmse": 0.9730614179573222
}
[08/28/2025 03:35:42 INFO]: Training loss at epoch 11: 1.0271579027175903
[08/28/2025 03:35:56 INFO]: Training loss at epoch 35: 1.0557813942432404
[08/28/2025 03:36:38 INFO]: Training loss at epoch 32: 0.9234973788261414
[08/28/2025 03:36:53 INFO]: Training loss at epoch 36: 1.2067770063877106
[08/28/2025 03:36:57 INFO]: Training loss at epoch 27: 1.045400619506836
[08/28/2025 03:37:33 INFO]: Training loss at epoch 34: 1.0756423771381378
[08/28/2025 03:37:50 INFO]: Training loss at epoch 37: 1.0248914957046509
[08/28/2025 03:38:19 INFO]: Training loss at epoch 25: 1.0013117790222168
[08/28/2025 03:38:23 INFO]: Training loss at epoch 39: 1.0796965658664703
[08/28/2025 03:38:25 INFO]: Training loss at epoch 13: 0.9338464736938477
[08/28/2025 03:38:27 INFO]: Training loss at epoch 62: 1.0386627316474915
[08/28/2025 03:38:48 INFO]: Training loss at epoch 38: 0.8439216315746307
[08/28/2025 03:39:20 INFO]: Training loss at epoch 34: 0.8600972890853882
[08/28/2025 03:39:37 INFO]: Training loss at epoch 15: 0.8965024054050446
[08/28/2025 03:39:45 INFO]: Training loss at epoch 39: 1.192176640033722
[08/28/2025 03:39:55 INFO]: Training loss at epoch 34: 1.1809574365615845
[08/28/2025 03:40:05 INFO]: Training stats: {
    "score": -0.9935258802423534,
    "rmse": 0.9935258802423534
}
[08/28/2025 03:40:05 INFO]: Val stats: {
    "score": -0.9231941036457206,
    "rmse": 0.9231941036457206
}
[08/28/2025 03:40:05 INFO]: Test stats: {
    "score": -0.9759379879098236,
    "rmse": 0.9759379879098236
}
[08/28/2025 03:40:13 INFO]: Training loss at epoch 35: 0.9460169672966003
[08/28/2025 03:40:54 INFO]: Training loss at epoch 12: 0.9942792057991028
[08/28/2025 03:41:03 INFO]: Training loss at epoch 40: 0.9380516707897186
[08/28/2025 03:41:22 INFO]: Training loss at epoch 20: 0.9825893342494965
[08/28/2025 03:41:37 INFO]: Training stats: {
    "score": -0.9847870942294772,
    "rmse": 0.9847870942294772
}
[08/28/2025 03:41:37 INFO]: Val stats: {
    "score": -0.9274320955347768,
    "rmse": 0.9274320955347768
}
[08/28/2025 03:41:37 INFO]: Test stats: {
    "score": -0.9682351430604254,
    "rmse": 0.9682351430604254
}
[08/28/2025 03:41:55 INFO]: Training loss at epoch 26: 1.0050503611564636
[08/28/2025 03:42:00 INFO]: Training loss at epoch 28: 0.9706786274909973
[08/28/2025 03:42:00 INFO]: Training loss at epoch 41: 0.9085395634174347
[08/28/2025 03:42:32 INFO]: Training loss at epoch 35: 0.9612258970737457
[08/28/2025 03:42:58 INFO]: Training loss at epoch 42: 0.8377625644207001
[08/28/2025 03:43:06 INFO]: Running Final Evaluation...
[08/28/2025 03:43:25 INFO]: Training loss at epoch 39: 1.0424840450286865
[08/28/2025 03:43:25 INFO]: Training accuracy: {
    "score": -1.0288865177181052,
    "rmse": 1.0288865177181052
}
[08/28/2025 03:43:25 INFO]: Val accuracy: {
    "score": -0.8849887410264591,
    "rmse": 0.8849887410264591
}
[08/28/2025 03:43:25 INFO]: Test accuracy: {
    "score": -1.0075854427721536,
    "rmse": 1.0075854427721536
}
[08/28/2025 03:43:25 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_107",
    "best_epoch": 11,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0075854427721536,
        "rmse": 1.0075854427721536
    },
    "train_stats": {
        "score": -1.0288865177181052,
        "rmse": 1.0288865177181052
    },
    "val_stats": {
        "score": -0.8849887410264591,
        "rmse": 0.8849887410264591
    }
}
[08/28/2025 03:43:25 INFO]: Procewss finished for trial maddest-Elbert_trial_107
[08/28/2025 03:43:25 INFO]: 
_________________________________________________

[08/28/2025 03:43:25 INFO]: train_net_for_optune.py main() running.
[08/28/2025 03:43:25 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 1.279792037381649
  attention_dropout: 0.1510466173452259
  ffn_dropout: 0.1510466173452259
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.125368801726923e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_108

[08/28/2025 03:43:26 INFO]: This ft_transformer has 20.773 million parameters.
[08/28/2025 03:43:26 INFO]: Training will start at epoch 0.
[08/28/2025 03:43:26 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 03:43:37 INFO]: Training loss at epoch 14: 0.7922899723052979
[08/28/2025 03:44:47 INFO]: Training loss at epoch 33: 1.119289606809616
[08/28/2025 03:45:07 INFO]: Training loss at epoch 63: 1.0301879048347473
[08/28/2025 03:45:45 INFO]: Running Final Evaluation...
[08/28/2025 03:45:55 INFO]: Running Final Evaluation...
[08/28/2025 03:46:06 INFO]: Training loss at epoch 13: 0.9663010239601135
[08/28/2025 03:46:40 INFO]: Training loss at epoch 18: 0.7907321453094482
[08/28/2025 03:46:44 INFO]: New best epoch, val score: -0.8824550568164292
[08/28/2025 03:46:44 INFO]: Saving model to: maddest-Elbert_trial_106/model_best.pth
[08/28/2025 03:46:49 INFO]: Training stats: {
    "score": -0.998396772938352,
    "rmse": 0.998396772938352
}
[08/28/2025 03:46:49 INFO]: Val stats: {
    "score": -0.9171475264296003,
    "rmse": 0.9171475264296003
}
[08/28/2025 03:46:49 INFO]: Test stats: {
    "score": -0.9715305582810977,
    "rmse": 0.9715305582810977
}
[08/28/2025 03:46:50 INFO]: Training loss at epoch 35: 0.9392924308776855
[08/28/2025 03:46:52 INFO]: Training loss at epoch 20: 1.0058898627758026
[08/28/2025 03:47:40 INFO]: Training loss at epoch 28: 0.9958303868770599
[08/28/2025 03:47:55 INFO]: Training loss at epoch 36: 0.9644989967346191
[08/28/2025 03:48:12 INFO]: Training accuracy: {
    "score": -1.0283173062600424,
    "rmse": 1.0283173062600424
}
[08/28/2025 03:48:12 INFO]: Val accuracy: {
    "score": -0.8765237083737718,
    "rmse": 0.8765237083737718
}
[08/28/2025 03:48:12 INFO]: Test accuracy: {
    "score": -1.0091188771412887,
    "rmse": 1.0091188771412887
}
[08/28/2025 03:48:12 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_79",
    "best_epoch": 32,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0091188771412887,
        "rmse": 1.0091188771412887
    },
    "train_stats": {
        "score": -1.0283173062600424,
        "rmse": 1.0283173062600424
    },
    "val_stats": {
        "score": -0.8765237083737718,
        "rmse": 0.8765237083737718
    }
}
[08/28/2025 03:48:12 INFO]: Procewss finished for trial maddest-Elbert_trial_79
[08/28/2025 03:48:12 INFO]: 
_________________________________________________

[08/28/2025 03:48:12 INFO]: train_net_for_optune.py main() running.
[08/28/2025 03:48:12 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 1.274457047496575
  attention_dropout: 0.028753364378821963
  ffn_dropout: 0.028753364378821963
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.074239008224204e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_109

[08/28/2025 03:48:12 INFO]: This ft_transformer has 20.727 million parameters.
[08/28/2025 03:48:12 INFO]: Training will start at epoch 0.
[08/28/2025 03:48:12 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 03:48:34 INFO]: Training accuracy: {
    "score": -1.03973245563109,
    "rmse": 1.03973245563109
}
[08/28/2025 03:48:34 INFO]: Val accuracy: {
    "score": -0.8767994363296527,
    "rmse": 0.8767994363296527
}
[08/28/2025 03:48:34 INFO]: Test accuracy: {
    "score": -1.0114423761334723,
    "rmse": 1.0114423761334723
}
[08/28/2025 03:48:34 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_98",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0114423761334723,
        "rmse": 1.0114423761334723
    },
    "train_stats": {
        "score": -1.03973245563109,
        "rmse": 1.03973245563109
    },
    "val_stats": {
        "score": -0.8767994363296527,
        "rmse": 0.8767994363296527
    }
}
[08/28/2025 03:48:34 INFO]: Procewss finished for trial maddest-Elbert_trial_98
[08/28/2025 03:48:34 INFO]: 
_________________________________________________

[08/28/2025 03:48:34 INFO]: train_net_for_optune.py main() running.
[08/28/2025 03:48:34 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 2.2452769390922134
  attention_dropout: 0.16182412559761375
  ffn_dropout: 0.16182412559761375
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.073264501710709e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_110

[08/28/2025 03:48:35 INFO]: This ft_transformer has 18.014 million parameters.
[08/28/2025 03:48:35 INFO]: Training will start at epoch 0.
[08/28/2025 03:48:35 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 03:48:45 INFO]: Training loss at epoch 15: 0.8087259829044342
[08/28/2025 03:49:01 INFO]: Training loss at epoch 26: 0.9966159462928772
[08/28/2025 03:49:07 INFO]: Training loss at epoch 35: 0.8699691295623779
[08/28/2025 03:49:55 INFO]: Training loss at epoch 16: 0.9670130908489227
[08/28/2025 03:50:03 INFO]: Training loss at epoch 35: 0.9316912591457367
[08/28/2025 03:50:15 INFO]: Training loss at epoch 36: 0.9494224190711975
[08/28/2025 03:51:08 INFO]: Training loss at epoch 40: 1.0148224830627441
[08/28/2025 03:51:18 INFO]: Training loss at epoch 14: 1.0501892864704132
[08/28/2025 03:52:34 INFO]: Training loss at epoch 0: 1.0192791819572449
[08/28/2025 03:52:39 INFO]: Training loss at epoch 27: 1.0295107960700989
[08/28/2025 03:52:46 INFO]: Training loss at epoch 29: 0.8415351808071136
[08/28/2025 03:52:58 INFO]: Training loss at epoch 21: 0.8016542792320251
[08/28/2025 03:53:48 INFO]: New best epoch, val score: -0.9525166601474546
[08/28/2025 03:53:48 INFO]: Saving model to: maddest-Elbert_trial_108/model_best.pth
[08/28/2025 03:53:59 INFO]: Training loss at epoch 16: 0.9776658117771149
[08/28/2025 03:55:36 INFO]: Training loss at epoch 37: 0.8330630660057068
[08/28/2025 03:56:04 INFO]: Training loss at epoch 36: 1.057627111673355
[08/28/2025 03:56:14 INFO]: Training loss at epoch 0: 1.104950726032257
[08/28/2025 03:56:23 INFO]: Training stats: {
    "score": -0.9718812673021172,
    "rmse": 0.9718812673021172
}
[08/28/2025 03:56:23 INFO]: Val stats: {
    "score": -0.9738211167222887,
    "rmse": 0.9738211167222887
}
[08/28/2025 03:56:23 INFO]: Test stats: {
    "score": -0.9875905784717233,
    "rmse": 0.9875905784717233
}
[08/28/2025 03:56:32 INFO]: Training loss at epoch 15: 1.0261314511299133
[08/28/2025 03:56:43 INFO]: Training loss at epoch 40: 0.9383321106433868
[08/28/2025 03:57:16 INFO]: New best epoch, val score: -0.9186693080052578
[08/28/2025 03:57:16 INFO]: Saving model to: maddest-Elbert_trial_110/model_best.pth
[08/28/2025 03:57:21 INFO]: Training loss at epoch 0: 0.9239177703857422
[08/28/2025 03:58:01 INFO]: Training loss at epoch 37: 0.9729920327663422
[08/28/2025 03:58:22 INFO]: Training loss at epoch 29: 0.9886207282543182
[08/28/2025 03:58:23 INFO]: Training loss at epoch 36: 1.0705779790878296
[08/28/2025 03:58:25 INFO]: Training loss at epoch 21: 0.7829103469848633
[08/28/2025 03:58:33 INFO]: New best epoch, val score: -0.9089999927795467
[08/28/2025 03:58:33 INFO]: Saving model to: maddest-Elbert_trial_109/model_best.pth
[08/28/2025 03:59:04 INFO]: Training loss at epoch 19: 0.7202783674001694
[08/28/2025 03:59:10 INFO]: Training loss at epoch 17: 0.8092555701732635
[08/28/2025 03:59:45 INFO]: Training loss at epoch 27: 0.9109724164009094
[08/28/2025 04:00:16 INFO]: Training loss at epoch 17: 0.8628604412078857
[08/28/2025 04:00:39 INFO]: Training loss at epoch 41: 1.1646214127540588
[08/28/2025 04:00:50 INFO]: Training loss at epoch 36: 0.9172893464565277
[08/28/2025 04:01:46 INFO]: Training loss at epoch 16: 0.9121610522270203
[08/28/2025 04:02:02 INFO]: Training stats: {
    "score": -0.9719424788167035,
    "rmse": 0.9719424788167035
}
[08/28/2025 04:02:02 INFO]: Val stats: {
    "score": -0.9722205377962742,
    "rmse": 0.9722205377962742
}
[08/28/2025 04:02:02 INFO]: Test stats: {
    "score": -0.9914441042617986,
    "rmse": 0.9914441042617986
}
[08/28/2025 04:02:59 INFO]: Training loss at epoch 1: 1.2188894152641296
[08/28/2025 04:03:18 INFO]: Training stats: {
    "score": -0.947635023958214,
    "rmse": 0.947635023958214
}
[08/28/2025 04:03:18 INFO]: Val stats: {
    "score": -0.8971600570280756,
    "rmse": 0.8971600570280756
}
[08/28/2025 04:03:18 INFO]: Test stats: {
    "score": -0.9890279284849941,
    "rmse": 0.9890279284849941
}
[08/28/2025 04:03:19 INFO]: Training loss at epoch 38: 0.7615402787923813
[08/28/2025 04:03:27 INFO]: Training loss at epoch 28: 0.8903306424617767
[08/28/2025 04:04:13 INFO]: New best epoch, val score: -0.9098975062441828
[08/28/2025 04:04:13 INFO]: Saving model to: maddest-Elbert_trial_108/model_best.pth
[08/28/2025 04:04:24 INFO]: Training loss at epoch 18: 0.9093046188354492
[08/28/2025 04:04:38 INFO]: Training loss at epoch 22: 0.8168112635612488
[08/28/2025 04:05:00 INFO]: Training loss at epoch 1: 0.9521787762641907
[08/28/2025 04:05:21 INFO]: Training loss at epoch 37: 1.1557200849056244
[08/28/2025 04:05:48 INFO]: Training loss at epoch 38: 0.9882094860076904
[08/28/2025 04:06:46 INFO]: Training loss at epoch 41: 0.9558515846729279
[08/28/2025 04:07:00 INFO]: Training loss at epoch 17: 1.0158277750015259
[08/28/2025 04:07:11 INFO]: Training loss at epoch 30: 0.9084221124649048
[08/28/2025 04:07:44 INFO]: Training loss at epoch 37: 0.8978087306022644
[08/28/2025 04:07:45 INFO]: Training loss at epoch 1: 0.8945729434490204
[08/28/2025 04:09:37 INFO]: Training loss at epoch 19: 0.9258051216602325
[08/28/2025 04:10:05 INFO]: Training loss at epoch 22: 0.9177009165287018
[08/28/2025 04:10:16 INFO]: Training loss at epoch 42: 1.211851567029953
[08/28/2025 04:10:34 INFO]: Training loss at epoch 28: 1.1518608331680298
[08/28/2025 04:10:42 INFO]: Training loss at epoch 18: 1.074292004108429
[08/28/2025 04:11:04 INFO]: Training loss at epoch 39: 0.9799104332923889
[08/28/2025 04:11:24 INFO]: Training stats: {
    "score": -0.947217182304114,
    "rmse": 0.947217182304114
}
[08/28/2025 04:11:24 INFO]: Val stats: {
    "score": -0.9008397374573376,
    "rmse": 0.9008397374573376
}
[08/28/2025 04:11:24 INFO]: Test stats: {
    "score": -0.9866062231203498,
    "rmse": 0.9866062231203498
}
[08/28/2025 04:11:41 INFO]: Training loss at epoch 37: 1.13642618060112
[08/28/2025 04:12:16 INFO]: Training loss at epoch 18: 1.0981188714504242
[08/28/2025 04:12:49 INFO]: Training loss at epoch 30: 0.9972730875015259
[08/28/2025 04:12:57 INFO]: Running Final Evaluation...
[08/28/2025 04:13:25 INFO]: Training loss at epoch 2: 1.3422990441322327
[08/28/2025 04:13:36 INFO]: Training loss at epoch 39: 0.9980506300926208
[08/28/2025 04:13:41 INFO]: Training stats: {
    "score": -0.9968094577714296,
    "rmse": 0.9968094577714296
}
[08/28/2025 04:13:41 INFO]: Val stats: {
    "score": -0.9302588483265507,
    "rmse": 0.9302588483265507
}
[08/28/2025 04:13:41 INFO]: Test stats: {
    "score": -0.9722271912592088,
    "rmse": 0.9722271912592088
}
[08/28/2025 04:13:49 INFO]: Training loss at epoch 2: 1.3950837552547455
[08/28/2025 04:14:15 INFO]: Training loss at epoch 29: 0.857543557882309
[08/28/2025 04:14:40 INFO]: Training loss at epoch 38: 0.8244105279445648
[08/28/2025 04:15:47 INFO]: Training loss at epoch 20: 0.9353234767913818
[08/28/2025 04:16:14 INFO]: Training stats: {
    "score": -0.994052585904726,
    "rmse": 0.994052585904726
}
[08/28/2025 04:16:14 INFO]: Val stats: {
    "score": -0.9601627563536965,
    "rmse": 0.9601627563536965
}
[08/28/2025 04:16:14 INFO]: Test stats: {
    "score": -0.9764326955130254,
    "rmse": 0.9764326955130254
}
[08/28/2025 04:16:18 INFO]: Training loss at epoch 23: 0.8260374963283539
[08/28/2025 04:16:37 INFO]: Training loss at epoch 20: 0.6833239495754242
[08/28/2025 04:16:42 INFO]: Training accuracy: {
    "score": -1.0059461571166013,
    "rmse": 1.0059461571166013
}
[08/28/2025 04:16:42 INFO]: Val accuracy: {
    "score": -0.8931006394421335,
    "rmse": 0.8931006394421335
}
[08/28/2025 04:16:42 INFO]: Test accuracy: {
    "score": -1.0153537681996336,
    "rmse": 1.0153537681996336
}
[08/28/2025 04:16:42 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_86",
    "best_epoch": 6,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0153537681996336,
        "rmse": 1.0153537681996336
    },
    "train_stats": {
        "score": -1.0059461571166013,
        "rmse": 1.0059461571166013
    },
    "val_stats": {
        "score": -0.8931006394421335,
        "rmse": 0.8931006394421335
    }
}
[08/28/2025 04:16:42 INFO]: Procewss finished for trial maddest-Elbert_trial_86
[08/28/2025 04:16:42 INFO]: 
_________________________________________________

[08/28/2025 04:16:42 INFO]: train_net_for_optune.py main() running.
[08/28/2025 04:16:42 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 0.9039463178385134
  attention_dropout: 0.15621157783513678
  ffn_dropout: 0.15621157783513678
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.794393171348485e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_111

[08/28/2025 04:16:43 INFO]: This ft_transformer has 12.381 million parameters.
[08/28/2025 04:16:43 INFO]: Training will start at epoch 0.
[08/28/2025 04:16:43 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 04:16:47 INFO]: Training loss at epoch 42: 0.9285367131233215
[08/28/2025 04:17:03 INFO]: Training loss at epoch 38: 1.0750646889209747
[08/28/2025 04:17:14 INFO]: New best epoch, val score: -0.8878428355954713
[08/28/2025 04:17:14 INFO]: Saving model to: maddest-Elbert_trial_105/model_best.pth
[08/28/2025 04:17:30 INFO]: Training loss at epoch 19: 1.0291833877563477
[08/28/2025 04:17:55 INFO]: Training stats: {
    "score": -0.9775839509386267,
    "rmse": 0.9775839509386267
}
[08/28/2025 04:17:55 INFO]: Val stats: {
    "score": -0.932941453512849,
    "rmse": 0.932941453512849
}
[08/28/2025 04:17:55 INFO]: Test stats: {
    "score": -0.9668925817724214,
    "rmse": 0.9668925817724214
}
[08/28/2025 04:18:01 INFO]: Training loss at epoch 31: 0.9719600081443787
[08/28/2025 04:18:08 INFO]: Training loss at epoch 2: 1.274596631526947
[08/28/2025 04:19:15 INFO]: Training stats: {
    "score": -0.9722085360447101,
    "rmse": 0.9722085360447101
}
[08/28/2025 04:19:15 INFO]: Val stats: {
    "score": -0.9008501258304071,
    "rmse": 0.9008501258304071
}
[08/28/2025 04:19:15 INFO]: Test stats: {
    "score": -0.9701121117366865,
    "rmse": 0.9701121117366865
}
[08/28/2025 04:19:49 INFO]: Training loss at epoch 43: 0.9402941465377808
[08/28/2025 04:21:04 INFO]: Training loss at epoch 19: 0.9857077598571777
[08/28/2025 04:21:20 INFO]: Training loss at epoch 29: 0.9735662043094635
[08/28/2025 04:21:23 INFO]: Training loss at epoch 40: 1.0018059611320496
[08/28/2025 04:21:43 INFO]: Training loss at epoch 23: 0.770770937204361
[08/28/2025 04:21:48 INFO]: Training loss at epoch 21: 1.0595855414867401
[08/28/2025 04:22:20 INFO]: Training loss at epoch 0: 1.190708041191101
[08/28/2025 04:22:33 INFO]: Training loss at epoch 3: 0.9554012417793274
[08/28/2025 04:23:04 INFO]: New best epoch, val score: -0.9312832841330596
[08/28/2025 04:23:04 INFO]: Saving model to: maddest-Elbert_trial_111/model_best.pth
[08/28/2025 04:23:33 INFO]: Training loss at epoch 31: 0.8281597793102264
[08/28/2025 04:23:50 INFO]: Training loss at epoch 3: 1.2882956862449646
[08/28/2025 04:23:56 INFO]: Training loss at epoch 39: 1.1784313023090363
[08/28/2025 04:23:59 INFO]: Training loss at epoch 40: 0.8869193196296692
[08/28/2025 04:24:28 INFO]: Training loss at epoch 20: 1.1162064969539642
[08/28/2025 04:24:35 INFO]: Training stats: {
    "score": -0.9789570113046459,
    "rmse": 0.9789570113046459
}
[08/28/2025 04:24:35 INFO]: Val stats: {
    "score": -0.9594310346656444,
    "rmse": 0.9594310346656444
}
[08/28/2025 04:24:35 INFO]: Test stats: {
    "score": -0.9743758662940681,
    "rmse": 0.9743758662940681
}
[08/28/2025 04:25:02 INFO]: Training stats: {
    "score": -1.0017029383774638,
    "rmse": 1.0017029383774638
}
[08/28/2025 04:25:02 INFO]: Val stats: {
    "score": -1.031332663591187,
    "rmse": 1.031332663591187
}
[08/28/2025 04:25:02 INFO]: Test stats: {
    "score": -1.0055972365370696,
    "rmse": 1.0055972365370696
}
[08/28/2025 04:26:19 INFO]: Training loss at epoch 39: 1.0215193033218384
[08/28/2025 04:26:46 INFO]: Training loss at epoch 43: 0.7934006154537201
[08/28/2025 04:27:03 INFO]: Training loss at epoch 22: 0.8259254693984985
[08/28/2025 04:27:06 INFO]: Training stats: {
    "score": -1.012106175229706,
    "rmse": 1.012106175229706
}
[08/28/2025 04:27:06 INFO]: Val stats: {
    "score": -0.9912967136980283,
    "rmse": 0.9912967136980283
}
[08/28/2025 04:27:06 INFO]: Test stats: {
    "score": -0.9875227840305324,
    "rmse": 0.9875227840305324
}
[08/28/2025 04:27:59 INFO]: Training loss at epoch 24: 1.2119343876838684
[08/28/2025 04:28:17 INFO]: Training loss at epoch 21: 1.1083945333957672
[08/28/2025 04:28:29 INFO]: Training loss at epoch 3: 1.117262065410614
[08/28/2025 04:28:40 INFO]: Training loss at epoch 30: 1.1048864424228668
[08/28/2025 04:28:43 INFO]: Training loss at epoch 1: 1.0258744955062866
[08/28/2025 04:28:49 INFO]: Training loss at epoch 32: 1.2864308953285217
[08/28/2025 04:29:07 INFO]: Training loss at epoch 41: 0.961408793926239
[08/28/2025 04:29:28 INFO]: Training loss at epoch 44: 0.9268280267715454
[08/28/2025 04:29:28 INFO]: Training stats: {
    "score": -0.9960208456187696,
    "rmse": 0.9960208456187696
}
[08/28/2025 04:29:28 INFO]: Val stats: {
    "score": -0.9313554944643992,
    "rmse": 0.9313554944643992
}
[08/28/2025 04:29:28 INFO]: Test stats: {
    "score": -0.9719093552532705,
    "rmse": 0.9719093552532705
}
[08/28/2025 04:29:29 INFO]: New best epoch, val score: -0.9223563127963175
[08/28/2025 04:29:29 INFO]: Saving model to: maddest-Elbert_trial_111/model_best.pth
[08/28/2025 04:29:43 INFO]: New best epoch, val score: -0.9057966940513041
[08/28/2025 04:29:43 INFO]: Saving model to: maddest-Elbert_trial_109/model_best.pth
[08/28/2025 04:29:45 INFO]: Training loss at epoch 21: 0.8459461331367493
[08/28/2025 04:30:21 INFO]: New best epoch, val score: -0.8819363924056426
[08/28/2025 04:30:21 INFO]: Saving model to: maddest-Elbert_trial_106/model_best.pth
[08/28/2025 04:31:17 INFO]: Training loss at epoch 4: 1.3024396896362305
[08/28/2025 04:31:46 INFO]: Training loss at epoch 41: 0.8518478870391846
[08/28/2025 04:32:16 INFO]: Training loss at epoch 23: 1.0617340803146362
[08/28/2025 04:33:21 INFO]: Training loss at epoch 24: 0.870760053396225
[08/28/2025 04:34:14 INFO]: Training loss at epoch 4: 0.937291294336319
[08/28/2025 04:34:19 INFO]: Training loss at epoch 32: 1.0028313398361206
[08/28/2025 04:34:43 INFO]: New best epoch, val score: -0.8756932143736926
[08/28/2025 04:34:43 INFO]: Saving model to: maddest-Elbert_trial_102/model_best.pth
[08/28/2025 04:34:58 INFO]: Training loss at epoch 20: 0.8687466681003571
[08/28/2025 04:34:58 INFO]: Training loss at epoch 22: 0.8807151913642883
[08/28/2025 04:35:08 INFO]: Training loss at epoch 2: 1.1153716444969177
[08/28/2025 04:35:27 INFO]: New best epoch, val score: -0.8941884680039764
[08/28/2025 04:35:27 INFO]: Saving model to: maddest-Elbert_trial_108/model_best.pth
[08/28/2025 04:35:48 INFO]: Training loss at epoch 30: 1.119950383901596
[08/28/2025 04:35:53 INFO]: New best epoch, val score: -0.8774097802728773
[08/28/2025 04:35:53 INFO]: Saving model to: maddest-Elbert_trial_111/model_best.pth
[08/28/2025 04:36:23 INFO]: Training loss at epoch 40: 1.0586101412773132
[08/28/2025 04:36:44 INFO]: Training loss at epoch 44: 1.028383046388626
[08/28/2025 04:36:51 INFO]: Training loss at epoch 42: 0.7571631968021393
[08/28/2025 04:37:28 INFO]: Training loss at epoch 24: 0.8580918908119202
[08/28/2025 04:37:45 INFO]: Running Final Evaluation...
[08/28/2025 04:38:38 INFO]: Training loss at epoch 40: 0.936286449432373
[08/28/2025 04:38:45 INFO]: Training loss at epoch 4: 0.9670364260673523
[08/28/2025 04:38:53 INFO]: Training loss at epoch 45: 0.863330215215683
[08/28/2025 04:39:19 INFO]: Training loss at epoch 31: 0.9842478632926941
[08/28/2025 04:39:23 INFO]: Training loss at epoch 42: 0.956504613161087
[08/28/2025 04:39:30 INFO]: Training loss at epoch 25: 0.8471069037914276
[08/28/2025 04:39:30 INFO]: Training loss at epoch 33: 0.8431457877159119
[08/28/2025 04:39:50 INFO]: Training loss at epoch 5: 1.1420789957046509
[08/28/2025 04:39:55 INFO]: New best epoch, val score: -0.8905217388205084
[08/28/2025 04:39:55 INFO]: Saving model to: maddest-Elbert_trial_109/model_best.pth
[08/28/2025 04:40:02 INFO]: Training loss at epoch 23: 0.9904657006263733
[08/28/2025 04:40:15 INFO]: Training accuracy: {
    "score": -1.0518769863674406,
    "rmse": 1.0518769863674406
}
[08/28/2025 04:40:15 INFO]: Val accuracy: {
    "score": -0.8815755583621478,
    "rmse": 0.8815755583621478
}
[08/28/2025 04:40:15 INFO]: Test accuracy: {
    "score": -1.0275370205157783,
    "rmse": 1.0275370205157783
}
[08/28/2025 04:40:15 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_97",
    "best_epoch": 11,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0275370205157783,
        "rmse": 1.0275370205157783
    },
    "train_stats": {
        "score": -1.0518769863674406,
        "rmse": 1.0518769863674406
    },
    "val_stats": {
        "score": -0.8815755583621478,
        "rmse": 0.8815755583621478
    }
}
[08/28/2025 04:40:15 INFO]: Procewss finished for trial maddest-Elbert_trial_97
[08/28/2025 04:40:15 INFO]: 
_________________________________________________

[08/28/2025 04:40:15 INFO]: train_net_for_optune.py main() running.
[08/28/2025 04:40:15 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 2.172567036537526
  attention_dropout: 0.15999573148853077
  ffn_dropout: 0.15999573148853077
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.281756062766326e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_112

[08/28/2025 04:40:15 INFO]: This ft_transformer has 17.711 million parameters.
[08/28/2025 04:40:15 INFO]: Training will start at epoch 0.
[08/28/2025 04:40:15 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 04:40:34 INFO]: Training loss at epoch 22: 0.943102240562439
[08/28/2025 04:41:16 INFO]: Training loss at epoch 3: 1.2243396639823914
[08/28/2025 04:42:22 INFO]: Training loss at epoch 25: 0.9615436792373657
[08/28/2025 04:44:11 INFO]: Training loss at epoch 5: 1.1111196279525757
[08/28/2025 04:44:30 INFO]: Training loss at epoch 25: 1.0111919939517975
[08/28/2025 04:44:34 INFO]: Training loss at epoch 33: 0.9752699434757233
[08/28/2025 04:44:50 INFO]: Training loss at epoch 21: 1.0825102031230927
[08/28/2025 04:44:54 INFO]: Training loss at epoch 24: 0.9953181743621826
[08/28/2025 04:45:07 INFO]: Training loss at epoch 41: 0.9989455342292786
[08/28/2025 04:45:17 INFO]: New best epoch, val score: -0.8914381715663856
[08/28/2025 04:45:17 INFO]: Saving model to: maddest-Elbert_trial_108/model_best.pth
[08/28/2025 04:45:42 INFO]: Running Final Evaluation...
[08/28/2025 04:45:55 INFO]: Training loss at epoch 31: 1.2711854577064514
[08/28/2025 04:46:03 INFO]: Training loss at epoch 45: 0.9980790317058563
[08/28/2025 04:46:33 INFO]: Training loss at epoch 43: 1.0277813076972961
[08/28/2025 04:47:09 INFO]: Training loss at epoch 26: 0.8631074726581573
[08/28/2025 04:47:09 INFO]: Training loss at epoch 4: 1.122783124446869
[08/28/2025 04:47:11 INFO]: Training loss at epoch 0: 1.1142449378967285
[08/28/2025 04:47:14 INFO]: Training loss at epoch 41: 0.9713570177555084
[08/28/2025 04:47:26 INFO]: Running Final Evaluation...
[08/28/2025 04:47:47 INFO]: Training loss at epoch 46: 1.0714929401874542
[08/28/2025 04:47:58 INFO]: Training loss at epoch 6: 1.0148819088935852
[08/28/2025 04:48:10 INFO]: New best epoch, val score: -0.9578210173096872
[08/28/2025 04:48:10 INFO]: Saving model to: maddest-Elbert_trial_112/model_best.pth
[08/28/2025 04:48:28 INFO]: Training loss at epoch 5: 1.0131934881210327
[08/28/2025 04:49:15 INFO]: Training accuracy: {
    "score": -1.0317817074142832,
    "rmse": 1.0317817074142832
}
[08/28/2025 04:49:15 INFO]: Val accuracy: {
    "score": -0.8871779397246747,
    "rmse": 0.8871779397246747
}
[08/28/2025 04:49:15 INFO]: Test accuracy: {
    "score": -1.0068911783589625,
    "rmse": 1.0068911783589625
}
[08/28/2025 04:49:15 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_93",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0068911783589625,
        "rmse": 1.0068911783589625
    },
    "train_stats": {
        "score": -1.0317817074142832,
        "rmse": 1.0317817074142832
    },
    "val_stats": {
        "score": -0.8871779397246747,
        "rmse": 0.8871779397246747
    }
}
[08/28/2025 04:49:15 INFO]: Procewss finished for trial maddest-Elbert_trial_93
[08/28/2025 04:49:15 INFO]: 
_________________________________________________

[08/28/2025 04:49:15 INFO]: train_net_for_optune.py main() running.
[08/28/2025 04:49:15 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 2.2781379920662657
  attention_dropout: 0.16441601884286366
  ffn_dropout: 0.16441601884286366
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.3949654438584225e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_113

[08/28/2025 04:49:15 INFO]: This ft_transformer has 18.154 million parameters.
[08/28/2025 04:49:15 INFO]: Training will start at epoch 0.
[08/28/2025 04:49:15 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 04:49:19 INFO]: Training loss at epoch 32: 0.8143536746501923
[08/28/2025 04:49:35 INFO]: Training loss at epoch 34: 0.832499086856842
[08/28/2025 04:49:47 INFO]: Training loss at epoch 25: 0.9548185765743256
[08/28/2025 04:49:58 INFO]: Training accuracy: {
    "score": -1.0254783276755532,
    "rmse": 1.0254783276755532
}
[08/28/2025 04:49:58 INFO]: Val accuracy: {
    "score": -0.8842771902154359,
    "rmse": 0.8842771902154359
}
[08/28/2025 04:49:58 INFO]: Test accuracy: {
    "score": -1.0022023722221174,
    "rmse": 1.0022023722221174
}
[08/28/2025 04:49:58 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_96",
    "best_epoch": 12,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0022023722221174,
        "rmse": 1.0022023722221174
    },
    "train_stats": {
        "score": -1.0254783276755532,
        "rmse": 1.0254783276755532
    },
    "val_stats": {
        "score": -0.8842771902154359,
        "rmse": 0.8842771902154359
    }
}
[08/28/2025 04:49:58 INFO]: Procewss finished for trial maddest-Elbert_trial_96
[08/28/2025 04:49:58 INFO]: 
_________________________________________________

[08/28/2025 04:49:58 INFO]: train_net_for_optune.py main() running.
[08/28/2025 04:49:58 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 2.063972328153202
  attention_dropout: 0.15241627622796805
  ffn_dropout: 0.15241627622796805
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.234294515529608e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_114

[08/28/2025 04:49:58 INFO]: This ft_transformer has 0.293 million parameters.
[08/28/2025 04:49:58 INFO]: Training will start at epoch 0.
[08/28/2025 04:49:58 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 04:50:19 INFO]: Training loss at epoch 26: 0.9524662792682648
[08/28/2025 04:50:33 INFO]: Training loss at epoch 0: 1.0559595823287964
[08/28/2025 04:50:38 INFO]: New best epoch, val score: -1.0857369400107515
[08/28/2025 04:50:38 INFO]: Saving model to: maddest-Elbert_trial_114/model_best.pth
[08/28/2025 04:51:16 INFO]: Training loss at epoch 1: 0.9177004992961884
[08/28/2025 04:51:21 INFO]: New best epoch, val score: -1.0183110286108823
[08/28/2025 04:51:21 INFO]: Saving model to: maddest-Elbert_trial_114/model_best.pth
[08/28/2025 04:51:57 INFO]: Training loss at epoch 2: 1.0565680265426636
[08/28/2025 04:52:01 INFO]: Training loss at epoch 27: 0.9525412917137146
[08/28/2025 04:52:03 INFO]: New best epoch, val score: -0.9421971965051021
[08/28/2025 04:52:03 INFO]: Saving model to: maddest-Elbert_trial_114/model_best.pth
[08/28/2025 04:52:08 INFO]: Training loss at epoch 23: 0.8477208018302917
[08/28/2025 04:52:38 INFO]: Training loss at epoch 3: 1.046473354101181
[08/28/2025 04:52:43 INFO]: New best epoch, val score: -0.8912248460401705
[08/28/2025 04:52:43 INFO]: Saving model to: maddest-Elbert_trial_114/model_best.pth
[08/28/2025 04:53:07 INFO]: Training loss at epoch 5: 0.908489853143692
[08/28/2025 04:53:17 INFO]: Training loss at epoch 4: 1.1760839819908142
[08/28/2025 04:53:22 INFO]: New best epoch, val score: -0.8820725328235303
[08/28/2025 04:53:22 INFO]: Saving model to: maddest-Elbert_trial_114/model_best.pth
[08/28/2025 04:53:44 INFO]: Training loss at epoch 6: 1.0933334827423096
[08/28/2025 04:53:48 INFO]: Training loss at epoch 42: 1.0258790254592896
[08/28/2025 04:53:57 INFO]: Training loss at epoch 5: 0.9408737421035767
[08/28/2025 04:54:21 INFO]: Training loss at epoch 22: 1.1063913404941559
[08/28/2025 04:54:32 INFO]: Training loss at epoch 26: 1.0346643328666687
[08/28/2025 04:54:37 INFO]: Training loss at epoch 6: 1.034861981868744
[08/28/2025 04:54:42 INFO]: New best epoch, val score: -0.8820441476258888
[08/28/2025 04:54:42 INFO]: Saving model to: maddest-Elbert_trial_114/model_best.pth
[08/28/2025 04:54:50 INFO]: New best epoch, val score: -0.8865707625454642
[08/28/2025 04:54:50 INFO]: Saving model to: maddest-Elbert_trial_108/model_best.pth
[08/28/2025 04:55:05 INFO]: Training loss at epoch 1: 1.4018746614456177
[08/28/2025 04:55:09 INFO]: Training loss at epoch 26: 0.777191549539566
[08/28/2025 04:55:17 INFO]: Training loss at epoch 46: 0.9175440967082977
[08/28/2025 04:55:18 INFO]: Training loss at epoch 7: 1.082651436328888
[08/28/2025 04:55:23 INFO]: New best epoch, val score: -0.8819583613897211
[08/28/2025 04:55:23 INFO]: Saving model to: maddest-Elbert_trial_114/model_best.pth
[08/28/2025 04:55:48 INFO]: Training loss at epoch 42: 1.0150286555290222
[08/28/2025 04:55:53 INFO]: Training loss at epoch 32: 0.9596855640411377
[08/28/2025 04:56:00 INFO]: Training loss at epoch 7: 0.9730461239814758
[08/28/2025 04:56:00 INFO]: Training loss at epoch 8: 0.9891811907291412
[08/28/2025 04:56:05 INFO]: New best epoch, val score: -0.881876493741647
[08/28/2025 04:56:05 INFO]: Saving model to: maddest-Elbert_trial_114/model_best.pth
[08/28/2025 04:56:15 INFO]: Training loss at epoch 0: 1.0061295926570892
[08/28/2025 04:56:34 INFO]: Training loss at epoch 47: 1.0267050564289093
[08/28/2025 04:56:40 INFO]: Training loss at epoch 9: 0.9331257045269012
[08/28/2025 04:56:43 INFO]: Training loss at epoch 28: 0.6623856872320175
[08/28/2025 04:56:54 INFO]: Training stats: {
    "score": -1.0393188645343854,
    "rmse": 1.0393188645343854
}
[08/28/2025 04:56:54 INFO]: Val stats: {
    "score": -0.8818368719409836,
    "rmse": 0.8818368719409836
}
[08/28/2025 04:56:54 INFO]: Test stats: {
    "score": -1.010549738513269,
    "rmse": 1.010549738513269
}
[08/28/2025 04:56:55 INFO]: New best epoch, val score: -0.9132774106141665
[08/28/2025 04:56:55 INFO]: Saving model to: maddest-Elbert_trial_110/model_best.pth
[08/28/2025 04:56:59 INFO]: New best epoch, val score: -0.8818368719409836
[08/28/2025 04:56:59 INFO]: Saving model to: maddest-Elbert_trial_114/model_best.pth
[08/28/2025 04:57:10 INFO]: New best epoch, val score: -0.8946067672477307
[08/28/2025 04:57:10 INFO]: Saving model to: maddest-Elbert_trial_113/model_best.pth
[08/28/2025 04:57:35 INFO]: Training loss at epoch 10: 0.902937114238739
[08/28/2025 04:57:52 INFO]: Training loss at epoch 6: 1.2111304998397827
[08/28/2025 04:58:14 INFO]: Training loss at epoch 11: 1.0121280252933502
[08/28/2025 04:58:54 INFO]: Training loss at epoch 6: 1.024625539779663
[08/28/2025 04:58:56 INFO]: Training loss at epoch 12: 1.0749779343605042
[08/28/2025 04:59:08 INFO]: Training loss at epoch 33: 1.1550851166248322
[08/28/2025 04:59:19 INFO]: Training loss at epoch 27: 0.7706018686294556
[08/28/2025 04:59:27 INFO]: Training loss at epoch 35: 0.8399303257465363
[08/28/2025 04:59:36 INFO]: Training loss at epoch 13: 1.0955396890640259
[08/28/2025 05:00:17 INFO]: Training loss at epoch 14: 1.1712081730365753
[08/28/2025 05:00:59 INFO]: Training loss at epoch 15: 1.0973046720027924
[08/28/2025 05:01:00 INFO]: Training loss at epoch 27: 0.9636533558368683
[08/28/2025 05:01:28 INFO]: Training loss at epoch 29: 0.8802090883255005
[08/28/2025 05:01:39 INFO]: Training loss at epoch 16: 0.9299126267433167
[08/28/2025 05:02:15 INFO]: Training loss at epoch 43: 1.230544924736023
[08/28/2025 05:02:18 INFO]: Training loss at epoch 17: 0.9508234262466431
[08/28/2025 05:02:50 INFO]: Training loss at epoch 2: 1.3491748571395874
[08/28/2025 05:02:58 INFO]: Training loss at epoch 18: 1.248742699623108
[08/28/2025 05:03:03 INFO]: Training stats: {
    "score": -0.9038561963824339,
    "rmse": 0.9038561963824339
}
[08/28/2025 05:03:03 INFO]: Val stats: {
    "score": -0.959062726859372,
    "rmse": 0.959062726859372
}
[08/28/2025 05:03:03 INFO]: Test stats: {
    "score": -0.99586606687214,
    "rmse": 0.99586606687214
}
[08/28/2025 05:03:12 INFO]: Training loss at epoch 7: 1.2312642335891724
[08/28/2025 05:03:27 INFO]: Training loss at epoch 24: 0.9041841328144073
[08/28/2025 05:03:37 INFO]: Training loss at epoch 19: 0.90495565533638
[08/28/2025 05:03:48 INFO]: Training loss at epoch 23: 1.0562785863876343
[08/28/2025 05:03:53 INFO]: Training stats: {
    "score": -1.0033409382559335,
    "rmse": 1.0033409382559335
}
[08/28/2025 05:03:53 INFO]: Val stats: {
    "score": -0.9417694463715092,
    "rmse": 0.9417694463715092
}
[08/28/2025 05:03:53 INFO]: Test stats: {
    "score": -0.9753911137888662,
    "rmse": 0.9753911137888662
}
[08/28/2025 05:03:55 INFO]: Training loss at epoch 8: 0.8851723372936249
[08/28/2025 05:04:04 INFO]: Training loss at epoch 28: 1.0406231880187988
[08/28/2025 05:04:11 INFO]: Training loss at epoch 1: 1.0783365964889526
[08/28/2025 05:04:15 INFO]: Training loss at epoch 43: 0.8774151802062988
[08/28/2025 05:04:21 INFO]: New best epoch, val score: -0.8821624449482459
[08/28/2025 05:04:21 INFO]: Saving model to: maddest-Elbert_trial_108/model_best.pth
[08/28/2025 05:04:22 INFO]: Training loss at epoch 47: 1.0266293287277222
[08/28/2025 05:04:34 INFO]: Training loss at epoch 20: 0.9446240067481995
[08/28/2025 05:04:45 INFO]: Training loss at epoch 7: 0.8969215750694275
[08/28/2025 05:04:55 INFO]: New best epoch, val score: -0.9080805733579348
[08/28/2025 05:04:55 INFO]: Saving model to: maddest-Elbert_trial_110/model_best.pth
[08/28/2025 05:05:16 INFO]: Training loss at epoch 21: 1.0561557412147522
[08/28/2025 05:05:17 INFO]: Training loss at epoch 48: 0.7615515291690826
[08/28/2025 05:05:42 INFO]: Training loss at epoch 33: 1.065280944108963
[08/28/2025 05:05:47 INFO]: Training loss at epoch 27: 0.8297601938247681
[08/28/2025 05:05:57 INFO]: Training loss at epoch 22: 0.978708028793335
[08/28/2025 05:06:38 INFO]: Training loss at epoch 23: 0.9522528052330017
[08/28/2025 05:07:18 INFO]: Training loss at epoch 24: 1.1449134051799774
[08/28/2025 05:07:20 INFO]: Training loss at epoch 7: 1.0529091358184814
[08/28/2025 05:07:53 INFO]: Training loss at epoch 30: 1.0576099455356598
[08/28/2025 05:08:00 INFO]: Training loss at epoch 25: 1.1972109973430634
[08/28/2025 05:08:40 INFO]: Training loss at epoch 26: 0.9603267312049866
[08/28/2025 05:08:55 INFO]: Training loss at epoch 29: 0.9475555419921875
[08/28/2025 05:08:59 INFO]: Training loss at epoch 34: 1.130830317735672
[08/28/2025 05:09:19 INFO]: Training loss at epoch 27: 0.8146743178367615
[08/28/2025 05:09:21 INFO]: Training loss at epoch 36: 0.8876760601997375
[08/28/2025 05:09:59 INFO]: Training loss at epoch 28: 1.0354329645633698
[08/28/2025 05:10:30 INFO]: Training stats: {
    "score": -0.9493331657465672,
    "rmse": 0.9493331657465672
}
[08/28/2025 05:10:30 INFO]: Val stats: {
    "score": -0.9276877575874248,
    "rmse": 0.9276877575874248
}
[08/28/2025 05:10:30 INFO]: Test stats: {
    "score": -0.9650045346376966,
    "rmse": 0.9650045346376966
}
[08/28/2025 05:10:34 INFO]: Training loss at epoch 8: 0.9920881390571594
[08/28/2025 05:10:39 INFO]: Training loss at epoch 29: 0.7546964436769485
[08/28/2025 05:10:39 INFO]: Training loss at epoch 3: 1.2758779525756836
[08/28/2025 05:10:50 INFO]: Training loss at epoch 44: 1.1944321393966675
[08/28/2025 05:10:53 INFO]: Training stats: {
    "score": -1.0016166009764935,
    "rmse": 1.0016166009764935
}
[08/28/2025 05:10:53 INFO]: Val stats: {
    "score": -0.9262608465606871,
    "rmse": 0.9262608465606871
}
[08/28/2025 05:10:53 INFO]: Test stats: {
    "score": -0.9733660229435279,
    "rmse": 0.9733660229435279
}
[08/28/2025 05:11:33 INFO]: Training loss at epoch 30: 1.1596398055553436
[08/28/2025 05:11:35 INFO]: New best epoch, val score: -0.9118667191131524
[08/28/2025 05:11:35 INFO]: Saving model to: maddest-Elbert_trial_112/model_best.pth
[08/28/2025 05:11:38 INFO]: Training loss at epoch 28: 0.7153081595897675
[08/28/2025 05:11:59 INFO]: Training loss at epoch 9: 0.9372355043888092
[08/28/2025 05:12:07 INFO]: Training loss at epoch 2: 1.38864403963089
[08/28/2025 05:12:14 INFO]: Training loss at epoch 31: 0.7819370627403259
[08/28/2025 05:12:37 INFO]: Training loss at epoch 31: 0.8166497051715851
[08/28/2025 05:12:45 INFO]: Training loss at epoch 44: 1.0353142619132996
[08/28/2025 05:12:48 INFO]: Training loss at epoch 8: 0.9249871075153351
[08/28/2025 05:12:56 INFO]: Training loss at epoch 32: 0.9638243317604065
[08/28/2025 05:13:19 INFO]: Training loss at epoch 24: 0.7895104885101318
[08/28/2025 05:13:30 INFO]: Training loss at epoch 48: 0.8552671670913696
[08/28/2025 05:13:37 INFO]: Training loss at epoch 33: 0.991397887468338
[08/28/2025 05:13:46 INFO]: Running Final Evaluation...
[08/28/2025 05:13:58 INFO]: New best epoch, val score: -0.8795799134918613
[08/28/2025 05:13:58 INFO]: Saving model to: maddest-Elbert_trial_108/model_best.pth
[08/28/2025 05:14:01 INFO]: Training loss at epoch 49: 0.8248527646064758
[08/28/2025 05:14:18 INFO]: Training loss at epoch 34: 1.0283589363098145
[08/28/2025 05:14:42 INFO]: Training stats: {
    "score": -1.0014525461615547,
    "rmse": 1.0014525461615547
}
[08/28/2025 05:14:42 INFO]: Val stats: {
    "score": -0.9058759780392316,
    "rmse": 0.9058759780392316
}
[08/28/2025 05:14:42 INFO]: Test stats: {
    "score": -0.9756240077929255,
    "rmse": 0.9756240077929255
}
[08/28/2025 05:14:54 INFO]: Training loss at epoch 25: 0.8924523591995239
[08/28/2025 05:14:57 INFO]: Training loss at epoch 35: 0.9679746925830841
[08/28/2025 05:15:17 INFO]: Training loss at epoch 30: 0.8117260634899139
[08/28/2025 05:15:32 INFO]: Training loss at epoch 34: 1.1228212118148804
[08/28/2025 05:15:37 INFO]: New best epoch, val score: -0.9058759780392316
[08/28/2025 05:15:37 INFO]: Saving model to: maddest-Elbert_trial_110/model_best.pth
[08/28/2025 05:15:37 INFO]: Training loss at epoch 36: 0.9091513752937317
[08/28/2025 05:16:17 INFO]: Training loss at epoch 37: 1.159682720899582
[08/28/2025 05:16:22 INFO]: Training loss at epoch 28: 0.7449557781219482
[08/28/2025 05:16:22 INFO]: Training loss at epoch 9: 1.0096815824508667
[08/28/2025 05:16:40 INFO]: Training accuracy: {
    "score": -1.0120613830212417,
    "rmse": 1.0120613830212417
}
[08/28/2025 05:16:40 INFO]: Val accuracy: {
    "score": -0.8866964104438184,
    "rmse": 0.8866964104438184
}
[08/28/2025 05:16:40 INFO]: Test accuracy: {
    "score": -0.9840535003883556,
    "rmse": 0.9840535003883556
}
[08/28/2025 05:16:40 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_90",
    "best_epoch": 13,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9840535003883556,
        "rmse": 0.9840535003883556
    },
    "train_stats": {
        "score": -1.0120613830212417,
        "rmse": 1.0120613830212417
    },
    "val_stats": {
        "score": -0.8866964104438184,
        "rmse": 0.8866964104438184
    }
}
[08/28/2025 05:16:40 INFO]: Procewss finished for trial maddest-Elbert_trial_90
[08/28/2025 05:16:40 INFO]: 
_________________________________________________

[08/28/2025 05:16:40 INFO]: train_net_for_optune.py main() running.
[08/28/2025 05:16:40 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 16
  d_ffn_factor: 2.13434035297078
  attention_dropout: 0.16159137071664126
  ffn_dropout: 0.16159137071664126
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.036119286911935e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_115

[08/28/2025 05:16:40 INFO]: This ft_transformer has 0.368 million parameters.
[08/28/2025 05:16:40 INFO]: Training will start at epoch 0.
[08/28/2025 05:16:40 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 05:16:44 INFO]: Training loss at epoch 8: 1.0104106068611145
[08/28/2025 05:16:55 INFO]: Training stats: {
    "score": -0.9794909270158045,
    "rmse": 0.9794909270158045
}
[08/28/2025 05:16:55 INFO]: Val stats: {
    "score": -0.9570838494943604,
    "rmse": 0.9570838494943604
}
[08/28/2025 05:16:55 INFO]: Test stats: {
    "score": -0.9726882827072543,
    "rmse": 0.9726882827072543
}
[08/28/2025 05:16:58 INFO]: Training loss at epoch 38: 0.8689691424369812
[08/28/2025 05:17:23 INFO]: Training loss at epoch 32: 0.6618364304304123
[08/28/2025 05:17:39 INFO]: Training loss at epoch 39: 0.9651578664779663
[08/28/2025 05:17:53 INFO]: Training stats: {
    "score": -1.0036706102642485,
    "rmse": 1.0036706102642485
}
[08/28/2025 05:17:53 INFO]: Val stats: {
    "score": -0.9053294567309197,
    "rmse": 0.9053294567309197
}
[08/28/2025 05:17:53 INFO]: Test stats: {
    "score": -0.9752640845890644,
    "rmse": 0.9752640845890644
}
[08/28/2025 05:17:57 INFO]: Running Final Evaluation...
[08/28/2025 05:18:21 INFO]: Training stats: {
    "score": -1.0058123066881655,
    "rmse": 1.0058123066881655
}
[08/28/2025 05:18:21 INFO]: Val stats: {
    "score": -0.8898031831883422,
    "rmse": 0.8898031831883422
}
[08/28/2025 05:18:21 INFO]: Test stats: {
    "score": -0.9809662753722818,
    "rmse": 0.9809662753722818
}
[08/28/2025 05:18:24 INFO]: Training loss at epoch 0: 1.4048691391944885
[08/28/2025 05:18:30 INFO]: Training loss at epoch 4: 1.0089510679244995
[08/28/2025 05:18:34 INFO]: Training loss at epoch 40: 0.9089604318141937
[08/28/2025 05:18:40 INFO]: Running Final Evaluation...
[08/28/2025 05:18:41 INFO]: New best epoch, val score: -0.8834580518347672
[08/28/2025 05:18:41 INFO]: Saving model to: maddest-Elbert_trial_115/model_best.pth
[08/28/2025 05:18:47 INFO]: Training loss at epoch 35: 0.9913448393344879
[08/28/2025 05:18:55 INFO]: Training accuracy: {
    "score": -1.0393188651716003,
    "rmse": 1.0393188651716003
}
[08/28/2025 05:18:55 INFO]: Val accuracy: {
    "score": -0.8818368719409836,
    "rmse": 0.8818368719409836
}
[08/28/2025 05:18:55 INFO]: Test accuracy: {
    "score": -1.010549738513269,
    "rmse": 1.010549738513269
}
[08/28/2025 05:18:55 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_114",
    "best_epoch": 9,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.010549738513269,
        "rmse": 1.010549738513269
    },
    "train_stats": {
        "score": -1.0393188651716003,
        "rmse": 1.0393188651716003
    },
    "val_stats": {
        "score": -0.8818368719409836,
        "rmse": 0.8818368719409836
    }
}
[08/28/2025 05:18:55 INFO]: Procewss finished for trial maddest-Elbert_trial_114
[08/28/2025 05:18:55 INFO]: 
_________________________________________________

[08/28/2025 05:18:55 INFO]: train_net_for_optune.py main() running.
[08/28/2025 05:18:55 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 1.2718585142303591
  attention_dropout: 0.21545185104227033
  ffn_dropout: 0.21545185104227033
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.1235055406660402e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_116

[08/28/2025 05:18:55 INFO]: This ft_transformer has 12.493 million parameters.
[08/28/2025 05:18:55 INFO]: Training will start at epoch 0.
[08/28/2025 05:18:55 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 05:19:14 INFO]: Training loss at epoch 37: 0.8546960055828094
[08/28/2025 05:19:23 INFO]: Training loss at epoch 45: 1.1300571262836456
[08/28/2025 05:20:05 INFO]: Training loss at epoch 31: 0.7797451615333557
[08/28/2025 05:20:10 INFO]: Training loss at epoch 3: 0.9541641175746918
[08/28/2025 05:20:25 INFO]: Training loss at epoch 1: 1.1285601258277893
[08/28/2025 05:21:01 INFO]: Training accuracy: {
    "score": -1.0326846778869736,
    "rmse": 1.0326846778869736
}
[08/28/2025 05:21:01 INFO]: Val accuracy: {
    "score": -0.8799404312000327,
    "rmse": 0.8799404312000327
}
[08/28/2025 05:21:01 INFO]: Test accuracy: {
    "score": -1.0094226764928043,
    "rmse": 1.0094226764928043
}
[08/28/2025 05:21:01 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_85",
    "best_epoch": 18,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0094226764928043,
        "rmse": 1.0094226764928043
    },
    "train_stats": {
        "score": -1.0326846778869736,
        "rmse": 1.0326846778869736
    },
    "val_stats": {
        "score": -0.8799404312000327,
        "rmse": 0.8799404312000327
    }
}
[08/28/2025 05:21:01 INFO]: Procewss finished for trial maddest-Elbert_trial_85
[08/28/2025 05:21:02 INFO]: 
_________________________________________________

[08/28/2025 05:21:02 INFO]: train_net_for_optune.py main() running.
[08/28/2025 05:21:02 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 1.292322222761714
  attention_dropout: 0.06940821523568749
  ffn_dropout: 0.06940821523568749
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.62766096865164e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_117

[08/28/2025 05:21:02 INFO]: This ft_transformer has 0.824 million parameters.
[08/28/2025 05:21:02 INFO]: Training will start at epoch 0.
[08/28/2025 05:21:02 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 05:22:11 INFO]: Training loss at epoch 33: 0.6213884055614471
[08/28/2025 05:22:22 INFO]: Training loss at epoch 9: 1.184385061264038
[08/28/2025 05:22:22 INFO]: Training loss at epoch 29: 0.9882996082305908
[08/28/2025 05:22:23 INFO]: Training loss at epoch 2: 1.0712714195251465
[08/28/2025 05:22:31 INFO]: Training loss at epoch 0: 1.3151233196258545
[08/28/2025 05:22:39 INFO]: Training loss at epoch 49: 0.9438074827194214
[08/28/2025 05:22:44 INFO]: New best epoch, val score: -0.8874331888705981
[08/28/2025 05:22:44 INFO]: Saving model to: maddest-Elbert_trial_117/model_best.pth
[08/28/2025 05:22:44 INFO]: Training loss at epoch 10: 0.8774208426475525
[08/28/2025 05:22:49 INFO]: Training loss at epoch 25: 0.8532974421977997
[08/28/2025 05:23:40 INFO]: New best epoch, val score: -0.9036195414315076
[08/28/2025 05:23:40 INFO]: Saving model to: maddest-Elbert_trial_110/model_best.pth
[08/28/2025 05:24:12 INFO]: Training loss at epoch 1: 1.0364307761192322
[08/28/2025 05:24:15 INFO]: Training loss at epoch 10: 1.0336499214172363
[08/28/2025 05:24:23 INFO]: Training loss at epoch 3: 1.0608997344970703
[08/28/2025 05:24:27 INFO]: Training loss at epoch 0: 1.3358541131019592
[08/28/2025 05:24:53 INFO]: Training loss at epoch 32: 0.9763277173042297
[08/28/2025 05:25:11 INFO]: New best epoch, val score: -0.8697344028163297
[08/28/2025 05:25:11 INFO]: Saving model to: maddest-Elbert_trial_116/model_best.pth
[08/28/2025 05:25:25 INFO]: Training loss at epoch 35: 0.8711848556995392
[08/28/2025 05:25:34 INFO]: Training stats: {
    "score": -1.012783454768331,
    "rmse": 1.012783454768331
}
[08/28/2025 05:25:34 INFO]: Val stats: {
    "score": -0.8797744952108255,
    "rmse": 0.8797744952108255
}
[08/28/2025 05:25:34 INFO]: Test stats: {
    "score": -1.0008745090746296,
    "rmse": 1.0008745090746296
}
[08/28/2025 05:25:46 INFO]: Training stats: {
    "score": -0.9992330005286202,
    "rmse": 0.9992330005286202
}
[08/28/2025 05:25:46 INFO]: Val stats: {
    "score": -0.9076070286785077,
    "rmse": 0.9076070286785077
}
[08/28/2025 05:25:46 INFO]: Test stats: {
    "score": -0.972799583556286,
    "rmse": 0.972799583556286
}
[08/28/2025 05:25:54 INFO]: Training loss at epoch 2: 0.9765752255916595
[08/28/2025 05:25:57 INFO]: Training stats: {
    "score": -0.9604298418285023,
    "rmse": 0.9604298418285023
}
[08/28/2025 05:25:57 INFO]: Val stats: {
    "score": -0.9051782886763997,
    "rmse": 0.9051782886763997
}
[08/28/2025 05:25:57 INFO]: Test stats: {
    "score": -0.9823687208387815,
    "rmse": 0.9823687208387815
}
[08/28/2025 05:26:16 INFO]: Training loss at epoch 9: 0.9273982644081116
[08/28/2025 05:26:21 INFO]: Training loss at epoch 4: 1.0153635144233704
[08/28/2025 05:26:22 INFO]: Training loss at epoch 26: 0.8734594583511353
[08/28/2025 05:26:22 INFO]: Training loss at epoch 5: 1.1910606622695923
[08/28/2025 05:26:57 INFO]: Training loss at epoch 34: 0.9186719954013824
[08/28/2025 05:27:02 INFO]: Training loss at epoch 29: 0.7585864067077637
[08/28/2025 05:27:34 INFO]: Training loss at epoch 3: 1.1273872256278992
[08/28/2025 05:27:54 INFO]: Training loss at epoch 46: 0.9210259318351746
[08/28/2025 05:28:09 INFO]: Training loss at epoch 4: 0.9979887306690216
[08/28/2025 05:28:21 INFO]: Training loss at epoch 5: 0.8574876189231873
[08/28/2025 05:28:41 INFO]: Training loss at epoch 36: 1.0333643853664398
[08/28/2025 05:29:06 INFO]: New best epoch, val score: -0.8870614634041452
[08/28/2025 05:29:06 INFO]: Saving model to: maddest-Elbert_trial_113/model_best.pth
[08/28/2025 05:29:14 INFO]: Training loss at epoch 38: 0.8377804756164551
[08/28/2025 05:29:18 INFO]: Training loss at epoch 4: 1.147502064704895
[08/28/2025 05:29:29 INFO]: Training stats: {
    "score": -0.977808587845382,
    "rmse": 0.977808587845382
}
[08/28/2025 05:29:29 INFO]: Val stats: {
    "score": -0.9142371864685932,
    "rmse": 0.9142371864685932
}
[08/28/2025 05:29:29 INFO]: Test stats: {
    "score": -0.9659396095496995,
    "rmse": 0.9659396095496995
}
[08/28/2025 05:29:46 INFO]: Training loss at epoch 33: 1.127217411994934
[08/28/2025 05:30:09 INFO]: Training loss at epoch 11: 1.0259990692138672
[08/28/2025 05:30:22 INFO]: Training loss at epoch 6: 0.9017994403839111
[08/28/2025 05:30:41 INFO]: Training stats: {
    "score": -0.8825712451390791,
    "rmse": 0.8825712451390791
}
[08/28/2025 05:30:41 INFO]: Val stats: {
    "score": -0.9053142100539092,
    "rmse": 0.9053142100539092
}
[08/28/2025 05:30:41 INFO]: Test stats: {
    "score": -0.9927966624155206,
    "rmse": 0.9927966624155206
}
[08/28/2025 05:30:47 INFO]: Training loss at epoch 1: 1.1581127047538757
[08/28/2025 05:30:50 INFO]: Training loss at epoch 11: 1.0407927632331848
[08/28/2025 05:30:59 INFO]: Training loss at epoch 5: 0.9185977578163147
[08/28/2025 05:31:48 INFO]: Training loss at epoch 35: 0.8154132664203644
[08/28/2025 05:32:21 INFO]: Training loss at epoch 7: 1.0331159234046936
[08/28/2025 05:32:25 INFO]: Training loss at epoch 26: 1.008655846118927
[08/28/2025 05:32:41 INFO]: Training loss at epoch 6: 0.9982961714267731
[08/28/2025 05:34:10 INFO]: Training loss at epoch 6: 1.2595494389533997
[08/28/2025 05:34:17 INFO]: Training loss at epoch 8: 0.995486855506897
[08/28/2025 05:34:19 INFO]: Training loss at epoch 7: 1.0055572986602783
[08/28/2025 05:34:33 INFO]: Training loss at epoch 34: 0.7907013893127441
[08/28/2025 05:34:56 INFO]: Training loss at epoch 50: 0.8293493390083313
[08/28/2025 05:35:06 INFO]: Training loss at epoch 10: 1.1997281610965729
[08/28/2025 05:35:18 INFO]: Training loss at epoch 36: 0.8206021785736084
[08/28/2025 05:35:58 INFO]: Training loss at epoch 12: 1.1370002031326294
[08/28/2025 05:36:00 INFO]: Training loss at epoch 8: 1.3590078353881836
[08/28/2025 05:36:09 INFO]: Training loss at epoch 5: 0.9858251512050629
[08/28/2025 05:36:15 INFO]: Training loss at epoch 9: 1.137650728225708
[08/28/2025 05:36:27 INFO]: Training loss at epoch 47: 1.0888358056545258
[08/28/2025 05:36:31 INFO]: Training loss at epoch 36: 0.8477133512496948
[08/28/2025 05:36:37 INFO]: Training loss at epoch 30: 0.9960072934627533
[08/28/2025 05:36:59 INFO]: Training stats: {
    "score": -1.0344568060041803,
    "rmse": 1.0344568060041803
}
[08/28/2025 05:36:59 INFO]: Val stats: {
    "score": -1.0400414024047346,
    "rmse": 1.0400414024047346
}
[08/28/2025 05:36:59 INFO]: Test stats: {
    "score": -1.0151174602150363,
    "rmse": 1.0151174602150363
}
[08/28/2025 05:37:00 INFO]: Training loss at epoch 2: 1.0220107436180115
[08/28/2025 05:37:41 INFO]: Training loss at epoch 9: 0.9879656136035919
[08/28/2025 05:37:48 INFO]: Training loss at epoch 27: 0.8714471459388733
[08/28/2025 05:38:18 INFO]: Training stats: {
    "score": -0.9996559362697208,
    "rmse": 0.9996559362697208
}
[08/28/2025 05:38:18 INFO]: Val stats: {
    "score": -0.9484497717594602,
    "rmse": 0.9484497717594602
}
[08/28/2025 05:38:18 INFO]: Test stats: {
    "score": -0.9731953730396734,
    "rmse": 0.9731953730396734
}
[08/28/2025 05:38:33 INFO]: Training loss at epoch 37: 0.9305449426174164
[08/28/2025 05:38:47 INFO]: Training loss at epoch 12: 0.9979620277881622
[08/28/2025 05:38:59 INFO]: Training loss at epoch 10: 0.7974603176116943
[08/28/2025 05:38:59 INFO]: Training loss at epoch 10: 0.7367342859506607
[08/28/2025 05:39:11 INFO]: Training loss at epoch 39: 0.6470242738723755
[08/28/2025 05:39:22 INFO]: Training loss at epoch 35: 0.9032726585865021
[08/28/2025 05:39:43 INFO]: Running Final Evaluation...
[08/28/2025 05:40:00 INFO]: Training loss at epoch 10: 1.0647499561309814
[08/28/2025 05:40:59 INFO]: Training loss at epoch 11: 0.9123502373695374
[08/28/2025 05:41:19 INFO]: Training loss at epoch 37: 0.7651360929012299
[08/28/2025 05:41:19 INFO]: Training loss at epoch 30: 0.6191089004278183
[08/28/2025 05:41:42 INFO]: Training loss at epoch 11: 0.8048976361751556
[08/28/2025 05:41:52 INFO]: Training loss at epoch 13: 1.0300809741020203
[08/28/2025 05:41:56 INFO]: Training loss at epoch 27: 0.9222745299339294
[08/28/2025 05:42:01 INFO]: Training loss at epoch 7: 1.121939867734909
[08/28/2025 05:42:34 INFO]: Training stats: {
    "score": -0.9446669742918029,
    "rmse": 0.9446669742918029
}
[08/28/2025 05:42:34 INFO]: Val stats: {
    "score": -0.8795503194311146,
    "rmse": 0.8795503194311146
}
[08/28/2025 05:42:34 INFO]: Test stats: {
    "score": -0.9804794014132754,
    "rmse": 0.9804794014132754
}
[08/28/2025 05:42:35 INFO]: New best epoch, val score: -0.8738087650781744
[08/28/2025 05:42:35 INFO]: Saving model to: maddest-Elbert_trial_102/model_best.pth
[08/28/2025 05:42:58 INFO]: Training loss at epoch 12: 0.9238276183605194
[08/28/2025 05:43:14 INFO]: Training accuracy: {
    "score": -1.0325419021771935,
    "rmse": 1.0325419021771935
}
[08/28/2025 05:43:14 INFO]: Val accuracy: {
    "score": -0.8831811107264494,
    "rmse": 0.8831811107264494
}
[08/28/2025 05:43:14 INFO]: Test accuracy: {
    "score": -1.0077325409313658,
    "rmse": 1.0077325409313658
}
[08/28/2025 05:43:14 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_94",
    "best_epoch": 6,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0077325409313658,
        "rmse": 1.0077325409313658
    },
    "train_stats": {
        "score": -1.0325419021771935,
        "rmse": 1.0325419021771935
    },
    "val_stats": {
        "score": -0.8831811107264494,
        "rmse": 0.8831811107264494
    }
}
[08/28/2025 05:43:14 INFO]: Procewss finished for trial maddest-Elbert_trial_94
[08/28/2025 05:43:14 INFO]: 
_________________________________________________

[08/28/2025 05:43:14 INFO]: train_net_for_optune.py main() running.
[08/28/2025 05:43:14 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 1.54101824331498
  attention_dropout: 0.22867836858842172
  ffn_dropout: 0.22867836858842172
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.0313064334711948e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_118

[08/28/2025 05:43:14 INFO]: This ft_transformer has 7.777 million parameters.
[08/28/2025 05:43:14 INFO]: Training will start at epoch 0.
[08/28/2025 05:43:14 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 05:43:19 INFO]: Training loss at epoch 3: 1.4189934134483337
[08/28/2025 05:43:22 INFO]: Training loss at epoch 12: 0.9607978165149689
[08/28/2025 05:44:05 INFO]: Training loss at epoch 51: 0.9675572216510773
[08/28/2025 05:44:07 INFO]: Training loss at epoch 6: 1.0806472301483154
[08/28/2025 05:44:09 INFO]: Training loss at epoch 36: 0.952544093132019
[08/28/2025 05:44:41 INFO]: Training loss at epoch 11: 1.044617772102356
[08/28/2025 05:44:56 INFO]: Training loss at epoch 13: 0.9768439531326294
[08/28/2025 05:45:00 INFO]: Training loss at epoch 48: 1.2052821218967438
[08/28/2025 05:45:04 INFO]: Training loss at epoch 13: 1.045115888118744
[08/28/2025 05:45:05 INFO]: New best epoch, val score: -0.8864316622488887
[08/28/2025 05:45:05 INFO]: Saving model to: maddest-Elbert_trial_113/model_best.pth
[08/28/2025 05:45:12 INFO]: Training loss at epoch 37: 0.9448662400245667
[08/28/2025 05:46:05 INFO]: Training loss at epoch 38: 0.727886974811554
[08/28/2025 05:46:45 INFO]: Training loss at epoch 14: 0.9990267753601074
[08/28/2025 05:46:50 INFO]: Training loss at epoch 13: 1.0692921876907349
[08/28/2025 05:46:55 INFO]: Training loss at epoch 14: 1.0650455355644226
[08/28/2025 05:47:13 INFO]: Training loss at epoch 0: 1.4636091589927673
[08/28/2025 05:47:21 INFO]: Training loss at epoch 31: 0.9054076969623566
[08/28/2025 05:47:44 INFO]: Training loss at epoch 14: 0.8802434802055359
[08/28/2025 05:47:48 INFO]: New best epoch, val score: -1.0356662603950126
[08/28/2025 05:47:48 INFO]: Saving model to: maddest-Elbert_trial_118/model_best.pth
[08/28/2025 05:48:30 INFO]: Training loss at epoch 15: 1.0238822102546692
[08/28/2025 05:48:33 INFO]: Training loss at epoch 11: 1.106033205986023
[08/28/2025 05:48:56 INFO]: Training loss at epoch 15: 1.248343139886856
[08/28/2025 05:49:02 INFO]: Training loss at epoch 37: 0.7938444912433624
[08/28/2025 05:49:17 INFO]: Training loss at epoch 28: 0.8355111479759216
[08/28/2025 05:49:38 INFO]: Training loss at epoch 4: 0.8750810623168945
[08/28/2025 05:49:49 INFO]: Training loss at epoch 8: 1.3217751383781433
[08/28/2025 05:50:13 INFO]: Training loss at epoch 16: 0.761047750711441
[08/28/2025 05:50:43 INFO]: New best epoch, val score: -0.9058334702998739
[08/28/2025 05:50:43 INFO]: Saving model to: maddest-Elbert_trial_112/model_best.pth
[08/28/2025 05:50:54 INFO]: Training loss at epoch 16: 1.00375097990036
[08/28/2025 05:50:55 INFO]: Training loss at epoch 39: 0.7025351226329803
[08/28/2025 05:51:25 INFO]: Training loss at epoch 28: 0.9642306864261627
[08/28/2025 05:51:45 INFO]: Training loss at epoch 1: 1.4183663129806519
[08/28/2025 05:51:57 INFO]: Training loss at epoch 17: 1.1996142268180847
[08/28/2025 05:52:03 INFO]: Training loss at epoch 31: 0.8153175413608551
[08/28/2025 05:52:10 INFO]: Training loss at epoch 7: 0.9306636452674866
[08/28/2025 05:52:17 INFO]: New best epoch, val score: -0.8850441767471633
[08/28/2025 05:52:17 INFO]: Saving model to: maddest-Elbert_trial_118/model_best.pth
[08/28/2025 05:52:33 INFO]: Training loss at epoch 40: 0.8604380488395691
[08/28/2025 05:52:34 INFO]: Training stats: {
    "score": -0.8369254206218644,
    "rmse": 0.8369254206218644
}
[08/28/2025 05:52:34 INFO]: Val stats: {
    "score": -0.9938256292906418,
    "rmse": 0.9938256292906418
}
[08/28/2025 05:52:34 INFO]: Test stats: {
    "score": -1.0228462806875982,
    "rmse": 1.0228462806875982
}
[08/28/2025 05:52:54 INFO]: Training loss at epoch 17: 0.8762798607349396
[08/28/2025 05:53:08 INFO]: New best epoch, val score: -0.8855025934376223
[08/28/2025 05:53:08 INFO]: Saving model to: maddest-Elbert_trial_113/model_best.pth
[08/28/2025 05:53:18 INFO]: Training loss at epoch 52: 1.0363911092281342
[08/28/2025 05:53:34 INFO]: Training loss at epoch 49: 0.9778247475624084
[08/28/2025 05:53:36 INFO]: Training loss at epoch 15: 0.9186924993991852
[08/28/2025 05:53:39 INFO]: Training loss at epoch 18: 1.0502980947494507
[08/28/2025 05:53:48 INFO]: Training loss at epoch 38: 1.0712464153766632
[08/28/2025 05:54:13 INFO]: Training loss at epoch 12: 1.3446663618087769
[08/28/2025 05:54:52 INFO]: Training loss at epoch 14: 1.1505059003829956
[08/28/2025 05:54:53 INFO]: Training loss at epoch 18: 1.1382689476013184
[08/28/2025 05:55:08 INFO]: Training loss at epoch 38: 0.773967057466507
[08/28/2025 05:55:21 INFO]: Training loss at epoch 19: 1.0062867403030396
[08/28/2025 05:55:56 INFO]: Training loss at epoch 5: 0.8576565682888031
[08/28/2025 05:55:58 INFO]: Training stats: {
    "score": -0.9992459917022248,
    "rmse": 0.9992459917022248
}
[08/28/2025 05:55:58 INFO]: Val stats: {
    "score": -0.9103794529260619,
    "rmse": 0.9103794529260619
}
[08/28/2025 05:55:58 INFO]: Test stats: {
    "score": -0.9739653185312062,
    "rmse": 0.9739653185312062
}
[08/28/2025 05:56:20 INFO]: Training loss at epoch 2: 1.1062058806419373
[08/28/2025 05:56:26 INFO]: Training stats: {
    "score": -0.9957397936386788,
    "rmse": 0.9957397936386788
}
[08/28/2025 05:56:26 INFO]: Val stats: {
    "score": -0.9289783134785489,
    "rmse": 0.9289783134785489
}
[08/28/2025 05:56:26 INFO]: Test stats: {
    "score": -0.971301577062769,
    "rmse": 0.971301577062769
}
[08/28/2025 05:56:51 INFO]: Training loss at epoch 19: 1.1211398541927338
[08/28/2025 05:57:19 INFO]: Training loss at epoch 40: 0.6381491422653198
[08/28/2025 05:57:35 INFO]: Training stats: {
    "score": -0.9981186539072546,
    "rmse": 0.9981186539072546
}
[08/28/2025 05:57:35 INFO]: Val stats: {
    "score": -0.9308015975492769,
    "rmse": 0.9308015975492769
}
[08/28/2025 05:57:35 INFO]: Test stats: {
    "score": -0.9758350762222966,
    "rmse": 0.9758350762222966
}
[08/28/2025 05:57:37 INFO]: Training loss at epoch 9: 1.2587539553642273
[08/28/2025 05:57:38 INFO]: Training loss at epoch 20: 0.9078653752803802
[08/28/2025 05:57:58 INFO]: Training loss at epoch 12: 0.9989525675773621
[08/28/2025 05:58:06 INFO]: Training loss at epoch 32: 1.0329106450080872
[08/28/2025 05:58:34 INFO]: Training loss at epoch 39: 0.97874316573143
[08/28/2025 05:59:20 INFO]: Training loss at epoch 21: 0.9414935111999512
[08/28/2025 05:59:24 INFO]: Training loss at epoch 16: 0.8978537619113922
[08/28/2025 05:59:35 INFO]: Training loss at epoch 20: 0.8364850282669067
[08/28/2025 06:00:07 INFO]: Training loss at epoch 8: 0.920906126499176
[08/28/2025 06:00:09 INFO]: Training stats: {
    "score": -0.9262611225271027,
    "rmse": 0.9262611225271027
}
[08/28/2025 06:00:09 INFO]: Val stats: {
    "score": -0.9065161187502109,
    "rmse": 0.9065161187502109
}
[08/28/2025 06:00:09 INFO]: Test stats: {
    "score": -0.9654965380576763,
    "rmse": 0.9654965380576763
}
[08/28/2025 06:00:13 INFO]: Training stats: {
    "score": -1.086882914227456,
    "rmse": 1.086882914227456
}
[08/28/2025 06:00:13 INFO]: Val stats: {
    "score": -0.8942086010881298,
    "rmse": 0.8942086010881298
}
[08/28/2025 06:00:13 INFO]: Test stats: {
    "score": -1.061209669669237,
    "rmse": 1.061209669669237
}
[08/28/2025 06:00:42 INFO]: Training loss at epoch 29: 0.9023884534835815
[08/28/2025 06:00:45 INFO]: Training loss at epoch 3: 1.6095998883247375
[08/28/2025 06:00:53 INFO]: Training loss at epoch 29: 0.9823558330535889
[08/28/2025 06:01:00 INFO]: Training loss at epoch 22: 0.846038818359375
[08/28/2025 06:01:01 INFO]: New best epoch, val score: -0.8850908665108888
[08/28/2025 06:01:01 INFO]: Saving model to: maddest-Elbert_trial_113/model_best.pth
[08/28/2025 06:01:07 INFO]: New best epoch, val score: -0.8942086010881298
[08/28/2025 06:01:07 INFO]: Saving model to: maddest-Elbert_trial_112/model_best.pth
[08/28/2025 06:01:32 INFO]: Training loss at epoch 21: 1.3233013153076172
[08/28/2025 06:02:01 INFO]: Training loss at epoch 41: 0.824959009885788
[08/28/2025 06:02:09 INFO]: Training loss at epoch 6: 0.9350725412368774
[08/28/2025 06:02:21 INFO]: Training loss at epoch 53: 0.9615042507648468
[08/28/2025 06:02:27 INFO]: Training loss at epoch 41: 0.8457843065261841
[08/28/2025 06:02:41 INFO]: Training loss at epoch 32: 0.7515222728252411
[08/28/2025 06:02:43 INFO]: Training loss at epoch 23: 0.9232372045516968
[08/28/2025 06:02:48 INFO]: Training loss at epoch 15: 1.083873450756073
[08/28/2025 06:03:31 INFO]: Training loss at epoch 22: 1.0026660859584808
[08/28/2025 06:03:40 INFO]: Training loss at epoch 13: 1.05656099319458
[08/28/2025 06:04:08 INFO]: Training stats: {
    "score": -0.9558407499892356,
    "rmse": 0.9558407499892356
}
[08/28/2025 06:04:08 INFO]: Val stats: {
    "score": -0.9223713360408909,
    "rmse": 0.9223713360408909
}
[08/28/2025 06:04:08 INFO]: Test stats: {
    "score": -0.9712873873903212,
    "rmse": 0.9712873873903212
}
[08/28/2025 06:04:26 INFO]: Training loss at epoch 24: 1.1513740420341492
[08/28/2025 06:04:35 INFO]: Training stats: {
    "score": -0.909211084336371,
    "rmse": 0.909211084336371
}
[08/28/2025 06:04:35 INFO]: Val stats: {
    "score": -0.8867204452398496,
    "rmse": 0.8867204452398496
}
[08/28/2025 06:04:35 INFO]: Test stats: {
    "score": -1.0090195955243013,
    "rmse": 1.0090195955243013
}
[08/28/2025 06:04:58 INFO]: Training loss at epoch 50: 1.0883391499519348
[08/28/2025 06:04:58 INFO]: Training loss at epoch 40: 0.6782426834106445
[08/28/2025 06:04:59 INFO]: Training loss at epoch 39: 0.9618885815143585
[08/28/2025 06:05:16 INFO]: Training loss at epoch 17: 0.8870892226696014
[08/28/2025 06:05:19 INFO]: Training loss at epoch 4: 1.3401326537132263
[08/28/2025 06:05:34 INFO]: Training loss at epoch 23: 1.0621044635772705
[08/28/2025 06:06:10 INFO]: Training loss at epoch 25: 1.1994523108005524
[08/28/2025 06:06:52 INFO]: Training loss at epoch 42: 0.7993561625480652
[08/28/2025 06:07:30 INFO]: Training loss at epoch 13: 0.7352948486804962
[08/28/2025 06:07:33 INFO]: Training loss at epoch 24: 1.0830417275428772
[08/28/2025 06:07:52 INFO]: Training loss at epoch 26: 1.004736602306366
[08/28/2025 06:08:06 INFO]: Training loss at epoch 10: 0.9823934137821198
[08/28/2025 06:08:10 INFO]: Training loss at epoch 9: 1.0540571212768555
[08/28/2025 06:08:21 INFO]: Training stats: {
    "score": -0.9548633756991008,
    "rmse": 0.9548633756991008
}
[08/28/2025 06:08:21 INFO]: Val stats: {
    "score": -0.9564813372139096,
    "rmse": 0.9564813372139096
}
[08/28/2025 06:08:21 INFO]: Test stats: {
    "score": -0.9759364431979676,
    "rmse": 0.9759364431979676
}
[08/28/2025 06:08:32 INFO]: Training loss at epoch 7: 1.3832656145095825
[08/28/2025 06:08:53 INFO]: Training loss at epoch 33: 0.8997863233089447
[08/28/2025 06:09:33 INFO]: Training loss at epoch 25: 0.9220736026763916
[08/28/2025 06:09:34 INFO]: Training loss at epoch 27: 1.133305311203003
[08/28/2025 06:09:48 INFO]: Training loss at epoch 41: 0.9432151317596436
[08/28/2025 06:09:49 INFO]: Training loss at epoch 5: 1.4003477692604065
[08/28/2025 06:10:51 INFO]: Training stats: {
    "score": -1.027159645824064,
    "rmse": 1.027159645824064
}
[08/28/2025 06:10:51 INFO]: Val stats: {
    "score": -0.8857423440068353,
    "rmse": 0.8857423440068353
}
[08/28/2025 06:10:51 INFO]: Test stats: {
    "score": -1.0008455388775384,
    "rmse": 1.0008455388775384
}
[08/28/2025 06:10:54 INFO]: Training loss at epoch 16: 1.284202516078949
[08/28/2025 06:11:08 INFO]: Training loss at epoch 18: 1.2148321270942688
[08/28/2025 06:11:16 INFO]: Training loss at epoch 28: 1.0792025923728943
[08/28/2025 06:11:32 INFO]: Training loss at epoch 54: 1.3510956466197968
[08/28/2025 06:11:32 INFO]: Training loss at epoch 26: 0.8527625799179077
[08/28/2025 06:11:40 INFO]: Training loss at epoch 43: 0.8708319365978241
[08/28/2025 06:12:26 INFO]: Training loss at epoch 42: 1.0646466612815857
[08/28/2025 06:12:58 INFO]: Training loss at epoch 29: 0.8506850898265839
[08/28/2025 06:13:15 INFO]: Training loss at epoch 14: 0.9845410883426666
[08/28/2025 06:13:28 INFO]: Training loss at epoch 33: 0.8908765912055969
[08/28/2025 06:13:31 INFO]: Training loss at epoch 27: 1.274434506893158
[08/28/2025 06:13:32 INFO]: Training loss at epoch 51: 0.8944965600967407
[08/28/2025 06:13:35 INFO]: Training stats: {
    "score": -0.9940687486569584,
    "rmse": 0.9940687486569584
}
[08/28/2025 06:13:35 INFO]: Val stats: {
    "score": -0.9288706623658434,
    "rmse": 0.9288706623658434
}
[08/28/2025 06:13:35 INFO]: Test stats: {
    "score": -0.9717072392803855,
    "rmse": 0.9717072392803855
}
[08/28/2025 06:13:42 INFO]: Training loss at epoch 30: 1.0452936887741089
[08/28/2025 06:14:17 INFO]: Training loss at epoch 6: 1.061519056558609
[08/28/2025 06:14:36 INFO]: Training loss at epoch 42: 0.7582379877567291
[08/28/2025 06:14:47 INFO]: Training loss at epoch 8: 1.1354586482048035
[08/28/2025 06:15:16 INFO]: Training loss at epoch 30: 1.1555622816085815
[08/28/2025 06:15:30 INFO]: Training loss at epoch 28: 0.9167051613330841
[08/28/2025 06:15:56 INFO]: Training loss at epoch 11: 1.045992910861969
[08/28/2025 06:16:01 INFO]: Training loss at epoch 30: 0.7927054166793823
[08/28/2025 06:16:25 INFO]: Training loss at epoch 44: 0.611778199672699
[08/28/2025 06:16:56 INFO]: Training loss at epoch 19: 1.0616028308868408
[08/28/2025 06:16:57 INFO]: Training loss at epoch 14: 1.0676105320453644
[08/28/2025 06:16:57 INFO]: Training loss at epoch 31: 0.8829119801521301
[08/28/2025 06:17:10 INFO]: Running Final Evaluation...
[08/28/2025 06:17:28 INFO]: Training loss at epoch 29: 0.8978090584278107
[08/28/2025 06:17:47 INFO]: Training accuracy: {
    "score": -1.05943602446625,
    "rmse": 1.05943602446625
}
[08/28/2025 06:17:47 INFO]: Val accuracy: {
    "score": -0.8874331888705981,
    "rmse": 0.8874331888705981
}
[08/28/2025 06:17:47 INFO]: Test accuracy: {
    "score": -1.0261580883051764,
    "rmse": 1.0261580883051764
}
[08/28/2025 06:17:47 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_117",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0261580883051764,
        "rmse": 1.0261580883051764
    },
    "train_stats": {
        "score": -1.05943602446625,
        "rmse": 1.05943602446625
    },
    "val_stats": {
        "score": -0.8874331888705981,
        "rmse": 0.8874331888705981
    }
}
[08/28/2025 06:17:47 INFO]: Procewss finished for trial maddest-Elbert_trial_117
[08/28/2025 06:17:47 INFO]: 
_________________________________________________

[08/28/2025 06:17:47 INFO]: train_net_for_optune.py main() running.
[08/28/2025 06:17:47 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 1.8721632992598773
  attention_dropout: 0.21978189052401662
  ffn_dropout: 0.21978189052401662
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.8703734189656436e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_119

[08/28/2025 06:17:47 INFO]: This ft_transformer has 11.500 million parameters.
[08/28/2025 06:17:47 INFO]: Training will start at epoch 0.
[08/28/2025 06:17:47 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 06:18:10 INFO]: Training loss at epoch 40: 1.131889522075653
[08/28/2025 06:18:11 INFO]: Training stats: {
    "score": -0.997215965109733,
    "rmse": 0.997215965109733
}
[08/28/2025 06:18:11 INFO]: Val stats: {
    "score": -0.9285908172260433,
    "rmse": 0.9285908172260433
}
[08/28/2025 06:18:11 INFO]: Test stats: {
    "score": -0.9749383897329191,
    "rmse": 0.9749383897329191
}
[08/28/2025 06:18:43 INFO]: Training loss at epoch 7: 1.2226910591125488
[08/28/2025 06:18:44 INFO]: Training loss at epoch 10: 0.9541943669319153
[08/28/2025 06:18:49 INFO]: Training loss at epoch 17: 1.1480774283409119
[08/28/2025 06:18:53 INFO]: Training stats: {
    "score": -0.9984350138708582,
    "rmse": 0.9984350138708582
}
[08/28/2025 06:18:53 INFO]: Val stats: {
    "score": -0.9459876003536879,
    "rmse": 0.9459876003536879
}
[08/28/2025 06:18:53 INFO]: Test stats: {
    "score": -0.9753002054285842,
    "rmse": 0.9753002054285842
}
[08/28/2025 06:19:21 INFO]: Training loss at epoch 43: 0.9837373793125153
[08/28/2025 06:19:29 INFO]: Training loss at epoch 34: 1.0894092321395874
[08/28/2025 06:20:08 INFO]: Training loss at epoch 30: 0.926244705915451
[08/28/2025 06:20:32 INFO]: Training loss at epoch 55: 1.1141225397586823
[08/28/2025 06:20:58 INFO]: Training loss at epoch 9: 0.7948804497718811
[08/28/2025 06:21:08 INFO]: Training loss at epoch 45: 0.7282231152057648
[08/28/2025 06:21:35 INFO]: Running Final Evaluation...
[08/28/2025 06:21:53 INFO]: Training loss at epoch 52: 0.9780948460102081
[08/28/2025 06:22:04 INFO]: Training loss at epoch 31: 1.560963898897171
[08/28/2025 06:22:16 INFO]: Training loss at epoch 43: 0.7697705924510956
[08/28/2025 06:22:20 INFO]: Running Final Evaluation...
[08/28/2025 06:22:35 INFO]: Training loss at epoch 15: 0.8489414155483246
[08/28/2025 06:22:51 INFO]: Training loss at epoch 0: 1.409635841846466
[08/28/2025 06:23:03 INFO]: Training accuracy: {
    "score": -1.0380138801469383,
    "rmse": 1.0380138801469383
}
[08/28/2025 06:23:03 INFO]: Val accuracy: {
    "score": -0.8834580518347672,
    "rmse": 0.8834580518347672
}
[08/28/2025 06:23:03 INFO]: Test accuracy: {
    "score": -1.0130062925999508,
    "rmse": 1.0130062925999508
}
[08/28/2025 06:23:03 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_115",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0130062925999508,
        "rmse": 1.0130062925999508
    },
    "train_stats": {
        "score": -1.0380138801469383,
        "rmse": 1.0380138801469383
    },
    "val_stats": {
        "score": -0.8834580518347672,
        "rmse": 0.8834580518347672
    }
}
[08/28/2025 06:23:03 INFO]: Procewss finished for trial maddest-Elbert_trial_115
[08/28/2025 06:23:03 INFO]: 
_________________________________________________

[08/28/2025 06:23:03 INFO]: train_net_for_optune.py main() running.
[08/28/2025 06:23:03 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 8
  d_ffn_factor: 0.8938302107952341
  attention_dropout: 0.13231792685372834
  ffn_dropout: 0.13231792685372834
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.0840009304531178e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_120

[08/28/2025 06:23:03 INFO]: This ft_transformer has 6.060 million parameters.
[08/28/2025 06:23:03 INFO]: Training will start at epoch 0.
[08/28/2025 06:23:03 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 06:23:04 INFO]: Training loss at epoch 31: 0.8173148930072784
[08/28/2025 06:23:04 INFO]: Training stats: {
    "score": -0.9878047100670686,
    "rmse": 0.9878047100670686
}
[08/28/2025 06:23:04 INFO]: Val stats: {
    "score": -0.9056971023786686,
    "rmse": 0.9056971023786686
}
[08/28/2025 06:23:04 INFO]: Test stats: {
    "score": -0.9766738555119798,
    "rmse": 0.9766738555119798
}
[08/28/2025 06:23:07 INFO]: Training loss at epoch 8: 1.2900854349136353
[08/28/2025 06:23:32 INFO]: New best epoch, val score: -0.8959377060277774
[08/28/2025 06:23:32 INFO]: Saving model to: maddest-Elbert_trial_119/model_best.pth
[08/28/2025 06:23:33 INFO]: Training loss at epoch 12: 1.5236011743545532
[08/28/2025 06:24:02 INFO]: Training loss at epoch 34: 0.7392154932022095
[08/28/2025 06:24:04 INFO]: Training loss at epoch 44: 0.9724047183990479
[08/28/2025 06:24:37 INFO]: Training loss at epoch 20: 0.958035409450531
[08/28/2025 06:24:49 INFO]: Training accuracy: {
    "score": -1.037471825721035,
    "rmse": 1.037471825721035
}
[08/28/2025 06:24:49 INFO]: Val accuracy: {
    "score": -0.880656577482466,
    "rmse": 0.880656577482466
}
[08/28/2025 06:24:49 INFO]: Test accuracy: {
    "score": -1.009745045323761,
    "rmse": 1.009745045323761
}
[08/28/2025 06:24:49 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_84",
    "best_epoch": 24,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.009745045323761,
        "rmse": 1.009745045323761
    },
    "train_stats": {
        "score": -1.037471825721035,
        "rmse": 1.037471825721035
    },
    "val_stats": {
        "score": -0.880656577482466,
        "rmse": 0.880656577482466
    }
}
[08/28/2025 06:24:49 INFO]: Procewss finished for trial maddest-Elbert_trial_84
[08/28/2025 06:24:49 INFO]: 
_________________________________________________

[08/28/2025 06:24:49 INFO]: train_net_for_optune.py main() running.
[08/28/2025 06:24:49 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 0.8706860962497323
  attention_dropout: 0.1320417528606392
  ffn_dropout: 0.1320417528606392
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.3749773298779139e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_121

[08/28/2025 06:24:50 INFO]: This ft_transformer has 10.592 million parameters.
[08/28/2025 06:24:50 INFO]: Training will start at epoch 0.
[08/28/2025 06:24:50 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 06:25:52 INFO]: Training loss at epoch 46: 0.9035094976425171
[08/28/2025 06:25:55 INFO]: Training loss at epoch 0: 1.1037104725837708
[08/28/2025 06:26:14 INFO]: Training loss at epoch 15: 0.7963870465755463
[08/28/2025 06:26:18 INFO]: New best epoch, val score: -1.0033915830975824
[08/28/2025 06:26:18 INFO]: Saving model to: maddest-Elbert_trial_120/model_best.pth
[08/28/2025 06:26:34 INFO]: Training loss at epoch 11: 1.0598925948143005
[08/28/2025 06:26:41 INFO]: Training loss at epoch 18: 1.1175122261047363
[08/28/2025 06:27:12 INFO]: Training loss at epoch 31: 0.8070532977581024
[08/28/2025 06:27:20 INFO]: New best epoch, val score: -0.8792998710802057
[08/28/2025 06:27:20 INFO]: Saving model to: maddest-Elbert_trial_109/model_best.pth
[08/28/2025 06:27:33 INFO]: Training loss at epoch 9: 1.3359381556510925
[08/28/2025 06:27:50 INFO]: Training loss at epoch 41: 0.9826496541500092
[08/28/2025 06:28:38 INFO]: Training loss at epoch 1: 1.1109384298324585
[08/28/2025 06:28:44 INFO]: Training loss at epoch 45: 0.7182452082633972
[08/28/2025 06:28:55 INFO]: Training loss at epoch 0: 1.0047079920768738
[08/28/2025 06:29:05 INFO]: Training stats: {
    "score": -1.0844590797286677,
    "rmse": 1.0844590797286677
}
[08/28/2025 06:29:05 INFO]: Val stats: {
    "score": -1.1212231503430747,
    "rmse": 1.1212231503430747
}
[08/28/2025 06:29:05 INFO]: Test stats: {
    "score": -1.074986923232235,
    "rmse": 1.074986923232235
}
[08/28/2025 06:29:08 INFO]: Training loss at epoch 1: 1.1217790246009827
[08/28/2025 06:29:15 INFO]: Training loss at epoch 10: 1.2911905944347382
[08/28/2025 06:29:28 INFO]: New best epoch, val score: -0.9382884574999207
[08/28/2025 06:29:28 INFO]: Saving model to: maddest-Elbert_trial_121/model_best.pth
[08/28/2025 06:29:31 INFO]: New best epoch, val score: -0.889505099495716
[08/28/2025 06:29:31 INFO]: Saving model to: maddest-Elbert_trial_120/model_best.pth
[08/28/2025 06:29:57 INFO]: Training loss at epoch 35: 1.002209335565567
[08/28/2025 06:30:15 INFO]: Training loss at epoch 53: 1.0069786310195923
[08/28/2025 06:30:20 INFO]: Training loss at epoch 21: 1.0513762831687927
[08/28/2025 06:30:34 INFO]: Training loss at epoch 47: 0.6020882725715637
[08/28/2025 06:31:16 INFO]: Training loss at epoch 13: 1.6397368907928467
[08/28/2025 06:31:58 INFO]: Training loss at epoch 16: 0.8691534698009491
[08/28/2025 06:32:02 INFO]: Training loss at epoch 44: 1.0426690578460693
[08/28/2025 06:32:24 INFO]: Training loss at epoch 2: 1.0620068311691284
[08/28/2025 06:32:27 INFO]: Training loss at epoch 32: 0.7929195463657379
[08/28/2025 06:32:47 INFO]: New best epoch, val score: -0.8849374813414611
[08/28/2025 06:32:47 INFO]: Saving model to: maddest-Elbert_trial_120/model_best.pth
[08/28/2025 06:33:10 INFO]: Running Final Evaluation...
[08/28/2025 06:33:27 INFO]: Training loss at epoch 46: 0.8411827087402344
[08/28/2025 06:33:31 INFO]: Training loss at epoch 10: 1.2052903175354004
[08/28/2025 06:33:36 INFO]: Training loss at epoch 1: 1.2472448945045471
[08/28/2025 06:34:08 INFO]: New best epoch, val score: -0.8751123742015365
[08/28/2025 06:34:08 INFO]: Saving model to: maddest-Elbert_trial_121/model_best.pth
[08/28/2025 06:34:24 INFO]: Training loss at epoch 12: 0.9358753859996796
[08/28/2025 06:34:25 INFO]: Training loss at epoch 2: 1.0809032022953033
[08/28/2025 06:34:30 INFO]: Training loss at epoch 35: 0.8505455851554871
[08/28/2025 06:34:31 INFO]: Training loss at epoch 19: 0.770970419049263
[08/28/2025 06:35:05 INFO]: New best epoch, val score: -0.873363496832783
[08/28/2025 06:35:05 INFO]: Saving model to: maddest-Elbert_trial_119/model_best.pth
[08/28/2025 06:35:14 INFO]: Training loss at epoch 48: 0.7803134322166443
[08/28/2025 06:35:25 INFO]: Training loss at epoch 11: 1.045232355594635
[08/28/2025 06:35:34 INFO]: Training loss at epoch 16: 1.0190003514289856
[08/28/2025 06:35:36 INFO]: Training loss at epoch 3: 1.0924237370491028
[08/28/2025 06:36:05 INFO]: Training loss at epoch 22: 0.9963816702365875
[08/28/2025 06:36:33 INFO]: Training accuracy: {
    "score": -1.0204080144117706,
    "rmse": 1.0204080144117706
}
[08/28/2025 06:36:33 INFO]: Val accuracy: {
    "score": -0.8680231416094331,
    "rmse": 0.8680231416094331
}
[08/28/2025 06:36:33 INFO]: Test accuracy: {
    "score": -1.0150812277912957,
    "rmse": 1.0150812277912957
}
[08/28/2025 06:36:33 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_92",
    "best_epoch": 13,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0150812277912957,
        "rmse": 1.0150812277912957
    },
    "train_stats": {
        "score": -1.0204080144117706,
        "rmse": 1.0204080144117706
    },
    "val_stats": {
        "score": -0.8680231416094331,
        "rmse": 0.8680231416094331
    }
}
[08/28/2025 06:36:33 INFO]: Procewss finished for trial maddest-Elbert_trial_92
[08/28/2025 06:36:33 INFO]: 
_________________________________________________

[08/28/2025 06:36:33 INFO]: train_net_for_optune.py main() running.
[08/28/2025 06:36:33 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 1.5175722377887564
  attention_dropout: 0.16485039324578482
  ffn_dropout: 0.16485039324578482
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.0713329924604117e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_122

[08/28/2025 06:36:33 INFO]: This ft_transformer has 7.715 million parameters.
[08/28/2025 06:36:33 INFO]: Training will start at epoch 0.
[08/28/2025 06:36:33 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 06:37:09 INFO]: Training stats: {
    "score": -1.0004362669911577,
    "rmse": 1.0004362669911577
}
[08/28/2025 06:37:09 INFO]: Val stats: {
    "score": -0.951226209876385,
    "rmse": 0.951226209876385
}
[08/28/2025 06:37:09 INFO]: Test stats: {
    "score": -0.9757034305419215,
    "rmse": 0.9757034305419215
}
[08/28/2025 06:37:30 INFO]: Training loss at epoch 42: 0.8280633389949799
[08/28/2025 06:37:55 INFO]: Training loss at epoch 11: 0.923295795917511
[08/28/2025 06:38:07 INFO]: Training loss at epoch 47: 0.8981600105762482
[08/28/2025 06:38:12 INFO]: Training loss at epoch 2: 0.9816024601459503
[08/28/2025 06:38:24 INFO]: Training loss at epoch 32: 0.6934942901134491
[08/28/2025 06:38:36 INFO]: Training loss at epoch 54: 0.9591621160507202
[08/28/2025 06:38:45 INFO]: New best epoch, val score: -0.8613564224430869
[08/28/2025 06:38:45 INFO]: Saving model to: maddest-Elbert_trial_121/model_best.pth
[08/28/2025 06:38:49 INFO]: Training loss at epoch 4: 1.2618255019187927
[08/28/2025 06:38:53 INFO]: Training loss at epoch 14: 1.1650180518627167
[08/28/2025 06:39:40 INFO]: Training loss at epoch 0: 1.158041089773178
[08/28/2025 06:39:54 INFO]: Training loss at epoch 49: 0.6744533181190491
[08/28/2025 06:40:05 INFO]: New best epoch, val score: -0.963066722758387
[08/28/2025 06:40:05 INFO]: Saving model to: maddest-Elbert_trial_122/model_best.pth
[08/28/2025 06:40:10 INFO]: Training loss at epoch 3: 1.2996838688850403
[08/28/2025 06:40:27 INFO]: Training loss at epoch 36: 0.7806384563446045
[08/28/2025 06:40:50 INFO]: New best epoch, val score: -0.8724194445642698
[08/28/2025 06:40:50 INFO]: Saving model to: maddest-Elbert_trial_119/model_best.pth
[08/28/2025 06:41:20 INFO]: Training loss at epoch 17: 0.9853580892086029
[08/28/2025 06:41:28 INFO]: Training stats: {
    "score": -0.8794050786701748,
    "rmse": 0.8794050786701748
}
[08/28/2025 06:41:28 INFO]: Val stats: {
    "score": -0.9829812056899008,
    "rmse": 0.9829812056899008
}
[08/28/2025 06:41:28 INFO]: Test stats: {
    "score": -1.1358886102463934,
    "rmse": 1.1358886102463934
}
[08/28/2025 06:41:32 INFO]: Training loss at epoch 12: 0.9247217774391174
[08/28/2025 06:41:43 INFO]: Training loss at epoch 33: 0.9005150496959686
[08/28/2025 06:41:46 INFO]: Training loss at epoch 23: 1.124975323677063
[08/28/2025 06:42:01 INFO]: Training loss at epoch 5: 1.0398702025413513
[08/28/2025 06:42:11 INFO]: Training loss at epoch 13: 0.9780628085136414
[08/28/2025 06:42:21 INFO]: Training loss at epoch 12: 1.3225995600223541
[08/28/2025 06:42:48 INFO]: Training loss at epoch 48: 0.8897621035575867
[08/28/2025 06:42:49 INFO]: Training loss at epoch 3: 0.9853292107582092
[08/28/2025 06:43:14 INFO]: Training loss at epoch 1: 0.9648429155349731
[08/28/2025 06:43:39 INFO]: New best epoch, val score: -0.8861927183964751
[08/28/2025 06:43:39 INFO]: Saving model to: maddest-Elbert_trial_122/model_best.pth
[08/28/2025 06:44:52 INFO]: Training loss at epoch 17: 0.8818847239017487
[08/28/2025 06:45:00 INFO]: Training loss at epoch 20: 0.8836294710636139
[08/28/2025 06:45:03 INFO]: Training loss at epoch 36: 0.7654080986976624
[08/28/2025 06:45:16 INFO]: Training loss at epoch 6: 1.0423700213432312
[08/28/2025 06:45:57 INFO]: Training loss at epoch 4: 1.1546186804771423
[08/28/2025 06:46:10 INFO]: Training loss at epoch 50: 0.8027054369449615
[08/28/2025 06:46:32 INFO]: Training loss at epoch 15: 1.0166394412517548
[08/28/2025 06:46:46 INFO]: Training loss at epoch 2: 0.9207019209861755
[08/28/2025 06:46:48 INFO]: Training loss at epoch 13: 1.210925579071045
[08/28/2025 06:46:57 INFO]: Training loss at epoch 55: 0.9528071582317352
[08/28/2025 06:47:11 INFO]: Training loss at epoch 43: 0.9907855987548828
[08/28/2025 06:47:27 INFO]: Training loss at epoch 4: 0.9397386610507965
[08/28/2025 06:47:31 INFO]: Training loss at epoch 49: 0.8598751425743103
[08/28/2025 06:47:32 INFO]: Training loss at epoch 24: 0.8529112935066223
[08/28/2025 06:47:42 INFO]: Training loss at epoch 13: 0.8351683020591736
[08/28/2025 06:47:55 INFO]: Running Final Evaluation...
[08/28/2025 06:48:28 INFO]: Training loss at epoch 7: 1.2276358008384705
[08/28/2025 06:49:05 INFO]: Training stats: {
    "score": -0.880484650606172,
    "rmse": 0.880484650606172
}
[08/28/2025 06:49:05 INFO]: Val stats: {
    "score": -0.9206613817284303,
    "rmse": 0.9206613817284303
}
[08/28/2025 06:49:05 INFO]: Test stats: {
    "score": -0.9632793527270793,
    "rmse": 0.9632793527270793
}
[08/28/2025 06:49:34 INFO]: Training loss at epoch 33: 0.6528276801109314
[08/28/2025 06:50:01 INFO]: Training loss at epoch 14: 0.905044674873352
[08/28/2025 06:50:16 INFO]: Training loss at epoch 3: 1.045598328113556
[08/28/2025 06:50:40 INFO]: Training loss at epoch 18: 1.0186340808868408
[08/28/2025 06:50:47 INFO]: Training accuracy: {
    "score": -1.0087398871897533,
    "rmse": 1.0087398871897533
}
[08/28/2025 06:50:47 INFO]: Val accuracy: {
    "score": -0.8919558080294512,
    "rmse": 0.8919558080294512
}
[08/28/2025 06:50:47 INFO]: Test accuracy: {
    "score": -0.9822070855919566,
    "rmse": 0.9822070855919566
}
[08/28/2025 06:50:47 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_89",
    "best_epoch": 24,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9822070855919566,
        "rmse": 0.9822070855919566
    },
    "train_stats": {
        "score": -1.0087398871897533,
        "rmse": 1.0087398871897533
    },
    "val_stats": {
        "score": -0.8919558080294512,
        "rmse": 0.8919558080294512
    }
}
[08/28/2025 06:50:47 INFO]: Procewss finished for trial maddest-Elbert_trial_89
[08/28/2025 06:50:47 INFO]: 
_________________________________________________

[08/28/2025 06:50:47 INFO]: train_net_for_optune.py main() running.
[08/28/2025 06:50:47 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 1.5327809062876976
  attention_dropout: 0.22300403721178275
  ffn_dropout: 0.22300403721178275
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00025164218359853677
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_123

[08/28/2025 06:50:47 INFO]: This ft_transformer has 7.756 million parameters.
[08/28/2025 06:50:47 INFO]: Training will start at epoch 0.
[08/28/2025 06:50:47 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 06:50:49 INFO]: Training loss at epoch 51: 0.5834407210350037
[08/28/2025 06:50:55 INFO]: Training loss at epoch 37: 0.9366689622402191
[08/28/2025 06:51:02 INFO]: Training loss at epoch 34: 1.0874130427837372
[08/28/2025 06:51:17 INFO]: Training loss at epoch 14: 1.2823655605316162
[08/28/2025 06:51:22 INFO]: Running Final Evaluation...
[08/28/2025 06:51:41 INFO]: Training loss at epoch 8: 1.0496392250061035
[08/28/2025 06:51:41 INFO]: Training loss at epoch 5: 0.9801439344882965
[08/28/2025 06:52:03 INFO]: Training loss at epoch 5: 1.165170669555664
[08/28/2025 06:52:50 INFO]: Training loss at epoch 21: 0.9753613770008087
[08/28/2025 06:53:04 INFO]: Training accuracy: {
    "score": -0.9746002746574015,
    "rmse": 0.9746002746574015
}
[08/28/2025 06:53:04 INFO]: Val accuracy: {
    "score": -0.8878428355954713,
    "rmse": 0.8878428355954713
}
[08/28/2025 06:53:04 INFO]: Test accuracy: {
    "score": -1.0161315494291427,
    "rmse": 1.0161315494291427
}
[08/28/2025 06:53:04 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_105",
    "best_epoch": 20,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0161315494291427,
        "rmse": 1.0161315494291427
    },
    "train_stats": {
        "score": -0.9746002746574015,
        "rmse": 0.9746002746574015
    },
    "val_stats": {
        "score": -0.8878428355954713,
        "rmse": 0.8878428355954713
    }
}
[08/28/2025 06:53:04 INFO]: Procewss finished for trial maddest-Elbert_trial_105
[08/28/2025 06:53:04 INFO]: 
_________________________________________________

[08/28/2025 06:53:04 INFO]: train_net_for_optune.py main() running.
[08/28/2025 06:53:04 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 8
  d_ffn_factor: 1.532601189295777
  attention_dropout: 0.18367761509764405
  ffn_dropout: 0.18367761509764405
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0001772793524985203
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_124

[08/28/2025 06:53:04 INFO]: This ft_transformer has 7.756 million parameters.
[08/28/2025 06:53:04 INFO]: Training will start at epoch 0.
[08/28/2025 06:53:04 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 06:53:17 INFO]: Training loss at epoch 25: 1.005780816078186
[08/28/2025 06:53:48 INFO]: Training loss at epoch 50: 0.7705411911010742
[08/28/2025 06:53:50 INFO]: Training loss at epoch 4: 1.2415737509727478
[08/28/2025 06:53:52 INFO]: Training loss at epoch 14: 0.9165971279144287
[08/28/2025 06:53:56 INFO]: Training loss at epoch 0: 1.1268659830093384
[08/28/2025 06:54:09 INFO]: Training loss at epoch 18: 1.0656462907791138
[08/28/2025 06:54:11 INFO]: Training loss at epoch 16: 0.9884008169174194
[08/28/2025 06:54:22 INFO]: New best epoch, val score: -0.895752326905748
[08/28/2025 06:54:22 INFO]: Saving model to: maddest-Elbert_trial_123/model_best.pth
[08/28/2025 06:54:58 INFO]: Training loss at epoch 9: 0.9393301904201508
[08/28/2025 06:55:37 INFO]: Training loss at epoch 37: 0.6283871531486511
[08/28/2025 06:55:47 INFO]: Training loss at epoch 15: 1.1234756112098694
[08/28/2025 06:56:04 INFO]: Training stats: {
    "score": -1.0030713748980706,
    "rmse": 1.0030713748980706
}
[08/28/2025 06:56:04 INFO]: Val stats: {
    "score": -0.9596319776524168,
    "rmse": 0.9596319776524168
}
[08/28/2025 06:56:04 INFO]: Test stats: {
    "score": -0.9840930169314538,
    "rmse": 0.9840930169314538
}
[08/28/2025 06:56:19 INFO]: New best epoch, val score: -0.8790546626056779
[08/28/2025 06:56:19 INFO]: Saving model to: maddest-Elbert_trial_118/model_best.pth
[08/28/2025 06:56:29 INFO]: Training loss at epoch 0: 1.171189546585083
[08/28/2025 06:56:44 INFO]: Training loss at epoch 6: 1.0158722698688507
[08/28/2025 06:56:55 INFO]: Training loss at epoch 44: 0.8327772617340088
[08/28/2025 06:56:57 INFO]: New best epoch, val score: -0.9530678843392573
[08/28/2025 06:56:57 INFO]: Saving model to: maddest-Elbert_trial_124/model_best.pth
[08/28/2025 06:57:26 INFO]: Training loss at epoch 5: 1.0338106751441956
[08/28/2025 06:57:33 INFO]: Training loss at epoch 6: 0.9426676332950592
[08/28/2025 06:57:33 INFO]: Training loss at epoch 1: 2.4544989466667175
[08/28/2025 06:57:56 INFO]: Training loss at epoch 15: 1.1174699068069458
[08/28/2025 06:58:37 INFO]: Training loss at epoch 51: 0.6711270362138748
[08/28/2025 06:59:07 INFO]: Training loss at epoch 26: 1.0213149785995483
[08/28/2025 06:59:22 INFO]: Training loss at epoch 10: 1.3513736426830292
[08/28/2025 07:00:07 INFO]: Training loss at epoch 15: 0.9786901473999023
[08/28/2025 07:00:09 INFO]: Training loss at epoch 19: 1.1025176346302032
[08/28/2025 07:00:18 INFO]: Training loss at epoch 16: 0.9035595655441284
[08/28/2025 07:00:27 INFO]: Training loss at epoch 1: 1.6416298151016235
[08/28/2025 07:00:32 INFO]: Training loss at epoch 35: 1.0684099793434143
[08/28/2025 07:00:49 INFO]: Training loss at epoch 22: 0.8128203451633453
[08/28/2025 07:00:53 INFO]: New best epoch, val score: -0.8770913548560307
[08/28/2025 07:00:53 INFO]: Saving model to: maddest-Elbert_trial_118/model_best.pth
[08/28/2025 07:00:56 INFO]: New best epoch, val score: -0.8812893810056801
[08/28/2025 07:00:56 INFO]: Saving model to: maddest-Elbert_trial_124/model_best.pth
[08/28/2025 07:00:58 INFO]: Training loss at epoch 34: 0.6547380983829498
[08/28/2025 07:01:05 INFO]: Training loss at epoch 6: 1.0603410005569458
[08/28/2025 07:01:14 INFO]: Training loss at epoch 2: 2.0904738903045654
[08/28/2025 07:01:32 INFO]: Training loss at epoch 7: 0.944974958896637
[08/28/2025 07:01:40 INFO]: Training loss at epoch 38: 0.645981103181839
[08/28/2025 07:02:07 INFO]: Training loss at epoch 17: 0.8588737845420837
[08/28/2025 07:02:48 INFO]: Training loss at epoch 11: 0.895494669675827
[08/28/2025 07:03:04 INFO]: New best epoch, val score: -0.8810301799848393
[08/28/2025 07:03:04 INFO]: Saving model to: maddest-Elbert_trial_112/model_best.pth
[08/28/2025 07:03:34 INFO]: Training loss at epoch 52: 0.9263975024223328
[08/28/2025 07:03:34 INFO]: Training stats: {
    "score": -0.980734802849298,
    "rmse": 0.980734802849298
}
[08/28/2025 07:03:34 INFO]: Val stats: {
    "score": -0.9025455885018018,
    "rmse": 0.9025455885018018
}
[08/28/2025 07:03:34 INFO]: Test stats: {
    "score": -0.9720781017879461,
    "rmse": 0.9720781017879461
}
[08/28/2025 07:03:36 INFO]: Training loss at epoch 7: 0.9801490306854248
[08/28/2025 07:03:49 INFO]: Training loss at epoch 19: 1.0218109488487244
[08/28/2025 07:04:10 INFO]: Running Final Evaluation...
[08/28/2025 07:04:32 INFO]: Training loss at epoch 2: 1.1129106879234314
[08/28/2025 07:04:50 INFO]: Training loss at epoch 7: 0.9877241849899292
[08/28/2025 07:05:02 INFO]: Training loss at epoch 3: 1.2299050688743591
[08/28/2025 07:05:02 INFO]: Training loss at epoch 17: 1.3513751029968262
[08/28/2025 07:05:12 INFO]: Training loss at epoch 27: 1.0690873265266418
[08/28/2025 07:05:37 INFO]: New best epoch, val score: -0.876909278878691
[08/28/2025 07:05:37 INFO]: Saving model to: maddest-Elbert_trial_118/model_best.pth
[08/28/2025 07:05:55 INFO]: Training accuracy: {
    "score": -1.001112258208811,
    "rmse": 1.001112258208811
}
[08/28/2025 07:05:55 INFO]: Val accuracy: {
    "score": -0.8819363924056426,
    "rmse": 0.8819363924056426
}
[08/28/2025 07:05:55 INFO]: Test accuracy: {
    "score": -1.00228267270175,
    "rmse": 1.00228267270175
}
[08/28/2025 07:05:55 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_106",
    "best_epoch": 21,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.00228267270175,
        "rmse": 1.00228267270175
    },
    "train_stats": {
        "score": -1.001112258208811,
        "rmse": 1.001112258208811
    },
    "val_stats": {
        "score": -0.8819363924056426,
        "rmse": 0.8819363924056426
    }
}
[08/28/2025 07:05:55 INFO]: Procewss finished for trial maddest-Elbert_trial_106
[08/28/2025 07:05:55 INFO]: 
_________________________________________________

[08/28/2025 07:05:55 INFO]: train_net_for_optune.py main() running.
[08/28/2025 07:05:55 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 1.5369273753252763
  attention_dropout: 0.20646051896881593
  ffn_dropout: 0.20646051896881593
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.0859183081886344e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_125

[08/28/2025 07:05:55 INFO]: This ft_transformer has 10.318 million parameters.
[08/28/2025 07:05:55 INFO]: Training will start at epoch 0.
[08/28/2025 07:05:55 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 07:06:13 INFO]: Training loss at epoch 16: 1.0162863731384277
[08/28/2025 07:06:15 INFO]: Training loss at epoch 12: 1.0364875197410583
[08/28/2025 07:06:34 INFO]: Training loss at epoch 8: 1.0646154284477234
[08/28/2025 07:06:35 INFO]: Training loss at epoch 38: 0.7762749195098877
[08/28/2025 07:06:45 INFO]: Training loss at epoch 16: 0.9769761562347412
[08/28/2025 07:07:10 INFO]: Training loss at epoch 45: 1.2090646028518677
[08/28/2025 07:07:11 INFO]: Training stats: {
    "score": -0.9887753545740695,
    "rmse": 0.9887753545740695
}
[08/28/2025 07:07:11 INFO]: Val stats: {
    "score": -1.0087356710274438,
    "rmse": 1.0087356710274438
}
[08/28/2025 07:07:11 INFO]: Test stats: {
    "score": -1.0080832084773277,
    "rmse": 1.0080832084773277
}
[08/28/2025 07:08:20 INFO]: Running Final Evaluation...
[08/28/2025 07:08:33 INFO]: Training loss at epoch 3: 1.4265325665473938
[08/28/2025 07:08:35 INFO]: Training loss at epoch 8: 0.9737231135368347
[08/28/2025 07:08:44 INFO]: Training loss at epoch 4: 1.1372848749160767
[08/28/2025 07:09:09 INFO]: New best epoch, val score: -0.8858596689088587
[08/28/2025 07:09:09 INFO]: Saving model to: maddest-Elbert_trial_123/model_best.pth
[08/28/2025 07:09:09 INFO]: Training loss at epoch 23: 1.1732128262519836
[08/28/2025 07:09:37 INFO]: Training loss at epoch 13: 1.2458919286727905
[08/28/2025 07:09:39 INFO]: Training loss at epoch 8: 1.3139052987098694
[08/28/2025 07:09:40 INFO]: Training loss at epoch 18: 1.1978806853294373
[08/28/2025 07:10:14 INFO]: Training loss at epoch 18: 1.0464098453521729
[08/28/2025 07:10:26 INFO]: Training loss at epoch 36: 0.9201900064945221
[08/28/2025 07:10:49 INFO]: Training loss at epoch 0: 1.0127699077129364
[08/28/2025 07:11:14 INFO]: Training loss at epoch 28: 0.9995083808898926
[08/28/2025 07:11:24 INFO]: Training loss at epoch 9: 1.1088572144508362
[08/28/2025 07:11:28 INFO]: New best epoch, val score: -0.8938788709311275
[08/28/2025 07:11:28 INFO]: Saving model to: maddest-Elbert_trial_125/model_best.pth
[08/28/2025 07:11:52 INFO]: Training accuracy: {
    "score": -1.0117711707660832,
    "rmse": 1.0117711707660832
}
[08/28/2025 07:11:52 INFO]: Val accuracy: {
    "score": -0.890025282385561,
    "rmse": 0.890025282385561
}
[08/28/2025 07:11:52 INFO]: Test accuracy: {
    "score": -1.001637596703699,
    "rmse": 1.001637596703699
}
[08/28/2025 07:11:52 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_95",
    "best_epoch": 14,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.001637596703699,
        "rmse": 1.001637596703699
    },
    "train_stats": {
        "score": -1.0117711707660832,
        "rmse": 1.0117711707660832
    },
    "val_stats": {
        "score": -0.890025282385561,
        "rmse": 0.890025282385561
    }
}
[08/28/2025 07:11:52 INFO]: Procewss finished for trial maddest-Elbert_trial_95
[08/28/2025 07:11:52 INFO]: 
_________________________________________________

[08/28/2025 07:11:52 INFO]: train_net_for_optune.py main() running.
[08/28/2025 07:11:52 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 1.87988789381394
  attention_dropout: 0.1887928874991953
  ffn_dropout: 0.1887928874991953
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.741282875674513e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_126

[08/28/2025 07:11:52 INFO]: This ft_transformer has 11.528 million parameters.
[08/28/2025 07:11:52 INFO]: Training will start at epoch 0.
[08/28/2025 07:11:52 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 07:12:20 INFO]: Training loss at epoch 9: 0.9229077100753784
[08/28/2025 07:12:29 INFO]: Training loss at epoch 5: 1.1493453979492188
[08/28/2025 07:12:35 INFO]: Training loss at epoch 4: 1.534992754459381
[08/28/2025 07:12:46 INFO]: Training loss at epoch 39: 0.9765011668205261
[08/28/2025 07:12:49 INFO]: Training loss at epoch 35: 0.8849595189094543
[08/28/2025 07:12:54 INFO]: New best epoch, val score: -0.8858059952160533
[08/28/2025 07:12:54 INFO]: Saving model to: maddest-Elbert_trial_123/model_best.pth
[08/28/2025 07:13:00 INFO]: Training loss at epoch 14: 1.1738752722740173
[08/28/2025 07:13:02 INFO]: New best epoch, val score: -0.8774317474305784
[08/28/2025 07:13:02 INFO]: Saving model to: maddest-Elbert_trial_124/model_best.pth
[08/28/2025 07:13:03 INFO]: Training stats: {
    "score": -0.9804287333099045,
    "rmse": 0.9804287333099045
}
[08/28/2025 07:13:03 INFO]: Val stats: {
    "score": -0.8854012734675485,
    "rmse": 0.8854012734675485
}
[08/28/2025 07:13:03 INFO]: Test stats: {
    "score": -0.9762430038383211,
    "rmse": 0.9762430038383211
}
[08/28/2025 07:13:09 INFO]: Training loss at epoch 17: 0.8007422685623169
[08/28/2025 07:13:25 INFO]: Training loss at epoch 20: 0.7858381271362305
[08/28/2025 07:13:34 INFO]: Training stats: {
    "score": -0.9842811692022453,
    "rmse": 0.9842811692022453
}
[08/28/2025 07:13:34 INFO]: Val stats: {
    "score": -0.9184372734702203,
    "rmse": 0.9184372734702203
}
[08/28/2025 07:13:34 INFO]: Test stats: {
    "score": -0.9725157630789634,
    "rmse": 0.9725157630789634
}
[08/28/2025 07:14:19 INFO]: Training loss at epoch 19: 1.0352561175823212
[08/28/2025 07:14:26 INFO]: Training loss at epoch 17: 0.9612138867378235
[08/28/2025 07:15:42 INFO]: Training loss at epoch 9: 1.045395851135254
[08/28/2025 07:15:53 INFO]: Training stats: {
    "score": -1.0195063008819005,
    "rmse": 1.0195063008819005
}
[08/28/2025 07:15:53 INFO]: Val stats: {
    "score": -0.8779227472783531,
    "rmse": 0.8779227472783531
}
[08/28/2025 07:15:53 INFO]: Test stats: {
    "score": -1.0076838246764837,
    "rmse": 1.0076838246764837
}
[08/28/2025 07:16:09 INFO]: Training loss at epoch 6: 0.9971059560775757
[08/28/2025 07:16:20 INFO]: Training loss at epoch 15: 1.3919133245944977
[08/28/2025 07:16:22 INFO]: Training loss at epoch 1: 0.9510504007339478
[08/28/2025 07:16:25 INFO]: Training stats: {
    "score": -0.9166672659339846,
    "rmse": 0.9166672659339846
}
[08/28/2025 07:16:25 INFO]: Val stats: {
    "score": -0.9177593903447221,
    "rmse": 0.9177593903447221
}
[08/28/2025 07:16:25 INFO]: Test stats: {
    "score": -0.9862648552642378,
    "rmse": 0.9862648552642378
}
[08/28/2025 07:16:32 INFO]: Training loss at epoch 5: 0.945953369140625
[08/28/2025 07:16:35 INFO]: New best epoch, val score: -0.884514762492605
[08/28/2025 07:16:35 INFO]: Saving model to: maddest-Elbert_trial_123/model_best.pth
[08/28/2025 07:16:55 INFO]: Training loss at epoch 20: 1.0297390818595886
[08/28/2025 07:17:01 INFO]: New best epoch, val score: -0.8772052489115821
[08/28/2025 07:17:01 INFO]: Saving model to: maddest-Elbert_trial_124/model_best.pth
[08/28/2025 07:17:13 INFO]: Training loss at epoch 0: 1.2084458470344543
[08/28/2025 07:17:16 INFO]: Training loss at epoch 10: 1.020362377166748
[08/28/2025 07:17:16 INFO]: Training loss at epoch 29: 0.9608335494995117
[08/28/2025 07:17:26 INFO]: Training loss at epoch 24: 0.9763338565826416
[08/28/2025 07:17:37 INFO]: Training loss at epoch 39: 0.7695475220680237
[08/28/2025 07:17:47 INFO]: Training stats: {
    "score": -1.0270479388750784,
    "rmse": 1.0270479388750784
}
[08/28/2025 07:17:47 INFO]: Val stats: {
    "score": -1.0374356903736481,
    "rmse": 1.0374356903736481
}
[08/28/2025 07:17:47 INFO]: Test stats: {
    "score": -1.0187394668022294,
    "rmse": 1.0187394668022294
}
[08/28/2025 07:17:48 INFO]: Running Final Evaluation...
[08/28/2025 07:17:57 INFO]: Training loss at epoch 10: 0.7858372330665588
[08/28/2025 07:17:57 INFO]: New best epoch, val score: -0.8861065072680809
[08/28/2025 07:17:57 INFO]: Saving model to: maddest-Elbert_trial_126/model_best.pth
[08/28/2025 07:18:20 INFO]: Training loss at epoch 19: 1.1337629556655884
[08/28/2025 07:19:22 INFO]: Training stats: {
    "score": -1.000187103266606,
    "rmse": 1.000187103266606
}
[08/28/2025 07:19:22 INFO]: Val stats: {
    "score": -0.8953012035131315,
    "rmse": 0.8953012035131315
}
[08/28/2025 07:19:22 INFO]: Test stats: {
    "score": -0.9757149266397895,
    "rmse": 0.9757149266397895
}
[08/28/2025 07:19:39 INFO]: Training loss at epoch 18: 1.0314948558807373
[08/28/2025 07:19:46 INFO]: Training loss at epoch 16: 1.0151654481887817
[08/28/2025 07:19:54 INFO]: Training loss at epoch 7: 1.0082904398441315
[08/28/2025 07:20:17 INFO]: Training loss at epoch 37: 0.735660970211029
[08/28/2025 07:20:22 INFO]: New best epoch, val score: -0.8827389506439628
[08/28/2025 07:20:22 INFO]: Saving model to: maddest-Elbert_trial_123/model_best.pth
[08/28/2025 07:20:32 INFO]: Training loss at epoch 20: 1.2090725302696228
[08/28/2025 07:20:36 INFO]: Training loss at epoch 6: 1.2247850894927979
[08/28/2025 07:21:01 INFO]: Training stats: {
    "score": -1.0639353165007785,
    "rmse": 1.0639353165007785
}
[08/28/2025 07:21:01 INFO]: Val stats: {
    "score": -0.8856307116914447,
    "rmse": 0.8856307116914447
}
[08/28/2025 07:21:01 INFO]: Test stats: {
    "score": -1.0373653493330008,
    "rmse": 1.0373653493330008
}
[08/28/2025 07:21:01 INFO]: Training loss at epoch 11: 1.224784940481186
[08/28/2025 07:21:20 INFO]: Training stats: {
    "score": -0.7765321044144727,
    "rmse": 0.7765321044144727
}
[08/28/2025 07:21:20 INFO]: Val stats: {
    "score": -0.9230114937844079,
    "rmse": 0.9230114937844079
}
[08/28/2025 07:21:20 INFO]: Test stats: {
    "score": -1.0640706799486643,
    "rmse": 1.0640706799486643
}
[08/28/2025 07:21:43 INFO]: Training accuracy: {
    "score": -1.0220906070359286,
    "rmse": 1.0220906070359286
}
[08/28/2025 07:21:43 INFO]: Val accuracy: {
    "score": -0.888003132861385,
    "rmse": 0.888003132861385
}
[08/28/2025 07:21:43 INFO]: Test accuracy: {
    "score": -1.0112533836876119,
    "rmse": 1.0112533836876119
}
[08/28/2025 07:21:43 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_101",
    "best_epoch": 8,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0112533836876119,
        "rmse": 1.0112533836876119
    },
    "train_stats": {
        "score": -1.0220906070359286,
        "rmse": 1.0220906070359286
    },
    "val_stats": {
        "score": -0.888003132861385,
        "rmse": 0.888003132861385
    }
}
[08/28/2025 07:21:43 INFO]: Procewss finished for trial maddest-Elbert_trial_101
[08/28/2025 07:21:43 INFO]: 
_________________________________________________

[08/28/2025 07:21:43 INFO]: train_net_for_optune.py main() running.
[08/28/2025 07:21:43 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 1.5326693672695133
  attention_dropout: 0.2115299382330011
  ffn_dropout: 0.2115299382330011
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00025537220237492673
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_127

[08/28/2025 07:21:43 INFO]: This ft_transformer has 10.300 million parameters.
[08/28/2025 07:21:43 INFO]: Training will start at epoch 0.
[08/28/2025 07:21:43 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 07:22:02 INFO]: Training loss at epoch 2: 1.0313392281532288
[08/28/2025 07:22:43 INFO]: Training loss at epoch 18: 0.8690843284130096
[08/28/2025 07:22:50 INFO]: Training loss at epoch 11: 0.9139769077301025
[08/28/2025 07:23:09 INFO]: Training loss at epoch 17: 1.2648726403713226
[08/28/2025 07:23:14 INFO]: Training loss at epoch 21: 0.908696711063385
[08/28/2025 07:23:19 INFO]: Training loss at epoch 1: 1.16130131483078
[08/28/2025 07:23:40 INFO]: Training loss at epoch 8: 0.9952618777751923
[08/28/2025 07:23:51 INFO]: Training loss at epoch 10: 0.8621823191642761
[08/28/2025 07:24:07 INFO]: New best epoch, val score: -0.8810757886008013
[08/28/2025 07:24:07 INFO]: Saving model to: maddest-Elbert_trial_123/model_best.pth
[08/28/2025 07:24:37 INFO]: Training loss at epoch 7: 1.0573067665100098
[08/28/2025 07:24:37 INFO]: Training loss at epoch 36: 0.9446742236614227
[08/28/2025 07:24:46 INFO]: Training loss at epoch 12: 1.2447277903556824
[08/28/2025 07:25:13 INFO]: Training loss at epoch 21: 0.9230952858924866
[08/28/2025 07:25:26 INFO]: Training loss at epoch 30: 0.9250232875347137
[08/28/2025 07:25:46 INFO]: Training loss at epoch 25: 0.8632734417915344
[08/28/2025 07:26:08 INFO]: Training loss at epoch 19: 0.8541580438613892
[08/28/2025 07:26:35 INFO]: Training loss at epoch 18: 0.95708167552948
[08/28/2025 07:26:41 INFO]: Training loss at epoch 0: 1.4002789855003357
[08/28/2025 07:26:48 INFO]: Training loss at epoch 21: 0.7014855295419693
[08/28/2025 07:27:19 INFO]: New best epoch, val score: -0.9775293771774848
[08/28/2025 07:27:19 INFO]: Saving model to: maddest-Elbert_trial_127/model_best.pth
[08/28/2025 07:27:26 INFO]: Training loss at epoch 9: 1.1373448967933655
[08/28/2025 07:27:38 INFO]: Training loss at epoch 3: 1.0512222051620483
[08/28/2025 07:27:43 INFO]: Training loss at epoch 12: 1.2039695084095001
[08/28/2025 07:28:24 INFO]: Training stats: {
    "score": -0.9815112720654572,
    "rmse": 0.9815112720654572
}
[08/28/2025 07:28:24 INFO]: Val stats: {
    "score": -0.9235377055228936,
    "rmse": 0.9235377055228936
}
[08/28/2025 07:28:24 INFO]: Test stats: {
    "score": -0.9782575160480604,
    "rmse": 0.9782575160480604
}
[08/28/2025 07:28:31 INFO]: Training loss at epoch 13: 1.0682330131530762
[08/28/2025 07:28:41 INFO]: Training loss at epoch 8: 1.1188754439353943
[08/28/2025 07:28:43 INFO]: Training stats: {
    "score": -1.0437624712075164,
    "rmse": 1.0437624712075164
}
[08/28/2025 07:28:43 INFO]: Val stats: {
    "score": -0.879790821119756,
    "rmse": 0.879790821119756
}
[08/28/2025 07:28:43 INFO]: Test stats: {
    "score": -1.0196863851534637,
    "rmse": 1.0196863851534637
}
[08/28/2025 07:29:07 INFO]: New best epoch, val score: -0.879790821119756
[08/28/2025 07:29:07 INFO]: Saving model to: maddest-Elbert_trial_123/model_best.pth
[08/28/2025 07:29:09 INFO]: Training loss at epoch 20: 0.8907173573970795
[08/28/2025 07:29:30 INFO]: Training loss at epoch 2: 1.4477108716964722
[08/28/2025 07:29:54 INFO]: Training loss at epoch 22: 1.175463616847992
[08/28/2025 07:29:58 INFO]: Training loss at epoch 11: 0.9244929254055023
[08/28/2025 07:29:59 INFO]: Training loss at epoch 19: 1.099985659122467
[08/28/2025 07:30:08 INFO]: New best epoch, val score: -0.8809135993274001
[08/28/2025 07:30:08 INFO]: Saving model to: maddest-Elbert_trial_112/model_best.pth
[08/28/2025 07:30:11 INFO]: Training loss at epoch 38: 0.7677071988582611
[08/28/2025 07:31:02 INFO]: Training loss at epoch 19: 0.8753516972064972
[08/28/2025 07:31:11 INFO]: Training stats: {
    "score": -0.997839878681751,
    "rmse": 0.997839878681751
}
[08/28/2025 07:31:11 INFO]: Val stats: {
    "score": -0.9604842823842185,
    "rmse": 0.9604842823842185
}
[08/28/2025 07:31:11 INFO]: Test stats: {
    "score": -0.9833015117024235,
    "rmse": 0.9833015117024235
}
[08/28/2025 07:31:28 INFO]: Training loss at epoch 31: 1.0825867652893066
[08/28/2025 07:32:15 INFO]: Training loss at epoch 14: 0.8462918102741241
[08/28/2025 07:32:16 INFO]: Training loss at epoch 1: 3.360556185245514
[08/28/2025 07:32:22 INFO]: Training loss at epoch 40: 0.5246016681194305
[08/28/2025 07:32:26 INFO]: Training loss at epoch 10: 1.0555121898651123
[08/28/2025 07:32:36 INFO]: Training loss at epoch 13: 1.011526882648468
[08/28/2025 07:32:39 INFO]: Training loss at epoch 9: 0.8297490477561951
[08/28/2025 07:33:08 INFO]: Training loss at epoch 22: 1.0769255757331848
[08/28/2025 07:33:15 INFO]: Training loss at epoch 4: 1.0004685521125793
[08/28/2025 07:33:46 INFO]: Training stats: {
    "score": -0.997295048520122,
    "rmse": 0.997295048520122
}
[08/28/2025 07:33:46 INFO]: Val stats: {
    "score": -0.9300107735260543,
    "rmse": 0.9300107735260543
}
[08/28/2025 07:33:46 INFO]: Test stats: {
    "score": -0.9722074882482777,
    "rmse": 0.9722074882482777
}
[08/28/2025 07:34:02 INFO]: Training stats: {
    "score": -1.0111432811846421,
    "rmse": 1.0111432811846421
}
[08/28/2025 07:34:02 INFO]: Val stats: {
    "score": -0.8831021565861009,
    "rmse": 0.8831021565861009
}
[08/28/2025 07:34:02 INFO]: Test stats: {
    "score": -0.9856458458899645,
    "rmse": 0.9856458458899645
}
[08/28/2025 07:34:04 INFO]: Training loss at epoch 26: 0.8839916586875916
[08/28/2025 07:34:31 INFO]: Training loss at epoch 23: 0.9425301849842072
[08/28/2025 07:34:31 INFO]: Training loss at epoch 20: 1.0395313501358032
[08/28/2025 07:34:51 INFO]: Training loss at epoch 20: 0.9453668594360352
[08/28/2025 07:35:33 INFO]: Training loss at epoch 3: 1.0522047281265259
[08/28/2025 07:35:57 INFO]: Training loss at epoch 15: 0.8489463627338409
[08/28/2025 07:36:02 INFO]: Training loss at epoch 12: 1.1356847882270813
[08/28/2025 07:36:10 INFO]: Training loss at epoch 11: 0.7927018404006958
[08/28/2025 07:36:27 INFO]: Training loss at epoch 37: 0.8491275906562805
[08/28/2025 07:36:37 INFO]: Training loss at epoch 22: 1.1090536713600159
[08/28/2025 07:37:14 INFO]: Training loss at epoch 21: 1.4096635580062866
[08/28/2025 07:37:28 INFO]: Training loss at epoch 14: 1.0691006183624268
[08/28/2025 07:37:30 INFO]: Training loss at epoch 32: 1.0947771966457367
[08/28/2025 07:37:51 INFO]: Training loss at epoch 2: 1.5596826672554016
[08/28/2025 07:37:57 INFO]: Training loss at epoch 21: 1.07161745429039
[08/28/2025 07:38:06 INFO]: Training loss at epoch 10: 1.057938039302826
[08/28/2025 07:38:51 INFO]: Training loss at epoch 5: 1.1883653402328491
[08/28/2025 07:39:10 INFO]: Training loss at epoch 24: 1.124190479516983
[08/28/2025 07:39:42 INFO]: Training loss at epoch 16: 0.9544785916805267
[08/28/2025 07:39:56 INFO]: Training loss at epoch 12: 1.1400622725486755
[08/28/2025 07:39:59 INFO]: Training loss at epoch 39: 0.9889467656612396
[08/28/2025 07:41:20 INFO]: Training loss at epoch 22: 0.9985874891281128
[08/28/2025 07:41:24 INFO]: Training loss at epoch 21: 1.0024052560329437
[08/28/2025 07:41:41 INFO]: Training loss at epoch 4: 1.2511792182922363
[08/28/2025 07:42:02 INFO]: Training loss at epoch 20: 1.0411139726638794
[08/28/2025 07:42:07 INFO]: Training loss at epoch 11: 0.9505380988121033
[08/28/2025 07:42:09 INFO]: Training loss at epoch 13: 1.324084222316742
[08/28/2025 07:42:21 INFO]: Training loss at epoch 15: 0.8922013938426971
[08/28/2025 07:42:21 INFO]: Training loss at epoch 27: 1.068428635597229
[08/28/2025 07:42:24 INFO]: New best epoch, val score: -0.874560109738845
[08/28/2025 07:42:24 INFO]: Saving model to: maddest-Elbert_trial_126/model_best.pth
[08/28/2025 07:43:03 INFO]: Training loss at epoch 23: 0.9503687918186188
[08/28/2025 07:43:19 INFO]: Training stats: {
    "score": -0.9290736903322715,
    "rmse": 0.9290736903322715
}
[08/28/2025 07:43:19 INFO]: Val stats: {
    "score": -0.8860577815240201,
    "rmse": 0.8860577815240201
}
[08/28/2025 07:43:19 INFO]: Test stats: {
    "score": -0.9779913624227412,
    "rmse": 0.9779913624227412
}
[08/28/2025 07:43:23 INFO]: Training loss at epoch 41: 0.5796091854572296
[08/28/2025 07:43:27 INFO]: Training loss at epoch 17: 0.9330575168132782
[08/28/2025 07:43:27 INFO]: Training loss at epoch 3: 1.5775401592254639
[08/28/2025 07:43:35 INFO]: Training loss at epoch 33: 1.1052184104919434
[08/28/2025 07:43:42 INFO]: Training loss at epoch 13: 1.1015825867652893
[08/28/2025 07:43:52 INFO]: Training loss at epoch 25: 1.0786085724830627
[08/28/2025 07:44:20 INFO]: Running Final Evaluation...
[08/28/2025 07:44:30 INFO]: Training loss at epoch 6: 1.062105417251587
[08/28/2025 07:44:46 INFO]: Training loss at epoch 23: 1.0526078939437866
[08/28/2025 07:45:28 INFO]: Training loss at epoch 22: 0.9015822112560272
[08/28/2025 07:46:08 INFO]: Training loss at epoch 12: 1.0458618998527527
[08/28/2025 07:46:24 INFO]: Training accuracy: {
    "score": -1.0443624345270885,
    "rmse": 1.0443624345270885
}
[08/28/2025 07:46:24 INFO]: Val accuracy: {
    "score": -0.8774097802728773,
    "rmse": 0.8774097802728773
}
[08/28/2025 07:46:24 INFO]: Test accuracy: {
    "score": -1.020132890478989,
    "rmse": 1.020132890478989
}
[08/28/2025 07:46:24 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_111",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.020132890478989,
        "rmse": 1.020132890478989
    },
    "train_stats": {
        "score": -1.0443624345270885,
        "rmse": 1.0443624345270885
    },
    "val_stats": {
        "score": -0.8774097802728773,
        "rmse": 0.8774097802728773
    }
}
[08/28/2025 07:46:24 INFO]: Procewss finished for trial maddest-Elbert_trial_111
[08/28/2025 07:46:24 INFO]: 
_________________________________________________

[08/28/2025 07:46:24 INFO]: train_net_for_optune.py main() running.
[08/28/2025 07:46:24 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 1.1603161581904842
  attention_dropout: 0.20111821146041858
  ffn_dropout: 0.20111821146041858
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0002553406322655615
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_128

[08/28/2025 07:46:25 INFO]: This ft_transformer has 8.980 million parameters.
[08/28/2025 07:46:25 INFO]: Training will start at epoch 0.
[08/28/2025 07:46:25 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 07:46:26 INFO]: Training loss at epoch 23: 0.8091685473918915
[08/28/2025 07:47:10 INFO]: Training loss at epoch 18: 1.0465766787528992
[08/28/2025 07:47:14 INFO]: Training loss at epoch 16: 1.0640778839588165
[08/28/2025 07:47:27 INFO]: Training loss at epoch 14: 1.064759910106659
[08/28/2025 07:47:36 INFO]: New best epoch, val score: -0.8787961899713906
[08/28/2025 07:47:36 INFO]: Saving model to: maddest-Elbert_trial_109/model_best.pth
[08/28/2025 07:47:49 INFO]: Training loss at epoch 5: 0.8147473037242889
[08/28/2025 07:47:55 INFO]: Training loss at epoch 22: 1.1516155004501343
[08/28/2025 07:48:08 INFO]: Training loss at epoch 24: 0.8618193864822388
[08/28/2025 07:48:15 INFO]: Training loss at epoch 14: 1.1853777766227722
[08/28/2025 07:48:18 INFO]: Training loss at epoch 38: 0.7352053821086884
[08/28/2025 07:48:31 INFO]: Training loss at epoch 26: 1.0389179587364197
[08/28/2025 07:49:03 INFO]: Training loss at epoch 4: 1.3590884804725647
[08/28/2025 07:50:04 INFO]: Training loss at epoch 7: 1.1205421686172485
[08/28/2025 07:50:11 INFO]: Training loss at epoch 13: 1.1969859600067139
[08/28/2025 07:50:18 INFO]: Training loss at epoch 21: 1.256777822971344
[08/28/2025 07:50:39 INFO]: Training loss at epoch 28: 0.8696562051773071
[08/28/2025 07:50:52 INFO]: Training loss at epoch 0: 1.1341102123260498
[08/28/2025 07:50:54 INFO]: Training loss at epoch 19: 1.0121500194072723
[08/28/2025 07:51:14 INFO]: Training loss at epoch 15: 0.8722476065158844
[08/28/2025 07:51:30 INFO]: New best epoch, val score: -0.9159403532827767
[08/28/2025 07:51:30 INFO]: Saving model to: maddest-Elbert_trial_128/model_best.pth
[08/28/2025 07:51:33 INFO]: Training loss at epoch 25: 0.9425580203533173
[08/28/2025 07:52:09 INFO]: Training stats: {
    "score": -0.9763341065033265,
    "rmse": 0.9763341065033265
}
[08/28/2025 07:52:09 INFO]: Val stats: {
    "score": -0.9079820148041334,
    "rmse": 0.9079820148041334
}
[08/28/2025 07:52:09 INFO]: Test stats: {
    "score": -0.9747760615345628,
    "rmse": 0.9747760615345628
}
[08/28/2025 07:52:09 INFO]: Training loss at epoch 17: 1.0226417779922485
[08/28/2025 07:52:58 INFO]: Training loss at epoch 24: 0.8801910281181335
[08/28/2025 07:53:08 INFO]: Training loss at epoch 27: 1.0260506868362427
[08/28/2025 07:53:10 INFO]: Training loss at epoch 40: 0.8845363259315491
[08/28/2025 07:53:27 INFO]: Training loss at epoch 23: 1.1301551759243011
[08/28/2025 07:53:53 INFO]: Training loss at epoch 6: 1.0035244226455688
[08/28/2025 07:54:09 INFO]: Training loss at epoch 14: 1.2439255714416504
[08/28/2025 07:54:16 INFO]: Training loss at epoch 15: 1.1176201105117798
[08/28/2025 07:54:22 INFO]: Training loss at epoch 23: 0.9319500923156738
[08/28/2025 07:54:22 INFO]: Training loss at epoch 42: 0.654269814491272
[08/28/2025 07:54:34 INFO]: Training loss at epoch 5: 1.0951736569404602
[08/28/2025 07:54:54 INFO]: Training loss at epoch 26: 1.164796143770218
[08/28/2025 07:54:56 INFO]: Training loss at epoch 16: 1.0846610367298126
[08/28/2025 07:55:37 INFO]: Training loss at epoch 8: 0.9761928021907806
[08/28/2025 07:55:53 INFO]: Training loss at epoch 20: 0.9262876212596893
[08/28/2025 07:55:55 INFO]: Training loss at epoch 1: 2.8915244340896606
[08/28/2025 07:56:19 INFO]: Training loss at epoch 24: 0.8263918161392212
[08/28/2025 07:57:07 INFO]: Training loss at epoch 18: 1.0432116985321045
[08/28/2025 07:57:51 INFO]: Training loss at epoch 28: 1.1737123429775238
[08/28/2025 07:58:11 INFO]: Training loss at epoch 15: 0.7833334803581238
[08/28/2025 07:58:17 INFO]: Training loss at epoch 27: 0.7543770521879196
[08/28/2025 07:58:30 INFO]: Training loss at epoch 22: 1.2055408358573914
[08/28/2025 07:58:39 INFO]: Training loss at epoch 17: 1.0867030918598175
[08/28/2025 07:58:57 INFO]: Training loss at epoch 29: 0.9928328990936279
[08/28/2025 07:59:36 INFO]: Training loss at epoch 21: 1.1573216021060944
[08/28/2025 08:00:02 INFO]: Training loss at epoch 7: 1.024259865283966
[08/28/2025 08:00:06 INFO]: Training loss at epoch 39: 0.8180041015148163
[08/28/2025 08:00:11 INFO]: Training loss at epoch 6: 1.019988089799881
[08/28/2025 08:00:22 INFO]: Training loss at epoch 16: 0.8878767490386963
[08/28/2025 08:00:52 INFO]: Training loss at epoch 24: 1.0287554860115051
[08/28/2025 08:00:55 INFO]: Training loss at epoch 2: 2.301372766494751
[08/28/2025 08:01:13 INFO]: Training loss at epoch 9: 0.8933130502700806
[08/28/2025 08:01:32 INFO]: New best epoch, val score: -0.9002216917030867
[08/28/2025 08:01:32 INFO]: Saving model to: maddest-Elbert_trial_128/model_best.pth
[08/28/2025 08:01:34 INFO]: Training loss at epoch 24: 0.9091706275939941
[08/28/2025 08:01:41 INFO]: Training loss at epoch 28: 1.0898470282554626
[08/28/2025 08:01:44 INFO]: Training stats: {
    "score": -1.0014828870887542,
    "rmse": 1.0014828870887542
}
[08/28/2025 08:01:44 INFO]: Val stats: {
    "score": -0.9592351037111607,
    "rmse": 0.9592351037111607
}
[08/28/2025 08:01:44 INFO]: Test stats: {
    "score": -0.9772780497653055,
    "rmse": 0.9772780497653055
}
[08/28/2025 08:01:56 INFO]: Training loss at epoch 19: 1.14566171169281
[08/28/2025 08:02:13 INFO]: Training loss at epoch 16: 0.8392178118228912
[08/28/2025 08:02:25 INFO]: Training loss at epoch 18: 0.8320654034614563
[08/28/2025 08:02:30 INFO]: Training loss at epoch 29: 1.0801799893379211
[08/28/2025 08:02:49 INFO]: Training loss at epoch 25: 0.9759105741977692
[08/28/2025 08:03:00 INFO]: Training loss at epoch 41: 0.7724685668945312
[08/28/2025 08:03:08 INFO]: Training stats: {
    "score": -0.9955040820664914,
    "rmse": 0.9955040820664914
}
[08/28/2025 08:03:08 INFO]: Val stats: {
    "score": -0.9052213421121751,
    "rmse": 0.9052213421121751
}
[08/28/2025 08:03:08 INFO]: Test stats: {
    "score": -0.9716220111287193,
    "rmse": 0.9716220111287193
}
[08/28/2025 08:03:19 INFO]: Training loss at epoch 22: 0.9502325654029846
[08/28/2025 08:03:37 INFO]: Training stats: {
    "score": -0.9740787707854337,
    "rmse": 0.9740787707854337
}
[08/28/2025 08:03:37 INFO]: Val stats: {
    "score": -0.9348602244247297,
    "rmse": 0.9348602244247297
}
[08/28/2025 08:03:37 INFO]: Test stats: {
    "score": -0.9778627884349954,
    "rmse": 0.9778627884349954
}
[08/28/2025 08:04:04 INFO]: Training stats: {
    "score": -0.8291964992370777,
    "rmse": 0.8291964992370777
}
[08/28/2025 08:04:04 INFO]: Val stats: {
    "score": -1.044858050456224,
    "rmse": 1.044858050456224
}
[08/28/2025 08:04:04 INFO]: Test stats: {
    "score": -1.0774693395357005,
    "rmse": 1.0774693395357005
}
[08/28/2025 08:04:06 INFO]: Training stats: {
    "score": -0.9972193175488652,
    "rmse": 0.9972193175488652
}
[08/28/2025 08:04:06 INFO]: Val stats: {
    "score": -0.9547763645142793,
    "rmse": 0.9547763645142793
}
[08/28/2025 08:04:06 INFO]: Test stats: {
    "score": -0.9897159847319829,
    "rmse": 0.9897159847319829
}
[08/28/2025 08:05:05 INFO]: Training loss at epoch 29: 0.991896241903305
[08/28/2025 08:05:22 INFO]: Training loss at epoch 43: 0.552325889468193
[08/28/2025 08:05:45 INFO]: Training loss at epoch 7: 1.0802261233329773
[08/28/2025 08:06:00 INFO]: Training loss at epoch 3: 1.3267061114311218
[08/28/2025 08:06:05 INFO]: Training loss at epoch 25: 0.9452299773693085
[08/28/2025 08:06:07 INFO]: Training loss at epoch 8: 0.8573330342769623
[08/28/2025 08:06:07 INFO]: Training loss at epoch 19: 0.9341320395469666
[08/28/2025 08:06:14 INFO]: Training stats: {
    "score": -0.9893621632741422,
    "rmse": 0.9893621632741422
}
[08/28/2025 08:06:14 INFO]: Val stats: {
    "score": -0.9072019438233024,
    "rmse": 0.9072019438233024
}
[08/28/2025 08:06:14 INFO]: Test stats: {
    "score": -0.9774882607718732,
    "rmse": 0.9774882607718732
}
[08/28/2025 08:06:15 INFO]: Training loss at epoch 17: 0.8642444610595703
[08/28/2025 08:06:28 INFO]: Training loss at epoch 17: 0.8379014134407043
[08/28/2025 08:06:46 INFO]: Training loss at epoch 23: 0.8888416588306427
[08/28/2025 08:07:02 INFO]: Training loss at epoch 23: 0.9721541702747345
[08/28/2025 08:07:24 INFO]: Training stats: {
    "score": -1.0052595425600233,
    "rmse": 1.0052595425600233
}
[08/28/2025 08:07:24 INFO]: Val stats: {
    "score": -0.8916135148174664,
    "rmse": 0.8916135148174664
}
[08/28/2025 08:07:24 INFO]: Test stats: {
    "score": -0.9783960786360878,
    "rmse": 0.9783960786360878
}
[08/28/2025 08:07:26 INFO]: Training loss at epoch 25: 0.8337162137031555
[08/28/2025 08:08:31 INFO]: Training loss at epoch 20: 0.9503321945667267
[08/28/2025 08:08:44 INFO]: Training loss at epoch 10: 0.7682847678661346
[08/28/2025 08:08:46 INFO]: Training loss at epoch 30: 0.9219104945659637
[08/28/2025 08:09:40 INFO]: Training loss at epoch 25: 1.014138102531433
[08/28/2025 08:09:40 INFO]: Training loss at epoch 30: 0.8130845129489899
[08/28/2025 08:10:05 INFO]: Training loss at epoch 30: 1.0797149538993835
[08/28/2025 08:10:21 INFO]: Training loss at epoch 18: 1.0459256768226624
[08/28/2025 08:10:48 INFO]: Training loss at epoch 24: 1.1878811717033386
[08/28/2025 08:11:05 INFO]: Training loss at epoch 4: 0.9555106461048126
[08/28/2025 08:11:10 INFO]: Training loss at epoch 20: 1.0017372071743011
[08/28/2025 08:11:22 INFO]: Training loss at epoch 8: 0.907277911901474
[08/28/2025 08:12:15 INFO]: Training loss at epoch 9: 0.9907086789608002
[08/28/2025 08:12:32 INFO]: Training loss at epoch 18: 0.8593812584877014
[08/28/2025 08:12:40 INFO]: Training loss at epoch 26: 0.977557510137558
[08/28/2025 08:12:50 INFO]: Training loss at epoch 42: 0.8230542242527008
[08/28/2025 08:13:05 INFO]: Training loss at epoch 31: 0.9040446877479553
[08/28/2025 08:13:24 INFO]: Training loss at epoch 31: 0.931976854801178
[08/28/2025 08:13:24 INFO]: Training loss at epoch 21: 0.8338125050067902
[08/28/2025 08:14:00 INFO]: Training loss at epoch 26: 0.9562814235687256
[08/28/2025 08:14:19 INFO]: Training stats: {
    "score": -0.9959225602258346,
    "rmse": 0.9959225602258346
}
[08/28/2025 08:14:19 INFO]: Val stats: {
    "score": -0.8857794294897761,
    "rmse": 0.8857794294897761
}
[08/28/2025 08:14:19 INFO]: Test stats: {
    "score": -0.9764422417501838,
    "rmse": 0.9764422417501838
}
[08/28/2025 08:14:19 INFO]: Training loss at epoch 11: 0.9021448194980621
[08/28/2025 08:14:25 INFO]: Training loss at epoch 19: 0.9679404199123383
[08/28/2025 08:14:31 INFO]: Training loss at epoch 25: 0.994408905506134
[08/28/2025 08:14:55 INFO]: Training loss at epoch 21: 1.1741724610328674
[08/28/2025 08:15:04 INFO]: Training loss at epoch 24: 0.9593303203582764
[08/28/2025 08:15:49 INFO]: Training stats: {
    "score": -1.0097237582067329,
    "rmse": 1.0097237582067329
}
[08/28/2025 08:15:49 INFO]: Val stats: {
    "score": -0.8831131721287507,
    "rmse": 0.8831131721287507
}
[08/28/2025 08:15:49 INFO]: Test stats: {
    "score": -0.9868249169803711,
    "rmse": 0.9868249169803711
}
[08/28/2025 08:15:54 INFO]: Training loss at epoch 40: 0.6203379929065704
[08/28/2025 08:15:58 INFO]: Training loss at epoch 26: 1.2281794548034668
[08/28/2025 08:16:07 INFO]: Training loss at epoch 5: 1.351448118686676
[08/28/2025 08:16:30 INFO]: Training loss at epoch 44: 0.5141517668962479
[08/28/2025 08:16:32 INFO]: Training loss at epoch 32: 0.9878318607807159
[08/28/2025 08:17:00 INFO]: Training loss at epoch 9: 1.1866344809532166
[08/28/2025 08:17:49 INFO]: Training loss at epoch 26: 1.0153737664222717
[08/28/2025 08:18:05 INFO]: Training loss at epoch 32: 0.9506819546222687
[08/28/2025 08:18:18 INFO]: Training loss at epoch 26: 1.0128118991851807
[08/28/2025 08:18:23 INFO]: Training loss at epoch 22: 0.8693256676197052
[08/28/2025 08:18:25 INFO]: Training loss at epoch 31: 0.9909520745277405
[08/28/2025 08:18:41 INFO]: Training loss at epoch 19: 0.9135881960391998
[08/28/2025 08:18:41 INFO]: Training loss at epoch 22: 1.006087601184845
[08/28/2025 08:18:52 INFO]: Training stats: {
    "score": -1.0271114402762378,
    "rmse": 1.0271114402762378
}
[08/28/2025 08:18:52 INFO]: Val stats: {
    "score": -1.0209198453894874,
    "rmse": 1.0209198453894874
}
[08/28/2025 08:18:52 INFO]: Test stats: {
    "score": -0.9996780604156288,
    "rmse": 0.9996780604156288
}
[08/28/2025 08:19:53 INFO]: Training loss at epoch 20: 0.8312961757183075
[08/28/2025 08:19:57 INFO]: Training loss at epoch 33: 0.9549762606620789
[08/28/2025 08:19:59 INFO]: Training loss at epoch 12: 0.8789288401603699
[08/28/2025 08:20:20 INFO]: Running Final Evaluation...
[08/28/2025 08:20:27 INFO]: Training loss at epoch 10: 1.0371156930923462
[08/28/2025 08:20:35 INFO]: Training loss at epoch 27: 1.050436556339264
[08/28/2025 08:20:44 INFO]: Training stats: {
    "score": -1.017404493636388,
    "rmse": 1.017404493636388
}
[08/28/2025 08:20:44 INFO]: Val stats: {
    "score": -1.0337432964856617,
    "rmse": 1.0337432964856617
}
[08/28/2025 08:20:44 INFO]: Test stats: {
    "score": -1.0180772040727761,
    "rmse": 1.0180772040727761
}
[08/28/2025 08:21:14 INFO]: Training loss at epoch 6: 1.389452576637268
[08/28/2025 08:21:35 INFO]: Training accuracy: {
    "score": -1.0559921194115764,
    "rmse": 1.0559921194115764
}
[08/28/2025 08:21:35 INFO]: Val accuracy: {
    "score": -0.8849374813414611,
    "rmse": 0.8849374813414611
}
[08/28/2025 08:21:35 INFO]: Test accuracy: {
    "score": -1.0262275385102915,
    "rmse": 1.0262275385102915
}
[08/28/2025 08:21:35 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_120",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0262275385102915,
        "rmse": 1.0262275385102915
    },
    "train_stats": {
        "score": -1.0559921194115764,
        "rmse": 1.0559921194115764
    },
    "val_stats": {
        "score": -0.8849374813414611,
        "rmse": 0.8849374813414611
    }
}
[08/28/2025 08:21:35 INFO]: Procewss finished for trial maddest-Elbert_trial_120
[08/28/2025 08:21:35 INFO]: 
_________________________________________________

[08/28/2025 08:21:35 INFO]: train_net_for_optune.py main() running.
[08/28/2025 08:21:35 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 1.1830019582960445
  attention_dropout: 0.2026048010979018
  ffn_dropout: 0.2026048010979018
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 5.6719781554274305e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_129

[08/28/2025 08:21:35 INFO]: This ft_transformer has 9.063 million parameters.
[08/28/2025 08:21:35 INFO]: Training will start at epoch 0.
[08/28/2025 08:21:35 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 08:22:01 INFO]: Training loss at epoch 27: 1.176977962255478
[08/28/2025 08:22:26 INFO]: Training loss at epoch 23: 1.0453696846961975
[08/28/2025 08:22:37 INFO]: Training loss at epoch 27: 0.9414606094360352
[08/28/2025 08:22:44 INFO]: Training loss at epoch 33: 0.9472492933273315
[08/28/2025 08:22:45 INFO]: Training loss at epoch 43: 0.8576333224773407
[08/28/2025 08:23:13 INFO]: Training loss at epoch 23: 1.0220537781715393
[08/28/2025 08:23:19 INFO]: Training loss at epoch 25: 0.919762521982193
[08/28/2025 08:23:53 INFO]: Training loss at epoch 21: 1.0565791726112366
[08/28/2025 08:24:25 INFO]: Training loss at epoch 10: 1.1327546834945679
[08/28/2025 08:25:05 INFO]: New best epoch, val score: -0.8947930768274703
[08/28/2025 08:25:05 INFO]: Saving model to: maddest-Elbert_trial_127/model_best.pth
[08/28/2025 08:25:30 INFO]: Training loss at epoch 13: 0.9114676415920258
[08/28/2025 08:25:41 INFO]: Training loss at epoch 28: 0.8474428057670593
[08/28/2025 08:25:44 INFO]: Training loss at epoch 27: 1.039299339056015
[08/28/2025 08:25:51 INFO]: Training loss at epoch 27: 1.1221239268779755
[08/28/2025 08:26:03 INFO]: Training loss at epoch 0: 1.1272599697113037
[08/28/2025 08:26:06 INFO]: Training loss at epoch 24: 0.9926481246948242
[08/28/2025 08:26:11 INFO]: Training loss at epoch 7: 1.269490122795105
[08/28/2025 08:26:31 INFO]: Training loss at epoch 11: 0.9160153269767761
[08/28/2025 08:26:41 INFO]: New best epoch, val score: -0.9019096977430522
[08/28/2025 08:26:41 INFO]: Saving model to: maddest-Elbert_trial_129/model_best.pth
[08/28/2025 08:26:42 INFO]: Training loss at epoch 32: 1.2022479176521301
[08/28/2025 08:26:45 INFO]: Training loss at epoch 20: 0.9296596348285675
[08/28/2025 08:27:02 INFO]: Training loss at epoch 28: 1.0230702757835388
[08/28/2025 08:27:20 INFO]: Training loss at epoch 34: 1.0034102201461792
[08/28/2025 08:27:32 INFO]: Training loss at epoch 45: 0.5906824469566345
[08/28/2025 08:27:43 INFO]: New best epoch, val score: -0.9016463978604107
[08/28/2025 08:27:43 INFO]: Saving model to: maddest-Elbert_trial_110/model_best.pth
[08/28/2025 08:27:46 INFO]: Training loss at epoch 41: 0.6438492238521576
[08/28/2025 08:27:56 INFO]: Training loss at epoch 22: 0.8206268548965454
[08/28/2025 08:28:06 INFO]: Training loss at epoch 24: 0.8470049500465393
[08/28/2025 08:29:08 INFO]: Running Final Evaluation...
[08/28/2025 08:29:29 INFO]: Training loss at epoch 29: 0.8079891502857208
[08/28/2025 08:29:53 INFO]: Training loss at epoch 25: 1.0059838891029358
[08/28/2025 08:30:04 INFO]: Training loss at epoch 11: 1.0352556109428406
[08/28/2025 08:30:42 INFO]: Training stats: {
    "score": -0.9681484640344606,
    "rmse": 0.9681484640344606
}
[08/28/2025 08:30:42 INFO]: Val stats: {
    "score": -0.9058282282909222,
    "rmse": 0.9058282282909222
}
[08/28/2025 08:30:42 INFO]: Test stats: {
    "score": -0.9761765617519998,
    "rmse": 0.9761765617519998
}
[08/28/2025 08:30:43 INFO]: New best epoch, val score: -0.8925575561209457
[08/28/2025 08:30:43 INFO]: Saving model to: maddest-Elbert_trial_127/model_best.pth
[08/28/2025 08:31:08 INFO]: Training loss at epoch 14: 0.9894936680793762
[08/28/2025 08:31:13 INFO]: Training loss at epoch 1: 0.9964568316936493
[08/28/2025 08:31:15 INFO]: Training loss at epoch 8: 1.040525734424591
[08/28/2025 08:31:35 INFO]: Training loss at epoch 26: 1.1234934031963348
[08/28/2025 08:31:49 INFO]: New best epoch, val score: -0.8978035400405477
[08/28/2025 08:31:49 INFO]: Saving model to: maddest-Elbert_trial_128/model_best.pth
[08/28/2025 08:31:56 INFO]: Training loss at epoch 23: 0.9008809924125671
[08/28/2025 08:31:57 INFO]: Training loss at epoch 35: 1.0383307933807373
[08/28/2025 08:32:24 INFO]: Training loss at epoch 28: 0.8590351343154907
[08/28/2025 08:32:34 INFO]: Training loss at epoch 44: 0.8603712022304535
[08/28/2025 08:32:38 INFO]: Training loss at epoch 12: 0.8302823007106781
[08/28/2025 08:32:52 INFO]: Training loss at epoch 21: 0.871386706829071
[08/28/2025 08:32:59 INFO]: Training loss at epoch 25: 0.7828970551490784
[08/28/2025 08:33:15 INFO]: Training accuracy: {
    "score": -0.9863578951192816,
    "rmse": 0.9863578951192816
}
[08/28/2025 08:33:15 INFO]: Val accuracy: {
    "score": -0.8754133653158028,
    "rmse": 0.8754133653158028
}
[08/28/2025 08:33:15 INFO]: Test accuracy: {
    "score": -1.008666714583163,
    "rmse": 1.008666714583163
}
[08/28/2025 08:33:15 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_103",
    "best_epoch": 10,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.008666714583163,
        "rmse": 1.008666714583163
    },
    "train_stats": {
        "score": -0.9863578951192816,
        "rmse": 0.9863578951192816
    },
    "val_stats": {
        "score": -0.8754133653158028,
        "rmse": 0.8754133653158028
    }
}
[08/28/2025 08:33:15 INFO]: Procewss finished for trial maddest-Elbert_trial_103
[08/28/2025 08:33:15 INFO]: 
_________________________________________________

[08/28/2025 08:33:15 INFO]: train_net_for_optune.py main() running.
[08/28/2025 08:33:15 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 1.92686108659691
  attention_dropout: 0.17157631165883516
  ffn_dropout: 0.17157631165883516
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.4242294628311e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_130

[08/28/2025 08:33:15 INFO]: This ft_transformer has 11.694 million parameters.
[08/28/2025 08:33:15 INFO]: Training will start at epoch 0.
[08/28/2025 08:33:15 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 08:33:35 INFO]: Training loss at epoch 29: 0.9041529893875122
[08/28/2025 08:33:39 INFO]: Training loss at epoch 26: 0.9249558448791504
[08/28/2025 08:34:01 INFO]: Training loss at epoch 28: 0.9709122180938721
[08/28/2025 08:34:30 INFO]: Training loss at epoch 30: 1.1408766508102417
[08/28/2025 08:35:02 INFO]: Training loss at epoch 33: 1.1911110877990723
[08/28/2025 08:35:39 INFO]: Training loss at epoch 28: 0.855833888053894
[08/28/2025 08:35:43 INFO]: Training loss at epoch 12: 1.1295084357261658
[08/28/2025 08:35:50 INFO]: Training stats: {
    "score": -0.9835947530966869,
    "rmse": 0.9835947530966869
}
[08/28/2025 08:35:50 INFO]: Val stats: {
    "score": -0.960523980024734,
    "rmse": 0.960523980024734
}
[08/28/2025 08:35:50 INFO]: Test stats: {
    "score": -0.9879178920802731,
    "rmse": 0.9879178920802731
}
[08/28/2025 08:36:01 INFO]: Training loss at epoch 24: 0.9550150334835052
[08/28/2025 08:36:20 INFO]: Training loss at epoch 9: 1.242512822151184
[08/28/2025 08:36:21 INFO]: Training loss at epoch 2: 0.8607928156852722
[08/28/2025 08:36:41 INFO]: Training loss at epoch 36: 1.1178134083747864
[08/28/2025 08:36:48 INFO]: Training loss at epoch 15: 1.1984383165836334
[08/28/2025 08:37:24 INFO]: Training loss at epoch 27: 0.9940313100814819
[08/28/2025 08:37:54 INFO]: Training loss at epoch 26: 1.0363644361495972
[08/28/2025 08:38:03 INFO]: Training stats: {
    "score": -1.0832288640078582,
    "rmse": 1.0832288640078582
}
[08/28/2025 08:38:03 INFO]: Val stats: {
    "score": -0.8919826210131528,
    "rmse": 0.8919826210131528
}
[08/28/2025 08:38:03 INFO]: Test stats: {
    "score": -1.0537681597568935,
    "rmse": 1.0537681597568935
}
[08/28/2025 08:38:14 INFO]: Training loss at epoch 31: 1.2200381755828857
[08/28/2025 08:38:25 INFO]: Training loss at epoch 0: 1.4485440850257874
[08/28/2025 08:38:36 INFO]: Training loss at epoch 46: 0.515650287270546
[08/28/2025 08:38:40 INFO]: New best epoch, val score: -0.8919826210131528
[08/28/2025 08:38:40 INFO]: Saving model to: maddest-Elbert_trial_128/model_best.pth
[08/28/2025 08:38:48 INFO]: Training loss at epoch 13: 1.1485052704811096
[08/28/2025 08:38:59 INFO]: Training loss at epoch 22: 1.0251730680465698
[08/28/2025 08:39:05 INFO]: New best epoch, val score: -0.877352958695367
[08/28/2025 08:39:05 INFO]: Saving model to: maddest-Elbert_trial_130/model_best.pth
[08/28/2025 08:39:56 INFO]: Training loss at epoch 27: 1.0757189989089966
[08/28/2025 08:40:06 INFO]: Training loss at epoch 25: 0.8854445815086365
[08/28/2025 08:41:10 INFO]: Training loss at epoch 28: 1.059114933013916
[08/28/2025 08:41:19 INFO]: Training loss at epoch 37: 0.9033024311065674
[08/28/2025 08:41:20 INFO]: Training loss at epoch 13: 1.1075044870376587
[08/28/2025 08:41:26 INFO]: Training loss at epoch 3: 1.0442925095558167
[08/28/2025 08:41:57 INFO]: Training loss at epoch 32: 0.7766267359256744
[08/28/2025 08:42:07 INFO]: Training loss at epoch 29: 1.084242582321167
[08/28/2025 08:42:20 INFO]: Training loss at epoch 29: 0.9115864038467407
[08/28/2025 08:42:22 INFO]: Training loss at epoch 30: 0.9793306887149811
[08/28/2025 08:42:24 INFO]: Training loss at epoch 16: 1.0652145445346832
[08/28/2025 08:42:25 INFO]: Running Final Evaluation...
[08/28/2025 08:42:31 INFO]: Training loss at epoch 45: 0.913017749786377
[08/28/2025 08:42:48 INFO]: Training loss at epoch 27: 1.0883780419826508
[08/28/2025 08:43:08 INFO]: Training loss at epoch 10: 0.9667900800704956
[08/28/2025 08:43:21 INFO]: Training loss at epoch 34: 0.8075411319732666
[08/28/2025 08:43:40 INFO]: Running Final Evaluation...
[08/28/2025 08:43:42 INFO]: Training accuracy: {
    "score": -1.0157726441409352,
    "rmse": 1.0157726441409352
}
[08/28/2025 08:43:42 INFO]: Val accuracy: {
    "score": -0.8861927183964751,
    "rmse": 0.8861927183964751
}
[08/28/2025 08:43:42 INFO]: Test accuracy: {
    "score": -0.9973558694074905,
    "rmse": 0.9973558694074905
}
[08/28/2025 08:43:42 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_122",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9973558694074905,
        "rmse": 0.9973558694074905
    },
    "train_stats": {
        "score": -1.0157726441409352,
        "rmse": 1.0157726441409352
    },
    "val_stats": {
        "score": -0.8861927183964751,
        "rmse": 0.8861927183964751
    }
}
[08/28/2025 08:43:42 INFO]: Procewss finished for trial maddest-Elbert_trial_122
[08/28/2025 08:43:42 INFO]: 
_________________________________________________

[08/28/2025 08:43:42 INFO]: train_net_for_optune.py main() running.
[08/28/2025 08:43:42 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 1.9103462337571389
  attention_dropout: 0.19089996092829792
  ffn_dropout: 0.19089996092829792
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.7449164967457107e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_131

[08/28/2025 08:43:42 INFO]: This ft_transformer has 11.638 million parameters.
[08/28/2025 08:43:42 INFO]: Training will start at epoch 0.
[08/28/2025 08:43:42 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 08:44:08 INFO]: Training loss at epoch 26: 0.8660385012626648
[08/28/2025 08:44:10 INFO]: Training loss at epoch 1: 1.351929485797882
[08/28/2025 08:44:50 INFO]: Training stats: {
    "score": -1.000721966749948,
    "rmse": 1.000721966749948
}
[08/28/2025 08:44:50 INFO]: Val stats: {
    "score": -0.9051809143515905,
    "rmse": 0.9051809143515905
}
[08/28/2025 08:44:50 INFO]: Test stats: {
    "score": -0.9738935551331118,
    "rmse": 0.9738935551331118
}
[08/28/2025 08:44:53 INFO]: Training loss at epoch 29: 0.9854785203933716
[08/28/2025 08:44:53 INFO]: Training loss at epoch 14: 0.8553022146224976
[08/28/2025 08:45:05 INFO]: Training loss at epoch 23: 0.9803840816020966
[08/28/2025 08:45:29 INFO]: Training loss at epoch 29: 0.7979250550270081
[08/28/2025 08:45:38 INFO]: Training stats: {
    "score": -0.9700819088277743,
    "rmse": 0.9700819088277743
}
[08/28/2025 08:45:38 INFO]: Val stats: {
    "score": -0.8982439969547991,
    "rmse": 0.8982439969547991
}
[08/28/2025 08:45:38 INFO]: Test stats: {
    "score": -0.9715124066877052,
    "rmse": 0.9715124066877052
}
[08/28/2025 08:45:58 INFO]: Training loss at epoch 38: 0.8580246567726135
[08/28/2025 08:46:09 INFO]: Training stats: {
    "score": -0.997434193976311,
    "rmse": 0.997434193976311
}
[08/28/2025 08:46:09 INFO]: Val stats: {
    "score": -0.9025346584139584,
    "rmse": 0.9025346584139584
}
[08/28/2025 08:46:09 INFO]: Test stats: {
    "score": -0.9721676974112764,
    "rmse": 0.9721676974112764
}
[08/28/2025 08:46:32 INFO]: Training loss at epoch 4: 0.9637852609157562
[08/28/2025 08:46:55 INFO]: Training loss at epoch 14: 1.1755943298339844
[08/28/2025 08:47:13 INFO]: Training accuracy: {
    "score": -1.0032399830755407,
    "rmse": 1.0032399830755407
}
[08/28/2025 08:47:13 INFO]: Val accuracy: {
    "score": -0.8780302216131838,
    "rmse": 0.8780302216131838
}
[08/28/2025 08:47:13 INFO]: Test accuracy: {
    "score": -0.9997321919650932,
    "rmse": 0.9997321919650932
}
[08/28/2025 08:47:13 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_104",
    "best_epoch": 14,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9997321919650932,
        "rmse": 0.9997321919650932
    },
    "train_stats": {
        "score": -1.0032399830755407,
        "rmse": 1.0032399830755407
    },
    "val_stats": {
        "score": -0.8780302216131838,
        "rmse": 0.8780302216131838
    }
}
[08/28/2025 08:47:13 INFO]: Procewss finished for trial maddest-Elbert_trial_104
[08/28/2025 08:47:13 INFO]: 
_________________________________________________

[08/28/2025 08:47:13 INFO]: train_net_for_optune.py main() running.
[08/28/2025 08:47:13 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 1.8789090754455573
  attention_dropout: 0.1872494377327534
  ffn_dropout: 0.1872494377327534
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.5245266403738274e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_132

[08/28/2025 08:47:13 INFO]: This ft_transformer has 11.528 million parameters.
[08/28/2025 08:47:13 INFO]: Training will start at epoch 0.
[08/28/2025 08:47:13 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 08:47:36 INFO]: New best epoch, val score: -0.8828169305761373
[08/28/2025 08:47:36 INFO]: Saving model to: maddest-Elbert_trial_127/model_best.pth
[08/28/2025 08:47:38 INFO]: Training loss at epoch 28: 1.0025128424167633
[08/28/2025 08:47:58 INFO]: Training loss at epoch 17: 0.9989235699176788
[08/28/2025 08:48:08 INFO]: Training loss at epoch 11: 0.9855847954750061
[08/28/2025 08:48:09 INFO]: Training loss at epoch 28: 1.0909011363983154
[08/28/2025 08:48:13 INFO]: Training loss at epoch 27: 1.1710264384746552
[08/28/2025 08:48:48 INFO]: Training stats: {
    "score": -0.9220820930140426,
    "rmse": 0.9220820930140426
}
[08/28/2025 08:48:48 INFO]: Val stats: {
    "score": -0.9448213690587702,
    "rmse": 0.9448213690587702
}
[08/28/2025 08:48:48 INFO]: Test stats: {
    "score": -0.9890864974395561,
    "rmse": 0.9890864974395561
}
[08/28/2025 08:48:50 INFO]: Training loss at epoch 31: 1.1823899149894714
[08/28/2025 08:49:08 INFO]: Training loss at epoch 0: 1.4383647441864014
[08/28/2025 08:49:36 INFO]: Training loss at epoch 47: 0.5591486692428589
[08/28/2025 08:49:37 INFO]: Running Final Evaluation...
[08/28/2025 08:49:49 INFO]: New best epoch, val score: -0.9419630738481158
[08/28/2025 08:49:49 INFO]: Saving model to: maddest-Elbert_trial_131/model_best.pth
[08/28/2025 08:49:52 INFO]: Training loss at epoch 30: 0.9169971942901611
[08/28/2025 08:49:56 INFO]: Training loss at epoch 2: 1.4504671692848206
[08/28/2025 08:50:37 INFO]: Training loss at epoch 39: 1.0322949290275574
[08/28/2025 08:51:01 INFO]: Training loss at epoch 15: 1.352333515882492
[08/28/2025 08:51:15 INFO]: Training loss at epoch 24: 1.295877754688263
[08/28/2025 08:51:35 INFO]: Training loss at epoch 35: 1.0496540665626526
[08/28/2025 08:51:38 INFO]: Training loss at epoch 5: 0.8967080116271973
[08/28/2025 08:52:03 INFO]: Training accuracy: {
    "score": -1.0160290083889874,
    "rmse": 1.0160290083889874
}
[08/28/2025 08:52:03 INFO]: Val accuracy: {
    "score": -0.8697344028163297,
    "rmse": 0.8697344028163297
}
[08/28/2025 08:52:03 INFO]: Test accuracy: {
    "score": -0.9938973394812112,
    "rmse": 0.9938973394812112
}
[08/28/2025 08:52:03 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_116",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9938973394812112,
        "rmse": 0.9938973394812112
    },
    "train_stats": {
        "score": -1.0160290083889874,
        "rmse": 1.0160290083889874
    },
    "val_stats": {
        "score": -0.8697344028163297,
        "rmse": 0.8697344028163297
    }
}
[08/28/2025 08:52:03 INFO]: Procewss finished for trial maddest-Elbert_trial_116
[08/28/2025 08:52:04 INFO]: 
_________________________________________________

[08/28/2025 08:52:04 INFO]: train_net_for_optune.py main() running.
[08/28/2025 08:52:04 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 1.170192037136837
  attention_dropout: 0.22959528023581516
  ffn_dropout: 0.22959528023581516
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0002528012317064488
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_133

[08/28/2025 08:52:04 INFO]: This ft_transformer has 15.962 million parameters.
[08/28/2025 08:52:04 INFO]: Training will start at epoch 0.
[08/28/2025 08:52:04 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 08:52:16 INFO]: Training stats: {
    "score": -0.9897146331870667,
    "rmse": 0.9897146331870667
}
[08/28/2025 08:52:16 INFO]: Val stats: {
    "score": -0.9029758163958782,
    "rmse": 0.9029758163958782
}
[08/28/2025 08:52:16 INFO]: Test stats: {
    "score": -0.9825283657290784,
    "rmse": 0.9825283657290784
}
[08/28/2025 08:52:18 INFO]: Training loss at epoch 28: 0.9238774478435516
[08/28/2025 08:52:36 INFO]: Training loss at epoch 29: 0.8726992011070251
[08/28/2025 08:52:36 INFO]: Training loss at epoch 15: 1.1178969144821167
[08/28/2025 08:52:39 INFO]: Training loss at epoch 0: 0.935303658246994
[08/28/2025 08:52:56 INFO]: Training loss at epoch 30: 1.0479460954666138
[08/28/2025 08:53:12 INFO]: Training loss at epoch 12: 1.292120635509491
[08/28/2025 08:53:22 INFO]: New best epoch, val score: -0.9361280466257772
[08/28/2025 08:53:22 INFO]: Saving model to: maddest-Elbert_trial_132/model_best.pth
[08/28/2025 08:53:36 INFO]: Training loss at epoch 18: 1.1116136610507965
[08/28/2025 08:53:38 INFO]: Training loss at epoch 31: 0.8226735591888428
[08/28/2025 08:54:17 INFO]: Training stats: {
    "score": -0.9592434438584749,
    "rmse": 0.9592434438584749
}
[08/28/2025 08:54:17 INFO]: Val stats: {
    "score": -0.9028145962327401,
    "rmse": 0.9028145962327401
}
[08/28/2025 08:54:17 INFO]: Test stats: {
    "score": -0.977034133436227,
    "rmse": 0.977034133436227
}
[08/28/2025 08:55:17 INFO]: Training loss at epoch 1: 1.1183218359947205
[08/28/2025 08:55:32 INFO]: Training loss at epoch 30: 0.9003471732139587
[08/28/2025 08:55:43 INFO]: Training loss at epoch 3: 0.9366917610168457
[08/28/2025 08:56:19 INFO]: Training loss at epoch 29: 1.1369059681892395
[08/28/2025 08:56:27 INFO]: Training loss at epoch 29: 0.9902677834033966
[08/28/2025 08:56:45 INFO]: Training loss at epoch 6: 0.9228070676326752
[08/28/2025 08:56:52 INFO]: Training loss at epoch 40: 1.0728946924209595
[08/28/2025 08:57:06 INFO]: Training loss at epoch 16: 0.9167706072330475
[08/28/2025 08:57:22 INFO]: Training loss at epoch 25: 1.3086068332195282
[08/28/2025 08:57:23 INFO]: Training loss at epoch 32: 1.1089829504489899
[08/28/2025 08:57:39 INFO]: Training stats: {
    "score": -1.0061051981827451,
    "rmse": 1.0061051981827451
}
[08/28/2025 08:57:39 INFO]: Val stats: {
    "score": -0.8852877291223707,
    "rmse": 0.8852877291223707
}
[08/28/2025 08:57:39 INFO]: Test stats: {
    "score": -0.9847570034271134,
    "rmse": 0.9847570034271134
}
[08/28/2025 08:58:12 INFO]: Training loss at epoch 16: 0.9735937416553497
[08/28/2025 08:58:16 INFO]: Training loss at epoch 13: 1.1973241567611694
[08/28/2025 08:58:40 INFO]: Training loss at epoch 30: 0.9167982339859009
[08/28/2025 08:58:43 INFO]: Training loss at epoch 0: 1.69515722990036
[08/28/2025 08:58:47 INFO]: Training loss at epoch 1: 0.9491647481918335
[08/28/2025 08:59:11 INFO]: Training loss at epoch 30: 0.8697359561920166
[08/28/2025 08:59:15 INFO]: Training loss at epoch 19: 1.0790260136127472
[08/28/2025 08:59:16 INFO]: Training stats: {
    "score": -0.9968113992043252,
    "rmse": 0.9968113992043252
}
[08/28/2025 08:59:16 INFO]: Val stats: {
    "score": -0.9158216428957726,
    "rmse": 0.9158216428957726
}
[08/28/2025 08:59:16 INFO]: Test stats: {
    "score": -0.9726801431603448,
    "rmse": 0.9726801431603448
}
[08/28/2025 08:59:31 INFO]: New best epoch, val score: -0.8820201252294724
[08/28/2025 08:59:31 INFO]: Saving model to: maddest-Elbert_trial_132/model_best.pth
[08/28/2025 08:59:36 INFO]: New best epoch, val score: -1.0053540020935905
[08/28/2025 08:59:36 INFO]: Saving model to: maddest-Elbert_trial_133/model_best.pth
[08/28/2025 08:59:56 INFO]: Training loss at epoch 36: 0.9737435281276703
[08/28/2025 09:00:42 INFO]: Training loss at epoch 48: 0.5135389119386673
[08/28/2025 09:01:03 INFO]: Training loss at epoch 31: 0.9760811030864716
[08/28/2025 09:01:08 INFO]: Training stats: {
    "score": -0.9857732899552353,
    "rmse": 0.9857732899552353
}
[08/28/2025 09:01:08 INFO]: Val stats: {
    "score": -0.9256033741446144,
    "rmse": 0.9256033741446144
}
[08/28/2025 09:01:08 INFO]: Test stats: {
    "score": -0.9706972453511383,
    "rmse": 0.9706972453511383
}
[08/28/2025 09:01:09 INFO]: Training loss at epoch 33: 0.88710156083107
[08/28/2025 09:01:29 INFO]: Training loss at epoch 2: 1.717219054698944
[08/28/2025 09:01:30 INFO]: Training loss at epoch 4: 1.1236651539802551
[08/28/2025 09:01:34 INFO]: Training loss at epoch 41: 1.0659133195877075
[08/28/2025 09:01:44 INFO]: Training loss at epoch 30: 1.0370448231697083
[08/28/2025 09:01:53 INFO]: Training loss at epoch 7: 1.0359826385974884
[08/28/2025 09:03:17 INFO]: Training loss at epoch 17: 0.7795476019382477
[08/28/2025 09:03:21 INFO]: Training loss at epoch 14: 1.2652621269226074
[08/28/2025 09:03:32 INFO]: Training loss at epoch 26: 0.9864616096019745
[08/28/2025 09:03:48 INFO]: Training loss at epoch 17: 0.9901629388332367
[08/28/2025 09:04:05 INFO]: Training loss at epoch 31: 0.9383683800697327
[08/28/2025 09:04:58 INFO]: Training loss at epoch 2: 0.8301591575145721
[08/28/2025 09:04:59 INFO]: Training loss at epoch 34: 1.0008293390274048
[08/28/2025 09:05:28 INFO]: Training loss at epoch 31: 0.8342115879058838
[08/28/2025 09:05:48 INFO]: Training loss at epoch 31: 0.9958973824977875
[08/28/2025 09:06:15 INFO]: Training loss at epoch 42: 1.1596090197563171
[08/28/2025 09:06:18 INFO]: Training loss at epoch 1: 3.9321815967559814
[08/28/2025 09:06:47 INFO]: Training loss at epoch 20: 1.0771215558052063
[08/28/2025 09:07:00 INFO]: Training loss at epoch 8: 1.042333722114563
[08/28/2025 09:07:16 INFO]: Training loss at epoch 5: 1.0162864327430725
[08/28/2025 09:07:34 INFO]: Training loss at epoch 30: 0.9122067391872406
[08/28/2025 09:07:39 INFO]: Training loss at epoch 3: 1.1583828330039978
[08/28/2025 09:08:16 INFO]: Training loss at epoch 37: 0.9246779382228851
[08/28/2025 09:08:23 INFO]: New best epoch, val score: -0.9036384220799064
[08/28/2025 09:08:23 INFO]: Saving model to: maddest-Elbert_trial_131/model_best.pth
[08/28/2025 09:08:24 INFO]: Training loss at epoch 15: 1.100756049156189
[08/28/2025 09:08:33 INFO]: Training loss at epoch 31: 1.015295922756195
[08/28/2025 09:08:42 INFO]: Training loss at epoch 35: 0.9363538324832916
[08/28/2025 09:09:00 INFO]: Training loss at epoch 32: 0.9790068864822388
[08/28/2025 09:09:13 INFO]: Training loss at epoch 32: 0.9422063231468201
[08/28/2025 09:09:23 INFO]: Training loss at epoch 18: 0.9393337965011597
[08/28/2025 09:09:26 INFO]: Training loss at epoch 18: 1.2128273844718933
[08/28/2025 09:09:39 INFO]: Training loss at epoch 27: 1.0877057313919067
[08/28/2025 09:09:52 INFO]: Training loss at epoch 32: 1.1637293994426727
[08/28/2025 09:10:56 INFO]: Training loss at epoch 43: 1.0653173923492432
[08/28/2025 09:11:07 INFO]: Training loss at epoch 3: 0.9715880155563354
[08/28/2025 09:11:48 INFO]: Training loss at epoch 49: 0.6348046958446503
[08/28/2025 09:12:07 INFO]: Training loss at epoch 9: 1.0043308734893799
[08/28/2025 09:12:26 INFO]: Training loss at epoch 21: 1.1375209391117096
[08/28/2025 09:12:30 INFO]: Training loss at epoch 36: 0.9755696058273315
[08/28/2025 09:13:03 INFO]: Training loss at epoch 6: 1.368706375360489
[08/28/2025 09:13:30 INFO]: Training loss at epoch 16: 1.20093834400177
[08/28/2025 09:13:51 INFO]: Training loss at epoch 2: 2.619982957839966
[08/28/2025 09:13:52 INFO]: Training loss at epoch 4: 1.119163691997528
[08/28/2025 09:13:54 INFO]: Training loss at epoch 33: 1.0508528053760529
[08/28/2025 09:13:54 INFO]: Training stats: {
    "score": -0.9803859104156576,
    "rmse": 0.9803859104156576
}
[08/28/2025 09:13:54 INFO]: Val stats: {
    "score": -0.9703393832386824,
    "rmse": 0.9703393832386824
}
[08/28/2025 09:13:54 INFO]: Test stats: {
    "score": -0.9786730140893639,
    "rmse": 0.9786730140893639
}
[08/28/2025 09:13:56 INFO]: Training loss at epoch 33: 0.9141442179679871
[08/28/2025 09:14:29 INFO]: Running Final Evaluation...
[08/28/2025 09:14:58 INFO]: Training loss at epoch 19: 1.1324719786643982
[08/28/2025 09:15:23 INFO]: Training loss at epoch 32: 1.0008111596107483
[08/28/2025 09:15:30 INFO]: Training stats: {
    "score": -0.6814262752915246,
    "rmse": 0.6814262752915246
}
[08/28/2025 09:15:30 INFO]: Val stats: {
    "score": -0.9162211256296848,
    "rmse": 0.9162211256296848
}
[08/28/2025 09:15:30 INFO]: Test stats: {
    "score": -1.132390425245285,
    "rmse": 1.132390425245285
}
[08/28/2025 09:15:33 INFO]: Training loss at epoch 19: 0.8756731450557709
[08/28/2025 09:15:36 INFO]: Training loss at epoch 44: 1.1152328848838806
[08/28/2025 09:15:45 INFO]: Training loss at epoch 28: 0.87381511926651
[08/28/2025 09:15:55 INFO]: Training loss at epoch 31: 0.8766120374202728
[08/28/2025 09:16:12 INFO]: Training accuracy: {
    "score": -1.032330992251342,
    "rmse": 1.032330992251342
}
[08/28/2025 09:16:12 INFO]: Val accuracy: {
    "score": -0.8613564224430869,
    "rmse": 0.8613564224430869
}
[08/28/2025 09:16:12 INFO]: Test accuracy: {
    "score": -1.019625764289784,
    "rmse": 1.019625764289784
}
[08/28/2025 09:16:12 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_121",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.019625764289784,
        "rmse": 1.019625764289784
    },
    "train_stats": {
        "score": -1.032330992251342,
        "rmse": 1.032330992251342
    },
    "val_stats": {
        "score": -0.8613564224430869,
        "rmse": 0.8613564224430869
    }
}
[08/28/2025 09:16:12 INFO]: Procewss finished for trial maddest-Elbert_trial_121
[08/28/2025 09:16:12 INFO]: 
_________________________________________________

[08/28/2025 09:16:12 INFO]: train_net_for_optune.py main() running.
[08/28/2025 09:16:12 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 16
  d_ffn_factor: 1.909025401656606
  attention_dropout: 0.20517640447784602
  ffn_dropout: 0.20517640447784602
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.3240845062155263e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_134

[08/28/2025 09:16:12 INFO]: This ft_transformer has 20.613 million parameters.
[08/28/2025 09:16:12 INFO]: Training will start at epoch 0.
[08/28/2025 09:16:12 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 09:16:15 INFO]: Training loss at epoch 37: 1.1593316793441772
[08/28/2025 09:16:35 INFO]: Training loss at epoch 38: 0.8948813080787659
[08/28/2025 09:16:53 INFO]: Training stats: {
    "score": -1.029487482265427,
    "rmse": 1.029487482265427
}
[08/28/2025 09:16:53 INFO]: Val stats: {
    "score": -1.0268980399670034,
    "rmse": 1.0268980399670034
}
[08/28/2025 09:16:53 INFO]: Test stats: {
    "score": -1.0031567906561507,
    "rmse": 1.0031567906561507
}
[08/28/2025 09:17:16 INFO]: Training loss at epoch 4: 1.0103131830692291
[08/28/2025 09:17:25 INFO]: Training loss at epoch 33: 0.9378413558006287
[08/28/2025 09:17:38 INFO]: Training stats: {
    "score": -0.9789778051455442,
    "rmse": 0.9789778051455442
}
[08/28/2025 09:17:38 INFO]: Val stats: {
    "score": -0.8923757107639191,
    "rmse": 0.8923757107639191
}
[08/28/2025 09:17:38 INFO]: Test stats: {
    "score": -0.9649468201400034,
    "rmse": 0.9649468201400034
}
[08/28/2025 09:17:55 INFO]: Training loss at epoch 34: 0.9281643927097321
[08/28/2025 09:18:02 INFO]: Training loss at epoch 22: 0.9200990200042725
[08/28/2025 09:18:28 INFO]: Training loss at epoch 32: 0.8136824369430542
[08/28/2025 09:18:36 INFO]: Training loss at epoch 17: 1.0931300818920135
[08/28/2025 09:18:48 INFO]: Training loss at epoch 7: 0.9869085848331451
[08/28/2025 09:19:01 INFO]: Training loss at epoch 10: 0.9851455390453339
[08/28/2025 09:19:13 INFO]: New best epoch, val score: -0.8809709756093139
[08/28/2025 09:19:13 INFO]: Saving model to: maddest-Elbert_trial_128/model_best.pth
[08/28/2025 09:19:39 INFO]: New best epoch, val score: -0.8993647018166181
[08/28/2025 09:19:39 INFO]: Saving model to: maddest-Elbert_trial_129/model_best.pth
[08/28/2025 09:20:01 INFO]: Training loss at epoch 38: 1.0296765565872192
[08/28/2025 09:20:02 INFO]: Training loss at epoch 5: 1.1464614570140839
[08/28/2025 09:20:17 INFO]: Training loss at epoch 45: 1.1422969102859497
[08/28/2025 09:21:24 INFO]: Training loss at epoch 3: 1.4527197480201721
[08/28/2025 09:21:52 INFO]: Training loss at epoch 29: 0.9466227889060974
[08/28/2025 09:21:57 INFO]: Training loss at epoch 35: 1.0240397453308105
[08/28/2025 09:22:25 INFO]: Training loss at epoch 20: 1.0438626408576965
[08/28/2025 09:23:20 INFO]: Training loss at epoch 5: 0.9233039021492004
[08/28/2025 09:23:36 INFO]: Training loss at epoch 23: 1.0584296584129333
[08/28/2025 09:23:38 INFO]: Training loss at epoch 18: 1.1397747993469238
[08/28/2025 09:23:40 INFO]: Training loss at epoch 20: 1.0874192714691162
[08/28/2025 09:23:41 INFO]: Training loss at epoch 39: 1.0537219047546387
[08/28/2025 09:23:51 INFO]: Training stats: {
    "score": -0.9806364817418174,
    "rmse": 0.9806364817418174
}
[08/28/2025 09:23:51 INFO]: Val stats: {
    "score": -0.9720654021016584,
    "rmse": 0.9720654021016584
}
[08/28/2025 09:23:51 INFO]: Test stats: {
    "score": -0.9928294466231542,
    "rmse": 0.9928294466231542
}
[08/28/2025 09:24:05 INFO]: Training loss at epoch 11: 1.1964203417301178
[08/28/2025 09:24:11 INFO]: Training loss at epoch 32: 0.9899192452430725
[08/28/2025 09:24:33 INFO]: Training loss at epoch 8: 0.9939910769462585
[08/28/2025 09:24:43 INFO]: New best epoch, val score: -0.8826343180295154
[08/28/2025 09:24:43 INFO]: Saving model to: maddest-Elbert_trial_129/model_best.pth
[08/28/2025 09:24:53 INFO]: Training loss at epoch 39: 1.2090309262275696
[08/28/2025 09:24:56 INFO]: Training loss at epoch 46: 0.8858720660209656
[08/28/2025 09:25:00 INFO]: Training stats: {
    "score": -0.9915919439977232,
    "rmse": 0.9915919439977232
}
[08/28/2025 09:25:00 INFO]: Val stats: {
    "score": -0.9293440982195732,
    "rmse": 0.9293440982195732
}
[08/28/2025 09:25:00 INFO]: Test stats: {
    "score": -0.9691365112521259,
    "rmse": 0.9691365112521259
}
[08/28/2025 09:25:19 INFO]: Training loss at epoch 33: 0.7955629527568817
[08/28/2025 09:25:31 INFO]: Training loss at epoch 34: 0.9233889579772949
[08/28/2025 09:25:33 INFO]: Training loss at epoch 0: 0.9811291694641113
[08/28/2025 09:26:00 INFO]: Training loss at epoch 36: 0.9237276911735535
[08/28/2025 09:26:12 INFO]: Training loss at epoch 6: 1.578230857849121
[08/28/2025 09:26:28 INFO]: Running Final Evaluation...
[08/28/2025 09:26:36 INFO]: Training loss at epoch 50: 0.5595916211605072
[08/28/2025 09:26:50 INFO]: New best epoch, val score: -0.8664757168420079
[08/28/2025 09:26:50 INFO]: Saving model to: maddest-Elbert_trial_134/model_best.pth
[08/28/2025 09:27:39 INFO]: Training stats: {
    "score": -0.995071752179987,
    "rmse": 0.995071752179987
}
[08/28/2025 09:27:39 INFO]: Val stats: {
    "score": -0.9230464709079962,
    "rmse": 0.9230464709079962
}
[08/28/2025 09:27:39 INFO]: Test stats: {
    "score": -0.9715475314501452,
    "rmse": 0.9715475314501452
}
[08/28/2025 09:27:57 INFO]: Training accuracy: {
    "score": -1.0354350272766193,
    "rmse": 1.0354350272766193
}
[08/28/2025 09:27:57 INFO]: Val accuracy: {
    "score": -0.8772052489115821,
    "rmse": 0.8772052489115821
}
[08/28/2025 09:27:57 INFO]: Test accuracy: {
    "score": -1.0103120124893339,
    "rmse": 1.0103120124893339
}
[08/28/2025 09:27:57 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_124",
    "best_epoch": 5,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0103120124893339,
        "rmse": 1.0103120124893339
    },
    "train_stats": {
        "score": -1.0354350272766193,
        "rmse": 1.0354350272766193
    },
    "val_stats": {
        "score": -0.8772052489115821,
        "rmse": 0.8772052489115821
    }
}
[08/28/2025 09:27:57 INFO]: Procewss finished for trial maddest-Elbert_trial_124
[08/28/2025 09:27:57 INFO]: 
_________________________________________________

[08/28/2025 09:27:57 INFO]: train_net_for_optune.py main() running.
[08/28/2025 09:27:57 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 16
  d_ffn_factor: 1.8911228519317187
  attention_dropout: 0.21772965569447433
  ffn_dropout: 0.21772965569447433
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.3871664184853997e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_135

[08/28/2025 09:27:58 INFO]: This ft_transformer has 20.503 million parameters.
[08/28/2025 09:27:58 INFO]: Training will start at epoch 0.
[08/28/2025 09:27:58 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 09:28:02 INFO]: Training loss at epoch 21: 1.0309539437294006
[08/28/2025 09:28:18 INFO]: Training loss at epoch 33: 0.8937229812145233
[08/28/2025 09:28:43 INFO]: Training loss at epoch 40: 0.906845897436142
[08/28/2025 09:28:44 INFO]: Training loss at epoch 19: 1.055929720401764
[08/28/2025 09:28:51 INFO]: Training loss at epoch 4: 1.5158370733261108
[08/28/2025 09:29:10 INFO]: Running Final Evaluation...
[08/28/2025 09:29:13 INFO]: Training loss at epoch 12: 1.032953917980194
[08/28/2025 09:29:15 INFO]: Training loss at epoch 24: 0.9680070281028748
[08/28/2025 09:29:30 INFO]: Training loss at epoch 6: 0.8531297445297241
[08/28/2025 09:29:35 INFO]: Training loss at epoch 47: 1.0031029284000397
[08/28/2025 09:29:44 INFO]: New best epoch, val score: -0.9644083149564247
[08/28/2025 09:29:44 INFO]: Saving model to: maddest-Elbert_trial_133/model_best.pth
[08/28/2025 09:29:50 INFO]: Training loss at epoch 21: 0.9885319471359253
[08/28/2025 09:30:00 INFO]: Training loss at epoch 30: 0.8645962476730347
[08/28/2025 09:30:21 INFO]: Training loss at epoch 9: 1.0125264823436737
[08/28/2025 09:30:27 INFO]: Training accuracy: {
    "score": -1.043762471020653,
    "rmse": 1.043762471020653
}
[08/28/2025 09:30:27 INFO]: Val accuracy: {
    "score": -0.879790821119756,
    "rmse": 0.879790821119756
}
[08/28/2025 09:30:27 INFO]: Test accuracy: {
    "score": -1.0196863851534637,
    "rmse": 1.0196863851534637
}
[08/28/2025 09:30:27 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_123",
    "best_epoch": 9,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0196863851534637,
        "rmse": 1.0196863851534637
    },
    "train_stats": {
        "score": -1.043762471020653,
        "rmse": 1.043762471020653
    },
    "val_stats": {
        "score": -0.879790821119756,
        "rmse": 0.879790821119756
    }
}
[08/28/2025 09:30:27 INFO]: Procewss finished for trial maddest-Elbert_trial_123
[08/28/2025 09:30:27 INFO]: 
_________________________________________________

[08/28/2025 09:30:27 INFO]: train_net_for_optune.py main() running.
[08/28/2025 09:30:27 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 16
  d_ffn_factor: 0.7227482335752079
  attention_dropout: 0.2147650376840122
  ffn_dropout: 0.2147650376840122
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.4135190434743727e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_136

[08/28/2025 09:30:28 INFO]: This ft_transformer has 13.145 million parameters.
[08/28/2025 09:30:28 INFO]: Training will start at epoch 0.
[08/28/2025 09:30:28 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 09:30:29 INFO]: Training stats: {
    "score": -1.0394269735512536,
    "rmse": 1.0394269735512536
}
[08/28/2025 09:30:29 INFO]: Val stats: {
    "score": -0.8802907803801565,
    "rmse": 0.8802907803801565
}
[08/28/2025 09:30:29 INFO]: Test stats: {
    "score": -1.0110214066126915,
    "rmse": 1.0110214066126915
}
[08/28/2025 09:31:06 INFO]: New best epoch, val score: -0.8802907803801565
[08/28/2025 09:31:06 INFO]: Saving model to: maddest-Elbert_trial_128/model_best.pth
[08/28/2025 09:32:15 INFO]: Training stats: {
    "score": -0.9848875146086908,
    "rmse": 0.9848875146086908
}
[08/28/2025 09:32:15 INFO]: Val stats: {
    "score": -0.9073299523629378,
    "rmse": 0.9073299523629378
}
[08/28/2025 09:32:15 INFO]: Test stats: {
    "score": -0.9696914532417792,
    "rmse": 0.9696914532417792
}
[08/28/2025 09:32:20 INFO]: Training loss at epoch 7: 1.212776780128479
[08/28/2025 09:32:30 INFO]: Training loss at epoch 33: 0.9196964204311371
[08/28/2025 09:33:34 INFO]: Training loss at epoch 22: 1.1606227457523346
[08/28/2025 09:33:35 INFO]: Training loss at epoch 35: 0.7538574039936066
[08/28/2025 09:34:12 INFO]: Training loss at epoch 48: 1.0020424127578735
[08/28/2025 09:34:16 INFO]: Training loss at epoch 13: 0.8496992588043213
[08/28/2025 09:34:47 INFO]: Running Final Evaluation...
[08/28/2025 09:34:48 INFO]: Training loss at epoch 25: 0.9886392951011658
[08/28/2025 09:34:53 INFO]: New best epoch, val score: -0.8808286303917306
[08/28/2025 09:34:53 INFO]: Saving model to: maddest-Elbert_trial_129/model_best.pth
[08/28/2025 09:35:06 INFO]: Training loss at epoch 34: 0.9199426174163818
[08/28/2025 09:35:33 INFO]: Training loss at epoch 20: 1.1108973622322083
[08/28/2025 09:35:34 INFO]: Training loss at epoch 7: 0.9416800737380981
[08/28/2025 09:35:52 INFO]: Training loss at epoch 40: 0.8763562440872192
[08/28/2025 09:35:53 INFO]: Training loss at epoch 22: 1.0774131119251251
[08/28/2025 09:36:02 INFO]: Training loss at epoch 31: 0.9304438829421997
[08/28/2025 09:36:02 INFO]: Training loss at epoch 1: 0.9086636006832123
[08/28/2025 09:36:18 INFO]: Training loss at epoch 5: 0.8788898289203644
[08/28/2025 09:36:28 INFO]: Training accuracy: {
    "score": -1.028410089734413,
    "rmse": 1.028410089734413
}
[08/28/2025 09:36:28 INFO]: Val accuracy: {
    "score": -0.876909278878691,
    "rmse": 0.876909278878691
}
[08/28/2025 09:36:28 INFO]: Test accuracy: {
    "score": -1.0159719887660112,
    "rmse": 1.0159719887660112
}
[08/28/2025 09:36:28 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_118",
    "best_epoch": 17,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0159719887660112,
        "rmse": 1.0159719887660112
    },
    "train_stats": {
        "score": -1.028410089734413,
        "rmse": 1.028410089734413
    },
    "val_stats": {
        "score": -0.876909278878691,
        "rmse": 0.876909278878691
    }
}
[08/28/2025 09:36:28 INFO]: Procewss finished for trial maddest-Elbert_trial_118
[08/28/2025 09:36:28 INFO]: 
_________________________________________________

[08/28/2025 09:36:28 INFO]: train_net_for_optune.py main() running.
[08/28/2025 09:36:28 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 16
  d_ffn_factor: 0.7638629359771967
  attention_dropout: 0.19274842579800147
  ffn_dropout: 0.19274842579800147
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.3478255474665969e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_137

[08/28/2025 09:36:28 INFO]: This ft_transformer has 13.403 million parameters.
[08/28/2025 09:36:28 INFO]: Training will start at epoch 0.
[08/28/2025 09:36:28 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 09:37:08 INFO]: Training loss at epoch 0: 1.1811913847923279
[08/28/2025 09:37:10 INFO]: Training loss at epoch 0: 1.2222344279289246
[08/28/2025 09:37:11 INFO]: New best epoch, val score: -0.9598963308343095
[08/28/2025 09:37:11 INFO]: Saving model to: maddest-Elbert_trial_133/model_best.pth
[08/28/2025 09:37:33 INFO]: Training loss at epoch 51: 0.5907957553863525
[08/28/2025 09:37:56 INFO]: Training loss at epoch 10: 0.9499054253101349
[08/28/2025 09:38:05 INFO]: New best epoch, val score: -0.9141423548721749
[08/28/2025 09:38:05 INFO]: Saving model to: maddest-Elbert_trial_136/model_best.pth
[08/28/2025 09:38:08 INFO]: Training loss at epoch 34: 0.6362990885972977
[08/28/2025 09:38:23 INFO]: New best epoch, val score: -1.064968288653763
[08/28/2025 09:38:23 INFO]: Saving model to: maddest-Elbert_trial_135/model_best.pth
[08/28/2025 09:38:27 INFO]: Training loss at epoch 8: 1.294433355331421
[08/28/2025 09:39:05 INFO]: Training loss at epoch 23: 1.1687746942043304
[08/28/2025 09:39:23 INFO]: Training loss at epoch 14: 0.9004216194152832
[08/28/2025 09:40:24 INFO]: Training loss at epoch 26: 0.9260674715042114
[08/28/2025 09:40:36 INFO]: Training loss at epoch 21: 1.0948367714881897
[08/28/2025 09:40:46 INFO]: Training loss at epoch 34: 0.9255169034004211
[08/28/2025 09:41:40 INFO]: Training loss at epoch 36: 0.9948717355728149
[08/28/2025 09:41:41 INFO]: Training loss at epoch 8: 0.9502736330032349
[08/28/2025 09:42:02 INFO]: Training loss at epoch 23: 0.8642747402191162
[08/28/2025 09:42:09 INFO]: Training loss at epoch 32: 0.9251276552677155
[08/28/2025 09:43:12 INFO]: Training loss at epoch 0: 1.0864214301109314
[08/28/2025 09:43:36 INFO]: Training loss at epoch 11: 1.1779212355613708
[08/28/2025 09:43:42 INFO]: Training loss at epoch 6: 1.1117503643035889
[08/28/2025 09:44:03 INFO]: Training loss at epoch 41: 0.9629878401756287
[08/28/2025 09:44:05 INFO]: New best epoch, val score: -1.0438998210173152
[08/28/2025 09:44:05 INFO]: Saving model to: maddest-Elbert_trial_137/model_best.pth
[08/28/2025 09:44:24 INFO]: Training loss at epoch 15: 0.9574918150901794
[08/28/2025 09:44:30 INFO]: Training loss at epoch 9: 1.2614848613739014
[08/28/2025 09:44:34 INFO]: Training loss at epoch 24: 0.9974675178527832
[08/28/2025 09:44:34 INFO]: New best epoch, val score: -0.9553988089296951
[08/28/2025 09:44:34 INFO]: Saving model to: maddest-Elbert_trial_133/model_best.pth
[08/28/2025 09:44:44 INFO]: Training loss at epoch 1: 0.8210853338241577
[08/28/2025 09:44:52 INFO]: Training loss at epoch 35: 1.1326572895050049
[08/28/2025 09:45:35 INFO]: Training loss at epoch 22: 1.070589542388916
[08/28/2025 09:45:40 INFO]: New best epoch, val score: -0.9085498374305448
[08/28/2025 09:45:40 INFO]: Saving model to: maddest-Elbert_trial_136/model_best.pth
[08/28/2025 09:45:53 INFO]: Training loss at epoch 27: 0.8737921118736267
[08/28/2025 09:46:29 INFO]: Training loss at epoch 2: 1.0461651682853699
[08/28/2025 09:46:35 INFO]: Training stats: {
    "score": -1.1232656444013354,
    "rmse": 1.1232656444013354
}
[08/28/2025 09:46:35 INFO]: Val stats: {
    "score": -0.911706104503059,
    "rmse": 0.911706104503059
}
[08/28/2025 09:46:35 INFO]: Test stats: {
    "score": -1.101790114514628,
    "rmse": 1.101790114514628
}
[08/28/2025 09:47:30 INFO]: Training loss at epoch 1: 1.0598481893539429
[08/28/2025 09:47:42 INFO]: Training loss at epoch 9: 1.0059100091457367
[08/28/2025 09:47:54 INFO]: Training loss at epoch 35: 0.8271745145320892
[08/28/2025 09:48:02 INFO]: Training loss at epoch 24: 0.97284796833992
[08/28/2025 09:48:15 INFO]: Training loss at epoch 33: 1.2678460478782654
[08/28/2025 09:48:32 INFO]: Training loss at epoch 52: 0.45023465156555176
[08/28/2025 09:48:44 INFO]: New best epoch, val score: -0.9398237641813484
[08/28/2025 09:48:44 INFO]: Saving model to: maddest-Elbert_trial_135/model_best.pth
[08/28/2025 09:48:58 INFO]: Training loss at epoch 35: 0.9055413007736206
[08/28/2025 09:49:21 INFO]: Training loss at epoch 12: 1.0561279654502869
[08/28/2025 09:49:31 INFO]: Training loss at epoch 16: 1.2124269604682922
[08/28/2025 09:49:39 INFO]: Training loss at epoch 37: 1.1655689477920532
[08/28/2025 09:49:44 INFO]: Training stats: {
    "score": -0.9728502893048212,
    "rmse": 0.9728502893048212
}
[08/28/2025 09:49:44 INFO]: Val stats: {
    "score": -0.9174325425105873,
    "rmse": 0.9174325425105873
}
[08/28/2025 09:49:44 INFO]: Test stats: {
    "score": -0.962420662402851,
    "rmse": 0.962420662402851
}
[08/28/2025 09:50:09 INFO]: Training loss at epoch 25: 0.8097667396068573
[08/28/2025 09:50:37 INFO]: Training loss at epoch 23: 0.8577787280082703
[08/28/2025 09:50:57 INFO]: Training loss at epoch 1: 0.9424538314342499
[08/28/2025 09:51:12 INFO]: Training loss at epoch 7: 1.172719806432724
[08/28/2025 09:51:33 INFO]: Training loss at epoch 28: 0.9730352461338043
[08/28/2025 09:51:53 INFO]: New best epoch, val score: -0.9038203181427972
[08/28/2025 09:51:53 INFO]: Saving model to: maddest-Elbert_trial_137/model_best.pth
[08/28/2025 09:52:03 INFO]: New best epoch, val score: -0.9508519822879098
[08/28/2025 09:52:03 INFO]: Saving model to: maddest-Elbert_trial_133/model_best.pth
[08/28/2025 09:52:20 INFO]: Training loss at epoch 42: 1.0546453595161438
[08/28/2025 09:52:25 INFO]: Training loss at epoch 2: 0.9594978392124176
[08/28/2025 09:52:44 INFO]: Training loss at epoch 10: 1.0193443298339844
[08/28/2025 09:53:21 INFO]: New best epoch, val score: -0.901484936033297
[08/28/2025 09:53:21 INFO]: Saving model to: maddest-Elbert_trial_136/model_best.pth
[08/28/2025 09:53:28 INFO]: New best epoch, val score: -0.886074052734706
[08/28/2025 09:53:28 INFO]: Saving model to: maddest-Elbert_trial_131/model_best.pth
[08/28/2025 09:54:08 INFO]: Training loss at epoch 25: 1.0006938874721527
[08/28/2025 09:54:18 INFO]: Training loss at epoch 34: 0.7452785074710846
[08/28/2025 09:54:36 INFO]: Training loss at epoch 17: 0.960179328918457
[08/28/2025 09:54:44 INFO]: Training loss at epoch 36: 1.0237810909748077
[08/28/2025 09:55:01 INFO]: Running Final Evaluation...
[08/28/2025 09:55:03 INFO]: Training loss at epoch 13: 0.9503414630889893
[08/28/2025 09:55:39 INFO]: Training loss at epoch 24: 0.8854627907276154
[08/28/2025 09:55:40 INFO]: Training loss at epoch 26: 0.9522057771682739
[08/28/2025 09:55:49 INFO]: Training loss at epoch 10: 1.131603628396988
[08/28/2025 09:57:01 INFO]: Training loss at epoch 3: 1.3192945718765259
[08/28/2025 09:57:07 INFO]: Training loss at epoch 29: 0.9016694724559784
[08/28/2025 09:57:07 INFO]: Training accuracy: {
    "score": -1.0432260508730282,
    "rmse": 1.0432260508730282
}
[08/28/2025 09:57:07 INFO]: Val accuracy: {
    "score": -0.8724194445642698,
    "rmse": 0.8724194445642698
}
[08/28/2025 09:57:07 INFO]: Test accuracy: {
    "score": -1.0280421684999852,
    "rmse": 1.0280421684999852
}
[08/28/2025 09:57:07 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_119",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0280421684999852,
        "rmse": 1.0280421684999852
    },
    "train_stats": {
        "score": -1.0432260508730282,
        "rmse": 1.0432260508730282
    },
    "val_stats": {
        "score": -0.8724194445642698,
        "rmse": 0.8724194445642698
    }
}
[08/28/2025 09:57:07 INFO]: Procewss finished for trial maddest-Elbert_trial_119
[08/28/2025 09:57:07 INFO]: 
_________________________________________________

[08/28/2025 09:57:07 INFO]: train_net_for_optune.py main() running.
[08/28/2025 09:57:07 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 16
  d_ffn_factor: 0.7957784566241527
  attention_dropout: 0.19226605710878705
  ffn_dropout: 0.19226605710878705
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.6765814044862475e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_138

[08/28/2025 09:57:08 INFO]: This ft_transformer has 13.600 million parameters.
[08/28/2025 09:57:08 INFO]: Training will start at epoch 0.
[08/28/2025 09:57:08 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 09:57:13 INFO]: Training loss at epoch 36: 0.9374339282512665
[08/28/2025 09:57:44 INFO]: Training loss at epoch 36: 0.7138144671916962
[08/28/2025 09:57:45 INFO]: Training loss at epoch 38: 0.7972475588321686
[08/28/2025 09:57:55 INFO]: Training loss at epoch 2: 1.095580130815506
[08/28/2025 09:58:17 INFO]: New best epoch, val score: -0.8620221664105433
[08/28/2025 09:58:17 INFO]: Saving model to: maddest-Elbert_trial_134/model_best.pth
[08/28/2025 09:58:41 INFO]: Training loss at epoch 8: 0.8070336282253265
[08/28/2025 09:58:42 INFO]: Training loss at epoch 2: 1.1359782218933105
[08/28/2025 09:58:55 INFO]: Training loss at epoch 11: 0.9664183855056763
[08/28/2025 09:59:02 INFO]: Training stats: {
    "score": -0.999775309766945,
    "rmse": 0.999775309766945
}
[08/28/2025 09:59:02 INFO]: Val stats: {
    "score": -0.9965554339564191,
    "rmse": 0.9965554339564191
}
[08/28/2025 09:59:02 INFO]: Test stats: {
    "score": -0.996776834121143,
    "rmse": 0.996776834121143
}
[08/28/2025 09:59:11 INFO]: New best epoch, val score: -0.900314421245374
[08/28/2025 09:59:11 INFO]: Saving model to: maddest-Elbert_trial_135/model_best.pth
[08/28/2025 09:59:32 INFO]: Training loss at epoch 53: 0.4675898253917694
[08/28/2025 09:59:35 INFO]: New best epoch, val score: -0.9463006223753634
[08/28/2025 09:59:35 INFO]: Saving model to: maddest-Elbert_trial_133/model_best.pth
[08/28/2025 09:59:38 INFO]: New best epoch, val score: -0.8970763395111663
[08/28/2025 09:59:38 INFO]: Saving model to: maddest-Elbert_trial_137/model_best.pth
[08/28/2025 09:59:42 INFO]: Training loss at epoch 18: 1.0014981925487518
[08/28/2025 10:00:02 INFO]: Training loss at epoch 3: 1.1200104653835297
[08/28/2025 10:00:18 INFO]: Training loss at epoch 26: 0.8551657199859619
[08/28/2025 10:00:35 INFO]: Training loss at epoch 43: 0.8839163780212402
[08/28/2025 10:00:42 INFO]: Training loss at epoch 25: 0.8252415060997009
[08/28/2025 10:00:49 INFO]: Training loss at epoch 14: 1.1280066072940826
[08/28/2025 10:01:01 INFO]: New best epoch, val score: -0.8851258973692835
[08/28/2025 10:01:01 INFO]: Saving model to: maddest-Elbert_trial_136/model_best.pth
[08/28/2025 10:01:15 INFO]: Training loss at epoch 27: 1.048096239566803
[08/28/2025 10:01:57 INFO]: Training loss at epoch 11: 0.9161841869354248
[08/28/2025 10:03:58 INFO]: Training loss at epoch 0: 1.589842438697815
[08/28/2025 10:04:37 INFO]: Training loss at epoch 37: 0.9664471745491028
[08/28/2025 10:04:37 INFO]: Training loss at epoch 30: 0.9924309551715851
[08/28/2025 10:04:47 INFO]: Training loss at epoch 19: 0.7934026122093201
[08/28/2025 10:04:55 INFO]: New best epoch, val score: -1.4562569611862908
[08/28/2025 10:04:55 INFO]: Saving model to: maddest-Elbert_trial_138/model_best.pth
[08/28/2025 10:05:01 INFO]: Training loss at epoch 12: 1.2794071733951569
[08/28/2025 10:05:27 INFO]: Training loss at epoch 37: 0.9190138578414917
[08/28/2025 10:05:46 INFO]: Training loss at epoch 26: 1.0901790261268616
[08/28/2025 10:05:46 INFO]: Training loss at epoch 39: 0.8194222748279572
[08/28/2025 10:06:11 INFO]: Training loss at epoch 9: 1.194770634174347
[08/28/2025 10:06:25 INFO]: Training loss at epoch 3: 1.0999996662139893
[08/28/2025 10:06:29 INFO]: Training loss at epoch 27: 0.8361831605434418
[08/28/2025 10:06:29 INFO]: Training stats: {
    "score": -0.9592650229858065,
    "rmse": 0.9592650229858065
}
[08/28/2025 10:06:29 INFO]: Val stats: {
    "score": -0.9002377972063128,
    "rmse": 0.9002377972063128
}
[08/28/2025 10:06:29 INFO]: Test stats: {
    "score": -0.9689073499490706,
    "rmse": 0.9689073499490706
}
[08/28/2025 10:06:33 INFO]: Training loss at epoch 15: 0.9674681425094604
[08/28/2025 10:06:44 INFO]: Training loss at epoch 28: 1.0141054391860962
[08/28/2025 10:07:28 INFO]: Training loss at epoch 4: 0.8876408338546753
[08/28/2025 10:07:28 INFO]: Training loss at epoch 37: 0.9066190421581268
[08/28/2025 10:07:41 INFO]: Training loss at epoch 4: 1.1039760112762451
[08/28/2025 10:08:01 INFO]: Training loss at epoch 12: 0.9813328385353088
[08/28/2025 10:08:21 INFO]: Training loss at epoch 3: 1.0616030097007751
[08/28/2025 10:08:29 INFO]: Training stats: {
    "score": -0.9984670176663578,
    "rmse": 0.9984670176663578
}
[08/28/2025 10:08:29 INFO]: Val stats: {
    "score": -0.9323756584785116,
    "rmse": 0.9323756584785116
}
[08/28/2025 10:08:29 INFO]: Test stats: {
    "score": -0.9722011398350688,
    "rmse": 0.9722011398350688
}
[08/28/2025 10:08:41 INFO]: Training stats: {
    "score": -1.0015549994769921,
    "rmse": 1.0015549994769921
}
[08/28/2025 10:08:41 INFO]: Val stats: {
    "score": -0.94203258404004,
    "rmse": 0.94203258404004
}
[08/28/2025 10:08:41 INFO]: Test stats: {
    "score": -0.9743993630050667,
    "rmse": 0.9743993630050667
}
[08/28/2025 10:08:48 INFO]: Training loss at epoch 44: 1.1057122349739075
[08/28/2025 10:09:33 INFO]: New best epoch, val score: -0.94203258404004
[08/28/2025 10:09:33 INFO]: Saving model to: maddest-Elbert_trial_133/model_best.pth
[08/28/2025 10:10:10 INFO]: Training loss at epoch 31: 0.9637632071971893
[08/28/2025 10:10:30 INFO]: Training loss at epoch 54: 0.4041273891925812
[08/28/2025 10:10:49 INFO]: Training loss at epoch 27: 0.8007773160934448
[08/28/2025 10:10:50 INFO]: Running Final Evaluation...
[08/28/2025 10:11:06 INFO]: Training loss at epoch 13: 1.0827153325080872
[08/28/2025 10:11:34 INFO]: Training loss at epoch 20: 0.938785970211029
[08/28/2025 10:11:43 INFO]: Training loss at epoch 1: 1.1391319632530212
[08/28/2025 10:12:19 INFO]: Training loss at epoch 29: 0.7735387682914734
[08/28/2025 10:12:19 INFO]: Training loss at epoch 16: 1.2396883368492126
[08/28/2025 10:12:33 INFO]: Training loss at epoch 28: 1.079557180404663
[08/28/2025 10:12:41 INFO]: New best epoch, val score: -0.9337438674804787
[08/28/2025 10:12:41 INFO]: Saving model to: maddest-Elbert_trial_138/model_best.pth
[08/28/2025 10:12:58 INFO]: Training accuracy: {
    "score": -1.0156526542871018,
    "rmse": 1.0156526542871018
}
[08/28/2025 10:12:58 INFO]: Val accuracy: {
    "score": -0.8938788709311275,
    "rmse": 0.8938788709311275
}
[08/28/2025 10:12:58 INFO]: Test accuracy: {
    "score": -0.9766466227290795,
    "rmse": 0.9766466227290795
}
[08/28/2025 10:12:58 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_125",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9766466227290795,
        "rmse": 0.9766466227290795
    },
    "train_stats": {
        "score": -1.0156526542871018,
        "rmse": 1.0156526542871018
    },
    "val_stats": {
        "score": -0.8938788709311275,
        "rmse": 0.8938788709311275
    }
}
[08/28/2025 10:12:58 INFO]: Procewss finished for trial maddest-Elbert_trial_125
[08/28/2025 10:12:58 INFO]: 
_________________________________________________

[08/28/2025 10:12:58 INFO]: train_net_for_optune.py main() running.
[08/28/2025 10:12:58 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 16
  d_ffn_factor: 0.7423093038270054
  attention_dropout: 0.21318086810579384
  ffn_dropout: 0.21318086810579384
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.4318830758397395e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_139

[08/28/2025 10:12:58 INFO]: This ft_transformer has 16.543 million parameters.
[08/28/2025 10:12:58 INFO]: Training will start at epoch 0.
[08/28/2025 10:12:58 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 10:13:40 INFO]: Training loss at epoch 38: 0.7887933552265167
[08/28/2025 10:14:06 INFO]: Training loss at epoch 13: 0.7941628396511078
[08/28/2025 10:14:10 INFO]: Training loss at epoch 4: 0.9924972355365753
[08/28/2025 10:14:12 INFO]: Training stats: {
    "score": -1.0061503168480923,
    "rmse": 1.0061503168480923
}
[08/28/2025 10:14:12 INFO]: Val stats: {
    "score": -0.9699960246339387,
    "rmse": 0.9699960246339387
}
[08/28/2025 10:14:12 INFO]: Test stats: {
    "score": -0.9791766848425043,
    "rmse": 0.9791766848425043
}
[08/28/2025 10:14:24 INFO]: Training loss at epoch 38: 0.857525646686554
[08/28/2025 10:15:20 INFO]: Training loss at epoch 5: 0.8238628506660461
[08/28/2025 10:15:49 INFO]: Training loss at epoch 28: 1.1228418946266174
[08/28/2025 10:16:11 INFO]: Training loss at epoch 10: 1.0518884658813477
[08/28/2025 10:16:33 INFO]: Training loss at epoch 40: 1.0820999145507812
[08/28/2025 10:16:38 INFO]: Training loss at epoch 21: 0.8641235530376434
[08/28/2025 10:17:03 INFO]: Training loss at epoch 45: 0.9561121761798859
[08/28/2025 10:17:04 INFO]: New best epoch, val score: -0.9021059651312384
[08/28/2025 10:17:04 INFO]: Saving model to: maddest-Elbert_trial_133/model_best.pth
[08/28/2025 10:17:14 INFO]: Training loss at epoch 14: 1.226092517375946
[08/28/2025 10:17:15 INFO]: Training loss at epoch 38: 0.6762764751911163
[08/28/2025 10:17:58 INFO]: Training loss at epoch 5: 1.0032752752304077
[08/28/2025 10:18:02 INFO]: Training loss at epoch 17: 0.9438788592815399
[08/28/2025 10:18:38 INFO]: Training loss at epoch 29: 1.100986808538437
[08/28/2025 10:18:51 INFO]: Training loss at epoch 4: 0.9918574690818787
[08/28/2025 10:19:32 INFO]: Training loss at epoch 2: 1.3161574006080627
[08/28/2025 10:19:43 INFO]: Training loss at epoch 30: 0.9646630585193634
[08/28/2025 10:20:10 INFO]: Training loss at epoch 14: 0.8228653371334076
[08/28/2025 10:20:39 INFO]: Training stats: {
    "score": -0.9650045817918981,
    "rmse": 0.9650045817918981
}
[08/28/2025 10:20:39 INFO]: Val stats: {
    "score": -0.9348728952591303,
    "rmse": 0.9348728952591303
}
[08/28/2025 10:20:39 INFO]: Test stats: {
    "score": -0.9584305052854976,
    "rmse": 0.9584305052854976
}
[08/28/2025 10:20:52 INFO]: Training loss at epoch 29: 0.9906432032585144
[08/28/2025 10:21:31 INFO]: Training loss at epoch 55: 0.41232362389564514
[08/28/2025 10:21:37 INFO]: Training loss at epoch 0: 0.9602999687194824
[08/28/2025 10:21:44 INFO]: Training loss at epoch 22: 0.8399957120418549
[08/28/2025 10:21:51 INFO]: Training loss at epoch 5: 0.9931811094284058
[08/28/2025 10:21:53 INFO]: Training loss at epoch 39: 1.1857917308807373
[08/28/2025 10:22:35 INFO]: Training stats: {
    "score": -1.0057858039035783,
    "rmse": 1.0057858039035783
}
[08/28/2025 10:22:35 INFO]: Val stats: {
    "score": -0.8932055326712632,
    "rmse": 0.8932055326712632
}
[08/28/2025 10:22:35 INFO]: Test stats: {
    "score": -0.9774663055699929,
    "rmse": 0.9774663055699929
}
[08/28/2025 10:22:47 INFO]: New best epoch, val score: -1.0384023040469932
[08/28/2025 10:22:47 INFO]: Saving model to: maddest-Elbert_trial_139/model_best.pth
[08/28/2025 10:22:55 INFO]: Training loss at epoch 6: 0.8689060807228088
[08/28/2025 10:23:21 INFO]: Training loss at epoch 15: 0.972067803144455
[08/28/2025 10:23:43 INFO]: Training loss at epoch 11: 1.2808076739311218
[08/28/2025 10:23:47 INFO]: Training loss at epoch 18: 0.8896392583847046
[08/28/2025 10:24:13 INFO]: Training loss at epoch 39: 1.2193910479545593
[08/28/2025 10:24:25 INFO]: New best epoch, val score: -0.8725294497688143
[08/28/2025 10:24:25 INFO]: Saving model to: maddest-Elbert_trial_130/model_best.pth
[08/28/2025 10:24:33 INFO]: New best epoch, val score: -0.8889775094572983
[08/28/2025 10:24:33 INFO]: Saving model to: maddest-Elbert_trial_133/model_best.pth
[08/28/2025 10:24:37 INFO]: Training loss at epoch 41: 0.9236628115177155
[08/28/2025 10:24:40 INFO]: Training stats: {
    "score": -0.995981565644864,
    "rmse": 0.995981565644864
}
[08/28/2025 10:24:40 INFO]: Val stats: {
    "score": -0.9375312401207105,
    "rmse": 0.9375312401207105
}
[08/28/2025 10:24:40 INFO]: Test stats: {
    "score": -0.9721027046680456,
    "rmse": 0.9721027046680456
}
[08/28/2025 10:25:14 INFO]: Training loss at epoch 31: 1.1407665610313416
[08/28/2025 10:25:17 INFO]: Training loss at epoch 46: 0.9157249331474304
[08/28/2025 10:25:37 INFO]: Running Final Evaluation...
[08/28/2025 10:26:12 INFO]: Training loss at epoch 15: 0.9436352252960205
[08/28/2025 10:26:40 INFO]: Training loss at epoch 30: 0.929336279630661
[08/28/2025 10:26:47 INFO]: Training loss at epoch 23: 1.0637556910514832
[08/28/2025 10:27:00 INFO]: Training loss at epoch 39: 0.8280533254146576
[08/28/2025 10:27:16 INFO]: Training loss at epoch 3: 1.4626052975654602
[08/28/2025 10:27:31 INFO]: Training stats: {
    "score": -0.9508615465009347,
    "rmse": 0.9508615465009347
}
[08/28/2025 10:27:31 INFO]: Val stats: {
    "score": -0.9012134129353356,
    "rmse": 0.9012134129353356
}
[08/28/2025 10:27:31 INFO]: Test stats: {
    "score": -0.9687406272198275,
    "rmse": 0.9687406272198275
}
[08/28/2025 10:27:34 INFO]: Training loss at epoch 30: 1.0213257670402527
[08/28/2025 10:28:10 INFO]: New best epoch, val score: -0.8869312344541693
[08/28/2025 10:28:10 INFO]: Saving model to: maddest-Elbert_trial_138/model_best.pth
[08/28/2025 10:28:24 INFO]: Training loss at epoch 6: 0.9630407691001892
[08/28/2025 10:28:37 INFO]: Running Final Evaluation...
[08/28/2025 10:28:53 INFO]: Training accuracy: {
    "score": -1.0350646895795867,
    "rmse": 1.0350646895795867
}
[08/28/2025 10:28:53 INFO]: Val accuracy: {
    "score": -0.8850908665108888,
    "rmse": 0.8850908665108888
}
[08/28/2025 10:28:53 INFO]: Test accuracy: {
    "score": -1.0088687816339492,
    "rmse": 1.0088687816339492
}
[08/28/2025 10:28:53 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_113",
    "best_epoch": 8,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0088687816339492,
        "rmse": 1.0088687816339492
    },
    "train_stats": {
        "score": -1.0350646895795867,
        "rmse": 1.0350646895795867
    },
    "val_stats": {
        "score": -0.8850908665108888,
        "rmse": 0.8850908665108888
    }
}
[08/28/2025 10:28:53 INFO]: Procewss finished for trial maddest-Elbert_trial_113
[08/28/2025 10:28:53 INFO]: 
_________________________________________________

[08/28/2025 10:28:53 INFO]: train_net_for_optune.py main() running.
[08/28/2025 10:28:53 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 16
  d_ffn_factor: 1.843647856451018
  attention_dropout: 0.20955452231851412
  ffn_dropout: 0.20955452231851412
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.654204626844527e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_140

[08/28/2025 10:28:54 INFO]: This ft_transformer has 25.202 million parameters.
[08/28/2025 10:28:54 INFO]: Training will start at epoch 0.
[08/28/2025 10:28:54 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 10:29:15 INFO]: Training loss at epoch 5: 1.0685889720916748
[08/28/2025 10:29:27 INFO]: Training loss at epoch 16: 1.0034884810447693
[08/28/2025 10:29:28 INFO]: Training loss at epoch 19: 0.9643338024616241
[08/28/2025 10:29:33 INFO]: Training loss at epoch 6: 1.2426990270614624
[08/28/2025 10:30:18 INFO]: Training stats: {
    "score": -0.8198213868925341,
    "rmse": 0.8198213868925341
}
[08/28/2025 10:30:18 INFO]: Val stats: {
    "score": -0.9772705952862881,
    "rmse": 0.9772705952862881
}
[08/28/2025 10:30:18 INFO]: Test stats: {
    "score": -1.0520662929656361,
    "rmse": 1.0520662929656361
}
[08/28/2025 10:30:35 INFO]: Training loss at epoch 7: 1.000641107559204
[08/28/2025 10:30:44 INFO]: Training loss at epoch 32: 1.0083068311214447
[08/28/2025 10:31:06 INFO]: Training loss at epoch 12: 1.0035734474658966
[08/28/2025 10:31:19 INFO]: Training stats: {
    "score": -0.9927592697589777,
    "rmse": 0.9927592697589777
}
[08/28/2025 10:31:19 INFO]: Val stats: {
    "score": -0.8713931299198907,
    "rmse": 0.8713931299198907
}
[08/28/2025 10:31:19 INFO]: Test stats: {
    "score": -0.9877030954686264,
    "rmse": 0.9877030954686264
}
[08/28/2025 10:31:19 INFO]: Training loss at epoch 1: 1.0935037732124329
[08/28/2025 10:31:50 INFO]: Training loss at epoch 24: 0.9412156045436859
[08/28/2025 10:32:01 INFO]: New best epoch, val score: -0.886003498706765
[08/28/2025 10:32:01 INFO]: Saving model to: maddest-Elbert_trial_133/model_best.pth
[08/28/2025 10:32:01 INFO]: New best epoch, val score: -0.8713931299198907
[08/28/2025 10:32:01 INFO]: Saving model to: maddest-Elbert_trial_130/model_best.pth
[08/28/2025 10:32:14 INFO]: Training accuracy: {
    "score": -1.0265917738117833,
    "rmse": 1.0265917738117833
}
[08/28/2025 10:32:14 INFO]: Val accuracy: {
    "score": -0.8795799134918613,
    "rmse": 0.8795799134918613
}
[08/28/2025 10:32:14 INFO]: Test accuracy: {
    "score": -1.0144519640866438,
    "rmse": 1.0144519640866438
}
[08/28/2025 10:32:14 INFO]: {
    "dataset": "ic_upstream2_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "maddest-Elbert_trial_108",
    "best_epoch": 8,
    "routine": "from_scratch",
    "test_stats": {
        "score": -1.0144519640866438,
        "rmse": 1.0144519640866438
    },
    "train_stats": {
        "score": -1.0265917738117833,
        "rmse": 1.0265917738117833
    },
    "val_stats": {
        "score": -0.8795799134918613,
        "rmse": 0.8795799134918613
    }
}
[08/28/2025 10:32:14 INFO]: Procewss finished for trial maddest-Elbert_trial_108
[08/28/2025 10:32:14 INFO]: 
_________________________________________________

[08/28/2025 10:32:14 INFO]: train_net_for_optune.py main() running.
[08/28/2025 10:32:14 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 16
  d_ffn_factor: 1.8740120244850782
  attention_dropout: 0.19042957032241842
  ffn_dropout: 0.19042957032241842
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.3382255012103062e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: maddest-Elbert_trial_141

[08/28/2025 10:32:15 INFO]: This ft_transformer has 20.392 million parameters.
[08/28/2025 10:32:15 INFO]: Training will start at epoch 0.
[08/28/2025 10:32:15 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 10:32:18 INFO]: Training loss at epoch 16: 1.0078125596046448
[08/28/2025 10:32:27 INFO]: Training loss at epoch 56: 0.36790476739406586
[08/28/2025 10:32:34 INFO]: New best epoch, val score: -0.9309159684836847
[08/28/2025 10:32:34 INFO]: Saving model to: maddest-Elbert_trial_139/model_best.pth
[08/28/2025 10:32:36 INFO]: Training loss at epoch 31: 1.0848729610443115
[08/28/2025 10:32:45 INFO]: Training loss at epoch 31: 0.9347150921821594
[08/28/2025 10:32:47 INFO]: Training loss at epoch 42: 0.9111908972263336
[08/28/2025 10:33:30 INFO]: Training loss at epoch 47: 0.9252469539642334
