[08/07/2025 10:01:05 INFO]: Building Dataset
[08/07/2025 10:01:05 INFO]: pre normalizer.fit

[08/07/2025 10:01:05 INFO]: pos normalizer.fit

[08/07/2025 10:01:09 INFO]: Task: regression, Dataset: ic_upstream2, n_numerical: 100, n_categorical: 0, n_classes: 1, n_train_samples: 282, n_val_samples: 66, n_test_samples: 87
[08/07/2025 10:01:33 INFO]: 
_________________________________________________

[08/07/2025 10:01:33 INFO]: train_net_for_optune.py main() running.
[08/07/2025 10:01:33 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 8
  d_ffn_factor: 1.2563937799538802
  attention_dropout: 0.21882748206252628
  ffn_dropout: 0.21882748206252628
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.9215043754563454e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: -1
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: graveless-Jacquleen

[08/07/2025 10:01:35 INFO]: This ft_transformer has 5.861 million parameters.
[08/07/2025 10:01:35 INFO]: Training will start at epoch 0.
[08/07/2025 10:02:25 INFO]: ==> Starting training for 200 epochs...
[08/07/2025 10:03:58 INFO]: Training loss at epoch 0: 1.1662338972091675
[08/07/2025 10:04:11 INFO]: New best epoch, val score: -0.9597131904569977
[08/07/2025 10:04:11 INFO]: Saving model to: model_best.pth
[08/07/2025 10:05:34 INFO]: Training loss at epoch 1: 1.0366418361663818
[08/07/2025 10:05:46 INFO]: New best epoch, val score: -0.9349810712630786
[08/07/2025 10:05:46 INFO]: Saving model to: model_best.pth
[08/07/2025 10:07:10 INFO]: Training loss at epoch 2: 0.8788792490959167
[08/07/2025 10:07:24 INFO]: New best epoch, val score: -0.9298604878082375
[08/07/2025 10:07:24 INFO]: Saving model to: model_best.pth
[08/07/2025 10:08:48 INFO]: Training loss at epoch 3: 1.2058905959129333
[08/07/2025 10:10:24 INFO]: Training loss at epoch 4: 1.113982081413269
[08/07/2025 10:12:00 INFO]: Training loss at epoch 5: 1.1091476678848267
[08/07/2025 10:13:35 INFO]: Training loss at epoch 6: 1.0690097212791443
[08/07/2025 10:15:12 INFO]: Training loss at epoch 7: 1.1450416445732117
[08/07/2025 10:16:52 INFO]: Training loss at epoch 8: 1.1770113706588745
[08/07/2025 10:18:32 INFO]: Training loss at epoch 9: 0.9616991579532623
[08/07/2025 10:19:04 INFO]: Training stats: {
    "score": -0.9994211527410318,
    "rmse": 0.9994211527410318
}
[08/07/2025 10:19:04 INFO]: Val stats: {
    "score": -1.002729103413055,
    "rmse": 1.002729103413055
}
[08/07/2025 10:19:04 INFO]: Test stats: {
    "score": -0.9601589612995489,
    "rmse": 0.9601589612995489
}
[08/07/2025 10:20:45 INFO]: Training loss at epoch 10: 0.9568648636341095
[08/07/2025 10:22:28 INFO]: Training loss at epoch 11: 1.016480416059494
[08/07/2025 10:22:40 INFO]: New best epoch, val score: -0.9130206746446003
[08/07/2025 10:22:40 INFO]: Saving model to: model_best.pth
[08/07/2025 10:24:11 INFO]: Training loss at epoch 12: 0.9127995371818542
[08/07/2025 10:25:54 INFO]: Training loss at epoch 13: 1.1003742814064026
[08/07/2025 10:27:37 INFO]: Training loss at epoch 14: 0.9383994340896606
[08/07/2025 10:29:20 INFO]: Training loss at epoch 15: 0.9089822769165039
[08/07/2025 10:31:03 INFO]: Training loss at epoch 16: 0.8202039897441864
[08/07/2025 10:31:15 INFO]: New best epoch, val score: -0.9094956912539297
[08/07/2025 10:31:15 INFO]: Saving model to: model_best.pth
[08/07/2025 10:32:46 INFO]: Training loss at epoch 17: 0.7931148111820221
[08/07/2025 10:34:29 INFO]: Training loss at epoch 18: 1.0989246368408203
[08/07/2025 10:36:12 INFO]: Training loss at epoch 19: 0.7621103227138519
[08/07/2025 10:36:47 INFO]: Training stats: {
    "score": -0.9668686411427756,
    "rmse": 0.9668686411427756
}
[08/07/2025 10:36:47 INFO]: Val stats: {
    "score": -0.9231571112168351,
    "rmse": 0.9231571112168351
}
[08/07/2025 10:36:47 INFO]: Test stats: {
    "score": -0.9010680466240172,
    "rmse": 0.9010680466240172
}
[08/07/2025 10:38:30 INFO]: Training loss at epoch 20: 1.0542082488536835
[08/07/2025 10:40:12 INFO]: Training loss at epoch 21: 1.0072742104530334
[08/07/2025 10:41:55 INFO]: Training loss at epoch 22: 0.9108428359031677
[08/07/2025 10:43:37 INFO]: Training loss at epoch 23: 0.973812997341156
[08/07/2025 10:45:20 INFO]: Training loss at epoch 24: 1.056762844324112
[08/07/2025 10:47:03 INFO]: Training loss at epoch 25: 1.1216577589511871
[08/07/2025 10:48:45 INFO]: Training loss at epoch 26: 0.9649636447429657
[08/07/2025 10:50:28 INFO]: Training loss at epoch 27: 0.9959670603275299
[08/07/2025 10:52:10 INFO]: Training loss at epoch 28: 0.873490035533905
[08/07/2025 10:53:53 INFO]: Training loss at epoch 29: 0.841506689786911
[08/07/2025 10:54:28 INFO]: Training stats: {
    "score": -0.9566588023933877,
    "rmse": 0.9566588023933877
}
[08/07/2025 10:54:28 INFO]: Val stats: {
    "score": -0.9072761720850542,
    "rmse": 0.9072761720850542
}
[08/07/2025 10:54:28 INFO]: Test stats: {
    "score": -0.8890096554611202,
    "rmse": 0.8890096554611202
}
[08/07/2025 10:54:40 INFO]: New best epoch, val score: -0.9072761720850542
[08/07/2025 10:54:40 INFO]: Saving model to: model_best.pth
[08/07/2025 10:56:11 INFO]: Training loss at epoch 30: 1.1523111760616302
[08/07/2025 10:56:24 INFO]: New best epoch, val score: -0.9032950665056189
[08/07/2025 10:56:24 INFO]: Saving model to: model_best.pth
[08/07/2025 10:57:55 INFO]: Training loss at epoch 31: 0.7669714391231537
[08/07/2025 10:58:07 INFO]: New best epoch, val score: -0.9019820149699124
[08/07/2025 10:58:07 INFO]: Saving model to: model_best.pth
[08/07/2025 10:59:38 INFO]: Training loss at epoch 32: 1.0923823118209839
[08/07/2025 10:59:50 INFO]: New best epoch, val score: -0.9010085062825393
[08/07/2025 10:59:50 INFO]: Saving model to: model_best.pth
[08/07/2025 11:01:21 INFO]: Training loss at epoch 33: 0.9464763700962067
[08/07/2025 11:03:04 INFO]: Training loss at epoch 34: 0.8352843523025513
[08/07/2025 11:04:46 INFO]: Training loss at epoch 35: 1.0754420757293701
[08/07/2025 11:06:29 INFO]: Training loss at epoch 36: 0.886349618434906
[08/07/2025 11:08:12 INFO]: Training loss at epoch 37: 1.0669813752174377
[08/07/2025 11:09:55 INFO]: Training loss at epoch 38: 0.6959637552499771
[08/07/2025 11:11:37 INFO]: Training loss at epoch 39: 0.9552764594554901
[08/07/2025 11:12:12 INFO]: Training stats: {
    "score": -0.9397997010965824,
    "rmse": 0.9397997010965824
}
[08/07/2025 11:12:12 INFO]: Val stats: {
    "score": -0.905206567223666,
    "rmse": 0.905206567223666
}
[08/07/2025 11:12:12 INFO]: Test stats: {
    "score": -0.8808983631705629,
    "rmse": 0.8808983631705629
}
[08/07/2025 11:13:55 INFO]: Training loss at epoch 40: 1.1241589784622192
[08/07/2025 11:15:37 INFO]: Training loss at epoch 41: 0.8173784613609314
[08/07/2025 11:17:20 INFO]: Training loss at epoch 42: 0.8832212090492249
[08/07/2025 11:17:32 INFO]: New best epoch, val score: -0.8955506052682725
[08/07/2025 11:17:32 INFO]: Saving model to: model_best.pth
[08/07/2025 11:19:03 INFO]: Training loss at epoch 43: 0.9646520018577576
[08/07/2025 11:19:15 INFO]: New best epoch, val score: -0.8904007193122754
[08/07/2025 11:19:15 INFO]: Saving model to: model_best.pth
[08/07/2025 11:20:46 INFO]: Training loss at epoch 44: 1.0448552668094635
[08/07/2025 11:20:58 INFO]: New best epoch, val score: -0.8901777859332128
[08/07/2025 11:20:58 INFO]: Saving model to: model_best.pth
[08/07/2025 11:22:29 INFO]: Training loss at epoch 45: 0.8226730227470398
[08/07/2025 11:24:12 INFO]: Training loss at epoch 46: 0.7126217484474182
[08/07/2025 11:25:55 INFO]: Training loss at epoch 47: 0.7712236940860748
[08/07/2025 11:26:07 INFO]: New best epoch, val score: -0.8883562599028649
[08/07/2025 11:26:07 INFO]: Saving model to: model_best.pth
[08/07/2025 11:27:38 INFO]: Training loss at epoch 48: 1.0167810320854187
[08/07/2025 11:29:20 INFO]: Training loss at epoch 49: 0.9308310747146606
[08/07/2025 11:29:55 INFO]: Training stats: {
    "score": -0.9221754887311164,
    "rmse": 0.9221754887311164
}
[08/07/2025 11:29:55 INFO]: Val stats: {
    "score": -0.9011274004113455,
    "rmse": 0.9011274004113455
}
[08/07/2025 11:29:55 INFO]: Test stats: {
    "score": -0.8719722661577601,
    "rmse": 0.8719722661577601
}
[08/07/2025 11:31:38 INFO]: Training loss at epoch 50: 0.7854558825492859
[08/07/2025 11:33:20 INFO]: Training loss at epoch 51: 1.0371359288692474
[08/07/2025 11:35:03 INFO]: Training loss at epoch 52: 0.8751020431518555
[08/07/2025 11:36:46 INFO]: Training loss at epoch 53: 0.8981576263904572
[08/07/2025 11:36:58 INFO]: New best epoch, val score: -0.8880267128685975
[08/07/2025 11:36:58 INFO]: Saving model to: model_best.pth
[08/07/2025 11:38:29 INFO]: Training loss at epoch 54: 0.8771434426307678
[08/07/2025 11:38:41 INFO]: New best epoch, val score: -0.8844327597407293
[08/07/2025 11:38:41 INFO]: Saving model to: model_best.pth
[08/07/2025 11:40:12 INFO]: Training loss at epoch 55: 1.0634408593177795
[08/07/2025 11:40:24 INFO]: New best epoch, val score: -0.879595520912431
[08/07/2025 11:40:24 INFO]: Saving model to: model_best.pth
[08/07/2025 11:41:55 INFO]: Training loss at epoch 56: 0.7543309330940247
[08/07/2025 11:42:07 INFO]: New best epoch, val score: -0.8735797963444885
[08/07/2025 11:42:07 INFO]: Saving model to: model_best.pth
[08/07/2025 11:43:38 INFO]: Training loss at epoch 57: 0.829012006521225
[08/07/2025 11:43:50 INFO]: New best epoch, val score: -0.8713455090862949
[08/07/2025 11:43:50 INFO]: Saving model to: model_best.pth
[08/07/2025 11:45:21 INFO]: Training loss at epoch 58: 0.7623758316040039
[08/07/2025 11:47:04 INFO]: Training loss at epoch 59: 1.0263988971710205
[08/07/2025 11:47:38 INFO]: Training stats: {
    "score": -0.8971689674014808,
    "rmse": 0.8971689674014808
}
[08/07/2025 11:47:38 INFO]: Val stats: {
    "score": -0.8740950492861317,
    "rmse": 0.8740950492861317
}
[08/07/2025 11:47:38 INFO]: Test stats: {
    "score": -0.8472048097745093,
    "rmse": 0.8472048097745093
}
[08/07/2025 11:49:21 INFO]: Training loss at epoch 60: 0.9143840372562408
[08/07/2025 11:49:33 INFO]: New best epoch, val score: -0.8693833927448793
[08/07/2025 11:49:33 INFO]: Saving model to: model_best.pth
[08/07/2025 11:51:03 INFO]: Training loss at epoch 61: 0.818614661693573
[08/07/2025 11:51:16 INFO]: New best epoch, val score: -0.8659343881547807
[08/07/2025 11:51:16 INFO]: Saving model to: model_best.pth
[08/07/2025 11:52:47 INFO]: Training loss at epoch 62: 0.891123503446579
[08/07/2025 11:54:29 INFO]: Training loss at epoch 63: 0.9658914506435394
[08/07/2025 11:56:12 INFO]: Training loss at epoch 64: 0.7749956250190735
[08/07/2025 11:57:54 INFO]: Training loss at epoch 65: 0.9144058227539062
[08/07/2025 11:59:37 INFO]: Training loss at epoch 66: 0.905648410320282
[08/07/2025 12:01:20 INFO]: Training loss at epoch 67: 0.832237958908081
[08/07/2025 12:01:32 INFO]: New best epoch, val score: -0.8597956599045744
[08/07/2025 12:01:32 INFO]: Saving model to: model_best.pth
[08/07/2025 12:03:03 INFO]: Training loss at epoch 68: 0.8376392424106598
[08/07/2025 12:03:15 INFO]: New best epoch, val score: -0.84494654524347
[08/07/2025 12:03:15 INFO]: Saving model to: model_best.pth
[08/07/2025 12:04:46 INFO]: Training loss at epoch 69: 0.7685110569000244
[08/07/2025 12:05:20 INFO]: Training stats: {
    "score": -0.8713008938682998,
    "rmse": 0.8713008938682998
}
[08/07/2025 12:05:20 INFO]: Val stats: {
    "score": -0.8362694062099162,
    "rmse": 0.8362694062099162
}
[08/07/2025 12:05:20 INFO]: Test stats: {
    "score": -0.8188483027774943,
    "rmse": 0.8188483027774943
}
[08/07/2025 12:05:33 INFO]: New best epoch, val score: -0.8362694062099162
[08/07/2025 12:05:33 INFO]: Saving model to: model_best.pth
[08/07/2025 12:07:03 INFO]: Training loss at epoch 70: 0.8053866326808929
[08/07/2025 12:07:15 INFO]: New best epoch, val score: -0.8332051250374147
[08/07/2025 12:07:15 INFO]: Saving model to: model_best.pth
[08/07/2025 12:08:46 INFO]: Training loss at epoch 71: 0.7159647941589355
[08/07/2025 12:08:58 INFO]: New best epoch, val score: -0.8323420380063444
[08/07/2025 12:08:58 INFO]: Saving model to: model_best.pth
[08/07/2025 12:10:29 INFO]: Training loss at epoch 72: 0.813308835029602
[08/07/2025 12:12:12 INFO]: Training loss at epoch 73: 0.7402091026306152
[08/07/2025 12:13:55 INFO]: Training loss at epoch 74: 0.7888323366641998
[08/07/2025 12:15:37 INFO]: Training loss at epoch 75: 0.7988477647304535
[08/07/2025 12:17:20 INFO]: Training loss at epoch 76: 0.9891257882118225
[08/07/2025 12:19:02 INFO]: Training loss at epoch 77: 0.7011721730232239
[08/07/2025 12:20:45 INFO]: Training loss at epoch 78: 0.7337448596954346
[08/07/2025 12:22:28 INFO]: Training loss at epoch 79: 0.6511411070823669
[08/07/2025 12:23:02 INFO]: Training stats: {
    "score": -0.8245763025484459,
    "rmse": 0.8245763025484459
}
[08/07/2025 12:23:02 INFO]: Val stats: {
    "score": -0.815070052598609,
    "rmse": 0.815070052598609
}
[08/07/2025 12:23:02 INFO]: Test stats: {
    "score": -0.7867575611642231,
    "rmse": 0.7867575611642231
}
[08/07/2025 12:23:14 INFO]: New best epoch, val score: -0.815070052598609
[08/07/2025 12:23:14 INFO]: Saving model to: model_best.pth
[08/07/2025 12:24:45 INFO]: Training loss at epoch 80: 0.8228517770767212
[08/07/2025 12:24:58 INFO]: New best epoch, val score: -0.8125298264693185
[08/07/2025 12:24:58 INFO]: Saving model to: model_best.pth
[08/07/2025 12:26:29 INFO]: Training loss at epoch 81: 0.7638051211833954
[08/07/2025 12:26:41 INFO]: New best epoch, val score: -0.8038196903080734
[08/07/2025 12:26:41 INFO]: Saving model to: model_best.pth
[08/07/2025 12:28:11 INFO]: Training loss at epoch 82: 0.6045047044754028
[08/07/2025 12:28:24 INFO]: New best epoch, val score: -0.8032818194312084
[08/07/2025 12:28:24 INFO]: Saving model to: model_best.pth
[08/07/2025 12:29:54 INFO]: Training loss at epoch 83: 0.7280076146125793
[08/07/2025 12:31:37 INFO]: Training loss at epoch 84: 0.7898175716400146
[08/07/2025 12:33:19 INFO]: Training loss at epoch 85: 0.592803493142128
[08/07/2025 12:35:02 INFO]: Training loss at epoch 86: 0.5884346067905426
[08/07/2025 12:36:44 INFO]: Training loss at epoch 87: 0.7829962074756622
[08/07/2025 12:36:57 INFO]: New best epoch, val score: -0.7947640706080933
[08/07/2025 12:36:57 INFO]: Saving model to: model_best.pth
[08/07/2025 12:38:28 INFO]: Training loss at epoch 88: 0.8370248377323151
[08/07/2025 12:38:40 INFO]: New best epoch, val score: -0.7893433950709599
[08/07/2025 12:38:40 INFO]: Saving model to: model_best.pth
[08/07/2025 12:40:10 INFO]: Training loss at epoch 89: 0.6986831426620483
[08/07/2025 12:40:45 INFO]: Training stats: {
    "score": -0.7830762780339919,
    "rmse": 0.7830762780339919
}
[08/07/2025 12:40:45 INFO]: Val stats: {
    "score": -0.8062024138731209,
    "rmse": 0.8062024138731209
}
[08/07/2025 12:40:45 INFO]: Test stats: {
    "score": -0.761797658593259,
    "rmse": 0.761797658593259
}
[08/07/2025 12:42:28 INFO]: Training loss at epoch 90: 0.7053776681423187
[08/07/2025 12:44:10 INFO]: Training loss at epoch 91: 0.6984521150588989
[08/07/2025 12:45:53 INFO]: Training loss at epoch 92: 0.7484813034534454
[08/07/2025 12:47:36 INFO]: Training loss at epoch 93: 0.6347516477108002
[08/07/2025 12:49:19 INFO]: Training loss at epoch 94: 0.673318088054657
[08/07/2025 12:49:31 INFO]: New best epoch, val score: -0.7851482845178324
[08/07/2025 12:49:31 INFO]: Saving model to: model_best.pth
[08/07/2025 12:51:02 INFO]: Training loss at epoch 95: 0.729028970003128
[08/07/2025 12:51:14 INFO]: New best epoch, val score: -0.7806868532072966
[08/07/2025 12:51:14 INFO]: Saving model to: model_best.pth
[08/07/2025 12:52:45 INFO]: Training loss at epoch 96: 0.662155419588089
[08/07/2025 12:54:27 INFO]: Training loss at epoch 97: 0.599370539188385
[08/07/2025 12:56:10 INFO]: Training loss at epoch 98: 0.7036139070987701
[08/07/2025 12:57:53 INFO]: Training loss at epoch 99: 0.7129811346530914
[08/07/2025 12:58:27 INFO]: Training stats: {
    "score": -0.7748251703765481,
    "rmse": 0.7748251703765481
}
[08/07/2025 12:58:27 INFO]: Val stats: {
    "score": -0.8624442815243419,
    "rmse": 0.8624442815243419
}
[08/07/2025 12:58:27 INFO]: Test stats: {
    "score": -0.7795454591970387,
    "rmse": 0.7795454591970387
}
[08/07/2025 13:00:10 INFO]: Training loss at epoch 100: 0.7600864470005035
[08/07/2025 13:01:52 INFO]: Training loss at epoch 101: 0.7343068718910217
[08/07/2025 13:03:35 INFO]: Training loss at epoch 102: 0.509223997592926
[08/07/2025 13:03:47 INFO]: New best epoch, val score: -0.760381324901324
[08/07/2025 13:03:47 INFO]: Saving model to: model_best.pth
[08/07/2025 13:05:18 INFO]: Training loss at epoch 103: 0.614077240228653
[08/07/2025 13:05:31 INFO]: New best epoch, val score: -0.7564007050360609
[08/07/2025 13:05:31 INFO]: Saving model to: model_best.pth
[08/07/2025 13:07:02 INFO]: Training loss at epoch 104: 0.6454097926616669
[08/07/2025 13:08:45 INFO]: Training loss at epoch 105: 0.5437582731246948
[08/07/2025 13:10:27 INFO]: Training loss at epoch 106: 0.557047888636589
[08/07/2025 13:12:10 INFO]: Training loss at epoch 107: 0.6384853720664978
[08/07/2025 13:13:52 INFO]: Training loss at epoch 108: 0.714491993188858
[08/07/2025 13:15:35 INFO]: Training loss at epoch 109: 0.5775699317455292
[08/07/2025 13:16:09 INFO]: Training stats: {
    "score": -0.7304775505202915,
    "rmse": 0.7304775505202915
}
[08/07/2025 13:16:09 INFO]: Val stats: {
    "score": -0.8098013148237877,
    "rmse": 0.8098013148237877
}
[08/07/2025 13:16:09 INFO]: Test stats: {
    "score": -0.7387774397237417,
    "rmse": 0.7387774397237417
}
[08/07/2025 13:17:52 INFO]: Training loss at epoch 110: 0.5687867105007172
[08/07/2025 13:19:35 INFO]: Training loss at epoch 111: 0.49218353629112244
[08/07/2025 13:21:17 INFO]: Training loss at epoch 112: 0.5481118261814117
[08/07/2025 13:23:00 INFO]: Training loss at epoch 113: 0.533650815486908
[08/07/2025 13:24:42 INFO]: Training loss at epoch 114: 0.4387698620557785
[08/07/2025 13:26:25 INFO]: Training loss at epoch 115: 0.6313113570213318
[08/07/2025 13:28:08 INFO]: Training loss at epoch 116: 0.5938779413700104
[08/07/2025 13:29:51 INFO]: Training loss at epoch 117: 0.49550260603427887
[08/07/2025 13:31:34 INFO]: Training loss at epoch 118: 0.7994401156902313
[08/07/2025 13:33:16 INFO]: Training loss at epoch 119: 0.5259659737348557
[08/07/2025 13:33:51 INFO]: Training stats: {
    "score": -0.7559249248153775,
    "rmse": 0.7559249248153775
}
[08/07/2025 13:33:51 INFO]: Val stats: {
    "score": -0.8717050265956351,
    "rmse": 0.8717050265956351
}
[08/07/2025 13:33:51 INFO]: Test stats: {
    "score": -0.7778312580932016,
    "rmse": 0.7778312580932016
}
[08/07/2025 13:35:33 INFO]: Training loss at epoch 120: 0.5318971127271652
[08/07/2025 13:37:15 INFO]: Training loss at epoch 121: 0.5604134202003479
[08/07/2025 13:38:58 INFO]: Training loss at epoch 122: 0.557186096906662
[08/07/2025 13:40:41 INFO]: Training loss at epoch 123: 0.6002395153045654
[08/07/2025 13:42:23 INFO]: Training loss at epoch 124: 0.5385082960128784
[08/07/2025 13:44:06 INFO]: Training loss at epoch 125: 0.5223278999328613
[08/07/2025 13:45:49 INFO]: Training loss at epoch 126: 0.5273185223340988
[08/07/2025 13:47:31 INFO]: Training loss at epoch 127: 0.44627103209495544
[08/07/2025 13:49:14 INFO]: Training loss at epoch 128: 0.40643611550331116
[08/07/2025 13:50:57 INFO]: Training loss at epoch 129: 0.5731212794780731
[08/07/2025 13:51:31 INFO]: Training stats: {
    "score": -0.78332268155088,
    "rmse": 0.78332268155088
}
[08/07/2025 13:51:31 INFO]: Val stats: {
    "score": -0.7772765151513428,
    "rmse": 0.7772765151513428
}
[08/07/2025 13:51:31 INFO]: Test stats: {
    "score": -0.7972357686651246,
    "rmse": 0.7972357686651246
}
[08/07/2025 13:53:14 INFO]: Training loss at epoch 130: 0.5842671096324921
[08/07/2025 13:54:56 INFO]: Training loss at epoch 131: 0.47283628582954407
[08/07/2025 13:56:39 INFO]: Training loss at epoch 132: 0.5360674560070038
[08/07/2025 13:58:22 INFO]: Training loss at epoch 133: 0.6023042798042297
[08/07/2025 14:00:05 INFO]: Training loss at epoch 134: 0.4897764325141907
[08/07/2025 14:00:17 INFO]: Running Final Evaluation...
[08/07/2025 14:01:15 INFO]: Training accuracy: {
    "score": -0.7667162514453578,
    "rmse": 0.7667162514453578
}
[08/07/2025 14:01:15 INFO]: Val accuracy: {
    "score": -0.7564007050360609,
    "rmse": 0.7564007050360609
}
[08/07/2025 14:01:15 INFO]: Test accuracy: {
    "score": -0.748052455329964,
    "rmse": 0.748052455329964
}
[08/07/2025 14:01:15 INFO]: {
    "dataset": "ic_upstream2",
    "model_name": "ft_transformer",
    "run_id": "graveless-Jacquleen",
    "best_epoch": 103,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.748052455329964,
        "rmse": 0.748052455329964
    },
    "train_stats": {
        "score": -0.7667162514453578,
        "rmse": 0.7667162514453578
    },
    "val_stats": {
        "score": -0.7564007050360609,
        "rmse": 0.7564007050360609
    }
}
[08/07/2025 14:01:15 INFO]: 
_________________________________________________

[08/07/2025 14:01:15 INFO]: train_net_for_optune.py main() running.
[08/07/2025 14:01:15 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.9939436395777879
  attention_dropout: 0.4108929754478363
  ffn_dropout: 0.4108929754478363
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 6.377423702552814e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: graveless-Jacquleen

[08/07/2025 14:01:15 INFO]: This ft_transformer has 1.708 million parameters.
[08/07/2025 14:01:15 INFO]: Training will start at epoch 0.
[08/07/2025 14:01:15 INFO]: ==> Starting training for 200 epochs...
[08/07/2025 14:01:34 INFO]: Training loss at epoch 0: 0.9582999348640442
[08/07/2025 14:01:36 INFO]: New best epoch, val score: -0.9384562094560072
[08/07/2025 14:01:36 INFO]: Saving model to: model_best.pth
[08/07/2025 14:01:56 INFO]: Training loss at epoch 1: 1.2557721734046936
[08/07/2025 14:02:17 INFO]: Training loss at epoch 2: 1.0287510752677917
[08/07/2025 14:02:38 INFO]: Training loss at epoch 3: 1.2686043977737427
[08/07/2025 14:02:59 INFO]: Training loss at epoch 4: 0.8860934972763062
[08/07/2025 14:03:21 INFO]: Training loss at epoch 5: 1.0844172239303589
[08/07/2025 14:03:42 INFO]: Training loss at epoch 6: 0.987434446811676
[08/07/2025 14:04:03 INFO]: Training loss at epoch 7: 0.8589664101600647
[08/07/2025 14:04:24 INFO]: Training loss at epoch 8: 0.9705986976623535
[08/07/2025 14:04:45 INFO]: Training loss at epoch 9: 1.153304249048233
[08/07/2025 14:04:52 INFO]: Training stats: {
    "score": -1.0036684406800052,
    "rmse": 1.0036684406800052
}
[08/07/2025 14:04:52 INFO]: Val stats: {
    "score": -1.0028231913989696,
    "rmse": 1.0028231913989696
}
[08/07/2025 14:04:52 INFO]: Test stats: {
    "score": -0.9562107860350132,
    "rmse": 0.9562107860350132
}
[08/07/2025 14:05:13 INFO]: Training loss at epoch 10: 1.1317092776298523
[08/07/2025 14:05:35 INFO]: Training loss at epoch 11: 1.002049595117569
[08/07/2025 14:05:56 INFO]: Training loss at epoch 12: 1.0671331286430359
[08/07/2025 14:06:17 INFO]: Training loss at epoch 13: 1.001982033252716
[08/07/2025 14:06:38 INFO]: Training loss at epoch 14: 1.0246161222457886
[08/07/2025 14:06:59 INFO]: Training loss at epoch 15: 1.0097230672836304
[08/07/2025 14:07:20 INFO]: Training loss at epoch 16: 1.0538944005966187
[08/07/2025 14:07:41 INFO]: Training loss at epoch 17: 1.0895519256591797
[08/07/2025 14:08:02 INFO]: Training loss at epoch 18: 0.9407131671905518
[08/07/2025 14:08:23 INFO]: Training loss at epoch 19: 1.1176433265209198
[08/07/2025 14:08:30 INFO]: Training stats: {
    "score": -0.9897277201152277,
    "rmse": 0.9897277201152277
}
[08/07/2025 14:08:30 INFO]: Val stats: {
    "score": -0.9865978823319991,
    "rmse": 0.9865978823319991
}
[08/07/2025 14:08:30 INFO]: Test stats: {
    "score": -0.9405299497388274,
    "rmse": 0.9405299497388274
}
[08/07/2025 14:08:52 INFO]: Training loss at epoch 20: 0.9661932587623596
[08/07/2025 14:09:13 INFO]: Training loss at epoch 21: 0.9213250577449799
[08/07/2025 14:09:34 INFO]: Training loss at epoch 22: 1.0265250205993652
[08/07/2025 14:09:55 INFO]: Training loss at epoch 23: 0.9241378307342529
[08/07/2025 14:10:16 INFO]: Training loss at epoch 24: 1.0356119573116302
[08/07/2025 14:10:37 INFO]: Training loss at epoch 25: 1.0136697888374329
[08/07/2025 14:10:58 INFO]: Training loss at epoch 26: 0.9077016711235046
[08/07/2025 14:11:19 INFO]: Training loss at epoch 27: 1.0119242072105408
[08/07/2025 14:11:40 INFO]: Training loss at epoch 28: 0.951591819524765
[08/07/2025 14:12:02 INFO]: Training loss at epoch 29: 0.9921106994152069
[08/07/2025 14:12:09 INFO]: Training stats: {
    "score": -0.9786574622867492,
    "rmse": 0.9786574622867492
}
[08/07/2025 14:12:09 INFO]: Val stats: {
    "score": -0.9757025428960266,
    "rmse": 0.9757025428960266
}
[08/07/2025 14:12:09 INFO]: Test stats: {
    "score": -0.9298754389457234,
    "rmse": 0.9298754389457234
}
[08/07/2025 14:12:30 INFO]: Training loss at epoch 30: 0.7626897543668747
[08/07/2025 14:12:51 INFO]: Training loss at epoch 31: 0.8571661710739136
[08/07/2025 14:12:53 INFO]: Running Final Evaluation...
[08/07/2025 14:13:01 INFO]: Training accuracy: {
    "score": -1.0308922307325448,
    "rmse": 1.0308922307325448
}
[08/07/2025 14:13:01 INFO]: Val accuracy: {
    "score": -0.9384562094560072,
    "rmse": 0.9384562094560072
}
[08/07/2025 14:13:01 INFO]: Test accuracy: {
    "score": -0.9327792545386655,
    "rmse": 0.9327792545386655
}
[08/07/2025 14:13:01 INFO]: {
    "dataset": "ic_upstream2",
    "model_name": "ft_transformer",
    "run_id": "graveless-Jacquleen",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9327792545386655,
        "rmse": 0.9327792545386655
    },
    "train_stats": {
        "score": -1.0308922307325448,
        "rmse": 1.0308922307325448
    },
    "val_stats": {
        "score": -0.9384562094560072,
        "rmse": 0.9384562094560072
    }
}
[08/07/2025 14:13:01 INFO]: 
_________________________________________________

[08/07/2025 14:13:01 INFO]: train_net_for_optune.py main() running.
[08/07/2025 14:13:01 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 0.7121632264457157
  attention_dropout: 0.21898682297089084
  ffn_dropout: 0.21898682297089084
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0004599293840074914
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: graveless-Jacquleen

[08/07/2025 14:13:01 INFO]: This ft_transformer has 0.840 million parameters.
[08/07/2025 14:13:01 INFO]: Training will start at epoch 0.
[08/07/2025 14:13:01 INFO]: ==> Starting training for 200 epochs...
[08/07/2025 14:13:26 INFO]: Training loss at epoch 0: 1.1222231984138489
[08/07/2025 14:13:29 INFO]: New best epoch, val score: -0.9381672598821458
[08/07/2025 14:13:29 INFO]: Saving model to: model_best.pth
[08/07/2025 14:13:54 INFO]: Training loss at epoch 1: 1.5546717643737793
[08/07/2025 14:14:21 INFO]: Training loss at epoch 2: 0.9949022531509399
[08/07/2025 14:14:49 INFO]: Training loss at epoch 3: 1.0372664034366608
[08/07/2025 14:14:53 INFO]: New best epoch, val score: -0.9234524124048084
[08/07/2025 14:14:53 INFO]: Saving model to: model_best.pth
[08/07/2025 14:15:17 INFO]: Training loss at epoch 4: 0.9584405422210693
[08/07/2025 14:15:46 INFO]: Training loss at epoch 5: 1.1732704043388367
[08/07/2025 14:16:14 INFO]: Training loss at epoch 6: 1.2005850076675415
[08/07/2025 14:16:42 INFO]: Training loss at epoch 7: 1.1644401550292969
[08/07/2025 14:17:10 INFO]: Training loss at epoch 8: 1.210431694984436
[08/07/2025 14:17:38 INFO]: Training loss at epoch 9: 1.031267672777176
[08/07/2025 14:17:47 INFO]: Training stats: {
    "score": -1.0424354087388659,
    "rmse": 1.0424354087388659
}
[08/07/2025 14:17:47 INFO]: Val stats: {
    "score": -1.0628287269707521,
    "rmse": 1.0628287269707521
}
[08/07/2025 14:17:47 INFO]: Test stats: {
    "score": -1.0093028235532016,
    "rmse": 1.0093028235532016
}
[08/07/2025 14:18:15 INFO]: Training loss at epoch 10: 1.1005271077156067
[08/07/2025 14:18:43 INFO]: Training loss at epoch 11: 0.9524021148681641
[08/07/2025 14:19:11 INFO]: Training loss at epoch 12: 1.15911003947258
[08/07/2025 14:19:14 INFO]: New best epoch, val score: -0.9223222757101573
[08/07/2025 14:19:14 INFO]: Saving model to: model_best.pth
[08/07/2025 14:19:38 INFO]: Training loss at epoch 13: 1.1067330241203308
[08/07/2025 14:19:42 INFO]: New best epoch, val score: -0.9204119104466305
[08/07/2025 14:19:42 INFO]: Saving model to: model_best.pth
[08/07/2025 14:20:07 INFO]: Training loss at epoch 14: 1.0385180115699768
[08/07/2025 14:20:34 INFO]: Training loss at epoch 15: 1.2515613436698914
[08/07/2025 14:21:02 INFO]: Training loss at epoch 16: 1.1864993572235107
[08/07/2025 14:21:30 INFO]: Training loss at epoch 17: 0.9671799838542938
[08/07/2025 14:21:58 INFO]: Training loss at epoch 18: 0.9320305287837982
[08/07/2025 14:22:26 INFO]: Training loss at epoch 19: 1.1909754276275635
[08/07/2025 14:22:36 INFO]: Training stats: {
    "score": -1.0089758403670261,
    "rmse": 1.0089758403670261
}
[08/07/2025 14:22:36 INFO]: Val stats: {
    "score": -1.0241788196227737,
    "rmse": 1.0241788196227737
}
[08/07/2025 14:22:36 INFO]: Test stats: {
    "score": -0.9707025666661251,
    "rmse": 0.9707025666661251
}
[08/07/2025 14:23:03 INFO]: Training loss at epoch 20: 0.9581622779369354
[08/07/2025 14:23:31 INFO]: Training loss at epoch 21: 0.9416774809360504
[08/07/2025 14:23:59 INFO]: Training loss at epoch 22: 0.8291207253932953
[08/07/2025 14:24:27 INFO]: Training loss at epoch 23: 0.8653463423252106
[08/07/2025 14:24:55 INFO]: Training loss at epoch 24: 0.9808624386787415
[08/07/2025 14:24:58 INFO]: New best epoch, val score: -0.9155151989641777
[08/07/2025 14:24:58 INFO]: Saving model to: model_best.pth
[08/07/2025 14:25:23 INFO]: Training loss at epoch 25: 1.1341540813446045
[08/07/2025 14:25:51 INFO]: Training loss at epoch 26: 0.8809731304645538
[08/07/2025 14:26:19 INFO]: Training loss at epoch 27: 0.824738472700119
[08/07/2025 14:26:46 INFO]: Training loss at epoch 28: 0.9259440004825592
[08/07/2025 14:27:14 INFO]: Training loss at epoch 29: 1.0983064770698547
[08/07/2025 14:27:24 INFO]: Training stats: {
    "score": -0.9373145906130331,
    "rmse": 0.9373145906130331
}
[08/07/2025 14:27:24 INFO]: Val stats: {
    "score": -0.9352625457253751,
    "rmse": 0.9352625457253751
}
[08/07/2025 14:27:24 INFO]: Test stats: {
    "score": -0.884884657337594,
    "rmse": 0.884884657337594
}
[08/07/2025 14:27:52 INFO]: Training loss at epoch 30: 0.7807458341121674
[08/07/2025 14:28:20 INFO]: Training loss at epoch 31: 1.1623570322990417
[08/07/2025 14:28:48 INFO]: Training loss at epoch 32: 0.80344557762146
[08/07/2025 14:28:51 INFO]: New best epoch, val score: -0.8869738082439667
[08/07/2025 14:28:51 INFO]: Saving model to: model_best.pth
[08/07/2025 14:29:16 INFO]: Training loss at epoch 33: 0.9184786081314087
[08/07/2025 14:29:19 INFO]: New best epoch, val score: -0.8446749223138581
[08/07/2025 14:29:19 INFO]: Saving model to: model_best.pth
[08/07/2025 14:29:44 INFO]: Training loss at epoch 34: 0.7845751345157623
[08/07/2025 14:29:47 INFO]: New best epoch, val score: -0.8245932211796827
[08/07/2025 14:29:47 INFO]: Saving model to: model_best.pth
[08/07/2025 14:30:12 INFO]: Training loss at epoch 35: 0.768361359834671
[08/07/2025 14:30:16 INFO]: New best epoch, val score: -0.8095135129922761
[08/07/2025 14:30:16 INFO]: Saving model to: model_best.pth
[08/07/2025 14:30:40 INFO]: Training loss at epoch 36: 0.5342620313167572
[08/07/2025 14:30:44 INFO]: New best epoch, val score: -0.7971673169761229
[08/07/2025 14:30:44 INFO]: Saving model to: model_best.pth
[08/07/2025 14:31:08 INFO]: Training loss at epoch 37: 0.6711958646774292
[08/07/2025 14:31:12 INFO]: New best epoch, val score: -0.7961097802919987
[08/07/2025 14:31:12 INFO]: Saving model to: model_best.pth
[08/07/2025 14:31:36 INFO]: Training loss at epoch 38: 0.7164615392684937
[08/07/2025 14:31:40 INFO]: New best epoch, val score: -0.7906484678234442
[08/07/2025 14:31:40 INFO]: Saving model to: model_best.pth
[08/07/2025 14:32:04 INFO]: Training loss at epoch 39: 0.5736916065216064
[08/07/2025 14:32:14 INFO]: Training stats: {
    "score": -0.747554819019463,
    "rmse": 0.747554819019463
}
[08/07/2025 14:32:14 INFO]: Val stats: {
    "score": -0.7820182656065365,
    "rmse": 0.7820182656065365
}
[08/07/2025 14:32:14 INFO]: Test stats: {
    "score": -0.7192112519135315,
    "rmse": 0.7192112519135315
}
[08/07/2025 14:32:17 INFO]: New best epoch, val score: -0.7820182656065365
[08/07/2025 14:32:17 INFO]: Saving model to: model_best.pth
[08/07/2025 14:32:42 INFO]: Training loss at epoch 40: 0.6666748225688934
[08/07/2025 14:32:45 INFO]: New best epoch, val score: -0.7800269878791002
[08/07/2025 14:32:45 INFO]: Saving model to: model_best.pth
[08/07/2025 14:33:10 INFO]: Training loss at epoch 41: 0.606255829334259
[08/07/2025 14:33:13 INFO]: New best epoch, val score: -0.7659969761523775
[08/07/2025 14:33:13 INFO]: Saving model to: model_best.pth
[08/07/2025 14:33:38 INFO]: Training loss at epoch 42: 0.5881714224815369
[08/07/2025 14:34:06 INFO]: Training loss at epoch 43: 0.5956237018108368
[08/07/2025 14:34:34 INFO]: Training loss at epoch 44: 0.5982817709445953
[08/07/2025 14:35:02 INFO]: Training loss at epoch 45: 0.3819879963994026
[08/07/2025 14:35:05 INFO]: New best epoch, val score: -0.7462464608038816
[08/07/2025 14:35:05 INFO]: Saving model to: model_best.pth
[08/07/2025 14:35:30 INFO]: Training loss at epoch 46: 0.6189461350440979
[08/07/2025 14:35:33 INFO]: New best epoch, val score: -0.7314888890433436
[08/07/2025 14:35:33 INFO]: Saving model to: model_best.pth
[08/07/2025 14:35:58 INFO]: Training loss at epoch 47: 0.5924634486436844
[08/07/2025 14:36:25 INFO]: Training loss at epoch 48: 0.5316291153430939
[08/07/2025 14:36:53 INFO]: Training loss at epoch 49: 0.41801856458187103
[08/07/2025 14:37:03 INFO]: Training stats: {
    "score": -0.6586793165524388,
    "rmse": 0.6586793165524388
}
[08/07/2025 14:37:03 INFO]: Val stats: {
    "score": -0.7077944137210702,
    "rmse": 0.7077944137210702
}
[08/07/2025 14:37:03 INFO]: Test stats: {
    "score": -0.6761074596583762,
    "rmse": 0.6761074596583762
}
[08/07/2025 14:37:06 INFO]: New best epoch, val score: -0.7077944137210702
[08/07/2025 14:37:06 INFO]: Saving model to: model_best.pth
[08/07/2025 14:37:31 INFO]: Training loss at epoch 50: 0.38766784965991974
[08/07/2025 14:37:34 INFO]: New best epoch, val score: -0.6891514679961329
[08/07/2025 14:37:34 INFO]: Saving model to: model_best.pth
[08/07/2025 14:37:59 INFO]: Training loss at epoch 51: 0.4150846302509308
[08/07/2025 14:38:28 INFO]: Training loss at epoch 52: 0.4892377257347107
[08/07/2025 14:38:56 INFO]: Training loss at epoch 53: 0.4572099894285202
[08/07/2025 14:38:59 INFO]: New best epoch, val score: -0.6878350545308044
[08/07/2025 14:38:59 INFO]: Saving model to: model_best.pth
[08/07/2025 14:39:24 INFO]: Training loss at epoch 54: 0.3789937496185303
[08/07/2025 14:39:52 INFO]: Training loss at epoch 55: 0.4886758327484131
[08/07/2025 14:40:20 INFO]: Training loss at epoch 56: 0.4485519677400589
[08/07/2025 14:40:47 INFO]: Training loss at epoch 57: 0.4514397084712982
[08/07/2025 14:41:15 INFO]: Training loss at epoch 58: 0.43757352232933044
[08/07/2025 14:41:43 INFO]: Training loss at epoch 59: 0.3403775990009308
[08/07/2025 14:41:53 INFO]: Training stats: {
    "score": -0.6278662845848377,
    "rmse": 0.6278662845848377
}
[08/07/2025 14:41:53 INFO]: Val stats: {
    "score": -0.7428527587241206,
    "rmse": 0.7428527587241206
}
[08/07/2025 14:41:53 INFO]: Test stats: {
    "score": -0.706654610635607,
    "rmse": 0.706654610635607
}
[08/07/2025 14:42:21 INFO]: Training loss at epoch 60: 0.3699990510940552
[08/07/2025 14:42:49 INFO]: Training loss at epoch 61: 0.32283614575862885
[08/07/2025 14:42:52 INFO]: New best epoch, val score: -0.6791241650764933
[08/07/2025 14:42:52 INFO]: Saving model to: model_best.pth
[08/07/2025 14:43:17 INFO]: Training loss at epoch 62: 0.3384592682123184
[08/07/2025 14:43:20 INFO]: New best epoch, val score: -0.6757785058077833
[08/07/2025 14:43:20 INFO]: Saving model to: model_best.pth
[08/07/2025 14:43:45 INFO]: Training loss at epoch 63: 0.2683825120329857
[08/07/2025 14:44:13 INFO]: Training loss at epoch 64: 0.4442182928323746
[08/07/2025 14:44:41 INFO]: Training loss at epoch 65: 0.40570808947086334
[08/07/2025 14:45:09 INFO]: Training loss at epoch 66: 0.384261816740036
[08/07/2025 14:45:37 INFO]: Training loss at epoch 67: 0.3863452672958374
[08/07/2025 14:46:04 INFO]: Training loss at epoch 68: 0.26811348646879196
[08/07/2025 14:46:33 INFO]: Training loss at epoch 69: 0.28629323840141296
[08/07/2025 14:46:42 INFO]: Training stats: {
    "score": -0.5451614586901209,
    "rmse": 0.5451614586901209
}
[08/07/2025 14:46:42 INFO]: Val stats: {
    "score": -0.6885256577120406,
    "rmse": 0.6885256577120406
}
[08/07/2025 14:46:42 INFO]: Test stats: {
    "score": -0.6526058342035406,
    "rmse": 0.6526058342035406
}
[08/07/2025 14:47:10 INFO]: Training loss at epoch 70: 0.3615444600582123
[08/07/2025 14:47:38 INFO]: Training loss at epoch 71: 0.3226325064897537
[08/07/2025 14:48:06 INFO]: Training loss at epoch 72: 0.3209652900695801
[08/07/2025 14:48:34 INFO]: Training loss at epoch 73: 0.4042276442050934
[08/07/2025 14:49:02 INFO]: Training loss at epoch 74: 0.27797580510377884
[08/07/2025 14:49:30 INFO]: Training loss at epoch 75: 0.27216965705156326
[08/07/2025 14:49:58 INFO]: Training loss at epoch 76: 0.32353267073631287
[08/07/2025 14:50:25 INFO]: Training loss at epoch 77: 0.28972238302230835
[08/07/2025 14:50:53 INFO]: Training loss at epoch 78: 0.3437650501728058
[08/07/2025 14:51:21 INFO]: Training loss at epoch 79: 0.2552531510591507
[08/07/2025 14:51:31 INFO]: Training stats: {
    "score": -0.5087755038826121,
    "rmse": 0.5087755038826121
}
[08/07/2025 14:51:31 INFO]: Val stats: {
    "score": -0.7116092132738615,
    "rmse": 0.7116092132738615
}
[08/07/2025 14:51:31 INFO]: Test stats: {
    "score": -0.6443942520822621,
    "rmse": 0.6443942520822621
}
[08/07/2025 14:51:59 INFO]: Training loss at epoch 80: 0.2897294759750366
[08/07/2025 14:52:27 INFO]: Training loss at epoch 81: 0.2966141849756241
[08/07/2025 14:52:54 INFO]: Training loss at epoch 82: 0.4033714532852173
[08/07/2025 14:53:22 INFO]: Training loss at epoch 83: 0.30340035259723663
[08/07/2025 14:53:50 INFO]: Training loss at epoch 84: 0.22346509993076324
[08/07/2025 14:54:18 INFO]: Training loss at epoch 85: 0.2509820833802223
[08/07/2025 14:54:46 INFO]: Training loss at epoch 86: 0.36767852306365967
[08/07/2025 14:55:14 INFO]: Training loss at epoch 87: 0.27366866171360016
[08/07/2025 14:55:42 INFO]: Training loss at epoch 88: 0.3584468811750412
[08/07/2025 14:56:10 INFO]: Training loss at epoch 89: 0.26678119599819183
[08/07/2025 14:56:20 INFO]: Training stats: {
    "score": -0.4966778025736239,
    "rmse": 0.4966778025736239
}
[08/07/2025 14:56:20 INFO]: Val stats: {
    "score": -0.7056516663100432,
    "rmse": 0.7056516663100432
}
[08/07/2025 14:56:20 INFO]: Test stats: {
    "score": -0.6518670504299244,
    "rmse": 0.6518670504299244
}
[08/07/2025 14:56:48 INFO]: Training loss at epoch 90: 0.24477270245552063
[08/07/2025 14:57:15 INFO]: Training loss at epoch 91: 0.26208143681287766
[08/07/2025 14:57:43 INFO]: Training loss at epoch 92: 0.31665507704019547
[08/07/2025 14:58:11 INFO]: Training loss at epoch 93: 0.29169437289237976
[08/07/2025 14:58:14 INFO]: Running Final Evaluation...
[08/07/2025 14:58:26 INFO]: Training accuracy: {
    "score": -0.5937511994555078,
    "rmse": 0.5937511994555078
}
[08/07/2025 14:58:26 INFO]: Val accuracy: {
    "score": -0.6757785058077833,
    "rmse": 0.6757785058077833
}
[08/07/2025 14:58:26 INFO]: Test accuracy: {
    "score": -0.6618931948808487,
    "rmse": 0.6618931948808487
}
[08/07/2025 14:58:26 INFO]: {
    "dataset": "ic_upstream2",
    "model_name": "ft_transformer",
    "run_id": "graveless-Jacquleen",
    "best_epoch": 62,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.6618931948808487,
        "rmse": 0.6618931948808487
    },
    "train_stats": {
        "score": -0.5937511994555078,
        "rmse": 0.5937511994555078
    },
    "val_stats": {
        "score": -0.6757785058077833,
        "rmse": 0.6757785058077833
    }
}
[08/07/2025 14:58:26 INFO]: 
_________________________________________________

[08/07/2025 14:58:26 INFO]: train_net_for_optune.py main() running.
[08/07/2025 14:58:26 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 8
  d_ffn_factor: 1.578398498525285
  attention_dropout: 0.051096598903251245
  ffn_dropout: 0.051096598903251245
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00028906913621125596
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: graveless-Jacquleen

[08/07/2025 14:58:26 INFO]: This ft_transformer has 11.392 million parameters.
[08/07/2025 14:58:26 INFO]: Training will start at epoch 0.
[08/07/2025 14:58:26 INFO]: ==> Starting training for 200 epochs...
[08/07/2025 15:01:00 INFO]: Training loss at epoch 0: 1.3423051834106445
[08/07/2025 15:01:21 INFO]: New best epoch, val score: -0.9728442698954994
[08/07/2025 15:01:21 INFO]: Saving model to: model_best.pth
[08/07/2025 15:03:56 INFO]: Training loss at epoch 1: 5.4476646184921265
[08/07/2025 15:06:50 INFO]: Training loss at epoch 2: 2.5798792243003845
[08/07/2025 15:09:45 INFO]: Training loss at epoch 3: 1.7034158110618591
[08/07/2025 15:10:06 INFO]: New best epoch, val score: -0.9431680837228388
[08/07/2025 15:10:06 INFO]: Saving model to: model_best.pth
[08/07/2025 15:12:41 INFO]: Training loss at epoch 4: 0.9382465481758118
[08/07/2025 15:15:36 INFO]: Training loss at epoch 5: 1.2760751247406006
[08/07/2025 15:18:31 INFO]: Training loss at epoch 6: 1.319863736629486
[08/07/2025 15:21:26 INFO]: Training loss at epoch 7: 1.1768200397491455
[08/07/2025 15:24:21 INFO]: Training loss at epoch 8: 1.3909387588500977
[08/07/2025 15:27:15 INFO]: Training loss at epoch 9: 1.1538823246955872
[08/07/2025 15:28:14 INFO]: Training stats: {
    "score": -1.0716742371354862,
    "rmse": 1.0716742371354862
}
[08/07/2025 15:28:14 INFO]: Val stats: {
    "score": -1.1045884806833615,
    "rmse": 1.1045884806833615
}
[08/07/2025 15:28:14 INFO]: Test stats: {
    "score": -1.0451231516583543,
    "rmse": 1.0451231516583543
}
[08/07/2025 15:31:09 INFO]: Training loss at epoch 10: 1.0568038821220398
[08/07/2025 15:31:30 INFO]: New best epoch, val score: -0.9422543526674765
[08/07/2025 15:31:30 INFO]: Saving model to: model_best.pth
[08/07/2025 15:34:04 INFO]: Training loss at epoch 11: 1.0038263201713562
[08/07/2025 15:36:59 INFO]: Training loss at epoch 12: 1.2709658741950989
[08/07/2025 15:39:54 INFO]: Training loss at epoch 13: 1.2733200788497925
[08/07/2025 15:42:49 INFO]: Training loss at epoch 14: 1.1749133467674255
[08/07/2025 15:45:44 INFO]: Training loss at epoch 15: 1.0838172137737274
[08/07/2025 15:46:05 INFO]: New best epoch, val score: -0.9320959801116953
[08/07/2025 15:46:05 INFO]: Saving model to: model_best.pth
[08/07/2025 15:48:39 INFO]: Training loss at epoch 16: 0.9748029708862305
[08/07/2025 15:51:34 INFO]: Training loss at epoch 17: 1.0555637180805206
[08/07/2025 15:54:29 INFO]: Training loss at epoch 18: 0.9212048053741455
[08/07/2025 15:57:24 INFO]: Training loss at epoch 19: 1.0328955352306366
[08/07/2025 15:58:22 INFO]: Training stats: {
    "score": -1.0419522839767492,
    "rmse": 1.0419522839767492
}
[08/07/2025 15:58:22 INFO]: Val stats: {
    "score": -1.0601954834348568,
    "rmse": 1.0601954834348568
}
[08/07/2025 15:58:22 INFO]: Test stats: {
    "score": -1.0059743601712394,
    "rmse": 1.0059743601712394
}
[08/07/2025 16:01:17 INFO]: Training loss at epoch 20: 0.921435534954071
[08/07/2025 16:04:12 INFO]: Training loss at epoch 21: 0.9455633461475372
[08/07/2025 16:07:07 INFO]: Training loss at epoch 22: 0.9350405335426331
[08/07/2025 16:10:02 INFO]: Training loss at epoch 23: 0.9914149940013885
[08/07/2025 16:12:57 INFO]: Training loss at epoch 24: 1.0894618928432465
[08/07/2025 16:15:52 INFO]: Training loss at epoch 25: 1.0278072655200958
[08/07/2025 16:18:47 INFO]: Training loss at epoch 26: 1.059310495853424
[08/07/2025 16:21:42 INFO]: Training loss at epoch 27: 0.9620344042778015
[08/07/2025 16:24:37 INFO]: Training loss at epoch 28: 0.9522143602371216
[08/07/2025 16:27:32 INFO]: Training loss at epoch 29: 1.0320237278938293
[08/07/2025 16:28:30 INFO]: Training stats: {
    "score": -0.9943607934257993,
    "rmse": 0.9943607934257993
}
[08/07/2025 16:28:30 INFO]: Val stats: {
    "score": -0.9605877461607165,
    "rmse": 0.9605877461607165
}
[08/07/2025 16:28:30 INFO]: Test stats: {
    "score": -0.9268926060842845,
    "rmse": 0.9268926060842845
}
[08/07/2025 16:31:25 INFO]: Training loss at epoch 30: 0.9500878155231476
[08/07/2025 16:34:20 INFO]: Training loss at epoch 31: 0.9328542649745941
[08/07/2025 16:37:15 INFO]: Training loss at epoch 32: 1.0219062566757202
[08/07/2025 16:40:10 INFO]: Training loss at epoch 33: 1.09555384516716
[08/07/2025 16:43:05 INFO]: Training loss at epoch 34: 1.1883925795555115
[08/07/2025 16:45:59 INFO]: Training loss at epoch 35: 1.2795232832431793
[08/07/2025 16:48:54 INFO]: Training loss at epoch 36: 0.8620565533638
[08/07/2025 16:51:49 INFO]: Training loss at epoch 37: 1.0387857556343079
[08/07/2025 16:52:10 INFO]: New best epoch, val score: -0.9307177134749476
[08/07/2025 16:52:10 INFO]: Saving model to: model_best.pth
[08/07/2025 16:54:45 INFO]: Training loss at epoch 38: 1.1575912833213806
[08/07/2025 16:55:05 INFO]: New best epoch, val score: -0.930618254006552
[08/07/2025 16:55:05 INFO]: Saving model to: model_best.pth
[08/07/2025 16:57:40 INFO]: Training loss at epoch 39: 0.9851865768432617
[08/07/2025 16:58:39 INFO]: Training stats: {
    "score": -0.9909965771971968,
    "rmse": 0.9909965771971968
}
[08/07/2025 16:58:39 INFO]: Val stats: {
    "score": -0.9350389922812102,
    "rmse": 0.9350389922812102
}
[08/07/2025 16:58:39 INFO]: Test stats: {
    "score": -0.9116248054507142,
    "rmse": 0.9116248054507142
}
[08/07/2025 17:01:33 INFO]: Training loss at epoch 40: 0.8818286061286926
[08/07/2025 17:04:28 INFO]: Training loss at epoch 41: 0.9882970750331879
[08/07/2025 17:07:23 INFO]: Training loss at epoch 42: 0.8914134502410889
[08/07/2025 17:10:18 INFO]: Training loss at epoch 43: 0.8959271311759949
[08/07/2025 17:13:13 INFO]: Training loss at epoch 44: 0.9520455598831177
[08/07/2025 17:16:08 INFO]: Training loss at epoch 45: 0.9248988032341003
[08/07/2025 17:19:03 INFO]: Training loss at epoch 46: 1.0421064496040344
[08/07/2025 17:21:57 INFO]: Training loss at epoch 47: 0.8871874511241913
[08/07/2025 17:24:52 INFO]: Training loss at epoch 48: 0.7814833521842957
[08/07/2025 17:25:13 INFO]: New best epoch, val score: -0.9284350907536684
[08/07/2025 17:25:13 INFO]: Saving model to: model_best.pth
[08/07/2025 17:27:47 INFO]: Training loss at epoch 49: 0.9581491947174072
[08/07/2025 17:28:46 INFO]: Training stats: {
    "score": -0.9864800180551605,
    "rmse": 0.9864800180551605
}
[08/07/2025 17:28:46 INFO]: Val stats: {
    "score": -0.9261082583480139,
    "rmse": 0.9261082583480139
}
[08/07/2025 17:28:46 INFO]: Test stats: {
    "score": -0.9053346390536035,
    "rmse": 0.9053346390536035
}
[08/07/2025 17:29:07 INFO]: New best epoch, val score: -0.9261082583480139
[08/07/2025 17:29:07 INFO]: Saving model to: model_best.pth
[08/07/2025 17:31:42 INFO]: Training loss at epoch 50: 1.1019290685653687
[08/07/2025 17:34:37 INFO]: Training loss at epoch 51: 1.078416109085083
[08/07/2025 17:37:31 INFO]: Training loss at epoch 52: 0.9277938902378082
[08/07/2025 17:40:26 INFO]: Training loss at epoch 53: 1.072661966085434
[08/07/2025 17:43:21 INFO]: Training loss at epoch 54: 0.899289458990097
[08/07/2025 17:46:16 INFO]: Training loss at epoch 55: 0.9409350454807281
[08/07/2025 17:49:11 INFO]: Training loss at epoch 56: 0.920182853937149
[08/07/2025 17:52:05 INFO]: Training loss at epoch 57: 0.7887259125709534
[08/07/2025 17:52:26 INFO]: New best epoch, val score: -0.9168631470866516
[08/07/2025 17:52:26 INFO]: Saving model to: model_best.pth
[08/07/2025 17:55:01 INFO]: Training loss at epoch 58: 0.8742991089820862
[08/07/2025 17:55:22 INFO]: New best epoch, val score: -0.9120017256858454
[08/07/2025 17:55:22 INFO]: Saving model to: model_best.pth
[08/07/2025 17:57:56 INFO]: Training loss at epoch 59: 1.1025544106960297
[08/07/2025 17:58:55 INFO]: Training stats: {
    "score": -0.9719159415626398,
    "rmse": 0.9719159415626398
}
[08/07/2025 17:58:55 INFO]: Val stats: {
    "score": -0.910475387949893,
    "rmse": 0.910475387949893
}
[08/07/2025 17:58:55 INFO]: Test stats: {
    "score": -0.8915064952975997,
    "rmse": 0.8915064952975997
}
[08/07/2025 17:59:16 INFO]: New best epoch, val score: -0.910475387949893
[08/07/2025 17:59:16 INFO]: Saving model to: model_best.pth
[08/07/2025 18:01:51 INFO]: Training loss at epoch 60: 0.8734529614448547
[08/07/2025 18:04:46 INFO]: Training loss at epoch 61: 0.8437047600746155
[08/07/2025 18:07:40 INFO]: Training loss at epoch 62: 0.9454175233840942
[08/07/2025 18:10:35 INFO]: Training loss at epoch 63: 0.7852335572242737
[08/07/2025 18:10:56 INFO]: New best epoch, val score: -0.8989344060767261
[08/07/2025 18:10:56 INFO]: Saving model to: model_best.pth
[08/07/2025 18:13:31 INFO]: Training loss at epoch 64: 0.7374738156795502
[08/07/2025 18:13:51 INFO]: New best epoch, val score: -0.8884536833161297
[08/07/2025 18:13:51 INFO]: Saving model to: model_best.pth
[08/07/2025 18:16:26 INFO]: Training loss at epoch 65: 1.0861019790172577
[08/07/2025 18:16:47 INFO]: New best epoch, val score: -0.8843248819116089
[08/07/2025 18:16:47 INFO]: Saving model to: model_best.pth
[08/07/2025 18:19:22 INFO]: Training loss at epoch 66: 0.8815229535102844
[08/07/2025 18:22:16 INFO]: Training loss at epoch 67: 0.98194819688797
[08/07/2025 18:25:11 INFO]: Training loss at epoch 68: 0.8132059574127197
[08/07/2025 18:28:06 INFO]: Training loss at epoch 69: 0.76397505402565
[08/07/2025 18:29:05 INFO]: Training stats: {
    "score": -0.9039973324375921,
    "rmse": 0.9039973324375921
}
[08/07/2025 18:29:05 INFO]: Val stats: {
    "score": -0.8982328938205354,
    "rmse": 0.8982328938205354
}
[08/07/2025 18:29:05 INFO]: Test stats: {
    "score": -0.8590363733799399,
    "rmse": 0.8590363733799399
}
[08/07/2025 18:32:00 INFO]: Training loss at epoch 70: 0.8565715253353119
[08/07/2025 18:32:20 INFO]: New best epoch, val score: -0.8714626190361952
[08/07/2025 18:32:20 INFO]: Saving model to: model_best.pth
[08/07/2025 18:34:55 INFO]: Training loss at epoch 71: 0.7467278242111206
[08/07/2025 18:35:16 INFO]: New best epoch, val score: -0.8470775169125113
[08/07/2025 18:35:16 INFO]: Saving model to: model_best.pth
[08/07/2025 18:37:50 INFO]: Training loss at epoch 72: 0.7869612574577332
[08/07/2025 18:38:11 INFO]: New best epoch, val score: -0.8228089185335549
[08/07/2025 18:38:11 INFO]: Saving model to: model_best.pth
[08/07/2025 18:40:46 INFO]: Training loss at epoch 73: 0.8805254101753235
[08/07/2025 18:41:07 INFO]: New best epoch, val score: -0.8096143152419498
[08/07/2025 18:41:07 INFO]: Saving model to: model_best.pth
[08/07/2025 18:43:41 INFO]: Training loss at epoch 74: 0.6221240758895874
[08/07/2025 18:44:02 INFO]: New best epoch, val score: -0.8052246751536459
[08/07/2025 18:44:02 INFO]: Saving model to: model_best.pth
[08/07/2025 18:46:36 INFO]: Training loss at epoch 75: 0.5816793888807297
[08/07/2025 18:46:57 INFO]: New best epoch, val score: -0.8004601164733117
[08/07/2025 18:46:57 INFO]: Saving model to: model_best.pth
[08/07/2025 18:49:32 INFO]: Training loss at epoch 76: 0.637766420841217
[08/07/2025 18:49:53 INFO]: New best epoch, val score: -0.7950534159709062
[08/07/2025 18:49:53 INFO]: Saving model to: model_best.pth
[08/07/2025 18:52:27 INFO]: Training loss at epoch 77: 0.666586846113205
[08/07/2025 18:52:48 INFO]: New best epoch, val score: -0.7946976824639445
[08/07/2025 18:52:48 INFO]: Saving model to: model_best.pth
[08/07/2025 18:55:23 INFO]: Training loss at epoch 78: 0.5242740660905838
[08/07/2025 18:58:18 INFO]: Training loss at epoch 79: 0.6046889126300812
[08/07/2025 18:59:17 INFO]: Training stats: {
    "score": -0.7731797480043562,
    "rmse": 0.7731797480043562
}
[08/07/2025 18:59:17 INFO]: Val stats: {
    "score": -0.8020075036198278,
    "rmse": 0.8020075036198278
}
[08/07/2025 18:59:17 INFO]: Test stats: {
    "score": -0.77097427307747,
    "rmse": 0.77097427307747
}
[08/07/2025 19:02:11 INFO]: Training loss at epoch 80: 0.5980291962623596
[08/07/2025 19:05:06 INFO]: Training loss at epoch 81: 0.6417838633060455
[08/07/2025 19:08:01 INFO]: Training loss at epoch 82: 0.6482225060462952
[08/07/2025 19:08:21 INFO]: New best epoch, val score: -0.7902795660075463
[08/07/2025 19:08:21 INFO]: Saving model to: model_best.pth
[08/07/2025 19:10:56 INFO]: Training loss at epoch 83: 0.6442415714263916
[08/07/2025 19:11:17 INFO]: New best epoch, val score: -0.7697156222592549
[08/07/2025 19:11:17 INFO]: Saving model to: model_best.pth
[08/07/2025 19:13:51 INFO]: Training loss at epoch 84: 0.4237697720527649
[08/07/2025 19:14:12 INFO]: New best epoch, val score: -0.757668702218512
[08/07/2025 19:14:12 INFO]: Saving model to: model_best.pth
[08/07/2025 19:16:47 INFO]: Training loss at epoch 85: 0.49128350615501404
[08/07/2025 19:17:07 INFO]: New best epoch, val score: -0.7538955336618389
[08/07/2025 19:17:07 INFO]: Saving model to: model_best.pth
[08/07/2025 19:19:42 INFO]: Training loss at epoch 86: 0.5985881090164185
[08/07/2025 19:22:37 INFO]: Training loss at epoch 87: 0.43331894278526306
[08/07/2025 19:25:31 INFO]: Training loss at epoch 88: 0.4525551497936249
[08/07/2025 19:28:26 INFO]: Training loss at epoch 89: 0.4566585272550583
[08/07/2025 19:29:25 INFO]: Training stats: {
    "score": -0.680218230317503,
    "rmse": 0.680218230317503
}
[08/07/2025 19:29:25 INFO]: Val stats: {
    "score": -0.7630162013193226,
    "rmse": 0.7630162013193226
}
[08/07/2025 19:29:25 INFO]: Test stats: {
    "score": -0.6871435371296065,
    "rmse": 0.6871435371296065
}
[08/07/2025 19:32:20 INFO]: Training loss at epoch 90: 0.39002974331378937
[08/07/2025 19:32:41 INFO]: New best epoch, val score: -0.7426055451513266
[08/07/2025 19:32:41 INFO]: Saving model to: model_best.pth
[08/07/2025 19:35:15 INFO]: Training loss at epoch 91: 0.373832032084465
[08/07/2025 19:35:36 INFO]: New best epoch, val score: -0.7271947325001779
[08/07/2025 19:35:36 INFO]: Saving model to: model_best.pth
[08/07/2025 19:38:11 INFO]: Training loss at epoch 92: 0.45442844927310944
[08/07/2025 19:41:06 INFO]: Training loss at epoch 93: 0.351908341050148
[08/07/2025 19:44:00 INFO]: Training loss at epoch 94: 0.405048131942749
[08/07/2025 19:46:55 INFO]: Training loss at epoch 95: 0.3340141996741295
[08/07/2025 19:47:16 INFO]: New best epoch, val score: -0.725735080185244
[08/07/2025 19:47:16 INFO]: Saving model to: model_best.pth
[08/07/2025 19:49:51 INFO]: Training loss at epoch 96: 0.48070839047431946
[08/07/2025 19:52:46 INFO]: Training loss at epoch 97: 0.456341028213501
[08/07/2025 19:55:40 INFO]: Training loss at epoch 98: 0.49313777685165405
[08/07/2025 19:58:35 INFO]: Training loss at epoch 99: 0.5133460462093353
[08/07/2025 19:59:34 INFO]: Training stats: {
    "score": -0.6582438369916084,
    "rmse": 0.6582438369916084
}
[08/07/2025 19:59:34 INFO]: Val stats: {
    "score": -0.7206691616388754,
    "rmse": 0.7206691616388754
}
[08/07/2025 19:59:34 INFO]: Test stats: {
    "score": -0.6752317617261635,
    "rmse": 0.6752317617261635
}
[08/07/2025 19:59:54 INFO]: New best epoch, val score: -0.7206691616388754
[08/07/2025 19:59:54 INFO]: Saving model to: model_best.pth
[08/07/2025 20:02:29 INFO]: Training loss at epoch 100: 0.4074719101190567
[08/07/2025 20:02:50 INFO]: New best epoch, val score: -0.7126334537737964
[08/07/2025 20:02:50 INFO]: Saving model to: model_best.pth
[08/07/2025 20:05:28 INFO]: Training loss at epoch 101: 0.3327508047223091
[08/07/2025 20:08:23 INFO]: Training loss at epoch 102: 0.3063970059156418
[08/07/2025 20:11:18 INFO]: Training loss at epoch 103: 0.39156848192214966
[08/07/2025 20:11:38 INFO]: New best epoch, val score: -0.7100725911482108
[08/07/2025 20:11:38 INFO]: Saving model to: model_best.pth
[08/07/2025 20:14:13 INFO]: Training loss at epoch 104: 0.454555407166481
[08/07/2025 20:17:08 INFO]: Training loss at epoch 105: 0.3987516611814499
[08/07/2025 20:20:03 INFO]: Training loss at epoch 106: 0.43386390805244446
[08/07/2025 20:22:57 INFO]: Training loss at epoch 107: 0.279680460691452
[08/07/2025 20:25:52 INFO]: Training loss at epoch 108: 0.3167661428451538
[08/07/2025 20:26:13 INFO]: New best epoch, val score: -0.7099669445632533
[08/07/2025 20:26:13 INFO]: Saving model to: model_best.pth
[08/07/2025 20:28:48 INFO]: Training loss at epoch 109: 0.3801335543394089
[08/07/2025 20:29:47 INFO]: Training stats: {
    "score": -0.565518253113236,
    "rmse": 0.565518253113236
}
[08/07/2025 20:29:47 INFO]: Val stats: {
    "score": -0.7095787603995203,
    "rmse": 0.7095787603995203
}
[08/07/2025 20:29:47 INFO]: Test stats: {
    "score": -0.65220915639954,
    "rmse": 0.65220915639954
}
[08/07/2025 20:30:07 INFO]: New best epoch, val score: -0.7095787603995203
[08/07/2025 20:30:07 INFO]: Saving model to: model_best.pth
[08/07/2025 20:32:42 INFO]: Training loss at epoch 110: 0.37350568175315857
[08/07/2025 20:35:36 INFO]: Training loss at epoch 111: 0.3596079498529434
[08/07/2025 20:38:31 INFO]: Training loss at epoch 112: 0.26785890758037567
[08/07/2025 20:38:52 INFO]: New best epoch, val score: -0.705651175716605
[08/07/2025 20:38:52 INFO]: Saving model to: model_best.pth
[08/07/2025 20:41:26 INFO]: Training loss at epoch 113: 0.3120504766702652
[08/07/2025 20:44:21 INFO]: Training loss at epoch 114: 0.3903948664665222
[08/07/2025 20:47:16 INFO]: Training loss at epoch 115: 0.2852661907672882
[08/07/2025 20:50:11 INFO]: Training loss at epoch 116: 0.37039513885974884
[08/07/2025 20:53:05 INFO]: Training loss at epoch 117: 0.31998561322689056
[08/07/2025 20:53:26 INFO]: New best epoch, val score: -0.7047464953064647
[08/07/2025 20:53:26 INFO]: Saving model to: model_best.pth
[08/07/2025 20:56:01 INFO]: Training loss at epoch 118: 0.3118685781955719
[08/07/2025 20:58:55 INFO]: Training loss at epoch 119: 0.29863008856773376
[08/07/2025 20:59:54 INFO]: Training stats: {
    "score": -0.539572946193103,
    "rmse": 0.539572946193103
}
[08/07/2025 20:59:54 INFO]: Val stats: {
    "score": -0.7562556266871396,
    "rmse": 0.7562556266871396
}
[08/07/2025 20:59:54 INFO]: Test stats: {
    "score": -0.7191074727889278,
    "rmse": 0.7191074727889278
}
[08/07/2025 21:02:49 INFO]: Training loss at epoch 120: 0.3419870585203171
[08/07/2025 21:05:44 INFO]: Training loss at epoch 121: 0.3108731359243393
[08/07/2025 21:08:39 INFO]: Training loss at epoch 122: 0.2948887348175049
[08/07/2025 21:11:34 INFO]: Training loss at epoch 123: 0.2679615020751953
[08/07/2025 21:14:28 INFO]: Training loss at epoch 124: 0.3007701635360718
[08/07/2025 21:14:49 INFO]: New best epoch, val score: -0.6973933169789406
[08/07/2025 21:14:49 INFO]: Saving model to: model_best.pth
[08/07/2025 21:17:24 INFO]: Training loss at epoch 125: 0.26008661836385727
[08/07/2025 21:17:45 INFO]: New best epoch, val score: -0.688752747633039
[08/07/2025 21:17:45 INFO]: Saving model to: model_best.pth
[08/07/2025 21:20:19 INFO]: Training loss at epoch 126: 0.30294986069202423
[08/07/2025 21:23:14 INFO]: Training loss at epoch 127: 0.40544092655181885
[08/07/2025 21:26:09 INFO]: Training loss at epoch 128: 0.3172536790370941
[08/07/2025 21:26:30 INFO]: New best epoch, val score: -0.6801154257016709
[08/07/2025 21:26:30 INFO]: Saving model to: model_best.pth
[08/07/2025 21:29:04 INFO]: Training loss at epoch 129: 0.309187650680542
[08/07/2025 21:30:03 INFO]: Training stats: {
    "score": -0.49316005058831375,
    "rmse": 0.49316005058831375
}
[08/07/2025 21:30:03 INFO]: Val stats: {
    "score": -0.6686076864596999,
    "rmse": 0.6686076864596999
}
[08/07/2025 21:30:03 INFO]: Test stats: {
    "score": -0.6804771038525786,
    "rmse": 0.6804771038525786
}
[08/07/2025 21:30:24 INFO]: New best epoch, val score: -0.6686076864596999
[08/07/2025 21:30:24 INFO]: Saving model to: model_best.pth
[08/07/2025 21:32:59 INFO]: Training loss at epoch 130: 0.25078460574150085
[08/07/2025 21:35:54 INFO]: Training loss at epoch 131: 0.2693185657262802
[08/07/2025 21:38:48 INFO]: Training loss at epoch 132: 0.24459517747163773
[08/07/2025 21:41:43 INFO]: Training loss at epoch 133: 0.2915775179862976
[08/07/2025 21:44:38 INFO]: Training loss at epoch 134: 0.24296557903289795
[08/07/2025 21:47:33 INFO]: Training loss at epoch 135: 0.2470293566584587
[08/07/2025 21:50:28 INFO]: Training loss at epoch 136: 0.22476394474506378
[08/07/2025 21:50:48 INFO]: New best epoch, val score: -0.6665800966233447
[08/07/2025 21:50:48 INFO]: Saving model to: model_best.pth
[08/07/2025 21:53:23 INFO]: Training loss at epoch 137: 0.18357129767537117
[08/07/2025 21:56:18 INFO]: Training loss at epoch 138: 0.2833185940980911
[08/07/2025 21:59:13 INFO]: Training loss at epoch 139: 0.24469485133886337
[08/07/2025 22:00:11 INFO]: Training stats: {
    "score": -0.5396566652506661,
    "rmse": 0.5396566652506661
}
[08/07/2025 22:00:11 INFO]: Val stats: {
    "score": -0.7502067369887834,
    "rmse": 0.7502067369887834
}
[08/07/2025 22:00:11 INFO]: Test stats: {
    "score": -0.7559055270498756,
    "rmse": 0.7559055270498756
}
[08/07/2025 22:03:06 INFO]: Training loss at epoch 140: 0.343854621052742
[08/07/2025 22:03:27 INFO]: New best epoch, val score: -0.6641249579271287
[08/07/2025 22:03:27 INFO]: Saving model to: model_best.pth
[08/07/2025 22:06:01 INFO]: Training loss at epoch 141: 0.21283726394176483
[08/07/2025 22:08:56 INFO]: Training loss at epoch 142: 0.2990756928920746
[08/07/2025 22:09:17 INFO]: New best epoch, val score: -0.6599324953386585
[08/07/2025 22:09:17 INFO]: Saving model to: model_best.pth
[08/07/2025 22:11:51 INFO]: Training loss at epoch 143: 0.2462686225771904
[08/07/2025 22:14:46 INFO]: Training loss at epoch 144: 0.27436476945877075
[08/07/2025 22:17:41 INFO]: Training loss at epoch 145: 0.18875975161790848
[08/07/2025 22:20:35 INFO]: Training loss at epoch 146: 0.3263333886861801
[08/07/2025 22:23:30 INFO]: Training loss at epoch 147: 0.2332524135708809
[08/07/2025 22:26:25 INFO]: Training loss at epoch 148: 0.3286765068769455
[08/07/2025 22:29:19 INFO]: Training loss at epoch 149: 0.2567046955227852
[08/07/2025 22:30:18 INFO]: Training stats: {
    "score": -0.4476242924626878,
    "rmse": 0.4476242924626878
}
[08/07/2025 22:30:18 INFO]: Val stats: {
    "score": -0.653227268855912,
    "rmse": 0.653227268855912
}
[08/07/2025 22:30:18 INFO]: Test stats: {
    "score": -0.6381269206901953,
    "rmse": 0.6381269206901953
}
[08/07/2025 22:30:39 INFO]: New best epoch, val score: -0.653227268855912
[08/07/2025 22:30:39 INFO]: Saving model to: model_best.pth
[08/07/2025 22:33:13 INFO]: Training loss at epoch 150: 0.2289465367794037
[08/07/2025 22:33:34 INFO]: New best epoch, val score: -0.6521168676166267
[08/07/2025 22:33:34 INFO]: Saving model to: model_best.pth
[08/07/2025 22:36:09 INFO]: Training loss at epoch 151: 0.19475097209215164
[08/07/2025 22:39:04 INFO]: Training loss at epoch 152: 0.17453710734844208
[08/07/2025 22:41:58 INFO]: Training loss at epoch 153: 0.27131443470716476
[08/07/2025 22:44:52 INFO]: Training loss at epoch 154: 0.19669976085424423
[08/07/2025 22:47:47 INFO]: Training loss at epoch 155: 0.18628844618797302
[08/07/2025 22:50:42 INFO]: Training loss at epoch 156: 0.19258521497249603
[08/07/2025 22:53:37 INFO]: Training loss at epoch 157: 0.18687530606985092
[08/07/2025 22:56:31 INFO]: Training loss at epoch 158: 0.20282885432243347
[08/07/2025 22:59:26 INFO]: Training loss at epoch 159: 0.21322984993457794
[08/07/2025 23:00:24 INFO]: Training stats: {
    "score": -0.4311477150723948,
    "rmse": 0.4311477150723948
}
[08/07/2025 23:00:24 INFO]: Val stats: {
    "score": -0.6663811367355664,
    "rmse": 0.6663811367355664
}
[08/07/2025 23:00:24 INFO]: Test stats: {
    "score": -0.6483081832331344,
    "rmse": 0.6483081832331344
}
[08/07/2025 23:03:19 INFO]: Training loss at epoch 160: 0.21942833065986633
[08/07/2025 23:06:14 INFO]: Training loss at epoch 161: 0.1550053246319294
[08/07/2025 23:09:08 INFO]: Training loss at epoch 162: 0.14684680104255676
[08/07/2025 23:12:03 INFO]: Training loss at epoch 163: 0.17183894664049149
[08/07/2025 23:14:57 INFO]: Training loss at epoch 164: 0.17548095434904099
[08/07/2025 23:17:52 INFO]: Training loss at epoch 165: 0.17993104457855225
[08/07/2025 23:20:47 INFO]: Training loss at epoch 166: 0.1580730900168419
[08/07/2025 23:23:42 INFO]: Training loss at epoch 167: 0.1668134704232216
[08/07/2025 23:26:36 INFO]: Training loss at epoch 168: 0.20327318459749222
[08/07/2025 23:29:31 INFO]: Training loss at epoch 169: 0.15918801724910736
[08/07/2025 23:30:30 INFO]: Training stats: {
    "score": -0.3934579281527135,
    "rmse": 0.3934579281527135
}
[08/07/2025 23:30:30 INFO]: Val stats: {
    "score": -0.7002117857409091,
    "rmse": 0.7002117857409091
}
[08/07/2025 23:30:30 INFO]: Test stats: {
    "score": -0.6742034439589716,
    "rmse": 0.6742034439589716
}
[08/07/2025 23:33:24 INFO]: Training loss at epoch 170: 0.18061069399118423
[08/07/2025 23:36:19 INFO]: Training loss at epoch 171: 0.13832952082157135
[08/07/2025 23:39:13 INFO]: Training loss at epoch 172: 0.16339749842882156
[08/07/2025 23:42:08 INFO]: Training loss at epoch 173: 0.15458961576223373
[08/07/2025 23:45:02 INFO]: Training loss at epoch 174: 0.1947239637374878
[08/07/2025 23:47:57 INFO]: Training loss at epoch 175: 0.15465814620256424
[08/07/2025 23:50:51 INFO]: Training loss at epoch 176: 0.18163806945085526
[08/07/2025 23:53:46 INFO]: Training loss at epoch 177: 0.14765677601099014
[08/07/2025 23:56:41 INFO]: Training loss at epoch 178: 0.14871934056282043
[08/07/2025 23:59:35 INFO]: Training loss at epoch 179: 0.20488803833723068
[08/08/2025 00:00:34 INFO]: Training stats: {
    "score": -0.3881311167775488,
    "rmse": 0.3881311167775488
}
[08/08/2025 00:00:34 INFO]: Val stats: {
    "score": -0.720092364893473,
    "rmse": 0.720092364893473
}
[08/08/2025 00:00:34 INFO]: Test stats: {
    "score": -0.7095429117874228,
    "rmse": 0.7095429117874228
}
[08/08/2025 00:03:29 INFO]: Training loss at epoch 180: 0.17069458961486816
[08/08/2025 00:06:23 INFO]: Training loss at epoch 181: 0.14479980245232582
[08/08/2025 00:06:44 INFO]: Running Final Evaluation...
[08/08/2025 00:07:56 INFO]: Training accuracy: {
    "score": -0.4714187005773634,
    "rmse": 0.4714187005773634
}
[08/08/2025 00:07:56 INFO]: Val accuracy: {
    "score": -0.6521168676166267,
    "rmse": 0.6521168676166267
}
[08/08/2025 00:07:56 INFO]: Test accuracy: {
    "score": -0.6379344446917831,
    "rmse": 0.6379344446917831
}
[08/08/2025 00:07:56 INFO]: {
    "dataset": "ic_upstream2",
    "model_name": "ft_transformer",
    "run_id": "graveless-Jacquleen",
    "best_epoch": 150,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.6379344446917831,
        "rmse": 0.6379344446917831
    },
    "train_stats": {
        "score": -0.4714187005773634,
        "rmse": 0.4714187005773634
    },
    "val_stats": {
        "score": -0.6521168676166267,
        "rmse": 0.6521168676166267
    }
}
[08/08/2025 00:07:56 INFO]: 
_________________________________________________

[08/08/2025 00:07:56 INFO]: train_net_for_optune.py main() running.
[08/08/2025 00:07:56 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 1.994740720424499
  attention_dropout: 0.08958052029947083
  ffn_dropout: 0.08958052029947083
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.8089387643965768e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: graveless-Jacquleen

[08/08/2025 00:07:56 INFO]: This ft_transformer has 0.819 million parameters.
[08/08/2025 00:07:56 INFO]: Training will start at epoch 0.
[08/08/2025 00:07:56 INFO]: ==> Starting training for 200 epochs...
[08/08/2025 00:08:18 INFO]: Training loss at epoch 0: 1.036509394645691
[08/08/2025 00:08:21 INFO]: New best epoch, val score: -0.9395929801112275
[08/08/2025 00:08:21 INFO]: Saving model to: model_best.pth
[08/08/2025 00:08:42 INFO]: Training loss at epoch 1: 0.947583943605423
[08/08/2025 00:08:45 INFO]: New best epoch, val score: -0.9338097829759474
[08/08/2025 00:08:45 INFO]: Saving model to: model_best.pth
[08/08/2025 00:09:05 INFO]: Training loss at epoch 2: 0.9138425886631012
[08/08/2025 00:09:08 INFO]: New best epoch, val score: -0.9308828921275244
[08/08/2025 00:09:08 INFO]: Saving model to: model_best.pth
[08/08/2025 00:09:28 INFO]: Training loss at epoch 3: 0.9096419811248779
[08/08/2025 00:09:31 INFO]: New best epoch, val score: -0.9292948803504629
[08/08/2025 00:09:31 INFO]: Saving model to: model_best.pth
[08/08/2025 00:09:52 INFO]: Training loss at epoch 4: 1.1892490983009338
[08/08/2025 00:10:15 INFO]: Training loss at epoch 5: 0.8063832223415375
[08/08/2025 00:10:38 INFO]: Training loss at epoch 6: 1.0799496471881866
[08/08/2025 00:11:01 INFO]: Training loss at epoch 7: 1.0573638081550598
[08/08/2025 00:11:24 INFO]: Training loss at epoch 8: 0.829313337802887
[08/08/2025 00:11:47 INFO]: Training loss at epoch 9: 0.9711799621582031
[08/08/2025 00:11:55 INFO]: Training stats: {
    "score": -0.9792937338585023,
    "rmse": 0.9792937338585023
}
[08/08/2025 00:11:55 INFO]: Val stats: {
    "score": -0.9388129647964387,
    "rmse": 0.9388129647964387
}
[08/08/2025 00:11:55 INFO]: Test stats: {
    "score": -0.9057108859850347,
    "rmse": 0.9057108859850347
}
[08/08/2025 00:12:19 INFO]: Training loss at epoch 10: 1.0746019184589386
[08/08/2025 00:12:42 INFO]: Training loss at epoch 11: 0.9349827468395233
[08/08/2025 00:13:05 INFO]: Training loss at epoch 12: 1.2953773438930511
[08/08/2025 00:13:28 INFO]: Training loss at epoch 13: 0.7698861658573151
[08/08/2025 00:13:51 INFO]: Training loss at epoch 14: 1.0522149801254272
[08/08/2025 00:14:14 INFO]: Training loss at epoch 15: 1.0861467719078064
[08/08/2025 00:14:37 INFO]: Training loss at epoch 16: 0.8584964573383331
[08/08/2025 00:15:00 INFO]: Training loss at epoch 17: 0.9986906945705414
[08/08/2025 00:15:24 INFO]: Training loss at epoch 18: 0.8816022276878357
[08/08/2025 00:15:47 INFO]: Training loss at epoch 19: 1.0602599382400513
[08/08/2025 00:15:55 INFO]: Training stats: {
    "score": -0.96238583108128,
    "rmse": 0.96238583108128
}
[08/08/2025 00:15:55 INFO]: Val stats: {
    "score": -0.9349739517079215,
    "rmse": 0.9349739517079215
}
[08/08/2025 00:15:55 INFO]: Test stats: {
    "score": -0.8957220789616163,
    "rmse": 0.8957220789616163
}
[08/08/2025 00:16:18 INFO]: Training loss at epoch 20: 1.1592147946357727
[08/08/2025 00:16:41 INFO]: Training loss at epoch 21: 1.0663162767887115
[08/08/2025 00:16:44 INFO]: New best epoch, val score: -0.9266115969557425
[08/08/2025 00:16:44 INFO]: Saving model to: model_best.pth
[08/08/2025 00:17:04 INFO]: Training loss at epoch 22: 0.884833425283432
[08/08/2025 00:17:07 INFO]: New best epoch, val score: -0.923191854877173
[08/08/2025 00:17:07 INFO]: Saving model to: model_best.pth
[08/08/2025 00:17:27 INFO]: Training loss at epoch 23: 1.0111610889434814
[08/08/2025 00:17:30 INFO]: New best epoch, val score: -0.9191104694112966
[08/08/2025 00:17:30 INFO]: Saving model to: model_best.pth
[08/08/2025 00:17:50 INFO]: Training loss at epoch 24: 1.0192084610462189
[08/08/2025 00:17:53 INFO]: New best epoch, val score: -0.9150484601596193
[08/08/2025 00:17:53 INFO]: Saving model to: model_best.pth
[08/08/2025 00:18:14 INFO]: Training loss at epoch 25: 0.911522388458252
[08/08/2025 00:18:17 INFO]: New best epoch, val score: -0.9111659407484987
[08/08/2025 00:18:17 INFO]: Saving model to: model_best.pth
[08/08/2025 00:18:37 INFO]: Training loss at epoch 26: 0.989438384771347
[08/08/2025 00:18:40 INFO]: New best epoch, val score: -0.9088584105647023
[08/08/2025 00:18:40 INFO]: Saving model to: model_best.pth
[08/08/2025 00:19:01 INFO]: Training loss at epoch 27: 0.9690893292427063
[08/08/2025 00:19:24 INFO]: Training loss at epoch 28: 1.0088897943496704
[08/08/2025 00:19:47 INFO]: Training loss at epoch 29: 0.9362935423851013
[08/08/2025 00:19:55 INFO]: Training stats: {
    "score": -0.9490892669099811,
    "rmse": 0.9490892669099811
}
[08/08/2025 00:19:55 INFO]: Val stats: {
    "score": -0.9171887173614731,
    "rmse": 0.9171887173614731
}
[08/08/2025 00:19:55 INFO]: Test stats: {
    "score": -0.8800594206085128,
    "rmse": 0.8800594206085128
}
[08/08/2025 00:20:19 INFO]: Training loss at epoch 30: 0.9882621467113495
[08/08/2025 00:20:42 INFO]: Training loss at epoch 31: 0.8198328018188477
[08/08/2025 00:21:05 INFO]: Training loss at epoch 32: 0.796748548746109
[08/08/2025 00:21:28 INFO]: Training loss at epoch 33: 0.7972889244556427
[08/08/2025 00:21:52 INFO]: Training loss at epoch 34: 0.9370236992835999
[08/08/2025 00:22:15 INFO]: Training loss at epoch 35: 0.9037522375583649
[08/08/2025 00:22:39 INFO]: Training loss at epoch 36: 0.8361031115055084
[08/08/2025 00:23:02 INFO]: Training loss at epoch 37: 0.8795117139816284
[08/08/2025 00:23:25 INFO]: Training loss at epoch 38: 0.7247918993234634
[08/08/2025 00:23:28 INFO]: New best epoch, val score: -0.9056116857467907
[08/08/2025 00:23:28 INFO]: Saving model to: model_best.pth
[08/08/2025 00:23:48 INFO]: Training loss at epoch 39: 0.9616706967353821
[08/08/2025 00:23:56 INFO]: Training stats: {
    "score": -0.9371532194262665,
    "rmse": 0.9371532194262665
}
[08/08/2025 00:23:56 INFO]: Val stats: {
    "score": -0.9009238241318515,
    "rmse": 0.9009238241318515
}
[08/08/2025 00:23:56 INFO]: Test stats: {
    "score": -0.8668565607538885,
    "rmse": 0.8668565607538885
}
[08/08/2025 00:23:59 INFO]: New best epoch, val score: -0.9009238241318515
[08/08/2025 00:23:59 INFO]: Saving model to: model_best.pth
[08/08/2025 00:24:20 INFO]: Training loss at epoch 40: 0.9194324910640717
[08/08/2025 00:24:23 INFO]: New best epoch, val score: -0.9004562026237276
[08/08/2025 00:24:23 INFO]: Saving model to: model_best.pth
[08/08/2025 00:24:44 INFO]: Training loss at epoch 41: 1.2727517783641815
[08/08/2025 00:24:47 INFO]: New best epoch, val score: -0.8979177107401881
[08/08/2025 00:24:47 INFO]: Saving model to: model_best.pth
[08/08/2025 00:25:07 INFO]: Training loss at epoch 42: 0.8762750327587128
[08/08/2025 00:25:30 INFO]: Training loss at epoch 43: 0.7046863734722137
[08/08/2025 00:25:54 INFO]: Training loss at epoch 44: 0.7277736663818359
[08/08/2025 00:26:17 INFO]: Training loss at epoch 45: 0.7770831286907196
[08/08/2025 00:26:40 INFO]: Training loss at epoch 46: 0.9829513132572174
[08/08/2025 00:27:03 INFO]: Training loss at epoch 47: 0.8971287608146667
[08/08/2025 00:27:26 INFO]: Training loss at epoch 48: 1.033155232667923
[08/08/2025 00:27:49 INFO]: Training loss at epoch 49: 0.868016242980957
[08/08/2025 00:27:57 INFO]: Training stats: {
    "score": -0.9219106251774353,
    "rmse": 0.9219106251774353
}
[08/08/2025 00:27:57 INFO]: Val stats: {
    "score": -0.9035638900563389,
    "rmse": 0.9035638900563389
}
[08/08/2025 00:27:57 INFO]: Test stats: {
    "score": -0.8609955920741237,
    "rmse": 0.8609955920741237
}
[08/08/2025 00:28:20 INFO]: Training loss at epoch 50: 0.9134601354598999
[08/08/2025 00:28:43 INFO]: Training loss at epoch 51: 0.938286691904068
[08/08/2025 00:28:46 INFO]: New best epoch, val score: -0.8942711088719724
[08/08/2025 00:28:46 INFO]: Saving model to: model_best.pth
[08/08/2025 00:29:06 INFO]: Training loss at epoch 52: 0.8091601729393005
[08/08/2025 00:29:09 INFO]: New best epoch, val score: -0.8903674463127842
[08/08/2025 00:29:09 INFO]: Saving model to: model_best.pth
[08/08/2025 00:29:29 INFO]: Training loss at epoch 53: 0.6819530427455902
[08/08/2025 00:29:32 INFO]: New best epoch, val score: -0.8858975525216685
[08/08/2025 00:29:32 INFO]: Saving model to: model_best.pth
[08/08/2025 00:29:53 INFO]: Training loss at epoch 54: 0.7073365747928619
[08/08/2025 00:29:56 INFO]: New best epoch, val score: -0.8828079378180065
[08/08/2025 00:29:56 INFO]: Saving model to: model_best.pth
[08/08/2025 00:30:16 INFO]: Training loss at epoch 55: 1.1892837285995483
[08/08/2025 00:30:19 INFO]: New best epoch, val score: -0.8820459716320789
[08/08/2025 00:30:19 INFO]: Saving model to: model_best.pth
[08/08/2025 00:30:39 INFO]: Training loss at epoch 56: 0.828260213136673
[08/08/2025 00:31:02 INFO]: Training loss at epoch 57: 0.8002305924892426
[08/08/2025 00:31:26 INFO]: Training loss at epoch 58: 1.0659719407558441
[08/08/2025 00:31:49 INFO]: Training loss at epoch 59: 0.9666089713573456
[08/08/2025 00:31:57 INFO]: Training stats: {
    "score": -0.9044919969868773,
    "rmse": 0.9044919969868773
}
[08/08/2025 00:31:57 INFO]: Val stats: {
    "score": -0.8875913899358903,
    "rmse": 0.8875913899358903
}
[08/08/2025 00:31:57 INFO]: Test stats: {
    "score": -0.8456482191275938,
    "rmse": 0.8456482191275938
}
[08/08/2025 00:32:20 INFO]: Training loss at epoch 60: 0.7549896240234375
[08/08/2025 00:32:22 INFO]: New best epoch, val score: -0.8806561370395125
[08/08/2025 00:32:22 INFO]: Saving model to: model_best.pth
[08/08/2025 00:32:43 INFO]: Training loss at epoch 61: 0.8879894316196442
[08/08/2025 00:32:46 INFO]: New best epoch, val score: -0.8776615845988509
[08/08/2025 00:32:46 INFO]: Saving model to: model_best.pth
[08/08/2025 00:33:06 INFO]: Training loss at epoch 62: 0.6775324046611786
[08/08/2025 00:33:09 INFO]: New best epoch, val score: -0.8759177193793528
[08/08/2025 00:33:09 INFO]: Saving model to: model_best.pth
[08/08/2025 00:33:29 INFO]: Training loss at epoch 63: 1.015727162361145
[08/08/2025 00:33:32 INFO]: New best epoch, val score: -0.8731237096893709
[08/08/2025 00:33:32 INFO]: Saving model to: model_best.pth
[08/08/2025 00:33:52 INFO]: Training loss at epoch 64: 0.654238685965538
[08/08/2025 00:33:55 INFO]: New best epoch, val score: -0.8687204810493001
[08/08/2025 00:33:55 INFO]: Saving model to: model_best.pth
[08/08/2025 00:34:16 INFO]: Training loss at epoch 65: 0.9494810402393341
[08/08/2025 00:34:18 INFO]: New best epoch, val score: -0.862947163879421
[08/08/2025 00:34:18 INFO]: Saving model to: model_best.pth
[08/08/2025 00:34:39 INFO]: Training loss at epoch 66: 0.9279398620128632
[08/08/2025 00:34:42 INFO]: New best epoch, val score: -0.8591898825361819
[08/08/2025 00:34:42 INFO]: Saving model to: model_best.pth
[08/08/2025 00:35:03 INFO]: Training loss at epoch 67: 0.8156006634235382
[08/08/2025 00:35:05 INFO]: New best epoch, val score: -0.8581616289637434
[08/08/2025 00:35:05 INFO]: Saving model to: model_best.pth
[08/08/2025 00:35:26 INFO]: Training loss at epoch 68: 1.0658591091632843
[08/08/2025 00:35:49 INFO]: Training loss at epoch 69: 0.8794369995594025
[08/08/2025 00:35:57 INFO]: Training stats: {
    "score": -0.8908672287581945,
    "rmse": 0.8908672287581945
}
[08/08/2025 00:35:57 INFO]: Val stats: {
    "score": -0.8613342775120753,
    "rmse": 0.8613342775120753
}
[08/08/2025 00:35:57 INFO]: Test stats: {
    "score": -0.828342953227641,
    "rmse": 0.828342953227641
}
[08/08/2025 00:36:20 INFO]: Training loss at epoch 70: 0.8217344582080841
[08/08/2025 00:36:43 INFO]: Training loss at epoch 71: 0.7562815248966217
[08/08/2025 00:37:06 INFO]: Training loss at epoch 72: 0.8791345655918121
[08/08/2025 00:37:29 INFO]: Training loss at epoch 73: 0.8883525729179382
[08/08/2025 00:37:52 INFO]: Training loss at epoch 74: 0.9700960516929626
[08/08/2025 00:38:15 INFO]: Training loss at epoch 75: 0.7852897644042969
[08/08/2025 00:38:39 INFO]: Training loss at epoch 76: 0.882553905248642
[08/08/2025 00:39:02 INFO]: Training loss at epoch 77: 0.9544002115726471
[08/08/2025 00:39:25 INFO]: Training loss at epoch 78: 0.7262759804725647
[08/08/2025 00:39:28 INFO]: New best epoch, val score: -0.8522942359994726
[08/08/2025 00:39:28 INFO]: Saving model to: model_best.pth
[08/08/2025 00:39:48 INFO]: Training loss at epoch 79: 0.947096049785614
[08/08/2025 00:39:56 INFO]: Training stats: {
    "score": -0.873503848948081,
    "rmse": 0.873503848948081
}
[08/08/2025 00:39:56 INFO]: Val stats: {
    "score": -0.8466101296849031,
    "rmse": 0.8466101296849031
}
[08/08/2025 00:39:56 INFO]: Test stats: {
    "score": -0.8147752364852379,
    "rmse": 0.8147752364852379
}
[08/08/2025 00:39:59 INFO]: New best epoch, val score: -0.8466101296849031
[08/08/2025 00:39:59 INFO]: Saving model to: model_best.pth
[08/08/2025 00:40:19 INFO]: Training loss at epoch 80: 0.7390638887882233
[08/08/2025 00:40:22 INFO]: New best epoch, val score: -0.8463992598944878
[08/08/2025 00:40:22 INFO]: Saving model to: model_best.pth
[08/08/2025 00:40:43 INFO]: Training loss at epoch 81: 0.7039972543716431
[08/08/2025 00:40:45 INFO]: New best epoch, val score: -0.8447814151322499
[08/08/2025 00:40:45 INFO]: Saving model to: model_best.pth
[08/08/2025 00:41:06 INFO]: Training loss at epoch 82: 0.9824518859386444
[08/08/2025 00:41:09 INFO]: New best epoch, val score: -0.8444085702605905
[08/08/2025 00:41:09 INFO]: Saving model to: model_best.pth
[08/08/2025 00:41:29 INFO]: Training loss at epoch 83: 0.73428675532341
[08/08/2025 00:41:53 INFO]: Training loss at epoch 84: 0.7995166778564453
[08/08/2025 00:42:16 INFO]: Training loss at epoch 85: 0.6314075738191605
[08/08/2025 00:42:39 INFO]: Training loss at epoch 86: 0.7394410073757172
[08/08/2025 00:42:42 INFO]: New best epoch, val score: -0.8437674811406634
[08/08/2025 00:42:42 INFO]: Saving model to: model_best.pth
[08/08/2025 00:43:02 INFO]: Training loss at epoch 87: 0.9265203773975372
[08/08/2025 00:43:05 INFO]: New best epoch, val score: -0.8397523648683348
[08/08/2025 00:43:05 INFO]: Saving model to: model_best.pth
[08/08/2025 00:43:26 INFO]: Training loss at epoch 88: 0.7445377707481384
[08/08/2025 00:43:29 INFO]: New best epoch, val score: -0.8358557886643437
[08/08/2025 00:43:29 INFO]: Saving model to: model_best.pth
[08/08/2025 00:43:49 INFO]: Training loss at epoch 89: 0.7622847557067871
[08/08/2025 00:43:57 INFO]: Training stats: {
    "score": -0.8544298131467486,
    "rmse": 0.8544298131467486
}
[08/08/2025 00:43:57 INFO]: Val stats: {
    "score": -0.8335405857727237,
    "rmse": 0.8335405857727237
}
[08/08/2025 00:43:57 INFO]: Test stats: {
    "score": -0.8017091995026264,
    "rmse": 0.8017091995026264
}
[08/08/2025 00:44:00 INFO]: New best epoch, val score: -0.8335405857727237
[08/08/2025 00:44:00 INFO]: Saving model to: model_best.pth
[08/08/2025 00:44:20 INFO]: Training loss at epoch 90: 0.706707090139389
[08/08/2025 00:44:43 INFO]: Training loss at epoch 91: 0.7599954903125763
[08/08/2025 00:45:07 INFO]: Training loss at epoch 92: 0.9399828612804413
[08/08/2025 00:45:30 INFO]: Training loss at epoch 93: 0.7796434760093689
[08/08/2025 00:45:53 INFO]: Training loss at epoch 94: 0.9639914035797119
[08/08/2025 00:46:16 INFO]: Training loss at epoch 95: 0.7529045343399048
[08/08/2025 00:46:39 INFO]: Training loss at epoch 96: 0.8747077286243439
[08/08/2025 00:47:02 INFO]: Training loss at epoch 97: 0.7859452366828918
[08/08/2025 00:47:25 INFO]: Training loss at epoch 98: 0.6091422438621521
[08/08/2025 00:47:49 INFO]: Training loss at epoch 99: 0.6924158334732056
[08/08/2025 00:47:57 INFO]: Training stats: {
    "score": -0.8272923338872475,
    "rmse": 0.8272923338872475
}
[08/08/2025 00:47:57 INFO]: Val stats: {
    "score": -0.8289617389436451,
    "rmse": 0.8289617389436451
}
[08/08/2025 00:47:57 INFO]: Test stats: {
    "score": -0.7866945019533299,
    "rmse": 0.7866945019533299
}
[08/08/2025 00:47:59 INFO]: New best epoch, val score: -0.8289617389436451
[08/08/2025 00:47:59 INFO]: Saving model to: model_best.pth
[08/08/2025 00:48:20 INFO]: Training loss at epoch 100: 0.7043339610099792
[08/08/2025 00:48:23 INFO]: New best epoch, val score: -0.8190336631333728
[08/08/2025 00:48:23 INFO]: Saving model to: model_best.pth
[08/08/2025 00:48:43 INFO]: Training loss at epoch 101: 0.8308982253074646
[08/08/2025 00:48:46 INFO]: New best epoch, val score: -0.8159468006162579
[08/08/2025 00:48:46 INFO]: Saving model to: model_best.pth
[08/08/2025 00:49:07 INFO]: Training loss at epoch 102: 0.7454539835453033
[08/08/2025 00:49:30 INFO]: Training loss at epoch 103: 0.788604199886322
[08/08/2025 00:49:53 INFO]: Training loss at epoch 104: 0.8442550897598267
[08/08/2025 00:50:16 INFO]: Training loss at epoch 105: 0.7385638654232025
[08/08/2025 00:50:39 INFO]: Training loss at epoch 106: 0.6958260238170624
[08/08/2025 00:51:02 INFO]: Training loss at epoch 107: 0.7370874583721161
[08/08/2025 00:51:25 INFO]: Training loss at epoch 108: 0.8169066905975342
[08/08/2025 00:51:48 INFO]: Training loss at epoch 109: 0.7474956512451172
[08/08/2025 00:51:56 INFO]: Training stats: {
    "score": -0.8090172159855565,
    "rmse": 0.8090172159855565
}
[08/08/2025 00:51:56 INFO]: Val stats: {
    "score": -0.8597048802081297,
    "rmse": 0.8597048802081297
}
[08/08/2025 00:51:56 INFO]: Test stats: {
    "score": -0.7888282223708283,
    "rmse": 0.7888282223708283
}
[08/08/2025 00:52:19 INFO]: Training loss at epoch 110: 0.5821801871061325
[08/08/2025 00:52:43 INFO]: Training loss at epoch 111: 0.6917260587215424
[08/08/2025 00:53:06 INFO]: Training loss at epoch 112: 0.77117520570755
[08/08/2025 00:53:09 INFO]: New best epoch, val score: -0.8135338042827284
[08/08/2025 00:53:09 INFO]: Saving model to: model_best.pth
[08/08/2025 00:53:29 INFO]: Training loss at epoch 113: 0.8004659116268158
[08/08/2025 00:53:32 INFO]: New best epoch, val score: -0.7967991439252159
[08/08/2025 00:53:32 INFO]: Saving model to: model_best.pth
[08/08/2025 00:53:52 INFO]: Training loss at epoch 114: 0.8127627074718475
[08/08/2025 00:53:55 INFO]: New best epoch, val score: -0.791523542039804
[08/08/2025 00:53:55 INFO]: Saving model to: model_best.pth
[08/08/2025 00:54:15 INFO]: Training loss at epoch 115: 0.6130816638469696
[08/08/2025 00:54:18 INFO]: New best epoch, val score: -0.7902388766344252
[08/08/2025 00:54:18 INFO]: Saving model to: model_best.pth
[08/08/2025 00:54:39 INFO]: Training loss at epoch 116: 0.6373761594295502
[08/08/2025 00:55:02 INFO]: Training loss at epoch 117: 0.7139014005661011
[08/08/2025 00:55:25 INFO]: Training loss at epoch 118: 0.7412386834621429
[08/08/2025 00:55:48 INFO]: Training loss at epoch 119: 0.7250351309776306
[08/08/2025 00:55:56 INFO]: Training stats: {
    "score": -0.7915047396661943,
    "rmse": 0.7915047396661943
}
[08/08/2025 00:55:56 INFO]: Val stats: {
    "score": -0.8463564723586033,
    "rmse": 0.8463564723586033
}
[08/08/2025 00:55:56 INFO]: Test stats: {
    "score": -0.776896175898599,
    "rmse": 0.776896175898599
}
[08/08/2025 00:56:19 INFO]: Training loss at epoch 120: 0.673075795173645
[08/08/2025 00:56:42 INFO]: Training loss at epoch 121: 0.8030862808227539
[08/08/2025 00:57:05 INFO]: Training loss at epoch 122: 0.5887274444103241
[08/08/2025 00:57:28 INFO]: Training loss at epoch 123: 0.654607355594635
[08/08/2025 00:57:51 INFO]: Training loss at epoch 124: 0.7737672030925751
[08/08/2025 00:58:14 INFO]: Training loss at epoch 125: 0.5916505455970764
[08/08/2025 00:58:17 INFO]: New best epoch, val score: -0.7816301926188214
[08/08/2025 00:58:17 INFO]: Saving model to: model_best.pth
[08/08/2025 00:58:38 INFO]: Training loss at epoch 126: 0.6434566378593445
[08/08/2025 00:58:40 INFO]: New best epoch, val score: -0.778275451804792
[08/08/2025 00:58:40 INFO]: Saving model to: model_best.pth
[08/08/2025 00:59:01 INFO]: Training loss at epoch 127: 0.6257066130638123
[08/08/2025 00:59:04 INFO]: New best epoch, val score: -0.777001303529724
[08/08/2025 00:59:04 INFO]: Saving model to: model_best.pth
[08/08/2025 00:59:24 INFO]: Training loss at epoch 128: 0.5976982712745667
[08/08/2025 00:59:27 INFO]: New best epoch, val score: -0.7766172838751793
[08/08/2025 00:59:27 INFO]: Saving model to: model_best.pth
[08/08/2025 00:59:47 INFO]: Training loss at epoch 129: 0.6655710637569427
[08/08/2025 00:59:55 INFO]: Training stats: {
    "score": -0.7895499247697901,
    "rmse": 0.7895499247697901
}
[08/08/2025 00:59:55 INFO]: Val stats: {
    "score": -0.7766900118264243,
    "rmse": 0.7766900118264243
}
[08/08/2025 00:59:55 INFO]: Test stats: {
    "score": -0.7618411441160399,
    "rmse": 0.7618411441160399
}
[08/08/2025 01:00:18 INFO]: Training loss at epoch 130: 0.6961817741394043
[08/08/2025 01:00:41 INFO]: Training loss at epoch 131: 0.6175658106803894
[08/08/2025 01:01:04 INFO]: Training loss at epoch 132: 0.5421670973300934
[08/08/2025 01:01:27 INFO]: Training loss at epoch 133: 0.5429407805204391
[08/08/2025 01:01:51 INFO]: Training loss at epoch 134: 0.7461263537406921
[08/08/2025 01:02:14 INFO]: Training loss at epoch 135: 0.7028505504131317
[08/08/2025 01:02:37 INFO]: Training loss at epoch 136: 0.7049873769283295
[08/08/2025 01:03:00 INFO]: Training loss at epoch 137: 0.5273664444684982
[08/08/2025 01:03:23 INFO]: Training loss at epoch 138: 0.616733193397522
[08/08/2025 01:03:26 INFO]: New best epoch, val score: -0.7739684876210997
[08/08/2025 01:03:26 INFO]: Saving model to: model_best.pth
[08/08/2025 01:03:46 INFO]: Training loss at epoch 139: 0.6069091856479645
[08/08/2025 01:03:54 INFO]: Training stats: {
    "score": -0.7671457806901067,
    "rmse": 0.7671457806901067
}
[08/08/2025 01:03:54 INFO]: Val stats: {
    "score": -0.7730017621195261,
    "rmse": 0.7730017621195261
}
[08/08/2025 01:03:54 INFO]: Test stats: {
    "score": -0.7483726015430545,
    "rmse": 0.7483726015430545
}
[08/08/2025 01:03:57 INFO]: New best epoch, val score: -0.7730017621195261
[08/08/2025 01:03:57 INFO]: Saving model to: model_best.pth
[08/08/2025 01:04:17 INFO]: Training loss at epoch 140: 0.6871653497219086
[08/08/2025 01:04:40 INFO]: Training loss at epoch 141: 0.5539034903049469
[08/08/2025 01:05:03 INFO]: Training loss at epoch 142: 0.6182906925678253
[08/08/2025 01:05:26 INFO]: Training loss at epoch 143: 0.648978054523468
[08/08/2025 01:05:50 INFO]: Training loss at epoch 144: 0.6899172365665436
[08/08/2025 01:06:13 INFO]: Training loss at epoch 145: 0.5359804034233093
[08/08/2025 01:06:36 INFO]: Training loss at epoch 146: 0.5019689798355103
[08/08/2025 01:06:39 INFO]: New best epoch, val score: -0.7714231077922936
[08/08/2025 01:06:39 INFO]: Saving model to: model_best.pth
[08/08/2025 01:06:59 INFO]: Training loss at epoch 147: 0.5187744200229645
[08/08/2025 01:07:02 INFO]: New best epoch, val score: -0.7593911173088864
[08/08/2025 01:07:02 INFO]: Saving model to: model_best.pth
[08/08/2025 01:07:22 INFO]: Training loss at epoch 148: 0.5693041682243347
[08/08/2025 01:07:25 INFO]: New best epoch, val score: -0.7531057794757752
[08/08/2025 01:07:25 INFO]: Saving model to: model_best.pth
[08/08/2025 01:07:46 INFO]: Training loss at epoch 149: 0.6385612785816193
[08/08/2025 01:07:54 INFO]: Training stats: {
    "score": -0.7836145284319356,
    "rmse": 0.7836145284319356
}
[08/08/2025 01:07:54 INFO]: Val stats: {
    "score": -0.7508194520553518,
    "rmse": 0.7508194520553518
}
[08/08/2025 01:07:54 INFO]: Test stats: {
    "score": -0.754842728513635,
    "rmse": 0.754842728513635
}
[08/08/2025 01:07:56 INFO]: New best epoch, val score: -0.7508194520553518
[08/08/2025 01:07:56 INFO]: Saving model to: model_best.pth
[08/08/2025 01:08:17 INFO]: Training loss at epoch 150: 0.7054627239704132
[08/08/2025 01:08:19 INFO]: New best epoch, val score: -0.7493041368688645
[08/08/2025 01:08:19 INFO]: Saving model to: model_best.pth
[08/08/2025 01:08:40 INFO]: Training loss at epoch 151: 0.5758617222309113
[08/08/2025 01:09:03 INFO]: Training loss at epoch 152: 0.6514318883419037
[08/08/2025 01:09:26 INFO]: Training loss at epoch 153: 0.5811866819858551
[08/08/2025 01:09:49 INFO]: Training loss at epoch 154: 0.6351281404495239
[08/08/2025 01:10:12 INFO]: Training loss at epoch 155: 0.631707489490509
[08/08/2025 01:10:36 INFO]: Training loss at epoch 156: 0.6008437871932983
[08/08/2025 01:10:59 INFO]: Training loss at epoch 157: 0.678891658782959
[08/08/2025 01:11:22 INFO]: Training loss at epoch 158: 0.5485062301158905
[08/08/2025 01:11:45 INFO]: Training loss at epoch 159: 0.4066070020198822
[08/08/2025 01:11:53 INFO]: Training stats: {
    "score": -0.7433517085638751,
    "rmse": 0.7433517085638751
}
[08/08/2025 01:11:53 INFO]: Val stats: {
    "score": -0.7477841619760697,
    "rmse": 0.7477841619760697
}
[08/08/2025 01:11:53 INFO]: Test stats: {
    "score": -0.721531810019466,
    "rmse": 0.721531810019466
}
[08/08/2025 01:11:56 INFO]: New best epoch, val score: -0.7477841619760697
[08/08/2025 01:11:56 INFO]: Saving model to: model_best.pth
[08/08/2025 01:12:16 INFO]: Training loss at epoch 160: 0.4863794296979904
[08/08/2025 01:12:19 INFO]: New best epoch, val score: -0.7427919977349382
[08/08/2025 01:12:19 INFO]: Saving model to: model_best.pth
[08/08/2025 01:12:39 INFO]: Training loss at epoch 161: 0.5638448596000671
[08/08/2025 01:12:42 INFO]: New best epoch, val score: -0.7410421713612333
[08/08/2025 01:12:42 INFO]: Saving model to: model_best.pth
[08/08/2025 01:13:03 INFO]: Training loss at epoch 162: 0.5728538334369659
[08/08/2025 01:13:26 INFO]: Training loss at epoch 163: 0.6497041583061218
[08/08/2025 01:13:49 INFO]: Training loss at epoch 164: 0.5076853632926941
[08/08/2025 01:14:12 INFO]: Training loss at epoch 165: 0.46462537348270416
[08/08/2025 01:14:35 INFO]: Training loss at epoch 166: 0.519268348813057
[08/08/2025 01:14:38 INFO]: New best epoch, val score: -0.7402390139008101
[08/08/2025 01:14:38 INFO]: Saving model to: model_best.pth
[08/08/2025 01:14:58 INFO]: Training loss at epoch 167: 0.5765016973018646
[08/08/2025 01:15:01 INFO]: New best epoch, val score: -0.735489026744232
[08/08/2025 01:15:01 INFO]: Saving model to: model_best.pth
[08/08/2025 01:15:22 INFO]: Training loss at epoch 168: 0.6265699565410614
[08/08/2025 01:15:45 INFO]: Training loss at epoch 169: 0.5303415656089783
[08/08/2025 01:15:53 INFO]: Training stats: {
    "score": -0.7307165468081069,
    "rmse": 0.7307165468081069
}
[08/08/2025 01:15:53 INFO]: Val stats: {
    "score": -0.7428530113069723,
    "rmse": 0.7428530113069723
}
[08/08/2025 01:15:53 INFO]: Test stats: {
    "score": -0.7124828020093898,
    "rmse": 0.7124828020093898
}
[08/08/2025 01:16:16 INFO]: Training loss at epoch 170: 0.6086931824684143
[08/08/2025 01:16:39 INFO]: Training loss at epoch 171: 0.546356588602066
[08/08/2025 01:17:02 INFO]: Training loss at epoch 172: 0.5410887598991394
[08/08/2025 01:17:25 INFO]: Training loss at epoch 173: 0.4406835734844208
[08/08/2025 01:17:48 INFO]: Training loss at epoch 174: 0.4960021674633026
[08/08/2025 01:18:12 INFO]: Training loss at epoch 175: 0.6794467866420746
[08/08/2025 01:18:35 INFO]: Training loss at epoch 176: 0.4751676917076111
[08/08/2025 01:18:58 INFO]: Training loss at epoch 177: 0.6047910153865814
[08/08/2025 01:19:21 INFO]: Training loss at epoch 178: 0.6373550891876221
[08/08/2025 01:19:44 INFO]: Training loss at epoch 179: 0.5959453880786896
[08/08/2025 01:19:52 INFO]: Training stats: {
    "score": -0.7124726369859433,
    "rmse": 0.7124726369859433
}
[08/08/2025 01:19:52 INFO]: Val stats: {
    "score": -0.7546483474151949,
    "rmse": 0.7546483474151949
}
[08/08/2025 01:19:52 INFO]: Test stats: {
    "score": -0.7032804652300816,
    "rmse": 0.7032804652300816
}
[08/08/2025 01:20:15 INFO]: Training loss at epoch 180: 0.51191645860672
[08/08/2025 01:20:38 INFO]: Training loss at epoch 181: 0.6671546101570129
[08/08/2025 01:21:01 INFO]: Training loss at epoch 182: 0.5958691537380219
[08/08/2025 01:21:24 INFO]: Training loss at epoch 183: 0.5787759125232697
[08/08/2025 01:21:47 INFO]: Training loss at epoch 184: 0.6799680590629578
[08/08/2025 01:22:10 INFO]: Training loss at epoch 185: 0.5720605254173279
[08/08/2025 01:22:34 INFO]: Training loss at epoch 186: 0.5111197680234909
[08/08/2025 01:22:57 INFO]: Training loss at epoch 187: 0.5797320902347565
[08/08/2025 01:23:20 INFO]: Training loss at epoch 188: 0.5409742593765259
[08/08/2025 01:23:43 INFO]: Training loss at epoch 189: 0.7487354129552841
[08/08/2025 01:23:51 INFO]: Training stats: {
    "score": -0.701281590513924,
    "rmse": 0.701281590513924
}
[08/08/2025 01:23:51 INFO]: Val stats: {
    "score": -0.7576645466245251,
    "rmse": 0.7576645466245251
}
[08/08/2025 01:23:51 INFO]: Test stats: {
    "score": -0.6931496821594283,
    "rmse": 0.6931496821594283
}
[08/08/2025 01:24:14 INFO]: Training loss at epoch 190: 0.5277986079454422
[08/08/2025 01:24:37 INFO]: Training loss at epoch 191: 0.5469872057437897
[08/08/2025 01:25:00 INFO]: Training loss at epoch 192: 0.4927441477775574
[08/08/2025 01:25:03 INFO]: New best epoch, val score: -0.7326689618711603
[08/08/2025 01:25:03 INFO]: Saving model to: model_best.pth
[08/08/2025 01:25:24 INFO]: Training loss at epoch 193: 0.5915195643901825
[08/08/2025 01:25:26 INFO]: New best epoch, val score: -0.7290467286437629
[08/08/2025 01:25:26 INFO]: Saving model to: model_best.pth
[08/08/2025 01:25:47 INFO]: Training loss at epoch 194: 0.5162627995014191
[08/08/2025 01:26:10 INFO]: Training loss at epoch 195: 0.5054406523704529
[08/08/2025 01:26:33 INFO]: Training loss at epoch 196: 0.5620377063751221
[08/08/2025 01:26:56 INFO]: Training loss at epoch 197: 0.5538020133972168
[08/08/2025 01:27:19 INFO]: Training loss at epoch 198: 0.4671473503112793
[08/08/2025 01:27:42 INFO]: Training loss at epoch 199: 0.4538152515888214
[08/08/2025 01:27:50 INFO]: Training stats: {
    "score": -0.7078098175360978,
    "rmse": 0.7078098175360978
}
[08/08/2025 01:27:50 INFO]: Val stats: {
    "score": -0.7246109110170188,
    "rmse": 0.7246109110170188
}
[08/08/2025 01:27:50 INFO]: Test stats: {
    "score": -0.6942889208153367,
    "rmse": 0.6942889208153367
}
[08/08/2025 01:27:53 INFO]: New best epoch, val score: -0.7246109110170188
[08/08/2025 01:27:53 INFO]: Saving model to: model_best.pth
[08/08/2025 01:27:53 INFO]: Running Final Evaluation...
[08/08/2025 01:28:01 INFO]: Training accuracy: {
    "score": -0.7078098176653513,
    "rmse": 0.7078098176653513
}
[08/08/2025 01:28:01 INFO]: Val accuracy: {
    "score": -0.7246109110170188,
    "rmse": 0.7246109110170188
}
[08/08/2025 01:28:01 INFO]: Test accuracy: {
    "score": -0.6942889208153367,
    "rmse": 0.6942889208153367
}
[08/08/2025 01:28:01 INFO]: {
    "dataset": "ic_upstream2",
    "model_name": "ft_transformer",
    "run_id": "graveless-Jacquleen",
    "best_epoch": 199,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.6942889208153367,
        "rmse": 0.6942889208153367
    },
    "train_stats": {
        "score": -0.7078098176653513,
        "rmse": 0.7078098176653513
    },
    "val_stats": {
        "score": -0.7246109110170188,
        "rmse": 0.7246109110170188
    }
}
[08/08/2025 01:28:01 INFO]: 
_________________________________________________

[08/08/2025 01:28:01 INFO]: train_net_for_optune.py main() running.
[08/08/2025 01:28:01 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 8
  d_ffn_factor: 1.446337973635853
  attention_dropout: 0.31696933229271484
  ffn_dropout: 0.31696933229271484
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0005799864680204953
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: graveless-Jacquleen

[08/08/2025 01:28:01 INFO]: This ft_transformer has 0.711 million parameters.
[08/08/2025 01:28:01 INFO]: Training will start at epoch 0.
[08/08/2025 01:28:01 INFO]: ==> Starting training for 200 epochs...
[08/08/2025 01:28:25 INFO]: Training loss at epoch 0: 1.5953501462936401
[08/08/2025 01:28:29 INFO]: New best epoch, val score: -1.28012007338098
[08/08/2025 01:28:29 INFO]: Saving model to: model_best.pth
[08/08/2025 01:28:53 INFO]: Training loss at epoch 1: 1.6144225001335144
[08/08/2025 01:28:56 INFO]: New best epoch, val score: -1.2319458816485231
[08/08/2025 01:28:56 INFO]: Saving model to: model_best.pth
[08/08/2025 01:29:21 INFO]: Training loss at epoch 2: 1.6788011193275452
[08/08/2025 01:29:24 INFO]: New best epoch, val score: -0.935234703839565
[08/08/2025 01:29:24 INFO]: Saving model to: model_best.pth
[08/08/2025 01:29:48 INFO]: Training loss at epoch 3: 1.050654023885727
[08/08/2025 01:30:16 INFO]: Training loss at epoch 4: 1.2450741529464722
[08/08/2025 01:30:44 INFO]: Training loss at epoch 5: 1.0462830662727356
[08/08/2025 01:31:11 INFO]: Training loss at epoch 6: 1.0005459487438202
[08/08/2025 01:31:39 INFO]: Training loss at epoch 7: 1.1582562923431396
[08/08/2025 01:32:06 INFO]: Training loss at epoch 8: 1.195517897605896
[08/08/2025 01:32:34 INFO]: Training loss at epoch 9: 1.0709823369979858
[08/08/2025 01:32:44 INFO]: Training stats: {
    "score": -1.0167883385786791,
    "rmse": 1.0167883385786791
}
[08/08/2025 01:32:44 INFO]: Val stats: {
    "score": -1.0142998290927396,
    "rmse": 1.0142998290927396
}
[08/08/2025 01:32:44 INFO]: Test stats: {
    "score": -0.9673087513171896,
    "rmse": 0.9673087513171896
}
[08/08/2025 01:33:11 INFO]: Training loss at epoch 10: 0.9336738288402557
[08/08/2025 01:33:39 INFO]: Training loss at epoch 11: 1.0137958228588104
[08/08/2025 01:34:06 INFO]: Training loss at epoch 12: 1.1808738708496094
[08/08/2025 01:34:10 INFO]: New best epoch, val score: -0.9341994560884614
[08/08/2025 01:34:10 INFO]: Saving model to: model_best.pth
[08/08/2025 01:34:34 INFO]: Training loss at epoch 13: 0.8392306864261627
[08/08/2025 01:34:38 INFO]: New best epoch, val score: -0.9329002835708303
[08/08/2025 01:34:38 INFO]: Saving model to: model_best.pth
[08/08/2025 01:35:02 INFO]: Training loss at epoch 14: 0.8622218370437622
[08/08/2025 01:35:29 INFO]: Training loss at epoch 15: 1.0236527919769287
[08/08/2025 01:35:57 INFO]: Training loss at epoch 16: 0.954991340637207
[08/08/2025 01:36:24 INFO]: Training loss at epoch 17: 0.9285216331481934
[08/08/2025 01:36:52 INFO]: Training loss at epoch 18: 0.9291934967041016
[08/08/2025 01:37:20 INFO]: Training loss at epoch 19: 0.8934366405010223
[08/08/2025 01:37:29 INFO]: Training stats: {
    "score": -0.9953376712220796,
    "rmse": 0.9953376712220796
}
[08/08/2025 01:37:29 INFO]: Val stats: {
    "score": -0.9563368890212172,
    "rmse": 0.9563368890212172
}
[08/08/2025 01:37:29 INFO]: Test stats: {
    "score": -0.9242096171148099,
    "rmse": 0.9242096171148099
}
[08/08/2025 01:37:57 INFO]: Training loss at epoch 20: 0.9773464798927307
[08/08/2025 01:38:24 INFO]: Training loss at epoch 21: 0.8288529217243195
[08/08/2025 01:38:52 INFO]: Training loss at epoch 22: 0.9943118691444397
[08/08/2025 01:39:19 INFO]: Training loss at epoch 23: 0.9312584698200226
[08/08/2025 01:39:47 INFO]: Training loss at epoch 24: 0.9469242990016937
[08/08/2025 01:40:15 INFO]: Training loss at epoch 25: 1.0915203392505646
[08/08/2025 01:40:42 INFO]: Training loss at epoch 26: 0.9241641759872437
[08/08/2025 01:41:10 INFO]: Training loss at epoch 27: 1.0487622022628784
[08/08/2025 01:41:38 INFO]: Training loss at epoch 28: 1.0681541562080383
[08/08/2025 01:42:05 INFO]: Training loss at epoch 29: 0.9623520374298096
[08/08/2025 01:42:15 INFO]: Training stats: {
    "score": -0.9907806289270177,
    "rmse": 0.9907806289270177
}
[08/08/2025 01:42:15 INFO]: Val stats: {
    "score": -0.9460585225564309,
    "rmse": 0.9460585225564309
}
[08/08/2025 01:42:15 INFO]: Test stats: {
    "score": -0.9166946144033222,
    "rmse": 0.9166946144033222
}
[08/08/2025 01:42:43 INFO]: Training loss at epoch 30: 0.9022690057754517
[08/08/2025 01:43:10 INFO]: Training loss at epoch 31: 1.0148167610168457
[08/08/2025 01:43:38 INFO]: Training loss at epoch 32: 0.9575572311878204
[08/08/2025 01:44:05 INFO]: Training loss at epoch 33: 0.9578868746757507
[08/08/2025 01:44:33 INFO]: Training loss at epoch 34: 1.0952756106853485
[08/08/2025 01:45:00 INFO]: Training loss at epoch 35: 0.9977962374687195
[08/08/2025 01:45:28 INFO]: Training loss at epoch 36: 1.0063359141349792
[08/08/2025 01:45:56 INFO]: Training loss at epoch 37: 1.009093463420868
[08/08/2025 01:46:23 INFO]: Training loss at epoch 38: 0.856038898229599
[08/08/2025 01:46:51 INFO]: Training loss at epoch 39: 1.0590604841709137
[08/08/2025 01:47:00 INFO]: Training stats: {
    "score": -0.9852018239556564,
    "rmse": 0.9852018239556564
}
[08/08/2025 01:47:00 INFO]: Val stats: {
    "score": -0.9587170977044415,
    "rmse": 0.9587170977044415
}
[08/08/2025 01:47:00 INFO]: Test stats: {
    "score": -0.9215850823743947,
    "rmse": 0.9215850823743947
}
[08/08/2025 01:47:28 INFO]: Training loss at epoch 40: 0.993028998374939
[08/08/2025 01:47:56 INFO]: Training loss at epoch 41: 0.8831997215747833
[08/08/2025 01:48:23 INFO]: Training loss at epoch 42: 0.9040890038013458
[08/08/2025 01:48:27 INFO]: New best epoch, val score: -0.9323294238300145
[08/08/2025 01:48:27 INFO]: Saving model to: model_best.pth
[08/08/2025 01:48:51 INFO]: Training loss at epoch 43: 0.857360303401947
[08/08/2025 01:48:54 INFO]: New best epoch, val score: -0.9275159784386778
[08/08/2025 01:48:54 INFO]: Saving model to: model_best.pth
[08/08/2025 01:49:18 INFO]: Training loss at epoch 44: 1.2326753735542297
[08/08/2025 01:49:22 INFO]: New best epoch, val score: -0.9244069797263813
[08/08/2025 01:49:22 INFO]: Saving model to: model_best.pth
[08/08/2025 01:49:46 INFO]: Training loss at epoch 45: 0.8886256515979767
[08/08/2025 01:50:14 INFO]: Training loss at epoch 46: 0.8669703900814056
[08/08/2025 01:50:41 INFO]: Training loss at epoch 47: 0.9600081741809845
[08/08/2025 01:51:09 INFO]: Training loss at epoch 48: 1.0244054794311523
[08/08/2025 01:51:37 INFO]: Training loss at epoch 49: 0.9023507833480835
[08/08/2025 01:51:46 INFO]: Training stats: {
    "score": -0.9704531775507275,
    "rmse": 0.9704531775507275
}
[08/08/2025 01:51:46 INFO]: Val stats: {
    "score": -0.9395027877688403,
    "rmse": 0.9395027877688403
}
[08/08/2025 01:51:46 INFO]: Test stats: {
    "score": -0.9048733915868297,
    "rmse": 0.9048733915868297
}
[08/08/2025 01:52:14 INFO]: Training loss at epoch 50: 0.9153843820095062
[08/08/2025 01:52:41 INFO]: Training loss at epoch 51: 0.9110206663608551
[08/08/2025 01:53:09 INFO]: Training loss at epoch 52: 0.9781734347343445
[08/08/2025 01:53:37 INFO]: Training loss at epoch 53: 0.8816387057304382
[08/08/2025 01:53:40 INFO]: New best epoch, val score: -0.9214802414448668
[08/08/2025 01:53:40 INFO]: Saving model to: model_best.pth
[08/08/2025 01:54:04 INFO]: Training loss at epoch 54: 1.0235904455184937
[08/08/2025 01:54:08 INFO]: New best epoch, val score: -0.9121566640120781
[08/08/2025 01:54:08 INFO]: Saving model to: model_best.pth
[08/08/2025 01:54:32 INFO]: Training loss at epoch 55: 0.9202103614807129
[08/08/2025 01:54:36 INFO]: New best epoch, val score: -0.9088679153317389
[08/08/2025 01:54:36 INFO]: Saving model to: model_best.pth
[08/08/2025 01:55:00 INFO]: Training loss at epoch 56: 0.8289632797241211
[08/08/2025 01:55:03 INFO]: New best epoch, val score: -0.9062241395799759
[08/08/2025 01:55:03 INFO]: Saving model to: model_best.pth
[08/08/2025 01:55:28 INFO]: Training loss at epoch 57: 0.7778895497322083
[08/08/2025 01:55:31 INFO]: New best epoch, val score: -0.902213147484536
[08/08/2025 01:55:31 INFO]: Saving model to: model_best.pth
[08/08/2025 01:55:55 INFO]: Training loss at epoch 58: 0.9073916375637054
[08/08/2025 01:55:59 INFO]: New best epoch, val score: -0.8942803133948412
[08/08/2025 01:55:59 INFO]: Saving model to: model_best.pth
[08/08/2025 01:56:23 INFO]: Training loss at epoch 59: 0.875520259141922
[08/08/2025 01:56:33 INFO]: Training stats: {
    "score": -0.9301156177444865,
    "rmse": 0.9301156177444865
}
[08/08/2025 01:56:33 INFO]: Val stats: {
    "score": -0.8859465362925331,
    "rmse": 0.8859465362925331
}
[08/08/2025 01:56:33 INFO]: Test stats: {
    "score": -0.8598286807817965,
    "rmse": 0.8598286807817965
}
[08/08/2025 01:56:36 INFO]: New best epoch, val score: -0.8859465362925331
[08/08/2025 01:56:36 INFO]: Saving model to: model_best.pth
[08/08/2025 01:57:00 INFO]: Training loss at epoch 60: 0.83833909034729
[08/08/2025 01:57:04 INFO]: New best epoch, val score: -0.8840519472203097
[08/08/2025 01:57:04 INFO]: Saving model to: model_best.pth
[08/08/2025 01:57:28 INFO]: Training loss at epoch 61: 0.6681487709283829
[08/08/2025 01:57:55 INFO]: Training loss at epoch 62: 0.8280258476734161
[08/08/2025 01:58:23 INFO]: Training loss at epoch 63: 0.7476925253868103
[08/08/2025 01:58:26 INFO]: New best epoch, val score: -0.870668672164863
[08/08/2025 01:58:26 INFO]: Saving model to: model_best.pth
[08/08/2025 01:58:51 INFO]: Training loss at epoch 64: 0.8039356470108032
[08/08/2025 01:58:54 INFO]: New best epoch, val score: -0.840939935517822
[08/08/2025 01:58:54 INFO]: Saving model to: model_best.pth
[08/08/2025 01:59:18 INFO]: Training loss at epoch 65: 0.7486664652824402
[08/08/2025 01:59:22 INFO]: New best epoch, val score: -0.8221696323938139
[08/08/2025 01:59:22 INFO]: Saving model to: model_best.pth
[08/08/2025 01:59:46 INFO]: Training loss at epoch 66: 0.7956719398498535
[08/08/2025 02:00:14 INFO]: Training loss at epoch 67: 0.7208526432514191
[08/08/2025 02:00:41 INFO]: Training loss at epoch 68: 0.7415831983089447
[08/08/2025 02:01:09 INFO]: Training loss at epoch 69: 0.5185423046350479
[08/08/2025 02:01:18 INFO]: Training stats: {
    "score": -0.772464152434456,
    "rmse": 0.772464152434456
}
[08/08/2025 02:01:18 INFO]: Val stats: {
    "score": -0.8001363844266511,
    "rmse": 0.8001363844266511
}
[08/08/2025 02:01:18 INFO]: Test stats: {
    "score": -0.7659677516078249,
    "rmse": 0.7659677516078249
}
[08/08/2025 02:01:22 INFO]: New best epoch, val score: -0.8001363844266511
[08/08/2025 02:01:22 INFO]: Saving model to: model_best.pth
[08/08/2025 02:01:46 INFO]: Training loss at epoch 70: 0.6260361075401306
[08/08/2025 02:02:14 INFO]: Training loss at epoch 71: 0.556198239326477
[08/08/2025 02:02:42 INFO]: Training loss at epoch 72: 0.6493740081787109
[08/08/2025 02:03:09 INFO]: Training loss at epoch 73: 0.7100242972373962
[08/08/2025 02:03:37 INFO]: Training loss at epoch 74: 0.5527162849903107
[08/08/2025 02:04:04 INFO]: Training loss at epoch 75: 0.5987788140773773
[08/08/2025 02:04:32 INFO]: Training loss at epoch 76: 0.5907751321792603
[08/08/2025 02:05:00 INFO]: Training loss at epoch 77: 0.5646825134754181
[08/08/2025 02:05:27 INFO]: Training loss at epoch 78: 0.6529079675674438
[08/08/2025 02:05:55 INFO]: Training loss at epoch 79: 0.45901307463645935
[08/08/2025 02:06:05 INFO]: Training stats: {
    "score": -0.688630010685225,
    "rmse": 0.688630010685225
}
[08/08/2025 02:06:05 INFO]: Val stats: {
    "score": -0.7587051778113393,
    "rmse": 0.7587051778113393
}
[08/08/2025 02:06:05 INFO]: Test stats: {
    "score": -0.7015898587604554,
    "rmse": 0.7015898587604554
}
[08/08/2025 02:06:08 INFO]: New best epoch, val score: -0.7587051778113393
[08/08/2025 02:06:08 INFO]: Saving model to: model_best.pth
[08/08/2025 02:06:32 INFO]: Training loss at epoch 80: 0.5362984985113144
[08/08/2025 02:06:36 INFO]: New best epoch, val score: -0.7556066308384057
[08/08/2025 02:06:36 INFO]: Saving model to: model_best.pth
[08/08/2025 02:07:00 INFO]: Training loss at epoch 81: 0.44436804950237274
[08/08/2025 02:07:04 INFO]: New best epoch, val score: -0.7452270835772518
[08/08/2025 02:07:04 INFO]: Saving model to: model_best.pth
[08/08/2025 02:07:28 INFO]: Training loss at epoch 82: 0.5909485518932343
[08/08/2025 02:07:31 INFO]: New best epoch, val score: -0.7376758178441595
[08/08/2025 02:07:31 INFO]: Saving model to: model_best.pth
[08/08/2025 02:07:56 INFO]: Training loss at epoch 83: 0.5609396547079086
[08/08/2025 02:08:23 INFO]: Training loss at epoch 84: 0.43660327792167664
[08/08/2025 02:08:51 INFO]: Training loss at epoch 85: 0.5964658111333847
[08/08/2025 02:09:18 INFO]: Training loss at epoch 86: 0.4445197880268097
[08/08/2025 02:09:46 INFO]: Training loss at epoch 87: 0.42998436093330383
[08/08/2025 02:10:13 INFO]: Training loss at epoch 88: 0.47250373661518097
[08/08/2025 02:10:41 INFO]: Training loss at epoch 89: 0.5065006464719772
[08/08/2025 02:10:51 INFO]: Training stats: {
    "score": -0.6696368711227979,
    "rmse": 0.6696368711227979
}
[08/08/2025 02:10:51 INFO]: Val stats: {
    "score": -0.7823486582841696,
    "rmse": 0.7823486582841696
}
[08/08/2025 02:10:51 INFO]: Test stats: {
    "score": -0.7156146323183638,
    "rmse": 0.7156146323183638
}
[08/08/2025 02:11:18 INFO]: Training loss at epoch 90: 0.40066999197006226
[08/08/2025 02:11:46 INFO]: Training loss at epoch 91: 0.3708212524652481
[08/08/2025 02:11:49 INFO]: New best epoch, val score: -0.7372273289827095
[08/08/2025 02:11:49 INFO]: Saving model to: model_best.pth
[08/08/2025 02:12:14 INFO]: Training loss at epoch 92: 0.7089766710996628
[08/08/2025 02:12:41 INFO]: Training loss at epoch 93: 0.4343404918909073
[08/08/2025 02:13:09 INFO]: Training loss at epoch 94: 0.4943455159664154
[08/08/2025 02:13:36 INFO]: Training loss at epoch 95: 0.5214188247919083
[08/08/2025 02:14:04 INFO]: Training loss at epoch 96: 0.36366409063339233
[08/08/2025 02:14:07 INFO]: New best epoch, val score: -0.7247909390509293
[08/08/2025 02:14:07 INFO]: Saving model to: model_best.pth
[08/08/2025 02:14:31 INFO]: Training loss at epoch 97: 0.5990941822528839
[08/08/2025 02:14:35 INFO]: New best epoch, val score: -0.7233578560734384
[08/08/2025 02:14:35 INFO]: Saving model to: model_best.pth
[08/08/2025 02:14:59 INFO]: Training loss at epoch 98: 0.42752446234226227
[08/08/2025 02:15:27 INFO]: Training loss at epoch 99: 0.45562879741191864
[08/08/2025 02:15:36 INFO]: Training stats: {
    "score": -0.6567665025348012,
    "rmse": 0.6567665025348012
}
[08/08/2025 02:15:36 INFO]: Val stats: {
    "score": -0.7999231016434314,
    "rmse": 0.7999231016434314
}
[08/08/2025 02:15:36 INFO]: Test stats: {
    "score": -0.6984520748479116,
    "rmse": 0.6984520748479116
}
[08/08/2025 02:16:04 INFO]: Training loss at epoch 100: 0.45344123244285583
[08/08/2025 02:16:32 INFO]: Training loss at epoch 101: 0.443246528506279
[08/08/2025 02:16:35 INFO]: New best epoch, val score: -0.710588894993654
[08/08/2025 02:16:35 INFO]: Saving model to: model_best.pth
[08/08/2025 02:16:59 INFO]: Training loss at epoch 102: 0.3846172094345093
[08/08/2025 02:17:27 INFO]: Training loss at epoch 103: 0.41403810679912567
[08/08/2025 02:17:54 INFO]: Training loss at epoch 104: 0.3536189943552017
[08/08/2025 02:18:22 INFO]: Training loss at epoch 105: 0.3718812018632889
[08/08/2025 02:18:26 INFO]: New best epoch, val score: -0.7050982661628165
[08/08/2025 02:18:26 INFO]: Saving model to: model_best.pth
[08/08/2025 02:18:50 INFO]: Training loss at epoch 106: 0.391207292675972
[08/08/2025 02:19:17 INFO]: Training loss at epoch 107: 0.3566150963306427
[08/08/2025 02:19:45 INFO]: Training loss at epoch 108: 0.41861869394779205
[08/08/2025 02:20:13 INFO]: Training loss at epoch 109: 0.43234002590179443
[08/08/2025 02:20:22 INFO]: Training stats: {
    "score": -0.6194520452146681,
    "rmse": 0.6194520452146681
}
[08/08/2025 02:20:22 INFO]: Val stats: {
    "score": -0.7026497896234496,
    "rmse": 0.7026497896234496
}
[08/08/2025 02:20:22 INFO]: Test stats: {
    "score": -0.6746503498170004,
    "rmse": 0.6746503498170004
}
[08/08/2025 02:20:26 INFO]: New best epoch, val score: -0.7026497896234496
[08/08/2025 02:20:26 INFO]: Saving model to: model_best.pth
[08/08/2025 02:20:50 INFO]: Training loss at epoch 110: 0.33393873274326324
[08/08/2025 02:21:17 INFO]: Training loss at epoch 111: 0.374341681599617
[08/08/2025 02:21:45 INFO]: Training loss at epoch 112: 0.39334382116794586
[08/08/2025 02:21:48 INFO]: New best epoch, val score: -0.6941352773227907
[08/08/2025 02:21:48 INFO]: Saving model to: model_best.pth
[08/08/2025 02:22:13 INFO]: Training loss at epoch 113: 0.33866578340530396
[08/08/2025 02:22:16 INFO]: New best epoch, val score: -0.6753692369650023
[08/08/2025 02:22:16 INFO]: Saving model to: model_best.pth
[08/08/2025 02:22:41 INFO]: Training loss at epoch 114: 0.40588486194610596
[08/08/2025 02:23:08 INFO]: Training loss at epoch 115: 0.3477686494588852
[08/08/2025 02:23:36 INFO]: Training loss at epoch 116: 0.28788795322179794
[08/08/2025 02:24:03 INFO]: Training loss at epoch 117: 0.32850413024425507
[08/08/2025 02:24:31 INFO]: Training loss at epoch 118: 0.4138851463794708
[08/08/2025 02:24:59 INFO]: Training loss at epoch 119: 0.3303009867668152
[08/08/2025 02:25:08 INFO]: Training stats: {
    "score": -0.6041411314052841,
    "rmse": 0.6041411314052841
}
[08/08/2025 02:25:08 INFO]: Val stats: {
    "score": -0.7438221337881503,
    "rmse": 0.7438221337881503
}
[08/08/2025 02:25:08 INFO]: Test stats: {
    "score": -0.6895568734261198,
    "rmse": 0.6895568734261198
}
[08/08/2025 02:25:36 INFO]: Training loss at epoch 120: 0.3763631582260132
[08/08/2025 02:26:04 INFO]: Training loss at epoch 121: 0.3923991024494171
[08/08/2025 02:26:31 INFO]: Training loss at epoch 122: 0.31143419444561005
[08/08/2025 02:26:58 INFO]: Training loss at epoch 123: 0.3274432569742203
[08/08/2025 02:27:02 INFO]: New best epoch, val score: -0.6689845682604841
[08/08/2025 02:27:02 INFO]: Saving model to: model_best.pth
[08/08/2025 02:27:26 INFO]: Training loss at epoch 124: 0.2631184011697769
[08/08/2025 02:27:54 INFO]: Training loss at epoch 125: 0.39964935183525085
[08/08/2025 02:28:21 INFO]: Training loss at epoch 126: 0.33774757385253906
[08/08/2025 02:28:49 INFO]: Training loss at epoch 127: 0.41976556181907654
[08/08/2025 02:28:52 INFO]: New best epoch, val score: -0.6631217715615978
[08/08/2025 02:28:52 INFO]: Saving model to: model_best.pth
[08/08/2025 02:29:16 INFO]: Training loss at epoch 128: 0.3135409653186798
[08/08/2025 02:29:44 INFO]: Training loss at epoch 129: 0.38413043320178986
[08/08/2025 02:29:54 INFO]: Training stats: {
    "score": -0.5728052094121532,
    "rmse": 0.5728052094121532
}
[08/08/2025 02:29:54 INFO]: Val stats: {
    "score": -0.709926494210878,
    "rmse": 0.709926494210878
}
[08/08/2025 02:29:54 INFO]: Test stats: {
    "score": -0.6787413922280291,
    "rmse": 0.6787413922280291
}
[08/08/2025 02:30:21 INFO]: Training loss at epoch 130: 0.3507539927959442
[08/08/2025 02:30:49 INFO]: Training loss at epoch 131: 0.3452618718147278
[08/08/2025 02:31:16 INFO]: Training loss at epoch 132: 0.31531383097171783
[08/08/2025 02:31:44 INFO]: Training loss at epoch 133: 0.37912537157535553
[08/08/2025 02:32:11 INFO]: Training loss at epoch 134: 0.43587248027324677
[08/08/2025 02:32:39 INFO]: Training loss at epoch 135: 0.32072797417640686
[08/08/2025 02:33:06 INFO]: Training loss at epoch 136: 0.3783574551343918
[08/08/2025 02:33:34 INFO]: Training loss at epoch 137: 0.3971826583147049
[08/08/2025 02:34:01 INFO]: Training loss at epoch 138: 0.32849544286727905
[08/08/2025 02:34:29 INFO]: Training loss at epoch 139: 0.3061151057481766
[08/08/2025 02:34:39 INFO]: Training stats: {
    "score": -0.5470958133635454,
    "rmse": 0.5470958133635454
}
[08/08/2025 02:34:39 INFO]: Val stats: {
    "score": -0.677463965459535,
    "rmse": 0.677463965459535
}
[08/08/2025 02:34:39 INFO]: Test stats: {
    "score": -0.6253959972830768,
    "rmse": 0.6253959972830768
}
[08/08/2025 02:35:06 INFO]: Training loss at epoch 140: 0.4011087566614151
[08/08/2025 02:35:34 INFO]: Training loss at epoch 141: 0.2621990740299225
[08/08/2025 02:36:01 INFO]: Training loss at epoch 142: 0.3692726194858551
[08/08/2025 02:36:29 INFO]: Training loss at epoch 143: 0.26773974299430847
[08/08/2025 02:36:56 INFO]: Training loss at epoch 144: 0.31921176612377167
[08/08/2025 02:37:24 INFO]: Training loss at epoch 145: 0.23054997622966766
[08/08/2025 02:37:51 INFO]: Training loss at epoch 146: 0.32536642253398895
[08/08/2025 02:38:19 INFO]: Training loss at epoch 147: 0.2805807441473007
[08/08/2025 02:38:46 INFO]: Training loss at epoch 148: 0.40188801288604736
[08/08/2025 02:39:14 INFO]: Training loss at epoch 149: 0.2639255076646805
[08/08/2025 02:39:23 INFO]: Training stats: {
    "score": -0.5415504190732277,
    "rmse": 0.5415504190732277
}
[08/08/2025 02:39:23 INFO]: Val stats: {
    "score": -0.7351226836308503,
    "rmse": 0.7351226836308503
}
[08/08/2025 02:39:23 INFO]: Test stats: {
    "score": -0.6697455559145604,
    "rmse": 0.6697455559145604
}
[08/08/2025 02:39:51 INFO]: Training loss at epoch 150: 0.316948726773262
[08/08/2025 02:40:18 INFO]: Training loss at epoch 151: 0.2894697040319443
[08/08/2025 02:40:45 INFO]: Training loss at epoch 152: 0.25684864073991776
[08/08/2025 02:41:13 INFO]: Training loss at epoch 153: 0.28935354948043823
[08/08/2025 02:41:41 INFO]: Training loss at epoch 154: 0.25982464104890823
[08/08/2025 02:42:08 INFO]: Training loss at epoch 155: 0.27831554412841797
[08/08/2025 02:42:36 INFO]: Training loss at epoch 156: 0.2889552414417267
[08/08/2025 02:43:03 INFO]: Training loss at epoch 157: 0.25034137815237045
[08/08/2025 02:43:31 INFO]: Training loss at epoch 158: 0.27020030468702316
[08/08/2025 02:43:34 INFO]: Running Final Evaluation...
[08/08/2025 02:43:44 INFO]: Training accuracy: {
    "score": -0.5642671888937223,
    "rmse": 0.5642671888937223
}
[08/08/2025 02:43:44 INFO]: Val accuracy: {
    "score": -0.6631217715615978,
    "rmse": 0.6631217715615978
}
[08/08/2025 02:43:44 INFO]: Test accuracy: {
    "score": -0.6473286701225685,
    "rmse": 0.6473286701225685
}
[08/08/2025 02:43:44 INFO]: {
    "dataset": "ic_upstream2",
    "model_name": "ft_transformer",
    "run_id": "graveless-Jacquleen",
    "best_epoch": 127,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.6473286701225685,
        "rmse": 0.6473286701225685
    },
    "train_stats": {
        "score": -0.5642671888937223,
        "rmse": 0.5642671888937223
    },
    "val_stats": {
        "score": -0.6631217715615978,
        "rmse": 0.6631217715615978
    }
}
[08/08/2025 02:43:44 INFO]: 
_________________________________________________

[08/08/2025 02:43:44 INFO]: train_net_for_optune.py main() running.
[08/08/2025 02:43:44 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.1765370749451556
  attention_dropout: 0.11484824698093093
  ffn_dropout: 0.11484824698093093
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 9.113888536433298e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: graveless-Jacquleen

[08/08/2025 02:43:44 INFO]: This ft_transformer has 0.889 million parameters.
[08/08/2025 02:43:44 INFO]: Training will start at epoch 0.
[08/08/2025 02:43:44 INFO]: ==> Starting training for 200 epochs...
[08/08/2025 02:43:55 INFO]: Training loss at epoch 0: 1.6359719634056091
[08/08/2025 02:43:57 INFO]: New best epoch, val score: -1.2854147307147366
[08/08/2025 02:43:57 INFO]: Saving model to: model_best.pth
[08/08/2025 02:44:10 INFO]: Training loss at epoch 1: 1.2006832361221313
[08/08/2025 02:44:11 INFO]: New best epoch, val score: -0.9805938421915972
[08/08/2025 02:44:11 INFO]: Saving model to: model_best.pth
[08/08/2025 02:44:23 INFO]: Training loss at epoch 2: 1.0442116856575012
[08/08/2025 02:44:36 INFO]: Training loss at epoch 3: 1.200166404247284
[08/08/2025 02:44:49 INFO]: Training loss at epoch 4: 1.1199237704277039
[08/08/2025 02:45:02 INFO]: Training loss at epoch 5: 1.074805349111557
[08/08/2025 02:45:15 INFO]: Training loss at epoch 6: 1.030905306339264
[08/08/2025 02:45:28 INFO]: Training loss at epoch 7: 1.5921640396118164
[08/08/2025 02:45:41 INFO]: Training loss at epoch 8: 1.0918454229831696
[08/08/2025 02:45:43 INFO]: New best epoch, val score: -0.973758399497788
[08/08/2025 02:45:43 INFO]: Saving model to: model_best.pth
[08/08/2025 02:45:55 INFO]: Training loss at epoch 9: 1.2201889753341675
[08/08/2025 02:45:59 INFO]: Training stats: {
    "score": -1.0994330273268722,
    "rmse": 1.0994330273268722
}
[08/08/2025 02:45:59 INFO]: Val stats: {
    "score": -0.9652518653581876,
    "rmse": 0.9652518653581876
}
[08/08/2025 02:45:59 INFO]: Test stats: {
    "score": -0.9829382688689835,
    "rmse": 0.9829382688689835
}
[08/08/2025 02:46:01 INFO]: New best epoch, val score: -0.9652518653581876
[08/08/2025 02:46:01 INFO]: Saving model to: model_best.pth
[08/08/2025 02:46:12 INFO]: Training loss at epoch 10: 1.0911136865615845
[08/08/2025 02:46:14 INFO]: New best epoch, val score: -0.9306963998659968
[08/08/2025 02:46:14 INFO]: Saving model to: model_best.pth
[08/08/2025 02:46:25 INFO]: Training loss at epoch 11: 1.0333325862884521
[08/08/2025 02:46:27 INFO]: New best epoch, val score: -0.9201989459119468
[08/08/2025 02:46:27 INFO]: Saving model to: model_best.pth
[08/08/2025 02:46:38 INFO]: Training loss at epoch 12: 1.0082131028175354
[08/08/2025 02:46:52 INFO]: Training loss at epoch 13: 0.9748333096504211
[08/08/2025 02:47:05 INFO]: Training loss at epoch 14: 0.942974865436554
[08/08/2025 02:47:18 INFO]: Training loss at epoch 15: 1.0567627549171448
[08/08/2025 02:47:31 INFO]: Training loss at epoch 16: 1.1893857717514038
[08/08/2025 02:47:44 INFO]: Training loss at epoch 17: 1.1470914483070374
[08/08/2025 02:47:57 INFO]: Training loss at epoch 18: 1.010880321264267
[08/08/2025 02:48:10 INFO]: Training loss at epoch 19: 1.0451309084892273
[08/08/2025 02:48:14 INFO]: Training stats: {
    "score": -0.9828175903365144,
    "rmse": 0.9828175903365144
}
[08/08/2025 02:48:14 INFO]: Val stats: {
    "score": -0.9796040517254141,
    "rmse": 0.9796040517254141
}
[08/08/2025 02:48:14 INFO]: Test stats: {
    "score": -0.9310977844727077,
    "rmse": 0.9310977844727077
}
[08/08/2025 02:48:27 INFO]: Training loss at epoch 20: 1.0345208048820496
[08/08/2025 02:48:40 INFO]: Training loss at epoch 21: 0.7987459897994995
[08/08/2025 02:48:53 INFO]: Training loss at epoch 22: 0.859774112701416
[08/08/2025 02:49:06 INFO]: Training loss at epoch 23: 0.9514092803001404
[08/08/2025 02:49:08 INFO]: New best epoch, val score: -0.9142655449645753
[08/08/2025 02:49:08 INFO]: Saving model to: model_best.pth
[08/08/2025 02:49:20 INFO]: Training loss at epoch 24: 0.8736990094184875
[08/08/2025 02:49:21 INFO]: New best epoch, val score: -0.9086348911755018
[08/08/2025 02:49:21 INFO]: Saving model to: model_best.pth
[08/08/2025 02:49:33 INFO]: Training loss at epoch 25: 0.7635305821895599
[08/08/2025 02:49:34 INFO]: New best epoch, val score: -0.9061230885139289
[08/08/2025 02:49:34 INFO]: Saving model to: model_best.pth
[08/08/2025 02:49:46 INFO]: Training loss at epoch 26: 1.026667833328247
[08/08/2025 02:49:48 INFO]: New best epoch, val score: -0.9052545683800446
[08/08/2025 02:49:48 INFO]: Saving model to: model_best.pth
[08/08/2025 02:49:59 INFO]: Training loss at epoch 27: 1.059816837310791
[08/08/2025 02:50:01 INFO]: New best epoch, val score: -0.9046638301472721
[08/08/2025 02:50:01 INFO]: Saving model to: model_best.pth
[08/08/2025 02:50:12 INFO]: Training loss at epoch 28: 0.9794877469539642
[08/08/2025 02:50:14 INFO]: New best epoch, val score: -0.9032115553010993
[08/08/2025 02:50:14 INFO]: Saving model to: model_best.pth
[08/08/2025 02:50:26 INFO]: Training loss at epoch 29: 0.8240466415882111
[08/08/2025 02:50:30 INFO]: Training stats: {
    "score": -0.9827314635618698,
    "rmse": 0.9827314635618698
}
[08/08/2025 02:50:30 INFO]: Val stats: {
    "score": -0.900743512715826,
    "rmse": 0.900743512715826
}
[08/08/2025 02:50:30 INFO]: Test stats: {
    "score": -0.8903266091594942,
    "rmse": 0.8903266091594942
}
[08/08/2025 02:50:32 INFO]: New best epoch, val score: -0.900743512715826
[08/08/2025 02:50:32 INFO]: Saving model to: model_best.pth
[08/08/2025 02:50:43 INFO]: Training loss at epoch 30: 0.9139475226402283
[08/08/2025 02:50:45 INFO]: New best epoch, val score: -0.8987213459041722
[08/08/2025 02:50:45 INFO]: Saving model to: model_best.pth
[08/08/2025 02:50:57 INFO]: Training loss at epoch 31: 0.8324535191059113
[08/08/2025 02:50:58 INFO]: New best epoch, val score: -0.8976899729153899
[08/08/2025 02:50:58 INFO]: Saving model to: model_best.pth
[08/08/2025 02:51:10 INFO]: Training loss at epoch 32: 0.8438065052032471
[08/08/2025 02:51:11 INFO]: New best epoch, val score: -0.8974894846105942
[08/08/2025 02:51:11 INFO]: Saving model to: model_best.pth
[08/08/2025 02:51:23 INFO]: Training loss at epoch 33: 0.7952567338943481
[08/08/2025 02:51:36 INFO]: Training loss at epoch 34: 0.9956927299499512
[08/08/2025 02:51:49 INFO]: Training loss at epoch 35: 0.9413018226623535
[08/08/2025 02:52:02 INFO]: Training loss at epoch 36: 0.9163798987865448
[08/08/2025 02:52:15 INFO]: Training loss at epoch 37: 0.7615859806537628
[08/08/2025 02:52:28 INFO]: Training loss at epoch 38: 1.0051162242889404
[08/08/2025 02:52:41 INFO]: Training loss at epoch 39: 0.7081289738416672
[08/08/2025 02:52:46 INFO]: Training stats: {
    "score": -0.9412383881412468,
    "rmse": 0.9412383881412468
}
[08/08/2025 02:52:46 INFO]: Val stats: {
    "score": -0.9394850951052245,
    "rmse": 0.9394850951052245
}
[08/08/2025 02:52:46 INFO]: Test stats: {
    "score": -0.8898242978541551,
    "rmse": 0.8898242978541551
}
[08/08/2025 02:52:59 INFO]: Training loss at epoch 40: 0.8470517992973328
[08/08/2025 02:53:12 INFO]: Training loss at epoch 41: 0.7773217856884003
[08/08/2025 02:53:25 INFO]: Training loss at epoch 42: 0.8502365350723267
[08/08/2025 02:53:38 INFO]: Training loss at epoch 43: 0.8506920337677002
[08/08/2025 02:53:51 INFO]: Training loss at epoch 44: 0.9328913986682892
[08/08/2025 02:53:53 INFO]: New best epoch, val score: -0.8851471729121038
[08/08/2025 02:53:53 INFO]: Saving model to: model_best.pth
[08/08/2025 02:54:04 INFO]: Training loss at epoch 45: 0.8309061229228973
[08/08/2025 02:54:06 INFO]: New best epoch, val score: -0.8773977198133832
[08/08/2025 02:54:06 INFO]: Saving model to: model_best.pth
[08/08/2025 02:54:17 INFO]: Training loss at epoch 46: 0.7592884004116058
[08/08/2025 02:54:19 INFO]: New best epoch, val score: -0.8716269769655375
[08/08/2025 02:54:19 INFO]: Saving model to: model_best.pth
[08/08/2025 02:54:31 INFO]: Training loss at epoch 47: 1.3228021264076233
[08/08/2025 02:54:32 INFO]: New best epoch, val score: -0.8688555852025543
[08/08/2025 02:54:32 INFO]: Saving model to: model_best.pth
[08/08/2025 02:54:44 INFO]: Training loss at epoch 48: 0.909838855266571
[08/08/2025 02:54:57 INFO]: Training loss at epoch 49: 0.6806556582450867
[08/08/2025 02:55:01 INFO]: Training stats: {
    "score": -0.9098393195618443,
    "rmse": 0.9098393195618443
}
[08/08/2025 02:55:01 INFO]: Val stats: {
    "score": -0.8716191601719436,
    "rmse": 0.8716191601719436
}
[08/08/2025 02:55:01 INFO]: Test stats: {
    "score": -0.8412000789675407,
    "rmse": 0.8412000789675407
}
[08/08/2025 02:55:14 INFO]: Training loss at epoch 50: 0.7891628742218018
[08/08/2025 02:55:27 INFO]: Training loss at epoch 51: 0.8444130718708038
[08/08/2025 02:55:40 INFO]: Training loss at epoch 52: 0.8434393405914307
[08/08/2025 02:55:53 INFO]: Training loss at epoch 53: 0.9128811657428741
[08/08/2025 02:56:07 INFO]: Training loss at epoch 54: 0.9565660655498505
[08/08/2025 02:56:20 INFO]: Training loss at epoch 55: 0.7257051765918732
[08/08/2025 02:56:33 INFO]: Training loss at epoch 56: 0.7045412063598633
[08/08/2025 02:56:46 INFO]: Training loss at epoch 57: 0.977489709854126
[08/08/2025 02:56:47 INFO]: New best epoch, val score: -0.868139916366002
[08/08/2025 02:56:47 INFO]: Saving model to: model_best.pth
[08/08/2025 02:56:59 INFO]: Training loss at epoch 58: 0.820933073759079
[08/08/2025 02:57:00 INFO]: New best epoch, val score: -0.8626587640143107
[08/08/2025 02:57:00 INFO]: Saving model to: model_best.pth
[08/08/2025 02:57:12 INFO]: Training loss at epoch 59: 0.9599220752716064
[08/08/2025 02:57:16 INFO]: Training stats: {
    "score": -0.865867150178296,
    "rmse": 0.865867150178296
}
[08/08/2025 02:57:16 INFO]: Val stats: {
    "score": -0.8586039676468684,
    "rmse": 0.8586039676468684
}
[08/08/2025 02:57:16 INFO]: Test stats: {
    "score": -0.8136929691305818,
    "rmse": 0.8136929691305818
}
[08/08/2025 02:57:18 INFO]: New best epoch, val score: -0.8586039676468684
[08/08/2025 02:57:18 INFO]: Saving model to: model_best.pth
[08/08/2025 02:57:30 INFO]: Training loss at epoch 60: 0.6571374833583832
[08/08/2025 02:57:31 INFO]: New best epoch, val score: -0.8542140022560564
[08/08/2025 02:57:31 INFO]: Saving model to: model_best.pth
[08/08/2025 02:57:43 INFO]: Training loss at epoch 61: 0.6737211048603058
[08/08/2025 02:57:44 INFO]: New best epoch, val score: -0.8476804737621233
[08/08/2025 02:57:44 INFO]: Saving model to: model_best.pth
[08/08/2025 02:57:56 INFO]: Training loss at epoch 62: 0.7275335192680359
[08/08/2025 02:57:58 INFO]: New best epoch, val score: -0.8395902307072654
[08/08/2025 02:57:58 INFO]: Saving model to: model_best.pth
[08/08/2025 02:58:09 INFO]: Training loss at epoch 63: 0.7117110788822174
[08/08/2025 02:58:11 INFO]: New best epoch, val score: -0.8292484194105191
[08/08/2025 02:58:11 INFO]: Saving model to: model_best.pth
[08/08/2025 02:58:22 INFO]: Training loss at epoch 64: 0.8145917356014252
[08/08/2025 02:58:24 INFO]: New best epoch, val score: -0.8198913176626529
[08/08/2025 02:58:24 INFO]: Saving model to: model_best.pth
[08/08/2025 02:58:35 INFO]: Training loss at epoch 65: 0.7064298093318939
[08/08/2025 02:58:37 INFO]: New best epoch, val score: -0.8141406862869961
[08/08/2025 02:58:37 INFO]: Saving model to: model_best.pth
[08/08/2025 02:58:49 INFO]: Training loss at epoch 66: 0.7444159686565399
[08/08/2025 02:58:50 INFO]: New best epoch, val score: -0.8109814923639443
[08/08/2025 02:58:50 INFO]: Saving model to: model_best.pth
[08/08/2025 02:59:02 INFO]: Training loss at epoch 67: 0.7143982350826263
[08/08/2025 02:59:15 INFO]: Training loss at epoch 68: 0.7174184620380402
[08/08/2025 02:59:28 INFO]: Training loss at epoch 69: 0.5731893479824066
[08/08/2025 02:59:32 INFO]: Training stats: {
    "score": -0.8043921070123528,
    "rmse": 0.8043921070123528
}
[08/08/2025 02:59:32 INFO]: Val stats: {
    "score": -0.8282677012331163,
    "rmse": 0.8282677012331163
}
[08/08/2025 02:59:32 INFO]: Test stats: {
    "score": -0.7682971572123898,
    "rmse": 0.7682971572123898
}
[08/08/2025 02:59:45 INFO]: Training loss at epoch 70: 0.6297342777252197
[08/08/2025 02:59:58 INFO]: Training loss at epoch 71: 0.606595516204834
[08/08/2025 03:00:11 INFO]: Training loss at epoch 72: 0.7025325298309326
[08/08/2025 03:00:25 INFO]: Training loss at epoch 73: 0.6257004737854004
[08/08/2025 03:00:26 INFO]: New best epoch, val score: -0.8063834263114715
[08/08/2025 03:00:26 INFO]: Saving model to: model_best.pth
[08/08/2025 03:00:38 INFO]: Training loss at epoch 74: 0.6057610511779785
[08/08/2025 03:00:39 INFO]: New best epoch, val score: -0.7973180099611582
[08/08/2025 03:00:39 INFO]: Saving model to: model_best.pth
[08/08/2025 03:00:51 INFO]: Training loss at epoch 75: 0.6326932311058044
[08/08/2025 03:00:53 INFO]: New best epoch, val score: -0.7902731099315505
[08/08/2025 03:00:53 INFO]: Saving model to: model_best.pth
[08/08/2025 03:01:04 INFO]: Training loss at epoch 76: 0.7114917039871216
[08/08/2025 03:01:06 INFO]: New best epoch, val score: -0.7887058503661966
[08/08/2025 03:01:06 INFO]: Saving model to: model_best.pth
[08/08/2025 03:01:17 INFO]: Training loss at epoch 77: 0.5741942226886749
[08/08/2025 03:01:30 INFO]: Training loss at epoch 78: 0.7352325618267059
[08/08/2025 03:01:43 INFO]: Training loss at epoch 79: 0.5513366907835007
[08/08/2025 03:01:48 INFO]: Training stats: {
    "score": -0.740866689041035,
    "rmse": 0.740866689041035
}
[08/08/2025 03:01:48 INFO]: Val stats: {
    "score": -0.8116443398536372,
    "rmse": 0.8116443398536372
}
[08/08/2025 03:01:48 INFO]: Test stats: {
    "score": -0.7276220253419079,
    "rmse": 0.7276220253419079
}
[08/08/2025 03:02:01 INFO]: Training loss at epoch 80: 0.7008255422115326
[08/08/2025 03:02:14 INFO]: Training loss at epoch 81: 0.5545492172241211
[08/08/2025 03:02:27 INFO]: Training loss at epoch 82: 0.5808304250240326
[08/08/2025 03:02:29 INFO]: New best epoch, val score: -0.7763346763350207
[08/08/2025 03:02:29 INFO]: Saving model to: model_best.pth
[08/08/2025 03:02:40 INFO]: Training loss at epoch 83: 0.681795209646225
[08/08/2025 03:02:42 INFO]: New best epoch, val score: -0.7763288022862579
[08/08/2025 03:02:42 INFO]: Saving model to: model_best.pth
[08/08/2025 03:02:53 INFO]: Training loss at epoch 84: 0.7230441272258759
[08/08/2025 03:03:06 INFO]: Training loss at epoch 85: 0.4355727732181549
[08/08/2025 03:03:19 INFO]: Training loss at epoch 86: 0.5293590873479843
[08/08/2025 03:03:32 INFO]: Training loss at epoch 87: 0.45178358256816864
[08/08/2025 03:03:46 INFO]: Training loss at epoch 88: 0.5591476261615753
[08/08/2025 03:03:59 INFO]: Training loss at epoch 89: 0.5753815025091171
[08/08/2025 03:04:03 INFO]: Training stats: {
    "score": -0.7457570945662171,
    "rmse": 0.7457570945662171
}
[08/08/2025 03:04:03 INFO]: Val stats: {
    "score": -0.7876863152392893,
    "rmse": 0.7876863152392893
}
[08/08/2025 03:04:03 INFO]: Test stats: {
    "score": -0.7421936503078064,
    "rmse": 0.7421936503078064
}
[08/08/2025 03:04:16 INFO]: Training loss at epoch 90: 0.49011681973934174
[08/08/2025 03:04:29 INFO]: Training loss at epoch 91: 0.5290979444980621
[08/08/2025 03:04:42 INFO]: Training loss at epoch 92: 0.5880383253097534
[08/08/2025 03:04:55 INFO]: Training loss at epoch 93: 0.49654270708560944
[08/08/2025 03:05:08 INFO]: Training loss at epoch 94: 0.7090411335229874
[08/08/2025 03:05:22 INFO]: Training loss at epoch 95: 0.5211947560310364
[08/08/2025 03:05:35 INFO]: Training loss at epoch 96: 0.583579033613205
[08/08/2025 03:05:36 INFO]: New best epoch, val score: -0.766382950814293
[08/08/2025 03:05:36 INFO]: Saving model to: model_best.pth
[08/08/2025 03:05:48 INFO]: Training loss at epoch 97: 0.5029239356517792
[08/08/2025 03:05:49 INFO]: New best epoch, val score: -0.7629760365519401
[08/08/2025 03:05:49 INFO]: Saving model to: model_best.pth
[08/08/2025 03:06:01 INFO]: Training loss at epoch 98: 0.5559746026992798
[08/08/2025 03:06:14 INFO]: Training loss at epoch 99: 0.4550166577100754
[08/08/2025 03:06:18 INFO]: Training stats: {
    "score": -0.6933909590125867,
    "rmse": 0.6933909590125867
}
[08/08/2025 03:06:18 INFO]: Val stats: {
    "score": -0.7832956723719935,
    "rmse": 0.7832956723719935
}
[08/08/2025 03:06:18 INFO]: Test stats: {
    "score": -0.7017520827673137,
    "rmse": 0.7017520827673137
}
[08/08/2025 03:06:31 INFO]: Training loss at epoch 100: 0.4862181097269058
[08/08/2025 03:06:44 INFO]: Training loss at epoch 101: 0.42924292385578156
[08/08/2025 03:06:58 INFO]: Training loss at epoch 102: 0.4707626551389694
[08/08/2025 03:06:59 INFO]: New best epoch, val score: -0.7486369869475867
[08/08/2025 03:06:59 INFO]: Saving model to: model_best.pth
[08/08/2025 03:07:11 INFO]: Training loss at epoch 103: 0.5065450817346573
[08/08/2025 03:07:12 INFO]: New best epoch, val score: -0.7429504326862232
[08/08/2025 03:07:12 INFO]: Saving model to: model_best.pth
[08/08/2025 03:07:24 INFO]: Training loss at epoch 104: 0.5250398814678192
[08/08/2025 03:07:25 INFO]: New best epoch, val score: -0.7404352400795572
[08/08/2025 03:07:25 INFO]: Saving model to: model_best.pth
[08/08/2025 03:07:37 INFO]: Training loss at epoch 105: 0.4411630481481552
[08/08/2025 03:07:50 INFO]: Training loss at epoch 106: 0.4688636213541031
[08/08/2025 03:08:03 INFO]: Training loss at epoch 107: 0.42664065957069397
[08/08/2025 03:08:16 INFO]: Training loss at epoch 108: 0.3964530974626541
[08/08/2025 03:08:18 INFO]: New best epoch, val score: -0.7303442329426734
[08/08/2025 03:08:18 INFO]: Saving model to: model_best.pth
[08/08/2025 03:08:29 INFO]: Training loss at epoch 109: 0.4871940463781357
[08/08/2025 03:08:34 INFO]: Training stats: {
    "score": -0.6799863063772791,
    "rmse": 0.6799863063772791
}
[08/08/2025 03:08:34 INFO]: Val stats: {
    "score": -0.7247000213952002,
    "rmse": 0.7247000213952002
}
[08/08/2025 03:08:34 INFO]: Test stats: {
    "score": -0.6849316892869692,
    "rmse": 0.6849316892869692
}
[08/08/2025 03:08:36 INFO]: New best epoch, val score: -0.7247000213952002
[08/08/2025 03:08:36 INFO]: Saving model to: model_best.pth
[08/08/2025 03:08:47 INFO]: Training loss at epoch 110: 0.3631620556116104
[08/08/2025 03:09:00 INFO]: Training loss at epoch 111: 0.49367964267730713
[08/08/2025 03:09:13 INFO]: Training loss at epoch 112: 0.4020637422800064
[08/08/2025 03:09:26 INFO]: Training loss at epoch 113: 0.38922320306301117
[08/08/2025 03:09:39 INFO]: Training loss at epoch 114: 0.46778005361557007
[08/08/2025 03:09:52 INFO]: Training loss at epoch 115: 0.5161416977643967
[08/08/2025 03:10:05 INFO]: Training loss at epoch 116: 0.4435623288154602
[08/08/2025 03:10:18 INFO]: Training loss at epoch 117: 0.4396526664495468
[08/08/2025 03:10:31 INFO]: Training loss at epoch 118: 0.47636233270168304
[08/08/2025 03:10:45 INFO]: Training loss at epoch 119: 0.5162736624479294
[08/08/2025 03:10:49 INFO]: Training stats: {
    "score": -0.6571337547914395,
    "rmse": 0.6571337547914395
}
[08/08/2025 03:10:49 INFO]: Val stats: {
    "score": -0.7579647524545042,
    "rmse": 0.7579647524545042
}
[08/08/2025 03:10:49 INFO]: Test stats: {
    "score": -0.6763954756104432,
    "rmse": 0.6763954756104432
}
[08/08/2025 03:11:02 INFO]: Training loss at epoch 120: 0.45183707773685455
[08/08/2025 03:11:15 INFO]: Training loss at epoch 121: 0.3614504337310791
[08/08/2025 03:11:28 INFO]: Training loss at epoch 122: 0.3387921527028084
[08/08/2025 03:11:30 INFO]: New best epoch, val score: -0.7220522941200415
[08/08/2025 03:11:30 INFO]: Saving model to: model_best.pth
[08/08/2025 03:11:41 INFO]: Training loss at epoch 123: 0.4650411456823349
[08/08/2025 03:11:43 INFO]: New best epoch, val score: -0.7187491276496978
[08/08/2025 03:11:43 INFO]: Saving model to: model_best.pth
[08/08/2025 03:11:54 INFO]: Training loss at epoch 124: 0.43284714221954346
[08/08/2025 03:12:07 INFO]: Training loss at epoch 125: 0.44593700766563416
[08/08/2025 03:12:21 INFO]: Training loss at epoch 126: 0.44309836626052856
[08/08/2025 03:12:34 INFO]: Training loss at epoch 127: 0.3746372014284134
[08/08/2025 03:12:35 INFO]: New best epoch, val score: -0.7118773963872407
[08/08/2025 03:12:35 INFO]: Saving model to: model_best.pth
[08/08/2025 03:12:47 INFO]: Training loss at epoch 128: 0.38387688994407654
[08/08/2025 03:12:48 INFO]: New best epoch, val score: -0.7081487651219043
[08/08/2025 03:12:48 INFO]: Saving model to: model_best.pth
[08/08/2025 03:13:00 INFO]: Training loss at epoch 129: 0.42229415476322174
[08/08/2025 03:13:05 INFO]: Training stats: {
    "score": -0.6604912041267342,
    "rmse": 0.6604912041267342
}
[08/08/2025 03:13:05 INFO]: Val stats: {
    "score": -0.7075035194452614,
    "rmse": 0.7075035194452614
}
[08/08/2025 03:13:05 INFO]: Test stats: {
    "score": -0.6868551314461954,
    "rmse": 0.6868551314461954
}
[08/08/2025 03:13:06 INFO]: New best epoch, val score: -0.7075035194452614
[08/08/2025 03:13:06 INFO]: Saving model to: model_best.pth
[08/08/2025 03:13:18 INFO]: Training loss at epoch 130: 0.39156343042850494
[08/08/2025 03:13:19 INFO]: New best epoch, val score: -0.7049980366096138
[08/08/2025 03:13:19 INFO]: Saving model to: model_best.pth
[08/08/2025 03:13:31 INFO]: Training loss at epoch 131: 0.4011269509792328
[08/08/2025 03:13:44 INFO]: Training loss at epoch 132: 0.41128984093666077
[08/08/2025 03:13:57 INFO]: Training loss at epoch 133: 0.3863040655851364
[08/08/2025 03:13:59 INFO]: New best epoch, val score: -0.7040174216700905
[08/08/2025 03:13:59 INFO]: Saving model to: model_best.pth
[08/08/2025 03:14:10 INFO]: Training loss at epoch 134: 0.5085905641317368
[08/08/2025 03:14:12 INFO]: New best epoch, val score: -0.6999086068623663
[08/08/2025 03:14:12 INFO]: Saving model to: model_best.pth
[08/08/2025 03:14:23 INFO]: Training loss at epoch 135: 0.30399490147829056
[08/08/2025 03:14:36 INFO]: Training loss at epoch 136: 0.4452321231365204
[08/08/2025 03:14:50 INFO]: Training loss at epoch 137: 0.36319831013679504
[08/08/2025 03:15:03 INFO]: Training loss at epoch 138: 0.3201879933476448
[08/08/2025 03:15:16 INFO]: Training loss at epoch 139: 0.4444025605916977
[08/08/2025 03:15:20 INFO]: Training stats: {
    "score": -0.6346572841573014,
    "rmse": 0.6346572841573014
}
[08/08/2025 03:15:20 INFO]: Val stats: {
    "score": -0.6950897124933781,
    "rmse": 0.6950897124933781
}
[08/08/2025 03:15:20 INFO]: Test stats: {
    "score": -0.6770886136233862,
    "rmse": 0.6770886136233862
}
[08/08/2025 03:15:22 INFO]: New best epoch, val score: -0.6950897124933781
[08/08/2025 03:15:22 INFO]: Saving model to: model_best.pth
[08/08/2025 03:15:33 INFO]: Training loss at epoch 140: 0.3732914626598358
[08/08/2025 03:15:46 INFO]: Training loss at epoch 141: 0.416705459356308
[08/08/2025 03:15:48 INFO]: New best epoch, val score: -0.6943572354832417
[08/08/2025 03:15:48 INFO]: Saving model to: model_best.pth
[08/08/2025 03:16:00 INFO]: Training loss at epoch 142: 0.40208451449871063
[08/08/2025 03:16:01 INFO]: New best epoch, val score: -0.6895993604297711
[08/08/2025 03:16:01 INFO]: Saving model to: model_best.pth
[08/08/2025 03:16:13 INFO]: Training loss at epoch 143: 0.3647312819957733
[08/08/2025 03:16:26 INFO]: Training loss at epoch 144: 0.4498206526041031
[08/08/2025 03:16:39 INFO]: Training loss at epoch 145: 0.38202767074108124
[08/08/2025 03:16:52 INFO]: Training loss at epoch 146: 0.4648927301168442
[08/08/2025 03:16:53 INFO]: New best epoch, val score: -0.6826784662056439
[08/08/2025 03:16:53 INFO]: Saving model to: model_best.pth
[08/08/2025 03:17:05 INFO]: Training loss at epoch 147: 0.3958638459444046
[08/08/2025 03:17:18 INFO]: Training loss at epoch 148: 0.37555795907974243
[08/08/2025 03:17:31 INFO]: Training loss at epoch 149: 0.33573608100414276
[08/08/2025 03:17:35 INFO]: Training stats: {
    "score": -0.6131850100010063,
    "rmse": 0.6131850100010063
}
[08/08/2025 03:17:35 INFO]: Val stats: {
    "score": -0.6772536143580322,
    "rmse": 0.6772536143580322
}
[08/08/2025 03:17:35 INFO]: Test stats: {
    "score": -0.6654172688197996,
    "rmse": 0.6654172688197996
}
[08/08/2025 03:17:37 INFO]: New best epoch, val score: -0.6772536143580322
[08/08/2025 03:17:37 INFO]: Saving model to: model_best.pth
[08/08/2025 03:17:49 INFO]: Training loss at epoch 150: 0.43952757120132446
[08/08/2025 03:18:02 INFO]: Training loss at epoch 151: 0.4146084040403366
[08/08/2025 03:18:15 INFO]: Training loss at epoch 152: 0.41961292922496796
[08/08/2025 03:18:16 INFO]: New best epoch, val score: -0.6702142791508546
[08/08/2025 03:18:16 INFO]: Saving model to: model_best.pth
[08/08/2025 03:18:28 INFO]: Training loss at epoch 153: 0.34846581518650055
[08/08/2025 03:18:41 INFO]: Training loss at epoch 154: 0.42468446493148804
[08/08/2025 03:18:54 INFO]: Training loss at epoch 155: 0.37174031138420105
[08/08/2025 03:18:56 INFO]: New best epoch, val score: -0.6686653552602281
[08/08/2025 03:18:56 INFO]: Saving model to: model_best.pth
[08/08/2025 03:19:08 INFO]: Training loss at epoch 156: 0.3346407264471054
[08/08/2025 03:19:21 INFO]: Training loss at epoch 157: 0.3577166795730591
[08/08/2025 03:19:34 INFO]: Training loss at epoch 158: 0.4308551996946335
[08/08/2025 03:19:47 INFO]: Training loss at epoch 159: 0.3465634733438492
[08/08/2025 03:19:51 INFO]: Training stats: {
    "score": -0.5913263482682644,
    "rmse": 0.5913263482682644
}
[08/08/2025 03:19:51 INFO]: Val stats: {
    "score": -0.6815623992612647,
    "rmse": 0.6815623992612647
}
[08/08/2025 03:19:51 INFO]: Test stats: {
    "score": -0.6711496468878576,
    "rmse": 0.6711496468878576
}
[08/08/2025 03:20:04 INFO]: Training loss at epoch 160: 0.3275477886199951
[08/08/2025 03:20:17 INFO]: Training loss at epoch 161: 0.42540642619132996
[08/08/2025 03:20:30 INFO]: Training loss at epoch 162: 0.3190452605485916
[08/08/2025 03:20:43 INFO]: Training loss at epoch 163: 0.36470362544059753
[08/08/2025 03:20:56 INFO]: Training loss at epoch 164: 0.47589635848999023
[08/08/2025 03:21:10 INFO]: Training loss at epoch 165: 0.3962964564561844
[08/08/2025 03:21:23 INFO]: Training loss at epoch 166: 0.4142783284187317
[08/08/2025 03:21:36 INFO]: Training loss at epoch 167: 0.29959776252508163
[08/08/2025 03:21:49 INFO]: Training loss at epoch 168: 0.3443250209093094
[08/08/2025 03:22:02 INFO]: Training loss at epoch 169: 0.34866365790367126
[08/08/2025 03:22:06 INFO]: Training stats: {
    "score": -0.587063032834101,
    "rmse": 0.587063032834101
}
[08/08/2025 03:22:06 INFO]: Val stats: {
    "score": -0.6725921458819821,
    "rmse": 0.6725921458819821
}
[08/08/2025 03:22:06 INFO]: Test stats: {
    "score": -0.68193593464374,
    "rmse": 0.68193593464374
}
[08/08/2025 03:22:19 INFO]: Training loss at epoch 170: 0.32005675137043
[08/08/2025 03:22:32 INFO]: Training loss at epoch 171: 0.35849079489707947
[08/08/2025 03:22:45 INFO]: Training loss at epoch 172: 0.4329739809036255
[08/08/2025 03:22:59 INFO]: Training loss at epoch 173: 0.32084009051322937
[08/08/2025 03:23:12 INFO]: Training loss at epoch 174: 0.3453897386789322
[08/08/2025 03:23:25 INFO]: Training loss at epoch 175: 0.3435288369655609
[08/08/2025 03:23:38 INFO]: Training loss at epoch 176: 0.4292014241218567
[08/08/2025 03:23:51 INFO]: Training loss at epoch 177: 0.3102712780237198
[08/08/2025 03:24:04 INFO]: Training loss at epoch 178: 0.3119347542524338
[08/08/2025 03:24:17 INFO]: Training loss at epoch 179: 0.3108070641756058
[08/08/2025 03:24:21 INFO]: Training stats: {
    "score": -0.5575812219767218,
    "rmse": 0.5575812219767218
}
[08/08/2025 03:24:21 INFO]: Val stats: {
    "score": -0.6760300542005291,
    "rmse": 0.6760300542005291
}
[08/08/2025 03:24:21 INFO]: Test stats: {
    "score": -0.6680281270264359,
    "rmse": 0.6680281270264359
}
[08/08/2025 03:24:34 INFO]: Training loss at epoch 180: 0.2634871378540993
[08/08/2025 03:24:47 INFO]: Training loss at epoch 181: 0.26800356805324554
[08/08/2025 03:25:01 INFO]: Training loss at epoch 182: 0.33131344616413116
[08/08/2025 03:25:14 INFO]: Training loss at epoch 183: 0.39698442816734314
[08/08/2025 03:25:27 INFO]: Training loss at epoch 184: 0.3088882118463516
[08/08/2025 03:25:40 INFO]: Training loss at epoch 185: 0.3273671269416809
[08/08/2025 03:25:53 INFO]: Training loss at epoch 186: 0.41398732364177704
[08/08/2025 03:25:54 INFO]: Running Final Evaluation...
[08/08/2025 03:25:59 INFO]: Training accuracy: {
    "score": -0.6023804793560132,
    "rmse": 0.6023804793560132
}
[08/08/2025 03:25:59 INFO]: Val accuracy: {
    "score": -0.6686653552602281,
    "rmse": 0.6686653552602281
}
[08/08/2025 03:25:59 INFO]: Test accuracy: {
    "score": -0.6632053123180994,
    "rmse": 0.6632053123180994
}
[08/08/2025 03:25:59 INFO]: {
    "dataset": "ic_upstream2",
    "model_name": "ft_transformer",
    "run_id": "graveless-Jacquleen",
    "best_epoch": 155,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.6632053123180994,
        "rmse": 0.6632053123180994
    },
    "train_stats": {
        "score": -0.6023804793560132,
        "rmse": 0.6023804793560132
    },
    "val_stats": {
        "score": -0.6686653552602281,
        "rmse": 0.6686653552602281
    }
}
[08/08/2025 03:25:59 INFO]: 
_________________________________________________

[08/08/2025 03:25:59 INFO]: train_net_for_optune.py main() running.
[08/08/2025 03:25:59 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 12
  d_ffn_factor: 0.8575160183041705
  attention_dropout: 0.4693277395379756
  ffn_dropout: 0.4693277395379756
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.752853255922827e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: graveless-Jacquleen

[08/08/2025 03:25:59 INFO]: This ft_transformer has 2.023 million parameters.
[08/08/2025 03:25:59 INFO]: Training will start at epoch 0.
[08/08/2025 03:25:59 INFO]: ==> Starting training for 200 epochs...
[08/08/2025 03:26:21 INFO]: Training loss at epoch 0: 1.11846262216568
[08/08/2025 03:26:24 INFO]: New best epoch, val score: -0.9388955174929493
[08/08/2025 03:26:24 INFO]: Saving model to: model_best.pth
[08/08/2025 03:26:47 INFO]: Training loss at epoch 1: 0.7961711287498474
[08/08/2025 03:27:12 INFO]: Training loss at epoch 2: 1.0397353172302246
[08/08/2025 03:27:38 INFO]: Training loss at epoch 3: 1.0759803652763367
[08/08/2025 03:28:04 INFO]: Training loss at epoch 4: 1.3510788083076477
[08/08/2025 03:28:29 INFO]: Training loss at epoch 5: 1.0918055772781372
[08/08/2025 03:28:55 INFO]: Training loss at epoch 6: 0.9782726168632507
[08/08/2025 03:29:20 INFO]: Training loss at epoch 7: 0.9557974338531494
[08/08/2025 03:29:46 INFO]: Training loss at epoch 8: 0.9248227775096893
[08/08/2025 03:30:12 INFO]: Training loss at epoch 9: 1.204498052597046
[08/08/2025 03:30:20 INFO]: Training stats: {
    "score": -1.0563164733965373,
    "rmse": 1.0563164733965373
}
[08/08/2025 03:30:20 INFO]: Val stats: {
    "score": -1.1210042554955195,
    "rmse": 1.1210042554955195
}
[08/08/2025 03:30:20 INFO]: Test stats: {
    "score": -1.0539789964269781,
    "rmse": 1.0539789964269781
}
[08/08/2025 03:30:46 INFO]: Training loss at epoch 10: 1.0490975975990295
[08/08/2025 03:31:12 INFO]: Training loss at epoch 11: 0.9516441524028778
[08/08/2025 03:31:37 INFO]: Training loss at epoch 12: 1.1329444646835327
[08/08/2025 03:32:03 INFO]: Training loss at epoch 13: 1.2148135900497437
[08/08/2025 03:32:28 INFO]: Training loss at epoch 14: 0.9451599717140198
[08/08/2025 03:32:54 INFO]: Training loss at epoch 15: 0.9548825323581696
[08/08/2025 03:33:20 INFO]: Training loss at epoch 16: 1.0272983312606812
[08/08/2025 03:33:45 INFO]: Training loss at epoch 17: 1.0603933036327362
[08/08/2025 03:34:11 INFO]: Training loss at epoch 18: 1.1020145416259766
[08/08/2025 03:34:36 INFO]: Training loss at epoch 19: 1.0523345470428467
[08/08/2025 03:34:45 INFO]: Training stats: {
    "score": -1.0051444927652413,
    "rmse": 1.0051444927652413
}
[08/08/2025 03:34:45 INFO]: Val stats: {
    "score": -1.0635055324685239,
    "rmse": 1.0635055324685239
}
[08/08/2025 03:34:45 INFO]: Test stats: {
    "score": -0.9932575059635192,
    "rmse": 0.9932575059635192
}
[08/08/2025 03:35:11 INFO]: Training loss at epoch 20: 1.1702651381492615
[08/08/2025 03:35:36 INFO]: Training loss at epoch 21: 0.935222864151001
[08/08/2025 03:36:02 INFO]: Training loss at epoch 22: 0.8890573084354401
[08/08/2025 03:36:27 INFO]: Training loss at epoch 23: 0.8960911631584167
[08/08/2025 03:36:53 INFO]: Training loss at epoch 24: 0.9647238850593567
[08/08/2025 03:37:19 INFO]: Training loss at epoch 25: 1.0798763036727905
[08/08/2025 03:37:44 INFO]: Training loss at epoch 26: 0.7616066336631775
[08/08/2025 03:38:10 INFO]: Training loss at epoch 27: 0.784137636423111
[08/08/2025 03:38:35 INFO]: Training loss at epoch 28: 1.028890699148178
[08/08/2025 03:39:01 INFO]: Training loss at epoch 29: 1.0196640491485596
[08/08/2025 03:39:10 INFO]: Training stats: {
    "score": -0.93752292175365,
    "rmse": 0.93752292175365
}
[08/08/2025 03:39:10 INFO]: Val stats: {
    "score": -0.9699547512474355,
    "rmse": 0.9699547512474355
}
[08/08/2025 03:39:10 INFO]: Test stats: {
    "score": -0.903609315250785,
    "rmse": 0.903609315250785
}
[08/08/2025 03:39:35 INFO]: Training loss at epoch 30: 0.9189409017562866
[08/08/2025 03:40:01 INFO]: Training loss at epoch 31: 0.8548893630504608
[08/08/2025 03:40:04 INFO]: Running Final Evaluation...
[08/08/2025 03:40:13 INFO]: Training accuracy: {
    "score": -0.9845954593644932,
    "rmse": 0.9845954593644932
}
[08/08/2025 03:40:13 INFO]: Val accuracy: {
    "score": -0.9388955174929493,
    "rmse": 0.9388955174929493
}
[08/08/2025 03:40:13 INFO]: Test accuracy: {
    "score": -0.9101049566424726,
    "rmse": 0.9101049566424726
}
[08/08/2025 03:40:13 INFO]: {
    "dataset": "ic_upstream2",
    "model_name": "ft_transformer",
    "run_id": "graveless-Jacquleen",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9101049566424726,
        "rmse": 0.9101049566424726
    },
    "train_stats": {
        "score": -0.9845954593644932,
        "rmse": 0.9845954593644932
    },
    "val_stats": {
        "score": -0.9388955174929493,
        "rmse": 0.9388955174929493
    }
}
[08/08/2025 03:40:13 INFO]: 
_________________________________________________

[08/08/2025 03:40:13 INFO]: train_net_for_optune.py main() running.
[08/08/2025 03:40:13 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 8
  d_ffn_factor: 0.8579197495515057
  attention_dropout: 0.1755911093290597
  ffn_dropout: 0.1755911093290597
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 8.582804580311361e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: graveless-Jacquleen

[08/08/2025 03:40:13 INFO]: This ft_transformer has 9.122 million parameters.
[08/08/2025 03:40:13 INFO]: Training will start at epoch 0.
[08/08/2025 03:40:13 INFO]: ==> Starting training for 200 epochs...
[08/08/2025 03:42:22 INFO]: Training loss at epoch 0: 1.4050970077514648
[08/08/2025 03:42:39 INFO]: New best epoch, val score: -1.221037552075635
[08/08/2025 03:42:39 INFO]: Saving model to: model_best.pth
[08/08/2025 03:44:48 INFO]: Training loss at epoch 1: 1.7296183109283447
[08/08/2025 03:47:14 INFO]: Training loss at epoch 2: 1.5020232796669006
[08/08/2025 03:47:32 INFO]: New best epoch, val score: -0.9324341625889269
[08/08/2025 03:47:32 INFO]: Saving model to: model_best.pth
[08/08/2025 03:49:41 INFO]: Training loss at epoch 3: 1.205233633518219
[08/08/2025 03:52:07 INFO]: Training loss at epoch 4: 1.4508394598960876
[08/08/2025 03:54:32 INFO]: Training loss at epoch 5: 1.1515462398529053
[08/08/2025 03:56:58 INFO]: Training loss at epoch 6: 1.1631985008716583
[08/08/2025 03:59:24 INFO]: Training loss at epoch 7: 1.132417619228363
[08/08/2025 04:01:50 INFO]: Training loss at epoch 8: 1.017778754234314
[08/08/2025 04:04:16 INFO]: Training loss at epoch 9: 1.0393837094306946
[08/08/2025 04:05:06 INFO]: Training stats: {
    "score": -0.9859032328929813,
    "rmse": 0.9859032328929813
}
[08/08/2025 04:05:06 INFO]: Val stats: {
    "score": -0.9472581768016969,
    "rmse": 0.9472581768016969
}
[08/08/2025 04:05:06 INFO]: Test stats: {
    "score": -0.9155536484356886,
    "rmse": 0.9155536484356886
}
[08/08/2025 04:07:32 INFO]: Training loss at epoch 10: 0.9675971567630768
[08/08/2025 04:07:49 INFO]: New best epoch, val score: -0.9281708914428872
[08/08/2025 04:07:49 INFO]: Saving model to: model_best.pth
[08/08/2025 04:09:58 INFO]: Training loss at epoch 11: 1.067903995513916
[08/08/2025 04:10:16 INFO]: New best epoch, val score: -0.9209992391803012
[08/08/2025 04:10:16 INFO]: Saving model to: model_best.pth
[08/08/2025 04:12:25 INFO]: Training loss at epoch 12: 0.9275209307670593
[08/08/2025 04:12:42 INFO]: New best epoch, val score: -0.9193106639728204
[08/08/2025 04:12:42 INFO]: Saving model to: model_best.pth
[08/08/2025 04:14:51 INFO]: Training loss at epoch 13: 0.908367931842804
[08/08/2025 04:15:09 INFO]: New best epoch, val score: -0.9188649131469028
[08/08/2025 04:15:09 INFO]: Saving model to: model_best.pth
[08/08/2025 04:17:18 INFO]: Training loss at epoch 14: 0.880841851234436
[08/08/2025 04:19:44 INFO]: Training loss at epoch 15: 0.9785467088222504
[08/08/2025 04:22:09 INFO]: Training loss at epoch 16: 0.9979647994041443
[08/08/2025 04:24:35 INFO]: Training loss at epoch 17: 1.0912486910820007
[08/08/2025 04:27:01 INFO]: Training loss at epoch 18: 0.8355750739574432
[08/08/2025 04:29:27 INFO]: Training loss at epoch 19: 0.9880571961402893
[08/08/2025 04:30:17 INFO]: Training stats: {
    "score": -0.9681331899369978,
    "rmse": 0.9681331899369978
}
[08/08/2025 04:30:17 INFO]: Val stats: {
    "score": -0.9337365923439662,
    "rmse": 0.9337365923439662
}
[08/08/2025 04:30:17 INFO]: Test stats: {
    "score": -0.8997459847459214,
    "rmse": 0.8997459847459214
}
[08/08/2025 04:32:43 INFO]: Training loss at epoch 20: 0.8949955105781555
[08/08/2025 04:35:09 INFO]: Training loss at epoch 21: 0.872016578912735
[08/08/2025 04:37:35 INFO]: Training loss at epoch 22: 1.0899566113948822
[08/08/2025 04:40:01 INFO]: Training loss at epoch 23: 0.9668863415718079
[08/08/2025 04:42:27 INFO]: Training loss at epoch 24: 0.8893199861049652
[08/08/2025 04:42:45 INFO]: New best epoch, val score: -0.914630561756803
[08/08/2025 04:42:45 INFO]: Saving model to: model_best.pth
[08/08/2025 04:44:54 INFO]: Training loss at epoch 25: 0.887720912694931
[08/08/2025 04:45:11 INFO]: New best epoch, val score: -0.9001179712606144
[08/08/2025 04:45:11 INFO]: Saving model to: model_best.pth
[08/08/2025 04:47:20 INFO]: Training loss at epoch 26: 1.03870090842247
[08/08/2025 04:47:38 INFO]: New best epoch, val score: -0.8951172494929114
[08/08/2025 04:47:38 INFO]: Saving model to: model_best.pth
[08/08/2025 04:49:47 INFO]: Training loss at epoch 27: 0.9401034712791443
[08/08/2025 04:50:04 INFO]: New best epoch, val score: -0.8924603678926684
[08/08/2025 04:50:04 INFO]: Saving model to: model_best.pth
[08/08/2025 04:52:13 INFO]: Training loss at epoch 28: 1.3102036118507385
[08/08/2025 04:54:39 INFO]: Training loss at epoch 29: 0.8798742592334747
[08/08/2025 04:55:29 INFO]: Training stats: {
    "score": -0.9441283178164297,
    "rmse": 0.9441283178164297
}
[08/08/2025 04:55:29 INFO]: Val stats: {
    "score": -0.9303867548243809,
    "rmse": 0.9303867548243809
}
[08/08/2025 04:55:29 INFO]: Test stats: {
    "score": -0.8888358331061514,
    "rmse": 0.8888358331061514
}
[08/08/2025 04:57:55 INFO]: Training loss at epoch 30: 0.8433728814125061
[08/08/2025 05:00:21 INFO]: Training loss at epoch 31: 0.8582801520824432
[08/08/2025 05:02:46 INFO]: Training loss at epoch 32: 0.7941761314868927
[08/08/2025 05:03:04 INFO]: New best epoch, val score: -0.8782386484953171
[08/08/2025 05:03:04 INFO]: Saving model to: model_best.pth
[08/08/2025 05:05:13 INFO]: Training loss at epoch 33: 0.8383653461933136
[08/08/2025 05:07:39 INFO]: Training loss at epoch 34: 0.7363399565219879
[08/08/2025 05:10:05 INFO]: Training loss at epoch 35: 1.0276786088943481
[08/08/2025 05:12:31 INFO]: Training loss at epoch 36: 1.0172122418880463
[08/08/2025 05:12:48 INFO]: New best epoch, val score: -0.8716177674273007
[08/08/2025 05:12:48 INFO]: Saving model to: model_best.pth
[08/08/2025 05:14:57 INFO]: Training loss at epoch 37: 0.8790432810783386
[08/08/2025 05:17:23 INFO]: Training loss at epoch 38: 0.8428913056850433
[08/08/2025 05:19:52 INFO]: Training loss at epoch 39: 1.0441384315490723
[08/08/2025 05:20:42 INFO]: Training stats: {
    "score": -0.9135094783524268,
    "rmse": 0.9135094783524268
}
[08/08/2025 05:20:42 INFO]: Val stats: {
    "score": -0.942789282334447,
    "rmse": 0.942789282334447
}
[08/08/2025 05:20:42 INFO]: Test stats: {
    "score": -0.8827931992026041,
    "rmse": 0.8827931992026041
}
[08/08/2025 05:23:08 INFO]: Training loss at epoch 40: 0.9217323660850525
[08/08/2025 05:25:34 INFO]: Training loss at epoch 41: 0.6336554437875748
[08/08/2025 05:25:52 INFO]: New best epoch, val score: -0.8441844256623848
[08/08/2025 05:25:52 INFO]: Saving model to: model_best.pth
[08/08/2025 05:28:01 INFO]: Training loss at epoch 42: 0.8724271059036255
[08/08/2025 05:30:27 INFO]: Training loss at epoch 43: 0.9897371232509613
[08/08/2025 05:32:52 INFO]: Training loss at epoch 44: 0.6817538440227509
[08/08/2025 05:33:10 INFO]: New best epoch, val score: -0.8341317112600551
[08/08/2025 05:33:10 INFO]: Saving model to: model_best.pth
[08/08/2025 05:35:19 INFO]: Training loss at epoch 45: 0.7216647565364838
[08/08/2025 05:37:45 INFO]: Training loss at epoch 46: 0.9559113085269928
[08/08/2025 05:40:11 INFO]: Training loss at epoch 47: 0.8448834717273712
[08/08/2025 05:42:37 INFO]: Training loss at epoch 48: 0.7724225521087646
[08/08/2025 05:42:55 INFO]: New best epoch, val score: -0.8049557244737275
[08/08/2025 05:42:55 INFO]: Saving model to: model_best.pth
[08/08/2025 05:45:04 INFO]: Training loss at epoch 49: 0.6853521466255188
[08/08/2025 05:45:54 INFO]: Training stats: {
    "score": -0.8935752610335036,
    "rmse": 0.8935752610335036
}
[08/08/2025 05:45:54 INFO]: Val stats: {
    "score": -0.8162464879670365,
    "rmse": 0.8162464879670365
}
[08/08/2025 05:45:54 INFO]: Test stats: {
    "score": -0.8231412869793009,
    "rmse": 0.8231412869793009
}
[08/08/2025 05:48:20 INFO]: Training loss at epoch 50: 1.0825095474720001
[08/08/2025 05:48:37 INFO]: New best epoch, val score: -0.7894375666733853
[08/08/2025 05:48:37 INFO]: Saving model to: model_best.pth
[08/08/2025 05:50:46 INFO]: Training loss at epoch 51: 0.7179355323314667
[08/08/2025 05:53:12 INFO]: Training loss at epoch 52: 0.6951642036437988
[08/08/2025 05:55:38 INFO]: Training loss at epoch 53: 0.6912491917610168
[08/08/2025 05:58:04 INFO]: Training loss at epoch 54: 0.6844492852687836
[08/08/2025 05:58:22 INFO]: New best epoch, val score: -0.7679885671492641
[08/08/2025 05:58:22 INFO]: Saving model to: model_best.pth
[08/08/2025 06:00:31 INFO]: Training loss at epoch 55: 0.6369991600513458
[08/08/2025 06:02:57 INFO]: Training loss at epoch 56: 0.6615100204944611
[08/08/2025 06:05:23 INFO]: Training loss at epoch 57: 0.6236875951290131
[08/08/2025 06:07:49 INFO]: Training loss at epoch 58: 0.5347857028245926
[08/08/2025 06:10:15 INFO]: Training loss at epoch 59: 0.5355799049139023
[08/08/2025 06:11:05 INFO]: Training stats: {
    "score": -0.748511965675835,
    "rmse": 0.748511965675835
}
[08/08/2025 06:11:05 INFO]: Val stats: {
    "score": -0.7794021927451579,
    "rmse": 0.7794021927451579
}
[08/08/2025 06:11:05 INFO]: Test stats: {
    "score": -0.7577601962716762,
    "rmse": 0.7577601962716762
}
[08/08/2025 06:13:31 INFO]: Training loss at epoch 60: 0.45145636796951294
[08/08/2025 06:15:57 INFO]: Training loss at epoch 61: 0.6079428493976593
[08/08/2025 06:18:24 INFO]: Training loss at epoch 62: 0.46629565954208374
[08/08/2025 06:20:50 INFO]: Training loss at epoch 63: 0.6072913110256195
[08/08/2025 06:23:16 INFO]: Training loss at epoch 64: 0.5462319254875183
[08/08/2025 06:25:42 INFO]: Training loss at epoch 65: 0.544217050075531
[08/08/2025 06:28:08 INFO]: Training loss at epoch 66: 0.5098216831684113
[08/08/2025 06:30:34 INFO]: Training loss at epoch 67: 0.3938533663749695
[08/08/2025 06:30:52 INFO]: New best epoch, val score: -0.765068747448082
[08/08/2025 06:30:52 INFO]: Saving model to: model_best.pth
[08/08/2025 06:33:01 INFO]: Training loss at epoch 68: 0.39826276898384094
[08/08/2025 06:33:19 INFO]: New best epoch, val score: -0.7571857223772288
[08/08/2025 06:33:19 INFO]: Saving model to: model_best.pth
[08/08/2025 06:35:28 INFO]: Training loss at epoch 69: 0.4652636796236038
[08/08/2025 06:36:17 INFO]: Training stats: {
    "score": -0.7058921039821074,
    "rmse": 0.7058921039821074
}
[08/08/2025 06:36:17 INFO]: Val stats: {
    "score": -0.7532369303993506,
    "rmse": 0.7532369303993506
}
[08/08/2025 06:36:17 INFO]: Test stats: {
    "score": -0.749069780467496,
    "rmse": 0.749069780467496
}
[08/08/2025 06:36:35 INFO]: New best epoch, val score: -0.7532369303993506
[08/08/2025 06:36:35 INFO]: Saving model to: model_best.pth
[08/08/2025 06:38:44 INFO]: Training loss at epoch 70: 0.4208170771598816
[08/08/2025 06:39:01 INFO]: New best epoch, val score: -0.7512902111517877
[08/08/2025 06:39:01 INFO]: Saving model to: model_best.pth
[08/08/2025 06:41:10 INFO]: Training loss at epoch 71: 0.3836442083120346
[08/08/2025 06:41:28 INFO]: New best epoch, val score: -0.7455999047967277
[08/08/2025 06:41:28 INFO]: Saving model to: model_best.pth
[08/08/2025 06:43:37 INFO]: Training loss at epoch 72: 0.38429751992225647
[08/08/2025 06:46:03 INFO]: Training loss at epoch 73: 0.4972222000360489
[08/08/2025 06:46:21 INFO]: New best epoch, val score: -0.7370043376740735
[08/08/2025 06:46:21 INFO]: Saving model to: model_best.pth
[08/08/2025 06:48:30 INFO]: Training loss at epoch 74: 0.5044666528701782
[08/08/2025 06:48:47 INFO]: New best epoch, val score: -0.7345195083493908
[08/08/2025 06:48:47 INFO]: Saving model to: model_best.pth
[08/08/2025 06:50:57 INFO]: Training loss at epoch 75: 0.45762762427330017
[08/08/2025 06:53:23 INFO]: Training loss at epoch 76: 0.5379050225019455
[08/08/2025 06:53:40 INFO]: New best epoch, val score: -0.7197811739476813
[08/08/2025 06:53:40 INFO]: Saving model to: model_best.pth
[08/08/2025 06:55:49 INFO]: Training loss at epoch 77: 0.4508327543735504
[08/08/2025 06:58:15 INFO]: Training loss at epoch 78: 0.4501902610063553
[08/08/2025 06:58:33 INFO]: New best epoch, val score: -0.7042065322569716
[08/08/2025 06:58:33 INFO]: Saving model to: model_best.pth
[08/08/2025 07:00:42 INFO]: Training loss at epoch 79: 0.4206039309501648
[08/08/2025 07:01:31 INFO]: Training stats: {
    "score": -0.6397399295069351,
    "rmse": 0.6397399295069351
}
[08/08/2025 07:01:31 INFO]: Val stats: {
    "score": -0.7587390903191602,
    "rmse": 0.7587390903191602
}
[08/08/2025 07:01:31 INFO]: Test stats: {
    "score": -0.7169166839585881,
    "rmse": 0.7169166839585881
}
[08/08/2025 07:03:58 INFO]: Training loss at epoch 80: 0.4793615788221359
[08/08/2025 07:06:24 INFO]: Training loss at epoch 81: 0.3894130140542984
[08/08/2025 07:08:50 INFO]: Training loss at epoch 82: 0.43895407021045685
[08/08/2025 07:11:16 INFO]: Training loss at epoch 83: 0.47516678273677826
[08/08/2025 07:13:42 INFO]: Training loss at epoch 84: 0.32719361037015915
[08/08/2025 07:16:08 INFO]: Training loss at epoch 85: 0.41382066905498505
[08/08/2025 07:18:34 INFO]: Training loss at epoch 86: 0.3572358638048172
[08/08/2025 07:21:00 INFO]: Training loss at epoch 87: 0.5857269912958145
[08/08/2025 07:21:18 INFO]: New best epoch, val score: -0.6971539546251878
[08/08/2025 07:21:18 INFO]: Saving model to: model_best.pth
[08/08/2025 07:23:27 INFO]: Training loss at epoch 88: 0.40453074872493744
[08/08/2025 07:25:53 INFO]: Training loss at epoch 89: 0.47363728284835815
[08/08/2025 07:26:42 INFO]: Training stats: {
    "score": -0.6037027298285672,
    "rmse": 0.6037027298285672
}
[08/08/2025 07:26:42 INFO]: Val stats: {
    "score": -0.6884844309999942,
    "rmse": 0.6884844309999942
}
[08/08/2025 07:26:42 INFO]: Test stats: {
    "score": -0.6842840999702333,
    "rmse": 0.6842840999702333
}
[08/08/2025 07:27:00 INFO]: New best epoch, val score: -0.6884844309999942
[08/08/2025 07:27:00 INFO]: Saving model to: model_best.pth
[08/08/2025 07:29:09 INFO]: Training loss at epoch 90: 0.4850310683250427
[08/08/2025 07:29:26 INFO]: New best epoch, val score: -0.6761212978772002
[08/08/2025 07:29:26 INFO]: Saving model to: model_best.pth
[08/08/2025 07:31:35 INFO]: Training loss at epoch 91: 0.4735230803489685
[08/08/2025 07:34:02 INFO]: Training loss at epoch 92: 0.3358299136161804
[08/08/2025 07:36:28 INFO]: Training loss at epoch 93: 0.4114517271518707
[08/08/2025 07:38:54 INFO]: Training loss at epoch 94: 0.3694098889827728
[08/08/2025 07:41:20 INFO]: Training loss at epoch 95: 0.3696069121360779
[08/08/2025 07:43:46 INFO]: Training loss at epoch 96: 0.4368458688259125
[08/08/2025 07:46:12 INFO]: Training loss at epoch 97: 0.3591160774230957
[08/08/2025 07:48:38 INFO]: Training loss at epoch 98: 0.3565428853034973
[08/08/2025 07:51:04 INFO]: Training loss at epoch 99: 0.4010372459888458
[08/08/2025 07:51:54 INFO]: Training stats: {
    "score": -0.5848481593839355,
    "rmse": 0.5848481593839355
}
[08/08/2025 07:51:54 INFO]: Val stats: {
    "score": -0.6901421579281288,
    "rmse": 0.6901421579281288
}
[08/08/2025 07:51:54 INFO]: Test stats: {
    "score": -0.6906246038549683,
    "rmse": 0.6906246038549683
}
[08/08/2025 07:54:20 INFO]: Training loss at epoch 100: 0.33379676938056946
[08/08/2025 07:56:46 INFO]: Training loss at epoch 101: 0.3713376522064209
[08/08/2025 07:59:12 INFO]: Training loss at epoch 102: 0.37194907665252686
[08/08/2025 08:01:38 INFO]: Training loss at epoch 103: 0.3886587619781494
[08/08/2025 08:04:04 INFO]: Training loss at epoch 104: 0.33547285199165344
[08/08/2025 08:06:30 INFO]: Training loss at epoch 105: 0.3580511808395386
[08/08/2025 08:08:56 INFO]: Training loss at epoch 106: 0.3523571342229843
[08/08/2025 08:11:22 INFO]: Training loss at epoch 107: 0.36147353053092957
[08/08/2025 08:13:49 INFO]: Training loss at epoch 108: 0.33104081451892853
[08/08/2025 08:16:15 INFO]: Training loss at epoch 109: 0.3058052659034729
[08/08/2025 08:17:04 INFO]: Training stats: {
    "score": -0.662463379541818,
    "rmse": 0.662463379541818
}
[08/08/2025 08:17:04 INFO]: Val stats: {
    "score": -0.7144087551056257,
    "rmse": 0.7144087551056257
}
[08/08/2025 08:17:04 INFO]: Test stats: {
    "score": -0.7514540872942562,
    "rmse": 0.7514540872942562
}
[08/08/2025 08:19:30 INFO]: Training loss at epoch 110: 0.3867201656103134
[08/08/2025 08:21:57 INFO]: Training loss at epoch 111: 0.3927783817052841
[08/08/2025 08:24:23 INFO]: Training loss at epoch 112: 0.41534972190856934
[08/08/2025 08:24:40 INFO]: New best epoch, val score: -0.6708601254611634
[08/08/2025 08:24:40 INFO]: Saving model to: model_best.pth
[08/08/2025 08:26:49 INFO]: Training loss at epoch 113: 0.48027338087558746
[08/08/2025 08:29:15 INFO]: Training loss at epoch 114: 0.3915761709213257
[08/08/2025 08:31:41 INFO]: Training loss at epoch 115: 0.3172581195831299
[08/08/2025 08:34:07 INFO]: Training loss at epoch 116: 0.3581409752368927
[08/08/2025 08:34:24 INFO]: New best epoch, val score: -0.6560293693311704
[08/08/2025 08:34:24 INFO]: Saving model to: model_best.pth
[08/08/2025 08:36:33 INFO]: Training loss at epoch 117: 0.3611039072275162
[08/08/2025 08:38:59 INFO]: Training loss at epoch 118: 0.3753175437450409
[08/08/2025 08:41:25 INFO]: Training loss at epoch 119: 0.29129548370838165
[08/08/2025 08:42:15 INFO]: Training stats: {
    "score": -0.6306474378038913,
    "rmse": 0.6306474378038913
}
[08/08/2025 08:42:15 INFO]: Val stats: {
    "score": -0.7920292765778832,
    "rmse": 0.7920292765778832
}
[08/08/2025 08:42:15 INFO]: Test stats: {
    "score": -0.7839518556477104,
    "rmse": 0.7839518556477104
}
[08/08/2025 08:44:41 INFO]: Training loss at epoch 120: 0.4099958688020706
[08/08/2025 08:47:07 INFO]: Training loss at epoch 121: 0.2905180752277374
[08/08/2025 08:49:33 INFO]: Training loss at epoch 122: 0.362350270152092
[08/08/2025 08:51:59 INFO]: Training loss at epoch 123: 0.38723504543304443
[08/08/2025 08:54:26 INFO]: Training loss at epoch 124: 0.3257944881916046
[08/08/2025 08:56:51 INFO]: Training loss at epoch 125: 0.29500745236873627
[08/08/2025 08:59:17 INFO]: Training loss at epoch 126: 0.3237319588661194
[08/08/2025 09:01:44 INFO]: Training loss at epoch 127: 0.3390233814716339
[08/08/2025 09:04:10 INFO]: Training loss at epoch 128: 0.4348861575126648
[08/08/2025 09:06:36 INFO]: Training loss at epoch 129: 0.26933377236127853
[08/08/2025 09:07:25 INFO]: Training stats: {
    "score": -0.5582940682982053,
    "rmse": 0.5582940682982053
}
[08/08/2025 09:07:25 INFO]: Val stats: {
    "score": -0.6642577491620056,
    "rmse": 0.6642577491620056
}
[08/08/2025 09:07:25 INFO]: Test stats: {
    "score": -0.6604878486923869,
    "rmse": 0.6604878486923869
}
[08/08/2025 09:09:51 INFO]: Training loss at epoch 130: 0.3048022389411926
[08/08/2025 09:12:17 INFO]: Training loss at epoch 131: 0.3998543322086334
[08/08/2025 09:14:43 INFO]: Training loss at epoch 132: 0.30375728011131287
[08/08/2025 09:17:09 INFO]: Training loss at epoch 133: 0.39439117908477783
[08/08/2025 09:19:35 INFO]: Training loss at epoch 134: 0.3924148380756378
[08/08/2025 09:22:01 INFO]: Training loss at epoch 135: 0.2717648148536682
[08/08/2025 09:24:27 INFO]: Training loss at epoch 136: 0.3159653842449188
[08/08/2025 09:26:53 INFO]: Training loss at epoch 137: 0.26333726197481155
[08/08/2025 09:29:18 INFO]: Training loss at epoch 138: 0.3015071302652359
[08/08/2025 09:31:44 INFO]: Training loss at epoch 139: 0.299601212143898
[08/08/2025 09:32:34 INFO]: Training stats: {
    "score": -0.5378004495464385,
    "rmse": 0.5378004495464385
}
[08/08/2025 09:32:34 INFO]: Val stats: {
    "score": -0.6918600964429853,
    "rmse": 0.6918600964429853
}
[08/08/2025 09:32:34 INFO]: Test stats: {
    "score": -0.6800872209396761,
    "rmse": 0.6800872209396761
}
[08/08/2025 09:35:00 INFO]: Training loss at epoch 140: 0.3271076828241348
[08/08/2025 09:37:26 INFO]: Training loss at epoch 141: 0.318816214799881
[08/08/2025 09:39:52 INFO]: Training loss at epoch 142: 0.28272293508052826
[08/08/2025 09:42:18 INFO]: Training loss at epoch 143: 0.32109974324703217
[08/08/2025 09:44:44 INFO]: Training loss at epoch 144: 0.26033105701208115
[08/08/2025 09:47:10 INFO]: Training loss at epoch 145: 0.2881179749965668
[08/08/2025 09:49:36 INFO]: Training loss at epoch 146: 0.3264627158641815
[08/08/2025 09:52:02 INFO]: Training loss at epoch 147: 0.37715257704257965
[08/08/2025 09:52:19 INFO]: Running Final Evaluation...
[08/08/2025 09:53:09 INFO]: Training accuracy: {
    "score": -0.5713290705836517,
    "rmse": 0.5713290705836517
}
[08/08/2025 09:53:09 INFO]: Val accuracy: {
    "score": -0.6560293693311704,
    "rmse": 0.6560293693311704
}
[08/08/2025 09:53:09 INFO]: Test accuracy: {
    "score": -0.6582793350702434,
    "rmse": 0.6582793350702434
}
[08/08/2025 09:53:09 INFO]: {
    "dataset": "ic_upstream2",
    "model_name": "ft_transformer",
    "run_id": "graveless-Jacquleen",
    "best_epoch": 116,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.6582793350702434,
        "rmse": 0.6582793350702434
    },
    "train_stats": {
        "score": -0.5713290705836517,
        "rmse": 0.5713290705836517
    },
    "val_stats": {
        "score": -0.6560293693311704,
        "rmse": 0.6560293693311704
    }
}
[08/08/2025 09:53:09 INFO]: 
_________________________________________________

[08/08/2025 09:53:09 INFO]: train_net_for_optune.py main() running.
[08/08/2025 09:53:09 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.1494644592289207
  attention_dropout: 0.17675088262489108
  ffn_dropout: 0.17675088262489108
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0008208011811923327
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: graveless-Jacquleen

[08/08/2025 09:53:09 INFO]: This ft_transformer has 3.415 million parameters.
[08/08/2025 09:53:09 INFO]: Training will start at epoch 0.
[08/08/2025 09:53:09 INFO]: ==> Starting training for 200 epochs...
[08/08/2025 09:53:43 INFO]: Training loss at epoch 0: 1.0674175024032593
[08/08/2025 09:53:47 INFO]: New best epoch, val score: -1.028924421864155
[08/08/2025 09:53:47 INFO]: Saving model to: model_best.pth
[08/08/2025 09:54:21 INFO]: Training loss at epoch 1: 6.273344337940216
[08/08/2025 09:54:59 INFO]: Training loss at epoch 2: 2.3100998401641846
[08/08/2025 09:55:37 INFO]: Training loss at epoch 3: 1.8130007982254028
[08/08/2025 09:55:41 INFO]: New best epoch, val score: -0.9730093850800385
[08/08/2025 09:55:41 INFO]: Saving model to: model_best.pth
[08/08/2025 09:56:15 INFO]: Training loss at epoch 4: 1.1477288603782654
[08/08/2025 09:56:53 INFO]: Training loss at epoch 5: 1.426561415195465
[08/08/2025 09:57:31 INFO]: Training loss at epoch 6: 1.0686495304107666
[08/08/2025 09:58:09 INFO]: Training loss at epoch 7: 1.1616607904434204
[08/08/2025 09:58:13 INFO]: New best epoch, val score: -0.9636637489615639
[08/08/2025 09:58:13 INFO]: Saving model to: model_best.pth
[08/08/2025 09:58:47 INFO]: Training loss at epoch 8: 0.8820589482784271
[08/08/2025 09:58:52 INFO]: New best epoch, val score: -0.9548905998659356
[08/08/2025 09:58:52 INFO]: Saving model to: model_best.pth
[08/08/2025 09:59:25 INFO]: Training loss at epoch 9: 1.094366431236267
[08/08/2025 09:59:38 INFO]: Training stats: {
    "score": -1.0650800611198497,
    "rmse": 1.0650800611198497
}
[08/08/2025 09:59:38 INFO]: Val stats: {
    "score": -0.9473420834914912,
    "rmse": 0.9473420834914912
}
[08/08/2025 09:59:38 INFO]: Test stats: {
    "score": -0.9546163726517385,
    "rmse": 0.9546163726517385
}
[08/08/2025 09:59:43 INFO]: New best epoch, val score: -0.9473420834914912
[08/08/2025 09:59:43 INFO]: Saving model to: model_best.pth
[08/08/2025 10:00:16 INFO]: Training loss at epoch 10: 1.1487388610839844
[08/08/2025 10:00:21 INFO]: New best epoch, val score: -0.9443704701898826
[08/08/2025 10:00:21 INFO]: Saving model to: model_best.pth
[08/08/2025 10:00:54 INFO]: Training loss at epoch 11: 1.0897093415260315
[08/08/2025 10:01:32 INFO]: Training loss at epoch 12: 0.9589124023914337
[08/08/2025 10:02:10 INFO]: Training loss at epoch 13: 1.2936215996742249
[08/08/2025 10:02:48 INFO]: Training loss at epoch 14: 1.030727505683899
[08/08/2025 10:03:26 INFO]: Training loss at epoch 15: 1.014899104833603
[08/08/2025 10:04:04 INFO]: Training loss at epoch 16: 1.0438962578773499
[08/08/2025 10:04:09 INFO]: New best epoch, val score: -0.9353220635181699
[08/08/2025 10:04:09 INFO]: Saving model to: model_best.pth
[08/08/2025 10:04:42 INFO]: Training loss at epoch 17: 1.0214462280273438
[08/08/2025 10:04:47 INFO]: New best epoch, val score: -0.9276909677963924
[08/08/2025 10:04:47 INFO]: Saving model to: model_best.pth
[08/08/2025 10:05:20 INFO]: Training loss at epoch 18: 0.9619640707969666
[08/08/2025 10:05:58 INFO]: Training loss at epoch 19: 1.306065559387207
[08/08/2025 10:06:11 INFO]: Training stats: {
    "score": -1.0189160814039806,
    "rmse": 1.0189160814039806
}
[08/08/2025 10:06:11 INFO]: Val stats: {
    "score": -0.9271687520930293,
    "rmse": 0.9271687520930293
}
[08/08/2025 10:06:11 INFO]: Test stats: {
    "score": -0.9209106974591561,
    "rmse": 0.9209106974591561
}
[08/08/2025 10:06:16 INFO]: New best epoch, val score: -0.9271687520930293
[08/08/2025 10:06:16 INFO]: Saving model to: model_best.pth
[08/08/2025 10:06:49 INFO]: Training loss at epoch 20: 0.8756109178066254
[08/08/2025 10:06:54 INFO]: New best epoch, val score: -0.9262775510034923
[08/08/2025 10:06:54 INFO]: Saving model to: model_best.pth
[08/08/2025 10:07:28 INFO]: Training loss at epoch 21: 0.9908017516136169
[08/08/2025 10:08:06 INFO]: Training loss at epoch 22: 1.07308030128479
[08/08/2025 10:08:43 INFO]: Training loss at epoch 23: 0.8405345678329468
[08/08/2025 10:09:21 INFO]: Training loss at epoch 24: 0.9743636250495911
[08/08/2025 10:09:59 INFO]: Training loss at epoch 25: 1.1789235770702362
[08/08/2025 10:10:37 INFO]: Training loss at epoch 26: 0.8730812966823578
[08/08/2025 10:11:15 INFO]: Training loss at epoch 27: 0.8089481592178345
[08/08/2025 10:11:53 INFO]: Training loss at epoch 28: 0.9142820239067078
[08/08/2025 10:11:58 INFO]: New best epoch, val score: -0.9195028392183902
[08/08/2025 10:11:58 INFO]: Saving model to: model_best.pth
[08/08/2025 10:12:31 INFO]: Training loss at epoch 29: 0.8261629343032837
[08/08/2025 10:12:44 INFO]: Training stats: {
    "score": -0.9917666339448492,
    "rmse": 0.9917666339448492
}
[08/08/2025 10:12:44 INFO]: Val stats: {
    "score": -0.9152923188289183,
    "rmse": 0.9152923188289183
}
[08/08/2025 10:12:44 INFO]: Test stats: {
    "score": -0.9025731024576827,
    "rmse": 0.9025731024576827
}
[08/08/2025 10:12:49 INFO]: New best epoch, val score: -0.9152923188289183
[08/08/2025 10:12:49 INFO]: Saving model to: model_best.pth
[08/08/2025 10:13:22 INFO]: Training loss at epoch 30: 0.7953765988349915
[08/08/2025 10:13:27 INFO]: New best epoch, val score: -0.9119104770922615
[08/08/2025 10:13:27 INFO]: Saving model to: model_best.pth
[08/08/2025 10:14:00 INFO]: Training loss at epoch 31: 1.0303987562656403
[08/08/2025 10:14:05 INFO]: New best epoch, val score: -0.9085192193636229
[08/08/2025 10:14:05 INFO]: Saving model to: model_best.pth
[08/08/2025 10:14:39 INFO]: Training loss at epoch 32: 1.032146692276001
[08/08/2025 10:15:17 INFO]: Training loss at epoch 33: 0.918501079082489
[08/08/2025 10:15:54 INFO]: Training loss at epoch 34: 0.8899904787540436
[08/08/2025 10:16:32 INFO]: Training loss at epoch 35: 0.8429147899150848
[08/08/2025 10:16:37 INFO]: New best epoch, val score: -0.9034025301950025
[08/08/2025 10:16:37 INFO]: Saving model to: model_best.pth
[08/08/2025 10:17:11 INFO]: Training loss at epoch 36: 0.9144051671028137
[08/08/2025 10:17:15 INFO]: New best epoch, val score: -0.8994735008146039
[08/08/2025 10:17:15 INFO]: Saving model to: model_best.pth
[08/08/2025 10:17:49 INFO]: Training loss at epoch 37: 0.7767493724822998
[08/08/2025 10:17:53 INFO]: New best epoch, val score: -0.8951420161265189
[08/08/2025 10:17:53 INFO]: Saving model to: model_best.pth
[08/08/2025 10:18:27 INFO]: Training loss at epoch 38: 0.8798777163028717
[08/08/2025 10:18:31 INFO]: New best epoch, val score: -0.8844550199172083
[08/08/2025 10:18:31 INFO]: Saving model to: model_best.pth
[08/08/2025 10:19:05 INFO]: Training loss at epoch 39: 0.8648948073387146
[08/08/2025 10:19:18 INFO]: Training stats: {
    "score": -0.8846792001692746,
    "rmse": 0.8846792001692746
}
[08/08/2025 10:19:18 INFO]: Val stats: {
    "score": -0.8744711384874186,
    "rmse": 0.8744711384874186
}
[08/08/2025 10:19:18 INFO]: Test stats: {
    "score": -0.8403519836778455,
    "rmse": 0.8403519836778455
}
[08/08/2025 10:19:22 INFO]: New best epoch, val score: -0.8744711384874186
[08/08/2025 10:19:22 INFO]: Saving model to: model_best.pth
[08/08/2025 10:19:56 INFO]: Training loss at epoch 40: 0.6949704885482788
[08/08/2025 10:20:01 INFO]: New best epoch, val score: -0.8725892493391602
[08/08/2025 10:20:01 INFO]: Saving model to: model_best.pth
[08/08/2025 10:20:34 INFO]: Training loss at epoch 41: 0.8459799587726593
[08/08/2025 10:20:39 INFO]: New best epoch, val score: -0.8432157422061473
[08/08/2025 10:20:39 INFO]: Saving model to: model_best.pth
[08/08/2025 10:21:12 INFO]: Training loss at epoch 42: 1.003581166267395
[08/08/2025 10:21:17 INFO]: New best epoch, val score: -0.8252560315430993
[08/08/2025 10:21:17 INFO]: Saving model to: model_best.pth
[08/08/2025 10:21:50 INFO]: Training loss at epoch 43: 0.6474829018115997
[08/08/2025 10:21:55 INFO]: New best epoch, val score: -0.8165622693810547
[08/08/2025 10:21:55 INFO]: Saving model to: model_best.pth
[08/08/2025 10:22:28 INFO]: Training loss at epoch 44: 0.7395811975002289
[08/08/2025 10:22:33 INFO]: New best epoch, val score: -0.8077419882165212
[08/08/2025 10:22:33 INFO]: Saving model to: model_best.pth
[08/08/2025 10:23:07 INFO]: Training loss at epoch 45: 0.5670949369668961
[08/08/2025 10:23:11 INFO]: New best epoch, val score: -0.7910015417570505
[08/08/2025 10:23:11 INFO]: Saving model to: model_best.pth
[08/08/2025 10:23:45 INFO]: Training loss at epoch 46: 0.7318779528141022
[08/08/2025 10:23:49 INFO]: New best epoch, val score: -0.7846040747377219
[08/08/2025 10:23:49 INFO]: Saving model to: model_best.pth
[08/08/2025 10:24:23 INFO]: Training loss at epoch 47: 0.669463187456131
[08/08/2025 10:25:01 INFO]: Training loss at epoch 48: 0.6401445865631104
[08/08/2025 10:25:39 INFO]: Training loss at epoch 49: 0.5287104994058609
[08/08/2025 10:25:52 INFO]: Training stats: {
    "score": -0.7321919553162698,
    "rmse": 0.7321919553162698
}
[08/08/2025 10:25:52 INFO]: Val stats: {
    "score": -0.8187658110709857,
    "rmse": 0.8187658110709857
}
[08/08/2025 10:25:52 INFO]: Test stats: {
    "score": -0.7288031152557461,
    "rmse": 0.7288031152557461
}
[08/08/2025 10:26:30 INFO]: Training loss at epoch 50: 0.5277548432350159
[08/08/2025 10:27:08 INFO]: Training loss at epoch 51: 0.4525742530822754
[08/08/2025 10:27:12 INFO]: New best epoch, val score: -0.7713016849162654
[08/08/2025 10:27:12 INFO]: Saving model to: model_best.pth
[08/08/2025 10:27:46 INFO]: Training loss at epoch 52: 0.5215642005205154
[08/08/2025 10:28:23 INFO]: Training loss at epoch 53: 0.468763068318367
[08/08/2025 10:29:01 INFO]: Training loss at epoch 54: 0.4330883026123047
[08/08/2025 10:29:06 INFO]: New best epoch, val score: -0.7616723817493114
[08/08/2025 10:29:06 INFO]: Saving model to: model_best.pth
[08/08/2025 10:29:39 INFO]: Training loss at epoch 55: 0.47975145280361176
[08/08/2025 10:29:44 INFO]: New best epoch, val score: -0.7468149242645977
[08/08/2025 10:29:44 INFO]: Saving model to: model_best.pth
[08/08/2025 10:30:17 INFO]: Training loss at epoch 56: 0.4996159076690674
[08/08/2025 10:30:55 INFO]: Training loss at epoch 57: 0.47997479140758514
[08/08/2025 10:31:33 INFO]: Training loss at epoch 58: 0.39625437557697296
[08/08/2025 10:32:11 INFO]: Training loss at epoch 59: 0.3195512443780899
[08/08/2025 10:32:24 INFO]: Training stats: {
    "score": -0.6359677757340871,
    "rmse": 0.6359677757340871
}
[08/08/2025 10:32:24 INFO]: Val stats: {
    "score": -0.7240825007607997,
    "rmse": 0.7240825007607997
}
[08/08/2025 10:32:24 INFO]: Test stats: {
    "score": -0.6848300761253516,
    "rmse": 0.6848300761253516
}
[08/08/2025 10:32:29 INFO]: New best epoch, val score: -0.7240825007607997
[08/08/2025 10:32:29 INFO]: Saving model to: model_best.pth
[08/08/2025 10:33:02 INFO]: Training loss at epoch 60: 0.4094514846801758
[08/08/2025 10:33:07 INFO]: New best epoch, val score: -0.7200496051756842
[08/08/2025 10:33:07 INFO]: Saving model to: model_best.pth
[08/08/2025 10:33:40 INFO]: Training loss at epoch 61: 0.39118413627147675
[08/08/2025 10:34:18 INFO]: Training loss at epoch 62: 0.4029745012521744
[08/08/2025 10:34:56 INFO]: Training loss at epoch 63: 0.32568229734897614
[08/08/2025 10:35:01 INFO]: New best epoch, val score: -0.6989740347649219
[08/08/2025 10:35:01 INFO]: Saving model to: model_best.pth
[08/08/2025 10:35:34 INFO]: Training loss at epoch 64: 0.46029306948184967
[08/08/2025 10:35:39 INFO]: New best epoch, val score: -0.6971974529236927
[08/08/2025 10:35:39 INFO]: Saving model to: model_best.pth
[08/08/2025 10:36:13 INFO]: Training loss at epoch 65: 0.5056842714548111
[08/08/2025 10:36:51 INFO]: Training loss at epoch 66: 0.3928176462650299
[08/08/2025 10:37:28 INFO]: Training loss at epoch 67: 0.29848431050777435
[08/08/2025 10:37:33 INFO]: New best epoch, val score: -0.6829428388477528
[08/08/2025 10:37:33 INFO]: Saving model to: model_best.pth
[08/08/2025 10:38:07 INFO]: Training loss at epoch 68: 0.5131852477788925
[08/08/2025 10:38:11 INFO]: New best epoch, val score: -0.6753172894839572
[08/08/2025 10:38:11 INFO]: Saving model to: model_best.pth
[08/08/2025 10:38:45 INFO]: Training loss at epoch 69: 0.3125158101320267
[08/08/2025 10:38:58 INFO]: Training stats: {
    "score": -0.5690323370322373,
    "rmse": 0.5690323370322373
}
[08/08/2025 10:38:58 INFO]: Val stats: {
    "score": -0.697581829083479,
    "rmse": 0.697581829083479
}
[08/08/2025 10:38:58 INFO]: Test stats: {
    "score": -0.6805470158323661,
    "rmse": 0.6805470158323661
}
[08/08/2025 10:39:36 INFO]: Training loss at epoch 70: 0.38513369858264923
[08/08/2025 10:40:14 INFO]: Training loss at epoch 71: 0.4097655117511749
[08/08/2025 10:40:52 INFO]: Training loss at epoch 72: 0.24866411834955215
[08/08/2025 10:41:29 INFO]: Training loss at epoch 73: 0.4535534083843231
[08/08/2025 10:42:07 INFO]: Training loss at epoch 74: 0.38248683512210846
[08/08/2025 10:42:45 INFO]: Training loss at epoch 75: 0.35046766698360443
[08/08/2025 10:43:23 INFO]: Training loss at epoch 76: 0.307467520236969
[08/08/2025 10:44:01 INFO]: Training loss at epoch 77: 0.2981364578008652
[08/08/2025 10:44:39 INFO]: Training loss at epoch 78: 0.30496953427791595
[08/08/2025 10:45:17 INFO]: Training loss at epoch 79: 0.2826485186815262
[08/08/2025 10:45:30 INFO]: Training stats: {
    "score": -0.5303951580498478,
    "rmse": 0.5303951580498478
}
[08/08/2025 10:45:30 INFO]: Val stats: {
    "score": -0.7037479169765131,
    "rmse": 0.7037479169765131
}
[08/08/2025 10:45:30 INFO]: Test stats: {
    "score": -0.6587101018302732,
    "rmse": 0.6587101018302732
}
[08/08/2025 10:46:08 INFO]: Training loss at epoch 80: 0.2852432131767273
[08/08/2025 10:46:46 INFO]: Training loss at epoch 81: 0.3080587387084961
[08/08/2025 10:47:24 INFO]: Training loss at epoch 82: 0.33188875019550323
[08/08/2025 10:48:02 INFO]: Training loss at epoch 83: 0.2877289950847626
[08/08/2025 10:48:40 INFO]: Training loss at epoch 84: 0.3058183789253235
[08/08/2025 10:49:18 INFO]: Training loss at epoch 85: 0.27203449606895447
[08/08/2025 10:49:55 INFO]: Training loss at epoch 86: 0.306598037481308
[08/08/2025 10:50:33 INFO]: Training loss at epoch 87: 0.29728278517723083
[08/08/2025 10:51:11 INFO]: Training loss at epoch 88: 0.2466537430882454
[08/08/2025 10:51:49 INFO]: Training loss at epoch 89: 0.3106207400560379
[08/08/2025 10:52:02 INFO]: Training stats: {
    "score": -0.5086361384085507,
    "rmse": 0.5086361384085507
}
[08/08/2025 10:52:02 INFO]: Val stats: {
    "score": -0.7148717378057039,
    "rmse": 0.7148717378057039
}
[08/08/2025 10:52:02 INFO]: Test stats: {
    "score": -0.7047151219394705,
    "rmse": 0.7047151219394705
}
[08/08/2025 10:52:40 INFO]: Training loss at epoch 90: 0.30045120418071747
[08/08/2025 10:53:18 INFO]: Training loss at epoch 91: 0.29446952044963837
[08/08/2025 10:53:56 INFO]: Training loss at epoch 92: 0.2746085971593857
[08/08/2025 10:54:34 INFO]: Training loss at epoch 93: 0.26122377067804337
[08/08/2025 10:55:12 INFO]: Training loss at epoch 94: 0.20355603471398354
[08/08/2025 10:55:50 INFO]: Training loss at epoch 95: 0.2916458994150162
[08/08/2025 10:56:28 INFO]: Training loss at epoch 96: 0.2755918726325035
[08/08/2025 10:57:06 INFO]: Training loss at epoch 97: 0.27563193440437317
[08/08/2025 10:57:44 INFO]: Training loss at epoch 98: 0.2474040612578392
[08/08/2025 10:58:22 INFO]: Training loss at epoch 99: 0.28694429993629456
[08/08/2025 10:58:34 INFO]: Training stats: {
    "score": -0.47451482404790246,
    "rmse": 0.47451482404790246
}
[08/08/2025 10:58:34 INFO]: Val stats: {
    "score": -0.6999608815638388,
    "rmse": 0.6999608815638388
}
[08/08/2025 10:58:34 INFO]: Test stats: {
    "score": -0.6626454037645753,
    "rmse": 0.6626454037645753
}
[08/08/2025 10:58:39 INFO]: Running Final Evaluation...
[08/08/2025 10:58:52 INFO]: Training accuracy: {
    "score": -0.5743918592295533,
    "rmse": 0.5743918592295533
}
[08/08/2025 10:58:52 INFO]: Val accuracy: {
    "score": -0.6753172894839572,
    "rmse": 0.6753172894839572
}
[08/08/2025 10:58:52 INFO]: Test accuracy: {
    "score": -0.659269847352833,
    "rmse": 0.659269847352833
}
[08/08/2025 10:58:52 INFO]: {
    "dataset": "ic_upstream2",
    "model_name": "ft_transformer",
    "run_id": "graveless-Jacquleen",
    "best_epoch": 68,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.659269847352833,
        "rmse": 0.659269847352833
    },
    "train_stats": {
        "score": -0.5743918592295533,
        "rmse": 0.5743918592295533
    },
    "val_stats": {
        "score": -0.6753172894839572,
        "rmse": 0.6753172894839572
    }
}
[08/08/2025 10:58:52 INFO]: 
_________________________________________________

[08/08/2025 10:58:52 INFO]: train_net_for_optune.py main() running.
[08/08/2025 10:58:52 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 2.5016810026647045
  attention_dropout: 0.0037020471973247746
  ffn_dropout: 0.0037020471973247746
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00029456024202692544
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: graveless-Jacquleen

[08/08/2025 10:58:52 INFO]: This ft_transformer has 7.624 million parameters.
[08/08/2025 10:58:52 INFO]: Training will start at epoch 0.
[08/08/2025 10:58:52 INFO]: ==> Starting training for 200 epochs...
[08/08/2025 11:00:54 INFO]: Training loss at epoch 0: 1.4255818128585815
[08/08/2025 11:01:10 INFO]: New best epoch, val score: -1.1888566016294622
[08/08/2025 11:01:10 INFO]: Saving model to: model_best.pth
[08/08/2025 11:03:11 INFO]: Training loss at epoch 1: 3.3376455307006836
[08/08/2025 11:03:27 INFO]: New best epoch, val score: -0.9939260145063581
[08/08/2025 11:03:27 INFO]: Saving model to: model_best.pth
[08/08/2025 11:05:29 INFO]: Training loss at epoch 2: 1.288400411605835
[08/08/2025 11:07:45 INFO]: Training loss at epoch 3: 1.1086362600326538
[08/08/2025 11:08:01 INFO]: New best epoch, val score: -0.9336185815696136
[08/08/2025 11:08:01 INFO]: Saving model to: model_best.pth
[08/08/2025 11:10:02 INFO]: Training loss at epoch 4: 0.9925602674484253
[08/08/2025 11:10:18 INFO]: New best epoch, val score: -0.9255160590258522
[08/08/2025 11:10:18 INFO]: Saving model to: model_best.pth
[08/08/2025 11:12:19 INFO]: Training loss at epoch 5: 1.2208904027938843
[08/08/2025 11:12:35 INFO]: New best epoch, val score: -0.9251586766195348
[08/08/2025 11:12:35 INFO]: Saving model to: model_best.pth
[08/08/2025 11:14:36 INFO]: Training loss at epoch 6: 1.1404566764831543
[08/08/2025 11:14:52 INFO]: New best epoch, val score: -0.9246289990494715
[08/08/2025 11:14:52 INFO]: Saving model to: model_best.pth
[08/08/2025 11:16:53 INFO]: Training loss at epoch 7: 1.0315614342689514
[08/08/2025 11:17:08 INFO]: New best epoch, val score: -0.9241793828527181
[08/08/2025 11:17:08 INFO]: Saving model to: model_best.pth
[08/08/2025 11:19:09 INFO]: Training loss at epoch 8: 1.1142669916152954
[08/08/2025 11:19:25 INFO]: New best epoch, val score: -0.9239852283684482
[08/08/2025 11:19:25 INFO]: Saving model to: model_best.pth
[08/08/2025 11:21:26 INFO]: Training loss at epoch 9: 0.9839136600494385
[08/08/2025 11:22:10 INFO]: Training stats: {
    "score": -1.004689490894345,
    "rmse": 1.004689490894345
}
[08/08/2025 11:22:10 INFO]: Val stats: {
    "score": -0.9241527550420581,
    "rmse": 0.9241527550420581
}
[08/08/2025 11:22:10 INFO]: Test stats: {
    "score": -0.910915885800372,
    "rmse": 0.910915885800372
}
[08/08/2025 11:24:26 INFO]: Training loss at epoch 10: 0.9361506402492523
[08/08/2025 11:26:43 INFO]: Training loss at epoch 11: 0.9480240941047668
[08/08/2025 11:28:59 INFO]: Training loss at epoch 12: 1.1578463315963745
[08/08/2025 11:31:16 INFO]: Training loss at epoch 13: 1.011879324913025
[08/08/2025 11:33:32 INFO]: Training loss at epoch 14: 0.8371660709381104
[08/08/2025 11:35:49 INFO]: Training loss at epoch 15: 0.9686895310878754
[08/08/2025 11:38:05 INFO]: Training loss at epoch 16: 0.9785109758377075
[08/08/2025 11:38:21 INFO]: New best epoch, val score: -0.9181888829037597
[08/08/2025 11:38:21 INFO]: Saving model to: model_best.pth
[08/08/2025 11:40:22 INFO]: Training loss at epoch 17: 0.9953599572181702
[08/08/2025 11:40:38 INFO]: New best epoch, val score: -0.9130462558932761
[08/08/2025 11:40:38 INFO]: Saving model to: model_best.pth
[08/08/2025 11:42:39 INFO]: Training loss at epoch 18: 0.7457417696714401
[08/08/2025 11:42:55 INFO]: New best epoch, val score: -0.9106146792439564
[08/08/2025 11:42:55 INFO]: Saving model to: model_best.pth
[08/08/2025 11:44:56 INFO]: Training loss at epoch 19: 1.0919222235679626
[08/08/2025 11:45:41 INFO]: Training stats: {
    "score": -0.973307057107476,
    "rmse": 0.973307057107476
}
[08/08/2025 11:45:41 INFO]: Val stats: {
    "score": -0.9108839168470344,
    "rmse": 0.9108839168470344
}
[08/08/2025 11:45:41 INFO]: Test stats: {
    "score": -0.888275440456911,
    "rmse": 0.888275440456911
}
[08/08/2025 11:47:58 INFO]: Training loss at epoch 20: 1.0765690207481384
[08/08/2025 11:50:15 INFO]: Training loss at epoch 21: 0.9402803480625153
[08/08/2025 11:52:31 INFO]: Training loss at epoch 22: 0.8230938911437988
[08/08/2025 11:54:48 INFO]: Training loss at epoch 23: 0.9125477373600006
[08/08/2025 11:57:04 INFO]: Training loss at epoch 24: 0.7757495641708374
[08/08/2025 11:59:21 INFO]: Training loss at epoch 25: 0.8642610609531403
[08/08/2025 11:59:36 INFO]: New best epoch, val score: -0.8831623908281988
[08/08/2025 11:59:36 INFO]: Saving model to: model_best.pth
[08/08/2025 12:01:37 INFO]: Training loss at epoch 26: 0.9829732775688171
[08/08/2025 12:01:53 INFO]: New best epoch, val score: -0.8735265341170039
[08/08/2025 12:01:53 INFO]: Saving model to: model_best.pth
[08/08/2025 12:03:54 INFO]: Training loss at epoch 27: 0.7712475061416626
[08/08/2025 12:06:11 INFO]: Training loss at epoch 28: 0.644849568605423
[08/08/2025 12:08:27 INFO]: Training loss at epoch 29: 0.7968391478061676
[08/08/2025 12:09:12 INFO]: Training stats: {
    "score": -0.9025160475280888,
    "rmse": 0.9025160475280888
}
[08/08/2025 12:09:12 INFO]: Val stats: {
    "score": -0.9492885418703149,
    "rmse": 0.9492885418703149
}
[08/08/2025 12:09:12 INFO]: Test stats: {
    "score": -0.8727071730697271,
    "rmse": 0.8727071730697271
}
[08/08/2025 12:11:29 INFO]: Training loss at epoch 30: 0.8574300110340118
[08/08/2025 12:13:45 INFO]: Training loss at epoch 31: 0.7815840244293213
[08/08/2025 12:14:01 INFO]: New best epoch, val score: -0.815503294701637
[08/08/2025 12:14:01 INFO]: Saving model to: model_best.pth
[08/08/2025 12:16:02 INFO]: Training loss at epoch 32: 0.660555899143219
[08/08/2025 12:16:18 INFO]: New best epoch, val score: -0.7841218838427148
[08/08/2025 12:16:18 INFO]: Saving model to: model_best.pth
[08/08/2025 12:18:19 INFO]: Training loss at epoch 33: 0.7615586221218109
[08/08/2025 12:20:36 INFO]: Training loss at epoch 34: 0.6117702126502991
[08/08/2025 12:22:52 INFO]: Training loss at epoch 35: 0.4998822659254074
[08/08/2025 12:23:08 INFO]: New best epoch, val score: -0.7379995597784046
[08/08/2025 12:23:08 INFO]: Saving model to: model_best.pth
[08/08/2025 12:25:09 INFO]: Training loss at epoch 36: 0.574417233467102
[08/08/2025 12:25:25 INFO]: New best epoch, val score: -0.7287892199010825
[08/08/2025 12:25:25 INFO]: Saving model to: model_best.pth
[08/08/2025 12:27:26 INFO]: Training loss at epoch 37: 0.4144367277622223
[08/08/2025 12:29:43 INFO]: Training loss at epoch 38: 0.551086813211441
[08/08/2025 12:32:00 INFO]: Training loss at epoch 39: 0.44564172625541687
[08/08/2025 12:32:45 INFO]: Training stats: {
    "score": -0.6885325046205653,
    "rmse": 0.6885325046205653
}
[08/08/2025 12:32:45 INFO]: Val stats: {
    "score": -0.7079811015220475,
    "rmse": 0.7079811015220475
}
[08/08/2025 12:32:45 INFO]: Test stats: {
    "score": -0.7042917073911567,
    "rmse": 0.7042917073911567
}
[08/08/2025 12:33:00 INFO]: New best epoch, val score: -0.7079811015220475
[08/08/2025 12:33:00 INFO]: Saving model to: model_best.pth
[08/08/2025 12:35:01 INFO]: Training loss at epoch 40: 0.367192767560482
[08/08/2025 12:37:17 INFO]: Training loss at epoch 41: 0.36437033861875534
[08/08/2025 12:39:34 INFO]: Training loss at epoch 42: 0.40278278291225433
[08/08/2025 12:41:51 INFO]: Training loss at epoch 43: 0.40916556119918823
[08/08/2025 12:44:08 INFO]: Training loss at epoch 44: 0.4781075417995453
[08/08/2025 12:46:25 INFO]: Training loss at epoch 45: 0.38670068979263306
[08/08/2025 12:46:41 INFO]: New best epoch, val score: -0.6756199973853331
[08/08/2025 12:46:41 INFO]: Saving model to: model_best.pth
[08/08/2025 12:48:42 INFO]: Training loss at epoch 46: 0.44443823397159576
[08/08/2025 12:50:58 INFO]: Training loss at epoch 47: 0.37553155422210693
[08/08/2025 12:53:15 INFO]: Training loss at epoch 48: 0.49896833300590515
[08/08/2025 12:53:31 INFO]: New best epoch, val score: -0.6698066219166502
[08/08/2025 12:53:31 INFO]: Saving model to: model_best.pth
[08/08/2025 12:55:32 INFO]: Training loss at epoch 49: 0.38889436423778534
[08/08/2025 12:56:17 INFO]: Training stats: {
    "score": -0.5832291897131864,
    "rmse": 0.5832291897131864
}
[08/08/2025 12:56:17 INFO]: Val stats: {
    "score": -0.6865366967232298,
    "rmse": 0.6865366967232298
}
[08/08/2025 12:56:17 INFO]: Test stats: {
    "score": -0.6630239250693818,
    "rmse": 0.6630239250693818
}
[08/08/2025 12:58:34 INFO]: Training loss at epoch 50: 0.41288696229457855
[08/08/2025 13:00:50 INFO]: Training loss at epoch 51: 0.3326677531003952
[08/08/2025 13:03:07 INFO]: Training loss at epoch 52: 0.382929727435112
[08/08/2025 13:03:23 INFO]: New best epoch, val score: -0.6479394964593519
[08/08/2025 13:03:23 INFO]: Saving model to: model_best.pth
[08/08/2025 13:05:24 INFO]: Training loss at epoch 53: 0.2908783406019211
[08/08/2025 13:07:41 INFO]: Training loss at epoch 54: 0.4469040334224701
[08/08/2025 13:07:57 INFO]: New best epoch, val score: -0.643864435444376
[08/08/2025 13:07:57 INFO]: Saving model to: model_best.pth
[08/08/2025 13:09:58 INFO]: Training loss at epoch 55: 0.33732376992702484
[08/08/2025 13:10:14 INFO]: New best epoch, val score: -0.6405420463618846
[08/08/2025 13:10:14 INFO]: Saving model to: model_best.pth
[08/08/2025 13:12:15 INFO]: Training loss at epoch 56: 0.3450195640325546
[08/08/2025 13:14:31 INFO]: Training loss at epoch 57: 0.2435220256447792
[08/08/2025 13:14:47 INFO]: New best epoch, val score: -0.6315101114507667
[08/08/2025 13:14:47 INFO]: Saving model to: model_best.pth
[08/08/2025 13:16:48 INFO]: Training loss at epoch 58: 0.3283955156803131
[08/08/2025 13:17:04 INFO]: New best epoch, val score: -0.6207126455186388
[08/08/2025 13:17:04 INFO]: Saving model to: model_best.pth
[08/08/2025 13:19:05 INFO]: Training loss at epoch 59: 0.26397790759801865
[08/08/2025 13:19:50 INFO]: Training stats: {
    "score": -0.515097708348963,
    "rmse": 0.515097708348963
}
[08/08/2025 13:19:50 INFO]: Val stats: {
    "score": -0.6223384608836801,
    "rmse": 0.6223384608836801
}
[08/08/2025 13:19:50 INFO]: Test stats: {
    "score": -0.6465374322280241,
    "rmse": 0.6465374322280241
}
[08/08/2025 13:22:06 INFO]: Training loss at epoch 60: 0.3135848194360733
[08/08/2025 13:22:22 INFO]: New best epoch, val score: -0.616295334244069
[08/08/2025 13:22:22 INFO]: Saving model to: model_best.pth
[08/08/2025 13:24:24 INFO]: Training loss at epoch 61: 0.24868665635585785
[08/08/2025 13:26:40 INFO]: Training loss at epoch 62: 0.23787835985422134
[08/08/2025 13:28:57 INFO]: Training loss at epoch 63: 0.2526286020874977
[08/08/2025 13:31:14 INFO]: Training loss at epoch 64: 0.2728053480386734
[08/08/2025 13:31:29 INFO]: New best epoch, val score: -0.6089075776110274
[08/08/2025 13:31:29 INFO]: Saving model to: model_best.pth
[08/08/2025 13:33:30 INFO]: Training loss at epoch 65: 0.2952028959989548
[08/08/2025 13:35:47 INFO]: Training loss at epoch 66: 0.22624127566814423
[08/08/2025 13:38:03 INFO]: Training loss at epoch 67: 0.22514215111732483
[08/08/2025 13:40:20 INFO]: Training loss at epoch 68: 0.20978939533233643
[08/08/2025 13:42:36 INFO]: Training loss at epoch 69: 0.3074767291545868
[08/08/2025 13:43:21 INFO]: Training stats: {
    "score": -0.49451739539003536,
    "rmse": 0.49451739539003536
}
[08/08/2025 13:43:21 INFO]: Val stats: {
    "score": -0.6139962033540833,
    "rmse": 0.6139962033540833
}
[08/08/2025 13:43:21 INFO]: Test stats: {
    "score": -0.6205660036561028,
    "rmse": 0.6205660036561028
}
[08/08/2025 13:45:38 INFO]: Training loss at epoch 70: 0.3164560943841934
[08/08/2025 13:47:55 INFO]: Training loss at epoch 71: 0.23482758551836014
[08/08/2025 13:50:11 INFO]: Training loss at epoch 72: 0.3230852782726288
[08/08/2025 13:52:28 INFO]: Training loss at epoch 73: 0.2997710257768631
[08/08/2025 13:54:45 INFO]: Training loss at epoch 74: 0.21849747002124786
[08/08/2025 13:57:01 INFO]: Training loss at epoch 75: 0.2581764683127403
[08/08/2025 13:59:18 INFO]: Training loss at epoch 76: 0.18650373071432114
[08/08/2025 14:01:35 INFO]: Training loss at epoch 77: 0.1847425401210785
[08/08/2025 14:03:51 INFO]: Training loss at epoch 78: 0.2143632099032402
[08/08/2025 14:06:08 INFO]: Training loss at epoch 79: 0.23611444234848022
[08/08/2025 14:06:52 INFO]: Training stats: {
    "score": -0.42934331063886455,
    "rmse": 0.42934331063886455
}
[08/08/2025 14:06:52 INFO]: Val stats: {
    "score": -0.6425823467898789,
    "rmse": 0.6425823467898789
}
[08/08/2025 14:06:52 INFO]: Test stats: {
    "score": -0.6495204377713939,
    "rmse": 0.6495204377713939
}
[08/08/2025 14:09:09 INFO]: Training loss at epoch 80: 0.19981811940670013
[08/08/2025 14:11:25 INFO]: Training loss at epoch 81: 0.18346567451953888
[08/08/2025 14:13:42 INFO]: Training loss at epoch 82: 0.1913841962814331
[08/08/2025 14:15:58 INFO]: Training loss at epoch 83: 0.2232271432876587
[08/08/2025 14:18:15 INFO]: Training loss at epoch 84: 0.2208089381456375
[08/08/2025 14:20:31 INFO]: Training loss at epoch 85: 0.20977062731981277
[08/08/2025 14:22:47 INFO]: Training loss at epoch 86: 0.2045348510146141
[08/08/2025 14:25:04 INFO]: Training loss at epoch 87: 0.15898581594228745
[08/08/2025 14:27:20 INFO]: Training loss at epoch 88: 0.17451024800539017
[08/08/2025 14:29:37 INFO]: Training loss at epoch 89: 0.14728767424821854
[08/08/2025 14:30:21 INFO]: Training stats: {
    "score": -0.3946318460074356,
    "rmse": 0.3946318460074356
}
[08/08/2025 14:30:21 INFO]: Val stats: {
    "score": -0.6402822266823174,
    "rmse": 0.6402822266823174
}
[08/08/2025 14:30:21 INFO]: Test stats: {
    "score": -0.6294984436536618,
    "rmse": 0.6294984436536618
}
[08/08/2025 14:32:38 INFO]: Training loss at epoch 90: 0.1992419958114624
[08/08/2025 14:34:55 INFO]: Training loss at epoch 91: 0.18739158660173416
[08/08/2025 14:37:11 INFO]: Training loss at epoch 92: 0.1334117315709591
[08/08/2025 14:39:28 INFO]: Training loss at epoch 93: 0.23076697438955307
[08/08/2025 14:41:44 INFO]: Training loss at epoch 94: 0.21966154873371124
[08/08/2025 14:44:00 INFO]: Training loss at epoch 95: 0.17371252924203873
[08/08/2025 14:44:16 INFO]: Running Final Evaluation...
[08/08/2025 14:45:07 INFO]: Training accuracy: {
    "score": -0.5153033398315265,
    "rmse": 0.5153033398315265
}
[08/08/2025 14:45:07 INFO]: Val accuracy: {
    "score": -0.6089075776110274,
    "rmse": 0.6089075776110274
}
[08/08/2025 14:45:07 INFO]: Test accuracy: {
    "score": -0.6234870244607056,
    "rmse": 0.6234870244607056
}
[08/08/2025 14:45:07 INFO]: {
    "dataset": "ic_upstream2",
    "model_name": "ft_transformer",
    "run_id": "graveless-Jacquleen",
    "best_epoch": 64,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.6234870244607056,
        "rmse": 0.6234870244607056
    },
    "train_stats": {
        "score": -0.5153033398315265,
        "rmse": 0.5153033398315265
    },
    "val_stats": {
        "score": -0.6089075776110274,
        "rmse": 0.6089075776110274
    }
}
[08/08/2025 14:45:07 INFO]: 
_________________________________________________

[08/08/2025 14:45:07 INFO]: train_net_for_optune.py main() running.
[08/08/2025 14:45:07 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 2.647508630846063
  attention_dropout: 0.007956390256118906
  ffn_dropout: 0.007956390256118906
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00019706361711404547
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: graveless-Jacquleen

[08/08/2025 14:45:07 INFO]: This ft_transformer has 7.909 million parameters.
[08/08/2025 14:45:07 INFO]: Training will start at epoch 0.
[08/08/2025 14:45:07 INFO]: ==> Starting training for 200 epochs...
[08/08/2025 14:47:14 INFO]: Training loss at epoch 0: 0.928889811038971
[08/08/2025 14:47:30 INFO]: New best epoch, val score: -0.9643035341471828
[08/08/2025 14:47:30 INFO]: Saving model to: model_best.pth
[08/08/2025 14:49:36 INFO]: Training loss at epoch 1: 1.4645283818244934
[08/08/2025 14:49:52 INFO]: New best epoch, val score: -0.8894206863006829
[08/08/2025 14:49:52 INFO]: Saving model to: model_best.pth
[08/08/2025 14:51:58 INFO]: Training loss at epoch 2: 1.2158527672290802
[08/08/2025 14:52:14 INFO]: New best epoch, val score: -0.8853408738935978
[08/08/2025 14:52:14 INFO]: Saving model to: model_best.pth
[08/08/2025 14:54:20 INFO]: Training loss at epoch 3: 0.8272883594036102
[08/08/2025 14:56:41 INFO]: Training loss at epoch 4: 1.0177327990531921
[08/08/2025 14:56:58 INFO]: New best epoch, val score: -0.8268182797100284
[08/08/2025 14:56:58 INFO]: Saving model to: model_best.pth
[08/08/2025 14:59:04 INFO]: Training loss at epoch 5: 0.6825908124446869
[08/08/2025 14:59:20 INFO]: New best epoch, val score: -0.8265904063404366
[08/08/2025 14:59:20 INFO]: Saving model to: model_best.pth
[08/08/2025 15:01:47 INFO]: Training loss at epoch 6: 0.6204284578561783
[08/08/2025 15:02:04 INFO]: New best epoch, val score: -0.8261159785017378
[08/08/2025 15:02:04 INFO]: Saving model to: model_best.pth
[08/08/2025 15:04:09 INFO]: Training loss at epoch 7: 0.8005048632621765
[08/08/2025 15:04:26 INFO]: New best epoch, val score: -0.8255167367455389
[08/08/2025 15:04:26 INFO]: Saving model to: model_best.pth
[08/08/2025 15:06:31 INFO]: Training loss at epoch 8: 0.8886336088180542
[08/08/2025 15:06:48 INFO]: New best epoch, val score: -0.8250247305145024
[08/08/2025 15:06:48 INFO]: Saving model to: model_best.pth
[08/08/2025 15:08:53 INFO]: Training loss at epoch 9: 0.71119424700737
[08/08/2025 15:09:39 INFO]: Training stats: {
    "score": -0.8686482558279375,
    "rmse": 0.8686482558279375
}
[08/08/2025 15:09:39 INFO]: Val stats: {
    "score": -0.8248428014474177,
    "rmse": 0.8248428014474177
}
[08/08/2025 15:09:39 INFO]: Test stats: {
    "score": -0.8194500006869118,
    "rmse": 0.8194500006869118
}
[08/08/2025 15:09:56 INFO]: New best epoch, val score: -0.8248428014474177
[08/08/2025 15:09:56 INFO]: Saving model to: model_best.pth
[08/08/2025 15:12:01 INFO]: Training loss at epoch 10: 0.8163754343986511
[08/08/2025 15:14:22 INFO]: Training loss at epoch 11: 0.8511025011539459
[08/08/2025 15:16:45 INFO]: Training loss at epoch 12: 0.6059884130954742
[08/08/2025 15:19:06 INFO]: Training loss at epoch 13: 0.5635170042514801
[08/08/2025 15:19:23 INFO]: New best epoch, val score: -0.7730522664782883
[08/08/2025 15:19:23 INFO]: Saving model to: model_best.pth
[08/08/2025 15:21:28 INFO]: Training loss at epoch 14: 0.6534054577350616
[08/08/2025 15:21:45 INFO]: New best epoch, val score: -0.767102411838204
[08/08/2025 15:21:45 INFO]: Saving model to: model_best.pth
[08/08/2025 15:23:50 INFO]: Training loss at epoch 15: 0.6484308540821075
[08/08/2025 15:26:12 INFO]: Training loss at epoch 16: 0.5948097407817841
[08/08/2025 15:28:34 INFO]: Training loss at epoch 17: 0.5125321745872498
[08/08/2025 15:28:50 INFO]: New best epoch, val score: -0.7597505248367474
[08/08/2025 15:28:50 INFO]: Saving model to: model_best.pth
[08/08/2025 15:30:56 INFO]: Training loss at epoch 18: 0.42766767740249634
[08/08/2025 15:31:12 INFO]: New best epoch, val score: -0.7303311267415153
[08/08/2025 15:31:12 INFO]: Saving model to: model_best.pth
[08/08/2025 15:33:18 INFO]: Training loss at epoch 19: 0.6213114261627197
[08/08/2025 15:34:04 INFO]: Training stats: {
    "score": -0.7075727729491289,
    "rmse": 0.7075727729491289
}
[08/08/2025 15:34:04 INFO]: Val stats: {
    "score": -0.7171572116408725,
    "rmse": 0.7171572116408725
}
[08/08/2025 15:34:04 INFO]: Test stats: {
    "score": -0.7186392484317475,
    "rmse": 0.7186392484317475
}
[08/08/2025 15:34:20 INFO]: New best epoch, val score: -0.7171572116408725
[08/08/2025 15:34:20 INFO]: Saving model to: model_best.pth
[08/08/2025 15:36:26 INFO]: Training loss at epoch 20: 0.4427512586116791
[08/08/2025 15:38:48 INFO]: Training loss at epoch 21: 0.6229239851236343
[08/08/2025 15:41:09 INFO]: Training loss at epoch 22: 0.4770257771015167
[08/08/2025 15:43:31 INFO]: Training loss at epoch 23: 0.3462635576725006
[08/08/2025 15:43:47 INFO]: New best epoch, val score: -0.6869470231700643
[08/08/2025 15:43:47 INFO]: Saving model to: model_best.pth
[08/08/2025 15:45:53 INFO]: Training loss at epoch 24: 0.3307364284992218
[08/08/2025 15:46:09 INFO]: New best epoch, val score: -0.6746692641750163
[08/08/2025 15:46:09 INFO]: Saving model to: model_best.pth
[08/08/2025 15:48:15 INFO]: Training loss at epoch 25: 0.445380762219429
[08/08/2025 15:50:37 INFO]: Training loss at epoch 26: 0.39142604172229767
[08/08/2025 15:52:58 INFO]: Training loss at epoch 27: 0.30220627784729004
[08/08/2025 15:53:15 INFO]: New best epoch, val score: -0.6682083026100946
[08/08/2025 15:53:15 INFO]: Saving model to: model_best.pth
[08/08/2025 15:55:20 INFO]: Training loss at epoch 28: 0.3905089646577835
[08/08/2025 15:55:36 INFO]: New best epoch, val score: -0.6638960669238124
[08/08/2025 15:55:36 INFO]: Saving model to: model_best.pth
[08/08/2025 15:57:42 INFO]: Training loss at epoch 29: 0.39358387887477875
[08/08/2025 15:58:29 INFO]: Training stats: {
    "score": -0.5879318203633672,
    "rmse": 0.5879318203633672
}
[08/08/2025 15:58:29 INFO]: Val stats: {
    "score": -0.6903924174629712,
    "rmse": 0.6903924174629712
}
[08/08/2025 15:58:29 INFO]: Test stats: {
    "score": -0.6594879154661599,
    "rmse": 0.6594879154661599
}
[08/08/2025 16:00:50 INFO]: Training loss at epoch 30: 0.35963235795497894
[08/08/2025 16:03:12 INFO]: Training loss at epoch 31: 0.3510413318872452
[08/08/2025 16:03:28 INFO]: New best epoch, val score: -0.6632270087486819
[08/08/2025 16:03:28 INFO]: Saving model to: model_best.pth
[08/08/2025 16:05:33 INFO]: Training loss at epoch 32: 0.35691188275814056
[08/08/2025 16:07:55 INFO]: Training loss at epoch 33: 0.25455442070961
[08/08/2025 16:10:16 INFO]: Training loss at epoch 34: 0.30993373692035675
[08/08/2025 16:12:38 INFO]: Training loss at epoch 35: 0.2825422137975693
[08/08/2025 16:14:59 INFO]: Training loss at epoch 36: 0.3362547606229782
[08/08/2025 16:15:15 INFO]: New best epoch, val score: -0.6552919185456163
[08/08/2025 16:15:15 INFO]: Saving model to: model_best.pth
[08/08/2025 16:17:21 INFO]: Training loss at epoch 37: 0.24592741578817368
[08/08/2025 16:17:37 INFO]: New best epoch, val score: -0.6464083083159752
[08/08/2025 16:17:37 INFO]: Saving model to: model_best.pth
[08/08/2025 16:19:43 INFO]: Training loss at epoch 38: 0.32604047656059265
[08/08/2025 16:22:05 INFO]: Training loss at epoch 39: 0.22652647644281387
[08/08/2025 16:22:51 INFO]: Training stats: {
    "score": -0.5196902561483369,
    "rmse": 0.5196902561483369
}
[08/08/2025 16:22:51 INFO]: Val stats: {
    "score": -0.6549706437058921,
    "rmse": 0.6549706437058921
}
[08/08/2025 16:22:51 INFO]: Test stats: {
    "score": -0.6476560158069911,
    "rmse": 0.6476560158069911
}
[08/08/2025 16:25:13 INFO]: Training loss at epoch 40: 0.27482570707798004
[08/08/2025 16:27:34 INFO]: Training loss at epoch 41: 0.28295744955539703
[08/08/2025 16:27:51 INFO]: New best epoch, val score: -0.6333751212949894
[08/08/2025 16:27:51 INFO]: Saving model to: model_best.pth
[08/08/2025 16:29:57 INFO]: Training loss at epoch 42: 0.21658579260110855
[08/08/2025 16:32:18 INFO]: Training loss at epoch 43: 0.3304370939731598
[08/08/2025 16:32:34 INFO]: New best epoch, val score: -0.6170728373570695
[08/08/2025 16:32:34 INFO]: Saving model to: model_best.pth
[08/08/2025 16:34:40 INFO]: Training loss at epoch 44: 0.23853880167007446
[08/08/2025 16:37:02 INFO]: Training loss at epoch 45: 0.25621306896209717
[08/08/2025 16:39:24 INFO]: Training loss at epoch 46: 0.2846912816166878
[08/08/2025 16:41:46 INFO]: Training loss at epoch 47: 0.2018139362335205
[08/08/2025 16:44:07 INFO]: Training loss at epoch 48: 0.33027032017707825
[08/08/2025 16:46:29 INFO]: Training loss at epoch 49: 0.21326370537281036
[08/08/2025 16:47:15 INFO]: Training stats: {
    "score": -0.559814471377487,
    "rmse": 0.559814471377487
}
[08/08/2025 16:47:15 INFO]: Val stats: {
    "score": -0.7363822489169917,
    "rmse": 0.7363822489169917
}
[08/08/2025 16:47:15 INFO]: Test stats: {
    "score": -0.7484722119314701,
    "rmse": 0.7484722119314701
}
[08/08/2025 16:49:37 INFO]: Training loss at epoch 50: 0.30708451569080353
[08/08/2025 16:51:58 INFO]: Training loss at epoch 51: 0.18783865123987198
[08/08/2025 16:54:20 INFO]: Training loss at epoch 52: 0.20593786984682083
[08/08/2025 16:56:43 INFO]: Training loss at epoch 53: 0.17498165369033813
[08/08/2025 16:59:15 INFO]: Training loss at epoch 54: 0.19723757356405258
[08/08/2025 17:01:37 INFO]: Training loss at epoch 55: 0.24536729604005814
[08/08/2025 17:03:58 INFO]: Training loss at epoch 56: 0.16050398349761963
[08/08/2025 17:06:19 INFO]: Training loss at epoch 57: 0.1675805002450943
[08/08/2025 17:08:41 INFO]: Training loss at epoch 58: 0.1578497365117073
[08/08/2025 17:11:02 INFO]: Training loss at epoch 59: 0.13353203609585762
[08/08/2025 17:11:49 INFO]: Training stats: {
    "score": -0.42995518633436663,
    "rmse": 0.42995518633436663
}
[08/08/2025 17:11:49 INFO]: Val stats: {
    "score": -0.654524805662909,
    "rmse": 0.654524805662909
}
[08/08/2025 17:11:49 INFO]: Test stats: {
    "score": -0.7045557489729859,
    "rmse": 0.7045557489729859
}
[08/08/2025 17:14:11 INFO]: Training loss at epoch 60: 0.21520870923995972
[08/08/2025 17:16:32 INFO]: Training loss at epoch 61: 0.13427437096834183
[08/08/2025 17:18:54 INFO]: Training loss at epoch 62: 0.19436123222112656
[08/08/2025 17:21:15 INFO]: Training loss at epoch 63: 0.15374745428562164
[08/08/2025 17:23:37 INFO]: Training loss at epoch 64: 0.14370104670524597
[08/08/2025 17:25:58 INFO]: Training loss at epoch 65: 0.21362534165382385
[08/08/2025 17:28:20 INFO]: Training loss at epoch 66: 0.14486826211214066
[08/08/2025 17:30:41 INFO]: Training loss at epoch 67: 0.1500670537352562
[08/08/2025 17:33:03 INFO]: Training loss at epoch 68: 0.18493249267339706
[08/08/2025 17:35:24 INFO]: Training loss at epoch 69: 0.14896149933338165
[08/08/2025 17:36:11 INFO]: Training stats: {
    "score": -0.3702608050804325,
    "rmse": 0.3702608050804325
}
[08/08/2025 17:36:11 INFO]: Val stats: {
    "score": -0.6389399658352288,
    "rmse": 0.6389399658352288
}
[08/08/2025 17:36:11 INFO]: Test stats: {
    "score": -0.6603971413933306,
    "rmse": 0.6603971413933306
}
[08/08/2025 17:38:32 INFO]: Training loss at epoch 70: 0.13422901928424835
[08/08/2025 17:40:54 INFO]: Training loss at epoch 71: 0.11987591162323952
[08/08/2025 17:43:16 INFO]: Training loss at epoch 72: 0.1568797156214714
[08/08/2025 17:45:37 INFO]: Training loss at epoch 73: 0.13922439888119698
[08/08/2025 17:47:59 INFO]: Training loss at epoch 74: 0.10269034467637539
[08/08/2025 17:48:15 INFO]: Running Final Evaluation...
[08/08/2025 17:49:27 INFO]: Training accuracy: {
    "score": -0.47696626607236653,
    "rmse": 0.47696626607236653
}
[08/08/2025 17:49:27 INFO]: Val accuracy: {
    "score": -0.6170728373570695,
    "rmse": 0.6170728373570695
}
[08/08/2025 17:49:27 INFO]: Test accuracy: {
    "score": -0.6627257820187586,
    "rmse": 0.6627257820187586
}
[08/08/2025 17:49:27 INFO]: {
    "dataset": "ic_upstream2",
    "model_name": "ft_transformer",
    "run_id": "graveless-Jacquleen",
    "best_epoch": 43,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.6627257820187586,
        "rmse": 0.6627257820187586
    },
    "train_stats": {
        "score": -0.47696626607236653,
        "rmse": 0.47696626607236653
    },
    "val_stats": {
        "score": -0.6170728373570695,
        "rmse": 0.6170728373570695
    }
}
[08/08/2025 17:49:27 INFO]: 
_________________________________________________

[08/08/2025 17:49:27 INFO]: train_net_for_optune.py main() running.
[08/08/2025 17:49:27 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 2.585887953683072
  attention_dropout: 0.006404069835450804
  ffn_dropout: 0.006404069835450804
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00022885773059578416
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: graveless-Jacquleen

[08/08/2025 17:49:27 INFO]: This ft_transformer has 7.786 million parameters.
[08/08/2025 17:49:27 INFO]: Training will start at epoch 0.
[08/08/2025 17:49:27 INFO]: ==> Starting training for 200 epochs...
[08/08/2025 17:51:32 INFO]: Training loss at epoch 0: 1.1418595910072327
[08/08/2025 17:51:48 INFO]: New best epoch, val score: -0.9519822774161522
[08/08/2025 17:51:48 INFO]: Saving model to: model_best.pth
[08/08/2025 17:53:53 INFO]: Training loss at epoch 1: 1.4919657111167908
[08/08/2025 17:54:09 INFO]: New best epoch, val score: -0.8914677203495586
[08/08/2025 17:54:09 INFO]: Saving model to: model_best.pth
[08/08/2025 17:56:13 INFO]: Training loss at epoch 2: 0.9833435118198395
[08/08/2025 17:58:33 INFO]: Training loss at epoch 3: 0.9474726319313049
[08/08/2025 18:00:53 INFO]: Training loss at epoch 4: 0.9169041514396667
[08/08/2025 18:03:12 INFO]: Training loss at epoch 5: 1.0090478360652924
[08/08/2025 18:05:32 INFO]: Training loss at epoch 6: 0.9380384981632233
[08/08/2025 18:07:52 INFO]: Training loss at epoch 7: 1.1446170508861542
[08/08/2025 18:10:11 INFO]: Training loss at epoch 8: 0.7981835901737213
[08/08/2025 18:12:31 INFO]: Training loss at epoch 9: 0.923111230134964
[08/08/2025 18:13:17 INFO]: Training stats: {
    "score": -0.9902110904650236,
    "rmse": 0.9902110904650236
}
[08/08/2025 18:13:17 INFO]: Val stats: {
    "score": -0.8925182713387307,
    "rmse": 0.8925182713387307
}
[08/08/2025 18:13:17 INFO]: Test stats: {
    "score": -0.8950851930067331,
    "rmse": 0.8950851930067331
}
[08/08/2025 18:15:36 INFO]: Training loss at epoch 10: 0.833965390920639
[08/08/2025 18:17:56 INFO]: Training loss at epoch 11: 0.8868851065635681
[08/08/2025 18:20:15 INFO]: Training loss at epoch 12: 1.0643321871757507
[08/08/2025 18:22:34 INFO]: Training loss at epoch 13: 1.2137346863746643
[08/08/2025 18:24:54 INFO]: Training loss at epoch 14: 0.9836467206478119
[08/08/2025 18:27:13 INFO]: Training loss at epoch 15: 0.8228910565376282
[08/08/2025 18:27:29 INFO]: New best epoch, val score: -0.8370361390532759
[08/08/2025 18:27:29 INFO]: Saving model to: model_best.pth
[08/08/2025 18:29:33 INFO]: Training loss at epoch 16: 0.611196294426918
[08/08/2025 18:29:49 INFO]: New best epoch, val score: -0.8266928472391567
[08/08/2025 18:29:49 INFO]: Saving model to: model_best.pth
[08/08/2025 18:31:53 INFO]: Training loss at epoch 17: 0.8382794857025146
[08/08/2025 18:32:09 INFO]: New best epoch, val score: -0.8110335239904903
[08/08/2025 18:32:09 INFO]: Saving model to: model_best.pth
[08/08/2025 18:34:13 INFO]: Training loss at epoch 18: 0.5774708390235901
[08/08/2025 18:34:29 INFO]: New best epoch, val score: -0.8084166536842228
[08/08/2025 18:34:29 INFO]: Saving model to: model_best.pth
[08/08/2025 18:36:33 INFO]: Training loss at epoch 19: 0.617244303226471
[08/08/2025 18:37:18 INFO]: Training stats: {
    "score": -0.801174871212413,
    "rmse": 0.801174871212413
}
[08/08/2025 18:37:18 INFO]: Val stats: {
    "score": -0.8541222619402222,
    "rmse": 0.8541222619402222
}
[08/08/2025 18:37:18 INFO]: Test stats: {
    "score": -0.7925912807442675,
    "rmse": 0.7925912807442675
}
[08/08/2025 18:39:38 INFO]: Training loss at epoch 20: 0.5807820558547974
[08/08/2025 18:41:58 INFO]: Training loss at epoch 21: 0.6802468001842499
[08/08/2025 18:44:18 INFO]: Training loss at epoch 22: 0.6491775214672089
[08/08/2025 18:44:34 INFO]: New best epoch, val score: -0.7802982729189585
[08/08/2025 18:44:34 INFO]: Saving model to: model_best.pth
[08/08/2025 18:46:38 INFO]: Training loss at epoch 23: 0.6861629486083984
[08/08/2025 18:46:55 INFO]: New best epoch, val score: -0.7747506158699666
[08/08/2025 18:46:55 INFO]: Saving model to: model_best.pth
[08/08/2025 18:48:59 INFO]: Training loss at epoch 24: 0.6069890558719635
[08/08/2025 18:51:19 INFO]: Training loss at epoch 25: 0.6045678555965424
[08/08/2025 18:53:39 INFO]: Training loss at epoch 26: 0.5225036442279816
[08/08/2025 18:53:55 INFO]: New best epoch, val score: -0.7160092916069997
[08/08/2025 18:53:55 INFO]: Saving model to: model_best.pth
[08/08/2025 18:55:59 INFO]: Training loss at epoch 27: 0.6059966534376144
[08/08/2025 18:56:15 INFO]: New best epoch, val score: -0.7012060432837537
[08/08/2025 18:56:15 INFO]: Saving model to: model_best.pth
[08/08/2025 18:58:19 INFO]: Training loss at epoch 28: 0.6024592816829681
[08/08/2025 19:00:40 INFO]: Training loss at epoch 29: 0.38972118496894836
[08/08/2025 19:01:26 INFO]: Training stats: {
    "score": -0.7254317105209573,
    "rmse": 0.7254317105209573
}
[08/08/2025 19:01:26 INFO]: Val stats: {
    "score": -0.8305055555185527,
    "rmse": 0.8305055555185527
}
[08/08/2025 19:01:26 INFO]: Test stats: {
    "score": -0.7676979460221824,
    "rmse": 0.7676979460221824
}
[08/08/2025 19:03:45 INFO]: Training loss at epoch 30: 0.5169495940208435
[08/08/2025 19:06:08 INFO]: Training loss at epoch 31: 0.3774883896112442
[08/08/2025 19:06:24 INFO]: New best epoch, val score: -0.6818563275243719
[08/08/2025 19:06:24 INFO]: Saving model to: model_best.pth
[08/08/2025 19:08:28 INFO]: Training loss at epoch 32: 0.48298750817775726
[08/08/2025 19:10:48 INFO]: Training loss at epoch 33: 0.3750823140144348
[08/08/2025 19:13:08 INFO]: Training loss at epoch 34: 0.5150042027235031
[08/08/2025 19:15:27 INFO]: Training loss at epoch 35: 0.5645075440406799
[08/08/2025 19:15:44 INFO]: New best epoch, val score: -0.6737572052581876
[08/08/2025 19:15:44 INFO]: Saving model to: model_best.pth
[08/08/2025 19:17:48 INFO]: Training loss at epoch 36: 0.39763280749320984
[08/08/2025 19:20:08 INFO]: Training loss at epoch 37: 0.39746899902820587
[08/08/2025 19:22:27 INFO]: Training loss at epoch 38: 0.3383154273033142
[08/08/2025 19:24:46 INFO]: Training loss at epoch 39: 0.2851444184780121
[08/08/2025 19:25:32 INFO]: Training stats: {
    "score": -0.61001266146731,
    "rmse": 0.61001266146731
}
[08/08/2025 19:25:32 INFO]: Val stats: {
    "score": -0.674676764612635,
    "rmse": 0.674676764612635
}
[08/08/2025 19:25:32 INFO]: Test stats: {
    "score": -0.6644694104385251,
    "rmse": 0.6644694104385251
}
[08/08/2025 19:27:52 INFO]: Training loss at epoch 40: 0.32768356800079346
[08/08/2025 19:30:11 INFO]: Training loss at epoch 41: 0.40220582485198975
[08/08/2025 19:32:31 INFO]: Training loss at epoch 42: 0.2582285851240158
[08/08/2025 19:34:50 INFO]: Training loss at epoch 43: 0.40841835737228394
[08/08/2025 19:37:10 INFO]: Training loss at epoch 44: 0.35396966338157654
[08/08/2025 19:39:29 INFO]: Training loss at epoch 45: 0.3312589228153229
[08/08/2025 19:39:45 INFO]: New best epoch, val score: -0.663464128060185
[08/08/2025 19:39:45 INFO]: Saving model to: model_best.pth
[08/08/2025 19:41:49 INFO]: Training loss at epoch 46: 0.32887102663517
[08/08/2025 19:44:09 INFO]: Training loss at epoch 47: 0.3733728528022766
[08/08/2025 19:44:25 INFO]: New best epoch, val score: -0.6451960247848744
[08/08/2025 19:44:25 INFO]: Saving model to: model_best.pth
[08/08/2025 19:46:28 INFO]: Training loss at epoch 48: 0.27343128621578217
[08/08/2025 19:48:48 INFO]: Training loss at epoch 49: 0.32332104444503784
[08/08/2025 19:49:34 INFO]: Training stats: {
    "score": -0.5350567470853025,
    "rmse": 0.5350567470853025
}
[08/08/2025 19:49:34 INFO]: Val stats: {
    "score": -0.6487206173683874,
    "rmse": 0.6487206173683874
}
[08/08/2025 19:49:34 INFO]: Test stats: {
    "score": -0.6661165762396641,
    "rmse": 0.6661165762396641
}
[08/08/2025 19:51:53 INFO]: Training loss at epoch 50: 0.3201381266117096
[08/08/2025 19:54:13 INFO]: Training loss at epoch 51: 0.3434058576822281
[08/08/2025 19:54:29 INFO]: New best epoch, val score: -0.6240250351629115
[08/08/2025 19:54:29 INFO]: Saving model to: model_best.pth
[08/08/2025 19:56:33 INFO]: Training loss at epoch 52: 0.34217195212841034
[08/08/2025 19:58:53 INFO]: Training loss at epoch 53: 0.3021840900182724
[08/08/2025 20:01:12 INFO]: Training loss at epoch 54: 0.2911318987607956
[08/08/2025 20:03:32 INFO]: Training loss at epoch 55: 0.2846827059984207
[08/08/2025 20:05:52 INFO]: Training loss at epoch 56: 0.24096327275037766
[08/08/2025 20:08:11 INFO]: Training loss at epoch 57: 0.20663756877183914
[08/08/2025 20:10:31 INFO]: Training loss at epoch 58: 0.27047373354434967
[08/08/2025 20:12:51 INFO]: Training loss at epoch 59: 0.267949141561985
[08/08/2025 20:13:37 INFO]: Training stats: {
    "score": -0.48352037116514635,
    "rmse": 0.48352037116514635
}
[08/08/2025 20:13:37 INFO]: Val stats: {
    "score": -0.6203834732068082,
    "rmse": 0.6203834732068082
}
[08/08/2025 20:13:37 INFO]: Test stats: {
    "score": -0.6374122394637418,
    "rmse": 0.6374122394637418
}
[08/08/2025 20:13:53 INFO]: New best epoch, val score: -0.6203834732068082
[08/08/2025 20:13:53 INFO]: Saving model to: model_best.pth
[08/08/2025 20:15:57 INFO]: Training loss at epoch 60: 0.29578641802072525
[08/08/2025 20:18:16 INFO]: Training loss at epoch 61: 0.19615691900253296
[08/08/2025 20:20:36 INFO]: Training loss at epoch 62: 0.23950447887182236
[08/08/2025 20:22:55 INFO]: Training loss at epoch 63: 0.21811993420124054
[08/08/2025 20:25:15 INFO]: Training loss at epoch 64: 0.20059197396039963
[08/08/2025 20:27:35 INFO]: Training loss at epoch 65: 0.1998031958937645
[08/08/2025 20:29:54 INFO]: Training loss at epoch 66: 0.18075786530971527
[08/08/2025 20:32:14 INFO]: Training loss at epoch 67: 0.2072095051407814
[08/08/2025 20:34:34 INFO]: Training loss at epoch 68: 0.22506918758153915
[08/08/2025 20:36:53 INFO]: Training loss at epoch 69: 0.2064557895064354
[08/08/2025 20:37:39 INFO]: Training stats: {
    "score": -0.44259158473529037,
    "rmse": 0.44259158473529037
}
[08/08/2025 20:37:39 INFO]: Val stats: {
    "score": -0.6441524917240381,
    "rmse": 0.6441524917240381
}
[08/08/2025 20:37:39 INFO]: Test stats: {
    "score": -0.6436250572039625,
    "rmse": 0.6436250572039625
}
[08/08/2025 20:39:59 INFO]: Training loss at epoch 70: 0.18482403457164764
[08/08/2025 20:42:18 INFO]: Training loss at epoch 71: 0.166863813996315
[08/08/2025 20:44:38 INFO]: Training loss at epoch 72: 0.16511695086956024
[08/08/2025 20:46:57 INFO]: Training loss at epoch 73: 0.15424097701907158
[08/08/2025 20:49:17 INFO]: Training loss at epoch 74: 0.1506315842270851
[08/08/2025 20:51:36 INFO]: Training loss at epoch 75: 0.17081793397665024
[08/08/2025 20:53:56 INFO]: Training loss at epoch 76: 0.184230275452137
[08/08/2025 20:56:16 INFO]: Training loss at epoch 77: 0.15477656573057175
[08/12/2025 16:31:35 INFO]: Building Dataset
[08/12/2025 16:31:35 INFO]: pre normalizer.fit

[08/12/2025 16:31:35 INFO]: pos normalizer.fit

[08/12/2025 16:31:37 INFO]: Task: regression, Dataset: ic_upstream2, n_numerical: 100, n_categorical: 0, n_classes: 1, n_train_samples: 282, n_val_samples: 66, n_test_samples: 87
[08/12/2025 16:31:50 INFO]: 
_________________________________________________

[08/12/2025 16:31:50 INFO]: 
_________________________________________________

[08/12/2025 16:31:50 INFO]: 
_________________________________________________

[08/12/2025 16:31:50 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:31:50 INFO]: 
_________________________________________________

[08/12/2025 16:31:50 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:31:50 INFO]: 
_________________________________________________

[08/12/2025 16:31:50 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:31:50 INFO]: 
_________________________________________________

[08/12/2025 16:31:51 INFO]: 
_________________________________________________

[08/12/2025 16:31:51 INFO]: 
_________________________________________________

[08/12/2025 16:31:51 INFO]: 
_________________________________________________

[08/12/2025 16:31:51 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:31:51 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:31:51 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 0.9245243684477294
  attention_dropout: 0.0862053007683779
  ffn_dropout: 0.0862053007683779
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.3789717413129504e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unforced-Rulon

[08/12/2025 16:31:51 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 1.3319819948952922
  attention_dropout: 0.010013845654746112
  ffn_dropout: 0.010013845654746112
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0002567752539752897
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: -1
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unforced-Rulon

[08/12/2025 16:31:51 INFO]: 
_________________________________________________

[08/12/2025 16:31:51 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:31:51 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:31:51 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 1.4385528778624406
  attention_dropout: 0.36743552563799553
  ffn_dropout: 0.36743552563799553
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00015248333260109957
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unforced-Rulon

[08/12/2025 16:31:51 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:31:51 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:31:51 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 2.1982622771641522
  attention_dropout: 0.3159488687169244
  ffn_dropout: 0.3159488687169244
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00012893339173688175
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unforced-Rulon

[08/12/2025 16:31:51 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 2.1268114214699194
  attention_dropout: 0.04534269052100304
  ffn_dropout: 0.04534269052100304
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0008432705961638474
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unforced-Rulon

[08/12/2025 16:31:51 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:31:51 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 2.550511424520133
  attention_dropout: 0.382955390962517
  ffn_dropout: 0.382955390962517
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00019323685933336145
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unforced-Rulon

[08/12/2025 16:31:51 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 12
  d_ffn_factor: 0.7515143465354147
  attention_dropout: 0.027179623953505427
  ffn_dropout: 0.027179623953505427
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.000279333299572177
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unforced-Rulon

[08/12/2025 16:31:51 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 4
  d_ffn_factor: 1.4603021723163128
  attention_dropout: 0.38836098754651377
  ffn_dropout: 0.38836098754651377
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.2863641441087446e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unforced-Rulon

[08/12/2025 16:31:51 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 1.4720777899620607
  attention_dropout: 0.1468524134062773
  ffn_dropout: 0.1468524134062773
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0002636210080291049
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unforced-Rulon

[08/12/2025 16:31:51 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 0.697669239482776
  attention_dropout: 0.04764511346879208
  ffn_dropout: 0.04764511346879208
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0003613241327333436
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unforced-Rulon

[08/12/2025 16:31:51 INFO]: This ft_transformer has 0.963 million parameters.
[08/12/2025 16:31:51 INFO]: Training will start at epoch 0.
[08/12/2025 16:31:52 INFO]: This ft_transformer has 0.127 million parameters.
[08/12/2025 16:31:52 INFO]: Training will start at epoch 0.
[08/12/2025 16:31:52 INFO]: This ft_transformer has 4.149 million parameters.
[08/12/2025 16:31:52 INFO]: Training will start at epoch 0.
[08/12/2025 16:31:52 INFO]: This ft_transformer has 3.807 million parameters.
[08/12/2025 16:31:52 INFO]: Training will start at epoch 0.
[08/12/2025 16:31:52 INFO]: This ft_transformer has 4.966 million parameters.
[08/12/2025 16:31:52 INFO]: Training will start at epoch 0.
[08/12/2025 16:31:52 INFO]: This ft_transformer has 4.508 million parameters.
[08/12/2025 16:31:52 INFO]: Training will start at epoch 0.
[08/12/2025 16:31:53 INFO]: This ft_transformer has 14.113 million parameters.
[08/12/2025 16:31:53 INFO]: Training will start at epoch 0.
[08/12/2025 16:31:53 INFO]: This ft_transformer has 0.365 million parameters.
[08/12/2025 16:31:53 INFO]: Training will start at epoch 0.
[08/12/2025 16:31:53 INFO]: This ft_transformer has 5.487 million parameters.
[08/12/2025 16:31:53 INFO]: Training will start at epoch 0.
[08/12/2025 16:31:53 INFO]: This ft_transformer has 5.579 million parameters.
[08/12/2025 16:31:53 INFO]: Training will start at epoch 0.
[08/12/2025 16:32:07 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:32:07 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:32:07 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:32:07 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:32:07 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:32:07 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:32:07 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:32:07 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:32:07 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:32:07 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:32:18 INFO]: Training loss at epoch 0: 1.180643916130066
[08/12/2025 16:32:20 INFO]: Training loss at epoch 0: 1.0728800296783447
[08/12/2025 16:32:20 INFO]: New best epoch, val score: -0.9389163099350574
[08/12/2025 16:32:20 INFO]: Saving model to: model_best.pth
[08/12/2025 16:32:21 INFO]: New best epoch, val score: -0.9374619594700461
[08/12/2025 16:32:21 INFO]: Saving model to: model_best.pth
[08/12/2025 16:32:30 INFO]: Training loss at epoch 1: 1.2016328573226929
[08/12/2025 16:32:31 INFO]: New best epoch, val score: -0.9374567907681488
[08/12/2025 16:32:32 INFO]: Saving model to: model_best.pth
[08/12/2025 16:32:32 INFO]: Training loss at epoch 1: 0.8891406953334808
[08/12/2025 16:32:34 INFO]: New best epoch, val score: -0.9350101885276062
[08/12/2025 16:32:34 INFO]: Saving model to: model_best.pth
[08/12/2025 16:32:39 INFO]: Training loss at epoch 0: 1.1393577456474304
[08/12/2025 16:32:41 INFO]: Training loss at epoch 2: 1.035634458065033
[08/12/2025 16:32:43 INFO]: New best epoch, val score: -1.1378318945374177
[08/12/2025 16:32:43 INFO]: Saving model to: model_best.pth
[08/12/2025 16:32:45 INFO]: Training loss at epoch 2: 1.2146368026733398
[08/12/2025 16:32:46 INFO]: New best epoch, val score: -0.9337627030132329
[08/12/2025 16:32:46 INFO]: Saving model to: model_best.pth
[08/12/2025 16:32:52 INFO]: Training loss at epoch 3: 0.9682084023952484
[08/12/2025 16:32:57 INFO]: Training loss at epoch 3: 0.8216455578804016
[08/12/2025 16:33:01 INFO]: Training loss at epoch 0: 1.2268367409706116
[08/12/2025 16:33:03 INFO]: Training loss at epoch 4: 0.9321616291999817
[08/12/2025 16:33:06 INFO]: Training loss at epoch 0: 0.9859527349472046
[08/12/2025 16:33:08 INFO]: New best epoch, val score: -1.085672294155995
[08/12/2025 16:33:08 INFO]: Saving model to: model_best.pth
[08/12/2025 16:33:10 INFO]: Training loss at epoch 4: 0.9318452179431915
[08/12/2025 16:33:13 INFO]: New best epoch, val score: -0.955971517914976
[08/12/2025 16:33:13 INFO]: Saving model to: model_best.pth
[08/12/2025 16:33:14 INFO]: Training loss at epoch 1: 1.06974196434021
[08/12/2025 16:33:15 INFO]: Training loss at epoch 5: 0.9781631827354431
[08/12/2025 16:33:18 INFO]: New best epoch, val score: -0.9332575031436386
[08/12/2025 16:33:18 INFO]: Saving model to: model_best.pth
[08/12/2025 16:33:22 INFO]: Training loss at epoch 5: 1.0906398296356201
[08/12/2025 16:33:26 INFO]: Training loss at epoch 6: 1.0530492663383484
[08/12/2025 16:33:30 INFO]: Training loss at epoch 0: 0.867180347442627
[08/12/2025 16:33:32 INFO]: Training loss at epoch 0: 0.9022911190986633
[08/12/2025 16:33:34 INFO]: Training loss at epoch 6: 1.1776219010353088
[08/12/2025 16:33:37 INFO]: Training loss at epoch 7: 1.0720319151878357
[08/12/2025 16:33:41 INFO]: Training loss at epoch 0: 2.8198312520980835
[08/12/2025 16:33:41 INFO]: New best epoch, val score: -0.9759390098369645
[08/12/2025 16:33:41 INFO]: Saving model to: model_best.pth
[08/12/2025 16:33:43 INFO]: New best epoch, val score: -0.966242756524977
[08/12/2025 16:33:43 INFO]: Saving model to: model_best.pth
[08/12/2025 16:33:47 INFO]: Training loss at epoch 7: 0.9256742000579834
[08/12/2025 16:33:47 INFO]: Training loss at epoch 8: 0.8138473927974701
[08/12/2025 16:33:48 INFO]: Training loss at epoch 2: 1.2741121649742126
[08/12/2025 16:33:53 INFO]: Training loss at epoch 0: 1.2346105575561523
[08/12/2025 16:33:54 INFO]: New best epoch, val score: -1.7990767596979589
[08/12/2025 16:33:54 INFO]: Saving model to: model_best.pth
[08/12/2025 16:33:59 INFO]: Training loss at epoch 9: 0.9086759090423584
[08/12/2025 16:33:59 INFO]: Training loss at epoch 8: 1.0883243680000305
[08/12/2025 16:34:00 INFO]: Training loss at epoch 1: 1.6278466582298279
[08/12/2025 16:34:03 INFO]: Training stats: {
    "score": -0.9945640826252798,
    "rmse": 0.9945640826252798
}
[08/12/2025 16:34:03 INFO]: Val stats: {
    "score": -0.9621424954112713,
    "rmse": 0.9621424954112713
}
[08/12/2025 16:34:03 INFO]: Test stats: {
    "score": -0.923494770607595,
    "rmse": 0.923494770607595
}
[08/12/2025 16:34:07 INFO]: New best epoch, val score: -0.9852573376778101
[08/12/2025 16:34:07 INFO]: Saving model to: model_best.pth
[08/12/2025 16:34:11 INFO]: Training loss at epoch 1: 1.0602419972419739
[08/12/2025 16:34:11 INFO]: Training loss at epoch 9: 1.213792860507965
[08/12/2025 16:34:14 INFO]: Training loss at epoch 10: 0.8966576457023621
[08/12/2025 16:34:16 INFO]: Training stats: {
    "score": -0.9986493910921717,
    "rmse": 0.9986493910921717
}
[08/12/2025 16:34:16 INFO]: Val stats: {
    "score": -0.936939209411263,
    "rmse": 0.936939209411263
}
[08/12/2025 16:34:16 INFO]: Test stats: {
    "score": -0.9159704332083799,
    "rmse": 0.9159704332083799
}
[08/12/2025 16:34:22 INFO]: Training loss at epoch 3: 1.1889048218727112
[08/12/2025 16:34:25 INFO]: Training loss at epoch 11: 0.8920745253562927
[08/12/2025 16:34:26 INFO]: New best epoch, val score: -0.9284399372181775
[08/12/2025 16:34:26 INFO]: Saving model to: model_best.pth
[08/12/2025 16:34:28 INFO]: Training loss at epoch 10: 1.132135808467865
[08/12/2025 16:34:36 INFO]: Training loss at epoch 12: 0.9053911864757538
[08/12/2025 16:34:40 INFO]: Training loss at epoch 11: 1.0319172739982605
[08/12/2025 16:34:47 INFO]: Training loss at epoch 13: 0.8728611767292023
[08/12/2025 16:34:52 INFO]: Training loss at epoch 12: 0.9291626811027527
[08/12/2025 16:34:56 INFO]: Training loss at epoch 4: 0.9503957331180573
[08/12/2025 16:34:58 INFO]: Training loss at epoch 14: 1.0784147679805756
[08/12/2025 16:35:00 INFO]: Training loss at epoch 2: 1.8511688709259033
[08/12/2025 16:35:04 INFO]: Training loss at epoch 1: 3.8685359358787537
[08/12/2025 16:35:05 INFO]: Training loss at epoch 13: 1.1923288702964783
[08/12/2025 16:35:07 INFO]: New best epoch, val score: -0.9604466970994309
[08/12/2025 16:35:07 INFO]: Saving model to: model_best.pth
[08/12/2025 16:35:08 INFO]: Training loss at epoch 1: 0.9433024227619171
[08/12/2025 16:35:09 INFO]: Training loss at epoch 15: 1.06547811627388
[08/12/2025 16:35:16 INFO]: Training loss at epoch 2: 1.1352795362472534
[08/12/2025 16:35:17 INFO]: Training loss at epoch 14: 1.1828306019306183
[08/12/2025 16:35:20 INFO]: Training loss at epoch 16: 0.9354137778282166
[08/12/2025 16:35:26 INFO]: Training loss at epoch 0: 1.234076738357544
[08/12/2025 16:35:27 INFO]: Training loss at epoch 1: 3.5241193771362305
[08/12/2025 16:35:30 INFO]: Training loss at epoch 15: 0.8995251953601837
[08/12/2025 16:35:30 INFO]: Training loss at epoch 5: 0.9698834419250488
[08/12/2025 16:35:31 INFO]: Training loss at epoch 17: 1.0648885369300842
[08/12/2025 16:35:39 INFO]: New best epoch, val score: -0.942972899721497
[08/12/2025 16:35:39 INFO]: Saving model to: model_best.pth
[08/12/2025 16:35:42 INFO]: Training loss at epoch 18: 0.8989636898040771
[08/12/2025 16:35:42 INFO]: Training loss at epoch 16: 0.8503285348415375
[08/12/2025 16:35:52 INFO]: Training loss at epoch 1: 1.936318814754486
[08/12/2025 16:35:53 INFO]: Training loss at epoch 19: 1.12895929813385
[08/12/2025 16:35:53 INFO]: New best epoch, val score: -0.9703455328225115
[08/12/2025 16:35:53 INFO]: Saving model to: model_best.pth
[08/12/2025 16:35:55 INFO]: Training loss at epoch 17: 1.1197651028633118
[08/12/2025 16:35:58 INFO]: Training stats: {
    "score": -0.990011828641507,
    "rmse": 0.990011828641507
}
[08/12/2025 16:35:58 INFO]: Val stats: {
    "score": -0.9517284036415297,
    "rmse": 0.9517284036415297
}
[08/12/2025 16:35:58 INFO]: Test stats: {
    "score": -0.9167179444103061,
    "rmse": 0.9167179444103061
}
[08/12/2025 16:36:01 INFO]: Training loss at epoch 3: 1.0796645283699036
[08/12/2025 16:36:04 INFO]: Training loss at epoch 6: 0.9431798458099365
[08/12/2025 16:36:08 INFO]: Training loss at epoch 18: 0.923751950263977
[08/12/2025 16:36:09 INFO]: Training loss at epoch 20: 0.9473313391208649
[08/12/2025 16:36:20 INFO]: Training loss at epoch 19: 1.1745165288448334
[08/12/2025 16:36:20 INFO]: Training loss at epoch 21: 1.0361360311508179
[08/12/2025 16:36:22 INFO]: Training loss at epoch 3: 1.179242193698883
[08/12/2025 16:36:24 INFO]: Training stats: {
    "score": -0.995042404463138,
    "rmse": 0.995042404463138
}
[08/12/2025 16:36:24 INFO]: Val stats: {
    "score": -0.9344772756501675,
    "rmse": 0.9344772756501675
}
[08/12/2025 16:36:24 INFO]: Test stats: {
    "score": -0.9132609817355993,
    "rmse": 0.9132609817355993
}
[08/12/2025 16:36:32 INFO]: Training loss at epoch 22: 0.9509122669696808
[08/12/2025 16:36:36 INFO]: Training loss at epoch 20: 1.3124882280826569
[08/12/2025 16:36:38 INFO]: Training loss at epoch 2: 1.6104243397712708
[08/12/2025 16:36:38 INFO]: Training loss at epoch 7: 0.9874787032604218
[08/12/2025 16:36:43 INFO]: Training loss at epoch 23: 0.8532167971134186
[08/12/2025 16:36:43 INFO]: Training loss at epoch 2: 2.18938809633255
[08/12/2025 16:36:49 INFO]: Training loss at epoch 21: 0.8956553041934967
[08/12/2025 16:36:50 INFO]: New best epoch, val score: -0.9325812069091695
[08/12/2025 16:36:50 INFO]: Saving model to: model_best.pth
[08/12/2025 16:36:54 INFO]: Training loss at epoch 24: 1.0015000700950623
[08/12/2025 16:36:55 INFO]: New best epoch, val score: -0.9000251277185987
[08/12/2025 16:36:55 INFO]: Saving model to: model_best.pth
[08/12/2025 16:37:00 INFO]: Training loss at epoch 4: 1.8404417037963867
[08/12/2025 16:37:01 INFO]: Training loss at epoch 22: 1.3028314709663391
[08/12/2025 16:37:03 INFO]: New best epoch, val score: -0.9315651335280069
[08/12/2025 16:37:03 INFO]: Saving model to: model_best.pth
[08/12/2025 16:37:05 INFO]: Training loss at epoch 25: 0.8258151113986969
[08/12/2025 16:37:07 INFO]: New best epoch, val score: -0.9369833428101185
[08/12/2025 16:37:07 INFO]: Saving model to: model_best.pth
[08/12/2025 16:37:12 INFO]: Training loss at epoch 8: 0.9396492838859558
[08/12/2025 16:37:13 INFO]: Training loss at epoch 2: 1.2471871972084045
[08/12/2025 16:37:14 INFO]: Training loss at epoch 23: 0.901999443769455
[08/12/2025 16:37:16 INFO]: New best epoch, val score: -0.9313747410388024
[08/12/2025 16:37:16 INFO]: Saving model to: model_best.pth
[08/12/2025 16:37:16 INFO]: Training loss at epoch 26: 0.8739686608314514
[08/12/2025 16:37:27 INFO]: Training loss at epoch 24: 1.0478023290634155
[08/12/2025 16:37:27 INFO]: Training loss at epoch 4: 1.0782317221164703
[08/12/2025 16:37:27 INFO]: Training loss at epoch 27: 0.961528480052948
[08/12/2025 16:37:28 INFO]: New best epoch, val score: -0.9311481830363442
[08/12/2025 16:37:28 INFO]: Saving model to: model_best.pth
[08/12/2025 16:37:38 INFO]: Training loss at epoch 28: 0.8554625511169434
[08/12/2025 16:37:39 INFO]: Training loss at epoch 25: 1.246613085269928
[08/12/2025 16:37:41 INFO]: New best epoch, val score: -0.9310061008961271
[08/12/2025 16:37:41 INFO]: Saving model to: model_best.pth
[08/12/2025 16:37:46 INFO]: Training loss at epoch 9: 1.1420117616653442
[08/12/2025 16:37:49 INFO]: Training loss at epoch 29: 1.1382985413074493
[08/12/2025 16:37:51 INFO]: Training loss at epoch 2: 1.3020607829093933
[08/12/2025 16:37:52 INFO]: Training loss at epoch 26: 0.7502856552600861
[08/12/2025 16:37:53 INFO]: New best epoch, val score: -0.9308094478704818
[08/12/2025 16:37:53 INFO]: Saving model to: model_best.pth
[08/12/2025 16:37:53 INFO]: Training stats: {
    "score": -0.9855257154642523,
    "rmse": 0.9855257154642523
}
[08/12/2025 16:37:53 INFO]: Val stats: {
    "score": -0.9459958854203175,
    "rmse": 0.9459958854203175
}
[08/12/2025 16:37:53 INFO]: Test stats: {
    "score": -0.9122284946263394,
    "rmse": 0.9122284946263394
}
[08/12/2025 16:37:58 INFO]: Training stats: {
    "score": -0.9972700825512482,
    "rmse": 0.9972700825512482
}
[08/12/2025 16:37:58 INFO]: Val stats: {
    "score": -0.9809678580607425,
    "rmse": 0.9809678580607425
}
[08/12/2025 16:37:58 INFO]: Test stats: {
    "score": -0.9396070423258822,
    "rmse": 0.9396070423258822
}
[08/12/2025 16:38:00 INFO]: Training loss at epoch 5: 0.938549131155014
[08/12/2025 16:38:04 INFO]: Training loss at epoch 27: 1.0889694094657898
[08/12/2025 16:38:04 INFO]: Training loss at epoch 30: 1.00167316198349
[08/12/2025 16:38:05 INFO]: New best epoch, val score: -0.9499502518891141
[08/12/2025 16:38:05 INFO]: Saving model to: model_best.pth
[08/12/2025 16:38:11 INFO]: Training loss at epoch 3: 1.2610650658607483
[08/12/2025 16:38:15 INFO]: Training loss at epoch 31: 0.8871491551399231
[08/12/2025 16:38:17 INFO]: Training loss at epoch 28: 1.1539035439491272
[08/12/2025 16:38:18 INFO]: Training loss at epoch 3: 1.455947995185852
[08/12/2025 16:38:22 INFO]: New best epoch, val score: -0.9285390129464018
[08/12/2025 16:38:22 INFO]: Saving model to: model_best.pth
[08/12/2025 16:38:26 INFO]: Training loss at epoch 32: 1.2105601131916046
[08/12/2025 16:38:28 INFO]: Running Final Evaluation...
[08/12/2025 16:38:29 INFO]: Training loss at epoch 29: 0.9908465445041656
[08/12/2025 16:38:31 INFO]: Training loss at epoch 5: 1.4002474546432495
[08/12/2025 16:38:32 INFO]: Training loss at epoch 10: 0.8995456993579865
[08/12/2025 16:38:33 INFO]: Training stats: {
    "score": -0.9934635568029917,
    "rmse": 0.9934635568029917
}
[08/12/2025 16:38:33 INFO]: Val stats: {
    "score": -0.9320165982477427,
    "rmse": 0.9320165982477427
}
[08/12/2025 16:38:33 INFO]: Test stats: {
    "score": -0.9113934421162109,
    "rmse": 0.9113934421162109
}
[08/12/2025 16:38:45 INFO]: Training loss at epoch 30: 1.125281810760498
[08/12/2025 16:38:57 INFO]: Training loss at epoch 3: 1.1819691061973572
[08/12/2025 16:38:57 INFO]: Training loss at epoch 31: 1.0843186676502228
[08/12/2025 16:39:00 INFO]: Training loss at epoch 6: 0.928886741399765
[08/12/2025 16:39:05 INFO]: Training loss at epoch 11: 0.9323844909667969
[08/12/2025 16:39:09 INFO]: Training loss at epoch 32: 1.2716382443904877
[08/12/2025 16:39:09 INFO]: New best epoch, val score: -0.9340638709084087
[08/12/2025 16:39:09 INFO]: Saving model to: model_best.pth
[08/12/2025 16:39:12 INFO]: Training loss at epoch 1: 0.8930759131908417
[08/12/2025 16:39:21 INFO]: Training loss at epoch 33: 1.140950858592987
[08/12/2025 16:39:33 INFO]: Training loss at epoch 34: 0.9055372178554535
[08/12/2025 16:39:36 INFO]: Training loss at epoch 6: 1.0308598577976227
[08/12/2025 16:39:38 INFO]: Training loss at epoch 12: 1.0129494071006775
[08/12/2025 16:39:41 INFO]: Training loss at epoch 4: 1.150870144367218
[08/12/2025 16:39:42 INFO]: New best epoch, val score: -0.9244754731385392
[08/12/2025 16:39:42 INFO]: Saving model to: model_best.pth
[08/12/2025 16:39:44 INFO]: Training loss at epoch 35: 0.9542210102081299
[08/12/2025 16:39:46 INFO]: New best epoch, val score: -0.9300562265659198
[08/12/2025 16:39:46 INFO]: Saving model to: model_best.pth
[08/12/2025 16:39:47 INFO]: Training loss at epoch 3: 1.1693155765533447
[08/12/2025 16:39:53 INFO]: Training loss at epoch 4: 1.3630435466766357
[08/12/2025 16:39:57 INFO]: Training loss at epoch 36: 1.0315334498882294
[08/12/2025 16:39:58 INFO]: New best epoch, val score: -0.9290288081429163
[08/12/2025 16:39:58 INFO]: Saving model to: model_best.pth
[08/12/2025 16:39:59 INFO]: Training loss at epoch 7: 1.0681812167167664
[08/12/2025 16:40:00 INFO]: New best epoch, val score: -0.9255445208463362
[08/12/2025 16:40:00 INFO]: Saving model to: model_best.pth
[08/12/2025 16:40:09 INFO]: Training loss at epoch 37: 1.0497718751430511
[08/12/2025 16:40:11 INFO]: New best epoch, val score: -0.9286301633502166
[08/12/2025 16:40:11 INFO]: Saving model to: model_best.pth
[08/12/2025 16:40:11 INFO]: Training loss at epoch 13: 0.9854899346828461
[08/12/2025 16:40:15 INFO]: New best epoch, val score: -0.9164464963355458
[08/12/2025 16:40:15 INFO]: Saving model to: model_best.pth
[08/12/2025 16:40:21 INFO]: Training loss at epoch 38: 1.2470993399620056
[08/12/2025 16:40:23 INFO]: New best epoch, val score: -0.9281648168462354
[08/12/2025 16:40:23 INFO]: Saving model to: model_best.pth
[08/12/2025 16:40:33 INFO]: Training loss at epoch 39: 1.040501892566681
[08/12/2025 16:40:38 INFO]: Training stats: {
    "score": -0.9926432578032678,
    "rmse": 0.9926432578032678
}
[08/12/2025 16:40:38 INFO]: Val stats: {
    "score": -0.9280651448162522,
    "rmse": 0.9280651448162522
}
[08/12/2025 16:40:38 INFO]: Test stats: {
    "score": -0.9091706991150332,
    "rmse": 0.9091706991150332
}
[08/12/2025 16:40:39 INFO]: New best epoch, val score: -0.9280651448162522
[08/12/2025 16:40:39 INFO]: Saving model to: model_best.pth
[08/12/2025 16:40:41 INFO]: Training loss at epoch 4: 1.0531439781188965
[08/12/2025 16:40:41 INFO]: Training loss at epoch 7: 1.2130756378173828
[08/12/2025 16:40:44 INFO]: Training loss at epoch 14: 1.3897766768932343
[08/12/2025 16:40:48 INFO]: New best epoch, val score: -0.9133392650218543
[08/12/2025 16:40:48 INFO]: Saving model to: model_best.pth
[08/12/2025 16:40:50 INFO]: Training loss at epoch 40: 1.1093437671661377
[08/12/2025 16:40:58 INFO]: Training loss at epoch 8: 0.9825313985347748
[08/12/2025 16:41:02 INFO]: Training loss at epoch 41: 1.195924460887909
[08/12/2025 16:41:13 INFO]: Training loss at epoch 5: 1.137108862400055
[08/12/2025 16:41:14 INFO]: Training loss at epoch 42: 0.9521902203559875
[08/12/2025 16:41:18 INFO]: Training loss at epoch 15: 0.9851058423519135
[08/12/2025 16:41:26 INFO]: Training loss at epoch 5: 1.1372613310813904
[08/12/2025 16:41:27 INFO]: Training loss at epoch 43: 1.0600959062576294
[08/12/2025 16:41:39 INFO]: Training loss at epoch 44: 1.140305519104004
[08/12/2025 16:41:44 INFO]: Training loss at epoch 4: 1.0553428530693054
[08/12/2025 16:41:47 INFO]: Training loss at epoch 8: 1.2627277970314026
[08/12/2025 16:41:51 INFO]: Training loss at epoch 45: 1.0896867513656616
[08/12/2025 16:41:51 INFO]: Training loss at epoch 16: 0.9284175038337708
[08/12/2025 16:41:56 INFO]: Training loss at epoch 9: 0.9454784095287323
[08/12/2025 16:42:03 INFO]: Training loss at epoch 46: 0.9588701128959656
[08/12/2025 16:42:15 INFO]: Training stats: {
    "score": -0.9943288145895242,
    "rmse": 0.9943288145895242
}
[08/12/2025 16:42:15 INFO]: Val stats: {
    "score": -0.9486141817852948,
    "rmse": 0.9486141817852948
}
[08/12/2025 16:42:15 INFO]: Test stats: {
    "score": -0.9204811366630199,
    "rmse": 0.9204811366630199
}
[08/12/2025 16:42:15 INFO]: Training loss at epoch 47: 0.8357490301132202
[08/12/2025 16:42:22 INFO]: Training loss at epoch 5: 1.0445640087127686
[08/12/2025 16:42:25 INFO]: Training loss at epoch 17: 1.0184892416000366
[08/12/2025 16:42:28 INFO]: Training loss at epoch 48: 1.0623002648353577
[08/12/2025 16:42:40 INFO]: Training loss at epoch 49: 1.1657278537750244
[08/12/2025 16:42:44 INFO]: Training stats: {
    "score": -0.9846149952098516,
    "rmse": 0.9846149952098516
}
[08/12/2025 16:42:44 INFO]: Val stats: {
    "score": -0.9329733881390274,
    "rmse": 0.9329733881390274
}
[08/12/2025 16:42:44 INFO]: Test stats: {
    "score": -0.9092672172788785,
    "rmse": 0.9092672172788785
}
[08/12/2025 16:42:45 INFO]: Training loss at epoch 6: 1.0046386420726776
[08/12/2025 16:42:51 INFO]: Training loss at epoch 2: 3.932436943054199
[08/12/2025 16:42:52 INFO]: Training loss at epoch 9: 0.9976358413696289
[08/12/2025 16:42:56 INFO]: Training loss at epoch 50: 0.9156706929206848
[08/12/2025 16:42:58 INFO]: Training loss at epoch 6: 0.8097520470619202
[08/12/2025 16:42:58 INFO]: Training loss at epoch 18: 0.8867502212524414
[08/12/2025 16:43:09 INFO]: Training loss at epoch 51: 1.1222988963127136
[08/12/2025 16:43:13 INFO]: Training loss at epoch 10: 0.8648311197757721
[08/12/2025 16:43:14 INFO]: Training stats: {
    "score": -1.093307512174377,
    "rmse": 1.093307512174377
}
[08/12/2025 16:43:14 INFO]: Val stats: {
    "score": -1.1598537365408348,
    "rmse": 1.1598537365408348
}
[08/12/2025 16:43:14 INFO]: Test stats: {
    "score": -1.0801907208082144,
    "rmse": 1.0801907208082144
}
[08/12/2025 16:43:21 INFO]: Training loss at epoch 52: 0.9980883300304413
[08/12/2025 16:43:32 INFO]: Training loss at epoch 19: 1.0519617795944214
[08/12/2025 16:43:33 INFO]: Training loss at epoch 53: 0.9795740842819214
[08/12/2025 16:43:41 INFO]: Training loss at epoch 5: 1.1038730144500732
[08/12/2025 16:43:44 INFO]: Training stats: {
    "score": -0.9564486902652638,
    "rmse": 0.9564486902652638
}
[08/12/2025 16:43:44 INFO]: Val stats: {
    "score": -0.9343117489746491,
    "rmse": 0.9343117489746491
}
[08/12/2025 16:43:44 INFO]: Test stats: {
    "score": -0.8985916330772935,
    "rmse": 0.8985916330772935
}
[08/12/2025 16:43:45 INFO]: Training loss at epoch 54: 1.103468894958496
[08/12/2025 16:43:58 INFO]: Training loss at epoch 55: 1.2477872669696808
[08/12/2025 16:44:03 INFO]: Training loss at epoch 6: 1.0833989977836609
[08/12/2025 16:44:10 INFO]: Training loss at epoch 56: 1.033439815044403
[08/12/2025 16:44:11 INFO]: Training loss at epoch 11: 1.1381380259990692
[08/12/2025 16:44:17 INFO]: Training loss at epoch 20: 0.8939667642116547
[08/12/2025 16:44:17 INFO]: Training loss at epoch 7: 1.065951406955719
[08/12/2025 16:44:19 INFO]: Training loss at epoch 10: 1.0629351735115051
[08/12/2025 16:44:23 INFO]: Training loss at epoch 57: 1.0482216477394104
[08/12/2025 16:44:26 INFO]: New best epoch, val score: -0.8991635232722774
[08/12/2025 16:44:26 INFO]: Saving model to: model_best.pth
[08/12/2025 16:44:30 INFO]: Training loss at epoch 7: 1.245571494102478
[08/12/2025 16:44:35 INFO]: Training loss at epoch 58: 1.068309247493744
[08/12/2025 16:44:47 INFO]: Training loss at epoch 59: 1.0925562977790833
[08/12/2025 16:44:51 INFO]: Training loss at epoch 21: 0.8608110845088959
[08/12/2025 16:44:51 INFO]: Training stats: {
    "score": -0.9801580473028444,
    "rmse": 0.9801580473028444
}
[08/12/2025 16:44:51 INFO]: Val stats: {
    "score": -0.9422945537232078,
    "rmse": 0.9422945537232078
}
[08/12/2025 16:44:51 INFO]: Test stats: {
    "score": -0.9135078859145519,
    "rmse": 0.9135078859145519
}
[08/12/2025 16:45:03 INFO]: Training loss at epoch 60: 1.0672058463096619
[08/12/2025 16:45:09 INFO]: Training loss at epoch 12: 1.1452054381370544
[08/12/2025 16:45:16 INFO]: Training loss at epoch 61: 1.0057713687419891
[08/12/2025 16:45:24 INFO]: Training loss at epoch 11: 1.4722163081169128
[08/12/2025 16:45:24 INFO]: Training loss at epoch 22: 0.9270166754722595
[08/12/2025 16:45:28 INFO]: Training loss at epoch 62: 1.1680352687835693
[08/12/2025 16:45:28 INFO]: New best epoch, val score: -0.9122594650663608
[08/12/2025 16:45:28 INFO]: Saving model to: model_best.pth
[08/12/2025 16:45:30 INFO]: New best epoch, val score: -0.9271864492679208
[08/12/2025 16:45:30 INFO]: Saving model to: model_best.pth
[08/12/2025 16:45:39 INFO]: Training loss at epoch 6: 1.2602369785308838
[08/12/2025 16:45:40 INFO]: Training loss at epoch 63: 1.0106698870658875
[08/12/2025 16:45:42 INFO]: New best epoch, val score: -0.9243373351666836
[08/12/2025 16:45:42 INFO]: Saving model to: model_best.pth
[08/12/2025 16:45:44 INFO]: Training loss at epoch 7: 1.0106124877929688
[08/12/2025 16:45:50 INFO]: Training loss at epoch 8: 1.0995301604270935
[08/12/2025 16:45:53 INFO]: Training loss at epoch 64: 0.8091136515140533
[08/12/2025 16:45:54 INFO]: New best epoch, val score: -0.9228878910690725
[08/12/2025 16:45:54 INFO]: Saving model to: model_best.pth
[08/12/2025 16:45:58 INFO]: Training loss at epoch 23: 0.9619277119636536
[08/12/2025 16:46:01 INFO]: New best epoch, val score: -0.9278893328316079
[08/12/2025 16:46:01 INFO]: Saving model to: model_best.pth
[08/12/2025 16:46:02 INFO]: New best epoch, val score: -0.9023870234808583
[08/12/2025 16:46:02 INFO]: Saving model to: model_best.pth
[08/12/2025 16:46:03 INFO]: Training loss at epoch 8: 1.3525118231773376
[08/12/2025 16:46:05 INFO]: Training loss at epoch 65: 0.8920566439628601
[08/12/2025 16:46:06 INFO]: Training loss at epoch 13: 0.9812560975551605
[08/12/2025 16:46:07 INFO]: New best epoch, val score: -0.9220525230492591
[08/12/2025 16:46:07 INFO]: Saving model to: model_best.pth
[08/12/2025 16:46:18 INFO]: Training loss at epoch 66: 1.2768924832344055
[08/12/2025 16:46:28 INFO]: Training loss at epoch 3: 1.114102691411972
[08/12/2025 16:46:29 INFO]: Training loss at epoch 12: 1.0569450557231903
[08/12/2025 16:46:30 INFO]: Training loss at epoch 67: 0.9761311113834381
[08/12/2025 16:46:31 INFO]: Training loss at epoch 24: 0.8664363622665405
[08/12/2025 16:46:36 INFO]: New best epoch, val score: -0.8927493535976188
[08/12/2025 16:46:36 INFO]: Saving model to: model_best.pth
[08/12/2025 16:46:42 INFO]: Training loss at epoch 68: 1.0975146293640137
[08/12/2025 16:46:55 INFO]: Training loss at epoch 69: 0.983758956193924
[08/12/2025 16:46:59 INFO]: Training stats: {
    "score": -0.9792565868531504,
    "rmse": 0.9792565868531504
}
[08/12/2025 16:46:59 INFO]: Val stats: {
    "score": -0.925720714626423,
    "rmse": 0.925720714626423
}
[08/12/2025 16:46:59 INFO]: Test stats: {
    "score": -0.9037407850478307,
    "rmse": 0.9037407850478307
}
[08/12/2025 16:47:04 INFO]: Training loss at epoch 14: 1.2190014123916626
[08/12/2025 16:47:05 INFO]: Training loss at epoch 25: 0.7006467878818512
[08/12/2025 16:47:09 INFO]: New best epoch, val score: -0.8788194359990873
[08/12/2025 16:47:09 INFO]: Saving model to: model_best.pth
[08/12/2025 16:47:11 INFO]: Training loss at epoch 70: 0.8903057873249054
[08/12/2025 16:47:22 INFO]: Training loss at epoch 9: 1.0514704585075378
[08/12/2025 16:47:26 INFO]: Training loss at epoch 8: 0.8682262599468231
[08/12/2025 16:47:29 INFO]: Training loss at epoch 71: 0.8936900496482849
[08/12/2025 16:47:34 INFO]: Training loss at epoch 13: 1.316918432712555
[08/12/2025 16:47:35 INFO]: Training loss at epoch 9: 1.0292773246765137
[08/12/2025 16:47:36 INFO]: Training loss at epoch 7: 1.170601487159729
[08/12/2025 16:47:38 INFO]: Training loss at epoch 26: 0.917193591594696
[08/12/2025 16:47:41 INFO]: Training loss at epoch 72: 1.3765519559383392
[08/12/2025 16:47:42 INFO]: New best epoch, val score: -0.8622531244808129
[08/12/2025 16:47:42 INFO]: Saving model to: model_best.pth
[08/12/2025 16:47:53 INFO]: Training loss at epoch 73: 1.0826263725757599
[08/12/2025 16:47:54 INFO]: Training stats: {
    "score": -1.0192984472353543,
    "rmse": 1.0192984472353543
}
[08/12/2025 16:47:54 INFO]: Val stats: {
    "score": -0.9263695183880934,
    "rmse": 0.9263695183880934
}
[08/12/2025 16:47:54 INFO]: Test stats: {
    "score": -0.9201675963623084,
    "rmse": 0.9201675963623084
}
[08/12/2025 16:48:05 INFO]: New best epoch, val score: -0.9263695183880934
[08/12/2025 16:48:05 INFO]: Saving model to: model_best.pth
[08/12/2025 16:48:05 INFO]: Training loss at epoch 74: 0.9682389199733734
[08/12/2025 16:48:08 INFO]: Training stats: {
    "score": -1.0131162987109887,
    "rmse": 1.0131162987109887
}
[08/12/2025 16:48:08 INFO]: Val stats: {
    "score": -0.9186845388980637,
    "rmse": 0.9186845388980637
}
[08/12/2025 16:48:08 INFO]: Test stats: {
    "score": -0.9146445831638745,
    "rmse": 0.9146445831638745
}
[08/12/2025 16:48:10 INFO]: Training loss at epoch 27: 0.8256314992904663
[08/12/2025 16:48:11 INFO]: Training loss at epoch 15: 0.9751309454441071
[08/12/2025 16:48:14 INFO]: New best epoch, val score: -0.8575773314044149
[08/12/2025 16:48:14 INFO]: Saving model to: model_best.pth
[08/12/2025 16:48:17 INFO]: Training loss at epoch 75: 0.957520455121994
[08/12/2025 16:48:29 INFO]: Training loss at epoch 76: 0.90916907787323
[08/12/2025 16:48:40 INFO]: Training loss at epoch 14: 1.110082060098648
[08/12/2025 16:48:42 INFO]: Training loss at epoch 77: 1.0996208488941193
[08/12/2025 16:48:44 INFO]: Training loss at epoch 28: 0.6954655647277832
[08/12/2025 16:48:48 INFO]: New best epoch, val score: -0.8974171721863307
[08/12/2025 16:48:48 INFO]: Saving model to: model_best.pth
[08/12/2025 16:48:54 INFO]: Training loss at epoch 78: 1.2174562811851501
[08/12/2025 16:49:06 INFO]: Training loss at epoch 79: 0.8686480522155762
[08/12/2025 16:49:09 INFO]: Training loss at epoch 9: 0.9561669826507568
[08/12/2025 16:49:09 INFO]: Training loss at epoch 16: 0.9197206795215607
[08/12/2025 16:49:10 INFO]: Training stats: {
    "score": -0.9750983483213238,
    "rmse": 0.9750983483213238
}
[08/12/2025 16:49:10 INFO]: Val stats: {
    "score": -0.9277299696476423,
    "rmse": 0.9277299696476423
}
[08/12/2025 16:49:10 INFO]: Test stats: {
    "score": -0.9033609420829984,
    "rmse": 0.9033609420829984
}
[08/12/2025 16:49:18 INFO]: Training loss at epoch 29: 0.6949540078639984
[08/12/2025 16:49:23 INFO]: Training loss at epoch 80: 1.0329121351242065
[08/12/2025 16:49:26 INFO]: Training loss at epoch 10: 0.8422890603542328
[08/12/2025 16:49:30 INFO]: Training stats: {
    "score": -0.8411109721488392,
    "rmse": 0.8411109721488392
}
[08/12/2025 16:49:30 INFO]: Val stats: {
    "score": -0.8809106503615064,
    "rmse": 0.8809106503615064
}
[08/12/2025 16:49:30 INFO]: Test stats: {
    "score": -0.8278299430431506,
    "rmse": 0.8278299430431506
}
[08/12/2025 16:49:32 INFO]: Training loss at epoch 8: 1.0393165946006775
[08/12/2025 16:49:35 INFO]: Training loss at epoch 81: 0.755823478102684
[08/12/2025 16:49:40 INFO]: Training loss at epoch 10: 0.9620326459407806
[08/12/2025 16:49:43 INFO]: Training stats: {
    "score": -1.0073495797608265,
    "rmse": 1.0073495797608265
}
[08/12/2025 16:49:43 INFO]: Val stats: {
    "score": -0.9373317420866016,
    "rmse": 0.9373317420866016
}
[08/12/2025 16:49:43 INFO]: Test stats: {
    "score": -0.9193782467905307,
    "rmse": 0.9193782467905307
}
[08/12/2025 16:49:46 INFO]: Training loss at epoch 15: 0.9471956789493561
[08/12/2025 16:49:47 INFO]: Training loss at epoch 82: 1.087627351284027
[08/12/2025 16:49:59 INFO]: Training loss at epoch 83: 1.0351443886756897
[08/12/2025 16:50:01 INFO]: New best epoch, val score: -0.920310791101511
[08/12/2025 16:50:01 INFO]: Saving model to: model_best.pth
[08/12/2025 16:50:04 INFO]: Training loss at epoch 30: 0.688641756772995
[08/12/2025 16:50:06 INFO]: Training loss at epoch 17: 1.0912935733795166
[08/12/2025 16:50:08 INFO]: Training loss at epoch 4: 1.558628112077713
[08/12/2025 16:50:08 INFO]: New best epoch, val score: -0.8558448557898499
[08/12/2025 16:50:08 INFO]: Saving model to: model_best.pth
[08/12/2025 16:50:12 INFO]: Training loss at epoch 84: 1.0092738270759583
[08/12/2025 16:50:13 INFO]: New best epoch, val score: -0.9188196102547879
[08/12/2025 16:50:13 INFO]: Saving model to: model_best.pth
[08/12/2025 16:50:24 INFO]: Training loss at epoch 85: 1.238711178302765
[08/12/2025 16:50:26 INFO]: New best epoch, val score: -0.9177394705190917
[08/12/2025 16:50:26 INFO]: Saving model to: model_best.pth
[08/12/2025 16:50:33 INFO]: New best epoch, val score: -0.9532286861130483
[08/12/2025 16:50:33 INFO]: Saving model to: model_best.pth
[08/12/2025 16:50:37 INFO]: Training loss at epoch 86: 1.1332832872867584
[08/12/2025 16:50:37 INFO]: Training loss at epoch 31: 0.7542005479335785
[08/12/2025 16:50:38 INFO]: New best epoch, val score: -0.9169473834205509
[08/12/2025 16:50:38 INFO]: Saving model to: model_best.pth
[08/12/2025 16:50:42 INFO]: New best epoch, val score: -0.8431271748046607
[08/12/2025 16:50:42 INFO]: Saving model to: model_best.pth
[08/12/2025 16:50:49 INFO]: Training loss at epoch 87: 0.9874133467674255
[08/12/2025 16:50:51 INFO]: Training loss at epoch 16: 0.7918066680431366
[08/12/2025 16:50:58 INFO]: Training loss at epoch 11: 0.850633978843689
[08/12/2025 16:51:01 INFO]: Training loss at epoch 88: 0.9938423931598663
[08/12/2025 16:51:05 INFO]: Training loss at epoch 18: 1.157974898815155
[08/12/2025 16:51:11 INFO]: Training loss at epoch 32: 0.6630011200904846
[08/12/2025 16:51:12 INFO]: Training loss at epoch 11: 1.0837724804878235
[08/12/2025 16:51:13 INFO]: Training loss at epoch 89: 0.8896427154541016
[08/12/2025 16:51:17 INFO]: Training stats: {
    "score": -0.9766116193599407,
    "rmse": 0.9766116193599407
}
[08/12/2025 16:51:17 INFO]: Val stats: {
    "score": -0.9175668318057915,
    "rmse": 0.9175668318057915
}
[08/12/2025 16:51:17 INFO]: Test stats: {
    "score": -0.8980498492347544,
    "rmse": 0.8980498492347544
}
[08/12/2025 16:51:25 INFO]: Training loss at epoch 10: 0.9402479827404022
[08/12/2025 16:51:30 INFO]: Training loss at epoch 90: 0.972061812877655
[08/12/2025 16:51:30 INFO]: Training loss at epoch 9: 1.1790346503257751
[08/12/2025 16:51:42 INFO]: Training loss at epoch 91: 1.1136330962181091
[08/12/2025 16:51:43 INFO]: New best epoch, val score: -0.916028534305613
[08/12/2025 16:51:43 INFO]: Saving model to: model_best.pth
[08/12/2025 16:51:44 INFO]: Training loss at epoch 33: 0.6219049394130707
[08/12/2025 16:51:54 INFO]: Training loss at epoch 92: 0.9424558281898499
[08/12/2025 16:51:55 INFO]: Training loss at epoch 17: 1.0754131078720093
[08/12/2025 16:51:56 INFO]: New best epoch, val score: -0.9151326545044529
[08/12/2025 16:51:56 INFO]: Saving model to: model_best.pth
[08/12/2025 16:52:03 INFO]: Training loss at epoch 19: 0.9421870112419128
[08/12/2025 16:52:07 INFO]: Training loss at epoch 93: 1.105561077594757
[08/12/2025 16:52:08 INFO]: New best epoch, val score: -0.9145709728075974
[08/12/2025 16:52:08 INFO]: Saving model to: model_best.pth
[08/12/2025 16:52:10 INFO]: Training stats: {
    "score": -1.057864418178076,
    "rmse": 1.057864418178076
}
[08/12/2025 16:52:10 INFO]: Val stats: {
    "score": -1.0949999939359787,
    "rmse": 1.0949999939359787
}
[08/12/2025 16:52:10 INFO]: Test stats: {
    "score": -1.0308821384270952,
    "rmse": 1.0308821384270952
}
[08/12/2025 16:52:18 INFO]: Training loss at epoch 34: 0.61386439204216
[08/12/2025 16:52:19 INFO]: Training loss at epoch 94: 0.9982525110244751
[08/12/2025 16:52:20 INFO]: New best epoch, val score: -0.91372467919548
[08/12/2025 16:52:20 INFO]: Saving model to: model_best.pth
[08/12/2025 16:52:22 INFO]: Training stats: {
    "score": -0.9940630911886078,
    "rmse": 0.9940630911886078
}
[08/12/2025 16:52:22 INFO]: Val stats: {
    "score": -0.9726683628680428,
    "rmse": 0.9726683628680428
}
[08/12/2025 16:52:22 INFO]: Test stats: {
    "score": -0.9331730950556583,
    "rmse": 0.9331730950556583
}
[08/12/2025 16:52:31 INFO]: Training loss at epoch 12: 0.9683403968811035
[08/12/2025 16:52:31 INFO]: Training loss at epoch 95: 1.2485060691833496
[08/12/2025 16:52:33 INFO]: New best epoch, val score: -0.912928489522666
[08/12/2025 16:52:33 INFO]: Saving model to: model_best.pth
[08/12/2025 16:52:43 INFO]: Training loss at epoch 96: 1.0583581328392029
[08/12/2025 16:52:45 INFO]: Training loss at epoch 12: 1.3655996322631836
[08/12/2025 16:52:51 INFO]: Training loss at epoch 35: 0.7071802616119385
[08/12/2025 16:52:55 INFO]: New best epoch, val score: -0.833869560827865
[08/12/2025 16:52:55 INFO]: Saving model to: model_best.pth
[08/12/2025 16:52:55 INFO]: Training loss at epoch 97: 1.0465803742408752
[08/12/2025 16:53:01 INFO]: Training loss at epoch 18: 0.9045027196407318
[08/12/2025 16:53:07 INFO]: Training loss at epoch 98: 0.893193244934082
[08/12/2025 16:53:07 INFO]: Training loss at epoch 11: 0.8699958920478821
[08/12/2025 16:53:19 INFO]: Training loss at epoch 99: 0.9443331360816956
[08/12/2025 16:53:21 INFO]: Training loss at epoch 20: 0.8132887184619904
[08/12/2025 16:53:23 INFO]: Training stats: {
    "score": -0.9739631450218449,
    "rmse": 0.9739631450218449
}
[08/12/2025 16:53:23 INFO]: Val stats: {
    "score": -0.9155338133124404,
    "rmse": 0.9155338133124404
}
[08/12/2025 16:53:23 INFO]: Test stats: {
    "score": -0.8957506264397032,
    "rmse": 0.8957506264397032
}
[08/12/2025 16:53:24 INFO]: Training loss at epoch 36: 0.7016191780567169
[08/12/2025 16:53:35 INFO]: Training loss at epoch 100: 0.8738091886043549
[08/12/2025 16:53:47 INFO]: Training loss at epoch 101: 0.9940430521965027
[08/12/2025 16:53:48 INFO]: Training loss at epoch 5: 0.9390185177326202
[08/12/2025 16:53:56 INFO]: Training loss at epoch 37: 0.6484018564224243
[08/12/2025 16:53:59 INFO]: Training loss at epoch 102: 1.2359228730201721
[08/12/2025 16:54:01 INFO]: Training loss at epoch 13: 1.0681617856025696
[08/12/2025 16:54:05 INFO]: Training loss at epoch 10: 0.943920910358429
[08/12/2025 16:54:06 INFO]: Training loss at epoch 19: 0.8831499814987183
[08/12/2025 16:54:11 INFO]: Training loss at epoch 103: 0.8265333473682404
[08/12/2025 16:54:19 INFO]: Training loss at epoch 13: 1.2121354341506958
[08/12/2025 16:54:20 INFO]: Training loss at epoch 21: 0.9111969172954559
[08/12/2025 16:54:23 INFO]: Training loss at epoch 104: 1.0474455058574677
[08/12/2025 16:54:28 INFO]: Training stats: {
    "score": -0.9998691766512098,
    "rmse": 0.9998691766512098
}
[08/12/2025 16:54:28 INFO]: Val stats: {
    "score": -1.0324059390109102,
    "rmse": 1.0324059390109102
}
[08/12/2025 16:54:28 INFO]: Test stats: {
    "score": -0.9670352611884048,
    "rmse": 0.9670352611884048
}
[08/12/2025 16:54:29 INFO]: Training loss at epoch 38: 0.7332330644130707
[08/12/2025 16:54:35 INFO]: Training loss at epoch 105: 1.1892215609550476
[08/12/2025 16:54:47 INFO]: Training loss at epoch 106: 1.0583257675170898
[08/12/2025 16:54:51 INFO]: Training loss at epoch 12: 1.1149746775627136
[08/12/2025 16:54:58 INFO]: Training loss at epoch 107: 1.100968599319458
[08/12/2025 16:55:02 INFO]: Training loss at epoch 39: 0.5338685661554337
[08/12/2025 16:55:11 INFO]: Training loss at epoch 108: 0.8921286463737488
[08/12/2025 16:55:14 INFO]: Training stats: {
    "score": -0.7399541110251076,
    "rmse": 0.7399541110251076
}
[08/12/2025 16:55:14 INFO]: Val stats: {
    "score": -0.8438442326522443,
    "rmse": 0.8438442326522443
}
[08/12/2025 16:55:14 INFO]: Test stats: {
    "score": -0.7927975307673113,
    "rmse": 0.7927975307673113
}
[08/12/2025 16:55:19 INFO]: Training loss at epoch 22: 0.8509029746055603
[08/12/2025 16:55:23 INFO]: Training loss at epoch 109: 1.059924066066742
[08/12/2025 16:55:27 INFO]: Training stats: {
    "score": -0.9672457047931202,
    "rmse": 0.9672457047931202
}
[08/12/2025 16:55:27 INFO]: Val stats: {
    "score": -0.9283203513239451,
    "rmse": 0.9283203513239451
}
[08/12/2025 16:55:27 INFO]: Test stats: {
    "score": -0.9003497070643928,
    "rmse": 0.9003497070643928
}
[08/12/2025 16:55:31 INFO]: Training loss at epoch 14: 1.0439595580101013
[08/12/2025 16:55:33 INFO]: Training loss at epoch 20: 0.8026340305805206
[08/12/2025 16:55:39 INFO]: Training loss at epoch 110: 0.7685321271419525
[08/12/2025 16:55:47 INFO]: Training loss at epoch 40: 0.4752623587846756
[08/12/2025 16:55:52 INFO]: Training loss at epoch 111: 0.9354687929153442
[08/12/2025 16:55:52 INFO]: Training loss at epoch 14: 1.2237879931926727
[08/12/2025 16:56:01 INFO]: Training loss at epoch 11: 1.0185253620147705
[08/12/2025 16:56:04 INFO]: Training loss at epoch 112: 0.9570854902267456
[08/12/2025 16:56:15 INFO]: New best epoch, val score: -0.9194080522098054
[08/12/2025 16:56:15 INFO]: Saving model to: model_best.pth
[08/12/2025 16:56:18 INFO]: Training loss at epoch 113: 0.9265521764755249
[08/12/2025 16:56:18 INFO]: Training loss at epoch 23: 0.9656399786472321
[08/12/2025 16:56:22 INFO]: Training loss at epoch 41: 0.5151175111532211
[08/12/2025 16:56:27 INFO]: New best epoch, val score: -0.820176835980236
[08/12/2025 16:56:27 INFO]: Saving model to: model_best.pth
[08/12/2025 16:56:30 INFO]: Training loss at epoch 114: 1.03390571475029
[08/12/2025 16:56:34 INFO]: Training loss at epoch 13: 0.9322788417339325
[08/12/2025 16:56:41 INFO]: Training loss at epoch 21: 0.9236022233963013
[08/12/2025 16:56:42 INFO]: Training loss at epoch 115: 0.9165672361850739
[08/12/2025 16:56:54 INFO]: Training loss at epoch 116: 0.9154826700687408
[08/12/2025 16:56:55 INFO]: Training loss at epoch 42: 0.7972680032253265
[08/12/2025 16:56:59 INFO]: New best epoch, val score: -0.8046679953767538
[08/12/2025 16:56:59 INFO]: Saving model to: model_best.pth
[08/12/2025 16:57:05 INFO]: Training loss at epoch 15: 0.9951521754264832
[08/12/2025 16:57:06 INFO]: Training loss at epoch 117: 1.0765660107135773
[08/12/2025 16:57:17 INFO]: Training loss at epoch 24: 1.1075882315635681
[08/12/2025 16:57:18 INFO]: Training loss at epoch 118: 0.9905572831630707
[08/12/2025 16:57:26 INFO]: Training loss at epoch 15: 0.8513040542602539
[08/12/2025 16:57:29 INFO]: Training loss at epoch 6: 1.0547987818717957
[08/12/2025 16:57:29 INFO]: Training loss at epoch 43: 0.4461366534233093
[08/12/2025 16:57:30 INFO]: Training loss at epoch 119: 0.9707281291484833
[08/12/2025 16:57:35 INFO]: Training stats: {
    "score": -0.9683081768635835,
    "rmse": 0.9683081768635835
}
[08/12/2025 16:57:35 INFO]: Val stats: {
    "score": -0.9123848915034483,
    "rmse": 0.9123848915034483
}
[08/12/2025 16:57:35 INFO]: Test stats: {
    "score": -0.8914976064617139,
    "rmse": 0.8914976064617139
}
[08/12/2025 16:57:36 INFO]: New best epoch, val score: -0.9123848915034483
[08/12/2025 16:57:36 INFO]: Saving model to: model_best.pth
[08/12/2025 16:57:46 INFO]: Training loss at epoch 22: 1.0220940113067627
[08/12/2025 16:57:47 INFO]: Training loss at epoch 120: 1.0256307423114777
[08/12/2025 16:57:48 INFO]: New best epoch, val score: -0.9115605367153823
[08/12/2025 16:57:48 INFO]: Saving model to: model_best.pth
[08/12/2025 16:57:54 INFO]: New best epoch, val score: -0.8841904860983827
[08/12/2025 16:57:54 INFO]: Saving model to: model_best.pth
[08/12/2025 16:58:00 INFO]: Training loss at epoch 121: 0.8783314228057861
[08/12/2025 16:58:01 INFO]: Training loss at epoch 12: 1.0176436603069305
[08/12/2025 16:58:02 INFO]: New best epoch, val score: -0.9105698732664566
[08/12/2025 16:58:02 INFO]: Saving model to: model_best.pth
[08/12/2025 16:58:03 INFO]: Training loss at epoch 44: 0.5379122197628021
[08/12/2025 16:58:12 INFO]: Training loss at epoch 122: 1.117891013622284
[08/12/2025 16:58:14 INFO]: New best epoch, val score: -0.9094013221009513
[08/12/2025 16:58:14 INFO]: Saving model to: model_best.pth
[08/12/2025 16:58:15 INFO]: Training loss at epoch 25: 1.0152419805526733
[08/12/2025 16:58:16 INFO]: Training loss at epoch 14: 0.9803411364555359
[08/12/2025 16:58:22 INFO]: New best epoch, val score: -0.9298836810830683
[08/12/2025 16:58:22 INFO]: Saving model to: model_best.pth
[08/12/2025 16:58:25 INFO]: Training loss at epoch 123: 1.282935082912445
[08/12/2025 16:58:26 INFO]: New best epoch, val score: -0.9093314921772178
[08/12/2025 16:58:26 INFO]: Saving model to: model_best.pth
[08/12/2025 16:58:37 INFO]: Training loss at epoch 45: 0.4935562014579773
[08/12/2025 16:58:37 INFO]: Training loss at epoch 124: 0.8149652779102325
[08/12/2025 16:58:38 INFO]: Training loss at epoch 16: 0.9702627956867218
[08/12/2025 16:58:41 INFO]: New best epoch, val score: -0.8037163472647055
[08/12/2025 16:58:41 INFO]: Saving model to: model_best.pth
[08/12/2025 16:58:50 INFO]: Training loss at epoch 125: 0.9820778965950012
[08/12/2025 16:58:52 INFO]: Training loss at epoch 23: 0.8861020803451538
[08/12/2025 16:58:59 INFO]: Training loss at epoch 16: 0.947164386510849
[08/12/2025 16:59:00 INFO]: New best epoch, val score: -0.8793651641479541
[08/12/2025 16:59:00 INFO]: Saving model to: model_best.pth
[08/12/2025 16:59:02 INFO]: Training loss at epoch 126: 0.9289076328277588
[08/12/2025 16:59:03 INFO]: New best epoch, val score: -0.9091033670949134
[08/12/2025 16:59:03 INFO]: Saving model to: model_best.pth
[08/12/2025 16:59:11 INFO]: Training loss at epoch 46: 0.47013211250305176
[08/12/2025 16:59:13 INFO]: Training loss at epoch 26: 1.1609222292900085
[08/12/2025 16:59:14 INFO]: Training loss at epoch 127: 1.3030818104743958
[08/12/2025 16:59:15 INFO]: New best epoch, val score: -0.7777741207160754
[08/12/2025 16:59:15 INFO]: Saving model to: model_best.pth
[08/12/2025 16:59:27 INFO]: Training loss at epoch 128: 0.8112847208976746
[08/12/2025 16:59:39 INFO]: Training loss at epoch 129: 0.7729818224906921
[08/12/2025 16:59:43 INFO]: Training stats: {
    "score": -0.9665776975162762,
    "rmse": 0.9665776975162762
}
[08/12/2025 16:59:43 INFO]: Val stats: {
    "score": -0.909288580675542,
    "rmse": 0.909288580675542
}
[08/12/2025 16:59:43 INFO]: Test stats: {
    "score": -0.8891686267848099,
    "rmse": 0.8891686267848099
}
[08/12/2025 16:59:44 INFO]: Training loss at epoch 47: 0.621689110994339
[08/12/2025 16:59:49 INFO]: New best epoch, val score: -0.7750421651188756
[08/12/2025 16:59:49 INFO]: Saving model to: model_best.pth
[08/12/2025 16:59:55 INFO]: Training loss at epoch 130: 0.9693911373615265
[08/12/2025 16:59:57 INFO]: Training loss at epoch 24: 0.9795035719871521
[08/12/2025 16:59:58 INFO]: Training loss at epoch 15: 0.8902910649776459
[08/12/2025 16:59:59 INFO]: Training loss at epoch 13: 1.0910166501998901
[08/12/2025 17:00:05 INFO]: New best epoch, val score: -0.8757755252033284
[08/12/2025 17:00:05 INFO]: Saving model to: model_best.pth
[08/12/2025 17:00:08 INFO]: Training loss at epoch 131: 0.8353833556175232
[08/12/2025 17:00:09 INFO]: New best epoch, val score: -0.9089728348642074
[08/12/2025 17:00:09 INFO]: Saving model to: model_best.pth
[08/12/2025 17:00:11 INFO]: Training loss at epoch 17: 0.8620554506778717
[08/12/2025 17:00:11 INFO]: Training loss at epoch 27: 1.1832148432731628
[08/12/2025 17:00:18 INFO]: Training loss at epoch 48: 0.4754108786582947
[08/12/2025 17:00:20 INFO]: Training loss at epoch 132: 1.1967926621437073
[08/12/2025 17:00:22 INFO]: New best epoch, val score: -0.9085408604887005
[08/12/2025 17:00:22 INFO]: Saving model to: model_best.pth
[08/12/2025 17:00:22 INFO]: New best epoch, val score: -0.9200278585690124
[08/12/2025 17:00:22 INFO]: Saving model to: model_best.pth
[08/12/2025 17:00:23 INFO]: New best epoch, val score: -0.7730361593587718
[08/12/2025 17:00:23 INFO]: Saving model to: model_best.pth
[08/12/2025 17:00:31 INFO]: Training loss at epoch 17: 1.2457640767097473
[08/12/2025 17:00:33 INFO]: Training loss at epoch 133: 1.1100568175315857
[08/12/2025 17:00:45 INFO]: Training loss at epoch 134: 0.9577983319759369
[08/12/2025 17:00:52 INFO]: Training loss at epoch 49: 0.4446578770875931
[08/12/2025 17:00:57 INFO]: Training loss at epoch 135: 1.0542628169059753
[08/12/2025 17:01:02 INFO]: Training loss at epoch 25: 0.7814320921897888
[08/12/2025 17:01:05 INFO]: Training stats: {
    "score": -0.6847731907052661,
    "rmse": 0.6847731907052661
}
[08/12/2025 17:01:05 INFO]: Val stats: {
    "score": -0.7818406089939196,
    "rmse": 0.7818406089939196
}
[08/12/2025 17:01:05 INFO]: Test stats: {
    "score": -0.7379705474131352,
    "rmse": 0.7379705474131352
}
[08/12/2025 17:01:07 INFO]: Training loss at epoch 7: 1.050809919834137
[08/12/2025 17:01:09 INFO]: Training loss at epoch 28: 0.8938226699829102
[08/12/2025 17:01:09 INFO]: Training loss at epoch 136: 0.8228667676448822
[08/12/2025 17:01:10 INFO]: New best epoch, val score: -0.8753263479612217
[08/12/2025 17:01:10 INFO]: Saving model to: model_best.pth
[08/12/2025 17:01:22 INFO]: Training loss at epoch 137: 1.021759033203125
[08/12/2025 17:01:34 INFO]: Training loss at epoch 138: 0.9891617000102997
[08/12/2025 17:01:38 INFO]: Training loss at epoch 50: 0.5623746812343597
[08/12/2025 17:01:40 INFO]: Training loss at epoch 16: 0.9262892305850983
[08/12/2025 17:01:43 INFO]: Training loss at epoch 18: 0.8882140517234802
[08/12/2025 17:01:46 INFO]: Training loss at epoch 139: 1.016688495874405
[08/12/2025 17:01:50 INFO]: Training stats: {
    "score": -0.9593012717414301,
    "rmse": 0.9593012717414301
}
[08/12/2025 17:01:50 INFO]: Val stats: {
    "score": -0.9184920560397267,
    "rmse": 0.9184920560397267
}
[08/12/2025 17:01:50 INFO]: Test stats: {
    "score": -0.891672623592596,
    "rmse": 0.891672623592596
}
[08/12/2025 17:01:55 INFO]: New best epoch, val score: -0.9175246621517443
[08/12/2025 17:01:55 INFO]: Saving model to: model_best.pth
[08/12/2025 17:01:57 INFO]: Training loss at epoch 14: 1.0520814657211304
[08/12/2025 17:02:03 INFO]: Training loss at epoch 140: 0.9065592288970947
[08/12/2025 17:02:03 INFO]: Training loss at epoch 18: 1.072180062532425
[08/12/2025 17:02:07 INFO]: Training loss at epoch 29: 0.9999122619628906
[08/12/2025 17:02:07 INFO]: Training loss at epoch 26: 0.833488792181015
[08/12/2025 17:02:11 INFO]: Training loss at epoch 51: 0.4342617690563202
[08/12/2025 17:02:15 INFO]: Training loss at epoch 141: 0.9989381730556488
[08/12/2025 17:02:27 INFO]: Training loss at epoch 142: 0.9809094071388245
[08/12/2025 17:02:27 INFO]: Training stats: {
    "score": -0.9956925495081458,
    "rmse": 0.9956925495081458
}
[08/12/2025 17:02:27 INFO]: Val stats: {
    "score": -0.9920659339697495,
    "rmse": 0.9920659339697495
}
[08/12/2025 17:02:27 INFO]: Test stats: {
    "score": -0.9451709736137365,
    "rmse": 0.9451709736137365
}
[08/12/2025 17:02:39 INFO]: Training loss at epoch 143: 1.0073089599609375
[08/12/2025 17:02:44 INFO]: Training loss at epoch 52: 0.4270833134651184
[08/12/2025 17:02:51 INFO]: Training loss at epoch 144: 0.9685003161430359
[08/12/2025 17:03:03 INFO]: Training loss at epoch 145: 0.8934416174888611
[08/12/2025 17:03:13 INFO]: Training loss at epoch 27: 0.801435261964798
[08/12/2025 17:03:15 INFO]: Training loss at epoch 19: 1.1777922213077545
[08/12/2025 17:03:16 INFO]: Training loss at epoch 146: 0.7907668948173523
[08/12/2025 17:03:18 INFO]: Training loss at epoch 53: 0.593584418296814
[08/12/2025 17:03:22 INFO]: Training loss at epoch 17: 1.5114474296569824
[08/12/2025 17:03:25 INFO]: Training loss at epoch 30: 1.110018104314804
[08/12/2025 17:03:28 INFO]: Training loss at epoch 147: 0.7737280428409576
[08/12/2025 17:03:35 INFO]: Training loss at epoch 19: 0.9469799995422363
[08/12/2025 17:03:40 INFO]: Training loss at epoch 148: 1.1761753261089325
[08/12/2025 17:03:47 INFO]: Training stats: {
    "score": -0.9940052843160582,
    "rmse": 0.9940052843160582
}
[08/12/2025 17:03:47 INFO]: Val stats: {
    "score": -0.9163383272847229,
    "rmse": 0.9163383272847229
}
[08/12/2025 17:03:47 INFO]: Test stats: {
    "score": -0.9026894930188791,
    "rmse": 0.9026894930188791
}
[08/12/2025 17:03:51 INFO]: Training loss at epoch 54: 0.5624149739742279
[08/12/2025 17:03:52 INFO]: Training loss at epoch 149: 0.9222738146781921
[08/12/2025 17:03:53 INFO]: Training loss at epoch 15: 0.9069004952907562
[08/12/2025 17:03:56 INFO]: Training stats: {
    "score": -0.9575049575300262,
    "rmse": 0.9575049575300262
}
[08/12/2025 17:03:56 INFO]: Val stats: {
    "score": -0.9107013321431116,
    "rmse": 0.9107013321431116
}
[08/12/2025 17:03:56 INFO]: Test stats: {
    "score": -0.8864951996152375,
    "rmse": 0.8864951996152375
}
[08/12/2025 17:03:58 INFO]: New best epoch, val score: -0.9163383272847229
[08/12/2025 17:03:58 INFO]: Saving model to: model_best.pth
[08/12/2025 17:04:07 INFO]: Training stats: {
    "score": -0.9507067897615417,
    "rmse": 0.9507067897615417
}
[08/12/2025 17:04:07 INFO]: Val stats: {
    "score": -0.9128045081853796,
    "rmse": 0.9128045081853796
}
[08/12/2025 17:04:07 INFO]: Test stats: {
    "score": -0.8802534979365255,
    "rmse": 0.8802534979365255
}
[08/12/2025 17:04:08 INFO]: New best epoch, val score: -0.9187877936532559
[08/12/2025 17:04:08 INFO]: Saving model to: model_best.pth
[08/12/2025 17:04:09 INFO]: Training loss at epoch 150: 1.5118193924427032
[08/12/2025 17:04:10 INFO]: New best epoch, val score: -0.9076506037322934
[08/12/2025 17:04:10 INFO]: Saving model to: model_best.pth
[08/12/2025 17:04:18 INFO]: Training loss at epoch 28: 0.828391432762146
[08/12/2025 17:04:21 INFO]: Training loss at epoch 151: 0.8911952376365662
[08/12/2025 17:04:23 INFO]: Training loss at epoch 31: 1.0345506072044373
[08/12/2025 17:04:23 INFO]: New best epoch, val score: -0.9062548857242455
[08/12/2025 17:04:23 INFO]: Saving model to: model_best.pth
[08/12/2025 17:04:25 INFO]: Training loss at epoch 55: 0.48084160685539246
[08/12/2025 17:04:34 INFO]: Training loss at epoch 152: 1.111530363559723
[08/12/2025 17:04:35 INFO]: New best epoch, val score: -0.9049594850770202
[08/12/2025 17:04:35 INFO]: Saving model to: model_best.pth
[08/12/2025 17:04:45 INFO]: Training loss at epoch 8: 1.0748620927333832
[08/12/2025 17:04:46 INFO]: Training loss at epoch 153: 1.0094422101974487
[08/12/2025 17:04:48 INFO]: New best epoch, val score: -0.904277170811252
[08/12/2025 17:04:48 INFO]: Saving model to: model_best.pth
[08/12/2025 17:04:58 INFO]: Training loss at epoch 154: 0.7302080988883972
[08/12/2025 17:04:59 INFO]: Training loss at epoch 56: 0.45473769307136536
[08/12/2025 17:05:00 INFO]: New best epoch, val score: -0.9041187719995556
[08/12/2025 17:05:00 INFO]: Saving model to: model_best.pth
[08/12/2025 17:05:03 INFO]: Training loss at epoch 18: 0.8936535716056824
[08/12/2025 17:05:11 INFO]: Training loss at epoch 155: 0.9061903953552246
[08/12/2025 17:05:20 INFO]: Training loss at epoch 20: 0.915910542011261
[08/12/2025 17:05:20 INFO]: Training loss at epoch 32: 1.0982917845249176
[08/12/2025 17:05:22 INFO]: Training loss at epoch 29: 0.8372896909713745
[08/12/2025 17:05:23 INFO]: Training loss at epoch 156: 1.0965314209461212
[08/12/2025 17:05:32 INFO]: Training loss at epoch 57: 0.5066189020872116
[08/12/2025 17:05:35 INFO]: Training loss at epoch 157: 0.9745941162109375
[08/12/2025 17:05:39 INFO]: Training loss at epoch 20: 0.8896295726299286
[08/12/2025 17:05:44 INFO]: Training stats: {
    "score": -0.9081414365506438,
    "rmse": 0.9081414365506438
}
[08/12/2025 17:05:44 INFO]: Val stats: {
    "score": -0.9186298541642591,
    "rmse": 0.9186298541642591
}
[08/12/2025 17:05:44 INFO]: Test stats: {
    "score": -0.8628133676408527,
    "rmse": 0.8628133676408527
}
[08/12/2025 17:05:48 INFO]: Training loss at epoch 158: 1.1737829744815826
[08/12/2025 17:05:51 INFO]: Training loss at epoch 16: 1.162835419178009
[08/12/2025 17:06:00 INFO]: Training loss at epoch 159: 0.8983429968357086
[08/12/2025 17:06:04 INFO]: Training stats: {
    "score": -0.9544608973935081,
    "rmse": 0.9544608973935081
}
[08/12/2025 17:06:04 INFO]: Val stats: {
    "score": -0.9123113924084293,
    "rmse": 0.9123113924084293
}
[08/12/2025 17:06:04 INFO]: Test stats: {
    "score": -0.8861362771378357,
    "rmse": 0.8861362771378357
}
[08/12/2025 17:06:06 INFO]: Training loss at epoch 58: 0.4094516932964325
[08/12/2025 17:06:16 INFO]: Training loss at epoch 160: 1.0269000232219696
[08/12/2025 17:06:18 INFO]: Training loss at epoch 33: 1.0405648350715637
[08/12/2025 17:06:28 INFO]: Training loss at epoch 161: 1.0301336348056793
[08/12/2025 17:06:39 INFO]: Training loss at epoch 59: 0.3444559574127197
[08/12/2025 17:06:40 INFO]: Training loss at epoch 162: 0.8277082443237305
[08/12/2025 17:06:45 INFO]: Training loss at epoch 19: 0.9536979496479034
[08/12/2025 17:06:50 INFO]: Training loss at epoch 30: 1.0156286656856537
[08/12/2025 17:06:51 INFO]: Training stats: {
    "score": -0.6469258925397545,
    "rmse": 0.6469258925397545
}
[08/12/2025 17:06:51 INFO]: Val stats: {
    "score": -0.7657903138904865,
    "rmse": 0.7657903138904865
}
[08/12/2025 17:06:51 INFO]: Test stats: {
    "score": -0.7252641552090117,
    "rmse": 0.7252641552090117
}
[08/12/2025 17:06:52 INFO]: Training loss at epoch 21: 0.9831287264823914
[08/12/2025 17:06:52 INFO]: Training loss at epoch 163: 1.065209686756134
[08/12/2025 17:06:55 INFO]: New best epoch, val score: -0.7657903138904865
[08/12/2025 17:06:55 INFO]: Saving model to: model_best.pth
[08/12/2025 17:07:04 INFO]: Training loss at epoch 164: 0.887712150812149
[08/12/2025 17:07:12 INFO]: Training loss at epoch 21: 0.8999722599983215
[08/12/2025 17:07:16 INFO]: Training loss at epoch 165: 0.9629475772380829
[08/12/2025 17:07:17 INFO]: Training loss at epoch 34: 0.8741992712020874
[08/12/2025 17:07:20 INFO]: Training stats: {
    "score": -0.9996020861249827,
    "rmse": 0.9996020861249827
}
[08/12/2025 17:07:20 INFO]: Val stats: {
    "score": -0.9489999652502931,
    "rmse": 0.9489999652502931
}
[08/12/2025 17:07:20 INFO]: Test stats: {
    "score": -0.9220879604055638,
    "rmse": 0.9220879604055638
}
[08/12/2025 17:07:23 INFO]: Training loss at epoch 60: 0.4913318604230881
[08/12/2025 17:07:28 INFO]: Training loss at epoch 166: 0.8516537249088287
[08/12/2025 17:07:40 INFO]: Training loss at epoch 167: 0.949739933013916
[08/12/2025 17:07:46 INFO]: Training loss at epoch 17: 0.9914127588272095
[08/12/2025 17:07:51 INFO]: Training loss at epoch 168: 1.020008534193039
[08/12/2025 17:07:55 INFO]: Training loss at epoch 31: 0.9313662052154541
[08/12/2025 17:07:56 INFO]: Training loss at epoch 61: 0.5973671823740005
[08/12/2025 17:08:03 INFO]: New best epoch, val score: -0.8456816260554789
[08/12/2025 17:08:03 INFO]: Saving model to: model_best.pth
[08/12/2025 17:08:04 INFO]: Training loss at epoch 169: 1.1532880067825317
[08/12/2025 17:08:08 INFO]: Training stats: {
    "score": -0.9515761769470751,
    "rmse": 0.9515761769470751
}
[08/12/2025 17:08:08 INFO]: Val stats: {
    "score": -0.9146929933805229,
    "rmse": 0.9146929933805229
}
[08/12/2025 17:08:08 INFO]: Test stats: {
    "score": -0.8861437379419278,
    "rmse": 0.8861437379419278
}
[08/12/2025 17:08:17 INFO]: Training loss at epoch 35: 0.8647000193595886
[08/12/2025 17:08:20 INFO]: Training loss at epoch 170: 0.8878131210803986
[08/12/2025 17:08:23 INFO]: Training loss at epoch 22: 1.0388704240322113
[08/12/2025 17:08:24 INFO]: New best epoch, val score: -0.9192519094247938
[08/12/2025 17:08:24 INFO]: Saving model to: model_best.pth
[08/12/2025 17:08:26 INFO]: Training loss at epoch 9: 1.1907318234443665
[08/12/2025 17:08:30 INFO]: Training loss at epoch 62: 0.4883183240890503
[08/12/2025 17:08:33 INFO]: Training loss at epoch 171: 0.8066956102848053
[08/12/2025 17:08:45 INFO]: Training loss at epoch 172: 1.1151277720928192
[08/12/2025 17:08:47 INFO]: Training loss at epoch 22: 1.1100887656211853
[08/12/2025 17:08:57 INFO]: Training loss at epoch 173: 0.8471893668174744
[08/12/2025 17:09:01 INFO]: Training loss at epoch 32: 0.6606185138225555
[08/12/2025 17:09:02 INFO]: Training loss at epoch 63: 0.4526832550764084
[08/12/2025 17:09:04 INFO]: Training loss at epoch 20: 1.015129029750824
[08/12/2025 17:09:09 INFO]: New best epoch, val score: -0.8354838066501491
[08/12/2025 17:09:09 INFO]: Saving model to: model_best.pth
[08/12/2025 17:09:09 INFO]: Training loss at epoch 174: 0.9469271600246429
[08/12/2025 17:09:16 INFO]: Training loss at epoch 36: 0.7776781320571899
[08/12/2025 17:09:21 INFO]: Training loss at epoch 175: 0.8136989176273346
[08/12/2025 17:09:23 INFO]: New best epoch, val score: -0.9040880579819842
[08/12/2025 17:09:23 INFO]: Saving model to: model_best.pth
[08/12/2025 17:09:23 INFO]: New best epoch, val score: -0.9143264920211428
[08/12/2025 17:09:23 INFO]: Saving model to: model_best.pth
[08/12/2025 17:09:34 INFO]: Training loss at epoch 176: 0.9944868087768555
[08/12/2025 17:09:35 INFO]: New best epoch, val score: -0.9023059973293621
[08/12/2025 17:09:35 INFO]: Saving model to: model_best.pth
[08/12/2025 17:09:36 INFO]: Training loss at epoch 64: 0.5051223039627075
[08/12/2025 17:09:41 INFO]: Training stats: {
    "score": -0.9972964724953715,
    "rmse": 0.9972964724953715
}
[08/12/2025 17:09:41 INFO]: Val stats: {
    "score": -0.9618232477625573,
    "rmse": 0.9618232477625573
}
[08/12/2025 17:09:41 INFO]: Test stats: {
    "score": -0.9277792553281969,
    "rmse": 0.9277792553281969
}
[08/12/2025 17:09:43 INFO]: Training loss at epoch 18: 1.2787167131900787
[08/12/2025 17:09:46 INFO]: Training loss at epoch 177: 1.0632811188697815
[08/12/2025 17:09:54 INFO]: Training loss at epoch 23: 0.8927435874938965
[08/12/2025 17:09:58 INFO]: Training loss at epoch 178: 1.0850822925567627
[08/12/2025 17:10:07 INFO]: Training loss at epoch 33: 0.9087227582931519
[08/12/2025 17:10:10 INFO]: Training loss at epoch 65: 0.48835553228855133
[08/12/2025 17:10:10 INFO]: Training loss at epoch 179: 1.168138474225998
[08/12/2025 17:10:14 INFO]: New best epoch, val score: -0.763050664797048
[08/12/2025 17:10:14 INFO]: Saving model to: model_best.pth
[08/12/2025 17:10:14 INFO]: Training loss at epoch 37: 1.1495469212532043
[08/12/2025 17:10:15 INFO]: Training stats: {
    "score": -0.9495943349279051,
    "rmse": 0.9495943349279051
}
[08/12/2025 17:10:15 INFO]: Val stats: {
    "score": -0.9058023936185983,
    "rmse": 0.9058023936185983
}
[08/12/2025 17:10:15 INFO]: Test stats: {
    "score": -0.8803979098658441,
    "rmse": 0.8803979098658441
}
[08/12/2025 17:10:15 INFO]: New best epoch, val score: -0.8277959664446787
[08/12/2025 17:10:15 INFO]: Saving model to: model_best.pth
[08/12/2025 17:10:20 INFO]: Training loss at epoch 23: 0.8521524369716644
[08/12/2025 17:10:27 INFO]: Training loss at epoch 180: 1.1692880690097809
[08/12/2025 17:10:39 INFO]: Training loss at epoch 181: 0.9742039442062378
[08/12/2025 17:10:43 INFO]: Training loss at epoch 66: 0.36874350905418396
[08/12/2025 17:10:46 INFO]: Training loss at epoch 21: 1.010791540145874
[08/12/2025 17:10:48 INFO]: New best epoch, val score: -0.7541903676471619
[08/12/2025 17:10:48 INFO]: Saving model to: model_best.pth
[08/12/2025 17:10:52 INFO]: Training loss at epoch 182: 0.9413305521011353
[08/12/2025 17:11:04 INFO]: Training loss at epoch 183: 0.8905770182609558
[08/12/2025 17:11:12 INFO]: Training loss at epoch 38: 1.03801628947258
[08/12/2025 17:11:12 INFO]: Training loss at epoch 34: 0.7199178338050842
[08/12/2025 17:11:16 INFO]: Training loss at epoch 184: 1.1333158910274506
[08/12/2025 17:11:17 INFO]: Training loss at epoch 67: 0.34844984859228134
[08/12/2025 17:11:20 INFO]: New best epoch, val score: -0.8242499076831313
[08/12/2025 17:11:20 INFO]: Saving model to: model_best.pth
[08/12/2025 17:11:26 INFO]: Training loss at epoch 24: 1.03560870885849
[08/12/2025 17:11:28 INFO]: Training loss at epoch 185: 0.8592358827590942
[08/12/2025 17:11:40 INFO]: Training loss at epoch 186: 0.9837392270565033
[08/12/2025 17:11:43 INFO]: Training loss at epoch 19: 0.9406872391700745
[08/12/2025 17:11:49 INFO]: Training loss at epoch 68: 0.4099404513835907
[08/12/2025 17:11:52 INFO]: Training loss at epoch 187: 0.8984362483024597
[08/12/2025 17:11:56 INFO]: Training loss at epoch 24: 0.9181232452392578
[08/12/2025 17:12:04 INFO]: Training loss at epoch 188: 1.0427910089492798
[08/12/2025 17:12:05 INFO]: New best epoch, val score: -0.9021704614802489
[08/12/2025 17:12:05 INFO]: Saving model to: model_best.pth
[08/12/2025 17:12:11 INFO]: Training loss at epoch 39: 1.068077951669693
[08/12/2025 17:12:16 INFO]: Training loss at epoch 35: 0.8966074585914612
[08/12/2025 17:12:16 INFO]: Training loss at epoch 189: 0.9294382929801941
[08/12/2025 17:12:20 INFO]: Training stats: {
    "score": -0.9467268811840183,
    "rmse": 0.9467268811840183
}
[08/12/2025 17:12:20 INFO]: Val stats: {
    "score": -0.9003898071287125,
    "rmse": 0.9003898071287125
}
[08/12/2025 17:12:20 INFO]: Test stats: {
    "score": -0.8762528066035009,
    "rmse": 0.8762528066035009
}
[08/12/2025 17:12:22 INFO]: New best epoch, val score: -0.9003898071287125
[08/12/2025 17:12:22 INFO]: Saving model to: model_best.pth
[08/12/2025 17:12:22 INFO]: Training loss at epoch 69: 0.47650302946567535
[08/12/2025 17:12:24 INFO]: Training stats: {
    "score": -1.0172524703247416,
    "rmse": 1.0172524703247416
}
[08/12/2025 17:12:24 INFO]: Val stats: {
    "score": -1.0375253309282526,
    "rmse": 1.0375253309282526
}
[08/12/2025 17:12:24 INFO]: Test stats: {
    "score": -0.9776010181235607,
    "rmse": 0.9776010181235607
}
[08/12/2025 17:12:28 INFO]: Training loss at epoch 22: 1.0449978113174438
[08/12/2025 17:12:31 INFO]: Training stats: {
    "score": -0.9808148295012004,
    "rmse": 0.9808148295012004
}
[08/12/2025 17:12:31 INFO]: Val stats: {
    "score": -0.9779742232979151,
    "rmse": 0.9779742232979151
}
[08/12/2025 17:12:31 INFO]: Test stats: {
    "score": -0.9308019281544423,
    "rmse": 0.9308019281544423
}
[08/12/2025 17:12:32 INFO]: Training loss at epoch 190: 0.9186452925205231
[08/12/2025 17:12:34 INFO]: New best epoch, val score: -0.8991223927452217
[08/12/2025 17:12:34 INFO]: Saving model to: model_best.pth
[08/12/2025 17:12:35 INFO]: Training stats: {
    "score": -0.605005944474352,
    "rmse": 0.605005944474352
}
[08/12/2025 17:12:35 INFO]: Val stats: {
    "score": -0.7748925775633095,
    "rmse": 0.7748925775633095
}
[08/12/2025 17:12:35 INFO]: Test stats: {
    "score": -0.7150592887237709,
    "rmse": 0.7150592887237709
}
[08/12/2025 17:12:45 INFO]: Training loss at epoch 191: 0.9659782648086548
[08/12/2025 17:12:46 INFO]: New best epoch, val score: -0.8987822640551157
[08/12/2025 17:12:46 INFO]: Saving model to: model_best.pth
[08/12/2025 17:12:57 INFO]: Training loss at epoch 192: 1.0300095677375793
[08/12/2025 17:12:57 INFO]: Training loss at epoch 25: 1.1230533123016357
[08/12/2025 17:13:08 INFO]: Training loss at epoch 70: 0.34463268518447876
[08/12/2025 17:13:09 INFO]: Training loss at epoch 193: 1.106087565422058
[08/12/2025 17:13:12 INFO]: New best epoch, val score: -0.7446832407519567
[08/12/2025 17:13:12 INFO]: Saving model to: model_best.pth
[08/12/2025 17:13:19 INFO]: Training loss at epoch 10: 1.0998519659042358
[08/12/2025 17:13:21 INFO]: Training loss at epoch 36: 0.8060047328472137
[08/12/2025 17:13:22 INFO]: Training loss at epoch 194: 0.8537041246891022
[08/12/2025 17:13:28 INFO]: Training loss at epoch 25: 0.7646557092666626
[08/12/2025 17:13:29 INFO]: Training loss at epoch 40: 0.9522968232631683
[08/12/2025 17:13:34 INFO]: Training loss at epoch 195: 0.8067260384559631
[08/12/2025 17:13:40 INFO]: New best epoch, val score: -0.8753225592551065
[08/12/2025 17:13:40 INFO]: Saving model to: model_best.pth
[08/12/2025 17:13:42 INFO]: Training loss at epoch 71: 0.4265381246805191
[08/12/2025 17:13:46 INFO]: New best epoch, val score: -0.7368792709733076
[08/12/2025 17:13:46 INFO]: Saving model to: model_best.pth
[08/12/2025 17:13:46 INFO]: Training loss at epoch 196: 0.9892706871032715
[08/12/2025 17:13:59 INFO]: Training loss at epoch 197: 1.091407597064972
[08/12/2025 17:14:09 INFO]: Training loss at epoch 23: 0.8490661680698395
[08/12/2025 17:14:11 INFO]: Training loss at epoch 198: 0.9648399949073792
[08/12/2025 17:14:15 INFO]: Training loss at epoch 72: 0.3527272120118141
[08/12/2025 17:14:23 INFO]: Training loss at epoch 199: 1.1549262702465057
[08/12/2025 17:14:23 INFO]: Training loss at epoch 20: 1.101156234741211
[08/12/2025 17:14:25 INFO]: Training loss at epoch 37: 0.8125677704811096
[08/12/2025 17:14:26 INFO]: Training loss at epoch 41: 1.0182937383651733
[08/12/2025 17:14:27 INFO]: Training stats: {
    "score": -0.9413908616149559,
    "rmse": 0.9413908616149559
}
[08/12/2025 17:14:27 INFO]: Val stats: {
    "score": -0.9064120504444031,
    "rmse": 0.9064120504444031
}
[08/12/2025 17:14:27 INFO]: Test stats: {
    "score": -0.8775788356793502,
    "rmse": 0.8775788356793502
}
[08/12/2025 17:14:29 INFO]: Running Final Evaluation...
[08/12/2025 17:14:30 INFO]: Training loss at epoch 26: 0.9406753182411194
[08/12/2025 17:14:48 INFO]: Training loss at epoch 73: 0.3832361698150635
[08/12/2025 17:15:01 INFO]: Training loss at epoch 26: 0.6592245846986771
[08/12/2025 17:15:12 INFO]: New best epoch, val score: -0.8582696980442867
[08/12/2025 17:15:12 INFO]: Saving model to: model_best.pth
[08/12/2025 17:15:21 INFO]: Training loss at epoch 74: 0.48832380771636963
[08/12/2025 17:15:24 INFO]: Training loss at epoch 42: 1.1681774854660034
[08/12/2025 17:15:28 INFO]: Training loss at epoch 38: 0.8814606666564941
[08/12/2025 17:15:50 INFO]: Training loss at epoch 24: 0.9599516689777374
[08/12/2025 17:15:53 INFO]: Training loss at epoch 75: 0.3869404196739197
[08/12/2025 17:15:59 INFO]: Training loss at epoch 27: 1.138625293970108
[08/12/2025 17:16:10 INFO]: New best epoch, val score: -0.9103792327429059
[08/12/2025 17:16:10 INFO]: Saving model to: model_best.pth
[08/12/2025 17:16:18 INFO]: Training loss at epoch 21: 0.8600470423698425
[08/12/2025 17:16:22 INFO]: Training loss at epoch 43: 0.9446321725845337
[08/12/2025 17:16:26 INFO]: Training loss at epoch 76: 0.4152723699808121
[08/12/2025 17:16:31 INFO]: Training loss at epoch 39: 0.6476134955883026
[08/12/2025 17:16:33 INFO]: Training loss at epoch 27: 0.8282263278961182
[08/12/2025 17:16:44 INFO]: New best epoch, val score: -0.8483152800856721
[08/12/2025 17:16:44 INFO]: Saving model to: model_best.pth
[08/12/2025 17:16:52 INFO]: Training stats: {
    "score": -0.7741845827473814,
    "rmse": 0.7741845827473814
}
[08/12/2025 17:16:52 INFO]: Val stats: {
    "score": -0.8017249814962967,
    "rmse": 0.8017249814962967
}
[08/12/2025 17:16:52 INFO]: Test stats: {
    "score": -0.7433067473406939,
    "rmse": 0.7433067473406939
}
[08/12/2025 17:16:56 INFO]: Training loss at epoch 11: 1.139937847852707
[08/12/2025 17:16:59 INFO]: Training loss at epoch 77: 0.3913290202617645
[08/12/2025 17:17:00 INFO]: New best epoch, val score: -0.8017249814962967
[08/12/2025 17:17:00 INFO]: Saving model to: model_best.pth
[08/12/2025 17:17:19 INFO]: Training loss at epoch 44: 1.0035902261734009
[08/12/2025 17:17:29 INFO]: Training loss at epoch 28: 1.1012420952320099
[08/12/2025 17:17:31 INFO]: Training loss at epoch 78: 0.3216943144798279
[08/12/2025 17:17:31 INFO]: Training loss at epoch 25: 0.9749481379985809
[08/12/2025 17:17:55 INFO]: Training loss at epoch 40: 0.6502111554145813
[08/12/2025 17:18:03 INFO]: New best epoch, val score: -0.7988432998773303
[08/12/2025 17:18:03 INFO]: Saving model to: model_best.pth
[08/12/2025 17:18:04 INFO]: Training loss at epoch 79: 0.510389432311058
[08/12/2025 17:18:05 INFO]: Training loss at epoch 28: 0.8692892789840698
[08/12/2025 17:18:12 INFO]: Training loss at epoch 22: 1.0068728923797607
[08/12/2025 17:18:16 INFO]: Training stats: {
    "score": -0.5763418328516098,
    "rmse": 0.5763418328516098
}
[08/12/2025 17:18:16 INFO]: Val stats: {
    "score": -0.7488617475350317,
    "rmse": 0.7488617475350317
}
[08/12/2025 17:18:16 INFO]: Test stats: {
    "score": -0.725666114198778,
    "rmse": 0.725666114198778
}
[08/12/2025 17:18:17 INFO]: Training loss at epoch 45: 0.9865376353263855
[08/12/2025 17:18:48 INFO]: Training loss at epoch 80: 0.45372165739536285
[08/12/2025 17:18:58 INFO]: Training loss at epoch 41: 0.7066412568092346
[08/12/2025 17:18:59 INFO]: Training loss at epoch 29: 0.8460812866687775
[08/12/2025 17:19:13 INFO]: Training loss at epoch 26: 0.987451434135437
[08/12/2025 17:19:15 INFO]: Training loss at epoch 46: 0.8996949791908264
[08/12/2025 17:19:21 INFO]: Training loss at epoch 81: 0.32449477910995483
[08/12/2025 17:19:30 INFO]: Training stats: {
    "score": -0.9439951412980324,
    "rmse": 0.9439951412980324
}
[08/12/2025 17:19:30 INFO]: Val stats: {
    "score": -0.9369960933139967,
    "rmse": 0.9369960933139967
}
[08/12/2025 17:19:30 INFO]: Test stats: {
    "score": -0.8901929819944062,
    "rmse": 0.8901929819944062
}
[08/12/2025 17:19:37 INFO]: Training loss at epoch 29: 0.8768058717250824
[08/12/2025 17:19:53 INFO]: Training loss at epoch 82: 0.33102676272392273
[08/12/2025 17:20:00 INFO]: Training loss at epoch 42: 0.6813079416751862
[08/12/2025 17:20:05 INFO]: Training loss at epoch 23: 0.9655698537826538
[08/12/2025 17:20:09 INFO]: Training stats: {
    "score": -0.869029056998224,
    "rmse": 0.869029056998224
}
[08/12/2025 17:20:09 INFO]: Val stats: {
    "score": -0.9094983282498283,
    "rmse": 0.9094983282498283
}
[08/12/2025 17:20:09 INFO]: Test stats: {
    "score": -0.8371594717546458,
    "rmse": 0.8371594717546458
}
[08/12/2025 17:20:13 INFO]: Training loss at epoch 47: 0.9538779556751251
[08/12/2025 17:20:25 INFO]: Training loss at epoch 83: 0.39157313108444214
[08/12/2025 17:20:33 INFO]: Training loss at epoch 12: 0.9364936053752899
[08/12/2025 17:20:54 INFO]: Training loss at epoch 27: 1.1491314768791199
[08/12/2025 17:20:58 INFO]: Training loss at epoch 84: 0.33644986152648926
[08/12/2025 17:20:59 INFO]: Training loss at epoch 30: 0.9153978228569031
[08/12/2025 17:21:03 INFO]: Training loss at epoch 43: 0.7256170213222504
[08/12/2025 17:21:10 INFO]: Training loss at epoch 48: 0.7738532423973083
[08/12/2025 17:21:17 INFO]: New best epoch, val score: -0.9134776733712788
[08/12/2025 17:21:17 INFO]: Saving model to: model_best.pth
[08/12/2025 17:21:30 INFO]: Training loss at epoch 85: 0.4453623592853546
[08/12/2025 17:21:35 INFO]: New best epoch, val score: -0.7271892327879569
[08/12/2025 17:21:35 INFO]: Saving model to: model_best.pth
[08/12/2025 17:21:41 INFO]: Training loss at epoch 30: 0.6894469261169434
[08/12/2025 17:21:59 INFO]: Training loss at epoch 24: 0.8842776119709015
[08/12/2025 17:22:03 INFO]: Training loss at epoch 86: 0.39815692603588104
[08/12/2025 17:22:06 INFO]: Training loss at epoch 44: 0.5977122187614441
[08/12/2025 17:22:07 INFO]: New best epoch, val score: -0.726939028964675
[08/12/2025 17:22:07 INFO]: Saving model to: model_best.pth
[08/12/2025 17:22:08 INFO]: Training loss at epoch 49: 0.9821422100067139
[08/12/2025 17:22:13 INFO]: New best epoch, val score: -0.9130865173986055
[08/12/2025 17:22:13 INFO]: Saving model to: model_best.pth
[08/12/2025 17:22:28 INFO]: Training stats: {
    "score": -0.9397288927372135,
    "rmse": 0.9397288927372135
}
[08/12/2025 17:22:28 INFO]: Val stats: {
    "score": -0.9031383870364187,
    "rmse": 0.9031383870364187
}
[08/12/2025 17:22:28 INFO]: Test stats: {
    "score": -0.8700588890730412,
    "rmse": 0.8700588890730412
}
[08/12/2025 17:22:29 INFO]: Training loss at epoch 31: 0.7850298881530762
[08/12/2025 17:22:35 INFO]: New best epoch, val score: -0.9031383870364187
[08/12/2025 17:22:35 INFO]: Saving model to: model_best.pth
[08/12/2025 17:22:35 INFO]: Training loss at epoch 28: 1.020838975906372
[08/12/2025 17:22:36 INFO]: Training loss at epoch 87: 0.3690079301595688
[08/12/2025 17:23:09 INFO]: Training loss at epoch 88: 0.44037581980228424
[08/12/2025 17:23:09 INFO]: Training loss at epoch 45: 0.6619356274604797
[08/12/2025 17:23:12 INFO]: Training loss at epoch 31: 0.7589243948459625
[08/12/2025 17:23:26 INFO]: Training loss at epoch 50: 0.8279229402542114
[08/12/2025 17:23:33 INFO]: New best epoch, val score: -0.8956236473639428
[08/12/2025 17:23:33 INFO]: Saving model to: model_best.pth
[08/12/2025 17:23:41 INFO]: Training loss at epoch 89: 0.3887229710817337
[08/12/2025 17:23:53 INFO]: Training stats: {
    "score": -0.5528951468785324,
    "rmse": 0.5528951468785324
}
[08/12/2025 17:23:53 INFO]: Val stats: {
    "score": -0.7371953748434991,
    "rmse": 0.7371953748434991
}
[08/12/2025 17:23:53 INFO]: Test stats: {
    "score": -0.7090572983360959,
    "rmse": 0.7090572983360959
}
[08/12/2025 17:23:53 INFO]: Training loss at epoch 25: 1.0452644526958466
[08/12/2025 17:23:59 INFO]: Training loss at epoch 32: 0.8499522507190704
[08/12/2025 17:24:07 INFO]: New best epoch, val score: -0.9074051369082863
[08/12/2025 17:24:07 INFO]: Saving model to: model_best.pth
[08/12/2025 17:24:10 INFO]: New best epoch, val score: -0.8871042976449683
[08/12/2025 17:24:10 INFO]: Saving model to: model_best.pth
[08/12/2025 17:24:10 INFO]: Training loss at epoch 13: 1.0894863605499268
[08/12/2025 17:24:12 INFO]: Training loss at epoch 46: 0.564397931098938
[08/12/2025 17:24:16 INFO]: Training loss at epoch 29: 1.0705682933330536
[08/12/2025 17:24:24 INFO]: Training loss at epoch 51: 0.8846322000026703
[08/12/2025 17:24:25 INFO]: Training loss at epoch 90: 0.35841551423072815
[08/12/2025 17:24:44 INFO]: Training loss at epoch 32: 0.5820369571447372
[08/12/2025 17:24:50 INFO]: Training stats: {
    "score": -0.9984811609382879,
    "rmse": 0.9984811609382879
}
[08/12/2025 17:24:50 INFO]: Val stats: {
    "score": -0.9564034802819306,
    "rmse": 0.9564034802819306
}
[08/12/2025 17:24:50 INFO]: Test stats: {
    "score": -0.9257261841852114,
    "rmse": 0.9257261841852114
}
[08/12/2025 17:24:55 INFO]: New best epoch, val score: -0.8131634922700111
[08/12/2025 17:24:55 INFO]: Saving model to: model_best.pth
[08/12/2025 17:24:58 INFO]: Training loss at epoch 91: 0.3401193469762802
[08/12/2025 17:25:15 INFO]: Training loss at epoch 47: 0.6522461771965027
[08/12/2025 17:25:22 INFO]: Training loss at epoch 52: 0.6833579242229462
[08/12/2025 17:25:29 INFO]: Training loss at epoch 33: 0.657143235206604
[08/12/2025 17:25:30 INFO]: Training loss at epoch 92: 0.4128778278827667
[08/12/2025 17:25:40 INFO]: New best epoch, val score: -0.8560361060188757
[08/12/2025 17:25:40 INFO]: Saving model to: model_best.pth
[08/12/2025 17:25:48 INFO]: Training loss at epoch 26: 0.9241752326488495
[08/12/2025 17:26:01 INFO]: New best epoch, val score: -0.9055969869970542
[08/12/2025 17:26:01 INFO]: Saving model to: model_best.pth
[08/12/2025 17:26:03 INFO]: Training loss at epoch 93: 0.312227264046669
[08/12/2025 17:26:16 INFO]: Training loss at epoch 33: 0.5949211120605469
[08/12/2025 17:26:17 INFO]: Training loss at epoch 48: 0.6181764900684357
[08/12/2025 17:26:19 INFO]: Training loss at epoch 53: 0.8007968366146088
[08/12/2025 17:26:27 INFO]: New best epoch, val score: -0.7796058462784763
[08/12/2025 17:26:27 INFO]: Saving model to: model_best.pth
[08/12/2025 17:26:32 INFO]: Training loss at epoch 30: 1.0734076499938965
[08/12/2025 17:26:36 INFO]: Training loss at epoch 94: 0.3515625
[08/12/2025 17:26:59 INFO]: Training loss at epoch 34: 0.8246934413909912
[08/12/2025 17:27:08 INFO]: Training loss at epoch 95: 0.37644046545028687
[08/12/2025 17:27:10 INFO]: New best epoch, val score: -0.8522651500111266
[08/12/2025 17:27:10 INFO]: Saving model to: model_best.pth
[08/12/2025 17:27:17 INFO]: Training loss at epoch 54: 0.7848130464553833
[08/12/2025 17:27:20 INFO]: Training loss at epoch 49: 0.7584018111228943
[08/12/2025 17:27:24 INFO]: New best epoch, val score: -0.8779762623360386
[08/12/2025 17:27:24 INFO]: Saving model to: model_best.pth
[08/12/2025 17:27:41 INFO]: Training loss at epoch 96: 0.3067232668399811
[08/12/2025 17:27:41 INFO]: Training stats: {
    "score": -0.8322027110264745,
    "rmse": 0.8322027110264745
}
[08/12/2025 17:27:41 INFO]: Val stats: {
    "score": -0.9615272556844169,
    "rmse": 0.9615272556844169
}
[08/12/2025 17:27:41 INFO]: Test stats: {
    "score": -0.8634455540835312,
    "rmse": 0.8634455540835312
}
[08/12/2025 17:27:42 INFO]: Training loss at epoch 27: 0.8295215368270874
[08/12/2025 17:27:45 INFO]: New best epoch, val score: -0.7131956075395011
[08/12/2025 17:27:45 INFO]: Saving model to: model_best.pth
[08/12/2025 17:27:47 INFO]: Training loss at epoch 14: 1.0771762132644653
[08/12/2025 17:27:48 INFO]: Training loss at epoch 34: 0.49915793538093567
[08/12/2025 17:27:59 INFO]: New best epoch, val score: -0.7772689529161089
[08/12/2025 17:27:59 INFO]: Saving model to: model_best.pth
[08/12/2025 17:28:13 INFO]: Training loss at epoch 31: 1.0551073849201202
[08/12/2025 17:28:14 INFO]: Training loss at epoch 97: 0.28196482360363007
[08/12/2025 17:28:15 INFO]: Training loss at epoch 55: 0.8038530051708221
[08/12/2025 17:28:19 INFO]: New best epoch, val score: -0.7048310803661171
[08/12/2025 17:28:19 INFO]: Saving model to: model_best.pth
[08/12/2025 17:28:22 INFO]: New best epoch, val score: -0.8610555263905318
[08/12/2025 17:28:22 INFO]: Saving model to: model_best.pth
[08/12/2025 17:28:29 INFO]: Training loss at epoch 35: 0.8297638297080994
[08/12/2025 17:28:40 INFO]: New best epoch, val score: -0.8358538117972559
[08/12/2025 17:28:40 INFO]: Saving model to: model_best.pth
[08/12/2025 17:28:45 INFO]: Training loss at epoch 50: 0.4430806636810303
[08/12/2025 17:28:47 INFO]: Training loss at epoch 98: 0.3608264625072479
[08/12/2025 17:29:13 INFO]: Training loss at epoch 56: 0.6852786839008331
[08/12/2025 17:29:19 INFO]: New best epoch, val score: -0.8522481175628975
[08/12/2025 17:29:19 INFO]: Saving model to: model_best.pth
[08/12/2025 17:29:20 INFO]: Training loss at epoch 35: 0.616062730550766
[08/12/2025 17:29:20 INFO]: Training loss at epoch 99: 0.3767978250980377
[08/12/2025 17:29:32 INFO]: Training stats: {
    "score": -0.5642378684381322,
    "rmse": 0.5642378684381322
}
[08/12/2025 17:29:32 INFO]: Val stats: {
    "score": -0.7850579156010297,
    "rmse": 0.7850579156010297
}
[08/12/2025 17:29:32 INFO]: Test stats: {
    "score": -0.7431155281385671,
    "rmse": 0.7431155281385671
}
[08/12/2025 17:29:38 INFO]: Training loss at epoch 28: 0.9524410367012024
[08/12/2025 17:29:48 INFO]: Training loss at epoch 51: 0.5665469765663147
[08/12/2025 17:29:54 INFO]: Training loss at epoch 32: 0.8731618225574493
[08/12/2025 17:29:59 INFO]: Training loss at epoch 36: 0.7775317430496216
[08/12/2025 17:30:04 INFO]: Training loss at epoch 100: 0.31316234171390533
[08/12/2025 17:30:10 INFO]: Training loss at epoch 57: 1.121105283498764
[08/12/2025 17:30:37 INFO]: Training loss at epoch 101: 0.3366320878267288
[08/12/2025 17:30:50 INFO]: Training loss at epoch 52: 0.3628096729516983
[08/12/2025 17:30:52 INFO]: Training loss at epoch 36: 0.5985834896564484
[08/12/2025 17:31:08 INFO]: Training loss at epoch 58: 0.8300206363201141
[08/12/2025 17:31:10 INFO]: Training loss at epoch 102: 0.31339961290359497
[08/12/2025 17:31:24 INFO]: Training loss at epoch 15: 0.8958715200424194
[08/12/2025 17:31:29 INFO]: Training loss at epoch 37: 0.7901105582714081
[08/12/2025 17:31:32 INFO]: Training loss at epoch 29: 0.9545150101184845
[08/12/2025 17:31:35 INFO]: Training loss at epoch 33: 0.84429731965065
[08/12/2025 17:31:42 INFO]: Training loss at epoch 103: 0.4691728353500366
[08/12/2025 17:31:50 INFO]: New best epoch, val score: -0.9363187959939393
[08/12/2025 17:31:50 INFO]: Saving model to: model_best.pth
[08/12/2025 17:31:54 INFO]: Training loss at epoch 53: 0.4530039578676224
[08/12/2025 17:32:06 INFO]: Training loss at epoch 59: 0.8250277936458588
[08/12/2025 17:32:11 INFO]: Training stats: {
    "score": -0.9504829208423284,
    "rmse": 0.9504829208423284
}
[08/12/2025 17:32:11 INFO]: Val stats: {
    "score": -0.9208783030279625,
    "rmse": 0.9208783030279625
}
[08/12/2025 17:32:11 INFO]: Test stats: {
    "score": -0.87968355936369,
    "rmse": 0.87968355936369
}
[08/12/2025 17:32:15 INFO]: Training loss at epoch 104: 0.36635836958885193
[08/12/2025 17:32:23 INFO]: Training loss at epoch 37: 0.7336222231388092
[08/12/2025 17:32:25 INFO]: Training stats: {
    "score": -0.9095515781514855,
    "rmse": 0.9095515781514855
}
[08/12/2025 17:32:25 INFO]: Val stats: {
    "score": -0.9718691291760435,
    "rmse": 0.9718691291760435
}
[08/12/2025 17:32:25 INFO]: Test stats: {
    "score": -0.8924730578102408,
    "rmse": 0.8924730578102408
}
[08/12/2025 17:32:48 INFO]: Training loss at epoch 105: 0.3855656087398529
[08/12/2025 17:32:56 INFO]: Training loss at epoch 54: 0.48870936036109924
[08/12/2025 17:32:59 INFO]: Training loss at epoch 38: 0.7773723900318146
[08/12/2025 17:33:17 INFO]: Training loss at epoch 34: 0.9995353519916534
[08/12/2025 17:33:20 INFO]: Training loss at epoch 106: 0.36929042637348175
[08/12/2025 17:33:23 INFO]: Training loss at epoch 60: 0.8857432901859283
[08/12/2025 17:33:29 INFO]: Running Final Evaluation...
[08/12/2025 17:33:52 INFO]: Training loss at epoch 107: 0.30002640932798386
[08/12/2025 17:33:54 INFO]: Training loss at epoch 38: 0.5015528798103333
[08/12/2025 17:33:57 INFO]: New best epoch, val score: -0.7036500201656277
[08/12/2025 17:33:57 INFO]: Saving model to: model_best.pth
[08/12/2025 17:33:59 INFO]: Training loss at epoch 55: 0.5183453261852264
[08/12/2025 17:34:05 INFO]: Training loss at epoch 30: 0.7434835433959961
[08/12/2025 17:34:05 INFO]: New best epoch, val score: -0.7755449533985481
[08/12/2025 17:34:05 INFO]: Saving model to: model_best.pth
[08/12/2025 17:34:19 INFO]: Training loss at epoch 61: 0.7174088358879089
[08/12/2025 17:34:25 INFO]: Training loss at epoch 108: 0.3762107640504837
[08/12/2025 17:34:26 INFO]: New best epoch, val score: -0.810535444159028
[08/12/2025 17:34:26 INFO]: Saving model to: model_best.pth
[08/12/2025 17:34:31 INFO]: Training loss at epoch 39: 0.5956647992134094
[08/12/2025 17:35:01 INFO]: Training loss at epoch 109: 0.3669183701276779
[08/12/2025 17:35:02 INFO]: Training loss at epoch 16: 0.9216305911540985
[08/12/2025 17:35:02 INFO]: Training stats: {
    "score": -0.7770271207413867,
    "rmse": 0.7770271207413867
}
[08/12/2025 17:35:02 INFO]: Val stats: {
    "score": -0.8104305462312125,
    "rmse": 0.8104305462312125
}
[08/12/2025 17:35:02 INFO]: Test stats: {
    "score": -0.7452052330092339,
    "rmse": 0.7452052330092339
}
[08/12/2025 17:35:04 INFO]: Training loss at epoch 56: 0.5319249927997589
[08/12/2025 17:35:12 INFO]: Training stats: {
    "score": -0.5231985341444899,
    "rmse": 0.5231985341444899
}
[08/12/2025 17:35:12 INFO]: Val stats: {
    "score": -0.7337126960998436,
    "rmse": 0.7337126960998436
}
[08/12/2025 17:35:12 INFO]: Test stats: {
    "score": -0.7203213995794547,
    "rmse": 0.7203213995794547
}
[08/12/2025 17:35:13 INFO]: New best epoch, val score: -0.8104305462312125
[08/12/2025 17:35:13 INFO]: Saving model to: model_best.pth
[08/12/2025 17:35:18 INFO]: Training loss at epoch 62: 0.7672905325889587
[08/12/2025 17:35:26 INFO]: Training loss at epoch 39: 0.4596959054470062
[08/12/2025 17:35:27 INFO]: New best epoch, val score: -0.9330578007668792
[08/12/2025 17:35:27 INFO]: Saving model to: model_best.pth
[08/12/2025 17:35:45 INFO]: Training loss at epoch 110: 0.3348512649536133
[08/12/2025 17:35:58 INFO]: Training stats: {
    "score": -0.7210062493811721,
    "rmse": 0.7210062493811721
}
[08/12/2025 17:35:58 INFO]: Val stats: {
    "score": -0.7773996358607967,
    "rmse": 0.7773996358607967
}
[08/12/2025 17:35:58 INFO]: Test stats: {
    "score": -0.7068562376108354,
    "rmse": 0.7068562376108354
}
[08/12/2025 17:36:01 INFO]: Training loss at epoch 31: 0.8509088158607483
[08/12/2025 17:36:07 INFO]: Training loss at epoch 57: 0.452329620718956
[08/12/2025 17:36:15 INFO]: Training loss at epoch 63: 0.7820284962654114
[08/12/2025 17:36:18 INFO]: Training loss at epoch 111: 0.278510719537735
[08/12/2025 17:36:32 INFO]: Training loss at epoch 40: 0.5563534498214722
[08/12/2025 17:36:43 INFO]: New best epoch, val score: -0.8084809221623971
[08/12/2025 17:36:43 INFO]: Saving model to: model_best.pth
[08/12/2025 17:36:51 INFO]: Training loss at epoch 112: 0.2625231519341469
[08/12/2025 17:37:10 INFO]: Training loss at epoch 58: 0.5521814823150635
[08/12/2025 17:37:12 INFO]: Training loss at epoch 64: 0.6762179434299469
[08/12/2025 17:37:23 INFO]: Training loss at epoch 113: 0.3183172643184662
[08/12/2025 17:37:28 INFO]: Training loss at epoch 40: 0.5872666239738464
[08/12/2025 17:37:55 INFO]: Training loss at epoch 32: 0.898051381111145
[08/12/2025 17:37:55 INFO]: Training loss at epoch 114: 0.4660850018262863
[08/12/2025 17:38:02 INFO]: Training loss at epoch 41: 0.6188561618328094
[08/12/2025 17:38:09 INFO]: Training loss at epoch 65: 0.6958940029144287
[08/12/2025 17:38:12 INFO]: Training loss at epoch 59: 0.4398297965526581
[08/12/2025 17:38:15 INFO]: New best epoch, val score: -0.8043752994549797
[08/12/2025 17:38:15 INFO]: Saving model to: model_best.pth
[08/12/2025 17:38:28 INFO]: Training loss at epoch 115: 0.37128861248493195
[08/12/2025 17:38:33 INFO]: Training stats: {
    "score": -0.7590204656906394,
    "rmse": 0.7590204656906394
}
[08/12/2025 17:38:33 INFO]: Val stats: {
    "score": -0.8074531347815911,
    "rmse": 0.8074531347815911
}
[08/12/2025 17:38:33 INFO]: Test stats: {
    "score": -0.822949905730358,
    "rmse": 0.822949905730358
}
[08/12/2025 17:38:36 INFO]: Training loss at epoch 17: 0.7825019955635071
[08/12/2025 17:38:58 INFO]: Training loss at epoch 41: 0.4939758628606796
[08/12/2025 17:39:00 INFO]: Training loss at epoch 116: 0.2893376648426056
[08/12/2025 17:39:06 INFO]: Training loss at epoch 66: 0.7485135197639465
[08/12/2025 17:39:31 INFO]: Training loss at epoch 42: 0.4725028723478317
[08/12/2025 17:39:32 INFO]: Training loss at epoch 117: 0.2905874699354172
[08/12/2025 17:39:36 INFO]: Training loss at epoch 60: 0.5143378376960754
[08/12/2025 17:39:49 INFO]: Training loss at epoch 33: 0.9387593269348145
[08/12/2025 17:40:02 INFO]: New best epoch, val score: -0.9026994138917396
[08/12/2025 17:40:02 INFO]: Saving model to: model_best.pth
[08/12/2025 17:40:03 INFO]: Training loss at epoch 67: 0.7986417412757874
[08/12/2025 17:40:05 INFO]: Training loss at epoch 118: 0.31022901833057404
[08/12/2025 17:40:28 INFO]: Training loss at epoch 42: 0.40480904281139374
[08/12/2025 17:40:37 INFO]: Training loss at epoch 119: 0.27893422544002533
[08/12/2025 17:40:38 INFO]: Training loss at epoch 61: 0.6414041221141815
[08/12/2025 17:40:49 INFO]: Training stats: {
    "score": -0.5221133068174934,
    "rmse": 0.5221133068174934
}
[08/12/2025 17:40:49 INFO]: Val stats: {
    "score": -0.7492643849374402,
    "rmse": 0.7492643849374402
}
[08/12/2025 17:40:49 INFO]: Test stats: {
    "score": -0.7201404275761666,
    "rmse": 0.7201404275761666
}
[08/12/2025 17:40:59 INFO]: Training loss at epoch 68: 0.9002975523471832
[08/12/2025 17:41:00 INFO]: Training loss at epoch 43: 0.5756820738315582
[08/12/2025 17:41:21 INFO]: Training loss at epoch 120: 0.25624779611825943
[08/12/2025 17:41:41 INFO]: Training loss at epoch 62: 0.5204465389251709
[08/12/2025 17:41:43 INFO]: Training loss at epoch 34: 0.9178087115287781
[08/12/2025 17:41:54 INFO]: Training loss at epoch 121: 0.3196689039468765
[08/12/2025 17:41:55 INFO]: Training loss at epoch 69: 0.5634610950946808
[08/12/2025 17:41:56 INFO]: New best epoch, val score: -0.8789704414777768
[08/12/2025 17:41:56 INFO]: Saving model to: model_best.pth
[08/12/2025 17:41:57 INFO]: Training loss at epoch 43: 0.46764394640922546
[08/12/2025 17:41:58 INFO]: New best epoch, val score: -0.6933729307720452
[08/12/2025 17:41:58 INFO]: Saving model to: model_best.pth
[08/12/2025 17:42:09 INFO]: Training loss at epoch 18: 0.9117232263088226
[08/12/2025 17:42:14 INFO]: Training stats: {
    "score": -0.7976926840360284,
    "rmse": 0.7976926840360284
}
[08/12/2025 17:42:14 INFO]: Val stats: {
    "score": -0.8445066756037333,
    "rmse": 0.8445066756037333
}
[08/12/2025 17:42:14 INFO]: Test stats: {
    "score": -0.7818232546731338,
    "rmse": 0.7818232546731338
}
[08/12/2025 17:42:27 INFO]: Training loss at epoch 122: 0.3381928503513336
[08/12/2025 17:42:30 INFO]: Training loss at epoch 44: 0.552192360162735
[08/12/2025 17:42:31 INFO]: New best epoch, val score: -0.6926497897376079
[08/12/2025 17:42:31 INFO]: Saving model to: model_best.pth
[08/12/2025 17:42:43 INFO]: Training loss at epoch 63: 0.3902047872543335
[08/12/2025 17:42:59 INFO]: Training loss at epoch 123: 0.2604702487587929
[08/12/2025 17:43:11 INFO]: Training loss at epoch 70: 0.6266821026802063
[08/12/2025 17:43:27 INFO]: Training loss at epoch 44: 0.5268566012382507
[08/12/2025 17:43:32 INFO]: Training loss at epoch 124: 0.3349110335111618
[08/12/2025 17:43:36 INFO]: Training loss at epoch 35: 1.0500713884830475
[08/12/2025 17:43:46 INFO]: Training loss at epoch 64: 0.5437861382961273
[08/12/2025 17:43:50 INFO]: New best epoch, val score: -0.8761104710549493
[08/12/2025 17:43:50 INFO]: Saving model to: model_best.pth
[08/12/2025 17:43:54 INFO]: New best epoch, val score: -0.7931293010806507
[08/12/2025 17:43:54 INFO]: Saving model to: model_best.pth
[08/12/2025 17:44:00 INFO]: Training loss at epoch 45: 0.5552651882171631
[08/12/2025 17:44:05 INFO]: Training loss at epoch 125: 0.3944963961839676
[08/12/2025 17:44:08 INFO]: Training loss at epoch 71: 0.7970969974994659
[08/12/2025 17:44:11 INFO]: New best epoch, val score: -0.7909898282374905
[08/12/2025 17:44:11 INFO]: Saving model to: model_best.pth
[08/12/2025 17:44:38 INFO]: Training loss at epoch 126: 0.30583274364471436
[08/12/2025 17:44:50 INFO]: Training loss at epoch 65: 0.49724170565605164
[08/12/2025 17:44:58 INFO]: Training loss at epoch 45: 0.4286428987979889
[08/12/2025 17:45:05 INFO]: Training loss at epoch 72: 0.5433565080165863
[08/12/2025 17:45:09 INFO]: New best epoch, val score: -0.7519128832062069
[08/12/2025 17:45:09 INFO]: Saving model to: model_best.pth
[08/12/2025 17:45:10 INFO]: Training loss at epoch 127: 0.2667919918894768
[08/12/2025 17:45:30 INFO]: Training loss at epoch 46: 0.4728124439716339
[08/12/2025 17:45:32 INFO]: Training loss at epoch 36: 0.76032754778862
[08/12/2025 17:45:42 INFO]: Training loss at epoch 19: 0.9147601127624512
[08/12/2025 17:45:43 INFO]: Training loss at epoch 128: 0.3130065053701401
[08/12/2025 17:45:53 INFO]: Training loss at epoch 66: 0.49178558588027954
[08/12/2025 17:46:02 INFO]: Training loss at epoch 73: 0.678975909948349
[08/12/2025 17:46:16 INFO]: Training loss at epoch 129: 0.20322509109973907
[08/12/2025 17:46:27 INFO]: Training stats: {
    "score": -0.48616183285133246,
    "rmse": 0.48616183285133246
}
[08/12/2025 17:46:27 INFO]: Val stats: {
    "score": -0.690700603334065,
    "rmse": 0.690700603334065
}
[08/12/2025 17:46:27 INFO]: Test stats: {
    "score": -0.6878443451079979,
    "rmse": 0.6878443451079979
}
[08/12/2025 17:46:28 INFO]: Training loss at epoch 46: 0.423391655087471
[08/12/2025 17:46:32 INFO]: New best epoch, val score: -0.690700603334065
[08/12/2025 17:46:32 INFO]: Saving model to: model_best.pth
[08/12/2025 17:46:38 INFO]: New best epoch, val score: -0.7370763329850777
[08/12/2025 17:46:38 INFO]: Saving model to: model_best.pth
[08/12/2025 17:46:54 INFO]: Training stats: {
    "score": -0.9812945537605875,
    "rmse": 0.9812945537605875
}
[08/12/2025 17:46:54 INFO]: Val stats: {
    "score": -0.9450253793509638,
    "rmse": 0.9450253793509638
}
[08/12/2025 17:46:54 INFO]: Test stats: {
    "score": -0.9102676194581932,
    "rmse": 0.9102676194581932
}
[08/12/2025 17:46:55 INFO]: Training loss at epoch 67: 0.5266787111759186
[08/12/2025 17:46:58 INFO]: Training loss at epoch 74: 0.5384500920772552
[08/12/2025 17:46:59 INFO]: Training loss at epoch 47: 0.4245958924293518
[08/12/2025 17:47:00 INFO]: Training loss at epoch 130: 0.27870385348796844
[08/12/2025 17:47:25 INFO]: Training loss at epoch 37: 0.7566220164299011
[08/12/2025 17:47:32 INFO]: Training loss at epoch 131: 0.3621253967285156
[08/12/2025 17:47:54 INFO]: Training loss at epoch 75: 0.654342532157898
[08/12/2025 17:47:57 INFO]: Training loss at epoch 47: 0.4836399108171463
[08/12/2025 17:47:58 INFO]: Training loss at epoch 68: 0.391349196434021
[08/12/2025 17:48:05 INFO]: Training loss at epoch 132: 0.2987288385629654
[08/12/2025 17:48:05 INFO]: New best epoch, val score: -0.7828647718155584
[08/12/2025 17:48:05 INFO]: Saving model to: model_best.pth
[08/12/2025 17:48:09 INFO]: New best epoch, val score: -0.6805086780591353
[08/12/2025 17:48:09 INFO]: Saving model to: model_best.pth
[08/12/2025 17:48:28 INFO]: Training loss at epoch 48: 0.5342232882976532
[08/12/2025 17:48:37 INFO]: Training loss at epoch 133: 0.2011362724006176
[08/12/2025 17:48:39 INFO]: New best epoch, val score: -0.7712071764607856
[08/12/2025 17:48:39 INFO]: Saving model to: model_best.pth
[08/12/2025 17:48:52 INFO]: Training loss at epoch 76: 0.5828090012073517
[08/12/2025 17:49:01 INFO]: Training loss at epoch 69: 0.461959645152092
[08/12/2025 17:49:10 INFO]: Training loss at epoch 134: 0.30304157733917236
[08/12/2025 17:49:14 INFO]: New best epoch, val score: -0.6763214039488247
[08/12/2025 17:49:14 INFO]: Saving model to: model_best.pth
[08/12/2025 17:49:19 INFO]: Training loss at epoch 38: 0.6442406475543976
[08/12/2025 17:49:22 INFO]: Training stats: {
    "score": -0.6800859273111779,
    "rmse": 0.6800859273111779
}
[08/12/2025 17:49:22 INFO]: Val stats: {
    "score": -0.792842355083134,
    "rmse": 0.792842355083134
}
[08/12/2025 17:49:22 INFO]: Test stats: {
    "score": -0.7390692956065864,
    "rmse": 0.7390692956065864
}
[08/12/2025 17:49:27 INFO]: Training loss at epoch 48: 0.5563225448131561
[08/12/2025 17:49:43 INFO]: Training loss at epoch 135: 0.24712885171175003
[08/12/2025 17:49:48 INFO]: Training loss at epoch 77: 0.5948072373867035
[08/12/2025 17:49:58 INFO]: Training loss at epoch 49: 0.5618626773357391
[08/12/2025 17:50:15 INFO]: Training loss at epoch 136: 0.27799779176712036
[08/12/2025 17:50:24 INFO]: Training loss at epoch 70: 0.4296039193868637
[08/12/2025 17:50:28 INFO]: Training loss at epoch 20: 1.0060104131698608
[08/12/2025 17:50:29 INFO]: Training stats: {
    "score": -0.7085868036763098,
    "rmse": 0.7085868036763098
}
[08/12/2025 17:50:29 INFO]: Val stats: {
    "score": -0.7453334263267487,
    "rmse": 0.7453334263267487
}
[08/12/2025 17:50:29 INFO]: Test stats: {
    "score": -0.6929143014441252,
    "rmse": 0.6929143014441252
}
[08/12/2025 17:50:40 INFO]: New best epoch, val score: -0.7453334263267487
[08/12/2025 17:50:40 INFO]: Saving model to: model_best.pth
[08/12/2025 17:50:45 INFO]: Training loss at epoch 78: 0.7090823352336884
[08/12/2025 17:50:48 INFO]: Training loss at epoch 137: 0.2757023796439171
[08/12/2025 17:50:57 INFO]: Training loss at epoch 49: 0.4253484755754471
[08/12/2025 17:51:12 INFO]: Training loss at epoch 39: 1.0315421521663666
[08/12/2025 17:51:20 INFO]: Training loss at epoch 138: 0.3493814021348953
[08/12/2025 17:51:27 INFO]: Training loss at epoch 71: 0.42850178480148315
[08/12/2025 17:51:28 INFO]: Training stats: {
    "score": -0.6586221916191956,
    "rmse": 0.6586221916191956
}
[08/12/2025 17:51:28 INFO]: Val stats: {
    "score": -0.7750270799125959,
    "rmse": 0.7750270799125959
}
[08/12/2025 17:51:28 INFO]: Test stats: {
    "score": -0.6838526027727545,
    "rmse": 0.6838526027727545
}
[08/12/2025 17:51:41 INFO]: Training loss at epoch 79: 0.5610487759113312
[08/12/2025 17:51:51 INFO]: Training stats: {
    "score": -0.8412768302717925,
    "rmse": 0.8412768302717925
}
[08/12/2025 17:51:51 INFO]: Val stats: {
    "score": -0.8544580712751362,
    "rmse": 0.8544580712751362
}
[08/12/2025 17:51:51 INFO]: Test stats: {
    "score": -0.7890485789518148,
    "rmse": 0.7890485789518148
}
[08/12/2025 17:51:53 INFO]: Training loss at epoch 139: 0.2875269204378128
[08/12/2025 17:51:59 INFO]: Training loss at epoch 50: 0.4917736202478409
[08/12/2025 17:52:00 INFO]: Training stats: {
    "score": -0.7549017340072107,
    "rmse": 0.7549017340072107
}
[08/12/2025 17:52:00 INFO]: Val stats: {
    "score": -0.803323922968517,
    "rmse": 0.803323922968517
}
[08/12/2025 17:52:00 INFO]: Test stats: {
    "score": -0.733213399225455,
    "rmse": 0.733213399225455
}
[08/12/2025 17:52:04 INFO]: Training stats: {
    "score": -0.4681749912746787,
    "rmse": 0.4681749912746787
}
[08/12/2025 17:52:04 INFO]: Val stats: {
    "score": -0.6797914868679085,
    "rmse": 0.6797914868679085
}
[08/12/2025 17:52:04 INFO]: Test stats: {
    "score": -0.6779767988958804,
    "rmse": 0.6779767988958804
}
[08/12/2025 17:52:04 INFO]: New best epoch, val score: -0.8544580712751362
[08/12/2025 17:52:04 INFO]: Saving model to: model_best.pth
[08/12/2025 17:52:07 INFO]: New best epoch, val score: -0.803323922968517
[08/12/2025 17:52:07 INFO]: Saving model to: model_best.pth
[08/12/2025 17:52:29 INFO]: Training loss at epoch 72: 0.41613711416721344
[08/12/2025 17:52:37 INFO]: Training loss at epoch 140: 0.3359347879886627
[08/12/2025 17:52:57 INFO]: Training loss at epoch 80: 0.5231172144412994
[08/12/2025 17:52:58 INFO]: Training loss at epoch 50: 0.49683381617069244
[08/12/2025 17:53:04 INFO]: New best epoch, val score: -0.8028755380429198
[08/12/2025 17:53:04 INFO]: Saving model to: model_best.pth
[08/12/2025 17:53:09 INFO]: Training loss at epoch 141: 0.3180634528398514
[08/12/2025 17:53:28 INFO]: Training loss at epoch 51: 0.5077251642942429
[08/12/2025 17:53:32 INFO]: Training loss at epoch 73: 0.41400574147701263
[08/12/2025 17:53:42 INFO]: Training loss at epoch 142: 0.3265220373868942
[08/12/2025 17:53:45 INFO]: Training loss at epoch 40: 0.9303675293922424
[08/12/2025 17:53:54 INFO]: Training loss at epoch 81: 0.5320676267147064
[08/12/2025 17:54:01 INFO]: Training loss at epoch 21: 0.9024704694747925
[08/12/2025 17:54:14 INFO]: Training loss at epoch 143: 0.3294917643070221
[08/12/2025 17:54:18 INFO]: New best epoch, val score: -0.6697118458802671
[08/12/2025 17:54:18 INFO]: Saving model to: model_best.pth
[08/12/2025 17:54:27 INFO]: New best epoch, val score: -0.9318891189312362
[08/12/2025 17:54:27 INFO]: Saving model to: model_best.pth
[08/12/2025 17:54:28 INFO]: Training loss at epoch 51: 0.4208638221025467
[08/12/2025 17:54:35 INFO]: Training loss at epoch 74: 0.3526615798473358
[08/12/2025 17:54:39 INFO]: New best epoch, val score: -0.7339571517337308
[08/12/2025 17:54:39 INFO]: Saving model to: model_best.pth
[08/12/2025 17:54:47 INFO]: Training loss at epoch 144: 0.27260228991508484
[08/12/2025 17:54:51 INFO]: Training loss at epoch 82: 0.5768686980009079
[08/12/2025 17:54:58 INFO]: Training loss at epoch 52: 0.6214032173156738
[08/12/2025 17:55:19 INFO]: Training loss at epoch 145: 0.2480083927512169
[08/12/2025 17:55:37 INFO]: Training loss at epoch 75: 0.4853750169277191
[08/12/2025 17:55:39 INFO]: Training loss at epoch 41: 1.0072475373744965
[08/12/2025 17:55:47 INFO]: Training loss at epoch 83: 0.40525607764720917
[08/12/2025 17:55:52 INFO]: Training loss at epoch 146: 0.30111952126026154
[08/12/2025 17:55:59 INFO]: Training loss at epoch 52: 0.36629724502563477
[08/12/2025 17:56:10 INFO]: New best epoch, val score: -0.7179699051500043
[08/12/2025 17:56:10 INFO]: Saving model to: model_best.pth
[08/12/2025 17:56:24 INFO]: Training loss at epoch 147: 0.27137590944767
[08/12/2025 17:56:27 INFO]: Training loss at epoch 53: 0.3450378403067589
[08/12/2025 17:56:38 INFO]: New best epoch, val score: -0.7278929397821544
[08/12/2025 17:56:38 INFO]: Saving model to: model_best.pth
[08/12/2025 17:56:40 INFO]: Training loss at epoch 76: 0.38750849664211273
[08/12/2025 17:56:44 INFO]: Training loss at epoch 84: 0.5516172647476196
[08/12/2025 17:56:57 INFO]: Training loss at epoch 148: 0.2742583900690079
[08/12/2025 17:57:29 INFO]: Training loss at epoch 149: 0.19977334141731262
[08/12/2025 17:57:30 INFO]: Training loss at epoch 53: 0.38299836218357086
[08/12/2025 17:57:33 INFO]: Training loss at epoch 42: 0.6627032160758972
[08/12/2025 17:57:36 INFO]: Training loss at epoch 22: 0.9971091151237488
[08/12/2025 17:57:41 INFO]: Training stats: {
    "score": -0.46256715013965727,
    "rmse": 0.46256715013965727
}
[08/12/2025 17:57:41 INFO]: Val stats: {
    "score": -0.7007183125887689,
    "rmse": 0.7007183125887689
}
[08/12/2025 17:57:41 INFO]: Test stats: {
    "score": -0.7019190862508259,
    "rmse": 0.7019190862508259
}
[08/12/2025 17:57:44 INFO]: Training loss at epoch 77: 0.4762174040079117
[08/12/2025 17:57:47 INFO]: Training loss at epoch 85: 0.5103636980056763
[08/12/2025 17:57:57 INFO]: Training loss at epoch 54: 0.5588670969009399
[08/12/2025 17:58:02 INFO]: New best epoch, val score: -0.9231077223366914
[08/12/2025 17:58:02 INFO]: Saving model to: model_best.pth
[08/12/2025 17:58:09 INFO]: New best epoch, val score: -0.7273933661708287
[08/12/2025 17:58:09 INFO]: Saving model to: model_best.pth
[08/12/2025 17:58:15 INFO]: Training loss at epoch 150: 0.2890387997031212
[08/12/2025 17:58:45 INFO]: Training loss at epoch 86: 0.5231987833976746
[08/12/2025 17:58:47 INFO]: Training loss at epoch 151: 0.2403343766927719
[08/12/2025 17:58:48 INFO]: Training loss at epoch 78: 0.4217274487018585
[08/12/2025 17:59:02 INFO]: Training loss at epoch 54: 0.4352593421936035
[08/12/2025 17:59:19 INFO]: Training loss at epoch 152: 0.2849823832511902
[08/12/2025 17:59:26 INFO]: Training loss at epoch 43: 0.7301766872406006
[08/12/2025 17:59:27 INFO]: Training loss at epoch 55: 0.533077597618103
[08/12/2025 17:59:40 INFO]: New best epoch, val score: -0.8290610324588147
[08/12/2025 17:59:40 INFO]: Saving model to: model_best.pth
[08/12/2025 17:59:43 INFO]: Training loss at epoch 87: 0.37904995679855347
[08/12/2025 17:59:51 INFO]: Training loss at epoch 153: 0.3336050510406494
[08/12/2025 17:59:52 INFO]: Training loss at epoch 79: 0.46669530868530273
[08/12/2025 18:00:13 INFO]: Training stats: {
    "score": -0.7372457479831185,
    "rmse": 0.7372457479831185
}
[08/12/2025 18:00:13 INFO]: Val stats: {
    "score": -0.8945770896283322,
    "rmse": 0.8945770896283322
}
[08/12/2025 18:00:13 INFO]: Test stats: {
    "score": -0.8344949147366565,
    "rmse": 0.8344949147366565
}
[08/12/2025 18:00:24 INFO]: Training loss at epoch 154: 0.22652661800384521
[08/12/2025 18:00:32 INFO]: Training loss at epoch 55: 0.5017910152673721
[08/12/2025 18:00:39 INFO]: Training loss at epoch 88: 0.545900285243988
[08/12/2025 18:00:56 INFO]: Training loss at epoch 155: 0.31072065234184265
[08/12/2025 18:00:56 INFO]: Training loss at epoch 56: 0.4926629364490509
[08/12/2025 18:01:11 INFO]: Training loss at epoch 23: 0.9910274744033813
[08/12/2025 18:01:16 INFO]: Training loss at epoch 80: 0.35993462800979614
[08/12/2025 18:01:20 INFO]: Training loss at epoch 44: 0.6072132587432861
[08/12/2025 18:01:29 INFO]: Training loss at epoch 156: 0.3028203248977661
[08/12/2025 18:01:34 INFO]: New best epoch, val score: -0.805178930634918
[08/12/2025 18:01:34 INFO]: Saving model to: model_best.pth
[08/12/2025 18:01:36 INFO]: Training loss at epoch 89: 0.4009810984134674
[08/12/2025 18:01:36 INFO]: New best epoch, val score: -0.9203287030205182
[08/12/2025 18:01:36 INFO]: Saving model to: model_best.pth
[08/12/2025 18:01:55 INFO]: Training stats: {
    "score": -0.7244777372692162,
    "rmse": 0.7244777372692162
}
[08/12/2025 18:01:55 INFO]: Val stats: {
    "score": -0.8272718935296849,
    "rmse": 0.8272718935296849
}
[08/12/2025 18:01:55 INFO]: Test stats: {
    "score": -0.7323735385962707,
    "rmse": 0.7323735385962707
}
[08/12/2025 18:02:02 INFO]: Training loss at epoch 157: 0.2647877186536789
[08/12/2025 18:02:02 INFO]: Training loss at epoch 56: 0.27443309128284454
[08/12/2025 18:02:13 INFO]: New best epoch, val score: -0.7131177932894112
[08/12/2025 18:02:13 INFO]: Saving model to: model_best.pth
[08/12/2025 18:02:19 INFO]: Training loss at epoch 81: 0.3884696364402771
[08/12/2025 18:02:26 INFO]: Training loss at epoch 57: 0.5531542897224426
[08/12/2025 18:02:34 INFO]: Training loss at epoch 158: 0.274711549282074
[08/12/2025 18:02:38 INFO]: New best epoch, val score: -0.6680382958539373
[08/12/2025 18:02:38 INFO]: Saving model to: model_best.pth
[08/12/2025 18:02:52 INFO]: Training loss at epoch 90: 0.426469087600708
[08/12/2025 18:03:06 INFO]: Training loss at epoch 159: 0.26084938645362854
[08/12/2025 18:03:14 INFO]: Training loss at epoch 45: 0.5402203798294067
[08/12/2025 18:03:18 INFO]: Training stats: {
    "score": -0.47097246482968047,
    "rmse": 0.47097246482968047
}
[08/12/2025 18:03:18 INFO]: Val stats: {
    "score": -0.6689683206750056,
    "rmse": 0.6689683206750056
}
[08/12/2025 18:03:18 INFO]: Test stats: {
    "score": -0.6874688960710479,
    "rmse": 0.6874688960710479
}
[08/12/2025 18:03:22 INFO]: Training loss at epoch 82: 0.4217926114797592
[08/12/2025 18:03:32 INFO]: Training loss at epoch 57: 0.41509342193603516
[08/12/2025 18:03:42 INFO]: New best epoch, val score: -0.7063436735472588
[08/12/2025 18:03:42 INFO]: Saving model to: model_best.pth
[08/12/2025 18:03:48 INFO]: Training loss at epoch 91: 0.523872971534729
[08/12/2025 18:03:50 INFO]: Training loss at epoch 160: 0.22338557243347168
[08/12/2025 18:03:55 INFO]: Training loss at epoch 58: 0.48938728868961334
[08/12/2025 18:04:06 INFO]: New best epoch, val score: -0.7203663433129477
[08/12/2025 18:04:06 INFO]: Saving model to: model_best.pth
[08/12/2025 18:04:23 INFO]: Training loss at epoch 161: 0.2703695744276047
[08/12/2025 18:04:25 INFO]: Training loss at epoch 83: 0.3563317209482193
[08/12/2025 18:04:45 INFO]: Training loss at epoch 92: 0.4654330164194107
[08/12/2025 18:04:45 INFO]: Training loss at epoch 24: 0.83839151263237
[08/12/2025 18:04:56 INFO]: Training loss at epoch 162: 0.31694161891937256
[08/12/2025 18:05:01 INFO]: Training loss at epoch 58: 0.3777868449687958
[08/12/2025 18:05:08 INFO]: Training loss at epoch 46: 0.4572497010231018
[08/12/2025 18:05:25 INFO]: Training loss at epoch 59: 0.46174994111061096
[08/12/2025 18:05:27 INFO]: Training loss at epoch 84: 0.4400140345096588
[08/12/2025 18:05:28 INFO]: Training loss at epoch 163: 0.239780992269516
[08/12/2025 18:05:41 INFO]: Training loss at epoch 93: 0.49686838686466217
[08/12/2025 18:05:56 INFO]: Training stats: {
    "score": -0.6838648929381045,
    "rmse": 0.6838648929381045
}
[08/12/2025 18:05:56 INFO]: Val stats: {
    "score": -0.7188678326224608,
    "rmse": 0.7188678326224608
}
[08/12/2025 18:05:56 INFO]: Test stats: {
    "score": -0.6816964713782534,
    "rmse": 0.6816964713782534
}
[08/12/2025 18:06:01 INFO]: Training loss at epoch 164: 0.29945365339517593
[08/12/2025 18:06:05 INFO]: New best epoch, val score: -0.664662596938547
[08/12/2025 18:06:05 INFO]: Saving model to: model_best.pth
[08/12/2025 18:06:07 INFO]: New best epoch, val score: -0.7188678326224608
[08/12/2025 18:06:07 INFO]: Saving model to: model_best.pth
[08/12/2025 18:06:31 INFO]: Training loss at epoch 85: 0.33873286843299866
[08/12/2025 18:06:31 INFO]: Training loss at epoch 59: 0.42809776961803436
[08/12/2025 18:06:34 INFO]: Training loss at epoch 165: 0.22473958879709244
[08/12/2025 18:06:38 INFO]: Training loss at epoch 94: 0.4184761345386505
[08/12/2025 18:06:45 INFO]: New best epoch, val score: -0.7779258700811269
[08/12/2025 18:06:45 INFO]: Saving model to: model_best.pth
[08/12/2025 18:07:03 INFO]: Training loss at epoch 47: 0.7112447619438171
[08/12/2025 18:07:04 INFO]: Training stats: {
    "score": -0.6339252364494928,
    "rmse": 0.6339252364494928
}
[08/12/2025 18:07:04 INFO]: Val stats: {
    "score": -0.7891642266301775,
    "rmse": 0.7891642266301775
}
[08/12/2025 18:07:04 INFO]: Test stats: {
    "score": -0.6943842868004738,
    "rmse": 0.6943842868004738
}
[08/12/2025 18:07:07 INFO]: Training loss at epoch 166: 0.2183115854859352
[08/12/2025 18:07:27 INFO]: Training loss at epoch 60: 0.371841236948967
[08/12/2025 18:07:34 INFO]: Training loss at epoch 86: 0.38378138840198517
[08/12/2025 18:07:36 INFO]: Training loss at epoch 95: 0.4077887386083603
[08/12/2025 18:07:39 INFO]: Training loss at epoch 167: 0.2441227212548256
[08/12/2025 18:07:42 INFO]: New best epoch, val score: -0.7760471966183486
[08/12/2025 18:07:42 INFO]: Saving model to: model_best.pth
[08/12/2025 18:08:12 INFO]: Training loss at epoch 168: 0.21925338357686996
[08/12/2025 18:08:19 INFO]: Training loss at epoch 25: 0.9885717332363129
[08/12/2025 18:08:32 INFO]: Training loss at epoch 96: 0.42955872416496277
[08/12/2025 18:08:34 INFO]: Training loss at epoch 60: 0.4149849861860275
[08/12/2025 18:08:37 INFO]: Training loss at epoch 87: 0.40915098786354065
[08/12/2025 18:08:44 INFO]: Training loss at epoch 169: 0.1942124217748642
[08/12/2025 18:08:44 INFO]: New best epoch, val score: -0.7014049961920664
[08/12/2025 18:08:44 INFO]: Saving model to: model_best.pth
[08/12/2025 18:08:56 INFO]: Training loss at epoch 48: 0.6484946012496948
[08/12/2025 18:08:56 INFO]: Training stats: {
    "score": -0.4687720086620901,
    "rmse": 0.4687720086620901
}
[08/12/2025 18:08:56 INFO]: Val stats: {
    "score": -0.6884913744526395,
    "rmse": 0.6884913744526395
}
[08/12/2025 18:08:56 INFO]: Test stats: {
    "score": -0.707906867331831,
    "rmse": 0.707906867331831
}
[08/12/2025 18:08:56 INFO]: Training loss at epoch 61: 0.43565361201763153
[08/12/2025 18:09:29 INFO]: Training loss at epoch 170: 0.285947248339653
[08/12/2025 18:09:29 INFO]: Training loss at epoch 97: 0.36271868646144867
[08/12/2025 18:09:39 INFO]: Training loss at epoch 88: 0.4656153470277786
[08/12/2025 18:10:01 INFO]: Training loss at epoch 171: 0.25072481483221054
[08/12/2025 18:10:04 INFO]: Training loss at epoch 61: 0.34306491911411285
[08/12/2025 18:10:15 INFO]: New best epoch, val score: -0.6931440322356915
[08/12/2025 18:10:15 INFO]: Saving model to: model_best.pth
[08/12/2025 18:10:26 INFO]: Training loss at epoch 62: 0.5449346601963043
[08/12/2025 18:10:26 INFO]: Training loss at epoch 98: 0.4242965131998062
[08/12/2025 18:10:34 INFO]: Training loss at epoch 172: 0.24926788359880447
[08/12/2025 18:10:42 INFO]: Training loss at epoch 89: 0.4279125928878784
[08/12/2025 18:10:50 INFO]: Training loss at epoch 49: 0.5767411887645721
[08/12/2025 18:11:03 INFO]: Training stats: {
    "score": -0.6513115936961276,
    "rmse": 0.6513115936961276
}
[08/12/2025 18:11:03 INFO]: Val stats: {
    "score": -0.793163247715949,
    "rmse": 0.793163247715949
}
[08/12/2025 18:11:03 INFO]: Test stats: {
    "score": -0.7560905236371798,
    "rmse": 0.7560905236371798
}
[08/12/2025 18:11:06 INFO]: Training loss at epoch 173: 0.20684444159269333
[08/12/2025 18:11:22 INFO]: Training loss at epoch 99: 0.3299436494708061
[08/12/2025 18:11:28 INFO]: Training stats: {
    "score": -0.8246948152006885,
    "rmse": 0.8246948152006885
}
[08/12/2025 18:11:28 INFO]: Val stats: {
    "score": -0.9435896636233756,
    "rmse": 0.9435896636233756
}
[08/12/2025 18:11:28 INFO]: Test stats: {
    "score": -0.8368788906588562,
    "rmse": 0.8368788906588562
}
[08/12/2025 18:11:34 INFO]: Training loss at epoch 62: 0.3475010097026825
[08/12/2025 18:11:38 INFO]: Training loss at epoch 174: 0.2220577970147133
[08/12/2025 18:11:41 INFO]: Training stats: {
    "score": -0.6592133387093005,
    "rmse": 0.6592133387093005
}
[08/12/2025 18:11:41 INFO]: Val stats: {
    "score": -0.7797765615402998,
    "rmse": 0.7797765615402998
}
[08/12/2025 18:11:41 INFO]: Test stats: {
    "score": -0.6814823096754081,
    "rmse": 0.6814823096754081
}
[08/12/2025 18:11:42 INFO]: New best epoch, val score: -0.6600842648699469
[08/12/2025 18:11:42 INFO]: Saving model to: model_best.pth
[08/12/2025 18:11:53 INFO]: Training loss at epoch 26: 0.9898492991924286
[08/12/2025 18:11:55 INFO]: Training loss at epoch 63: 0.3558248281478882
[08/12/2025 18:12:05 INFO]: Training loss at epoch 90: 0.3193725496530533
[08/12/2025 18:12:06 INFO]: New best epoch, val score: -0.7156196006818601
[08/12/2025 18:12:06 INFO]: Saving model to: model_best.pth
[08/12/2025 18:12:11 INFO]: Training loss at epoch 175: 0.22403236478567123
[08/12/2025 18:12:13 INFO]: New best epoch, val score: -0.7734968465790847
[08/12/2025 18:12:13 INFO]: Saving model to: model_best.pth
[08/12/2025 18:12:38 INFO]: Training loss at epoch 100: 0.549412190914154
[08/12/2025 18:12:44 INFO]: Training loss at epoch 176: 0.23434241116046906
[08/12/2025 18:13:04 INFO]: Training loss at epoch 63: 0.32835324108600616
[08/12/2025 18:13:08 INFO]: Training loss at epoch 91: 0.33090998977422714
[08/12/2025 18:13:16 INFO]: New best epoch, val score: -0.7487499136785593
[08/12/2025 18:13:16 INFO]: Saving model to: model_best.pth
[08/12/2025 18:13:16 INFO]: Training loss at epoch 177: 0.18388623744249344
[08/12/2025 18:13:22 INFO]: Training loss at epoch 50: 0.7040209770202637
[08/12/2025 18:13:25 INFO]: Training loss at epoch 64: 0.5547627508640289
[08/12/2025 18:13:35 INFO]: Training loss at epoch 101: 0.5017790049314499
[08/12/2025 18:13:35 INFO]: New best epoch, val score: -0.7120447835838045
[08/12/2025 18:13:35 INFO]: Saving model to: model_best.pth
[08/12/2025 18:13:49 INFO]: Training loss at epoch 178: 0.26400458812713623
[08/12/2025 18:14:11 INFO]: Training loss at epoch 92: 0.36399032175540924
[08/12/2025 18:14:21 INFO]: Training loss at epoch 179: 0.3087303340435028
[08/12/2025 18:14:32 INFO]: Training loss at epoch 102: 0.37784741818904877
[08/12/2025 18:14:33 INFO]: Training stats: {
    "score": -0.43225311138495676,
    "rmse": 0.43225311138495676
}
[08/12/2025 18:14:33 INFO]: Val stats: {
    "score": -0.6877929106295146,
    "rmse": 0.6877929106295146
}
[08/12/2025 18:14:33 INFO]: Test stats: {
    "score": -0.7193343176062554,
    "rmse": 0.7193343176062554
}
[08/12/2025 18:14:34 INFO]: Training loss at epoch 64: 0.38994425535202026
[08/12/2025 18:14:54 INFO]: Training loss at epoch 65: 0.5089365243911743
[08/12/2025 18:15:05 INFO]: Training loss at epoch 180: 0.2725009173154831
[08/12/2025 18:15:14 INFO]: Training loss at epoch 93: 0.30090805143117905
[08/12/2025 18:15:16 INFO]: Training loss at epoch 51: 0.5145223140716553
[08/12/2025 18:15:26 INFO]: Training loss at epoch 27: 0.8669566214084625
[08/12/2025 18:15:28 INFO]: Training loss at epoch 103: 0.40117305517196655
[08/12/2025 18:15:38 INFO]: Training loss at epoch 181: 0.21389605849981308
[08/12/2025 18:16:04 INFO]: Training loss at epoch 65: 0.2940377965569496
[08/12/2025 18:16:12 INFO]: Training loss at epoch 182: 0.26610932499170303
[08/12/2025 18:16:16 INFO]: Training loss at epoch 94: 0.3067014440894127
[08/12/2025 18:16:23 INFO]: Training loss at epoch 66: 0.45444734394550323
[08/12/2025 18:16:23 INFO]: New best epoch, val score: -0.7377317327219521
[08/12/2025 18:16:23 INFO]: Saving model to: model_best.pth
[08/12/2025 18:16:26 INFO]: Training loss at epoch 104: 0.5360726118087769
[08/12/2025 18:16:32 INFO]: New best epoch, val score: -0.7741424238794382
[08/12/2025 18:16:32 INFO]: Saving model to: model_best.pth
[08/12/2025 18:16:46 INFO]: Training loss at epoch 183: 0.20005730539560318
[08/12/2025 18:17:08 INFO]: Training loss at epoch 52: 0.634813517332077
[08/12/2025 18:17:19 INFO]: Training loss at epoch 95: 0.41729016602039337
[08/12/2025 18:17:20 INFO]: Training loss at epoch 184: 0.20462623983621597
[08/12/2025 18:17:24 INFO]: New best epoch, val score: -0.6563093265111515
[08/12/2025 18:17:24 INFO]: Saving model to: model_best.pth
[08/12/2025 18:17:24 INFO]: Training loss at epoch 105: 0.4195142984390259
[08/12/2025 18:17:26 INFO]: New best epoch, val score: -0.7361710084513713
[08/12/2025 18:17:26 INFO]: Saving model to: model_best.pth
[08/12/2025 18:17:36 INFO]: Training loss at epoch 66: 0.36179301142692566
[08/12/2025 18:17:52 INFO]: Training loss at epoch 67: 0.4390339255332947
[08/12/2025 18:17:53 INFO]: Training loss at epoch 185: 0.2691745460033417
[08/12/2025 18:18:21 INFO]: Training loss at epoch 96: 0.2786189168691635
[08/12/2025 18:18:22 INFO]: Training loss at epoch 106: 0.4417410045862198
[08/12/2025 18:18:25 INFO]: Training loss at epoch 186: 0.20635618269443512
[08/12/2025 18:18:58 INFO]: Training loss at epoch 187: 0.29122649133205414
[08/12/2025 18:19:01 INFO]: Training loss at epoch 53: 0.5323713421821594
[08/12/2025 18:19:03 INFO]: Training loss at epoch 28: 0.9623197317123413
[08/12/2025 18:19:08 INFO]: Training loss at epoch 67: 0.31908711791038513
[08/12/2025 18:19:19 INFO]: New best epoch, val score: -0.6911285972565038
[08/12/2025 18:19:19 INFO]: Saving model to: model_best.pth
[08/12/2025 18:19:20 INFO]: Training loss at epoch 107: 0.5960701107978821
[08/12/2025 18:19:20 INFO]: Training loss at epoch 68: 0.3003644421696663
[08/12/2025 18:19:23 INFO]: Training loss at epoch 97: 0.360270231962204
[08/12/2025 18:19:30 INFO]: Training loss at epoch 188: 0.22355867177248
[08/12/2025 18:19:31 INFO]: New best epoch, val score: -0.7065553532223345
[08/12/2025 18:19:31 INFO]: Saving model to: model_best.pth
[08/12/2025 18:20:02 INFO]: Training loss at epoch 189: 0.27376826107501984
[08/12/2025 18:20:14 INFO]: Training stats: {
    "score": -0.43953412476991804,
    "rmse": 0.43953412476991804
}
[08/12/2025 18:20:14 INFO]: Val stats: {
    "score": -0.6852112734855671,
    "rmse": 0.6852112734855671
}
[08/12/2025 18:20:14 INFO]: Test stats: {
    "score": -0.7126878033747983,
    "rmse": 0.7126878033747983
}
[08/12/2025 18:20:16 INFO]: Training loss at epoch 108: 0.4532318562269211
[08/12/2025 18:20:23 INFO]: New best epoch, val score: -0.7668587570228127
[08/12/2025 18:20:23 INFO]: Saving model to: model_best.pth
[08/12/2025 18:20:26 INFO]: Training loss at epoch 98: 0.5655952394008636
[08/12/2025 18:20:42 INFO]: Training loss at epoch 68: 0.24792903661727905
[08/12/2025 18:20:47 INFO]: Training loss at epoch 190: 0.195139080286026
[08/12/2025 18:20:49 INFO]: Training loss at epoch 69: 0.43372562527656555
[08/12/2025 18:20:53 INFO]: New best epoch, val score: -0.6671002962690751
[08/12/2025 18:20:53 INFO]: Saving model to: model_best.pth
[08/12/2025 18:20:54 INFO]: Training loss at epoch 54: 0.4661221355199814
[08/12/2025 18:21:14 INFO]: Training loss at epoch 109: 0.4057270586490631
[08/12/2025 18:21:19 INFO]: Training loss at epoch 191: 0.2583960071206093
[08/12/2025 18:21:20 INFO]: Training stats: {
    "score": -0.6779734831100447,
    "rmse": 0.6779734831100447
}
[08/12/2025 18:21:20 INFO]: Val stats: {
    "score": -0.7089550311851444,
    "rmse": 0.7089550311851444
}
[08/12/2025 18:21:20 INFO]: Test stats: {
    "score": -0.6789896796508431,
    "rmse": 0.6789896796508431
}
[08/12/2025 18:21:27 INFO]: Training loss at epoch 99: 0.24102108925580978
[08/12/2025 18:21:34 INFO]: Training stats: {
    "score": -0.6982046370007721,
    "rmse": 0.6982046370007721
}
[08/12/2025 18:21:34 INFO]: Val stats: {
    "score": -0.7729714088079261,
    "rmse": 0.7729714088079261
}
[08/12/2025 18:21:34 INFO]: Test stats: {
    "score": -0.7199592559311635,
    "rmse": 0.7199592559311635
}
[08/12/2025 18:21:48 INFO]: Training stats: {
    "score": -0.6206162293646615,
    "rmse": 0.6206162293646615
}
[08/12/2025 18:21:48 INFO]: Val stats: {
    "score": -0.7605630621986391,
    "rmse": 0.7605630621986391
}
[08/12/2025 18:21:48 INFO]: Test stats: {
    "score": -0.7344453961548332,
    "rmse": 0.7344453961548332
}
[08/12/2025 18:21:52 INFO]: Training loss at epoch 192: 0.25562314689159393
[08/12/2025 18:22:16 INFO]: Training loss at epoch 69: 0.42315053939819336
[08/12/2025 18:22:25 INFO]: Training loss at epoch 193: 0.19671960175037384
[08/12/2025 18:22:31 INFO]: Training loss at epoch 110: 0.5077821016311646
[08/12/2025 18:22:39 INFO]: Training loss at epoch 29: 0.6953215599060059
[08/12/2025 18:22:46 INFO]: Training loss at epoch 55: 0.5387063920497894
[08/12/2025 18:22:48 INFO]: Training stats: {
    "score": -0.560288772330697,
    "rmse": 0.560288772330697
}
[08/12/2025 18:22:48 INFO]: Val stats: {
    "score": -0.6718821518025118,
    "rmse": 0.6718821518025118
}
[08/12/2025 18:22:48 INFO]: Test stats: {
    "score": -0.6592192197039405,
    "rmse": 0.6592192197039405
}
[08/12/2025 18:22:48 INFO]: Training loss at epoch 70: 0.4637931138277054
[08/12/2025 18:22:50 INFO]: Training loss at epoch 100: 0.26613418757915497
[08/12/2025 18:22:57 INFO]: Training loss at epoch 194: 0.2573898807168007
[08/12/2025 18:22:59 INFO]: New best epoch, val score: -0.7932835036991058
[08/12/2025 18:22:59 INFO]: Saving model to: model_best.pth
[08/12/2025 18:23:29 INFO]: Training loss at epoch 111: 0.35557566583156586
[08/12/2025 18:23:30 INFO]: Training loss at epoch 195: 0.28448329865932465
[08/12/2025 18:23:34 INFO]: New best epoch, val score: -0.6509677459354295
[08/12/2025 18:23:34 INFO]: Saving model to: model_best.pth
[08/12/2025 18:23:52 INFO]: Training loss at epoch 101: 0.4534773826599121
[08/12/2025 18:23:53 INFO]: Training stats: {
    "score": -0.9535013378104845,
    "rmse": 0.9535013378104845
}
[08/12/2025 18:23:53 INFO]: Val stats: {
    "score": -0.9137182469346435,
    "rmse": 0.9137182469346435
}
[08/12/2025 18:23:53 INFO]: Test stats: {
    "score": -0.8797588953554609,
    "rmse": 0.8797588953554609
}
[08/12/2025 18:24:03 INFO]: Training loss at epoch 196: 0.2437559738755226
[08/12/2025 18:24:07 INFO]: New best epoch, val score: -0.6476827362062588
[08/12/2025 18:24:07 INFO]: Saving model to: model_best.pth
[08/12/2025 18:24:16 INFO]: Training loss at epoch 71: 0.5181768834590912
[08/12/2025 18:24:19 INFO]: New best epoch, val score: -0.9137182469346435
[08/12/2025 18:24:19 INFO]: Saving model to: model_best.pth
[08/12/2025 18:24:20 INFO]: Training loss at epoch 70: 0.301888182759285
[08/12/2025 18:24:27 INFO]: Training loss at epoch 112: 0.4027773588895798
[08/12/2025 18:24:36 INFO]: Training loss at epoch 197: 0.2800348401069641
[08/12/2025 18:24:38 INFO]: Training loss at epoch 56: 0.645832359790802
[08/12/2025 18:24:53 INFO]: Training loss at epoch 102: 0.382093608379364
[08/12/2025 18:25:09 INFO]: Training loss at epoch 198: 0.29543131589889526
[08/12/2025 18:25:25 INFO]: Training loss at epoch 113: 0.3852149695158005
[08/12/2025 18:25:41 INFO]: Training loss at epoch 199: 0.23913652449846268
[08/12/2025 18:25:44 INFO]: Training loss at epoch 72: 0.38212642073631287
[08/12/2025 18:25:52 INFO]: Training loss at epoch 71: 0.3558236062526703
[08/12/2025 18:25:53 INFO]: Training stats: {
    "score": -0.42346095591515687,
    "rmse": 0.42346095591515687
}
[08/12/2025 18:25:53 INFO]: Val stats: {
    "score": -0.6991206253970647,
    "rmse": 0.6991206253970647
}
[08/12/2025 18:25:53 INFO]: Test stats: {
    "score": -0.6911931718743242,
    "rmse": 0.6911931718743242
}
[08/12/2025 18:25:55 INFO]: Training loss at epoch 103: 0.33683159947395325
[08/12/2025 18:25:57 INFO]: Running Final Evaluation...
[08/12/2025 18:26:03 INFO]: New best epoch, val score: -0.6619455551175778
[08/12/2025 18:26:03 INFO]: Saving model to: model_best.pth
[08/12/2025 18:26:22 INFO]: Training loss at epoch 114: 0.5080308020114899
[08/12/2025 18:26:30 INFO]: Training loss at epoch 57: 0.5507809519767761
[08/12/2025 18:26:57 INFO]: Training loss at epoch 104: 0.38348188996315
[08/12/2025 18:27:12 INFO]: Training loss at epoch 73: 0.29459601640701294
[08/12/2025 18:27:19 INFO]: Training loss at epoch 115: 0.365483358502388
[08/12/2025 18:27:22 INFO]: Training loss at epoch 72: 0.3153277635574341
[08/12/2025 18:27:23 INFO]: New best epoch, val score: -0.6878734767226293
[08/12/2025 18:27:23 INFO]: Saving model to: model_best.pth
[08/12/2025 18:27:30 INFO]: Training loss at epoch 30: 1.0115154683589935
[08/12/2025 18:27:33 INFO]: New best epoch, val score: -0.6592845513179906
[08/12/2025 18:27:33 INFO]: Saving model to: model_best.pth
[08/12/2025 18:27:57 INFO]: New best epoch, val score: -0.9101954951187935
[08/12/2025 18:27:57 INFO]: Saving model to: model_best.pth
[08/12/2025 18:28:02 INFO]: Training loss at epoch 105: 0.27871596068143845
[08/12/2025 18:28:18 INFO]: Training loss at epoch 116: 0.3909645676612854
[08/12/2025 18:28:22 INFO]: Training loss at epoch 58: 0.5244808942079544
[08/12/2025 18:28:35 INFO]: New best epoch, val score: -0.7761914866685409
[08/12/2025 18:28:35 INFO]: Saving model to: model_best.pth
[08/12/2025 18:28:42 INFO]: Training loss at epoch 74: 0.3693079501390457
[08/12/2025 18:28:54 INFO]: Training loss at epoch 73: 0.31061114370822906
[08/12/2025 18:29:04 INFO]: Training loss at epoch 106: 0.35686154663562775
[08/12/2025 18:29:15 INFO]: Training loss at epoch 117: 0.3059713840484619
[08/12/2025 18:30:06 INFO]: Training loss at epoch 107: 0.49638718366622925
[08/12/2025 18:30:10 INFO]: Training loss at epoch 75: 0.41012734174728394
[08/12/2025 18:30:11 INFO]: Training loss at epoch 118: 0.41833382844924927
[08/12/2025 18:30:15 INFO]: Training loss at epoch 59: 0.4674380421638489
[08/12/2025 18:30:23 INFO]: Training loss at epoch 74: 0.2826412171125412
[08/12/2025 18:30:53 INFO]: Training stats: {
    "score": -0.6967008243229076,
    "rmse": 0.6967008243229076
}
[08/12/2025 18:30:53 INFO]: Val stats: {
    "score": -0.7926642502459549,
    "rmse": 0.7926642502459549
}
[08/12/2025 18:30:53 INFO]: Test stats: {
    "score": -0.6994691982280133,
    "rmse": 0.6994691982280133
}
[08/12/2025 18:31:06 INFO]: Training loss at epoch 31: 1.0195261538028717
[08/12/2025 18:31:08 INFO]: Training loss at epoch 108: 0.35626180469989777
[08/12/2025 18:31:08 INFO]: Training loss at epoch 119: 0.3534848093986511
[08/12/2025 18:31:27 INFO]: Training stats: {
    "score": -0.6544901103687875,
    "rmse": 0.6544901103687875
}
[08/12/2025 18:31:27 INFO]: Val stats: {
    "score": -0.781776310485318,
    "rmse": 0.781776310485318
}
[08/12/2025 18:31:27 INFO]: Test stats: {
    "score": -0.7057812480009914,
    "rmse": 0.7057812480009914
}
[08/12/2025 18:31:37 INFO]: Training loss at epoch 76: 0.36068879067897797
[08/12/2025 18:31:53 INFO]: Training loss at epoch 75: 0.23682452738285065
[08/12/2025 18:32:09 INFO]: Training loss at epoch 109: 0.4035913646221161
[08/12/2025 18:32:23 INFO]: Training loss at epoch 120: 0.47927461564540863
[08/12/2025 18:32:30 INFO]: Training stats: {
    "score": -0.6094079882205564,
    "rmse": 0.6094079882205564
}
[08/12/2025 18:32:30 INFO]: Val stats: {
    "score": -0.7765935865414944,
    "rmse": 0.7765935865414944
}
[08/12/2025 18:32:30 INFO]: Test stats: {
    "score": -0.7513653919388353,
    "rmse": 0.7513653919388353
}
[08/12/2025 18:32:44 INFO]: Training loss at epoch 60: 0.5024705529212952
[08/12/2025 18:32:57 INFO]: New best epoch, val score: -0.762789475017111
[08/12/2025 18:32:57 INFO]: Saving model to: model_best.pth
[08/12/2025 18:33:05 INFO]: Training loss at epoch 77: 0.4771481156349182
[08/12/2025 18:33:20 INFO]: Training loss at epoch 121: 0.3677881509065628
[08/12/2025 18:33:23 INFO]: Training loss at epoch 76: 0.4298197478055954
[08/12/2025 18:33:27 INFO]: New best epoch, val score: -0.7573087505190148
[08/12/2025 18:33:27 INFO]: Saving model to: model_best.pth
[08/12/2025 18:33:32 INFO]: Training loss at epoch 110: 0.32081905007362366
[08/12/2025 18:34:17 INFO]: Training loss at epoch 122: 0.4006074517965317
[08/12/2025 18:34:23 INFO]: New best epoch, val score: -0.7301035585410187
[08/12/2025 18:34:23 INFO]: Saving model to: model_best.pth
[08/12/2025 18:34:33 INFO]: Training loss at epoch 111: 0.40030622482299805
[08/12/2025 18:34:33 INFO]: Training loss at epoch 78: 0.3035258874297142
[08/12/2025 18:34:37 INFO]: Training loss at epoch 61: 0.4639611542224884
[08/12/2025 18:34:39 INFO]: Training loss at epoch 32: 0.9294750094413757
[08/12/2025 18:34:44 INFO]: New best epoch, val score: -0.679414080156605
[08/12/2025 18:34:44 INFO]: Saving model to: model_best.pth
[08/12/2025 18:34:52 INFO]: Training loss at epoch 77: 0.37453386187553406
[08/12/2025 18:35:13 INFO]: Training loss at epoch 123: 0.3521661013364792
[08/12/2025 18:35:20 INFO]: New best epoch, val score: -0.7288612114461033
[08/12/2025 18:35:20 INFO]: Saving model to: model_best.pth
[08/12/2025 18:35:35 INFO]: Training loss at epoch 112: 0.3657867908477783
[08/12/2025 18:36:01 INFO]: Training loss at epoch 79: 0.3086753711104393
[08/12/2025 18:36:10 INFO]: Training loss at epoch 124: 0.31510621309280396
[08/12/2025 18:36:22 INFO]: Training loss at epoch 78: 0.43203073740005493
[08/12/2025 18:36:28 INFO]: Training loss at epoch 62: 0.41369472444057465
[08/12/2025 18:36:32 INFO]: Training stats: {
    "score": -0.5888465711067904,
    "rmse": 0.5888465711067904
}
[08/12/2025 18:36:32 INFO]: Val stats: {
    "score": -0.674864254847113,
    "rmse": 0.674864254847113
}
[08/12/2025 18:36:32 INFO]: Test stats: {
    "score": -0.6298363812499546,
    "rmse": 0.6298363812499546
}
[08/12/2025 18:36:33 INFO]: New best epoch, val score: -0.6578428416323616
[08/12/2025 18:36:33 INFO]: Saving model to: model_best.pth
[08/12/2025 18:36:37 INFO]: Training loss at epoch 113: 0.364359587430954
[08/12/2025 18:36:43 INFO]: New best epoch, val score: -0.674864254847113
[08/12/2025 18:36:43 INFO]: Saving model to: model_best.pth
[08/12/2025 18:37:06 INFO]: Training loss at epoch 125: 0.31434354186058044
[08/12/2025 18:37:39 INFO]: Training loss at epoch 114: 0.25358471274375916
[08/12/2025 18:37:51 INFO]: Training loss at epoch 79: 0.30479925870895386
[08/12/2025 18:38:01 INFO]: Training loss at epoch 80: 0.39855216443538666
[08/12/2025 18:38:03 INFO]: Training loss at epoch 126: 0.2918471693992615
[08/12/2025 18:38:09 INFO]: New best epoch, val score: -0.7241961290365655
[08/12/2025 18:38:09 INFO]: Saving model to: model_best.pth
[08/12/2025 18:38:12 INFO]: Training loss at epoch 33: 0.7499132752418518
[08/12/2025 18:38:20 INFO]: Training loss at epoch 63: 0.5714734792709351
[08/12/2025 18:38:22 INFO]: Training stats: {
    "score": -0.5491341638659143,
    "rmse": 0.5491341638659143
}
[08/12/2025 18:38:22 INFO]: Val stats: {
    "score": -0.6599941536634785,
    "rmse": 0.6599941536634785
}
[08/12/2025 18:38:22 INFO]: Test stats: {
    "score": -0.6178454151501561,
    "rmse": 0.6178454151501561
}
[08/12/2025 18:38:40 INFO]: Training loss at epoch 115: 0.3420831114053726
[08/12/2025 18:38:59 INFO]: Training loss at epoch 127: 0.4204360395669937
[08/12/2025 18:39:28 INFO]: Training loss at epoch 81: 0.30065523087978363
[08/12/2025 18:39:42 INFO]: Training loss at epoch 116: 0.2897470146417618
[08/12/2025 18:39:52 INFO]: Training loss at epoch 80: 0.2875644415616989
[08/12/2025 18:39:56 INFO]: Training loss at epoch 128: 0.3086392432451248
[08/12/2025 18:40:12 INFO]: Training loss at epoch 64: 0.5211325585842133
[08/12/2025 18:40:25 INFO]: New best epoch, val score: -0.7330129420424072
[08/12/2025 18:40:25 INFO]: Saving model to: model_best.pth
[08/12/2025 18:40:43 INFO]: Training loss at epoch 117: 0.3127019852399826
[08/12/2025 18:40:52 INFO]: Training loss at epoch 129: 0.27451251447200775
[08/12/2025 18:40:56 INFO]: Training loss at epoch 82: 0.39414162933826447
[08/12/2025 18:41:11 INFO]: Training stats: {
    "score": -0.6640954816216001,
    "rmse": 0.6640954816216001
}
[08/12/2025 18:41:11 INFO]: Val stats: {
    "score": -0.8145826167109844,
    "rmse": 0.8145826167109844
}
[08/12/2025 18:41:11 INFO]: Test stats: {
    "score": -0.7388835963555569,
    "rmse": 0.7388835963555569
}
[08/12/2025 18:41:21 INFO]: Training loss at epoch 81: 0.275886707007885
[08/12/2025 18:41:44 INFO]: Training loss at epoch 34: 0.8348799347877502
[08/12/2025 18:41:45 INFO]: Training loss at epoch 118: 0.25174491852521896
[08/12/2025 18:42:03 INFO]: Training loss at epoch 65: 0.5480281710624695
[08/12/2025 18:42:07 INFO]: Training loss at epoch 130: 0.3277035355567932
[08/12/2025 18:42:09 INFO]: New best epoch, val score: -0.9077768925485449
[08/12/2025 18:42:09 INFO]: Saving model to: model_best.pth
[08/12/2025 18:42:25 INFO]: Training loss at epoch 83: 0.25797075033187866
[08/12/2025 18:42:47 INFO]: Training loss at epoch 119: 0.24097701162099838
[08/12/2025 18:42:52 INFO]: Training loss at epoch 82: 0.26184166222810745
[08/12/2025 18:43:05 INFO]: Training loss at epoch 131: 0.321634441614151
[08/12/2025 18:43:08 INFO]: Training stats: {
    "score": -0.6394737563632273,
    "rmse": 0.6394737563632273
}
[08/12/2025 18:43:08 INFO]: Val stats: {
    "score": -0.782812517164365,
    "rmse": 0.782812517164365
}
[08/12/2025 18:43:08 INFO]: Test stats: {
    "score": -0.7965085990820862,
    "rmse": 0.7965085990820862
}
[08/12/2025 18:43:52 INFO]: Training loss at epoch 84: 0.4106768071651459
[08/12/2025 18:43:56 INFO]: Training loss at epoch 66: 0.5624389350414276
[08/12/2025 18:44:01 INFO]: Training loss at epoch 132: 0.46384747326374054
[08/12/2025 18:44:03 INFO]: New best epoch, val score: -0.6717841214640594
[08/12/2025 18:44:03 INFO]: Saving model to: model_best.pth
[08/12/2025 18:44:10 INFO]: Training loss at epoch 120: 0.36331064999103546
[08/12/2025 18:44:21 INFO]: Training loss at epoch 83: 0.2543851062655449
[08/12/2025 18:44:58 INFO]: Training loss at epoch 133: 0.3389967083930969
[08/12/2025 18:45:11 INFO]: Training loss at epoch 121: 0.28082774579524994
[08/12/2025 18:45:17 INFO]: Training loss at epoch 35: 0.8587665557861328
[08/12/2025 18:45:20 INFO]: Training loss at epoch 85: 0.29013004153966904
[08/12/2025 18:45:43 INFO]: New best epoch, val score: -0.8935706421910246
[08/12/2025 18:45:43 INFO]: Saving model to: model_best.pth
[08/12/2025 18:45:49 INFO]: Training loss at epoch 67: 0.5646806359291077
[08/12/2025 18:45:50 INFO]: Training loss at epoch 84: 0.3090771287679672
[08/12/2025 18:45:54 INFO]: Training loss at epoch 134: 0.5009725391864777
[08/12/2025 18:46:13 INFO]: Training loss at epoch 122: 0.3139685243368149
[08/12/2025 18:46:49 INFO]: Training loss at epoch 86: 0.2823575362563133
[08/12/2025 18:46:50 INFO]: Training loss at epoch 135: 0.38331276178359985
[08/12/2025 18:46:57 INFO]: New best epoch, val score: -0.7135271493379034
[08/12/2025 18:46:57 INFO]: Saving model to: model_best.pth
[08/12/2025 18:47:15 INFO]: Training loss at epoch 123: 0.2868959829211235
[08/12/2025 18:47:19 INFO]: Training loss at epoch 85: 0.31824061274528503
[08/12/2025 18:47:40 INFO]: Training loss at epoch 68: 0.3675697222352028
[08/12/2025 18:47:47 INFO]: Training loss at epoch 136: 0.3399537652730942
[08/12/2025 18:47:54 INFO]: New best epoch, val score: -0.7042700482331343
[08/12/2025 18:47:54 INFO]: Saving model to: model_best.pth
[08/12/2025 18:48:17 INFO]: Training loss at epoch 124: 0.4473562091588974
[08/12/2025 18:48:17 INFO]: Training loss at epoch 87: 0.41816820204257965
[08/12/2025 18:48:44 INFO]: Training loss at epoch 137: 0.36408287286758423
[08/12/2025 18:48:49 INFO]: Training loss at epoch 86: 0.34095677733421326
[08/12/2025 18:48:50 INFO]: Training loss at epoch 36: 0.7838562726974487
[08/12/2025 18:49:16 INFO]: New best epoch, val score: -0.8662573472458149
[08/12/2025 18:49:16 INFO]: Saving model to: model_best.pth
[08/12/2025 18:49:19 INFO]: Training loss at epoch 125: 0.4718140810728073
[08/12/2025 18:49:33 INFO]: Training loss at epoch 69: 0.5328655242919922
[08/12/2025 18:49:41 INFO]: Training loss at epoch 138: 0.37585748732089996
[08/12/2025 18:49:46 INFO]: Training loss at epoch 88: 0.2934481054544449
[08/12/2025 18:49:56 INFO]: New best epoch, val score: -0.6597043969089978
[08/12/2025 18:49:56 INFO]: Saving model to: model_best.pth
[08/12/2025 18:50:11 INFO]: Training stats: {
    "score": -0.6990221338734667,
    "rmse": 0.6990221338734667
}
[08/12/2025 18:50:11 INFO]: Val stats: {
    "score": -0.7282690900633291,
    "rmse": 0.7282690900633291
}
[08/12/2025 18:50:11 INFO]: Test stats: {
    "score": -0.7110822293569447,
    "rmse": 0.7110822293569447
}
[08/12/2025 18:50:19 INFO]: Training loss at epoch 87: 0.3046853244304657
[08/12/2025 18:50:21 INFO]: Training loss at epoch 126: 0.28105227649211884
[08/12/2025 18:50:24 INFO]: New best epoch, val score: -0.7282690900633291
[08/12/2025 18:50:24 INFO]: Saving model to: model_best.pth
[08/12/2025 18:50:28 INFO]: Running Final Evaluation...
[08/12/2025 18:50:38 INFO]: Training loss at epoch 139: 0.4648010730743408
[08/12/2025 18:50:57 INFO]: Training stats: {
    "score": -0.612095678979508,
    "rmse": 0.612095678979508
}
[08/12/2025 18:50:57 INFO]: Val stats: {
    "score": -0.6997521595732382,
    "rmse": 0.6997521595732382
}
[08/12/2025 18:50:57 INFO]: Test stats: {
    "score": -0.6841077434384388,
    "rmse": 0.6841077434384388
}
[08/12/2025 18:51:03 INFO]: New best epoch, val score: -0.6997521595732382
[08/12/2025 18:51:03 INFO]: Saving model to: model_best.pth
[08/12/2025 18:51:15 INFO]: Training loss at epoch 89: 0.26142194867134094
[08/12/2025 18:51:45 INFO]: Training stats: {
    "score": -0.5985674591521615,
    "rmse": 0.5985674591521615
}
[08/12/2025 18:51:45 INFO]: Val stats: {
    "score": -0.659364632160852,
    "rmse": 0.659364632160852
}
[08/12/2025 18:51:45 INFO]: Test stats: {
    "score": -0.6222776307406895,
    "rmse": 0.6222776307406895
}
[08/12/2025 18:51:49 INFO]: Training loss at epoch 88: 0.2639472484588623
[08/12/2025 18:51:53 INFO]: Training loss at epoch 140: 0.33743157982826233
[08/12/2025 18:51:56 INFO]: New best epoch, val score: -0.659364632160852
[08/12/2025 18:51:56 INFO]: Saving model to: model_best.pth
[08/12/2025 18:52:04 INFO]: Training loss at epoch 70: 0.48578882217407227
[08/12/2025 18:52:24 INFO]: Training loss at epoch 37: 0.7075597047805786
[08/12/2025 18:52:49 INFO]: New best epoch, val score: -0.8390232940250633
[08/12/2025 18:52:49 INFO]: Saving model to: model_best.pth
[08/12/2025 18:52:50 INFO]: Training loss at epoch 141: 0.44026803970336914
[08/12/2025 18:53:15 INFO]: Training loss at epoch 90: 0.30770719796419144
[08/12/2025 18:53:19 INFO]: Training loss at epoch 89: 0.36307474970817566
[08/12/2025 18:53:47 INFO]: Training loss at epoch 142: 0.27730026841163635
[08/12/2025 18:53:49 INFO]: Training stats: {
    "score": -0.5185563621329932,
    "rmse": 0.5185563621329932
}
[08/12/2025 18:53:49 INFO]: Val stats: {
    "score": -0.6944353410337335,
    "rmse": 0.6944353410337335
}
[08/12/2025 18:53:49 INFO]: Test stats: {
    "score": -0.6482142939756148,
    "rmse": 0.6482142939756148
}
[08/12/2025 18:53:57 INFO]: Training loss at epoch 71: 0.5070092082023621
[08/12/2025 18:54:43 INFO]: Training loss at epoch 143: 0.4285966008901596
[08/12/2025 18:54:43 INFO]: Training loss at epoch 91: 0.3070312738418579
[08/12/2025 18:55:19 INFO]: Training loss at epoch 90: 0.4192245155572891
[08/12/2025 18:55:39 INFO]: Training loss at epoch 144: 0.3320987820625305
[08/12/2025 18:55:51 INFO]: Training loss at epoch 72: 0.47926995158195496
[08/12/2025 18:55:57 INFO]: Training loss at epoch 38: 0.9298075437545776
[08/12/2025 18:56:04 INFO]: New best epoch, val score: -0.7143300820003616
[08/12/2025 18:56:04 INFO]: Saving model to: model_best.pth
[08/12/2025 18:56:13 INFO]: Training loss at epoch 92: 0.37676766514778137
[08/12/2025 18:56:22 INFO]: New best epoch, val score: -0.8265008645872838
[08/12/2025 18:56:22 INFO]: Saving model to: model_best.pth
[08/12/2025 18:56:36 INFO]: Training loss at epoch 145: 0.3753783255815506
[08/12/2025 18:56:48 INFO]: Training loss at epoch 91: 0.32198566198349
[08/12/2025 18:57:32 INFO]: Training loss at epoch 146: 0.2739838510751724
[08/12/2025 18:57:43 INFO]: Training loss at epoch 93: 0.3929472416639328
[08/12/2025 18:57:45 INFO]: Training loss at epoch 73: 0.5529090464115143
[08/12/2025 18:57:59 INFO]: New best epoch, val score: -0.7115085670543252
[08/12/2025 18:57:59 INFO]: Saving model to: model_best.pth
[08/12/2025 18:58:18 INFO]: Training loss at epoch 92: 0.24228686839342117
[08/12/2025 18:58:28 INFO]: Training loss at epoch 147: 0.368101567029953
[08/12/2025 18:59:12 INFO]: Training loss at epoch 94: 0.30843645334243774
[08/12/2025 18:59:26 INFO]: Training loss at epoch 148: 0.3254629820585251
[08/12/2025 18:59:32 INFO]: Training loss at epoch 39: 0.5933210849761963
[08/12/2025 18:59:37 INFO]: Training loss at epoch 74: 0.4225582331418991
[08/12/2025 18:59:48 INFO]: Training loss at epoch 93: 0.294744148850441
[08/12/2025 19:00:22 INFO]: Training loss at epoch 149: 0.35351143777370453
[08/12/2025 19:00:39 INFO]: Training loss at epoch 95: 0.24239147454500198
[08/12/2025 19:00:41 INFO]: Training stats: {
    "score": -0.6414616406663036,
    "rmse": 0.6414616406663036
}
[08/12/2025 19:00:41 INFO]: Val stats: {
    "score": -0.730931762368547,
    "rmse": 0.730931762368547
}
[08/12/2025 19:00:41 INFO]: Test stats: {
    "score": -0.6892304850725398,
    "rmse": 0.6892304850725398
}
[08/12/2025 19:00:45 INFO]: Training stats: {
    "score": -0.8159626904377975,
    "rmse": 0.8159626904377975
}
[08/12/2025 19:00:45 INFO]: Val stats: {
    "score": -0.8345788778521825,
    "rmse": 0.8345788778521825
}
[08/12/2025 19:00:45 INFO]: Test stats: {
    "score": -0.7696703369094939,
    "rmse": 0.7696703369094939
}
[08/12/2025 19:01:17 INFO]: Training loss at epoch 94: 0.2648516744375229
[08/12/2025 19:01:28 INFO]: Training loss at epoch 75: 0.39477571845054626
[08/12/2025 19:01:37 INFO]: Training loss at epoch 150: 0.34669622778892517
[08/12/2025 19:02:07 INFO]: Training loss at epoch 96: 0.28481660783290863
[08/12/2025 19:02:36 INFO]: Training loss at epoch 151: 0.36910830438137054
[08/12/2025 19:02:48 INFO]: Training loss at epoch 95: 0.29338011145591736
[08/12/2025 19:03:20 INFO]: Training loss at epoch 76: 0.48549604415893555
[08/12/2025 19:03:32 INFO]: Training loss at epoch 152: 0.32878316938877106
[08/12/2025 19:03:35 INFO]: Training loss at epoch 97: 0.2874596267938614
[08/12/2025 19:04:17 INFO]: Training loss at epoch 96: 0.22309938073158264
[08/12/2025 19:04:18 INFO]: Training loss at epoch 40: 0.6396419703960419
[08/12/2025 19:04:28 INFO]: Training loss at epoch 153: 0.3548582047224045
[08/12/2025 19:05:02 INFO]: Training loss at epoch 98: 0.3695371448993683
[08/12/2025 19:05:11 INFO]: Training loss at epoch 77: 0.3686688169836998
[08/12/2025 19:05:29 INFO]: Training loss at epoch 154: 0.3266247361898422
[08/12/2025 19:05:51 INFO]: Training loss at epoch 97: 0.29026326537132263
[08/12/2025 19:06:26 INFO]: Training loss at epoch 155: 0.283095046877861
[08/12/2025 19:06:30 INFO]: Training loss at epoch 99: 0.45052410662174225
[08/12/2025 19:07:00 INFO]: Training stats: {
    "score": -0.545963835961714,
    "rmse": 0.545963835961714
}
[08/12/2025 19:07:00 INFO]: Val stats: {
    "score": -0.6762848342457755,
    "rmse": 0.6762848342457755
}
[08/12/2025 19:07:00 INFO]: Test stats: {
    "score": -0.6468561705074694,
    "rmse": 0.6468561705074694
}
[08/12/2025 19:07:02 INFO]: Training loss at epoch 78: 0.41981540620326996
[08/12/2025 19:07:24 INFO]: Training loss at epoch 156: 0.32846564054489136
[08/12/2025 19:07:28 INFO]: Training loss at epoch 98: 0.3521041125059128
[08/12/2025 19:07:50 INFO]: Training loss at epoch 41: 0.8038803040981293
[08/12/2025 19:08:15 INFO]: New best epoch, val score: -0.8070983596834123
[08/12/2025 19:08:15 INFO]: Saving model to: model_best.pth
[08/12/2025 19:08:22 INFO]: Training loss at epoch 157: 0.3754294514656067
[08/12/2025 19:08:28 INFO]: Training loss at epoch 100: 0.315205380320549
[08/12/2025 19:08:54 INFO]: Training loss at epoch 79: 0.48400840163230896
[08/12/2025 19:08:59 INFO]: Training loss at epoch 99: 0.239591583609581
[08/12/2025 19:09:20 INFO]: Training loss at epoch 158: 0.3763897120952606
[08/12/2025 19:09:31 INFO]: Training stats: {
    "score": -0.4983843862948869,
    "rmse": 0.4983843862948869
}
[08/12/2025 19:09:31 INFO]: Val stats: {
    "score": -0.717044091327776,
    "rmse": 0.717044091327776
}
[08/12/2025 19:09:31 INFO]: Test stats: {
    "score": -0.6733541663998117,
    "rmse": 0.6733541663998117
}
[08/12/2025 19:09:32 INFO]: Training stats: {
    "score": -0.6882890527852454,
    "rmse": 0.6882890527852454
}
[08/12/2025 19:09:32 INFO]: Val stats: {
    "score": -0.8155080600129947,
    "rmse": 0.8155080600129947
}
[08/12/2025 19:09:32 INFO]: Test stats: {
    "score": -0.7176378435126978,
    "rmse": 0.7176378435126978
}
[08/12/2025 19:09:55 INFO]: Training loss at epoch 101: 0.3976444900035858
[08/12/2025 19:10:16 INFO]: Training loss at epoch 159: 0.32836008071899414
[08/12/2025 19:10:36 INFO]: Training stats: {
    "score": -0.5891760049710099,
    "rmse": 0.5891760049710099
}
[08/12/2025 19:10:36 INFO]: Val stats: {
    "score": -0.7541867323507345,
    "rmse": 0.7541867323507345
}
[08/12/2025 19:10:36 INFO]: Test stats: {
    "score": -0.6870949938808447,
    "rmse": 0.6870949938808447
}
[08/12/2025 19:11:01 INFO]: Training loss at epoch 100: 0.2401246652007103
[08/12/2025 19:11:22 INFO]: Training loss at epoch 42: 0.5282436460256577
[08/12/2025 19:11:23 INFO]: Training loss at epoch 102: 0.3723800629377365
[08/12/2025 19:11:23 INFO]: Training loss at epoch 80: 0.43935534358024597
[08/12/2025 19:11:32 INFO]: Training loss at epoch 160: 0.3252704441547394
[08/12/2025 19:11:48 INFO]: New best epoch, val score: -0.7862280450541493
[08/12/2025 19:11:48 INFO]: Saving model to: model_best.pth
[08/12/2025 19:12:32 INFO]: Training loss at epoch 161: 0.2944899797439575
[08/12/2025 19:12:33 INFO]: Training loss at epoch 101: 0.24897516518831253
[08/12/2025 19:12:52 INFO]: Training loss at epoch 103: 0.32521258294582367
[08/12/2025 19:13:17 INFO]: Training loss at epoch 81: 0.41746973991394043
[08/12/2025 19:13:31 INFO]: Training loss at epoch 162: 0.2804935947060585
[08/12/2025 19:14:05 INFO]: Training loss at epoch 102: 0.2399565875530243
[08/12/2025 19:14:20 INFO]: Training loss at epoch 104: 0.34522484242916107
[08/12/2025 19:14:27 INFO]: Training loss at epoch 163: 0.31200531125068665
[08/12/2025 19:14:56 INFO]: Training loss at epoch 43: 0.49099959433078766
[08/12/2025 19:15:08 INFO]: Training loss at epoch 82: 0.46382224559783936
[08/12/2025 19:15:21 INFO]: New best epoch, val score: -0.7068712701616808
[08/12/2025 19:15:21 INFO]: Saving model to: model_best.pth
[08/12/2025 19:15:25 INFO]: Training loss at epoch 164: 0.2633994594216347
[08/12/2025 19:15:37 INFO]: Training loss at epoch 103: 0.24814468622207642
[08/12/2025 19:15:48 INFO]: Training loss at epoch 105: 0.29630571603775024
[08/12/2025 19:16:23 INFO]: Training loss at epoch 165: 0.2867397665977478
[08/12/2025 19:17:00 INFO]: Training loss at epoch 83: 0.5231181681156158
[08/12/2025 19:17:08 INFO]: Training loss at epoch 104: 0.1918947771191597
[08/12/2025 19:17:16 INFO]: Training loss at epoch 106: 0.31479310989379883
[08/12/2025 19:17:20 INFO]: Training loss at epoch 166: 0.34352171421051025
[08/12/2025 19:18:19 INFO]: Training loss at epoch 167: 0.3431422859430313
[08/12/2025 19:18:29 INFO]: Training loss at epoch 44: 0.5225502848625183
[08/12/2025 19:18:39 INFO]: Training loss at epoch 105: 0.26725853234529495
[08/12/2025 19:18:44 INFO]: Training loss at epoch 107: 0.31117716431617737
[08/12/2025 19:18:51 INFO]: Training loss at epoch 84: 0.46381546556949615
[08/12/2025 19:19:15 INFO]: Training loss at epoch 168: 0.3492758572101593
[08/12/2025 19:20:09 INFO]: Training loss at epoch 106: 0.31815557926893234
[08/12/2025 19:20:11 INFO]: Training loss at epoch 108: 0.3700484186410904
[08/12/2025 19:20:12 INFO]: Training loss at epoch 169: 0.3920563757419586
[08/12/2025 19:20:30 INFO]: Training stats: {
    "score": -0.6560163979403387,
    "rmse": 0.6560163979403387
}
[08/12/2025 19:20:30 INFO]: Val stats: {
    "score": -0.759430713024664,
    "rmse": 0.759430713024664
}
[08/12/2025 19:20:30 INFO]: Test stats: {
    "score": -0.7247916586420626,
    "rmse": 0.7247916586420626
}
[08/12/2025 19:20:42 INFO]: Training loss at epoch 85: 0.5061334669589996
[08/12/2025 19:21:29 INFO]: Training loss at epoch 170: 0.4631878137588501
[08/12/2025 19:21:36 INFO]: Running Final Evaluation...
[08/12/2025 19:21:38 INFO]: Training loss at epoch 109: 0.3362298458814621
[08/12/2025 19:21:41 INFO]: Training loss at epoch 107: 0.2513221949338913
[08/12/2025 19:22:00 INFO]: Training loss at epoch 45: 0.5317573100328445
[08/12/2025 19:22:08 INFO]: Training stats: {
    "score": -0.5333167414516957,
    "rmse": 0.5333167414516957
}
[08/12/2025 19:22:08 INFO]: Val stats: {
    "score": -0.7170872146341839,
    "rmse": 0.7170872146341839
}
[08/12/2025 19:22:08 INFO]: Test stats: {
    "score": -0.6223654314003423,
    "rmse": 0.6223654314003423
}
[08/12/2025 19:22:36 INFO]: Training loss at epoch 86: 0.4460107535123825
[08/12/2025 19:22:50 INFO]: New best epoch, val score: -0.6831238740567122
[08/12/2025 19:22:50 INFO]: Saving model to: model_best.pth
[08/12/2025 19:23:14 INFO]: Training loss at epoch 108: 0.2583000212907791
[08/12/2025 19:23:46 INFO]: Training loss at epoch 110: 0.2761233150959015
[08/12/2025 19:24:51 INFO]: Training loss at epoch 87: 0.4059835225343704
[08/12/2025 19:25:02 INFO]: Training loss at epoch 109: 0.2701867073774338
[08/12/2025 19:25:34 INFO]: Training loss at epoch 111: 0.3057383596897125
[08/12/2025 19:25:39 INFO]: Training stats: {
    "score": -0.4655586637557077,
    "rmse": 0.4655586637557077
}
[08/12/2025 19:25:39 INFO]: Val stats: {
    "score": -0.6907405055119142,
    "rmse": 0.6907405055119142
}
[08/12/2025 19:25:39 INFO]: Test stats: {
    "score": -0.6635351743499766,
    "rmse": 0.6635351743499766
}
[08/12/2025 19:25:52 INFO]: Running Final Evaluation...
[08/12/2025 19:26:05 INFO]: Training loss at epoch 46: 0.4984375685453415
[08/12/2025 19:27:07 INFO]: Training loss at epoch 88: 0.35000909864902496
[08/12/2025 19:27:21 INFO]: Training loss at epoch 112: 0.29308561980724335
[08/12/2025 19:29:09 INFO]: Training loss at epoch 113: 0.3238265663385391
[08/12/2025 19:29:22 INFO]: Training loss at epoch 89: 0.34542208909988403
[08/12/2025 19:30:08 INFO]: Training stats: {
    "score": -0.6795982542952466,
    "rmse": 0.6795982542952466
}
[08/12/2025 19:30:08 INFO]: Val stats: {
    "score": -0.8109225911176182,
    "rmse": 0.8109225911176182
}
[08/12/2025 19:30:08 INFO]: Test stats: {
    "score": -0.738675879837512,
    "rmse": 0.738675879837512
}
[08/12/2025 19:30:22 INFO]: Training loss at epoch 47: 0.5938573777675629
[08/12/2025 19:30:56 INFO]: Training loss at epoch 114: 0.24765978008508682
[08/12/2025 19:32:23 INFO]: Training loss at epoch 90: 0.4288705289363861
[08/12/2025 19:32:44 INFO]: Training loss at epoch 115: 0.3289985954761505
[08/12/2025 19:34:34 INFO]: Training loss at epoch 116: 0.24250950664281845
[08/12/2025 19:34:40 INFO]: Training loss at epoch 91: 0.37168754637241364
[08/12/2025 19:34:40 INFO]: Training loss at epoch 48: 0.4796750694513321
[08/12/2025 19:36:21 INFO]: Training loss at epoch 117: 0.32388487458229065
[08/12/2025 19:36:57 INFO]: Training loss at epoch 92: 0.48982295393943787
[08/12/2025 19:38:07 INFO]: Training loss at epoch 118: 0.36849863827228546
[08/12/2025 19:38:58 INFO]: Training loss at epoch 49: 0.5386103242635727
[08/12/2025 19:39:14 INFO]: Training loss at epoch 93: 0.4346413016319275
[08/12/2025 19:39:55 INFO]: Training loss at epoch 119: 0.29528096318244934
[08/12/2025 19:40:26 INFO]: Training stats: {
    "score": -0.6807307065638819,
    "rmse": 0.6807307065638819
}
[08/12/2025 19:40:26 INFO]: Val stats: {
    "score": -0.8002896506511243,
    "rmse": 0.8002896506511243
}
[08/12/2025 19:40:26 INFO]: Test stats: {
    "score": -0.6881687644912943,
    "rmse": 0.6881687644912943
}
[08/12/2025 19:40:33 INFO]: Training stats: {
    "score": -0.5710626875844279,
    "rmse": 0.5710626875844279
}
[08/12/2025 19:40:33 INFO]: Val stats: {
    "score": -0.7637673144772298,
    "rmse": 0.7637673144772298
}
[08/12/2025 19:40:33 INFO]: Test stats: {
    "score": -0.7311249835034237,
    "rmse": 0.7311249835034237
}
[08/12/2025 19:41:31 INFO]: Training loss at epoch 94: 0.4422016888856888
[08/12/2025 19:42:21 INFO]: Training loss at epoch 120: 0.3078780919313431
[08/12/2025 19:42:34 INFO]: Running Final Evaluation...
[08/12/2025 19:43:48 INFO]: Training loss at epoch 95: 0.36707818508148193
[08/12/2025 19:44:42 INFO]: Training loss at epoch 50: 0.45210734009742737
[08/12/2025 19:45:13 INFO]: New best epoch, val score: -0.7623236508646213
[08/12/2025 19:45:13 INFO]: Saving model to: model_best.pth
[08/12/2025 19:46:06 INFO]: Training loss at epoch 96: 0.38910478353500366
[08/12/2025 19:48:22 INFO]: Training loss at epoch 97: 0.36104443669319153
[08/12/2025 19:49:00 INFO]: Training loss at epoch 51: 0.47537195682525635
[08/12/2025 19:50:40 INFO]: Training loss at epoch 98: 0.33759352564811707
[08/12/2025 19:52:58 INFO]: Training loss at epoch 99: 0.4144538789987564
[08/12/2025 19:53:18 INFO]: Training loss at epoch 52: 0.48102526366710663
[08/12/2025 19:53:45 INFO]: Training stats: {
    "score": -0.6311993454276102,
    "rmse": 0.6311993454276102
}
[08/12/2025 19:53:45 INFO]: Val stats: {
    "score": -0.7652190710642942,
    "rmse": 0.7652190710642942
}
[08/12/2025 19:53:45 INFO]: Test stats: {
    "score": -0.7033574238057846,
    "rmse": 0.7033574238057846
}
[08/12/2025 19:53:49 INFO]: New best epoch, val score: -0.7124622778017103
[08/12/2025 19:53:49 INFO]: Saving model to: model_best.pth
[08/12/2025 19:56:03 INFO]: Training loss at epoch 100: 0.3391086012125015
[08/12/2025 19:57:38 INFO]: Training loss at epoch 53: 0.3854457587003708
[08/12/2025 19:58:20 INFO]: Training loss at epoch 101: 0.3381151854991913
[08/12/2025 20:00:38 INFO]: Training loss at epoch 102: 0.42728424072265625
[08/12/2025 20:01:57 INFO]: Training loss at epoch 54: 0.5529161393642426
[08/12/2025 20:02:55 INFO]: Training loss at epoch 103: 0.3854590505361557
[08/12/2025 20:05:14 INFO]: Training loss at epoch 104: 0.41267387568950653
[08/12/2025 20:06:15 INFO]: Training loss at epoch 55: 0.42893776297569275
[08/12/2025 20:07:31 INFO]: Training loss at epoch 105: 0.4745033085346222
[08/12/2025 20:09:49 INFO]: Training loss at epoch 106: 0.40656088292598724
[08/12/2025 20:10:34 INFO]: Training loss at epoch 56: 0.4231356233358383
[08/12/2025 20:11:05 INFO]: New best epoch, val score: -0.6917383424612442
[08/12/2025 20:11:05 INFO]: Saving model to: model_best.pth
[08/12/2025 20:12:06 INFO]: Training loss at epoch 107: 0.35770612955093384
[08/12/2025 20:14:23 INFO]: Training loss at epoch 108: 0.29384902864694595
[08/12/2025 20:14:53 INFO]: Training loss at epoch 57: 0.4452556073665619
[08/12/2025 20:16:41 INFO]: Training loss at epoch 109: 0.3762071430683136
[08/12/2025 20:17:28 INFO]: Training stats: {
    "score": -0.604623542948333,
    "rmse": 0.604623542948333
}
[08/12/2025 20:17:28 INFO]: Val stats: {
    "score": -0.7276617818400027,
    "rmse": 0.7276617818400027
}
[08/12/2025 20:17:28 INFO]: Test stats: {
    "score": -0.7000317220135606,
    "rmse": 0.7000317220135606
}
[08/12/2025 20:19:11 INFO]: Training loss at epoch 58: 0.5420764684677124
[08/12/2025 20:19:46 INFO]: Training loss at epoch 110: 0.49368155002593994
[08/12/2025 20:22:04 INFO]: Training loss at epoch 111: 0.3637276589870453
[08/12/2025 20:23:29 INFO]: Training loss at epoch 59: 0.4456115663051605
[08/12/2025 20:24:22 INFO]: Training loss at epoch 112: 0.32045990228652954
[08/12/2025 20:24:57 INFO]: Training stats: {
    "score": -0.623132425052918,
    "rmse": 0.623132425052918
}
[08/12/2025 20:24:57 INFO]: Val stats: {
    "score": -0.7560095537182964,
    "rmse": 0.7560095537182964
}
[08/12/2025 20:24:57 INFO]: Test stats: {
    "score": -0.6621374338564585,
    "rmse": 0.6621374338564585
}
[08/12/2025 20:26:38 INFO]: Training loss at epoch 113: 0.3819824606180191
[08/12/2025 20:28:56 INFO]: Training loss at epoch 114: 0.4112504720687866
[08/12/2025 20:29:16 INFO]: Training loss at epoch 60: 0.3685770779848099
[08/12/2025 20:29:47 INFO]: New best epoch, val score: -0.6872823757738878
[08/12/2025 20:29:47 INFO]: Saving model to: model_best.pth
[08/12/2025 20:31:13 INFO]: Training loss at epoch 115: 0.4408668130636215
[08/12/2025 20:33:31 INFO]: Training loss at epoch 116: 0.46269460022449493
[08/12/2025 20:33:34 INFO]: Training loss at epoch 61: 0.41274210810661316
[08/12/2025 20:33:47 INFO]: New best epoch, val score: -0.6804790669126579
[08/12/2025 20:33:47 INFO]: Saving model to: model_best.pth
[08/12/2025 20:35:53 INFO]: Training loss at epoch 117: 0.4462403357028961
[08/12/2025 20:36:09 INFO]: New best epoch, val score: -0.6789711226588289
[08/12/2025 20:36:09 INFO]: Saving model to: model_best.pth
[08/12/2025 20:37:57 INFO]: Training loss at epoch 62: 0.37702685594558716
[08/12/2025 20:38:10 INFO]: Training loss at epoch 118: 0.3594072014093399
[08/12/2025 20:40:27 INFO]: Training loss at epoch 119: 0.3307984173297882
[08/12/2025 20:41:14 INFO]: Training stats: {
    "score": -0.5733088785052629,
    "rmse": 0.5733088785052629
}
[08/12/2025 20:41:14 INFO]: Val stats: {
    "score": -0.6951817176634483,
    "rmse": 0.6951817176634483
}
[08/12/2025 20:41:14 INFO]: Test stats: {
    "score": -0.6797445034425851,
    "rmse": 0.6797445034425851
}
[08/12/2025 20:42:16 INFO]: Training loss at epoch 63: 0.3410418778657913
[08/12/2025 20:43:33 INFO]: Training loss at epoch 120: 0.3583161234855652
[08/12/2025 20:43:49 INFO]: New best epoch, val score: -0.6707032840038777
[08/12/2025 20:43:49 INFO]: Saving model to: model_best.pth
[08/12/2025 20:45:51 INFO]: Training loss at epoch 121: 0.39053812623023987
[08/12/2025 20:46:32 INFO]: Training loss at epoch 64: 0.35202300548553467
[08/12/2025 20:48:08 INFO]: Training loss at epoch 122: 0.3197571039199829
[08/12/2025 20:50:26 INFO]: Training loss at epoch 123: 0.3827155977487564
[08/12/2025 20:50:51 INFO]: Training loss at epoch 65: 0.3643791079521179
[08/12/2025 20:52:43 INFO]: Training loss at epoch 124: 0.2984143793582916
[08/12/2025 20:55:00 INFO]: Training loss at epoch 125: 0.3340858519077301
[08/12/2025 20:55:08 INFO]: Training loss at epoch 66: 0.2887391000986099
[08/12/2025 20:57:17 INFO]: Training loss at epoch 126: 0.32998208701610565
[08/12/2025 20:59:27 INFO]: Training loss at epoch 67: 0.3278321623802185
[08/12/2025 20:59:34 INFO]: Training loss at epoch 127: 0.35329142212867737
[08/12/2025 21:01:51 INFO]: Training loss at epoch 128: 0.30251574516296387
[08/12/2025 21:03:46 INFO]: Training loss at epoch 68: 0.37686997652053833
[08/12/2025 21:04:08 INFO]: Training loss at epoch 129: 0.3101138547062874
[08/12/2025 21:04:54 INFO]: Training stats: {
    "score": -0.5747891395155811,
    "rmse": 0.5747891395155811
}
[08/12/2025 21:04:54 INFO]: Val stats: {
    "score": -0.7095075728801232,
    "rmse": 0.7095075728801232
}
[08/12/2025 21:04:54 INFO]: Test stats: {
    "score": -0.7003522463386951,
    "rmse": 0.7003522463386951
}
[08/12/2025 21:07:11 INFO]: Training loss at epoch 130: 0.31410928070545197
[08/12/2025 21:08:05 INFO]: Training loss at epoch 69: 0.42917636036872864
[08/12/2025 21:09:28 INFO]: Training loss at epoch 131: 0.36230234801769257
[08/12/2025 21:09:32 INFO]: Training stats: {
    "score": -0.5743739778723266,
    "rmse": 0.5743739778723266
}
[08/12/2025 21:09:32 INFO]: Val stats: {
    "score": -0.7112962956416737,
    "rmse": 0.7112962956416737
}
[08/12/2025 21:09:32 INFO]: Test stats: {
    "score": -0.6203123301652691,
    "rmse": 0.6203123301652691
}
[08/12/2025 21:11:45 INFO]: Training loss at epoch 132: 0.38108237087726593
[08/12/2025 21:13:50 INFO]: Training loss at epoch 70: 0.2931249141693115
[08/12/2025 21:14:03 INFO]: Training loss at epoch 133: 0.27642980217933655
[08/12/2025 21:14:19 INFO]: New best epoch, val score: -0.6672453095809817
[08/12/2025 21:14:19 INFO]: Saving model to: model_best.pth
