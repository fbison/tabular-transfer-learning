[08/27/2025 14:34:58 INFO]: Building Dataset
[08/27/2025 14:34:58 INFO]: pre normalizer.fit

[08/27/2025 14:34:58 INFO]: pos normalizer.fit

[08/27/2025 14:35:00 INFO]: Task: regression, Dataset: ic_upstream4_ImputacaoEstatistica_exp_100_1, n_numerical: 165, n_categorical: 0, n_classes: 1, n_train_samples: 282, n_val_samples: 66, n_test_samples: 87
[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 4
  d_ffn_factor: 2.133574820264212
  attention_dropout: 0.16003420062280188
  ffn_dropout: 0.16003420062280188
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 5.602026141189282e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: -1
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_1

[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 16
  d_ffn_factor: 0.9775799672564308
  attention_dropout: 0.4169169489837243
  ffn_dropout: 0.4169169489837243
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.386881667911262e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_2

[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 1.5492062843524268
  attention_dropout: 0.48782676501811856
  ffn_dropout: 0.48782676501811856
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 8.432997652623752e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_3

[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 1.8229086446032086
  attention_dropout: 0.058405454417818636
  ffn_dropout: 0.058405454417818636
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.2739438796678846e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_4

[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 1.5272868064598182
  attention_dropout: 0.4471083224168941
  ffn_dropout: 0.4471083224168941
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00097930565230119
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_5

[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 4
  d_ffn_factor: 0.8439770385375941
  attention_dropout: 0.20485157878495758
  ffn_dropout: 0.20485157878495758
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.1503756597780539e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_6

[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 4
  d_ffn_factor: 0.8800975628253555
  attention_dropout: 0.14169041558067297
  ffn_dropout: 0.14169041558067297
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 5.8762378517025053e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_8

[08/27/2025 14:35:08 INFO]: 
_________________________________________________

[08/27/2025 14:35:08 INFO]: 
_________________________________________________

[08/27/2025 14:35:08 INFO]: 
_________________________________________________

[08/27/2025 14:35:08 INFO]: 
_________________________________________________

[08/27/2025 14:35:08 INFO]: 
_________________________________________________

[08/27/2025 14:35:08 INFO]: 
_________________________________________________

[08/27/2025 14:35:08 INFO]: 
_________________________________________________

[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 16
  d_ffn_factor: 1.9676299438838045
  attention_dropout: 0.42708452394010016
  ffn_dropout: 0.42708452394010016
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00038900848318130344
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_7

[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 8
  d_ffn_factor: 0.7950508907133044
  attention_dropout: 0.34216780925711454
  ffn_dropout: 0.34216780925711454
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00010630430829925442
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_9

[08/27/2025 14:35:08 INFO]: 
_________________________________________________

[08/27/2025 14:35:08 INFO]: 
_________________________________________________

[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: This ft_transformer has 1.174 million parameters.
[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.4837209541566274
  attention_dropout: 0.4879138291184643
  ffn_dropout: 0.4879138291184643
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.3344440616230535e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_10

[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 16
  d_ffn_factor: 1.551504049906176
  attention_dropout: 0.4730211576020092
  ffn_dropout: 0.4730211576020092
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0008182760735982511
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_12

[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 0.9934923524332347
  attention_dropout: 0.4679764682934022
  ffn_dropout: 0.4679764682934022
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.0543964136494592e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_11

[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 16
  d_ffn_factor: 1.0755150585575226
  attention_dropout: 0.19765174708312105
  ffn_dropout: 0.19765174708312105
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 5.20097116814699e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_15

[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 0.8148163955879274
  attention_dropout: 0.1786577996831406
  ffn_dropout: 0.1786577996831406
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.536866009688434e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_19

[08/27/2025 14:35:08 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 16
  d_ffn_factor: 2.197497562229252
  attention_dropout: 0.006434223928588667
  ffn_dropout: 0.006434223928588667
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 6.473147310269153e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_14

[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 1.951276390395551
  attention_dropout: 0.36554520754527414
  ffn_dropout: 0.36554520754527414
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00048744251305505855
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_17

[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 2.03331060880332
  attention_dropout: 0.07399028652371892
  ffn_dropout: 0.07399028652371892
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.5929794441060486e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_16

[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 16
  d_ffn_factor: 1.4674017980838205
  attention_dropout: 0.05811240447271432
  ffn_dropout: 0.05811240447271432
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.5312723110010375e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_20

[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 16
  d_ffn_factor: 1.9408232808117853
  attention_dropout: 0.17567174008918424
  ffn_dropout: 0.17567174008918424
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0001064505318765676
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_13

[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 2.0591085951640333
  attention_dropout: 0.46868571143944676
  ffn_dropout: 0.46868571143944676
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.665768027907903e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_18

[08/27/2025 14:35:09 INFO]: This ft_transformer has 0.845 million parameters.
[08/27/2025 14:35:09 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:09 INFO]: This ft_transformer has 1.277 million parameters.
[08/27/2025 14:35:09 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:09 INFO]: This ft_transformer has 3.832 million parameters.
[08/27/2025 14:35:09 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:09 INFO]: This ft_transformer has 2.126 million parameters.
[08/27/2025 14:35:09 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:09 INFO]: This ft_transformer has 3.251 million parameters.
[08/27/2025 14:35:09 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:09 INFO]: This ft_transformer has 0.320 million parameters.
[08/27/2025 14:35:09 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:09 INFO]: This ft_transformer has 4.719 million parameters.
[08/27/2025 14:35:09 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:09 INFO]: This ft_transformer has 3.816 million parameters.
[08/27/2025 14:35:09 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:10 INFO]: This ft_transformer has 6.428 million parameters.
[08/27/2025 14:35:10 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:10 INFO]: This ft_transformer has 0.162 million parameters.
[08/27/2025 14:35:10 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:10 INFO]: This ft_transformer has 0.638 million parameters.
[08/27/2025 14:35:10 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:10 INFO]: This ft_transformer has 6.379 million parameters.
[08/27/2025 14:35:10 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:10 INFO]: This ft_transformer has 1.088 million parameters.
[08/27/2025 14:35:10 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:10 INFO]: This ft_transformer has 6.231 million parameters.
[08/27/2025 14:35:10 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:10 INFO]: This ft_transformer has 0.352 million parameters.
[08/27/2025 14:35:10 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:10 INFO]: This ft_transformer has 0.316 million parameters.
[08/27/2025 14:35:10 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:11 INFO]: This ft_transformer has 5.898 million parameters.
[08/27/2025 14:35:11 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:11 INFO]: This ft_transformer has 26.712 million parameters.
[08/27/2025 14:35:11 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:11 INFO]: This ft_transformer has 7.540 million parameters.
[08/27/2025 14:35:11 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:47 INFO]: Training loss at epoch 0: 1.7234211564064026
[08/27/2025 14:35:47 INFO]: Training loss at epoch 0: 0.938514918088913
[08/27/2025 14:35:50 INFO]: New best epoch, val score: -1.0069151013846376
[08/27/2025 14:35:50 INFO]: Saving model to: blotchy-Amado_trial_14/model_best.pth
[08/27/2025 14:35:50 INFO]: New best epoch, val score: -0.6912685042805469
[08/27/2025 14:35:50 INFO]: Saving model to: blotchy-Amado_trial_8/model_best.pth
[08/27/2025 14:35:52 INFO]: Training loss at epoch 0: 0.96861332654953
[08/27/2025 14:35:54 INFO]: Training loss at epoch 0: 1.102752685546875
[08/27/2025 14:35:55 INFO]: New best epoch, val score: -0.7326014227784402
[08/27/2025 14:35:55 INFO]: Saving model to: blotchy-Amado_trial_1/model_best.pth
[08/27/2025 14:35:58 INFO]: New best epoch, val score: -0.7023471268474333
[08/27/2025 14:35:58 INFO]: Saving model to: blotchy-Amado_trial_6/model_best.pth
[08/27/2025 14:36:09 INFO]: Training loss at epoch 0: 1.1380958557128906
[08/27/2025 14:36:10 INFO]: Training loss at epoch 1: 1.1387507617473602
[08/27/2025 14:36:10 INFO]: Training loss at epoch 1: 0.8744679093360901
[08/27/2025 14:36:13 INFO]: New best epoch, val score: -0.8303863446319897
[08/27/2025 14:36:13 INFO]: Saving model to: blotchy-Amado_trial_14/model_best.pth
[08/27/2025 14:36:15 INFO]: Training loss at epoch 0: 0.8876268267631531
[08/27/2025 14:36:15 INFO]: New best epoch, val score: -0.662214321742333
[08/27/2025 14:36:15 INFO]: Saving model to: blotchy-Amado_trial_9/model_best.pth
[08/27/2025 14:36:19 INFO]: Training loss at epoch 1: 0.8914225995540619
[08/27/2025 14:36:21 INFO]: New best epoch, val score: -0.6673346973745341
[08/27/2025 14:36:21 INFO]: Saving model to: blotchy-Amado_trial_13/model_best.pth
[08/27/2025 14:36:23 INFO]: New best epoch, val score: -0.6624334657602752
[08/27/2025 14:36:23 INFO]: Saving model to: blotchy-Amado_trial_1/model_best.pth
[08/27/2025 14:36:24 INFO]: Training loss at epoch 1: 1.1048570275306702
[08/27/2025 14:36:28 INFO]: New best epoch, val score: -0.6693094223736629
[08/27/2025 14:36:28 INFO]: Saving model to: blotchy-Amado_trial_6/model_best.pth
[08/27/2025 14:36:32 INFO]: Training loss at epoch 0: 1.1635593175888062
[08/27/2025 14:36:32 INFO]: Training loss at epoch 2: 1.1057099103927612
[08/27/2025 14:36:33 INFO]: Training loss at epoch 2: 0.9915229380130768
[08/27/2025 14:36:34 INFO]: Training loss at epoch 0: 1.2602221369743347
[08/27/2025 14:36:36 INFO]: New best epoch, val score: -0.6792451536007634
[08/27/2025 14:36:36 INFO]: Saving model to: blotchy-Amado_trial_14/model_best.pth
[08/27/2025 14:36:39 INFO]: Training loss at epoch 0: 1.1871162950992584
[08/27/2025 14:36:40 INFO]: New best epoch, val score: -0.9534203009986741
[08/27/2025 14:36:40 INFO]: Saving model to: blotchy-Amado_trial_17/model_best.pth
[08/27/2025 14:36:41 INFO]: Training loss at epoch 0: 1.1737885475158691
[08/27/2025 14:36:43 INFO]: New best epoch, val score: -0.6654142799784024
[08/27/2025 14:36:43 INFO]: Saving model to: blotchy-Amado_trial_10/model_best.pth
[08/27/2025 14:36:48 INFO]: Training loss at epoch 2: 0.8978815972805023
[08/27/2025 14:36:49 INFO]: New best epoch, val score: -0.6689562818664876
[08/27/2025 14:36:49 INFO]: Saving model to: blotchy-Amado_trial_20/model_best.pth
[08/27/2025 14:36:50 INFO]: Training loss at epoch 0: 1.1542075276374817
[08/27/2025 14:36:52 INFO]: Training loss at epoch 0: 0.963901549577713
[08/27/2025 14:36:53 INFO]: New best epoch, val score: -0.8115135287308164
[08/27/2025 14:36:53 INFO]: Saving model to: blotchy-Amado_trial_5/model_best.pth
[08/27/2025 14:36:55 INFO]: Training loss at epoch 3: 1.0541528463363647
[08/27/2025 14:36:55 INFO]: Training loss at epoch 2: 0.9050646424293518
[08/27/2025 14:36:56 INFO]: Training loss at epoch 3: 1.1116533279418945
[08/27/2025 14:36:56 INFO]: Training loss at epoch 0: 1.351405918598175
[08/27/2025 14:36:57 INFO]: Training loss at epoch 1: 1.0139962136745453
[08/27/2025 14:36:59 INFO]: New best epoch, val score: -0.6678424479448154
[08/27/2025 14:36:59 INFO]: Saving model to: blotchy-Amado_trial_14/model_best.pth
[08/27/2025 14:37:01 INFO]: New best epoch, val score: -0.7163950440698322
[08/27/2025 14:37:01 INFO]: Saving model to: blotchy-Amado_trial_12/model_best.pth
[08/27/2025 14:37:04 INFO]: New best epoch, val score: -0.6620609687891977
[08/27/2025 14:37:04 INFO]: Saving model to: blotchy-Amado_trial_4/model_best.pth
[08/27/2025 14:37:09 INFO]: New best epoch, val score: -0.6758880196611604
[08/27/2025 14:37:09 INFO]: Saving model to: blotchy-Amado_trial_11/model_best.pth
[08/27/2025 14:37:09 INFO]: Training loss at epoch 1: 1.3340922594070435
[08/27/2025 14:37:16 INFO]: Training loss at epoch 3: 0.8082598149776459
[08/27/2025 14:37:18 INFO]: Training loss at epoch 4: 1.004057765007019
[08/27/2025 14:37:19 INFO]: Training loss at epoch 4: 1.033396691083908
[08/27/2025 14:37:26 INFO]: Training loss at epoch 3: 1.0949731171131134
[08/27/2025 14:37:42 INFO]: Training loss at epoch 5: 1.174140751361847
[08/27/2025 14:37:43 INFO]: Training loss at epoch 0: 1.0666846632957458
[08/27/2025 14:37:43 INFO]: Training loss at epoch 5: 1.1006790399551392
[08/27/2025 14:37:45 INFO]: Training loss at epoch 1: 1.4212055802345276
[08/27/2025 14:37:45 INFO]: Training loss at epoch 2: 0.9955768287181854
[08/27/2025 14:37:46 INFO]: Training loss at epoch 4: 1.0006015002727509
[08/27/2025 14:37:53 INFO]: Training loss at epoch 1: 1.026897132396698
[08/27/2025 14:37:53 INFO]: New best epoch, val score: -0.9030762516923863
[08/27/2025 14:37:53 INFO]: Saving model to: blotchy-Amado_trial_17/model_best.pth
[08/27/2025 14:38:00 INFO]: Training loss at epoch 4: 0.969572126865387
[08/27/2025 14:38:04 INFO]: New best epoch, val score: -0.6608057683403766
[08/27/2025 14:38:04 INFO]: Saving model to: blotchy-Amado_trial_10/model_best.pth
[08/27/2025 14:38:05 INFO]: New best epoch, val score: -0.9576318242613829
[08/27/2025 14:38:05 INFO]: Saving model to: blotchy-Amado_trial_7/model_best.pth
[08/27/2025 14:38:07 INFO]: Training loss at epoch 6: 1.0063128769397736
[08/27/2025 14:38:07 INFO]: Training loss at epoch 1: 1.0862273871898651
[08/27/2025 14:38:07 INFO]: Training loss at epoch 6: 1.0833807587623596
[08/27/2025 14:38:08 INFO]: Training loss at epoch 2: 1.1160534024238586
[08/27/2025 14:38:10 INFO]: Training loss at epoch 1: 1.0253421068191528
[08/27/2025 14:38:16 INFO]: Training loss at epoch 5: 0.9132681787014008
[08/27/2025 14:38:17 INFO]: New best epoch, val score: -0.6626968115314277
[08/27/2025 14:38:17 INFO]: Saving model to: blotchy-Amado_trial_20/model_best.pth
[08/27/2025 14:38:21 INFO]: Training loss at epoch 0: 1.3547285199165344
[08/27/2025 14:38:27 INFO]: Training loss at epoch 0: 1.386223316192627
[08/27/2025 14:38:31 INFO]: Training loss at epoch 7: 1.1004346013069153
[08/27/2025 14:38:32 INFO]: Training loss at epoch 7: 1.1013785600662231
[08/27/2025 14:38:32 INFO]: Training loss at epoch 1: 1.3897753357887268
[08/27/2025 14:38:34 INFO]: Training loss at epoch 5: 1.1463232040405273
[08/27/2025 14:38:35 INFO]: Training loss at epoch 3: 0.9018140435218811
[08/27/2025 14:38:36 INFO]: Training loss at epoch 1: 0.9811981022357941
[08/27/2025 14:38:42 INFO]: Training loss at epoch 1: 1.019317477941513
[08/27/2025 14:38:45 INFO]: New best epoch, val score: -0.868948397823736
[08/27/2025 14:38:45 INFO]: Saving model to: blotchy-Amado_trial_18/model_best.pth
[08/27/2025 14:38:47 INFO]: Training loss at epoch 6: 1.1896457076072693
[08/27/2025 14:38:54 INFO]: New best epoch, val score: -0.7215851392517634
[08/27/2025 14:38:54 INFO]: Saving model to: blotchy-Amado_trial_3/model_best.pth
[08/27/2025 14:38:56 INFO]: Training loss at epoch 8: 1.1284538507461548
[08/27/2025 14:38:57 INFO]: Training loss at epoch 8: 1.1298888325691223
[08/27/2025 14:38:57 INFO]: New best epoch, val score: -0.6692692318409442
[08/27/2025 14:38:57 INFO]: Saving model to: blotchy-Amado_trial_11/model_best.pth
[08/27/2025 14:39:01 INFO]: Training loss at epoch 2: 1.3063529133796692
[08/27/2025 14:39:07 INFO]: Training loss at epoch 3: 0.9425483644008636
[08/27/2025 14:39:08 INFO]: Training loss at epoch 6: 1.1613258123397827
[08/27/2025 14:39:10 INFO]: New best epoch, val score: -0.7828555602812252
[08/27/2025 14:39:10 INFO]: Saving model to: blotchy-Amado_trial_17/model_best.pth
[08/27/2025 14:39:16 INFO]: New best epoch, val score: -0.6636102814796865
[08/27/2025 14:39:16 INFO]: Saving model to: blotchy-Amado_trial_13/model_best.pth
[08/27/2025 14:39:18 INFO]: Training loss at epoch 7: 1.23524010181427
[08/27/2025 14:39:19 INFO]: Training loss at epoch 2: 1.173080325126648
[08/27/2025 14:39:21 INFO]: Training loss at epoch 9: 0.8728550970554352
[08/27/2025 14:39:21 INFO]: Training loss at epoch 9: 1.0431856513023376
[08/27/2025 14:39:23 INFO]: Training loss at epoch 0: 1.4271529912948608
[08/27/2025 14:39:25 INFO]: Training loss at epoch 4: 0.8790029585361481
[08/27/2025 14:39:28 INFO]: New best epoch, val score: -0.6605238625322187
[08/27/2025 14:39:28 INFO]: Saving model to: blotchy-Amado_trial_10/model_best.pth
[08/27/2025 14:39:30 INFO]: Training stats: {
    "score": -1.012976934961847,
    "rmse": 1.012976934961847
}
[08/27/2025 14:39:30 INFO]: Val stats: {
    "score": -0.7466148875035364,
    "rmse": 0.7466148875035364
}
[08/27/2025 14:39:30 INFO]: Test stats: {
    "score": -0.9129764676594769,
    "rmse": 0.9129764676594769
}
[08/27/2025 14:39:31 INFO]: Training stats: {
    "score": -1.037690726988956,
    "rmse": 1.037690726988956
}
[08/27/2025 14:39:31 INFO]: Val stats: {
    "score": -0.6718270224299153,
    "rmse": 0.6718270224299153
}
[08/27/2025 14:39:31 INFO]: Test stats: {
    "score": -0.8942334887674552,
    "rmse": 0.8942334887674552
}
[08/27/2025 14:39:38 INFO]: Training loss at epoch 2: 0.9231707155704498
[08/27/2025 14:39:42 INFO]: Training loss at epoch 7: 1.0307792127132416
[08/27/2025 14:39:43 INFO]: Training loss at epoch 2: 1.5755943059921265
[08/27/2025 14:39:49 INFO]: Training loss at epoch 8: 0.939441055059433
[08/27/2025 14:39:54 INFO]: New best epoch, val score: -0.6700567311552582
[08/27/2025 14:39:54 INFO]: Saving model to: blotchy-Amado_trial_5/model_best.pth
[08/27/2025 14:39:55 INFO]: Training loss at epoch 10: 1.1489092707633972
[08/27/2025 14:39:55 INFO]: Training loss at epoch 10: 0.9759494364261627
[08/27/2025 14:39:57 INFO]: New best epoch, val score: -0.8964909703956062
[08/27/2025 14:39:57 INFO]: Saving model to: blotchy-Amado_trial_19/model_best.pth
[08/27/2025 14:39:58 INFO]: New best epoch, val score: -0.662472820125061
[08/27/2025 14:39:58 INFO]: Saving model to: blotchy-Amado_trial_14/model_best.pth
[08/27/2025 14:40:08 INFO]: Training loss at epoch 4: 0.9228941798210144
[08/27/2025 14:40:16 INFO]: Training loss at epoch 8: 1.2058871388435364
[08/27/2025 14:40:16 INFO]: Training loss at epoch 5: 1.1094245910644531
[08/27/2025 14:40:17 INFO]: Training loss at epoch 2: 0.9924798011779785
[08/27/2025 14:40:18 INFO]: Training loss at epoch 3: 0.9488095939159393
[08/27/2025 14:40:18 INFO]: Training loss at epoch 9: 1.1252455711364746
[08/27/2025 14:40:19 INFO]: Training loss at epoch 11: 1.0504705905914307
[08/27/2025 14:40:20 INFO]: Training loss at epoch 11: 0.8970801830291748
[08/27/2025 14:40:22 INFO]: New best epoch, val score: -0.6779609795426919
[08/27/2025 14:40:22 INFO]: Saving model to: blotchy-Amado_trial_8/model_best.pth
[08/27/2025 14:40:23 INFO]: New best epoch, val score: -0.6604671813524948
[08/27/2025 14:40:23 INFO]: Saving model to: blotchy-Amado_trial_14/model_best.pth
[08/27/2025 14:40:24 INFO]: Training loss at epoch 2: 0.9898490607738495
[08/27/2025 14:40:27 INFO]: New best epoch, val score: -0.7118651419888252
[08/27/2025 14:40:27 INFO]: Saving model to: blotchy-Amado_trial_17/model_best.pth
[08/27/2025 14:40:28 INFO]: Training loss at epoch 1: 0.8800040781497955
[08/27/2025 14:40:28 INFO]: Training stats: {
    "score": -1.008723267881413,
    "rmse": 1.008723267881413
}
[08/27/2025 14:40:28 INFO]: Val stats: {
    "score": -0.7351009579690557,
    "rmse": 0.7351009579690557
}
[08/27/2025 14:40:28 INFO]: Test stats: {
    "score": -0.9061656718486808,
    "rmse": 0.9061656718486808
}
[08/27/2025 14:40:29 INFO]: New best epoch, val score: -0.6630915159142885
[08/27/2025 14:40:29 INFO]: Saving model to: blotchy-Amado_trial_12/model_best.pth
[08/27/2025 14:40:31 INFO]: Training loss at epoch 2: 1.0537463426589966
[08/27/2025 14:40:33 INFO]: Training loss at epoch 0: 1.1194045543670654
[08/27/2025 14:40:43 INFO]: Training loss at epoch 3: 0.9448738694190979
[08/27/2025 14:40:44 INFO]: Training loss at epoch 12: 0.9891003370285034
[08/27/2025 14:40:44 INFO]: Training loss at epoch 12: 0.7769641876220703
[08/27/2025 14:40:47 INFO]: New best epoch, val score: -0.6687330912051161
[08/27/2025 14:40:47 INFO]: Saving model to: blotchy-Amado_trial_8/model_best.pth
[08/27/2025 14:40:49 INFO]: Training loss at epoch 9: 0.9182782769203186
[08/27/2025 14:40:50 INFO]: New best epoch, val score: -0.6729524757681324
[08/27/2025 14:40:50 INFO]: Saving model to: blotchy-Amado_trial_7/model_best.pth
[08/27/2025 14:40:53 INFO]: New best epoch, val score: -0.6599697150541689
[08/27/2025 14:40:53 INFO]: Saving model to: blotchy-Amado_trial_10/model_best.pth
[08/27/2025 14:40:58 INFO]: Training loss at epoch 10: 1.035167396068573
[08/27/2025 14:40:59 INFO]: Training stats: {
    "score": -1.0138687067223668,
    "rmse": 1.0138687067223668
}
[08/27/2025 14:40:59 INFO]: Val stats: {
    "score": -0.7407046937819297,
    "rmse": 0.7407046937819297
}
[08/27/2025 14:40:59 INFO]: Test stats: {
    "score": -0.9030291418302713,
    "rmse": 0.9030291418302713
}
[08/27/2025 14:41:03 INFO]: Training loss at epoch 0: 1.2380918860435486
[08/27/2025 14:41:06 INFO]: Training loss at epoch 6: 1.086006224155426
[08/27/2025 14:41:07 INFO]: Training loss at epoch 5: 1.1265219449996948
[08/27/2025 14:41:08 INFO]: Training loss at epoch 13: 1.058890700340271
[08/27/2025 14:41:08 INFO]: Training loss at epoch 13: 1.1220876574516296
[08/27/2025 14:41:09 INFO]: Training loss at epoch 3: 0.8943700194358826
[08/27/2025 14:41:12 INFO]: New best epoch, val score: -0.6669994893248212
[08/27/2025 14:41:12 INFO]: Saving model to: blotchy-Amado_trial_8/model_best.pth
[08/27/2025 14:41:14 INFO]: Training loss at epoch 3: 1.2491433918476105
[08/27/2025 14:41:17 INFO]: New best epoch, val score: -0.7357252835219027
[08/27/2025 14:41:17 INFO]: Saving model to: blotchy-Amado_trial_2/model_best.pth
[08/27/2025 14:41:27 INFO]: Training loss at epoch 11: 0.9472556710243225
[08/27/2025 14:41:32 INFO]: Training loss at epoch 4: 0.9236404895782471
[08/27/2025 14:41:33 INFO]: Training loss at epoch 14: 1.0363490283489227
[08/27/2025 14:41:33 INFO]: Training loss at epoch 10: 0.9753037989139557
[08/27/2025 14:41:33 INFO]: Training loss at epoch 14: 1.0544163584709167
[08/27/2025 14:41:41 INFO]: New best epoch, val score: -0.6622026350076331
[08/27/2025 14:41:41 INFO]: Saving model to: blotchy-Amado_trial_17/model_best.pth
[08/27/2025 14:41:48 INFO]: Training loss at epoch 1: 1.1012068390846252
[08/27/2025 14:41:50 INFO]: New best epoch, val score: -0.6619580601065796
[08/27/2025 14:41:50 INFO]: Saving model to: blotchy-Amado_trial_15/model_best.pth
[08/27/2025 14:41:54 INFO]: Training loss at epoch 7: 1.1741331219673157
[08/27/2025 14:41:54 INFO]: Training loss at epoch 12: 1.029810905456543
[08/27/2025 14:41:55 INFO]: Training loss at epoch 15: 0.9899874329566956
[08/27/2025 14:41:55 INFO]: Training loss at epoch 15: 1.038145750761032
[08/27/2025 14:41:57 INFO]: Training loss at epoch 3: 1.1791325807571411
[08/27/2025 14:42:00 INFO]: Training loss at epoch 1: 1.0244328081607819
[08/27/2025 14:42:03 INFO]: Training loss at epoch 4: 1.182225227355957
[08/27/2025 14:42:04 INFO]: Training loss at epoch 6: 1.0451813042163849
[08/27/2025 14:42:06 INFO]: Training loss at epoch 3: 1.034954845905304
[08/27/2025 14:42:07 INFO]: Training loss at epoch 11: 0.9817973673343658
[08/27/2025 14:42:10 INFO]: New best epoch, val score: -0.7324466964009063
[08/27/2025 14:42:10 INFO]: Saving model to: blotchy-Amado_trial_18/model_best.pth
[08/27/2025 14:42:16 INFO]: Training loss at epoch 3: 0.9658788442611694
[08/27/2025 14:42:17 INFO]: Training loss at epoch 16: 1.0824746489524841
[08/27/2025 14:42:18 INFO]: Training loss at epoch 16: 0.9611093103885651
[08/27/2025 14:42:22 INFO]: Training loss at epoch 13: 1.2363466918468475
[08/27/2025 14:42:33 INFO]: Training loss at epoch 4: 1.1768317818641663
[08/27/2025 14:42:37 INFO]: Training loss at epoch 12: 1.0296270251274109
[08/27/2025 14:42:40 INFO]: Training loss at epoch 4: 1.1545823812484741
[08/27/2025 14:42:40 INFO]: Training loss at epoch 17: 0.963181883096695
[08/27/2025 14:42:40 INFO]: Training loss at epoch 8: 0.8364790081977844
[08/27/2025 14:42:41 INFO]: Training loss at epoch 17: 0.9032958745956421
[08/27/2025 14:42:42 INFO]: Training loss at epoch 5: 1.0613409280776978
[08/27/2025 14:42:51 INFO]: Training loss at epoch 14: 1.1165343523025513
[08/27/2025 14:43:01 INFO]: Training loss at epoch 7: 1.1142593622207642
[08/27/2025 14:43:05 INFO]: Training loss at epoch 18: 1.073565125465393
[08/27/2025 14:43:05 INFO]: Training loss at epoch 18: 0.9259305000305176
[08/27/2025 14:43:06 INFO]: Training loss at epoch 2: 1.3940016031265259
[08/27/2025 14:43:10 INFO]: Training loss at epoch 13: 1.0277630686759949
[08/27/2025 14:43:19 INFO]: Training loss at epoch 15: 1.148027241230011
[08/27/2025 14:43:21 INFO]: Training loss at epoch 5: 0.8963579833507538
[08/27/2025 14:43:28 INFO]: New best epoch, val score: -0.6660848517919992
[08/27/2025 14:43:28 INFO]: Saving model to: blotchy-Amado_trial_7/model_best.pth
[08/27/2025 14:43:28 INFO]: Training loss at epoch 19: 1.0683575868606567
[08/27/2025 14:43:29 INFO]: Training loss at epoch 19: 1.1335965394973755
[08/27/2025 14:43:29 INFO]: Training loss at epoch 9: 1.0642185807228088
[08/27/2025 14:43:36 INFO]: Training loss at epoch 4: 1.6186158061027527
[08/27/2025 14:43:37 INFO]: Training stats: {
    "score": -1.0023448975656812,
    "rmse": 1.0023448975656812
}
[08/27/2025 14:43:37 INFO]: Val stats: {
    "score": -0.7188444207033109,
    "rmse": 0.7188444207033109
}
[08/27/2025 14:43:37 INFO]: Test stats: {
    "score": -0.8961622119085231,
    "rmse": 0.8961622119085231
}
[08/27/2025 14:43:38 INFO]: Training stats: {
    "score": -1.0007895554872133,
    "rmse": 1.0007895554872133
}
[08/27/2025 14:43:38 INFO]: Val stats: {
    "score": -0.699937018635667,
    "rmse": 0.699937018635667
}
[08/27/2025 14:43:38 INFO]: Test stats: {
    "score": -0.8907554893671779,
    "rmse": 0.8907554893671779
}
[08/27/2025 14:43:43 INFO]: Training loss at epoch 14: 1.2118918299674988
[08/27/2025 14:43:46 INFO]: Training loss at epoch 4: 1.1268826723098755
[08/27/2025 14:43:47 INFO]: Training stats: {
    "score": -1.0193641659842816,
    "rmse": 1.0193641659842816
}
[08/27/2025 14:43:47 INFO]: Val stats: {
    "score": -0.7529425596511098,
    "rmse": 0.7529425596511098
}
[08/27/2025 14:43:47 INFO]: Test stats: {
    "score": -0.9227509387485502,
    "rmse": 0.9227509387485502
}
[08/27/2025 14:43:49 INFO]: Training loss at epoch 16: 0.9850259721279144
[08/27/2025 14:43:52 INFO]: Training loss at epoch 1: 1.2564783096313477
[08/27/2025 14:43:55 INFO]: Training loss at epoch 6: 1.1063965559005737
[08/27/2025 14:43:58 INFO]: Training loss at epoch 8: 0.9746882319450378
[08/27/2025 14:44:00 INFO]: Training loss at epoch 20: 0.9248141944408417
[08/27/2025 14:44:00 INFO]: Training loss at epoch 4: 1.0521881580352783
[08/27/2025 14:44:01 INFO]: Training loss at epoch 5: 0.9485395550727844
[08/27/2025 14:44:01 INFO]: Training loss at epoch 20: 1.0217043161392212
[08/27/2025 14:44:09 INFO]: Training loss at epoch 5: 0.9516935348510742
[08/27/2025 14:44:16 INFO]: Training loss at epoch 15: 1.112002193927765
[08/27/2025 14:44:19 INFO]: Training loss at epoch 17: 0.8921409845352173
[08/27/2025 14:44:24 INFO]: Training loss at epoch 21: 0.866785079240799
[08/27/2025 14:44:25 INFO]: Training loss at epoch 21: 1.1751052737236023
[08/27/2025 14:44:25 INFO]: New best epoch, val score: -0.6732087890398715
[08/27/2025 14:44:25 INFO]: Saving model to: blotchy-Amado_trial_19/model_best.pth
[08/27/2025 14:44:36 INFO]: Training loss at epoch 10: 1.0566033124923706
[08/27/2025 14:44:44 INFO]: Training loss at epoch 6: 1.161143720149994
[08/27/2025 14:44:48 INFO]: Training loss at epoch 18: 0.9710008203983307
[08/27/2025 14:44:48 INFO]: Training loss at epoch 16: 0.9143590331077576
[08/27/2025 14:44:48 INFO]: Training loss at epoch 22: 1.1236634850502014
[08/27/2025 14:44:49 INFO]: Training loss at epoch 22: 0.9429071247577667
[08/27/2025 14:44:56 INFO]: Training loss at epoch 9: 0.932719349861145
[08/27/2025 14:45:08 INFO]: Training loss at epoch 2: 1.1370744109153748
[08/27/2025 14:45:09 INFO]: Training loss at epoch 7: 1.0708461999893188
[08/27/2025 14:45:12 INFO]: Training loss at epoch 23: 1.0995284914970398
[08/27/2025 14:45:13 INFO]: Training loss at epoch 23: 0.9582901298999786
[08/27/2025 14:45:17 INFO]: Training stats: {
    "score": -1.0157776516402786,
    "rmse": 1.0157776516402786
}
[08/27/2025 14:45:17 INFO]: Val stats: {
    "score": -0.6638724652001146,
    "rmse": 0.6638724652001146
}
[08/27/2025 14:45:17 INFO]: Test stats: {
    "score": -0.8907909341970183,
    "rmse": 0.8907909341970183
}
[08/27/2025 14:45:18 INFO]: Training loss at epoch 19: 0.9850971400737762
[08/27/2025 14:45:18 INFO]: Training loss at epoch 5: 1.0832698345184326
[08/27/2025 14:45:21 INFO]: Training loss at epoch 17: 0.9844939708709717
[08/27/2025 14:45:25 INFO]: Training loss at epoch 11: 1.006596714258194
[08/27/2025 14:45:27 INFO]: Training loss at epoch 2: 1.179480791091919
[08/27/2025 14:45:27 INFO]: Training stats: {
    "score": -1.0087180017232262,
    "rmse": 1.0087180017232262
}
[08/27/2025 14:45:27 INFO]: Val stats: {
    "score": -0.7402703895531237,
    "rmse": 0.7402703895531237
}
[08/27/2025 14:45:27 INFO]: Test stats: {
    "score": -0.9090888369634459,
    "rmse": 0.9090888369634459
}
[08/27/2025 14:45:29 INFO]: Training loss at epoch 5: 0.934799313545227
[08/27/2025 14:45:29 INFO]: Training loss at epoch 6: 0.9907135665416718
[08/27/2025 14:45:36 INFO]: Training loss at epoch 24: 1.0655718445777893
[08/27/2025 14:45:37 INFO]: Training loss at epoch 24: 1.1562345623970032
[08/27/2025 14:45:38 INFO]: Training loss at epoch 6: 1.2897073030471802
[08/27/2025 14:45:47 INFO]: Training loss at epoch 5: 1.0765917301177979
[08/27/2025 14:45:48 INFO]: Training loss at epoch 3: 1.0702154636383057
[08/27/2025 14:45:53 INFO]: New best epoch, val score: -0.659262612958537
[08/27/2025 14:45:53 INFO]: Saving model to: blotchy-Amado_trial_3/model_best.pth
[08/27/2025 14:45:54 INFO]: Training loss at epoch 18: 1.1535664796829224
[08/27/2025 14:45:57 INFO]: Training loss at epoch 20: 0.9073979556560516
[08/27/2025 14:45:57 INFO]: New best epoch, val score: -0.6683715489182348
[08/27/2025 14:45:57 INFO]: Saving model to: blotchy-Amado_trial_6/model_best.pth
[08/27/2025 14:45:59 INFO]: Training loss at epoch 25: 1.1184485256671906
[08/27/2025 14:46:01 INFO]: Training loss at epoch 25: 1.0573132038116455
[08/27/2025 14:46:05 INFO]: Training loss at epoch 7: 1.125358760356903
[08/27/2025 14:46:14 INFO]: Training loss at epoch 12: 1.0338832139968872
[08/27/2025 14:46:14 INFO]: Training loss at epoch 10: 0.8690946698188782
[08/27/2025 14:46:21 INFO]: Training loss at epoch 1: 0.9852118194103241
[08/27/2025 14:46:24 INFO]: Training loss at epoch 26: 0.9261272847652435
[08/27/2025 14:46:24 INFO]: Training loss at epoch 8: 1.1061943173408508
[08/27/2025 14:46:25 INFO]: Training loss at epoch 26: 1.0484225153923035
[08/27/2025 14:46:27 INFO]: Training loss at epoch 21: 1.110429435968399
[08/27/2025 14:46:27 INFO]: Training loss at epoch 19: 0.9886356890201569
[08/27/2025 14:46:38 INFO]: Training stats: {
    "score": -1.009192092951972,
    "rmse": 1.009192092951972
}
[08/27/2025 14:46:38 INFO]: Val stats: {
    "score": -0.6679556564375569,
    "rmse": 0.6679556564375569
}
[08/27/2025 14:46:38 INFO]: Test stats: {
    "score": -0.8684136663218358,
    "rmse": 0.8684136663218358
}
[08/27/2025 14:46:41 INFO]: New best epoch, val score: -0.6679556564375569
[08/27/2025 14:46:41 INFO]: Saving model to: blotchy-Amado_trial_6/model_best.pth
[08/27/2025 14:46:47 INFO]: Training loss at epoch 27: 0.9323921501636505
[08/27/2025 14:46:49 INFO]: Training loss at epoch 27: 0.8780719339847565
[08/27/2025 14:46:56 INFO]: Training loss at epoch 22: 1.4384394586086273
[08/27/2025 14:47:00 INFO]: Training loss at epoch 7: 0.7095241248607635
[08/27/2025 14:47:00 INFO]: Training loss at epoch 6: 1.1069253385066986
[08/27/2025 14:47:04 INFO]: Training loss at epoch 13: 0.8116992712020874
[08/27/2025 14:47:05 INFO]: New best epoch, val score: -0.6564286867374425
[08/27/2025 14:47:05 INFO]: Saving model to: blotchy-Amado_trial_2/model_best.pth
[08/27/2025 14:47:07 INFO]: Training loss at epoch 7: 1.0506142377853394
[08/27/2025 14:47:11 INFO]: Training loss at epoch 20: 0.9846700131893158
[08/27/2025 14:47:11 INFO]: Training loss at epoch 6: 0.9675048887729645
[08/27/2025 14:47:13 INFO]: Training loss at epoch 28: 0.9774465560913086
[08/27/2025 14:47:13 INFO]: Training loss at epoch 11: 1.2194603085517883
[08/27/2025 14:47:13 INFO]: Training loss at epoch 28: 0.9133034944534302
[08/27/2025 14:47:13 INFO]: Training loss at epoch 0: 1.343874454498291
[08/27/2025 14:47:26 INFO]: Training loss at epoch 23: 0.9737587571144104
[08/27/2025 14:47:28 INFO]: Training loss at epoch 1: 1.0338395535945892
[08/27/2025 14:47:28 INFO]: Training loss at epoch 8: 1.053861141204834
[08/27/2025 14:47:32 INFO]: Training loss at epoch 6: 1.0800969004631042
[08/27/2025 14:47:37 INFO]: Training loss at epoch 29: 1.037278801202774
[08/27/2025 14:47:37 INFO]: Training loss at epoch 29: 1.1430096924304962
[08/27/2025 14:47:38 INFO]: Training loss at epoch 9: 0.9289595782756805
[08/27/2025 14:47:44 INFO]: Training loss at epoch 21: 1.238864004611969
[08/27/2025 14:47:45 INFO]: Training stats: {
    "score": -0.9969538303243698,
    "rmse": 0.9969538303243698
}
[08/27/2025 14:47:45 INFO]: Val stats: {
    "score": -0.6731725841649037,
    "rmse": 0.6731725841649037
}
[08/27/2025 14:47:45 INFO]: Test stats: {
    "score": -0.8742623106486525,
    "rmse": 0.8742623106486525
}
[08/27/2025 14:47:46 INFO]: Training stats: {
    "score": -0.9993998611952518,
    "rmse": 0.9993998611952518
}
[08/27/2025 14:47:46 INFO]: Val stats: {
    "score": -0.6666913046671432,
    "rmse": 0.6666913046671432
}
[08/27/2025 14:47:46 INFO]: Test stats: {
    "score": -0.8750839015283407,
    "rmse": 0.8750839015283407
}
[08/27/2025 14:47:53 INFO]: Training loss at epoch 14: 1.1010737419128418
[08/27/2025 14:47:56 INFO]: Training loss at epoch 24: 0.9917503893375397
[08/27/2025 14:48:04 INFO]: Training stats: {
    "score": -1.0049944839174723,
    "rmse": 1.0049944839174723
}
[08/27/2025 14:48:04 INFO]: Val stats: {
    "score": -0.6641323584637516,
    "rmse": 0.6641323584637516
}
[08/27/2025 14:48:04 INFO]: Test stats: {
    "score": -0.8693337821476042,
    "rmse": 0.8693337821476042
}
[08/27/2025 14:48:09 INFO]: Training loss at epoch 30: 1.1164395809173584
[08/27/2025 14:48:10 INFO]: Training loss at epoch 30: 0.862225353717804
[08/27/2025 14:48:11 INFO]: Training loss at epoch 12: 0.9878998100757599
[08/27/2025 14:48:16 INFO]: Training loss at epoch 22: 1.1331436336040497
[08/27/2025 14:48:23 INFO]: Training loss at epoch 25: 1.141703486442566
[08/27/2025 14:48:25 INFO]: Training loss at epoch 2: 1.0096992254257202
[08/27/2025 14:48:27 INFO]: Training loss at epoch 8: 0.8361341655254364
[08/27/2025 14:48:29 INFO]: Training loss at epoch 4: 0.8988855183124542
[08/27/2025 14:48:30 INFO]: Training loss at epoch 3: 1.215391993522644
[08/27/2025 14:48:31 INFO]: Training loss at epoch 31: 0.9738492071628571
[08/27/2025 14:48:33 INFO]: Training loss at epoch 31: 0.9197998940944672
[08/27/2025 14:48:35 INFO]: Training loss at epoch 8: 0.9500393569469452
[08/27/2025 14:48:39 INFO]: Training loss at epoch 7: 1.3027520775794983
[08/27/2025 14:48:40 INFO]: Training loss at epoch 15: 1.0121398568153381
[08/27/2025 14:48:46 INFO]: New best epoch, val score: -0.8134007769132394
[08/27/2025 14:48:46 INFO]: Saving model to: blotchy-Amado_trial_16/model_best.pth
[08/27/2025 14:48:46 INFO]: Training loss at epoch 9: 1.1394290924072266
[08/27/2025 14:48:48 INFO]: Training loss at epoch 23: 0.9355140924453735
[08/27/2025 14:48:50 INFO]: Training loss at epoch 26: 1.0410219430923462
[08/27/2025 14:48:52 INFO]: Training loss at epoch 7: 1.0084470510482788
[08/27/2025 14:48:55 INFO]: Training loss at epoch 32: 1.2971434891223907
[08/27/2025 14:48:56 INFO]: Training loss at epoch 32: 1.01667982339859
[08/27/2025 14:48:57 INFO]: Training loss at epoch 3: 1.28616064786911
[08/27/2025 14:49:07 INFO]: Training loss at epoch 13: 1.0844230651855469
[08/27/2025 14:49:15 INFO]: Training stats: {
    "score": -1.0078985462169654,
    "rmse": 1.0078985462169654
}
[08/27/2025 14:49:15 INFO]: Val stats: {
    "score": -0.6603333440083683,
    "rmse": 0.6603333440083683
}
[08/27/2025 14:49:15 INFO]: Test stats: {
    "score": -0.8733681222543181,
    "rmse": 0.8733681222543181
}
[08/27/2025 14:49:16 INFO]: Training loss at epoch 7: 0.9723499119281769
[08/27/2025 14:49:16 INFO]: Training loss at epoch 10: 1.224804550409317
[08/27/2025 14:49:19 INFO]: Training loss at epoch 33: 1.0971831381320953
[08/27/2025 14:49:20 INFO]: Training loss at epoch 27: 0.7791683226823807
[08/27/2025 14:49:20 INFO]: Training loss at epoch 33: 0.9287719428539276
[08/27/2025 14:49:21 INFO]: Training loss at epoch 24: 0.9388408958911896
[08/27/2025 14:49:29 INFO]: Training loss at epoch 16: 0.9036677777767181
[08/27/2025 14:49:44 INFO]: Training loss at epoch 34: 0.8814665675163269
[08/27/2025 14:49:45 INFO]: Training loss at epoch 34: 0.8712041974067688
[08/27/2025 14:49:50 INFO]: Training loss at epoch 28: 1.1467191576957703
[08/27/2025 14:49:55 INFO]: Training loss at epoch 25: 0.9068012535572052
[08/27/2025 14:49:57 INFO]: Training loss at epoch 9: 0.9595080316066742
[08/27/2025 14:50:06 INFO]: Training loss at epoch 14: 1.0650615692138672
[08/27/2025 14:50:07 INFO]: Training loss at epoch 9: 1.133711040019989
[08/27/2025 14:50:08 INFO]: Training loss at epoch 35: 1.232013612985611
[08/27/2025 14:50:09 INFO]: Training loss at epoch 35: 1.1287036538124084
[08/27/2025 14:50:18 INFO]: Training loss at epoch 17: 1.1586218476295471
[08/27/2025 14:50:19 INFO]: Training loss at epoch 29: 0.8471775352954865
[08/27/2025 14:50:22 INFO]: Training loss at epoch 8: 0.9420174360275269
[08/27/2025 14:50:26 INFO]: Training stats: {
    "score": -0.9974701427569119,
    "rmse": 0.9974701427569119
}
[08/27/2025 14:50:26 INFO]: Val stats: {
    "score": -0.6660388245467982,
    "rmse": 0.6660388245467982
}
[08/27/2025 14:50:26 INFO]: Test stats: {
    "score": -0.8688931385809152,
    "rmse": 0.8688931385809152
}
[08/27/2025 14:50:27 INFO]: Training loss at epoch 26: 0.9467985332012177
[08/27/2025 14:50:29 INFO]: Training stats: {
    "score": -0.9947439863853896,
    "rmse": 0.9947439863853896
}
[08/27/2025 14:50:29 INFO]: Val stats: {
    "score": -0.6731800396689522,
    "rmse": 0.6731800396689522
}
[08/27/2025 14:50:29 INFO]: Test stats: {
    "score": -0.8728532641214279,
    "rmse": 0.8728532641214279
}
[08/27/2025 14:50:30 INFO]: Training loss at epoch 11: 1.2282783389091492
[08/27/2025 14:50:31 INFO]: Training loss at epoch 36: 1.0771563053131104
[08/27/2025 14:50:33 INFO]: Training loss at epoch 36: 1.1093729138374329
[08/27/2025 14:50:37 INFO]: Training loss at epoch 8: 1.2251792550086975
[08/27/2025 14:50:38 INFO]: Training loss at epoch 10: 1.107861340045929
[08/27/2025 14:50:39 INFO]: Training stats: {
    "score": -1.0403861070694238,
    "rmse": 1.0403861070694238
}
[08/27/2025 14:50:39 INFO]: Val stats: {
    "score": -0.799613711563944,
    "rmse": 0.799613711563944
}
[08/27/2025 14:50:39 INFO]: Test stats: {
    "score": -0.950434410808844,
    "rmse": 0.950434410808844
}
[08/27/2025 14:50:55 INFO]: Training loss at epoch 37: 0.9114570915699005
[08/27/2025 14:50:57 INFO]: Training loss at epoch 37: 0.9565492868423462
[08/27/2025 14:50:59 INFO]: Training loss at epoch 30: 1.0854215621948242
[08/27/2025 14:51:01 INFO]: Training loss at epoch 27: 1.4818939864635468
[08/27/2025 14:51:02 INFO]: Training loss at epoch 8: 1.3706637620925903
[08/27/2025 14:51:04 INFO]: Training loss at epoch 15: 1.0612079501152039
[08/27/2025 14:51:08 INFO]: Training loss at epoch 18: 1.0020687878131866
[08/27/2025 14:51:11 INFO]: Training loss at epoch 5: 1.3882896304130554
[08/27/2025 14:51:20 INFO]: Training loss at epoch 38: 0.9658384025096893
[08/27/2025 14:51:21 INFO]: Training loss at epoch 38: 0.9923024475574493
[08/27/2025 14:51:28 INFO]: Training loss at epoch 31: 1.0253996849060059
[08/27/2025 14:51:33 INFO]: Training loss at epoch 28: 1.0447735786437988
[08/27/2025 14:51:42 INFO]: Training loss at epoch 39: 1.0129039585590363
[08/27/2025 14:51:43 INFO]: Training loss at epoch 12: 1.0996482372283936
[08/27/2025 14:51:43 INFO]: Training loss at epoch 39: 0.8568625152111053
[08/27/2025 14:51:50 INFO]: Training stats: {
    "score": -0.9942688043027997,
    "rmse": 0.9942688043027997
}
[08/27/2025 14:51:50 INFO]: Val stats: {
    "score": -0.6832800199900826,
    "rmse": 0.6832800199900826
}
[08/27/2025 14:51:50 INFO]: Test stats: {
    "score": -0.878227540321664,
    "rmse": 0.878227540321664
}
[08/27/2025 14:51:51 INFO]: Training stats: {
    "score": -0.996858189434203,
    "rmse": 0.996858189434203
}
[08/27/2025 14:51:51 INFO]: Val stats: {
    "score": -0.6865630401687739,
    "rmse": 0.6865630401687739
}
[08/27/2025 14:51:51 INFO]: Test stats: {
    "score": -0.8839022641540882,
    "rmse": 0.8839022641540882
}
[08/27/2025 14:51:52 INFO]: Training loss at epoch 4: 1.3080329895019531
[08/27/2025 14:51:53 INFO]: Training loss at epoch 10: 0.9786560833454132
[08/27/2025 14:51:54 INFO]: Training loss at epoch 19: 1.0637646317481995
[08/27/2025 14:51:55 INFO]: Training loss at epoch 32: 0.891629695892334
[08/27/2025 14:51:57 INFO]: Training loss at epoch 11: 1.2945043444633484
[08/27/2025 14:51:58 INFO]: Running Final Evaluation...
[08/27/2025 14:51:59 INFO]: Training loss at epoch 16: 1.0617802143096924
[08/27/2025 14:52:01 INFO]: Training loss at epoch 9: 1.4561133980751038
[08/27/2025 14:52:03 INFO]: Training loss at epoch 29: 1.1135696172714233
[08/27/2025 14:52:06 INFO]: Training loss at epoch 10: 1.1509933471679688
[08/27/2025 14:52:07 INFO]: Training accuracy: {
    "score": -1.0178415438995256,
    "rmse": 1.0178415438995256
}
[08/27/2025 14:52:07 INFO]: Val accuracy: {
    "score": -0.6624334657602752,
    "rmse": 0.6624334657602752
}
[08/27/2025 14:52:07 INFO]: Test accuracy: {
    "score": -0.8772497312015267,
    "rmse": 0.8772497312015267
}
[08/27/2025 14:52:07 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_1",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8772497312015267,
        "rmse": 0.8772497312015267
    },
    "train_stats": {
        "score": -1.0178415438995256,
        "rmse": 1.0178415438995256
    },
    "val_stats": {
        "score": -0.6624334657602752,
        "rmse": 0.6624334657602752
    }
}
[08/27/2025 14:52:07 INFO]: Procewss finished for trial blotchy-Amado_trial_1
[08/27/2025 14:52:07 INFO]: 
_________________________________________________

[08/27/2025 14:52:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:52:07 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 8
  d_ffn_factor: 2.028089431835195
  attention_dropout: 0.01872140366105174
  ffn_dropout: 0.01872140366105174
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.0538436094279153e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_21

[08/27/2025 14:52:07 INFO]: This ft_transformer has 3.269 million parameters.
[08/27/2025 14:52:07 INFO]: Training will start at epoch 0.
[08/27/2025 14:52:07 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:52:09 INFO]: Training loss at epoch 2: 0.8757676184177399
[08/27/2025 14:52:11 INFO]: Training stats: {
    "score": -1.0007001711800334,
    "rmse": 1.0007001711800334
}
[08/27/2025 14:52:11 INFO]: Val stats: {
    "score": -0.6696891698604243,
    "rmse": 0.6696891698604243
}
[08/27/2025 14:52:11 INFO]: Test stats: {
    "score": -0.8755097707494378,
    "rmse": 0.8755097707494378
}
[08/27/2025 14:52:12 INFO]: Training loss at epoch 40: 1.066925048828125
[08/27/2025 14:52:13 INFO]: Training stats: {
    "score": -1.0009078253897998,
    "rmse": 1.0009078253897998
}
[08/27/2025 14:52:13 INFO]: Val stats: {
    "score": -0.687108505326073,
    "rmse": 0.687108505326073
}
[08/27/2025 14:52:13 INFO]: Test stats: {
    "score": -0.8728422987384009,
    "rmse": 0.8728422987384009
}
[08/27/2025 14:52:14 INFO]: Training loss at epoch 40: 1.2077863812446594
[08/27/2025 14:52:14 INFO]: New best epoch, val score: -0.716085568736074
[08/27/2025 14:52:14 INFO]: Saving model to: blotchy-Amado_trial_18/model_best.pth
[08/27/2025 14:52:15 INFO]: Training loss at epoch 9: 0.9744381606578827
[08/27/2025 14:52:23 INFO]: Training loss at epoch 4: 0.9147383272647858
[08/27/2025 14:52:33 INFO]: Training stats: {
    "score": -1.1038142055599394,
    "rmse": 1.1038142055599394
}
[08/27/2025 14:52:33 INFO]: Val stats: {
    "score": -0.7343334892664418,
    "rmse": 0.7343334892664418
}
[08/27/2025 14:52:33 INFO]: Test stats: {
    "score": -0.9525957623618443,
    "rmse": 0.9525957623618443
}
[08/27/2025 14:52:34 INFO]: Training loss at epoch 41: 1.0955846309661865
[08/27/2025 14:52:36 INFO]: Training loss at epoch 41: 0.9612958431243896
[08/27/2025 14:52:43 INFO]: Training loss at epoch 9: 1.0449970364570618
[08/27/2025 14:52:44 INFO]: Training loss at epoch 30: 0.8434535264968872
[08/27/2025 14:52:47 INFO]: Training stats: {
    "score": -0.9980884637849542,
    "rmse": 0.9980884637849542
}
[08/27/2025 14:52:47 INFO]: Val stats: {
    "score": -0.6654670923220596,
    "rmse": 0.6654670923220596
}
[08/27/2025 14:52:47 INFO]: Test stats: {
    "score": -0.8766317033948401,
    "rmse": 0.8766317033948401
}
[08/27/2025 14:52:51 INFO]: Training loss at epoch 3: 1.4968777298927307
[08/27/2025 14:52:52 INFO]: Training loss at epoch 13: 1.1622343063354492
[08/27/2025 14:52:52 INFO]: Training loss at epoch 17: 0.779481828212738
[08/27/2025 14:52:57 INFO]: Training loss at epoch 20: 1.2373738586902618
[08/27/2025 14:52:58 INFO]: Training loss at epoch 42: 1.0068683326244354
[08/27/2025 14:52:59 INFO]: Training loss at epoch 42: 1.1971581280231476
[08/27/2025 14:53:02 INFO]: Running Final Evaluation...
[08/27/2025 14:53:10 INFO]: Training accuracy: {
    "score": -1.0088812776753466,
    "rmse": 1.0088812776753466
}
[08/27/2025 14:53:10 INFO]: Val accuracy: {
    "score": -0.6604671813524948,
    "rmse": 0.6604671813524948
}
[08/27/2025 14:53:10 INFO]: Test accuracy: {
    "score": -0.8757780031442322,
    "rmse": 0.8757780031442322
}
[08/27/2025 14:53:10 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_14",
    "best_epoch": 11,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8757780031442322,
        "rmse": 0.8757780031442322
    },
    "train_stats": {
        "score": -1.0088812776753466,
        "rmse": 1.0088812776753466
    },
    "val_stats": {
        "score": -0.6604671813524948,
        "rmse": 0.6604671813524948
    }
}
[08/27/2025 14:53:10 INFO]: Procewss finished for trial blotchy-Amado_trial_14
[08/27/2025 14:53:10 INFO]: 
_________________________________________________

[08/27/2025 14:53:10 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:53:10 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 1.148178017998943
  attention_dropout: 0.012157029875246361
  ffn_dropout: 0.012157029875246361
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00019135759034794364
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_22

[08/27/2025 14:53:10 INFO]: This ft_transformer has 0.232 million parameters.
[08/27/2025 14:53:10 INFO]: Training will start at epoch 0.
[08/27/2025 14:53:10 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:53:12 INFO]: Training loss at epoch 12: 0.9966557621955872
[08/27/2025 14:53:14 INFO]: Training loss at epoch 31: 0.8489150404930115
[08/27/2025 14:53:15 INFO]: Training loss at epoch 11: 1.0542932450771332
[08/27/2025 14:53:19 INFO]: Training stats: {
    "score": -0.999182788201035,
    "rmse": 0.999182788201035
}
[08/27/2025 14:53:19 INFO]: Val stats: {
    "score": -0.70339155361782,
    "rmse": 0.70339155361782
}
[08/27/2025 14:53:19 INFO]: Test stats: {
    "score": -0.8834947247404488,
    "rmse": 0.8834947247404488
}
[08/27/2025 14:53:20 INFO]: Training loss at epoch 43: 0.8662202954292297
[08/27/2025 14:53:31 INFO]: Training loss at epoch 11: 0.9683034718036652
[08/27/2025 14:53:42 INFO]: New best epoch, val score: -0.6664383156249742
[08/27/2025 14:53:42 INFO]: Saving model to: blotchy-Amado_trial_5/model_best.pth
[08/27/2025 14:53:42 INFO]: Training loss at epoch 44: 1.0788865089416504
[08/27/2025 14:53:43 INFO]: Training loss at epoch 2: 1.1489520072937012
[08/27/2025 14:53:44 INFO]: Training loss at epoch 6: 1.1272308230400085
[08/27/2025 14:53:44 INFO]: Training loss at epoch 21: 1.0823265314102173
[08/27/2025 14:53:44 INFO]: Training loss at epoch 32: 1.0711399018764496
[08/27/2025 14:53:45 INFO]: Running Final Evaluation...
[08/27/2025 14:53:47 INFO]: Training loss at epoch 18: 1.0538724660873413
[08/27/2025 14:53:53 INFO]: Training accuracy: {
    "score": -1.0046709141548396,
    "rmse": 1.0046709141548396
}
[08/27/2025 14:53:53 INFO]: Val accuracy: {
    "score": -0.6669994893248212,
    "rmse": 0.6669994893248212
}
[08/27/2025 14:53:53 INFO]: Test accuracy: {
    "score": -0.8728251576428089,
    "rmse": 0.8728251576428089
}
[08/27/2025 14:53:53 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_8",
    "best_epoch": 13,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8728251576428089,
        "rmse": 0.8728251576428089
    },
    "train_stats": {
        "score": -1.0046709141548396,
        "rmse": 1.0046709141548396
    },
    "val_stats": {
        "score": -0.6669994893248212,
        "rmse": 0.6669994893248212
    }
}
[08/27/2025 14:53:53 INFO]: Procewss finished for trial blotchy-Amado_trial_8
[08/27/2025 14:53:53 INFO]: 
_________________________________________________

[08/27/2025 14:53:53 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:53:53 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 0.9616215013181969
  attention_dropout: 0.19951887397730217
  ffn_dropout: 0.19951887397730217
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.5077761147288328e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_23

[08/27/2025 14:53:53 INFO]: This ft_transformer has 0.170 million parameters.
[08/27/2025 14:53:53 INFO]: Training will start at epoch 0.
[08/27/2025 14:53:53 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:53:53 INFO]: New best epoch, val score: -0.6593146516584193
[08/27/2025 14:53:53 INFO]: Saving model to: blotchy-Amado_trial_13/model_best.pth
[08/27/2025 14:54:01 INFO]: Training loss at epoch 14: 0.8921452164649963
[08/27/2025 14:54:04 INFO]: Training loss at epoch 0: 0.9053670465946198
[08/27/2025 14:54:07 INFO]: Training loss at epoch 10: 0.9602454900741577
[08/27/2025 14:54:07 INFO]: Training loss at epoch 0: 0.9260711371898651
[08/27/2025 14:54:15 INFO]: New best epoch, val score: -0.6848456783152114
[08/27/2025 14:54:15 INFO]: Saving model to: blotchy-Amado_trial_22/model_best.pth
[08/27/2025 14:54:18 INFO]: Training loss at epoch 33: 1.0983784198760986
[08/27/2025 14:54:20 INFO]: New best epoch, val score: -0.6729457669924186
[08/27/2025 14:54:20 INFO]: Saving model to: blotchy-Amado_trial_21/model_best.pth
[08/27/2025 14:54:22 INFO]: Training loss at epoch 10: 0.9096322953701019
[08/27/2025 14:54:28 INFO]: Training loss at epoch 13: 1.293684482574463
[08/27/2025 14:54:31 INFO]: Training loss at epoch 22: 1.1815150380134583
[08/27/2025 14:54:36 INFO]: Training loss at epoch 12: 0.9292345345020294
[08/27/2025 14:54:41 INFO]: Training loss at epoch 19: 1.081391304731369
[08/27/2025 14:54:49 INFO]: Training loss at epoch 34: 0.8935399055480957
[08/27/2025 14:54:55 INFO]: Training loss at epoch 12: 1.0274101495742798
[08/27/2025 14:54:59 INFO]: Training loss at epoch 10: 1.241415023803711
[08/27/2025 14:54:59 INFO]: Training stats: {
    "score": -0.9806132517344945,
    "rmse": 0.9806132517344945
}
[08/27/2025 14:54:59 INFO]: Val stats: {
    "score": -0.6578732485257733,
    "rmse": 0.6578732485257733
}
[08/27/2025 14:54:59 INFO]: Test stats: {
    "score": -0.8776571263098406,
    "rmse": 0.8776571263098406
}
[08/27/2025 14:55:01 INFO]: Training loss at epoch 5: 0.7920555472373962
[08/27/2025 14:55:06 INFO]: Training loss at epoch 0: 1.004155457019806
[08/27/2025 14:55:06 INFO]: New best epoch, val score: -0.6578732485257733
[08/27/2025 14:55:06 INFO]: Saving model to: blotchy-Amado_trial_13/model_best.pth
[08/27/2025 14:55:06 INFO]: New best epoch, val score: -0.6601982475983518
[08/27/2025 14:55:06 INFO]: Saving model to: blotchy-Amado_trial_5/model_best.pth
[08/27/2025 14:55:10 INFO]: Training loss at epoch 15: 1.0248019099235535
[08/27/2025 14:55:12 INFO]: Training loss at epoch 1: 1.0406216382980347
[08/27/2025 14:55:17 INFO]: New best epoch, val score: -0.7424079955063111
[08/27/2025 14:55:17 INFO]: Saving model to: blotchy-Amado_trial_23/model_best.pth
[08/27/2025 14:55:17 INFO]: Training loss at epoch 23: 1.1188929975032806
[08/27/2025 14:55:19 INFO]: Training loss at epoch 35: 1.10182386636734
[08/27/2025 14:55:21 INFO]: New best epoch, val score: -0.6654930129167559
[08/27/2025 14:55:21 INFO]: Saving model to: blotchy-Amado_trial_22/model_best.pth
[08/27/2025 14:55:22 INFO]: New best epoch, val score: -0.7072266565751878
[08/27/2025 14:55:22 INFO]: Saving model to: blotchy-Amado_trial_18/model_best.pth
[08/27/2025 14:55:37 INFO]: Training loss at epoch 5: 1.314337432384491
[08/27/2025 14:55:42 INFO]: Training loss at epoch 11: 1.3051332831382751
[08/27/2025 14:55:43 INFO]: Training loss at epoch 14: 1.1934846639633179
[08/27/2025 14:55:50 INFO]: Training loss at epoch 36: 1.2118603587150574
[08/27/2025 14:55:54 INFO]: Training loss at epoch 20: 1.0334356427192688
[08/27/2025 14:55:58 INFO]: Training loss at epoch 11: 0.9410512447357178
[08/27/2025 14:55:59 INFO]: Training loss at epoch 13: 1.111586570739746
[08/27/2025 14:56:04 INFO]: Training loss at epoch 24: 1.1346225142478943
[08/27/2025 14:56:17 INFO]: Training loss at epoch 7: 0.8248729109764099
[08/27/2025 14:56:18 INFO]: Training loss at epoch 2: 1.0910493731498718
[08/27/2025 14:56:18 INFO]: Training loss at epoch 1: 0.8484043478965759
[08/27/2025 14:56:20 INFO]: Training loss at epoch 16: 1.0924199223518372
[08/27/2025 14:56:21 INFO]: Training loss at epoch 13: 1.060824453830719
[08/27/2025 14:56:21 INFO]: Training loss at epoch 37: 0.8732676208019257
[08/27/2025 14:56:26 INFO]: New best epoch, val score: -0.6590656949623787
[08/27/2025 14:56:26 INFO]: Saving model to: blotchy-Amado_trial_22/model_best.pth
[08/27/2025 14:56:29 INFO]: New best epoch, val score: -0.6618615262139194
[08/27/2025 14:56:29 INFO]: Saving model to: blotchy-Amado_trial_17/model_best.pth
[08/27/2025 14:56:30 INFO]: Training loss at epoch 1: 1.0848842859268188
[08/27/2025 14:56:38 INFO]: Training loss at epoch 11: 1.069372296333313
[08/27/2025 14:56:41 INFO]: New best epoch, val score: -0.7212083860184639
[08/27/2025 14:56:41 INFO]: Saving model to: blotchy-Amado_trial_23/model_best.pth
[08/27/2025 14:56:50 INFO]: Training loss at epoch 21: 1.0545199513435364
[08/27/2025 14:56:50 INFO]: Training loss at epoch 25: 1.1346921920776367
[08/27/2025 14:56:51 INFO]: Training loss at epoch 38: 1.1580405235290527
[08/27/2025 14:56:58 INFO]: Training loss at epoch 15: 0.9786455631256104
[08/27/2025 14:57:04 INFO]: Training loss at epoch 4: 1.5015548467636108
[08/27/2025 14:57:16 INFO]: Training loss at epoch 12: 1.9120227098464966
[08/27/2025 14:57:21 INFO]: Training loss at epoch 14: 0.9557039141654968
[08/27/2025 14:57:21 INFO]: Training loss at epoch 39: 1.1097370982170105
[08/27/2025 14:57:22 INFO]: Training loss at epoch 3: 0.9884975552558899
[08/27/2025 14:57:30 INFO]: Training loss at epoch 17: 0.9387795925140381
[08/27/2025 14:57:33 INFO]: Training stats: {
    "score": -1.0088137568402993,
    "rmse": 1.0088137568402993
}
[08/27/2025 14:57:33 INFO]: Val stats: {
    "score": -0.6676586866051811,
    "rmse": 0.6676586866051811
}
[08/27/2025 14:57:33 INFO]: Test stats: {
    "score": -0.8685132824447589,
    "rmse": 0.8685132824447589
}
[08/27/2025 14:57:36 INFO]: Training loss at epoch 12: 0.958835244178772
[08/27/2025 14:57:37 INFO]: New best epoch, val score: -0.6676586866051811
[08/27/2025 14:57:37 INFO]: Saving model to: blotchy-Amado_trial_6/model_best.pth
[08/27/2025 14:57:37 INFO]: Training loss at epoch 26: 1.1710031032562256
[08/27/2025 14:57:41 INFO]: Training loss at epoch 3: 0.9698329865932465
[08/27/2025 14:57:46 INFO]: Training loss at epoch 22: 0.9224192500114441
[08/27/2025 14:57:47 INFO]: Training loss at epoch 14: 0.9204663038253784
[08/27/2025 14:57:55 INFO]: Training loss at epoch 2: 0.9782806932926178
[08/27/2025 14:58:05 INFO]: Training loss at epoch 40: 1.0393287539482117
[08/27/2025 14:58:06 INFO]: New best epoch, val score: -0.6970834914995555
[08/27/2025 14:58:06 INFO]: Saving model to: blotchy-Amado_trial_23/model_best.pth
[08/27/2025 14:58:08 INFO]: New best epoch, val score: -0.6676377952280655
[08/27/2025 14:58:08 INFO]: Saving model to: blotchy-Amado_trial_6/model_best.pth
[08/27/2025 14:58:12 INFO]: Training loss at epoch 6: 1.2071500420570374
[08/27/2025 14:58:16 INFO]: Training loss at epoch 16: 0.9721910953521729
[08/27/2025 14:58:20 INFO]: Training loss at epoch 12: 0.8507700264453888
[08/27/2025 14:58:24 INFO]: Training loss at epoch 27: 1.0219043791294098
[08/27/2025 14:58:24 INFO]: New best epoch, val score: -0.6598050471891321
[08/27/2025 14:58:24 INFO]: Saving model to: blotchy-Amado_trial_10/model_best.pth
[08/27/2025 14:58:29 INFO]: Training loss at epoch 4: 0.8274172246456146
[08/27/2025 14:58:33 INFO]: Training loss at epoch 2: 0.9565552771091461
[08/27/2025 14:58:34 INFO]: New best epoch, val score: -0.69934341443475
[08/27/2025 14:58:34 INFO]: Saving model to: blotchy-Amado_trial_18/model_best.pth
[08/27/2025 14:58:36 INFO]: Training loss at epoch 41: 0.9430857598781586
[08/27/2025 14:58:41 INFO]: Training loss at epoch 18: 1.0883123874664307
[08/27/2025 14:58:41 INFO]: Training loss at epoch 23: 1.236587941646576
[08/27/2025 14:58:45 INFO]: Training loss at epoch 15: 0.9897946715354919
[08/27/2025 14:58:49 INFO]: New best epoch, val score: -0.6619808853939663
[08/27/2025 14:58:49 INFO]: Saving model to: blotchy-Amado_trial_21/model_best.pth
[08/27/2025 14:58:52 INFO]: Training loss at epoch 8: 1.2792868614196777
[08/27/2025 14:58:53 INFO]: Training loss at epoch 13: 1.5810766220092773
[08/27/2025 14:58:56 INFO]: Training loss at epoch 6: 1.0228080749511719
[08/27/2025 14:59:08 INFO]: Training loss at epoch 42: 0.9090820550918579
[08/27/2025 14:59:12 INFO]: Training loss at epoch 28: 1.010432481765747
[08/27/2025 14:59:13 INFO]: Training loss at epoch 15: 0.931394636631012
[08/27/2025 14:59:15 INFO]: Training loss at epoch 13: 0.9969289898872375
[08/27/2025 14:59:19 INFO]: Training loss at epoch 3: 0.9245255589485168
[08/27/2025 14:59:30 INFO]: New best epoch, val score: -0.6747615072848694
[08/27/2025 14:59:30 INFO]: Saving model to: blotchy-Amado_trial_23/model_best.pth
[08/27/2025 14:59:34 INFO]: Training loss at epoch 17: 1.1185186505317688
[08/27/2025 14:59:37 INFO]: Training loss at epoch 5: 0.8517476916313171
[08/27/2025 14:59:37 INFO]: Training loss at epoch 24: 1.180009663105011
[08/27/2025 14:59:39 INFO]: Training loss at epoch 43: 0.8999292850494385
[08/27/2025 14:59:43 INFO]: New best epoch, val score: -0.659797179421411
[08/27/2025 14:59:43 INFO]: Saving model to: blotchy-Amado_trial_10/model_best.pth
[08/27/2025 14:59:49 INFO]: Training loss at epoch 3: 0.92460897564888
[08/27/2025 14:59:52 INFO]: Training loss at epoch 19: 1.0790218114852905
[08/27/2025 14:59:58 INFO]: Training loss at epoch 29: 1.29461669921875
[08/27/2025 15:00:02 INFO]: Training loss at epoch 13: 1.0099348723888397
[08/27/2025 15:00:05 INFO]: Training loss at epoch 1: 1.3238293528556824
[08/27/2025 15:00:10 INFO]: Training loss at epoch 16: 0.8545726537704468
[08/27/2025 15:00:10 INFO]: Training loss at epoch 44: 0.9724416732788086
[08/27/2025 15:00:15 INFO]: Training stats: {
    "score": -0.999086087473422,
    "rmse": 0.999086087473422
}
[08/27/2025 15:00:15 INFO]: Val stats: {
    "score": -0.680373749900938,
    "rmse": 0.680373749900938
}
[08/27/2025 15:00:15 INFO]: Test stats: {
    "score": -0.8792125542140521,
    "rmse": 0.8792125542140521
}
[08/27/2025 15:00:16 INFO]: Training stats: {
    "score": -1.027944570753002,
    "rmse": 1.027944570753002
}
[08/27/2025 15:00:16 INFO]: Val stats: {
    "score": -0.6652764013716225,
    "rmse": 0.6652764013716225
}
[08/27/2025 15:00:16 INFO]: Test stats: {
    "score": -0.8806199572002711,
    "rmse": 0.8806199572002711
}
[08/27/2025 15:00:29 INFO]: Training loss at epoch 14: 0.9839527904987335
[08/27/2025 15:00:32 INFO]: Training loss at epoch 25: 0.9547835886478424
[08/27/2025 15:00:41 INFO]: Training loss at epoch 16: 1.1775828003883362
[08/27/2025 15:00:43 INFO]: Training loss at epoch 45: 1.2497583031654358
[08/27/2025 15:00:43 INFO]: Training loss at epoch 4: 0.9590681195259094
[08/27/2025 15:00:43 INFO]: Training loss at epoch 6: 1.1036226749420166
[08/27/2025 15:00:51 INFO]: Training loss at epoch 3: 1.0472393035888672
[08/27/2025 15:00:52 INFO]: Training loss at epoch 18: 0.9845620393753052
[08/27/2025 15:00:52 INFO]: Training loss at epoch 14: 1.0505143404006958
[08/27/2025 15:00:54 INFO]: New best epoch, val score: -0.6611098658827654
[08/27/2025 15:00:54 INFO]: Saving model to: blotchy-Amado_trial_23/model_best.pth
[08/27/2025 15:01:04 INFO]: Training loss at epoch 30: 1.0764328837394714
[08/27/2025 15:01:07 INFO]: New best epoch, val score: -0.6619294124355357
[08/27/2025 15:01:07 INFO]: Saving model to: blotchy-Amado_trial_21/model_best.pth
[08/27/2025 15:01:16 INFO]: Training loss at epoch 46: 1.2713523507118225
[08/27/2025 15:01:25 INFO]: Training loss at epoch 5: 1.2155746221542358
[08/27/2025 15:01:29 INFO]: Training loss at epoch 7: 1.0239864587783813
[08/27/2025 15:01:29 INFO]: Training loss at epoch 20: 1.0119642615318298
[08/27/2025 15:01:30 INFO]: Training loss at epoch 9: 1.134122610092163
[08/27/2025 15:01:30 INFO]: Training loss at epoch 26: 0.8896484673023224
[08/27/2025 15:01:38 INFO]: Training loss at epoch 17: 0.9709015488624573
[08/27/2025 15:01:39 INFO]: New best epoch, val score: -0.6611211855905492
[08/27/2025 15:01:39 INFO]: Saving model to: blotchy-Amado_trial_17/model_best.pth
[08/27/2025 15:01:47 INFO]: Training loss at epoch 14: 1.1253588795661926
[08/27/2025 15:01:49 INFO]: Training loss at epoch 47: 1.001171499490738
[08/27/2025 15:01:52 INFO]: Training loss at epoch 7: 0.9448041915893555
[08/27/2025 15:01:53 INFO]: New best epoch, val score: -0.6924024383429358
[08/27/2025 15:01:53 INFO]: Saving model to: blotchy-Amado_trial_18/model_best.pth
[08/27/2025 15:01:55 INFO]: Training loss at epoch 31: 1.0194266736507416
[08/27/2025 15:02:01 INFO]: Running Final Evaluation...
[08/27/2025 15:02:12 INFO]: Training loss at epoch 5: 0.990238606929779
[08/27/2025 15:02:14 INFO]: Training loss at epoch 15: 0.8941679000854492
[08/27/2025 15:02:14 INFO]: Training loss at epoch 17: 1.0664722919464111
[08/27/2025 15:02:15 INFO]: Training loss at epoch 19: 1.0356742143630981
[08/27/2025 15:02:20 INFO]: Training accuracy: {
    "score": -1.0106756149545837,
    "rmse": 1.0106756149545837
}
[08/27/2025 15:02:20 INFO]: Val accuracy: {
    "score": -0.662214321742333,
    "rmse": 0.662214321742333
}
[08/27/2025 15:02:20 INFO]: Test accuracy: {
    "score": -0.8753001963842321,
    "rmse": 0.8753001963842321
}
[08/27/2025 15:02:20 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_9",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8753001963842321,
        "rmse": 0.8753001963842321
    },
    "train_stats": {
        "score": -1.0106756149545837,
        "rmse": 1.0106756149545837
    },
    "val_stats": {
        "score": -0.662214321742333,
        "rmse": 0.662214321742333
    }
}
[08/27/2025 15:02:20 INFO]: Procewss finished for trial blotchy-Amado_trial_9
[08/27/2025 15:02:20 INFO]: 
_________________________________________________

[08/27/2025 15:02:20 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:02:20 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 16
  d_ffn_factor: 1.4917640813948605
  attention_dropout: 0.129874524211591
  ffn_dropout: 0.129874524211591
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00011257114662934621
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_24

[08/27/2025 15:02:20 INFO]: This ft_transformer has 1.201 million parameters.
[08/27/2025 15:02:20 INFO]: Training will start at epoch 0.
[08/27/2025 15:02:20 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:02:23 INFO]: Training loss at epoch 7: 1.1300987005233765
[08/27/2025 15:02:23 INFO]: Training loss at epoch 48: 1.0434134006500244
[08/27/2025 15:02:23 INFO]: New best epoch, val score: -0.6605547352204078
[08/27/2025 15:02:23 INFO]: Saving model to: blotchy-Amado_trial_23/model_best.pth
[08/27/2025 15:02:29 INFO]: Training loss at epoch 27: 1.0742804110050201
[08/27/2025 15:02:32 INFO]: Training stats: {
    "score": -1.078711893947741,
    "rmse": 1.078711893947741
}
[08/27/2025 15:02:32 INFO]: Val stats: {
    "score": -0.874307224427118,
    "rmse": 0.874307224427118
}
[08/27/2025 15:02:32 INFO]: Test stats: {
    "score": -1.003690724348348,
    "rmse": 1.003690724348348
}
[08/27/2025 15:02:35 INFO]: Training loss at epoch 15: 1.0475274920463562
[08/27/2025 15:02:42 INFO]: Training stats: {
    "score": -1.009793342827543,
    "rmse": 1.009793342827543
}
[08/27/2025 15:02:42 INFO]: Val stats: {
    "score": -0.660088446380756,
    "rmse": 0.660088446380756
}
[08/27/2025 15:02:42 INFO]: Test stats: {
    "score": -0.874094230495996,
    "rmse": 0.874094230495996
}
[08/27/2025 15:02:45 INFO]: Training loss at epoch 21: 1.0865553617477417
[08/27/2025 15:02:56 INFO]: Training loss at epoch 0: 1.2361612915992737
[08/27/2025 15:02:56 INFO]: Training loss at epoch 49: 1.1332164406776428
[08/27/2025 15:03:01 INFO]: New best epoch, val score: -0.7242468020837914
[08/27/2025 15:03:01 INFO]: Saving model to: blotchy-Amado_trial_24/model_best.pth
[08/27/2025 15:03:02 INFO]: Training loss at epoch 8: 0.8432730436325073
[08/27/2025 15:03:07 INFO]: Training stats: {
    "score": -1.0011508625071395,
    "rmse": 1.0011508625071395
}
[08/27/2025 15:03:07 INFO]: Val stats: {
    "score": -0.6776040564235414,
    "rmse": 0.6776040564235414
}
[08/27/2025 15:03:07 INFO]: Test stats: {
    "score": -0.869378492081402,
    "rmse": 0.869378492081402
}
[08/27/2025 15:03:07 INFO]: Training loss at epoch 18: 0.9091971814632416
[08/27/2025 15:03:14 INFO]: Training loss at epoch 4: 1.0776318311691284
[08/27/2025 15:03:23 INFO]: Training loss at epoch 4: 1.162757933139801
[08/27/2025 15:03:25 INFO]: Training loss at epoch 28: 1.2043399512767792
[08/27/2025 15:03:32 INFO]: Training loss at epoch 15: 0.9692745804786682
[08/27/2025 15:03:35 INFO]: Training loss at epoch 1: 1.2605772614479065
[08/27/2025 15:03:37 INFO]: Training loss at epoch 6: 1.0698964893817902
[08/27/2025 15:03:37 INFO]: Training loss at epoch 50: 0.9271330237388611
[08/27/2025 15:03:43 INFO]: Training loss at epoch 18: 0.9595453143119812
[08/27/2025 15:03:48 INFO]: New best epoch, val score: -0.6602005037192775
[08/27/2025 15:03:48 INFO]: Saving model to: blotchy-Amado_trial_23/model_best.pth
[08/27/2025 15:03:51 INFO]: Training loss at epoch 16: 1.016997516155243
[08/27/2025 15:03:55 INFO]: Training loss at epoch 22: 0.9469382762908936
[08/27/2025 15:03:59 INFO]: Training loss at epoch 20: 1.0602833032608032
[08/27/2025 15:04:08 INFO]: Training loss at epoch 9: 1.015963613986969
[08/27/2025 15:04:09 INFO]: Training loss at epoch 51: 1.0382437705993652
[08/27/2025 15:04:14 INFO]: Training loss at epoch 2: 0.9168544411659241
[08/27/2025 15:04:14 INFO]: Training loss at epoch 16: 1.1139554977416992
[08/27/2025 15:04:22 INFO]: Training loss at epoch 29: 0.9963841140270233
[08/27/2025 15:04:31 INFO]: Training loss at epoch 19: 1.1602222323417664
[08/27/2025 15:04:33 INFO]: Training stats: {
    "score": -1.0449629135279763,
    "rmse": 1.0449629135279763
}
[08/27/2025 15:04:33 INFO]: Val stats: {
    "score": -0.8121103393345797,
    "rmse": 0.8121103393345797
}
[08/27/2025 15:04:33 INFO]: Test stats: {
    "score": -0.962913515904039,
    "rmse": 0.962913515904039
}
[08/27/2025 15:04:40 INFO]: Training stats: {
    "score": -0.9802646115020731,
    "rmse": 0.9802646115020731
}
[08/27/2025 15:04:40 INFO]: Val stats: {
    "score": -0.6609070352886178,
    "rmse": 0.6609070352886178
}
[08/27/2025 15:04:40 INFO]: Test stats: {
    "score": -0.8841559022987658,
    "rmse": 0.8841559022987658
}
[08/27/2025 15:04:41 INFO]: Training loss at epoch 52: 0.8966978490352631
[08/27/2025 15:04:49 INFO]: Training loss at epoch 8: 1.0912679433822632
[08/27/2025 15:04:55 INFO]: Training loss at epoch 3: 1.2816323041915894
[08/27/2025 15:05:02 INFO]: Training stats: {
    "score": -0.9939734174093648,
    "rmse": 0.9939734174093648
}
[08/27/2025 15:05:02 INFO]: Val stats: {
    "score": -0.6684678216361635,
    "rmse": 0.6684678216361635
}
[08/27/2025 15:05:02 INFO]: Test stats: {
    "score": -0.8701115705846179,
    "rmse": 0.8701115705846179
}
[08/27/2025 15:05:03 INFO]: Training loss at epoch 7: 0.88868647813797
[08/27/2025 15:05:09 INFO]: Training loss at epoch 23: 1.05317223072052
[08/27/2025 15:05:10 INFO]: Training loss at epoch 10: 1.0734766125679016
[08/27/2025 15:05:12 INFO]: New best epoch, val score: -0.6867477423957894
[08/27/2025 15:05:12 INFO]: Saving model to: blotchy-Amado_trial_18/model_best.pth
[08/27/2025 15:05:12 INFO]: Training loss at epoch 19: 1.3871923685073853
[08/27/2025 15:05:14 INFO]: New best epoch, val score: -0.6600991273336647
[08/27/2025 15:05:14 INFO]: Saving model to: blotchy-Amado_trial_23/model_best.pth
[08/27/2025 15:05:15 INFO]: Training loss at epoch 53: 0.8719872832298279
[08/27/2025 15:05:18 INFO]: Training loss at epoch 16: 0.8421326279640198
[08/27/2025 15:05:20 INFO]: Training loss at epoch 21: 0.9507045149803162
[08/27/2025 15:05:30 INFO]: Training loss at epoch 17: 1.0860677063465118
[08/27/2025 15:05:33 INFO]: Training loss at epoch 5: 1.0391684770584106
[08/27/2025 15:05:34 INFO]: Training loss at epoch 4: 0.8921769261360168
[08/27/2025 15:05:39 INFO]: Training loss at epoch 30: 0.8143865466117859
[08/27/2025 15:05:40 INFO]: New best epoch, val score: -0.6650118139282377
[08/27/2025 15:05:40 INFO]: Saving model to: blotchy-Amado_trial_24/model_best.pth
[08/27/2025 15:05:42 INFO]: Training loss at epoch 10: 0.9341543316841125
[08/27/2025 15:05:46 INFO]: Training stats: {
    "score": -1.000951587358793,
    "rmse": 1.000951587358793
}
[08/27/2025 15:05:46 INFO]: Val stats: {
    "score": -0.670760183902189,
    "rmse": 0.670760183902189
}
[08/27/2025 15:05:46 INFO]: Test stats: {
    "score": -0.8701988424095032,
    "rmse": 0.8701988424095032
}
[08/27/2025 15:05:47 INFO]: Training loss at epoch 8: 1.041130006313324
[08/27/2025 15:05:49 INFO]: Training loss at epoch 54: 1.0234352350234985
[08/27/2025 15:05:52 INFO]: Training loss at epoch 6: 1.3027386665344238
[08/27/2025 15:05:55 INFO]: Training loss at epoch 17: 0.7558359205722809
[08/27/2025 15:06:09 INFO]: Training loss at epoch 4: 1.0646916031837463
[08/27/2025 15:06:16 INFO]: Training loss at epoch 5: 1.0450035333633423
[08/27/2025 15:06:22 INFO]: Training loss at epoch 55: 1.1327750086784363
[08/27/2025 15:06:23 INFO]: Training loss at epoch 24: 0.9037463963031769
[08/27/2025 15:06:31 INFO]: Training loss at epoch 20: 0.9197867512702942
[08/27/2025 15:06:31 INFO]: Training loss at epoch 8: 1.1458592414855957
[08/27/2025 15:06:38 INFO]: Training loss at epoch 31: 0.8901715874671936
[08/27/2025 15:06:43 INFO]: New best epoch, val score: -0.6600339720491989
[08/27/2025 15:06:43 INFO]: Saving model to: blotchy-Amado_trial_23/model_best.pth
[08/27/2025 15:06:43 INFO]: Training loss at epoch 22: 0.9580687582492828
[08/27/2025 15:06:52 INFO]: Training loss at epoch 11: 0.9600498974323273
[08/27/2025 15:06:56 INFO]: Training loss at epoch 56: 1.1999156177043915
[08/27/2025 15:06:59 INFO]: Training loss at epoch 6: 1.0814982652664185
[08/27/2025 15:07:06 INFO]: Training loss at epoch 17: 1.1635996103286743
[08/27/2025 15:07:15 INFO]: Training loss at epoch 18: 1.0471878945827484
[08/27/2025 15:07:17 INFO]: Training loss at epoch 20: 1.0186255276203156
[08/27/2025 15:07:31 INFO]: Training loss at epoch 57: 0.8919974863529205
[08/27/2025 15:07:37 INFO]: Training loss at epoch 32: 1.0192789435386658
[08/27/2025 15:07:39 INFO]: Training loss at epoch 25: 1.0037413239479065
[08/27/2025 15:07:40 INFO]: Training loss at epoch 7: 1.012764185667038
[08/27/2025 15:07:40 INFO]: Training loss at epoch 18: 1.2729527354240417
[08/27/2025 15:07:46 INFO]: New best epoch, val score: -0.6641857327765358
[08/27/2025 15:07:46 INFO]: Saving model to: blotchy-Amado_trial_24/model_best.pth
[08/27/2025 15:07:53 INFO]: Training loss at epoch 11: 0.805862307548523
[08/27/2025 15:08:00 INFO]: Training loss at epoch 9: 0.8326994478702545
[08/27/2025 15:08:01 INFO]: Training loss at epoch 21: 1.0157575905323029
[08/27/2025 15:08:01 INFO]: Training loss at epoch 6: 1.0163848400115967
[08/27/2025 15:08:02 INFO]: Training loss at epoch 12: 0.8865949213504791
[08/27/2025 15:08:04 INFO]: Training loss at epoch 58: 0.9595758616924286
[08/27/2025 15:08:06 INFO]: Training loss at epoch 23: 1.0848751068115234
[08/27/2025 15:08:10 INFO]: New best epoch, val score: -0.6589034706412974
[08/27/2025 15:08:10 INFO]: Saving model to: blotchy-Amado_trial_22/model_best.pth
[08/27/2025 15:08:14 INFO]: Training loss at epoch 9: 1.0748569667339325
[08/27/2025 15:08:20 INFO]: Training loss at epoch 8: 1.0823511481285095
[08/27/2025 15:08:25 INFO]: New best epoch, val score: -0.6627124182204079
[08/27/2025 15:08:25 INFO]: Saving model to: blotchy-Amado_trial_24/model_best.pth
[08/27/2025 15:08:31 INFO]: Training stats: {
    "score": -1.0024064773348942,
    "rmse": 1.0024064773348942
}
[08/27/2025 15:08:31 INFO]: Val stats: {
    "score": -0.6600124887400184,
    "rmse": 0.6600124887400184
}
[08/27/2025 15:08:31 INFO]: Test stats: {
    "score": -0.8685570309461109,
    "rmse": 0.8685570309461109
}
[08/27/2025 15:08:34 INFO]: Training loss at epoch 33: 0.9096212089061737
[08/27/2025 15:08:34 INFO]: Training loss at epoch 59: 1.0890033841133118
[08/27/2025 15:08:41 INFO]: New best epoch, val score: -0.6600124887400184
[08/27/2025 15:08:41 INFO]: Saving model to: blotchy-Amado_trial_23/model_best.pth
[08/27/2025 15:08:44 INFO]: Training stats: {
    "score": -0.9995496507710465,
    "rmse": 0.9995496507710465
}
[08/27/2025 15:08:44 INFO]: Val stats: {
    "score": -0.6855573751683591,
    "rmse": 0.6855573751683591
}
[08/27/2025 15:08:44 INFO]: Test stats: {
    "score": -0.8725469203287839,
    "rmse": 0.8725469203287839
}
[08/27/2025 15:08:46 INFO]: Training loss at epoch 21: 0.9743161201477051
[08/27/2025 15:08:50 INFO]: Training loss at epoch 18: 0.9627162218093872
[08/27/2025 15:08:51 INFO]: Training loss at epoch 26: 0.8970040082931519
[08/27/2025 15:08:55 INFO]: Training loss at epoch 19: 1.0470316410064697
[08/27/2025 15:08:59 INFO]: Training loss at epoch 9: 0.9051778316497803
[08/27/2025 15:09:06 INFO]: Training loss at epoch 13: 1.1005316376686096
[08/27/2025 15:09:09 INFO]: Training loss at epoch 5: 1.1288873553276062
[08/27/2025 15:09:12 INFO]: Training stats: {
    "score": -1.0159939908849156,
    "rmse": 1.0159939908849156
}
[08/27/2025 15:09:12 INFO]: Val stats: {
    "score": -0.6615288239207099,
    "rmse": 0.6615288239207099
}
[08/27/2025 15:09:12 INFO]: Test stats: {
    "score": -0.8780702601639239,
    "rmse": 0.8780702601639239
}
[08/27/2025 15:09:15 INFO]: Training loss at epoch 9: 0.9905760586261749
[08/27/2025 15:09:15 INFO]: Training loss at epoch 60: 0.817918449640274
[08/27/2025 15:09:16 INFO]: Training stats: {
    "score": -1.0012804644693625,
    "rmse": 1.0012804644693625
}
[08/27/2025 15:09:16 INFO]: Val stats: {
    "score": -0.6826733769103621,
    "rmse": 0.6826733769103621
}
[08/27/2025 15:09:16 INFO]: Test stats: {
    "score": -0.893500229731613,
    "rmse": 0.893500229731613
}
[08/27/2025 15:09:17 INFO]: New best epoch, val score: -0.6615288239207099
[08/27/2025 15:09:17 INFO]: Saving model to: blotchy-Amado_trial_24/model_best.pth
[08/27/2025 15:09:18 INFO]: Training loss at epoch 19: 1.0003427565097809
[08/27/2025 15:09:22 INFO]: Training loss at epoch 24: 0.7740969508886337
[08/27/2025 15:09:23 INFO]: Training loss at epoch 22: 0.8109412491321564
[08/27/2025 15:09:27 INFO]: Training stats: {
    "score": -1.0127694860028627,
    "rmse": 1.0127694860028627
}
[08/27/2025 15:09:27 INFO]: Val stats: {
    "score": -0.6605608934095644,
    "rmse": 0.6605608934095644
}
[08/27/2025 15:09:27 INFO]: Test stats: {
    "score": -0.8732160431716358,
    "rmse": 0.8732160431716358
}
[08/27/2025 15:09:28 INFO]: Training loss at epoch 34: 0.8699598908424377
[08/27/2025 15:09:37 INFO]: New best epoch, val score: -0.6826733769103621
[08/27/2025 15:09:37 INFO]: Saving model to: blotchy-Amado_trial_18/model_best.pth
[08/27/2025 15:09:38 INFO]: New best epoch, val score: -0.6605608934095644
[08/27/2025 15:09:38 INFO]: Saving model to: blotchy-Amado_trial_12/model_best.pth
[08/27/2025 15:09:46 INFO]: Training loss at epoch 61: 1.053418219089508
[08/27/2025 15:09:51 INFO]: Training stats: {
    "score": -0.9898112979376629,
    "rmse": 0.9898112979376629
}
[08/27/2025 15:09:51 INFO]: Val stats: {
    "score": -0.680793328454791,
    "rmse": 0.680793328454791
}
[08/27/2025 15:09:51 INFO]: Test stats: {
    "score": -0.8826832720416337,
    "rmse": 0.8826832720416337
}
[08/27/2025 15:09:51 INFO]: Training loss at epoch 10: 1.0815278887748718
[08/27/2025 15:09:55 INFO]: Training loss at epoch 10: 1.020920991897583
[08/27/2025 15:10:01 INFO]: Training loss at epoch 27: 1.0176222324371338
[08/27/2025 15:10:12 INFO]: Training loss at epoch 22: 0.9221333563327789
[08/27/2025 15:10:14 INFO]: Training loss at epoch 14: 0.9106862545013428
[08/27/2025 15:10:16 INFO]: Training loss at epoch 7: 0.9145102202892303
[08/27/2025 15:10:18 INFO]: Training loss at epoch 7: 1.2385309338569641
[08/27/2025 15:10:18 INFO]: Training loss at epoch 62: 0.8816665709018707
[08/27/2025 15:10:22 INFO]: Training stats: {
    "score": -1.0500785811232578,
    "rmse": 1.0500785811232578
}
[08/27/2025 15:10:22 INFO]: Val stats: {
    "score": -0.6767430648704426,
    "rmse": 0.6767430648704426
}
[08/27/2025 15:10:22 INFO]: Test stats: {
    "score": -0.8999821456501296,
    "rmse": 0.8999821456501296
}
[08/27/2025 15:10:22 INFO]: Training loss at epoch 35: 1.1437996625900269
[08/27/2025 15:10:28 INFO]: Training loss at epoch 12: 1.0296469926834106
[08/27/2025 15:10:29 INFO]: Training loss at epoch 11: 0.9640974700450897
[08/27/2025 15:10:32 INFO]: Training loss at epoch 19: 1.0721436142921448
[08/27/2025 15:10:38 INFO]: Training loss at epoch 25: 0.9413406550884247
[08/27/2025 15:10:44 INFO]: Training loss at epoch 23: 0.9065189361572266
[08/27/2025 15:10:49 INFO]: Training loss at epoch 63: 0.9722683429718018
[08/27/2025 15:11:02 INFO]: Training loss at epoch 20: 1.0880230069160461
[08/27/2025 15:11:07 INFO]: Training loss at epoch 12: 1.0075922012329102
[08/27/2025 15:11:08 INFO]: Training stats: {
    "score": -0.9978633132801666,
    "rmse": 0.9978633132801666
}
[08/27/2025 15:11:08 INFO]: Val stats: {
    "score": -0.6935414963732689,
    "rmse": 0.6935414963732689
}
[08/27/2025 15:11:08 INFO]: Test stats: {
    "score": -0.8786857715376721,
    "rmse": 0.8786857715376721
}
[08/27/2025 15:11:10 INFO]: Training loss at epoch 28: 0.8809268474578857
[08/27/2025 15:11:16 INFO]: Training loss at epoch 36: 0.8951114416122437
[08/27/2025 15:11:18 INFO]: Training loss at epoch 11: 0.694385439157486
[08/27/2025 15:11:20 INFO]: Training loss at epoch 64: 1.0386897325515747
[08/27/2025 15:11:20 INFO]: Training loss at epoch 15: 1.1776143908500671
[08/27/2025 15:11:25 INFO]: Training loss at epoch 20: 1.1493678390979767
[08/27/2025 15:11:29 INFO]: New best epoch, val score: -0.659968775197531
[08/27/2025 15:11:29 INFO]: Saving model to: blotchy-Amado_trial_23/model_best.pth
[08/27/2025 15:11:37 INFO]: Training loss at epoch 23: 1.089942753314972
[08/27/2025 15:11:45 INFO]: Training loss at epoch 13: 0.8706725239753723
[08/27/2025 15:11:50 INFO]: Training loss at epoch 65: 1.0905550122261047
[08/27/2025 15:11:54 INFO]: Training loss at epoch 26: 1.0992391109466553
[08/27/2025 15:12:08 INFO]: Training loss at epoch 24: 1.0127495527267456
[08/27/2025 15:12:14 INFO]: Training loss at epoch 37: 0.8879657685756683
[08/27/2025 15:12:21 INFO]: Training loss at epoch 29: 0.9846732020378113
[08/27/2025 15:12:23 INFO]: Training loss at epoch 66: 0.9481673538684845
[08/27/2025 15:12:23 INFO]: Training loss at epoch 5: 0.994834691286087
[08/27/2025 15:12:27 INFO]: Training loss at epoch 14: 0.9251525402069092
[08/27/2025 15:12:28 INFO]: Training loss at epoch 16: 0.9227838516235352
[08/27/2025 15:12:28 INFO]: Training loss at epoch 10: 0.9368562698364258
[08/27/2025 15:12:32 INFO]: Training loss at epoch 8: 1.0908378958702087
[08/27/2025 15:12:42 INFO]: Training loss at epoch 21: 1.0243759155273438
[08/27/2025 15:12:43 INFO]: Training loss at epoch 12: 1.2876316010951996
[08/27/2025 15:12:47 INFO]: Training stats: {
    "score": -1.0005549854162448,
    "rmse": 1.0005549854162448
}
[08/27/2025 15:12:47 INFO]: Val stats: {
    "score": -0.6944774853907681,
    "rmse": 0.6944774853907681
}
[08/27/2025 15:12:47 INFO]: Test stats: {
    "score": -0.8812648393148912,
    "rmse": 0.8812648393148912
}
[08/27/2025 15:12:51 INFO]: Training loss at epoch 20: 1.2361209988594055
[08/27/2025 15:12:51 INFO]: New best epoch, val score: -0.6634444886682624
[08/27/2025 15:12:51 INFO]: Saving model to: blotchy-Amado_trial_18/model_best.pth
[08/27/2025 15:12:55 INFO]: New best epoch, val score: -0.659813511764355
[08/27/2025 15:12:55 INFO]: Saving model to: blotchy-Amado_trial_23/model_best.pth
[08/27/2025 15:12:56 INFO]: Training loss at epoch 67: 1.2824221551418304
[08/27/2025 15:13:04 INFO]: Training loss at epoch 13: 1.2599033117294312
[08/27/2025 15:13:05 INFO]: Training loss at epoch 21: 1.0731545686721802
[08/27/2025 15:13:06 INFO]: Training loss at epoch 24: 1.098173201084137
[08/27/2025 15:13:07 INFO]: Training loss at epoch 15: 0.9979738593101501
[08/27/2025 15:13:12 INFO]: Training loss at epoch 38: 0.9819126725196838
[08/27/2025 15:13:14 INFO]: Training loss at epoch 2: 1.7074685096740723
[08/27/2025 15:13:16 INFO]: Training loss at epoch 27: 1.0048566460609436
[08/27/2025 15:13:26 INFO]: New best epoch, val score: -0.6625051402883257
[08/27/2025 15:13:26 INFO]: Saving model to: blotchy-Amado_trial_7/model_best.pth
[08/27/2025 15:13:28 INFO]: Training loss at epoch 68: 1.1330934762954712
[08/27/2025 15:13:37 INFO]: Training loss at epoch 25: 0.947373777627945
[08/27/2025 15:13:37 INFO]: Training loss at epoch 17: 0.9996272027492523
[08/27/2025 15:13:47 INFO]: Training loss at epoch 10: 0.9832509160041809
[08/27/2025 15:13:50 INFO]: Training loss at epoch 16: 1.147275984287262
[08/27/2025 15:14:01 INFO]: Training loss at epoch 30: 1.0638468861579895
[08/27/2025 15:14:01 INFO]: Training loss at epoch 69: 0.9430155158042908
[08/27/2025 15:14:10 INFO]: Training loss at epoch 39: 0.9342791140079498
[08/27/2025 15:14:10 INFO]: Training loss at epoch 13: 1.1667299270629883
[08/27/2025 15:14:12 INFO]: Training stats: {
    "score": -0.9990765329736325,
    "rmse": 0.9990765329736325
}
[08/27/2025 15:14:12 INFO]: Val stats: {
    "score": -0.6871594004990388,
    "rmse": 0.6871594004990388
}
[08/27/2025 15:14:12 INFO]: Test stats: {
    "score": -0.8734434547364807,
    "rmse": 0.8734434547364807
}
[08/27/2025 15:14:22 INFO]: Training loss at epoch 22: 1.0581895112991333
[08/27/2025 15:14:28 INFO]: Training loss at epoch 17: 1.3044424653053284
[08/27/2025 15:14:29 INFO]: Training stats: {
    "score": -0.9481840444040305,
    "rmse": 0.9481840444040305
}
[08/27/2025 15:14:29 INFO]: Val stats: {
    "score": -0.6800437907707404,
    "rmse": 0.6800437907707404
}
[08/27/2025 15:14:29 INFO]: Test stats: {
    "score": -0.895671753191403,
    "rmse": 0.895671753191403
}
[08/27/2025 15:14:36 INFO]: Training loss at epoch 25: 1.2386250793933868
[08/27/2025 15:14:36 INFO]: Training loss at epoch 28: 0.9260270893573761
[08/27/2025 15:14:36 INFO]: Training loss at epoch 21: 0.9047649502754211
[08/27/2025 15:14:42 INFO]: Training loss at epoch 8: 1.1469759345054626
[08/27/2025 15:14:44 INFO]: Training loss at epoch 70: 1.2291072607040405
[08/27/2025 15:14:45 INFO]: Training loss at epoch 18: 1.0371409058570862
[08/27/2025 15:14:48 INFO]: Training loss at epoch 22: 0.9874557852745056
[08/27/2025 15:14:48 INFO]: New best epoch, val score: -0.7102194253216392
[08/27/2025 15:14:48 INFO]: Saving model to: blotchy-Amado_trial_16/model_best.pth
[08/27/2025 15:14:53 INFO]: Training loss at epoch 6: 0.8214943706989288
[08/27/2025 15:14:53 INFO]: Training loss at epoch 9: 1.1347679495811462
[08/27/2025 15:15:04 INFO]: Training loss at epoch 26: 0.7377016991376877
[08/27/2025 15:15:10 INFO]: Training loss at epoch 18: 1.0117630362510681
[08/27/2025 15:15:13 INFO]: Training loss at epoch 31: 0.8442841470241547
[08/27/2025 15:15:15 INFO]: Training loss at epoch 71: 1.0615389943122864
[08/27/2025 15:15:18 INFO]: Running Final Evaluation...
[08/27/2025 15:15:26 INFO]: Training loss at epoch 40: 0.8862951099872589
[08/27/2025 15:15:31 INFO]: Training accuracy: {
    "score": -1.008864310389764,
    "rmse": 1.008864310389764
}
[08/27/2025 15:15:31 INFO]: Val accuracy: {
    "score": -0.6676377952280655,
    "rmse": 0.6676377952280655
}
[08/27/2025 15:15:31 INFO]: Test accuracy: {
    "score": -0.8685331331833177,
    "rmse": 0.8685331331833177
}
[08/27/2025 15:15:31 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_6",
    "best_epoch": 40,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8685331331833177,
        "rmse": 0.8685331331833177
    },
    "train_stats": {
        "score": -1.008864310389764,
        "rmse": 1.008864310389764
    },
    "val_stats": {
        "score": -0.6676377952280655,
        "rmse": 0.6676377952280655
    }
}
[08/27/2025 15:15:31 INFO]: Procewss finished for trial blotchy-Amado_trial_6
[08/27/2025 15:15:31 INFO]: 
_________________________________________________

[08/27/2025 15:15:31 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:15:31 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 8
  d_ffn_factor: 0.8897101333114837
  attention_dropout: 0.29125318168967673
  ffn_dropout: 0.29125318168967673
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00015605711747438582
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_25

[08/27/2025 15:15:31 INFO]: This ft_transformer has 0.166 million parameters.
[08/27/2025 15:15:31 INFO]: Training will start at epoch 0.
[08/27/2025 15:15:31 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:15:37 INFO]: Training loss at epoch 14: 1.0035135746002197
[08/27/2025 15:15:41 INFO]: Training stats: {
    "score": -0.9998696934688479,
    "rmse": 0.9998696934688479
}
[08/27/2025 15:15:41 INFO]: Val stats: {
    "score": -0.6817936891507078,
    "rmse": 0.6817936891507078
}
[08/27/2025 15:15:41 INFO]: Test stats: {
    "score": -0.873487490642824,
    "rmse": 0.873487490642824
}
[08/27/2025 15:15:44 INFO]: Training loss at epoch 14: 0.9817468225955963
[08/27/2025 15:15:49 INFO]: Training loss at epoch 11: 0.9746260046958923
[08/27/2025 15:15:51 INFO]: Training loss at epoch 19: 0.9617051780223846
[08/27/2025 15:15:53 INFO]: Training loss at epoch 19: 0.8401682078838348
[08/27/2025 15:15:57 INFO]: Training loss at epoch 29: 1.0313649773597717
[08/27/2025 15:16:04 INFO]: Training loss at epoch 23: 1.043886661529541
[08/27/2025 15:16:06 INFO]: Training stats: {
    "score": -0.9917707448991632,
    "rmse": 0.9917707448991632
}
[08/27/2025 15:16:06 INFO]: Val stats: {
    "score": -0.672632677393108,
    "rmse": 0.672632677393108
}
[08/27/2025 15:16:06 INFO]: Test stats: {
    "score": -0.8740878181325755,
    "rmse": 0.8740878181325755
}
[08/27/2025 15:16:06 INFO]: Training loss at epoch 26: 0.8634787797927856
[08/27/2025 15:16:13 INFO]: New best epoch, val score: -0.6549243309482161
[08/27/2025 15:16:13 INFO]: Saving model to: blotchy-Amado_trial_18/model_best.pth
[08/27/2025 15:16:14 INFO]: Training loss at epoch 0: 1.076608955860138
[08/27/2025 15:16:19 INFO]: Training stats: {
    "score": -0.9989780054052426,
    "rmse": 0.9989780054052426
}
[08/27/2025 15:16:19 INFO]: Val stats: {
    "score": -0.6912209138398122,
    "rmse": 0.6912209138398122
}
[08/27/2025 15:16:19 INFO]: Test stats: {
    "score": -0.8833300694816469,
    "rmse": 0.8833300694816469
}
[08/27/2025 15:16:20 INFO]: New best epoch, val score: -0.6647534353204361
[08/27/2025 15:16:20 INFO]: Saving model to: blotchy-Amado_trial_25/model_best.pth
[08/27/2025 15:16:21 INFO]: Training loss at epoch 22: 0.9642159938812256
[08/27/2025 15:16:23 INFO]: Training stats: {
    "score": -1.0170387977204944,
    "rmse": 1.0170387977204944
}
[08/27/2025 15:16:23 INFO]: Val stats: {
    "score": -0.6605519071517911,
    "rmse": 0.6605519071517911
}
[08/27/2025 15:16:23 INFO]: Test stats: {
    "score": -0.8778289278855784,
    "rmse": 0.8778289278855784
}
[08/27/2025 15:16:24 INFO]: Training loss at epoch 41: 0.9898129105567932
[08/27/2025 15:16:26 INFO]: Training loss at epoch 32: 1.0969966053962708
[08/27/2025 15:16:30 INFO]: Training loss at epoch 23: 0.9310821294784546
[08/27/2025 15:16:31 INFO]: Training loss at epoch 27: 0.8990633487701416
[08/27/2025 15:16:45 INFO]: Training loss at epoch 20: 0.8633835017681122
[08/27/2025 15:17:03 INFO]: Training loss at epoch 15: 0.8699992001056671
[08/27/2025 15:17:03 INFO]: Training loss at epoch 1: 1.1191973686218262
[08/27/2025 15:17:13 INFO]: Training loss at epoch 11: 1.144893079996109
[08/27/2025 15:17:21 INFO]: Training loss at epoch 42: 1.0044150650501251
[08/27/2025 15:17:26 INFO]: Training loss at epoch 21: 0.9046390652656555
[08/27/2025 15:17:28 INFO]: Training loss at epoch 20: 1.1327588856220245
[08/27/2025 15:17:34 INFO]: Training loss at epoch 27: 0.8014984428882599
[08/27/2025 15:17:40 INFO]: Training loss at epoch 33: 0.9738959074020386
[08/27/2025 15:17:43 INFO]: Training loss at epoch 24: 0.9292208552360535
[08/27/2025 15:17:45 INFO]: Training loss at epoch 30: 0.835433304309845
[08/27/2025 15:17:53 INFO]: Training loss at epoch 2: 1.37151899933815
[08/27/2025 15:18:01 INFO]: Training loss at epoch 28: 1.0536319613456726
[08/27/2025 15:18:03 INFO]: Training loss at epoch 10: 0.8959011733531952
[08/27/2025 15:18:07 INFO]: Training loss at epoch 23: 0.8915939033031464
[08/27/2025 15:18:07 INFO]: Training loss at epoch 22: 1.0425843000411987
[08/27/2025 15:18:11 INFO]: New best epoch, val score: -0.6612836191252096
[08/27/2025 15:18:11 INFO]: Saving model to: blotchy-Amado_trial_20/model_best.pth
[08/27/2025 15:18:14 INFO]: Training loss at epoch 24: 0.8734510838985443
[08/27/2025 15:18:20 INFO]: Training loss at epoch 43: 0.7769360542297363
[08/27/2025 15:18:25 INFO]: Training loss at epoch 15: 1.0491295456886292
[08/27/2025 15:18:31 INFO]: Training loss at epoch 16: 1.2175554633140564
[08/27/2025 15:18:36 INFO]: Training loss at epoch 21: 0.9827893972396851
[08/27/2025 15:18:40 INFO]: Training loss at epoch 3: 0.8482824265956879
[08/27/2025 15:18:46 INFO]: Training loss at epoch 6: 1.07760888338089
[08/27/2025 15:18:46 INFO]: Training loss at epoch 23: 0.9686196744441986
[08/27/2025 15:18:52 INFO]: Training loss at epoch 34: 0.9876479208469391
[08/27/2025 15:19:02 INFO]: Training loss at epoch 28: 1.1673616766929626
[08/27/2025 15:19:03 INFO]: Training loss at epoch 31: 0.9852307140827179
[08/27/2025 15:19:08 INFO]: Training loss at epoch 12: 1.14475017786026
[08/27/2025 15:19:10 INFO]: Training loss at epoch 9: 1.336448073387146
[08/27/2025 15:19:15 INFO]: Training loss at epoch 44: 0.7632197737693787
[08/27/2025 15:19:20 INFO]: Training loss at epoch 25: 0.7872922867536545
[08/27/2025 15:19:24 INFO]: Training loss at epoch 24: 0.9586029946804047
[08/27/2025 15:19:25 INFO]: Training loss at epoch 29: 1.0588847994804382
[08/27/2025 15:19:26 INFO]: Training loss at epoch 4: 1.0471888184547424
[08/27/2025 15:19:29 INFO]: New best epoch, val score: -0.6509419468418597
[08/27/2025 15:19:29 INFO]: Saving model to: blotchy-Amado_trial_18/model_best.pth
[08/27/2025 15:19:41 INFO]: Training loss at epoch 22: 1.0410995483398438
[08/27/2025 15:19:46 INFO]: Training loss at epoch 24: 1.2285822033882141
[08/27/2025 15:19:49 INFO]: Training loss at epoch 25: 0.8485134243965149
[08/27/2025 15:19:53 INFO]: Training stats: {
    "score": -0.9994691366246873,
    "rmse": 0.9994691366246873
}
[08/27/2025 15:19:53 INFO]: Val stats: {
    "score": -0.6581456437991539,
    "rmse": 0.6581456437991539
}
[08/27/2025 15:19:53 INFO]: Test stats: {
    "score": -0.8690474134667737,
    "rmse": 0.8690474134667737
}
[08/27/2025 15:19:53 INFO]: Training loss at epoch 17: 0.9448915719985962
[08/27/2025 15:20:02 INFO]: Training loss at epoch 35: 0.9021328389644623
[08/27/2025 15:20:02 INFO]: Training loss at epoch 25: 1.1804574131965637
[08/27/2025 15:20:03 INFO]: New best epoch, val score: -0.6581456437991539
[08/27/2025 15:20:03 INFO]: Saving model to: blotchy-Amado_trial_20/model_best.pth
[08/27/2025 15:20:09 INFO]: Training loss at epoch 45: 0.952352911233902
[08/27/2025 15:20:13 INFO]: Training loss at epoch 5: 1.087045669555664
[08/27/2025 15:20:16 INFO]: Training loss at epoch 11: 1.0050538778305054
[08/27/2025 15:20:21 INFO]: Training loss at epoch 32: 0.9031897783279419
[08/27/2025 15:20:28 INFO]: Training loss at epoch 29: 1.1389749944210052
[08/27/2025 15:20:32 INFO]: Training loss at epoch 12: 0.9175941050052643
[08/27/2025 15:20:34 INFO]: Training loss at epoch 7: 0.9188046455383301
[08/27/2025 15:20:37 INFO]: Training stats: {
    "score": -1.0791182222092612,
    "rmse": 1.0791182222092612
}
[08/27/2025 15:20:37 INFO]: Val stats: {
    "score": -0.8813542687066269,
    "rmse": 0.8813542687066269
}
[08/27/2025 15:20:37 INFO]: Test stats: {
    "score": -1.015441929468304,
    "rmse": 1.015441929468304
}
[08/27/2025 15:20:41 INFO]: Training loss at epoch 26: 1.0128079950809479
[08/27/2025 15:20:46 INFO]: Training loss at epoch 23: 0.9724481105804443
[08/27/2025 15:20:55 INFO]: Training loss at epoch 26: 1.0496087074279785
[08/27/2025 15:20:58 INFO]: Training loss at epoch 16: 0.8775860071182251
[08/27/2025 15:20:59 INFO]: Training loss at epoch 6: 0.9257006645202637
[08/27/2025 15:21:00 INFO]: Training stats: {
    "score": -1.0003191541638725,
    "rmse": 1.0003191541638725
}
[08/27/2025 15:21:00 INFO]: Val stats: {
    "score": -0.6749607816268032,
    "rmse": 0.6749607816268032
}
[08/27/2025 15:21:00 INFO]: Test stats: {
    "score": -0.8717879594347523,
    "rmse": 0.8717879594347523
}
[08/27/2025 15:21:03 INFO]: Training loss at epoch 46: 0.7493416666984558
[08/27/2025 15:21:11 INFO]: Training loss at epoch 36: 1.1195881366729736
[08/27/2025 15:21:15 INFO]: Training loss at epoch 30: 0.9752631783485413
[08/27/2025 15:21:17 INFO]: Training loss at epoch 18: 0.7947750687599182
[08/27/2025 15:21:19 INFO]: Training loss at epoch 27: 1.2515670359134674
[08/27/2025 15:21:24 INFO]: Training loss at epoch 26: 0.97612264752388
[08/27/2025 15:21:26 INFO]: Training loss at epoch 25: 0.9407188892364502
[08/27/2025 15:21:37 INFO]: Training loss at epoch 33: 1.0007103383541107
[08/27/2025 15:21:46 INFO]: Training loss at epoch 7: 1.157500982284546
[08/27/2025 15:21:52 INFO]: Training loss at epoch 24: 1.0504305958747864
[08/27/2025 15:22:00 INFO]: Training loss at epoch 47: 1.027076005935669
[08/27/2025 15:22:00 INFO]: Training loss at epoch 28: 1.1397895216941833
[08/27/2025 15:22:20 INFO]: Training loss at epoch 13: 0.9668818712234497
[08/27/2025 15:22:24 INFO]: Training loss at epoch 37: 0.936980277299881
[08/27/2025 15:22:27 INFO]: Training loss at epoch 30: 0.9290564060211182
[08/27/2025 15:22:33 INFO]: Training loss at epoch 12: 0.9018264412879944
[08/27/2025 15:22:34 INFO]: Training loss at epoch 27: 0.9341498613357544
[08/27/2025 15:22:36 INFO]: Training loss at epoch 8: 0.7882170081138611
[08/27/2025 15:22:41 INFO]: Training loss at epoch 29: 1.1427893042564392
[08/27/2025 15:22:43 INFO]: Training loss at epoch 31: 0.9187603890895844
[08/27/2025 15:22:43 INFO]: Training loss at epoch 19: 0.9985484778881073
[08/27/2025 15:22:43 INFO]: New best epoch, val score: -0.6496974889231703
[08/27/2025 15:22:43 INFO]: Saving model to: blotchy-Amado_trial_18/model_best.pth
[08/27/2025 15:22:47 INFO]: New best epoch, val score: -0.660560259628813
[08/27/2025 15:22:47 INFO]: Saving model to: blotchy-Amado_trial_12/model_best.pth
[08/27/2025 15:22:54 INFO]: Training stats: {
    "score": -0.9958227154860945,
    "rmse": 0.9958227154860945
}
[08/27/2025 15:22:54 INFO]: Val stats: {
    "score": -0.7215074381987472,
    "rmse": 0.7215074381987472
}
[08/27/2025 15:22:54 INFO]: Test stats: {
    "score": -0.9035396264493829,
    "rmse": 0.9035396264493829
}
[08/27/2025 15:22:56 INFO]: Training loss at epoch 48: 0.8415801227092743
[08/27/2025 15:22:57 INFO]: Training loss at epoch 34: 0.9038946628570557
[08/27/2025 15:22:59 INFO]: Training loss at epoch 25: 0.8763075768947601
[08/27/2025 15:23:04 INFO]: Training loss at epoch 27: 0.8605717718601227
[08/27/2025 15:23:10 INFO]: Training loss at epoch 26: 0.9050635099411011
[08/27/2025 15:23:14 INFO]: Training stats: {
    "score": -1.0007970010936487,
    "rmse": 1.0007970010936487
}
[08/27/2025 15:23:14 INFO]: Val stats: {
    "score": -0.6622628101998473,
    "rmse": 0.6622628101998473
}
[08/27/2025 15:23:14 INFO]: Test stats: {
    "score": -0.8688657879754306,
    "rmse": 0.8688657879754306
}
[08/27/2025 15:23:22 INFO]: Training loss at epoch 9: 0.9292744994163513
[08/27/2025 15:23:32 INFO]: Training loss at epoch 30: 0.8710209131240845
[08/27/2025 15:23:34 INFO]: Training loss at epoch 38: 0.8551361560821533
[08/27/2025 15:23:34 INFO]: Training loss at epoch 17: 1.2075235247612
[08/27/2025 15:23:39 INFO]: Training stats: {
    "score": -1.0094621993827295,
    "rmse": 1.0094621993827295
}
[08/27/2025 15:23:39 INFO]: Val stats: {
    "score": -0.7304786428589205,
    "rmse": 0.7304786428589205
}
[08/27/2025 15:23:39 INFO]: Test stats: {
    "score": -0.9005476021380161,
    "rmse": 0.9005476021380161
}
[08/27/2025 15:23:51 INFO]: Training loss at epoch 49: 1.051451861858368
[08/27/2025 15:23:52 INFO]: Training loss at epoch 31: 0.8902676999568939
[08/27/2025 15:23:53 INFO]: Training loss at epoch 13: 1.1468793153762817
[08/27/2025 15:24:03 INFO]: Training loss at epoch 26: 0.8965363800525665
[08/27/2025 15:24:05 INFO]: Training loss at epoch 32: 0.9848244488239288
[08/27/2025 15:24:10 INFO]: Training stats: {
    "score": -0.9381810930396981,
    "rmse": 0.9381810930396981
}
[08/27/2025 15:24:10 INFO]: Val stats: {
    "score": -0.7167546707529772,
    "rmse": 0.7167546707529772
}
[08/27/2025 15:24:10 INFO]: Test stats: {
    "score": -0.9286448793750466,
    "rmse": 0.9286448793750466
}
[08/27/2025 15:24:11 INFO]: Training loss at epoch 28: 0.9691123366355896
[08/27/2025 15:24:11 INFO]: Training loss at epoch 31: 0.7134607583284378
[08/27/2025 15:24:14 INFO]: Training loss at epoch 35: 0.898547500371933
[08/27/2025 15:24:27 INFO]: Training loss at epoch 10: 1.0190978646278381
[08/27/2025 15:24:39 INFO]: Training loss at epoch 20: 1.0645884275436401
[08/27/2025 15:24:43 INFO]: Training loss at epoch 28: 1.2713496685028076
[08/27/2025 15:24:48 INFO]: Training loss at epoch 39: 0.8793077170848846
[08/27/2025 15:24:50 INFO]: Training loss at epoch 13: 0.9598517417907715
[08/27/2025 15:24:53 INFO]: Training loss at epoch 32: 1.1284343600273132
[08/27/2025 15:24:54 INFO]: Training loss at epoch 27: 1.052700698375702
[08/27/2025 15:24:55 INFO]: Training loss at epoch 7: 1.1252243518829346
[08/27/2025 15:24:57 INFO]: Training loss at epoch 10: 1.2714306116104126
[08/27/2025 15:25:09 INFO]: Training loss at epoch 50: 0.7831839323043823
[08/27/2025 15:25:13 INFO]: Training loss at epoch 27: 1.0068251192569733
[08/27/2025 15:25:15 INFO]: Training stats: {
    "score": -1.003192039090051,
    "rmse": 1.003192039090051
}
[08/27/2025 15:25:15 INFO]: Val stats: {
    "score": -0.7082337774977205,
    "rmse": 0.7082337774977205
}
[08/27/2025 15:25:15 INFO]: Test stats: {
    "score": -0.8893241151822342,
    "rmse": 0.8893241151822342
}
[08/27/2025 15:25:16 INFO]: Training loss at epoch 11: 1.1091758906841278
[08/27/2025 15:25:17 INFO]: Running Final Evaluation...
[08/27/2025 15:25:23 INFO]: Training loss at epoch 32: 1.0385527610778809
[08/27/2025 15:25:33 INFO]: Training loss at epoch 33: 0.9576027691364288
[08/27/2025 15:25:35 INFO]: Training loss at epoch 33: 0.8639639616012573
[08/27/2025 15:25:35 INFO]: Training loss at epoch 36: 0.995212972164154
[08/27/2025 15:25:36 INFO]: Training accuracy: {
    "score": -0.9806132509729364,
    "rmse": 0.9806132509729364
}
[08/27/2025 15:25:36 INFO]: Val accuracy: {
    "score": -0.6578732485257733,
    "rmse": 0.6578732485257733
}
[08/27/2025 15:25:36 INFO]: Test accuracy: {
    "score": -0.8776571263098406,
    "rmse": 0.8776571263098406
}
[08/27/2025 15:25:36 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_13",
    "best_epoch": 19,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8776571263098406,
        "rmse": 0.8776571263098406
    },
    "train_stats": {
        "score": -0.9806132509729364,
        "rmse": 0.9806132509729364
    },
    "val_stats": {
        "score": -0.6578732485257733,
        "rmse": 0.6578732485257733
    }
}
[08/27/2025 15:25:36 INFO]: Procewss finished for trial blotchy-Amado_trial_13
[08/27/2025 15:25:36 INFO]: 
_________________________________________________

[08/27/2025 15:25:36 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:25:36 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 1.3144238915775257
  attention_dropout: 0.22581618901346684
  ffn_dropout: 0.22581618901346684
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0002901181258181132
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_26

[08/27/2025 15:25:36 INFO]: This ft_transformer has 0.154 million parameters.
[08/27/2025 15:25:36 INFO]: Training will start at epoch 0.
[08/27/2025 15:25:36 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:25:38 INFO]: Training loss at epoch 14: 0.9423193335533142
[08/27/2025 15:25:54 INFO]: Training loss at epoch 29: 1.1836153864860535
[08/27/2025 15:25:55 INFO]: Training loss at epoch 0: 1.059394121170044
[08/27/2025 15:25:57 INFO]: New best epoch, val score: -0.68134012795166
[08/27/2025 15:25:57 INFO]: Saving model to: blotchy-Amado_trial_26/model_best.pth
[08/27/2025 15:26:05 INFO]: Training loss at epoch 12: 0.7628234922885895
[08/27/2025 15:26:07 INFO]: Training loss at epoch 21: 0.9149685502052307
[08/27/2025 15:26:14 INFO]: Training loss at epoch 1: 1.0876150727272034
[08/27/2025 15:26:14 INFO]: Training loss at epoch 34: 0.8887519538402557
[08/27/2025 15:26:15 INFO]: Training loss at epoch 18: 0.8172860741615295
[08/27/2025 15:26:15 INFO]: Training loss at epoch 8: 1.1815763115882874
[08/27/2025 15:26:17 INFO]: Training loss at epoch 3: 1.222455620765686
[08/27/2025 15:26:23 INFO]: Training loss at epoch 28: 1.0739984810352325
[08/27/2025 15:26:25 INFO]: Training loss at epoch 29: 1.0522897243499756
[08/27/2025 15:26:27 INFO]: Training loss at epoch 40: 0.9921542108058929
[08/27/2025 15:26:27 INFO]: Training stats: {
    "score": -1.0205916450138557,
    "rmse": 1.0205916450138557
}
[08/27/2025 15:26:27 INFO]: Val stats: {
    "score": -0.6637715530836393,
    "rmse": 0.6637715530836393
}
[08/27/2025 15:26:27 INFO]: Test stats: {
    "score": -0.8799184043242683,
    "rmse": 0.8799184043242683
}
[08/27/2025 15:26:33 INFO]: Training loss at epoch 2: 1.0394980311393738
[08/27/2025 15:26:36 INFO]: New best epoch, val score: -0.6622464185474473
[08/27/2025 15:26:36 INFO]: Saving model to: blotchy-Amado_trial_7/model_best.pth
[08/27/2025 15:26:40 INFO]: Training loss at epoch 28: 0.9773626029491425
[08/27/2025 15:26:52 INFO]: Training loss at epoch 33: 1.3738961219787598
[08/27/2025 15:26:53 INFO]: Training loss at epoch 13: 1.237047165632248
[08/27/2025 15:26:53 INFO]: Training loss at epoch 3: 1.024380624294281
[08/27/2025 15:26:55 INFO]: New best epoch, val score: -0.6804779984750683
[08/27/2025 15:26:55 INFO]: Saving model to: blotchy-Amado_trial_26/model_best.pth
[08/27/2025 15:26:56 INFO]: Training loss at epoch 37: 1.0487699806690216
[08/27/2025 15:26:57 INFO]: Training loss at epoch 35: 0.838140070438385
[08/27/2025 15:27:00 INFO]: Training stats: {
    "score": -0.9884187048881492,
    "rmse": 0.9884187048881492
}
[08/27/2025 15:27:00 INFO]: Val stats: {
    "score": -0.674569481770578,
    "rmse": 0.674569481770578
}
[08/27/2025 15:27:00 INFO]: Test stats: {
    "score": -0.8823776358370261,
    "rmse": 0.8823776358370261
}
[08/27/2025 15:27:03 INFO]: Training loss at epoch 34: 0.9346775114536285
[08/27/2025 15:27:12 INFO]: Training loss at epoch 14: 1.2481018602848053
[08/27/2025 15:27:13 INFO]: Training loss at epoch 4: 0.9467646479606628
[08/27/2025 15:27:15 INFO]: New best epoch, val score: -0.6732057482751695
[08/27/2025 15:27:15 INFO]: Saving model to: blotchy-Amado_trial_26/model_best.pth
[08/27/2025 15:27:24 INFO]: Training loss at epoch 14: 1.0894229412078857
[08/27/2025 15:27:34 INFO]: Training loss at epoch 5: 0.9963676631450653
[08/27/2025 15:27:35 INFO]: Training loss at epoch 22: 0.9007556438446045
[08/27/2025 15:27:35 INFO]: Training loss at epoch 29: 0.9249724447727203
[08/27/2025 15:27:38 INFO]: Training loss at epoch 36: 0.9435788094997406
[08/27/2025 15:27:41 INFO]: Training loss at epoch 41: 0.9763967096805573
[08/27/2025 15:27:43 INFO]: Training loss at epoch 14: 1.0480272769927979
[08/27/2025 15:27:49 INFO]: New best epoch, val score: -0.6637843524562785
[08/27/2025 15:27:49 INFO]: Saving model to: blotchy-Amado_trial_25/model_best.pth
[08/27/2025 15:27:52 INFO]: Training loss at epoch 6: 0.9823399484157562
[08/27/2025 15:27:59 INFO]: Training stats: {
    "score": -1.0024098997253301,
    "rmse": 1.0024098997253301
}
[08/27/2025 15:27:59 INFO]: Val stats: {
    "score": -0.6622998355299626,
    "rmse": 0.6622998355299626
}
[08/27/2025 15:27:59 INFO]: Test stats: {
    "score": -0.8719552507174213,
    "rmse": 0.8719552507174213
}
[08/27/2025 15:28:07 INFO]: Training loss at epoch 30: 1.3773022294044495
[08/27/2025 15:28:10 INFO]: Training loss at epoch 7: 1.0557235479354858
[08/27/2025 15:28:14 INFO]: Training loss at epoch 38: 0.9561905860900879
[08/27/2025 15:28:17 INFO]: Training loss at epoch 37: 1.0083484649658203
[08/27/2025 15:28:20 INFO]: Training loss at epoch 34: 1.047532081604004
[08/27/2025 15:28:24 INFO]: Training loss at epoch 29: 1.0603098273277283
[08/27/2025 15:28:27 INFO]: Training loss at epoch 35: 0.9644220471382141
[08/27/2025 15:28:29 INFO]: Training loss at epoch 8: 1.0627965927124023
[08/27/2025 15:28:29 INFO]: Training loss at epoch 15: 1.161280244588852
[08/27/2025 15:28:35 INFO]: New best epoch, val score: -0.6626691563315809
[08/27/2025 15:28:35 INFO]: Saving model to: blotchy-Amado_trial_25/model_best.pth
[08/27/2025 15:28:38 INFO]: Training loss at epoch 30: 0.7805077135562897
[08/27/2025 15:28:48 INFO]: Training loss at epoch 9: 0.8984872102737427
[08/27/2025 15:28:51 INFO]: Training loss at epoch 42: 1.0356923341751099
[08/27/2025 15:28:53 INFO]: Training loss at epoch 15: 0.8957124650478363
[08/27/2025 15:28:53 INFO]: Training loss at epoch 19: 0.9216906428337097
[08/27/2025 15:28:54 INFO]: Training stats: {
    "score": -0.9971680304628353,
    "rmse": 0.9971680304628353
}
[08/27/2025 15:28:54 INFO]: Val stats: {
    "score": -0.6753374530181424,
    "rmse": 0.6753374530181424
}
[08/27/2025 15:28:54 INFO]: Test stats: {
    "score": -0.8756372105539038,
    "rmse": 0.8756372105539038
}
[08/27/2025 15:28:56 INFO]: Training loss at epoch 38: 1.0580370128154755
[08/27/2025 15:28:57 INFO]: Training loss at epoch 23: 1.010298490524292
[08/27/2025 15:29:01 INFO]: Training stats: {
    "score": -0.997954448828018,
    "rmse": 0.997954448828018
}
[08/27/2025 15:29:01 INFO]: Val stats: {
    "score": -0.6833028921467833,
    "rmse": 0.6833028921467833
}
[08/27/2025 15:29:01 INFO]: Test stats: {
    "score": -0.8739634411682345,
    "rmse": 0.8739634411682345
}
[08/27/2025 15:29:06 INFO]: Training loss at epoch 30: 1.2627902925014496
[08/27/2025 15:29:12 INFO]: Training loss at epoch 10: 0.9592284560203552
[08/27/2025 15:29:16 INFO]: Training loss at epoch 16: 1.0846270322799683
[08/27/2025 15:29:19 INFO]: Training loss at epoch 11: 1.0883845686912537
[08/27/2025 15:29:26 INFO]: Training loss at epoch 15: 0.842373788356781
[08/27/2025 15:29:29 INFO]: Training loss at epoch 39: 0.9170724749565125
[08/27/2025 15:29:30 INFO]: Training loss at epoch 11: 1.0560022592544556
[08/27/2025 15:29:34 INFO]: Training loss at epoch 39: 0.8180361092090607
[08/27/2025 15:29:41 INFO]: Training loss at epoch 31: 0.8898498117923737
[08/27/2025 15:29:46 INFO]: Training loss at epoch 35: 1.0407399535179138
[08/27/2025 15:29:47 INFO]: Training stats: {
    "score": -0.9845145954118891,
    "rmse": 0.9845145954118891
}
[08/27/2025 15:29:47 INFO]: Val stats: {
    "score": -0.6653850356909226,
    "rmse": 0.6653850356909226
}
[08/27/2025 15:29:47 INFO]: Test stats: {
    "score": -0.8746233597739383,
    "rmse": 0.8746233597739383
}
[08/27/2025 15:29:49 INFO]: Training loss at epoch 12: 1.0175888538360596
[08/27/2025 15:29:50 INFO]: Training loss at epoch 36: 0.9951100647449493
[08/27/2025 15:29:50 INFO]: Training stats: {
    "score": -1.0086453811208873,
    "rmse": 1.0086453811208873
}
[08/27/2025 15:29:50 INFO]: Val stats: {
    "score": -0.6623667813810311,
    "rmse": 0.6623667813810311
}
[08/27/2025 15:29:50 INFO]: Test stats: {
    "score": -0.8670439511770965,
    "rmse": 0.8670439511770965
}
[08/27/2025 15:29:51 INFO]: New best epoch, val score: -0.6707121163816375
[08/27/2025 15:29:51 INFO]: Saving model to: blotchy-Amado_trial_26/model_best.pth
[08/27/2025 15:29:54 INFO]: Training stats: {
    "score": -0.998091936139841,
    "rmse": 0.998091936139841
}
[08/27/2025 15:29:54 INFO]: Val stats: {
    "score": -0.6755968407173725,
    "rmse": 0.6755968407173725
}
[08/27/2025 15:29:54 INFO]: Test stats: {
    "score": -0.8770776094978419,
    "rmse": 0.8770776094978419
}
[08/27/2025 15:30:00 INFO]: Training loss at epoch 43: 0.8842540085315704
[08/27/2025 15:30:02 INFO]: Training loss at epoch 17: 0.9693819284439087
[08/27/2025 15:30:07 INFO]: Training loss at epoch 13: 0.9994816482067108
[08/27/2025 15:30:13 INFO]: Training loss at epoch 31: 1.0177724659442902
[08/27/2025 15:30:13 INFO]: Training loss at epoch 31: 0.8273190855979919
[08/27/2025 15:30:22 INFO]: Training loss at epoch 24: 0.9632896780967712
[08/27/2025 15:30:24 INFO]: Running Final Evaluation...
[08/27/2025 15:30:25 INFO]: Training loss at epoch 40: 0.9911060631275177
[08/27/2025 15:30:26 INFO]: Training loss at epoch 14: 0.9175786972045898
[08/27/2025 15:30:30 INFO]: Running Final Evaluation...
[08/27/2025 15:30:41 INFO]: Training loss at epoch 30: 1.0458135604858398
[08/27/2025 15:30:41 INFO]: Training loss at epoch 15: 0.9648065865039825
[08/27/2025 15:30:44 INFO]: Training accuracy: {
    "score": -1.0159939908529794,
    "rmse": 1.0159939908529794
}
[08/27/2025 15:30:44 INFO]: Val accuracy: {
    "score": -0.6615288239207099,
    "rmse": 0.6615288239207099
}
[08/27/2025 15:30:44 INFO]: Test accuracy: {
    "score": -0.8780702601639239,
    "rmse": 0.8780702601639239
}
[08/27/2025 15:30:44 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_24",
    "best_epoch": 9,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8780702601639239,
        "rmse": 0.8780702601639239
    },
    "train_stats": {
        "score": -1.0159939908529794,
        "rmse": 1.0159939908529794
    },
    "val_stats": {
        "score": -0.6615288239207099,
        "rmse": 0.6615288239207099
    }
}
[08/27/2025 15:30:44 INFO]: Procewss finished for trial blotchy-Amado_trial_24
[08/27/2025 15:30:44 INFO]: 
_________________________________________________

[08/27/2025 15:30:44 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:30:44 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 1.9835342062199621
  attention_dropout: 0.4154600274330602
  ffn_dropout: 0.4154600274330602
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.5240694005290192e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_27

[08/27/2025 15:30:44 INFO]: This ft_transformer has 11.912 million parameters.
[08/27/2025 15:30:44 INFO]: Training will start at epoch 0.
[08/27/2025 15:30:44 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:30:44 INFO]: Training loss at epoch 15: 1.09438157081604
[08/27/2025 15:30:49 INFO]: Training loss at epoch 18: 0.969994068145752
[08/27/2025 15:31:03 INFO]: Training accuracy: {
    "score": -1.0030355330074858,
    "rmse": 1.0030355330074858
}
[08/27/2025 15:31:03 INFO]: Val accuracy: {
    "score": -0.6620609687891977,
    "rmse": 0.6620609687891977
}
[08/27/2025 15:31:03 INFO]: Test accuracy: {
    "score": -0.874304161956937,
    "rmse": 0.874304161956937
}
[08/27/2025 15:31:03 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_4",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.874304161956937,
        "rmse": 0.874304161956937
    },
    "train_stats": {
        "score": -1.0030355330074858,
        "rmse": 1.0030355330074858
    },
    "val_stats": {
        "score": -0.6620609687891977,
        "rmse": 0.6620609687891977
    }
}
[08/27/2025 15:31:03 INFO]: Procewss finished for trial blotchy-Amado_trial_4
[08/27/2025 15:31:03 INFO]: 
_________________________________________________

[08/27/2025 15:31:03 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:31:03 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 8
  d_ffn_factor: 0.808192789260355
  attention_dropout: 0.252950544733864
  ffn_dropout: 0.252950544733864
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.9048491016337793e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_28

[08/27/2025 15:31:03 INFO]: This ft_transformer has 3.573 million parameters.
[08/27/2025 15:31:03 INFO]: Training will start at epoch 0.
[08/27/2025 15:31:03 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:31:03 INFO]: Training loss at epoch 16: 1.060074657201767
[08/27/2025 15:31:08 INFO]: Training loss at epoch 8: 0.9716694355010986
[08/27/2025 15:31:10 INFO]: Training loss at epoch 44: 0.9058280885219574
[08/27/2025 15:31:12 INFO]: Training loss at epoch 40: 0.9645565152168274
[08/27/2025 15:31:12 INFO]: Training loss at epoch 36: 1.0619300603866577
[08/27/2025 15:31:14 INFO]: Training loss at epoch 37: 0.9410613179206848
[08/27/2025 15:31:17 INFO]: Training loss at epoch 32: 1.1221871376037598
[08/27/2025 15:31:22 INFO]: Training loss at epoch 32: 1.0824955105781555
[08/27/2025 15:31:23 INFO]: Training loss at epoch 17: 1.07050621509552
[08/27/2025 15:31:29 INFO]: New best epoch, val score: -0.6599218285612326
[08/27/2025 15:31:29 INFO]: Saving model to: blotchy-Amado_trial_12/model_best.pth
[08/27/2025 15:31:38 INFO]: Training loss at epoch 19: 0.9294090569019318
[08/27/2025 15:31:42 INFO]: Training loss at epoch 18: 0.953986793756485
[08/27/2025 15:31:43 INFO]: Training loss at epoch 16: 1.0499687492847443
[08/27/2025 15:31:48 INFO]: Training loss at epoch 25: 0.9799343645572662
[08/27/2025 15:31:50 INFO]: Training loss at epoch 9: 0.981385737657547
[08/27/2025 15:31:57 INFO]: Training stats: {
    "score": -1.003728844723992,
    "rmse": 1.003728844723992
}
[08/27/2025 15:31:57 INFO]: Val stats: {
    "score": -0.6651641690832012,
    "rmse": 0.6651641690832012
}
[08/27/2025 15:31:57 INFO]: Test stats: {
    "score": -0.8675940859904823,
    "rmse": 0.8675940859904823
}
[08/27/2025 15:32:02 INFO]: Training loss at epoch 19: 0.9445003867149353
[08/27/2025 15:32:06 INFO]: Training loss at epoch 16: 0.9464972913265228
[08/27/2025 15:32:09 INFO]: Training stats: {
    "score": -0.9943019921724303,
    "rmse": 0.9943019921724303
}
[08/27/2025 15:32:09 INFO]: Val stats: {
    "score": -0.6876419214664907,
    "rmse": 0.6876419214664907
}
[08/27/2025 15:32:09 INFO]: Test stats: {
    "score": -0.8846927739194586,
    "rmse": 0.8846927739194586
}
[08/27/2025 15:32:25 INFO]: Training loss at epoch 45: 0.8617731332778931
[08/27/2025 15:32:27 INFO]: Training loss at epoch 31: 0.9405289590358734
[08/27/2025 15:32:29 INFO]: Training loss at epoch 20: 1.1177887916564941
[08/27/2025 15:32:30 INFO]: Training loss at epoch 20: 1.1737343072891235
[08/27/2025 15:32:32 INFO]: Training loss at epoch 33: 0.9081799983978271
[08/27/2025 15:32:33 INFO]: Training loss at epoch 41: 1.063433587551117
[08/27/2025 15:32:42 INFO]: Training loss at epoch 38: 0.9234989285469055
[08/27/2025 15:32:43 INFO]: Training loss at epoch 37: 1.0578948855400085
[08/27/2025 15:32:45 INFO]: Training loss at epoch 20: 0.7846238315105438
[08/27/2025 15:32:48 INFO]: Training loss at epoch 21: 0.985520750284195
[08/27/2025 15:32:57 INFO]: Training loss at epoch 33: 1.0732505023479462
[08/27/2025 15:33:06 INFO]: Training loss at epoch 22: 1.2738075256347656
[08/27/2025 15:33:08 INFO]: New best epoch, val score: -0.6677996327790747
[08/27/2025 15:33:08 INFO]: Saving model to: blotchy-Amado_trial_26/model_best.pth
[08/27/2025 15:33:13 INFO]: Training loss at epoch 26: 0.9165695607662201
[08/27/2025 15:33:14 INFO]: Training loss at epoch 0: 0.9595574140548706
[08/27/2025 15:33:25 INFO]: Training loss at epoch 23: 1.1842247247695923
[08/27/2025 15:33:27 INFO]: New best epoch, val score: -0.6642840280052201
[08/27/2025 15:33:27 INFO]: Saving model to: blotchy-Amado_trial_26/model_best.pth
[08/27/2025 15:33:31 INFO]: Training loss at epoch 21: 1.1885559856891632
[08/27/2025 15:33:32 INFO]: New best epoch, val score: -0.6728301279114383
[08/27/2025 15:33:32 INFO]: Saving model to: blotchy-Amado_trial_28/model_best.pth
[08/27/2025 15:33:34 INFO]: Training loss at epoch 46: 1.1673099100589752
[08/27/2025 15:33:39 INFO]: Training loss at epoch 12: 1.0253430008888245
[08/27/2025 15:33:39 INFO]: Training loss at epoch 34: 0.7862093150615692
[08/27/2025 15:33:44 INFO]: Training loss at epoch 24: 1.0293245911598206
[08/27/2025 15:33:46 INFO]: New best epoch, val score: -0.6633118634590397
[08/27/2025 15:33:46 INFO]: Saving model to: blotchy-Amado_trial_26/model_best.pth
[08/27/2025 15:33:48 INFO]: Training loss at epoch 42: 0.8766680955886841
[08/27/2025 15:33:49 INFO]: Training stats: {
    "score": -1.0360509068007457,
    "rmse": 1.0360509068007457
}
[08/27/2025 15:33:49 INFO]: Val stats: {
    "score": -0.7884461581226085,
    "rmse": 0.7884461581226085
}
[08/27/2025 15:33:49 INFO]: Test stats: {
    "score": -0.9404304527121318,
    "rmse": 0.9404304527121318
}
[08/27/2025 15:34:02 INFO]: Training loss at epoch 17: 1.1002795100212097
[08/27/2025 15:34:04 INFO]: Training loss at epoch 25: 1.0042922794818878
[08/27/2025 15:34:06 INFO]: Training loss at epoch 16: 1.1623207032680511
[08/27/2025 15:34:06 INFO]: Training loss at epoch 39: 1.0994289815425873
[08/27/2025 15:34:09 INFO]: Training loss at epoch 32: 0.9920285046100616
[08/27/2025 15:34:09 INFO]: Training loss at epoch 38: 0.9711812734603882
[08/27/2025 15:34:20 INFO]: Training loss at epoch 22: 1.4910303056240082
[08/27/2025 15:34:23 INFO]: Running Final Evaluation...
[08/27/2025 15:34:23 INFO]: Training loss at epoch 26: 0.9895553290843964
[08/27/2025 15:34:35 INFO]: Training loss at epoch 34: 0.8743220269680023
[08/27/2025 15:34:38 INFO]: Training stats: {
    "score": -0.989309617967716,
    "rmse": 0.989309617967716
}
[08/27/2025 15:34:38 INFO]: Val stats: {
    "score": -0.672283109589558,
    "rmse": 0.672283109589558
}
[08/27/2025 15:34:38 INFO]: Test stats: {
    "score": -0.872484910203322,
    "rmse": 0.872484910203322
}
[08/27/2025 15:34:39 INFO]: Training loss at epoch 27: 0.9472398459911346
[08/27/2025 15:34:43 INFO]: Training loss at epoch 27: 1.263031154870987
[08/27/2025 15:34:47 INFO]: Training loss at epoch 47: 0.9060356914997101
[08/27/2025 15:34:50 INFO]: Training loss at epoch 35: 0.9790350496768951
[08/27/2025 15:35:01 INFO]: Training loss at epoch 28: 0.8116019666194916
[08/27/2025 15:35:02 INFO]: Training accuracy: {
    "score": -1.0153377415312455,
    "rmse": 1.0153377415312455
}
[08/27/2025 15:35:02 INFO]: Val accuracy: {
    "score": -0.6692692318409442,
    "rmse": 0.6692692318409442
}
[08/27/2025 15:35:02 INFO]: Test accuracy: {
    "score": -0.875653228554618,
    "rmse": 0.875653228554618
}
[08/27/2025 15:35:02 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_11",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.875653228554618,
        "rmse": 0.875653228554618
    },
    "train_stats": {
        "score": -1.0153377415312455,
        "rmse": 1.0153377415312455
    },
    "val_stats": {
        "score": -0.6692692318409442,
        "rmse": 0.6692692318409442
    }
}
[08/27/2025 15:35:02 INFO]: Procewss finished for trial blotchy-Amado_trial_11
[08/27/2025 15:35:02 INFO]: 
_________________________________________________

[08/27/2025 15:35:02 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:35:02 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 2.3494384100277284
  attention_dropout: 0.16581266545496903
  ffn_dropout: 0.16581266545496903
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0001357805158920419
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_29

[08/27/2025 15:35:02 INFO]: This ft_transformer has 0.239 million parameters.
[08/27/2025 15:35:02 INFO]: Training will start at epoch 0.
[08/27/2025 15:35:02 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:35:09 INFO]: Training loss at epoch 23: 0.888515830039978
[08/27/2025 15:35:09 INFO]: Training loss at epoch 21: 0.9421963095664978
[08/27/2025 15:35:10 INFO]: Training loss at epoch 43: 1.1013261675834656
[08/27/2025 15:35:22 INFO]: Training loss at epoch 29: 0.8627365529537201
[08/27/2025 15:35:24 INFO]: Training loss at epoch 17: 1.022177278995514
[08/27/2025 15:35:28 INFO]: Training stats: {
    "score": -0.9915205622822193,
    "rmse": 0.9915205622822193
}
[08/27/2025 15:35:28 INFO]: Val stats: {
    "score": -0.6878383212184346,
    "rmse": 0.6878383212184346
}
[08/27/2025 15:35:28 INFO]: Test stats: {
    "score": -0.8858623153560019,
    "rmse": 0.8858623153560019
}
[08/27/2025 15:35:39 INFO]: Training loss at epoch 39: 1.0496569871902466
[08/27/2025 15:35:48 INFO]: Training loss at epoch 1: 1.1215431988239288
[08/27/2025 15:35:48 INFO]: Training loss at epoch 30: 0.985406368970871
[08/27/2025 15:35:58 INFO]: Training loss at epoch 24: 1.0382722616195679
[08/27/2025 15:36:00 INFO]: Training loss at epoch 36: 1.2106789648532867
[08/27/2025 15:36:02 INFO]: Training loss at epoch 48: 1.0841758847236633
[08/27/2025 15:36:06 INFO]: Training loss at epoch 28: 1.3235006630420685
[08/27/2025 15:36:07 INFO]: Training loss at epoch 40: 0.8734681606292725
[08/27/2025 15:36:08 INFO]: Training loss at epoch 31: 0.9438241422176361
[08/27/2025 15:36:13 INFO]: Training stats: {
    "score": -1.0022221803940075,
    "rmse": 1.0022221803940075
}
[08/27/2025 15:36:13 INFO]: Val stats: {
    "score": -0.6999358743310683,
    "rmse": 0.6999358743310683
}
[08/27/2025 15:36:13 INFO]: Test stats: {
    "score": -0.8843853488489924,
    "rmse": 0.8843853488489924
}
[08/27/2025 15:36:16 INFO]: Training loss at epoch 35: 0.8567676246166229
[08/27/2025 15:36:22 INFO]: Training loss at epoch 0: 0.9409598112106323
[08/27/2025 15:36:25 INFO]: Training loss at epoch 18: 1.1360536217689514
[08/27/2025 15:36:27 INFO]: Training loss at epoch 32: 0.9451020956039429
[08/27/2025 15:36:32 INFO]: Training loss at epoch 44: 1.0787093043327332
[08/27/2025 15:36:33 INFO]: New best epoch, val score: -0.7542552869292932
[08/27/2025 15:36:33 INFO]: Saving model to: blotchy-Amado_trial_29/model_best.pth
[08/27/2025 15:36:46 INFO]: Training loss at epoch 33: 1.0781272947788239
[08/27/2025 15:36:46 INFO]: Training loss at epoch 25: 0.9492929577827454
[08/27/2025 15:37:03 INFO]: Training loss at epoch 0: 1.021134078502655
[08/27/2025 15:37:05 INFO]: Training loss at epoch 34: 1.061772346496582
[08/27/2025 15:37:09 INFO]: Training loss at epoch 37: 0.9493686556816101
[08/27/2025 15:37:14 INFO]: Training loss at epoch 49: 0.9943346381187439
[08/27/2025 15:37:25 INFO]: Training loss at epoch 35: 1.1892609596252441
[08/27/2025 15:37:30 INFO]: Training loss at epoch 9: 0.7383326590061188
[08/27/2025 15:37:31 INFO]: Training loss at epoch 29: 0.9306643605232239
[08/27/2025 15:37:32 INFO]: Training loss at epoch 41: 1.0555331408977509
[08/27/2025 15:37:33 INFO]: Training loss at epoch 26: 0.9482595920562744
[08/27/2025 15:37:37 INFO]: Training loss at epoch 17: 1.0075090527534485
[08/27/2025 15:37:40 INFO]: Training stats: {
    "score": -0.9993642819186688,
    "rmse": 0.9993642819186688
}
[08/27/2025 15:37:40 INFO]: Val stats: {
    "score": -0.6894978446561079,
    "rmse": 0.6894978446561079
}
[08/27/2025 15:37:40 INFO]: Test stats: {
    "score": -0.879089517369604,
    "rmse": 0.879089517369604
}
[08/27/2025 15:37:41 INFO]: Training loss at epoch 40: 0.9688107371330261
[08/27/2025 15:37:44 INFO]: Training loss at epoch 36: 1.000667691230774
[08/27/2025 15:37:50 INFO]: Training loss at epoch 45: 1.0043139457702637
[08/27/2025 15:37:50 INFO]: Training loss at epoch 22: 0.980918288230896
[08/27/2025 15:37:50 INFO]: Training loss at epoch 1: 1.0609694123268127
[08/27/2025 15:37:52 INFO]: New best epoch, val score: -0.6778943754302005
[08/27/2025 15:37:52 INFO]: Saving model to: blotchy-Amado_trial_27/model_best.pth
[08/27/2025 15:37:55 INFO]: Training loss at epoch 36: 0.8901360630989075
[08/27/2025 15:38:04 INFO]: Training loss at epoch 37: 0.9738543331623077
[08/27/2025 15:38:04 INFO]: Training stats: {
    "score": -1.000356050577017,
    "rmse": 1.000356050577017
}
[08/27/2025 15:38:04 INFO]: Val stats: {
    "score": -0.6627802239663105,
    "rmse": 0.6627802239663105
}
[08/27/2025 15:38:04 INFO]: Test stats: {
    "score": -0.8690191089766404,
    "rmse": 0.8690191089766404
}
[08/27/2025 15:38:05 INFO]: New best epoch, val score: -0.6842178006980902
[08/27/2025 15:38:05 INFO]: Saving model to: blotchy-Amado_trial_29/model_best.pth
[08/27/2025 15:38:08 INFO]: Training loss at epoch 13: 1.103757381439209
[08/27/2025 15:38:16 INFO]: Training loss at epoch 2: 0.9137644171714783
[08/27/2025 15:38:17 INFO]: Training loss at epoch 38: 1.0271674394607544
[08/27/2025 15:38:21 INFO]: Training loss at epoch 27: 1.310253083705902
[08/27/2025 15:38:23 INFO]: Training loss at epoch 38: 1.0332565009593964
[08/27/2025 15:38:39 INFO]: New best epoch, val score: -0.6680817152737889
[08/27/2025 15:38:39 INFO]: Saving model to: blotchy-Amado_trial_19/model_best.pth
[08/27/2025 15:38:41 INFO]: Training loss at epoch 39: 1.0425347685813904
[08/27/2025 15:38:41 INFO]: Training loss at epoch 18: 1.156432181596756
[08/27/2025 15:38:43 INFO]: Training loss at epoch 19: 0.8880020976066589
[08/27/2025 15:38:48 INFO]: Training stats: {
    "score": -0.9868149371486871,
    "rmse": 0.9868149371486871
}
[08/27/2025 15:38:48 INFO]: Val stats: {
    "score": -0.6832253774702673,
    "rmse": 0.6832253774702673
}
[08/27/2025 15:38:48 INFO]: Test stats: {
    "score": -0.8858155170482209,
    "rmse": 0.8858155170482209
}
[08/27/2025 15:38:51 INFO]: Training loss at epoch 50: 1.1013948917388916
[08/27/2025 15:38:55 INFO]: Training loss at epoch 42: 0.971156895160675
[08/27/2025 15:39:06 INFO]: Training loss at epoch 40: 0.968724399805069
[08/27/2025 15:39:06 INFO]: Training loss at epoch 28: 0.997601330280304
[08/27/2025 15:39:07 INFO]: Training loss at epoch 46: 1.0505470037460327
[08/27/2025 15:39:09 INFO]: Training loss at epoch 41: 0.9712354242801666
[08/27/2025 15:39:13 INFO]: Training loss at epoch 4: 1.2779022455215454
[08/27/2025 15:39:20 INFO]: Training loss at epoch 2: 1.0123869180679321
[08/27/2025 15:39:23 INFO]: Training loss at epoch 39: 0.9609069228172302
[08/27/2025 15:39:25 INFO]: Training loss at epoch 41: 1.1342479586601257
[08/27/2025 15:39:27 INFO]: Training loss at epoch 30: 0.9164775907993317
[08/27/2025 15:39:27 INFO]: Training stats: {
    "score": -1.0029925940825843,
    "rmse": 1.0029925940825843
}
[08/27/2025 15:39:27 INFO]: Val stats: {
    "score": -0.711251947514647,
    "rmse": 0.711251947514647
}
[08/27/2025 15:39:27 INFO]: Test stats: {
    "score": -0.8912577060859526,
    "rmse": 0.8912577060859526
}
[08/27/2025 15:39:28 INFO]: Training loss at epoch 37: 0.8781474530696869
[08/27/2025 15:39:29 INFO]: Training loss at epoch 10: 0.9316707849502563
[08/27/2025 15:39:32 INFO]: New best epoch, val score: -0.6715277679903546
[08/27/2025 15:39:32 INFO]: Saving model to: blotchy-Amado_trial_29/model_best.pth
[08/27/2025 15:39:37 INFO]: Training stats: {
    "score": -1.0004440767063096,
    "rmse": 1.0004440767063096
}
[08/27/2025 15:39:37 INFO]: Val stats: {
    "score": -0.740361045083773,
    "rmse": 0.740361045083773
}
[08/27/2025 15:39:37 INFO]: Test stats: {
    "score": -0.9152313908887278,
    "rmse": 0.9152313908887278
}
[08/27/2025 15:39:43 INFO]: Training loss at epoch 42: 0.8713313639163971
[08/27/2025 15:39:47 INFO]: Training stats: {
    "score": -0.9978547983171201,
    "rmse": 0.9978547983171201
}
[08/27/2025 15:39:47 INFO]: Val stats: {
    "score": -0.6911104509020646,
    "rmse": 0.6911104509020646
}
[08/27/2025 15:39:47 INFO]: Test stats: {
    "score": -0.8832357321051552,
    "rmse": 0.8832357321051552
}
[08/27/2025 15:39:53 INFO]: Training loss at epoch 29: 0.9206622540950775
[08/27/2025 15:40:02 INFO]: Training loss at epoch 51: 0.9365369975566864
[08/27/2025 15:40:03 INFO]: Training loss at epoch 43: 0.9786851108074188
[08/27/2025 15:40:11 INFO]: Running Final Evaluation...
[08/27/2025 15:40:12 INFO]: Training stats: {
    "score": -0.9980996441493938,
    "rmse": 0.9980996441493938
}
[08/27/2025 15:40:12 INFO]: Val stats: {
    "score": -0.6780145157051299,
    "rmse": 0.6780145157051299
}
[08/27/2025 15:40:12 INFO]: Test stats: {
    "score": -0.8697770946843286,
    "rmse": 0.8697770946843286
}
[08/27/2025 15:40:20 INFO]: Training loss at epoch 43: 1.0695361495018005
[08/27/2025 15:40:22 INFO]: Training loss at epoch 44: 1.0263897776603699
[08/27/2025 15:40:25 INFO]: Training loss at epoch 47: 0.9024251103401184
[08/27/2025 15:40:27 INFO]: Training loss at epoch 23: 0.8207353949546814
[08/27/2025 15:40:36 INFO]: Training accuracy: {
    "score": -1.0162811824312175,
    "rmse": 1.0162811824312175
}
[08/27/2025 15:40:36 INFO]: Val accuracy: {
    "score": -0.6611211855905492,
    "rmse": 0.6611211855905492
}
[08/27/2025 15:40:36 INFO]: Test accuracy: {
    "score": -0.8733626522986884,
    "rmse": 0.8733626522986884
}
[08/27/2025 15:40:36 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_17",
    "best_epoch": 20,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8733626522986884,
        "rmse": 0.8733626522986884
    },
    "train_stats": {
        "score": -1.0162811824312175,
        "rmse": 1.0162811824312175
    },
    "val_stats": {
        "score": -0.6611211855905492,
        "rmse": 0.6611211855905492
    }
}
[08/27/2025 15:40:36 INFO]: Procewss finished for trial blotchy-Amado_trial_17
[08/27/2025 15:40:36 INFO]: 
_________________________________________________

[08/27/2025 15:40:36 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:40:36 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 2.606780061753774
  attention_dropout: 0.3031472198804187
  ffn_dropout: 0.3031472198804187
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0004394407468219375
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_30

[08/27/2025 15:40:36 INFO]: This ft_transformer has 17.612 million parameters.
[08/27/2025 15:40:36 INFO]: Training will start at epoch 0.
[08/27/2025 15:40:36 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:40:38 INFO]: Training loss at epoch 42: 0.9991857409477234
[08/27/2025 15:40:42 INFO]: Training loss at epoch 45: 0.855505108833313
[08/27/2025 15:40:42 INFO]: Training loss at epoch 3: 0.9050852954387665
[08/27/2025 15:40:50 INFO]: Training loss at epoch 3: 1.0417670011520386
[08/27/2025 15:40:53 INFO]: Training loss at epoch 31: 0.9307104051113129
[08/27/2025 15:40:57 INFO]: Training loss at epoch 40: 1.1497234106063843
[08/27/2025 15:40:59 INFO]: Training loss at epoch 30: 1.019317090511322
[08/27/2025 15:41:00 INFO]: Training loss at epoch 18: 1.0829781293869019
[08/27/2025 15:41:01 INFO]: Training loss at epoch 46: 1.0174779891967773
[08/27/2025 15:41:07 INFO]: Training loss at epoch 38: 1.1563559174537659
[08/27/2025 15:41:19 INFO]: Training loss at epoch 47: 0.8578905463218689
[08/27/2025 15:41:24 INFO]: New best epoch, val score: -0.6582997986031841
[08/27/2025 15:41:24 INFO]: Saving model to: blotchy-Amado_trial_3/model_best.pth
[08/27/2025 15:41:37 INFO]: Training loss at epoch 48: 0.7937145233154297
[08/27/2025 15:41:44 INFO]: Training loss at epoch 48: 1.0639637112617493
[08/27/2025 15:41:47 INFO]: Training loss at epoch 31: 0.9784735441207886
[08/27/2025 15:41:47 INFO]: Training loss at epoch 44: 0.8445518314838409
[08/27/2025 15:41:47 INFO]: Training loss at epoch 20: 0.9716571569442749
[08/27/2025 15:41:54 INFO]: Running Final Evaluation...
[08/27/2025 15:41:57 INFO]: Training loss at epoch 19: 0.8643486499786377
[08/27/2025 15:41:58 INFO]: Training loss at epoch 49: 0.9057714939117432
[08/27/2025 15:42:05 INFO]: Training loss at epoch 41: 0.9022197425365448
[08/27/2025 15:42:05 INFO]: Training stats: {
    "score": -0.9949906986013164,
    "rmse": 0.9949906986013164
}
[08/27/2025 15:42:05 INFO]: Val stats: {
    "score": -0.6683380100162025,
    "rmse": 0.6683380100162025
}
[08/27/2025 15:42:05 INFO]: Test stats: {
    "score": -0.8874173339170675,
    "rmse": 0.8874173339170675
}
[08/27/2025 15:42:08 INFO]: Training loss at epoch 43: 1.0039607286453247
[08/27/2025 15:42:18 INFO]: Training loss at epoch 32: 1.12483549118042
[08/27/2025 15:42:20 INFO]: Running Final Evaluation...
[08/27/2025 15:42:20 INFO]: Training loss at epoch 4: 1.0392466187477112
[08/27/2025 15:42:23 INFO]: Training accuracy: {
    "score": -1.0135570722759364,
    "rmse": 1.0135570722759364
}
[08/27/2025 15:42:23 INFO]: Val accuracy: {
    "score": -0.659797179421411,
    "rmse": 0.659797179421411
}
[08/27/2025 15:42:23 INFO]: Test accuracy: {
    "score": -0.8755730240412612,
    "rmse": 0.8755730240412612
}
[08/27/2025 15:42:23 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_10",
    "best_epoch": 17,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8755730240412612,
        "rmse": 0.8755730240412612
    },
    "train_stats": {
        "score": -1.0135570722759364,
        "rmse": 1.0135570722759364
    },
    "val_stats": {
        "score": -0.659797179421411,
        "rmse": 0.659797179421411
    }
}
[08/27/2025 15:42:23 INFO]: Procewss finished for trial blotchy-Amado_trial_10
[08/27/2025 15:42:23 INFO]: 
_________________________________________________

[08/27/2025 15:42:23 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:42:23 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 1.4261984710194566
  attention_dropout: 0.2928145256651429
  ffn_dropout: 0.2928145256651429
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00029641204974943295
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_31

[08/27/2025 15:42:23 INFO]: This ft_transformer has 21.929 million parameters.
[08/27/2025 15:42:23 INFO]: Training will start at epoch 0.
[08/27/2025 15:42:23 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:42:25 INFO]: Training loss at epoch 50: 0.95378577709198
[08/27/2025 15:42:31 INFO]: Training loss at epoch 14: 1.0470417737960815
[08/27/2025 15:42:35 INFO]: Training loss at epoch 32: 1.0572270154953003
[08/27/2025 15:42:43 INFO]: Training loss at epoch 51: 1.0360353291034698
[08/27/2025 15:42:46 INFO]: Training loss at epoch 39: 1.1213049292564392
[08/27/2025 15:42:51 INFO]: Training accuracy: {
    "score": -1.014874309102224,
    "rmse": 1.014874309102224
}
[08/27/2025 15:42:51 INFO]: Val accuracy: {
    "score": -0.6601982475983518,
    "rmse": 0.6601982475983518
}
[08/27/2025 15:42:51 INFO]: Test accuracy: {
    "score": -0.871982840667063,
    "rmse": 0.871982840667063
}
[08/27/2025 15:42:52 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_5",
    "best_epoch": 12,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.871982840667063,
        "rmse": 0.871982840667063
    },
    "train_stats": {
        "score": -1.014874309102224,
        "rmse": 1.014874309102224
    },
    "val_stats": {
        "score": -0.6601982475983518,
        "rmse": 0.6601982475983518
    }
}
[08/27/2025 15:42:52 INFO]: Procewss finished for trial blotchy-Amado_trial_5
[08/27/2025 15:42:52 INFO]: 
_________________________________________________

[08/27/2025 15:42:52 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:42:52 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 1.256652948232864
  attention_dropout: 0.3276816476809786
  ffn_dropout: 0.3276816476809786
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00023247487345284413
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_32

[08/27/2025 15:42:52 INFO]: This ft_transformer has 8.338 million parameters.
[08/27/2025 15:42:52 INFO]: Training will start at epoch 0.
[08/27/2025 15:42:52 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:43:01 INFO]: Training stats: {
    "score": -0.9939402070390475,
    "rmse": 0.9939402070390475
}
[08/27/2025 15:43:01 INFO]: Val stats: {
    "score": -0.669403749122194,
    "rmse": 0.669403749122194
}
[08/27/2025 15:43:01 INFO]: Test stats: {
    "score": -0.8863451230231149,
    "rmse": 0.8863451230231149
}
[08/27/2025 15:43:02 INFO]: Training loss at epoch 52: 0.9991792738437653
[08/27/2025 15:43:07 INFO]: Training loss at epoch 24: 1.1616100072860718
[08/27/2025 15:43:09 INFO]: Training loss at epoch 4: 1.0020064115524292
[08/27/2025 15:43:12 INFO]: Training loss at epoch 45: 0.9899902939796448
[08/27/2025 15:43:12 INFO]: Training loss at epoch 42: 1.1932904124259949
[08/27/2025 15:43:18 INFO]: Training stats: {
    "score": -0.9910778594816445,
    "rmse": 0.9910778594816445
}
[08/27/2025 15:43:18 INFO]: Val stats: {
    "score": -0.6674985172173048,
    "rmse": 0.6674985172173048
}
[08/27/2025 15:43:18 INFO]: Test stats: {
    "score": -0.8725298406423804,
    "rmse": 0.8725298406423804
}
[08/27/2025 15:43:20 INFO]: Training loss at epoch 53: 1.0405638813972473
[08/27/2025 15:43:21 INFO]: Training loss at epoch 33: 0.9572494328022003
[08/27/2025 15:43:40 INFO]: Training loss at epoch 54: 0.8635731637477875
[08/27/2025 15:43:41 INFO]: Training loss at epoch 33: 1.0318613052368164
[08/27/2025 15:43:47 INFO]: Training loss at epoch 5: 1.018538773059845
[08/27/2025 15:43:58 INFO]: Training loss at epoch 55: 0.9049855172634125
[08/27/2025 15:44:01 INFO]: Running Final Evaluation...
[08/27/2025 15:44:03 INFO]: Training loss at epoch 1: 0.9768570959568024
[08/27/2025 15:44:06 INFO]: Training loss at epoch 21: 0.9760072231292725
[08/27/2025 15:44:07 INFO]: Training accuracy: {
    "score": -1.0009118972395463,
    "rmse": 1.0009118972395463
}
[08/27/2025 15:44:07 INFO]: Val accuracy: {
    "score": -0.6633118634590397,
    "rmse": 0.6633118634590397
}
[08/27/2025 15:44:07 INFO]: Test accuracy: {
    "score": -0.8788923812365196,
    "rmse": 0.8788923812365196
}
[08/27/2025 15:44:07 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_26",
    "best_epoch": 24,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8788923812365196,
        "rmse": 0.8788923812365196
    },
    "train_stats": {
        "score": -1.0009118972395463,
        "rmse": 1.0009118972395463
    },
    "val_stats": {
        "score": -0.6633118634590397,
        "rmse": 0.6633118634590397
    }
}
[08/27/2025 15:44:07 INFO]: Procewss finished for trial blotchy-Amado_trial_26
[08/27/2025 15:44:07 INFO]: 
_________________________________________________

[08/27/2025 15:44:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:44:07 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 2.6229774765052234
  attention_dropout: 0.30730350293302056
  ffn_dropout: 0.30730350293302056
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00021289444387607168
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_33

[08/27/2025 15:44:08 INFO]: This ft_transformer has 12.639 million parameters.
[08/27/2025 15:44:08 INFO]: Training will start at epoch 0.
[08/27/2025 15:44:08 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:44:09 INFO]: Training loss at epoch 34: 1.1680369079113007
[08/27/2025 15:44:20 INFO]: Training loss at epoch 43: 1.022921234369278
[08/27/2025 15:44:25 INFO]: Training loss at epoch 19: 0.9770854115486145
[08/27/2025 15:44:29 INFO]: Running Final Evaluation...
[08/27/2025 15:44:38 INFO]: Training loss at epoch 46: 0.8782038986682892
[08/27/2025 15:44:56 INFO]: Training accuracy: {
    "score": -1.0144331493396619,
    "rmse": 1.0144331493396619
}
[08/27/2025 15:44:56 INFO]: Val accuracy: {
    "score": -0.6589034706412974,
    "rmse": 0.6589034706412974
}
[08/27/2025 15:44:56 INFO]: Test accuracy: {
    "score": -0.8755349017443171,
    "rmse": 0.8755349017443171
}
[08/27/2025 15:44:56 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_22",
    "best_epoch": 12,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8755349017443171,
        "rmse": 0.8755349017443171
    },
    "train_stats": {
        "score": -1.0144331493396619,
        "rmse": 1.0144331493396619
    },
    "val_stats": {
        "score": -0.6589034706412974,
        "rmse": 0.6589034706412974
    }
}
[08/27/2025 15:44:56 INFO]: Procewss finished for trial blotchy-Amado_trial_22
[08/27/2025 15:44:56 INFO]: 
_________________________________________________

[08/27/2025 15:44:56 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:44:56 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 16
  d_ffn_factor: 1.1605919085537946
  attention_dropout: 0.293227018857881
  ffn_dropout: 0.293227018857881
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00020689042169311111
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_34

[08/27/2025 15:44:56 INFO]: This ft_transformer has 7.611 million parameters.
[08/27/2025 15:44:56 INFO]: Training will start at epoch 0.
[08/27/2025 15:44:56 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:44:58 INFO]: Training loss at epoch 35: 1.086946040391922
[08/27/2025 15:44:59 INFO]: Training loss at epoch 40: 1.0124565660953522
[08/27/2025 15:45:07 INFO]: Training loss at epoch 34: 1.0430907905101776
[08/27/2025 15:45:10 INFO]: Training loss at epoch 11: 1.1690146923065186
[08/27/2025 15:45:17 INFO]: Training loss at epoch 6: 1.0883504748344421
[08/27/2025 15:45:37 INFO]: Training stats: {
    "score": -1.001553194084942,
    "rmse": 1.001553194084942
}
[08/27/2025 15:45:37 INFO]: Val stats: {
    "score": -0.6616430090164774,
    "rmse": 0.6616430090164774
}
[08/27/2025 15:45:37 INFO]: Test stats: {
    "score": -0.8740515927206208,
    "rmse": 0.8740515927206208
}
[08/27/2025 15:45:41 INFO]: Training loss at epoch 5: 1.3490336537361145
[08/27/2025 15:45:48 INFO]: Training loss at epoch 36: 1.0574149191379547
[08/27/2025 15:45:48 INFO]: Training loss at epoch 25: 1.0698906183242798
[08/27/2025 15:45:58 INFO]: Training loss at epoch 10: 0.8711234927177429
[08/27/2025 15:46:08 INFO]: Training loss at epoch 47: 0.9769639074802399
[08/27/2025 15:46:12 INFO]: Training loss at epoch 0: 1.2318546772003174
[08/27/2025 15:46:23 INFO]: Training loss at epoch 20: 0.988371729850769
[08/27/2025 15:46:29 INFO]: Training loss at epoch 22: 1.0199708938598633
[08/27/2025 15:46:35 INFO]: Training loss at epoch 35: 0.772194892168045
[08/27/2025 15:46:36 INFO]: Training loss at epoch 37: 0.8790347576141357
[08/27/2025 15:46:40 INFO]: Training loss at epoch 41: 0.9103200435638428
[08/27/2025 15:46:40 INFO]: New best epoch, val score: -0.6596721879223724
[08/27/2025 15:46:40 INFO]: Saving model to: blotchy-Amado_trial_32/model_best.pth
[08/27/2025 15:46:47 INFO]: Training loss at epoch 7: 0.8677073419094086
[08/27/2025 15:46:56 INFO]: Training loss at epoch 15: 0.9090999960899353
[08/27/2025 15:47:24 INFO]: Training loss at epoch 38: 0.9784679412841797
[08/27/2025 15:47:35 INFO]: Training loss at epoch 48: 0.9707333743572235
[08/27/2025 15:47:59 INFO]: Training loss at epoch 36: 1.2064819931983948
[08/27/2025 15:48:09 INFO]: Training loss at epoch 6: 1.0056737065315247
[08/27/2025 15:48:09 INFO]: Training loss at epoch 39: 0.7928561270236969
[08/27/2025 15:48:15 INFO]: Training loss at epoch 8: 1.0123902559280396
[08/27/2025 15:48:16 INFO]: Training loss at epoch 42: 1.1172502040863037
[08/27/2025 15:48:24 INFO]: Training loss at epoch 26: 0.9381177127361298
[08/27/2025 15:48:28 INFO]: Training stats: {
    "score": -1.0004962622962246,
    "rmse": 1.0004962622962246
}
[08/27/2025 15:48:28 INFO]: Val stats: {
    "score": -0.6670474733939604,
    "rmse": 0.6670474733939604
}
[08/27/2025 15:48:28 INFO]: Test stats: {
    "score": -0.8666062107961148,
    "rmse": 0.8666062107961148
}
[08/27/2025 15:48:43 INFO]: Training loss at epoch 0: 0.8783507645130157
[08/27/2025 15:48:43 INFO]: Training loss at epoch 23: 0.9720415472984314
[08/27/2025 15:48:51 INFO]: Training loss at epoch 0: 0.9312957227230072
[08/27/2025 15:48:56 INFO]: Training loss at epoch 49: 0.8823222815990448
[08/27/2025 15:48:59 INFO]: Training loss at epoch 20: 1.0021893382072449
[08/27/2025 15:49:14 INFO]: Training loss at epoch 40: 1.0390572547912598
[08/27/2025 15:49:17 INFO]: New best epoch, val score: -0.7264860640744538
[08/27/2025 15:49:17 INFO]: Saving model to: blotchy-Amado_trial_33/model_best.pth
[08/27/2025 15:49:20 INFO]: Training loss at epoch 37: 1.0037236213684082
[08/27/2025 15:49:24 INFO]: Training stats: {
    "score": -0.987275404498517,
    "rmse": 0.987275404498517
}
[08/27/2025 15:49:24 INFO]: Val stats: {
    "score": -0.6799294761074453,
    "rmse": 0.6799294761074453
}
[08/27/2025 15:49:24 INFO]: Test stats: {
    "score": -0.8768892066293397,
    "rmse": 0.8768892066293397
}
[08/27/2025 15:49:34 INFO]: Training loss at epoch 21: 1.0199062824249268
[08/27/2025 15:49:40 INFO]: Training loss at epoch 9: 1.1845747530460358
[08/27/2025 15:49:47 INFO]: Training loss at epoch 0: 0.8129781484603882
[08/27/2025 15:49:49 INFO]: Training loss at epoch 43: 1.1284874081611633
[08/27/2025 15:49:49 INFO]: Training loss at epoch 1: 1.0438658595085144
[08/27/2025 15:49:53 INFO]: New best epoch, val score: -0.7003179714590949
[08/27/2025 15:49:53 INFO]: Saving model to: blotchy-Amado_trial_30/model_best.pth
[08/27/2025 15:49:59 INFO]: Training loss at epoch 41: 1.0859327614307404
[08/27/2025 15:50:11 INFO]: Training stats: {
    "score": -1.0029635988744199,
    "rmse": 1.0029635988744199
}
[08/27/2025 15:50:11 INFO]: Val stats: {
    "score": -0.7066946522932666,
    "rmse": 0.7066946522932666
}
[08/27/2025 15:50:11 INFO]: Test stats: {
    "score": -0.8922639392371786,
    "rmse": 0.8922639392371786
}
[08/27/2025 15:50:27 INFO]: New best epoch, val score: -0.7211007148477031
[08/27/2025 15:50:27 INFO]: Saving model to: blotchy-Amado_trial_34/model_best.pth
[08/27/2025 15:50:32 INFO]: Training loss at epoch 7: 0.7709251791238785
[08/27/2025 15:50:43 INFO]: Training loss at epoch 38: 1.1798988580703735
[08/27/2025 15:50:45 INFO]: Training loss at epoch 12: 1.1288570165634155
[08/27/2025 15:50:46 INFO]: Training loss at epoch 42: 1.1029410064220428
[08/27/2025 15:50:47 INFO]: Training loss at epoch 50: 0.8944565057754517
[08/27/2025 15:50:57 INFO]: Training loss at epoch 27: 0.9797132611274719
[08/27/2025 15:50:57 INFO]: Training loss at epoch 24: 1.0050622820854187
[08/27/2025 15:51:01 INFO]: Training loss at epoch 2: 1.2440422773361206
[08/27/2025 15:51:09 INFO]: Training loss at epoch 16: 1.0934305787086487
[08/27/2025 15:51:25 INFO]: Training loss at epoch 44: 0.8875004947185516
[08/27/2025 15:51:34 INFO]: Training loss at epoch 43: 1.00876846909523
[08/27/2025 15:51:39 INFO]: Training loss at epoch 10: 1.0664509534835815
[08/27/2025 15:52:05 INFO]: Training loss at epoch 11: 1.04363214969635
[08/27/2025 15:52:08 INFO]: Training loss at epoch 0: 1.1396531462669373
[08/27/2025 15:52:10 INFO]: Training loss at epoch 39: 1.0402241945266724
[08/27/2025 15:52:14 INFO]: Training loss at epoch 5: 1.0139212012290955
[08/27/2025 15:52:15 INFO]: Training loss at epoch 51: 0.9066013693809509
[08/27/2025 15:52:20 INFO]: Training loss at epoch 21: 0.9550973773002625
[08/27/2025 15:52:23 INFO]: Training loss at epoch 44: 0.9666091203689575
[08/27/2025 15:52:41 INFO]: Training stats: {
    "score": -1.0005204958426142,
    "rmse": 1.0005204958426142
}
[08/27/2025 15:52:41 INFO]: Val stats: {
    "score": -0.6621471518035645,
    "rmse": 0.6621471518035645
}
[08/27/2025 15:52:41 INFO]: Test stats: {
    "score": -0.8688367552223638,
    "rmse": 0.8688367552223638
}
[08/27/2025 15:52:49 INFO]: Training loss at epoch 22: 0.9377705156803131
[08/27/2025 15:53:02 INFO]: Training loss at epoch 8: 1.045188307762146
[08/27/2025 15:53:05 INFO]: Training loss at epoch 45: 0.8482425808906555
[08/27/2025 15:53:08 INFO]: Training loss at epoch 11: 1.198333978652954
[08/27/2025 15:53:09 INFO]: Training loss at epoch 45: 1.106406569480896
[08/27/2025 15:53:16 INFO]: Training loss at epoch 25: 0.9307999312877655
[08/27/2025 15:53:25 INFO]: New best epoch, val score: -0.6948658160377279
[08/27/2025 15:53:25 INFO]: Saving model to: blotchy-Amado_trial_31/model_best.pth
[08/27/2025 15:53:31 INFO]: Training loss at epoch 2: 0.8895983099937439
[08/27/2025 15:53:34 INFO]: Training loss at epoch 28: 1.0070651769638062
[08/27/2025 15:53:40 INFO]: Training loss at epoch 52: 0.7763009369373322
[08/27/2025 15:53:49 INFO]: Training loss at epoch 1: 1.1942212581634521
[08/27/2025 15:53:58 INFO]: Training loss at epoch 46: 0.8583874404430389
[08/27/2025 15:54:04 INFO]: Running Final Evaluation...
[08/27/2025 15:54:08 INFO]: Training loss at epoch 40: 0.9427041709423065
[08/27/2025 15:54:23 INFO]: Training accuracy: {
    "score": -1.0094118816893853,
    "rmse": 1.0094118816893853
}
[08/27/2025 15:54:23 INFO]: Val accuracy: {
    "score": -0.6626691563315809,
    "rmse": 0.6626691563315809
}
[08/27/2025 15:54:23 INFO]: Test accuracy: {
    "score": -0.8689260757750119,
    "rmse": 0.8689260757750119
}
[08/27/2025 15:54:23 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_25",
    "best_epoch": 15,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8689260757750119,
        "rmse": 0.8689260757750119
    },
    "train_stats": {
        "score": -1.0094118816893853,
        "rmse": 1.0094118816893853
    },
    "val_stats": {
        "score": -0.6626691563315809,
        "rmse": 0.6626691563315809
    }
}
[08/27/2025 15:54:23 INFO]: Procewss finished for trial blotchy-Amado_trial_25
[08/27/2025 15:54:23 INFO]: 
_________________________________________________

[08/27/2025 15:54:23 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:54:23 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 16
  d_ffn_factor: 2.6156125065364177
  attention_dropout: 0.065143910297663
  ffn_dropout: 0.065143910297663
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00023288382032249656
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_35

[08/27/2025 15:54:24 INFO]: This ft_transformer has 13.781 million parameters.
[08/27/2025 15:54:24 INFO]: Training will start at epoch 0.
[08/27/2025 15:54:24 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:54:38 INFO]: Training loss at epoch 12: 0.8992828726768494
[08/27/2025 15:54:46 INFO]: Training loss at epoch 46: 1.0311477184295654
[08/27/2025 15:55:10 INFO]: Training loss at epoch 53: 0.8375133872032166
[08/27/2025 15:55:26 INFO]: Training loss at epoch 1: 2.575558066368103
[08/27/2025 15:55:35 INFO]: Training loss at epoch 9: 1.017543911933899
[08/27/2025 15:55:35 INFO]: Training loss at epoch 41: 0.8742936551570892
[08/27/2025 15:55:36 INFO]: Training loss at epoch 17: 0.9403982758522034
[08/27/2025 15:55:37 INFO]: Training loss at epoch 26: 0.9921758770942688
[08/27/2025 15:55:47 INFO]: Training loss at epoch 22: 1.0861608386039734
[08/27/2025 15:56:08 INFO]: Training loss at epoch 13: 1.137647658586502
[08/27/2025 15:56:12 INFO]: Training loss at epoch 23: 1.159432828426361
[08/27/2025 15:56:14 INFO]: Training loss at epoch 29: 1.1345958709716797
[08/27/2025 15:56:28 INFO]: Training stats: {
    "score": -1.0003195452636964,
    "rmse": 1.0003195452636964
}
[08/27/2025 15:56:28 INFO]: Val stats: {
    "score": -0.6961226402928559,
    "rmse": 0.6961226402928559
}
[08/27/2025 15:56:28 INFO]: Test stats: {
    "score": -0.8816392440358094,
    "rmse": 0.8816392440358094
}
[08/27/2025 15:56:28 INFO]: Training loss at epoch 47: 1.1039744913578033
[08/27/2025 15:56:31 INFO]: Training loss at epoch 13: 0.8581525385379791
[08/27/2025 15:56:39 INFO]: Training loss at epoch 54: 1.0518829226493835
[08/27/2025 15:57:02 INFO]: Training loss at epoch 42: 0.9373322129249573
[08/27/2025 15:57:15 INFO]: Training stats: {
    "score": -0.9985547750368062,
    "rmse": 0.9985547750368062
}
[08/27/2025 15:57:15 INFO]: Val stats: {
    "score": -0.678464610746257,
    "rmse": 0.678464610746257
}
[08/27/2025 15:57:15 INFO]: Test stats: {
    "score": -0.8707489676049494,
    "rmse": 0.8707489676049494
}
[08/27/2025 15:57:20 INFO]: Training loss at epoch 3: 1.2286208271980286
[08/27/2025 15:57:37 INFO]: Training loss at epoch 14: 0.8564318418502808
[08/27/2025 15:57:56 INFO]: Training loss at epoch 27: 0.8039025068283081
[08/27/2025 15:58:03 INFO]: Training loss at epoch 55: 0.8486369252204895
[08/27/2025 15:58:04 INFO]: Training loss at epoch 48: 0.9118772447109222
[08/27/2025 15:58:05 INFO]: Training loss at epoch 3: 0.8253621459007263
[08/27/2025 15:58:11 INFO]: Training loss at epoch 1: 5.109038531780243
[08/27/2025 15:58:21 INFO]: Training loss at epoch 12: 1.1125481724739075
[08/27/2025 15:58:24 INFO]: Training loss at epoch 43: 1.077060580253601
[08/27/2025 15:58:35 INFO]: Running Final Evaluation...
[08/27/2025 15:58:51 INFO]: Training loss at epoch 10: 0.9725541174411774
[08/27/2025 15:58:52 INFO]: New best epoch, val score: -0.6636294269282988
[08/27/2025 15:58:52 INFO]: Saving model to: blotchy-Amado_trial_27/model_best.pth
[08/27/2025 15:59:00 INFO]: Training loss at epoch 2: 1.1791128516197205
[08/27/2025 15:59:02 INFO]: Training loss at epoch 15: 1.0468085408210754
[08/27/2025 15:59:07 INFO]: Training loss at epoch 23: 0.8051869869232178
[08/27/2025 15:59:07 INFO]: Training accuracy: {
    "score": -1.0025055526184423,
    "rmse": 1.0025055526184423
}
[08/27/2025 15:59:07 INFO]: Val accuracy: {
    "score": -0.659813511764355,
    "rmse": 0.659813511764355
}
[08/27/2025 15:59:07 INFO]: Test accuracy: {
    "score": -0.8685318028807686,
    "rmse": 0.8685318028807686
}
[08/27/2025 15:59:07 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_23",
    "best_epoch": 12,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8685318028807686,
        "rmse": 0.8685318028807686
    },
    "train_stats": {
        "score": -1.0025055526184423,
        "rmse": 1.0025055526184423
    },
    "val_stats": {
        "score": -0.659813511764355,
        "rmse": 0.659813511764355
    }
}
[08/27/2025 15:59:07 INFO]: Procewss finished for trial blotchy-Amado_trial_23
[08/27/2025 15:59:08 INFO]: 
_________________________________________________

[08/27/2025 15:59:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:59:08 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 1.1901412141542524
  attention_dropout: 0.0665018100894005
  ffn_dropout: 0.0665018100894005
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0002379738585661329
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_36

[08/27/2025 15:59:08 INFO]: This ft_transformer has 9.575 million parameters.
[08/27/2025 15:59:08 INFO]: Training will start at epoch 0.
[08/27/2025 15:59:08 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:59:23 INFO]: Training loss at epoch 24: 1.0896337628364563
[08/27/2025 15:59:24 INFO]: Training loss at epoch 56: 1.3358564376831055
[08/27/2025 15:59:37 INFO]: Training loss at epoch 49: 0.9752296507358551
[08/27/2025 15:59:47 INFO]: Training loss at epoch 30: 0.9913641810417175
[08/27/2025 15:59:54 INFO]: Training loss at epoch 18: 1.21991366147995
[08/27/2025 16:00:10 INFO]: Training loss at epoch 28: 1.113333374261856
[08/27/2025 16:00:12 INFO]: Training stats: {
    "score": -0.9884649300303213,
    "rmse": 0.9884649300303213
}
[08/27/2025 16:00:12 INFO]: Val stats: {
    "score": -0.6631725647135188,
    "rmse": 0.6631725647135188
}
[08/27/2025 16:00:12 INFO]: Test stats: {
    "score": -0.8744423535693476,
    "rmse": 0.8744423535693476
}
[08/27/2025 16:00:30 INFO]: Training loss at epoch 16: 0.9170952141284943
[08/27/2025 16:00:54 INFO]: Training loss at epoch 57: 1.0500443875789642
[08/27/2025 16:00:56 INFO]: Training loss at epoch 2: 1.3302554488182068
[08/27/2025 16:00:58 INFO]: Training loss at epoch 4: 1.1594364643096924
[08/27/2025 16:01:21 INFO]: Training loss at epoch 11: 1.1130822002887726
[08/27/2025 16:01:54 INFO]: Training loss at epoch 50: 0.9286588430404663
[08/27/2025 16:02:02 INFO]: Training loss at epoch 17: 1.0464879274368286
[08/27/2025 16:02:12 INFO]: Training loss at epoch 14: 0.9743223190307617
[08/27/2025 16:02:14 INFO]: Training loss at epoch 0: 1.413823664188385
[08/27/2025 16:02:23 INFO]: Training loss at epoch 58: 1.043305903673172
[08/27/2025 16:02:29 INFO]: Training loss at epoch 31: 0.8411845862865448
[08/27/2025 16:02:35 INFO]: Training loss at epoch 29: 1.2326301336288452
[08/27/2025 16:02:35 INFO]: Training loss at epoch 24: 1.1118610501289368
[08/27/2025 16:02:45 INFO]: Training loss at epoch 25: 0.9961243569850922
[08/27/2025 16:03:15 INFO]: New best epoch, val score: -1.1111201370645007
[08/27/2025 16:03:15 INFO]: Saving model to: blotchy-Amado_trial_35/model_best.pth
[08/27/2025 16:03:20 INFO]: Training loss at epoch 1: 2.934129238128662
[08/27/2025 16:03:21 INFO]: Training stats: {
    "score": -0.9980782710498837,
    "rmse": 0.9980782710498837
}
[08/27/2025 16:03:21 INFO]: Val stats: {
    "score": -0.6704709021503105,
    "rmse": 0.6704709021503105
}
[08/27/2025 16:03:21 INFO]: Test stats: {
    "score": -0.8710488861613448,
    "rmse": 0.8710488861613448
}
[08/27/2025 16:03:30 INFO]: Training loss at epoch 18: 1.0029665231704712
[08/27/2025 16:03:32 INFO]: Training loss at epoch 51: 1.2025413513183594
[08/27/2025 16:03:47 INFO]: Training loss at epoch 59: 0.895539790391922
[08/27/2025 16:03:49 INFO]: Training loss at epoch 12: 1.1338410675525665
[08/27/2025 16:04:14 INFO]: Training loss at epoch 3: 1.3322353959083557
[08/27/2025 16:04:18 INFO]: Training stats: {
    "score": -0.9857549862310844,
    "rmse": 0.9857549862310844
}
[08/27/2025 16:04:18 INFO]: Val stats: {
    "score": -0.6663809012698915,
    "rmse": 0.6663809012698915
}
[08/27/2025 16:04:18 INFO]: Test stats: {
    "score": -0.8717124362631766,
    "rmse": 0.8717124362631766
}
[08/27/2025 16:04:21 INFO]: Training loss at epoch 19: 0.9765394032001495
[08/27/2025 16:04:26 INFO]: Training loss at epoch 0: 0.9020223617553711
[08/27/2025 16:04:38 INFO]: Training loss at epoch 13: 1.397377610206604
[08/27/2025 16:04:46 INFO]: Training loss at epoch 5: 0.8895190358161926
[08/27/2025 16:05:00 INFO]: Training loss at epoch 19: 0.8293064832687378
[08/27/2025 16:05:06 INFO]: Training loss at epoch 32: 0.9456766247749329
[08/27/2025 16:05:11 INFO]: Training loss at epoch 4: 1.457040250301361
[08/27/2025 16:05:11 INFO]: New best epoch, val score: -0.7130972280219123
[08/27/2025 16:05:11 INFO]: Saving model to: blotchy-Amado_trial_36/model_best.pth
[08/27/2025 16:05:14 INFO]: Training loss at epoch 52: 1.1424522697925568
[08/27/2025 16:05:24 INFO]: Training loss at epoch 6: 1.0578223168849945
[08/27/2025 16:05:35 INFO]: Training stats: {
    "score": -0.999996835812693,
    "rmse": 0.999996835812693
}
[08/27/2025 16:05:35 INFO]: Val stats: {
    "score": -0.6941896731483145,
    "rmse": 0.6941896731483145
}
[08/27/2025 16:05:35 INFO]: Test stats: {
    "score": -0.8862339284660944,
    "rmse": 0.8862339284660944
}
[08/27/2025 16:05:43 INFO]: Training loss at epoch 30: 0.9191087484359741
[08/27/2025 16:05:47 INFO]: Training loss at epoch 60: 0.8977261781692505
[08/27/2025 16:05:55 INFO]: Training stats: {
    "score": -1.014068999147327,
    "rmse": 1.014068999147327
}
[08/27/2025 16:05:55 INFO]: Val stats: {
    "score": -0.6675217806416321,
    "rmse": 0.6675217806416321
}
[08/27/2025 16:05:55 INFO]: Test stats: {
    "score": -0.8824170179619737,
    "rmse": 0.8824170179619737
}
[08/27/2025 16:05:57 INFO]: Running Final Evaluation...
[08/27/2025 16:06:01 INFO]: Training loss at epoch 25: 1.0043621063232422
[08/27/2025 16:06:04 INFO]: Training loss at epoch 26: 0.9768510162830353
[08/27/2025 16:06:22 INFO]: Training loss at epoch 13: 1.039782702922821
[08/27/2025 16:06:28 INFO]: New best epoch, val score: -0.6675217806416321
[08/27/2025 16:06:28 INFO]: Saving model to: blotchy-Amado_trial_19/model_best.pth
[08/27/2025 16:06:30 INFO]: Training accuracy: {
    "score": -0.9994691366256423,
    "rmse": 0.9994691366256423
}
[08/27/2025 16:06:30 INFO]: Val accuracy: {
    "score": -0.6581456437991539,
    "rmse": 0.6581456437991539
}
[08/27/2025 16:06:30 INFO]: Test accuracy: {
    "score": -0.8690474134667737,
    "rmse": 0.8690474134667737
}
[08/27/2025 16:06:30 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_20",
    "best_epoch": 29,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8690474134667737,
        "rmse": 0.8690474134667737
    },
    "train_stats": {
        "score": -0.9994691366256423,
        "rmse": 0.9994691366256423
    },
    "val_stats": {
        "score": -0.6581456437991539,
        "rmse": 0.6581456437991539
    }
}
[08/27/2025 16:06:30 INFO]: Procewss finished for trial blotchy-Amado_trial_20
[08/27/2025 16:06:30 INFO]: 
_________________________________________________

[08/27/2025 16:06:30 INFO]: train_net_for_optune.py main() running.
[08/27/2025 16:06:30 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.6386567997727015
  attention_dropout: 0.07539940006574297
  ffn_dropout: 0.07539940006574297
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.880640735573009e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_37

[08/27/2025 16:06:30 INFO]: This ft_transformer has 12.688 million parameters.
[08/27/2025 16:06:30 INFO]: Training will start at epoch 0.
[08/27/2025 16:06:30 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 16:06:36 INFO]: Training loss at epoch 3: 1.8958497643470764
[08/27/2025 16:06:52 INFO]: Training loss at epoch 53: 0.8875642418861389
[08/27/2025 16:07:03 INFO]: Training loss at epoch 20: 1.0050744414329529
[08/27/2025 16:07:15 INFO]: New best epoch, val score: -0.6728517335281592
[08/27/2025 16:07:15 INFO]: Saving model to: blotchy-Amado_trial_34/model_best.pth
[08/27/2025 16:07:31 INFO]: Training loss at epoch 2: 1.0934513211250305
[08/27/2025 16:07:44 INFO]: Training loss at epoch 33: 1.2010589838027954
[08/27/2025 16:07:50 INFO]: Training loss at epoch 15: 0.9099145531654358
[08/27/2025 16:07:57 INFO]: Training loss at epoch 31: 1.2970232963562012
[08/27/2025 16:08:25 INFO]: Training loss at epoch 6: 0.940515547990799
[08/27/2025 16:08:25 INFO]: Training loss at epoch 54: 1.0894934833049774
[08/27/2025 16:08:28 INFO]: Training loss at epoch 21: 0.930069088935852
[08/27/2025 16:08:44 INFO]: Training loss at epoch 14: 1.2217881083488464
[08/27/2025 16:09:15 INFO]: Training loss at epoch 27: 1.1074689030647278
[08/27/2025 16:09:20 INFO]: Training loss at epoch 26: 0.8523676991462708
[08/27/2025 16:09:22 INFO]: Training loss at epoch 4: 1.2152274549007416
[08/27/2025 16:09:56 INFO]: Training loss at epoch 22: 1.2969127595424652
[08/27/2025 16:09:59 INFO]: New best epoch, val score: -0.6650600716481938
[08/27/2025 16:09:59 INFO]: Saving model to: blotchy-Amado_trial_33/model_best.pth
[08/27/2025 16:10:03 INFO]: Training loss at epoch 55: 0.8631933331489563
[08/27/2025 16:10:14 INFO]: Training loss at epoch 32: 1.1249423027038574
[08/27/2025 16:10:14 INFO]: Training loss at epoch 20: 0.9551018476486206
[08/27/2025 16:10:20 INFO]: Training loss at epoch 34: 0.9937450587749481
[08/27/2025 16:10:23 INFO]: Training loss at epoch 1: 3.104665696620941
[08/27/2025 16:10:52 INFO]: Training loss at epoch 14: 0.8758801519870758
[08/27/2025 16:11:02 INFO]: Training loss at epoch 1: 2.5675764679908752
[08/27/2025 16:11:16 INFO]: Training loss at epoch 15: 0.8722790777683258
[08/27/2025 16:11:21 INFO]: Training loss at epoch 0: 1.0007913410663605
[08/27/2025 16:11:29 INFO]: Training loss at epoch 23: 1.0825912356376648
[08/27/2025 16:11:46 INFO]: Training loss at epoch 56: 0.9269377887248993
[08/27/2025 16:12:03 INFO]: New best epoch, val score: -0.6754713998312076
[08/27/2025 16:12:03 INFO]: Saving model to: blotchy-Amado_trial_37/model_best.pth
[08/27/2025 16:12:09 INFO]: New best epoch, val score: -1.0642243783798637
[08/27/2025 16:12:09 INFO]: Saving model to: blotchy-Amado_trial_35/model_best.pth
[08/27/2025 16:12:10 INFO]: Training loss at epoch 4: 1.1631363928318024
[08/27/2025 16:12:14 INFO]: Training loss at epoch 7: 0.8912414908409119
[08/27/2025 16:12:15 INFO]: Training loss at epoch 5: 1.0336852073669434
[08/27/2025 16:12:37 INFO]: Training loss at epoch 33: 1.0370822548866272
[08/27/2025 16:12:40 INFO]: Training loss at epoch 28: 0.9762510657310486
[08/27/2025 16:12:52 INFO]: Training loss at epoch 27: 1.0207385122776031
[08/27/2025 16:13:01 INFO]: Training loss at epoch 35: 1.2023674249649048
[08/27/2025 16:13:01 INFO]: Training loss at epoch 24: 0.9435884952545166
[08/27/2025 16:13:14 INFO]: New best epoch, val score: -0.6657087294328541
[08/27/2025 16:13:14 INFO]: Saving model to: blotchy-Amado_trial_29/model_best.pth
[08/27/2025 16:13:27 INFO]: Training loss at epoch 57: 0.8694673776626587
[08/27/2025 16:13:34 INFO]: Training loss at epoch 16: 0.832747757434845
[08/27/2025 16:13:50 INFO]: Training loss at epoch 16: 0.8448984622955322
[08/27/2025 16:14:31 INFO]: Training loss at epoch 25: 1.1056111454963684
[08/27/2025 16:14:34 INFO]: Training loss at epoch 2: 1.433467984199524
[08/27/2025 16:14:43 INFO]: New best epoch, val score: -0.6626454930028869
[08/27/2025 16:14:43 INFO]: Saving model to: blotchy-Amado_trial_29/model_best.pth
[08/27/2025 16:14:43 INFO]: Training loss at epoch 5: 1.0579413771629333
[08/27/2025 16:14:44 INFO]: Training loss at epoch 21: 1.179406762123108
[08/27/2025 16:14:58 INFO]: Training loss at epoch 34: 0.9234107136726379
[08/27/2025 16:15:11 INFO]: Training loss at epoch 58: 0.8619998097419739
[08/27/2025 16:15:15 INFO]: Running Final Evaluation...
[08/27/2025 16:15:22 INFO]: New best epoch, val score: -0.6627605559360196
[08/27/2025 16:15:22 INFO]: Saving model to: blotchy-Amado_trial_33/model_best.pth
[08/27/2025 16:15:40 INFO]: Training loss at epoch 36: 1.1210442781448364
[08/27/2025 16:15:56 INFO]: New best epoch, val score: -0.6830817764813631
[08/27/2025 16:15:56 INFO]: Saving model to: blotchy-Amado_trial_31/model_best.pth
[08/27/2025 16:16:05 INFO]: Training loss at epoch 29: 1.1009777188301086
[08/27/2025 16:16:06 INFO]: Training loss at epoch 8: 1.248510867357254
[08/27/2025 16:16:07 INFO]: Training loss at epoch 26: 1.1822370886802673
[08/27/2025 16:16:11 INFO]: Training accuracy: {
    "score": -1.0162806970664482,
    "rmse": 1.0162806970664482
}
[08/27/2025 16:16:11 INFO]: Val accuracy: {
    "score": -0.6619294124355357,
    "rmse": 0.6619294124355357
}
[08/27/2025 16:16:11 INFO]: Test accuracy: {
    "score": -0.8715428656602197,
    "rmse": 0.8715428656602197
}
[08/27/2025 16:16:11 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_21",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8715428656602197,
        "rmse": 0.8715428656602197
    },
    "train_stats": {
        "score": -1.0162806970664482,
        "rmse": 1.0162806970664482
    },
    "val_stats": {
        "score": -0.6619294124355357,
        "rmse": 0.6619294124355357
    }
}
[08/27/2025 16:16:11 INFO]: Procewss finished for trial blotchy-Amado_trial_21
[08/27/2025 16:16:11 INFO]: 
_________________________________________________

[08/27/2025 16:16:11 INFO]: train_net_for_optune.py main() running.
[08/27/2025 16:16:11 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.5715942758559054
  attention_dropout: 0.1028430539981671
  ffn_dropout: 0.1028430539981671
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.068794539961469e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_38

[08/27/2025 16:16:11 INFO]: This ft_transformer has 12.479 million parameters.
[08/27/2025 16:16:11 INFO]: Training will start at epoch 0.
[08/27/2025 16:16:11 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 16:16:19 INFO]: New best epoch, val score: -0.6605004970604583
[08/27/2025 16:16:19 INFO]: Saving model to: blotchy-Amado_trial_29/model_best.pth
[08/27/2025 16:16:21 INFO]: Training loss at epoch 17: 0.8815268576145172
[08/27/2025 16:16:23 INFO]: Training loss at epoch 28: 1.10061514377594
[08/27/2025 16:16:34 INFO]: Training loss at epoch 2: 2.121575891971588
[08/27/2025 16:16:38 INFO]: New best epoch, val score: -0.6700406341387144
[08/27/2025 16:16:38 INFO]: Saving model to: blotchy-Amado_trial_28/model_best.pth
[08/27/2025 16:16:51 INFO]: Training loss at epoch 59: 1.1523293852806091
[08/27/2025 16:16:58 INFO]: Training loss at epoch 3: 1.1119968891143799
[08/27/2025 16:17:01 INFO]: Training loss at epoch 1: 0.8663517236709595
[08/27/2025 16:17:06 INFO]: Training stats: {
    "score": -0.9913650909513679,
    "rmse": 0.9913650909513679
}
[08/27/2025 16:17:06 INFO]: Val stats: {
    "score": -0.655676506969948,
    "rmse": 0.655676506969948
}
[08/27/2025 16:17:06 INFO]: Test stats: {
    "score": -0.8795739840556676,
    "rmse": 0.8795739840556676
}
[08/27/2025 16:17:13 INFO]: Training loss at epoch 15: 0.9432562291622162
[08/27/2025 16:17:23 INFO]: Training stats: {
    "score": -0.955887704395157,
    "rmse": 0.955887704395157
}
[08/27/2025 16:17:23 INFO]: Val stats: {
    "score": -0.6804778219787349,
    "rmse": 0.6804778219787349
}
[08/27/2025 16:17:23 INFO]: Test stats: {
    "score": -0.8914491483825341,
    "rmse": 0.8914491483825341
}
[08/27/2025 16:17:33 INFO]: Training loss at epoch 27: 0.9152300357818604
[08/27/2025 16:17:44 INFO]: New best epoch, val score: -0.6594881891360977
[08/27/2025 16:17:44 INFO]: Saving model to: blotchy-Amado_trial_29/model_best.pth
[08/27/2025 16:17:45 INFO]: Training loss at epoch 5: 1.4403706789016724
[08/27/2025 16:18:19 INFO]: Training loss at epoch 37: 1.0769537389278412
[08/27/2025 16:18:27 INFO]: Training loss at epoch 7: 1.2091702222824097
[08/27/2025 16:18:42 INFO]: Training loss at epoch 18: 1.1646742522716522
[08/27/2025 16:18:57 INFO]: Training loss at epoch 60: 0.8912772238254547
[08/27/2025 16:18:57 INFO]: Training loss at epoch 28: 1.039135992527008
[08/27/2025 16:18:58 INFO]: New best epoch, val score: -0.6680291916005244
[08/27/2025 16:18:58 INFO]: Saving model to: blotchy-Amado_trial_28/model_best.pth
[08/27/2025 16:19:02 INFO]: Training loss at epoch 22: 0.9580996334552765
[08/27/2025 16:19:11 INFO]: Training loss at epoch 17: 1.3087490797042847
[08/27/2025 16:19:12 INFO]: Training loss at epoch 6: 0.9931062161922455
[08/27/2025 16:19:35 INFO]: Training loss at epoch 29: 1.1356038451194763
[08/27/2025 16:19:36 INFO]: Training loss at epoch 9: 0.8795566856861115
[08/27/2025 16:19:46 INFO]: Training loss at epoch 6: 1.076282560825348
[08/27/2025 16:19:51 INFO]: Training loss at epoch 2: 1.8392430543899536
[08/27/2025 16:20:12 INFO]: Training loss at epoch 30: 1.0065904557704926
[08/27/2025 16:20:20 INFO]: New best epoch, val score: -0.6602655584388335
[08/27/2025 16:20:20 INFO]: Saving model to: blotchy-Amado_trial_33/model_best.pth
[08/27/2025 16:20:22 INFO]: Training loss at epoch 29: 0.8742249608039856
[08/27/2025 16:20:30 INFO]: Training loss at epoch 61: 0.9449391067028046
[08/27/2025 16:20:42 INFO]: Training stats: {
    "score": -1.0023178497559362,
    "rmse": 1.0023178497559362
}
[08/27/2025 16:20:42 INFO]: Val stats: {
    "score": -0.6599411007673519,
    "rmse": 0.6599411007673519
}
[08/27/2025 16:20:42 INFO]: Test stats: {
    "score": -0.8738304857211521,
    "rmse": 0.8738304857211521
}
[08/27/2025 16:20:46 INFO]: Training loss at epoch 0: 1.1869485974311829
[08/27/2025 16:20:48 INFO]: Training stats: {
    "score": -0.992825086612051,
    "rmse": 0.992825086612051
}
[08/27/2025 16:20:48 INFO]: Val stats: {
    "score": -0.6645301774650612,
    "rmse": 0.6645301774650612
}
[08/27/2025 16:20:48 INFO]: Test stats: {
    "score": -0.8814229610395637,
    "rmse": 0.8814229610395637
}
[08/27/2025 16:20:50 INFO]: Training loss at epoch 38: 0.9923359453678131
[08/27/2025 16:20:51 INFO]: New best epoch, val score: -1.0564834625211783
[08/27/2025 16:20:51 INFO]: Saving model to: blotchy-Amado_trial_35/model_best.pth
[08/27/2025 16:20:55 INFO]: Training stats: {
    "score": -0.9991817781467622,
    "rmse": 0.9991817781467622
}
[08/27/2025 16:20:55 INFO]: Val stats: {
    "score": -0.6634149851244199,
    "rmse": 0.6634149851244199
}
[08/27/2025 16:20:55 INFO]: Test stats: {
    "score": -0.8722531133925789,
    "rmse": 0.8722531133925789
}
[08/27/2025 16:21:02 INFO]: Training loss at epoch 19: 0.8480530381202698
[08/27/2025 16:21:23 INFO]: New best epoch, val score: -0.7999594519099864
[08/27/2025 16:21:23 INFO]: Saving model to: blotchy-Amado_trial_38/model_best.pth
[08/27/2025 16:21:50 INFO]: Training stats: {
    "score": -0.999880720762585,
    "rmse": 0.999880720762585
}
[08/27/2025 16:21:50 INFO]: Val stats: {
    "score": -0.6702623099525099,
    "rmse": 0.6702623099525099
}
[08/27/2025 16:21:50 INFO]: Test stats: {
    "score": -0.8693820148256548,
    "rmse": 0.8693820148256548
}
[08/27/2025 16:22:07 INFO]: Training loss at epoch 62: 0.9644306302070618
[08/27/2025 16:22:13 INFO]: Training loss at epoch 3: 1.1777908205986023
[08/27/2025 16:22:17 INFO]: Training loss at epoch 2: 0.8954607248306274
[08/27/2025 16:22:20 INFO]: Training loss at epoch 30: 0.8679234385490417
[08/27/2025 16:22:54 INFO]: New best epoch, val score: -0.6919712436659234
[08/27/2025 16:22:54 INFO]: Saving model to: blotchy-Amado_trial_36/model_best.pth
[08/27/2025 16:23:03 INFO]: Training loss at epoch 6: 1.4981397986412048
[08/27/2025 16:23:11 INFO]: Training loss at epoch 16: 1.052357792854309
[08/27/2025 16:23:13 INFO]: Training loss at epoch 23: 0.9729695320129395
[08/27/2025 16:23:21 INFO]: Training loss at epoch 31: 0.8501434922218323
[08/27/2025 16:23:24 INFO]: Training loss at epoch 39: 0.9233736395835876
[08/27/2025 16:23:42 INFO]: Training loss at epoch 63: 0.8303867876529694
[08/27/2025 16:23:45 INFO]: Training loss at epoch 31: 1.0996565520763397
[08/27/2025 16:23:55 INFO]: Running Final Evaluation...
[08/27/2025 16:24:00 INFO]: Training loss at epoch 30: 0.9017638266086578
[08/27/2025 16:24:15 INFO]: Training loss at epoch 20: 1.1143320202827454
[08/27/2025 16:24:22 INFO]: Training stats: {
    "score": -0.9997866627138935,
    "rmse": 0.9997866627138935
}
[08/27/2025 16:24:22 INFO]: Val stats: {
    "score": -0.6705994037856032,
    "rmse": 0.6705994037856032
}
[08/27/2025 16:24:22 INFO]: Test stats: {
    "score": -0.8684864594801792,
    "rmse": 0.8684864594801792
}
[08/27/2025 16:24:23 INFO]: Training loss at epoch 10: 1.006660372018814
[08/27/2025 16:24:31 INFO]: Training accuracy: {
    "score": -1.004158478085927,
    "rmse": 1.004158478085927
}
[08/27/2025 16:24:31 INFO]: Val accuracy: {
    "score": -0.6599218285612326,
    "rmse": 0.6599218285612326
}
[08/27/2025 16:24:31 INFO]: Test accuracy: {
    "score": -0.8720316390263431,
    "rmse": 0.8720316390263431
}
[08/27/2025 16:24:32 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_12",
    "best_epoch": 32,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8720316390263431,
        "rmse": 0.8720316390263431
    },
    "train_stats": {
        "score": -1.004158478085927,
        "rmse": 1.004158478085927
    },
    "val_stats": {
        "score": -0.6599218285612326,
        "rmse": 0.6599218285612326
    }
}
[08/27/2025 16:24:32 INFO]: Procewss finished for trial blotchy-Amado_trial_12
[08/27/2025 16:24:32 INFO]: 
_________________________________________________

[08/27/2025 16:24:32 INFO]: train_net_for_optune.py main() running.
[08/27/2025 16:24:32 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.5163900887332638
  attention_dropout: 0.08911278065922262
  ffn_dropout: 0.08911278065922262
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.7299529550104386e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_39

[08/27/2025 16:24:32 INFO]: This ft_transformer has 5.472 million parameters.
[08/27/2025 16:24:32 INFO]: Training will start at epoch 0.
[08/27/2025 16:24:32 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 16:24:39 INFO]: Training loss at epoch 18: 0.8480487763881683
[08/27/2025 16:24:48 INFO]: Training loss at epoch 7: 0.9357677400112152
[08/27/2025 16:25:15 INFO]: Training loss at epoch 32: 0.8540910482406616
[08/27/2025 16:25:25 INFO]: New best epoch, val score: -0.6585974082350837
[08/27/2025 16:25:25 INFO]: Saving model to: blotchy-Amado_trial_33/model_best.pth
[08/27/2025 16:25:39 INFO]: Training loss at epoch 3: 1.157391607761383
[08/27/2025 16:26:07 INFO]: Training loss at epoch 4: 1.4572668075561523
[08/27/2025 16:26:09 INFO]: Training loss at epoch 7: 1.0770970582962036
[08/27/2025 16:26:11 INFO]: Training loss at epoch 1: 1.0187584459781647
[08/27/2025 16:26:47 INFO]: Training loss at epoch 32: 1.0993549227714539
[08/27/2025 16:26:53 INFO]: Training loss at epoch 33: 1.0266762971878052
[08/27/2025 16:26:55 INFO]: Training loss at epoch 21: 1.2007533013820648
[08/27/2025 16:27:10 INFO]: Training loss at epoch 40: 0.9188259840011597
[08/27/2025 16:27:16 INFO]: New best epoch, val score: -0.6976317216155326
[08/27/2025 16:27:16 INFO]: Saving model to: blotchy-Amado_trial_30/model_best.pth
[08/27/2025 16:27:26 INFO]: Training loss at epoch 0: 0.8514938950538635
[08/27/2025 16:27:37 INFO]: Training loss at epoch 31: 1.0563623905181885
[08/27/2025 16:27:47 INFO]: New best epoch, val score: -0.6805749099472412
[08/27/2025 16:27:47 INFO]: Saving model to: blotchy-Amado_trial_39/model_best.pth
[08/27/2025 16:27:48 INFO]: Training loss at epoch 24: 0.9283563196659088
[08/27/2025 16:27:57 INFO]: Training loss at epoch 3: 1.1574621498584747
[08/27/2025 16:28:20 INFO]: Training loss at epoch 4: 1.264284610748291
[08/27/2025 16:28:20 INFO]: Training loss at epoch 11: 0.9469976127147675
[08/27/2025 16:28:22 INFO]: Training loss at epoch 34: 1.1444045007228851
[08/27/2025 16:28:44 INFO]: Training loss at epoch 3: 1.3566213846206665
[08/27/2025 16:28:46 INFO]: Training loss at epoch 7: 1.4565568566322327
[08/27/2025 16:29:03 INFO]: New best epoch, val score: -0.6763167209416965
[08/27/2025 16:29:03 INFO]: Saving model to: blotchy-Amado_trial_36/model_best.pth
[08/27/2025 16:29:26 INFO]: Training loss at epoch 22: 0.838473379611969
[08/27/2025 16:29:38 INFO]: Training loss at epoch 17: 1.2145894467830658
[08/27/2025 16:29:48 INFO]: Training loss at epoch 41: 0.8194093704223633
[08/27/2025 16:29:51 INFO]: New best epoch, val score: -0.6786781386089821
[08/27/2025 16:29:51 INFO]: Saving model to: blotchy-Amado_trial_35/model_best.pth
[08/27/2025 16:29:57 INFO]: Training loss at epoch 35: 1.0858608782291412
[08/27/2025 16:30:08 INFO]: Training loss at epoch 33: 0.9452639520168304
[08/27/2025 16:30:15 INFO]: Training loss at epoch 8: 0.9218669533729553
[08/27/2025 16:30:37 INFO]: Training loss at epoch 19: 0.9616989195346832
[08/27/2025 16:30:38 INFO]: Training loss at epoch 1: 0.9263713657855988
[08/27/2025 16:31:02 INFO]: New best epoch, val score: -0.6750865727787787
[08/27/2025 16:31:02 INFO]: Saving model to: blotchy-Amado_trial_39/model_best.pth
[08/27/2025 16:31:10 INFO]: Training loss at epoch 32: 0.9274471700191498
[08/27/2025 16:31:27 INFO]: Training loss at epoch 36: 0.9604344964027405
[08/27/2025 16:31:34 INFO]: Training loss at epoch 8: 1.0542185306549072
[08/27/2025 16:31:51 INFO]: Training loss at epoch 2: 1.2333186864852905
[08/27/2025 16:32:03 INFO]: Training loss at epoch 23: 1.2236174941062927
[08/27/2025 16:32:13 INFO]: Training loss at epoch 12: 0.9062094688415527
[08/27/2025 16:32:21 INFO]: Training loss at epoch 25: 0.8938175439834595
[08/27/2025 16:32:29 INFO]: New best epoch, val score: -0.6585505897204982
[08/27/2025 16:32:29 INFO]: Saving model to: blotchy-Amado_trial_38/model_best.pth
[08/27/2025 16:32:33 INFO]: Training loss at epoch 42: 1.0401330888271332
[08/27/2025 16:32:40 INFO]: Training stats: {
    "score": -1.0365013455801597,
    "rmse": 1.0365013455801597
}
[08/27/2025 16:32:40 INFO]: Val stats: {
    "score": -0.7913986686487191,
    "rmse": 0.7913986686487191
}
[08/27/2025 16:32:40 INFO]: Test stats: {
    "score": -0.9432913557331204,
    "rmse": 0.9432913557331204
}
[08/27/2025 16:32:54 INFO]: Training loss at epoch 37: 1.026256263256073
[08/27/2025 16:33:21 INFO]: Training loss at epoch 8: 1.0267753303050995
[08/27/2025 16:33:25 INFO]: Training loss at epoch 34: 0.9645697474479675
[08/27/2025 16:33:39 INFO]: Training loss at epoch 4: 0.8621387779712677
[08/27/2025 16:33:40 INFO]: Training loss at epoch 2: 0.9035727381706238
[08/27/2025 16:34:18 INFO]: Training loss at epoch 38: 0.9928504526615143
[08/27/2025 16:34:20 INFO]: Training loss at epoch 5: 1.1774250268936157
[08/27/2025 16:34:22 INFO]: Training loss at epoch 8: 1.5924864411354065
[08/27/2025 16:34:23 INFO]: Training loss at epoch 24: 0.8413744568824768
[08/27/2025 16:34:28 INFO]: Training loss at epoch 33: 1.1286669075489044
[08/27/2025 16:35:03 INFO]: New best epoch, val score: -0.6738214501451484
[08/27/2025 16:35:03 INFO]: Saving model to: blotchy-Amado_trial_36/model_best.pth
[08/27/2025 16:35:07 INFO]: Training loss at epoch 43: 0.9863342046737671
[08/27/2025 16:35:24 INFO]: Training loss at epoch 9: 1.148849606513977
[08/27/2025 16:35:36 INFO]: Training loss at epoch 5: 1.0193707644939423
[08/27/2025 16:35:49 INFO]: Training loss at epoch 13: 1.036335825920105
[08/27/2025 16:35:49 INFO]: Training loss at epoch 39: 0.8345697224140167
[08/27/2025 16:35:54 INFO]: Training loss at epoch 18: 0.9454435706138611
[08/27/2025 16:36:22 INFO]: Training stats: {
    "score": -0.9984984585100305,
    "rmse": 0.9984984585100305
}
[08/27/2025 16:36:22 INFO]: Val stats: {
    "score": -0.6921100791464189,
    "rmse": 0.6921100791464189
}
[08/27/2025 16:36:22 INFO]: Test stats: {
    "score": -0.8858552702610117,
    "rmse": 0.8858552702610117
}
[08/27/2025 16:36:38 INFO]: Training loss at epoch 26: 0.9348369240760803
[08/27/2025 16:36:42 INFO]: Training loss at epoch 35: 0.9812965393066406
[08/27/2025 16:36:42 INFO]: Training loss at epoch 3: 1.0698158740997314
[08/27/2025 16:36:54 INFO]: Training loss at epoch 25: 0.9484732151031494
[08/27/2025 16:37:00 INFO]: Training loss at epoch 4: 1.0444520711898804
[08/27/2025 16:37:05 INFO]: New best epoch, val score: -0.66744309398991
[08/27/2025 16:37:05 INFO]: Saving model to: blotchy-Amado_trial_39/model_best.pth
[08/27/2025 16:37:12 INFO]: Training stats: {
    "score": -0.9879647176907312,
    "rmse": 0.9879647176907312
}
[08/27/2025 16:37:12 INFO]: Val stats: {
    "score": -0.6621076864048809,
    "rmse": 0.6621076864048809
}
[08/27/2025 16:37:12 INFO]: Test stats: {
    "score": -0.8736582038437839,
    "rmse": 0.8736582038437839
}
[08/27/2025 16:37:15 INFO]: Training loss at epoch 3: 0.991278886795044
[08/27/2025 16:37:43 INFO]: Training loss at epoch 4: 1.148522526025772
[08/27/2025 16:37:47 INFO]: Training loss at epoch 44: 0.8062262833118439
[08/27/2025 16:37:54 INFO]: Training loss at epoch 40: 1.0839591324329376
[08/27/2025 16:38:00 INFO]: Training loss at epoch 34: 1.1835688352584839
[08/27/2025 16:38:20 INFO]: Training loss at epoch 20: 1.0436739325523376
[08/27/2025 16:38:20 INFO]: New best epoch, val score: -0.6624765705119884
[08/27/2025 16:38:20 INFO]: Saving model to: blotchy-Amado_trial_31/model_best.pth
[08/27/2025 16:38:23 INFO]: New best epoch, val score: -0.6581350918086136
[08/27/2025 16:38:23 INFO]: Saving model to: blotchy-Amado_trial_3/model_best.pth
[08/27/2025 16:39:21 INFO]: Training loss at epoch 5: 0.9589583575725555
[08/27/2025 16:39:21 INFO]: Training loss at epoch 41: 0.9313653111457825
[08/27/2025 16:39:25 INFO]: Training loss at epoch 26: 1.0744067430496216
[08/27/2025 16:39:35 INFO]: Training loss at epoch 14: 1.0536146759986877
[08/27/2025 16:39:49 INFO]: Training loss at epoch 4: 1.2565515637397766
[08/27/2025 16:40:01 INFO]: Training loss at epoch 9: 1.4095213413238525
[08/27/2025 16:40:04 INFO]: Training loss at epoch 36: 0.9989816844463348
[08/27/2025 16:40:12 INFO]: New best epoch, val score: -0.657141569764391
[08/27/2025 16:40:12 INFO]: Saving model to: blotchy-Amado_trial_39/model_best.pth
[08/27/2025 16:40:24 INFO]: Training loss at epoch 45: 0.9999222755432129
[08/27/2025 16:40:25 INFO]: Training loss at epoch 6: 1.0682750344276428
[08/27/2025 16:40:28 INFO]: Training loss at epoch 9: 1.174743413925171
[08/27/2025 16:40:52 INFO]: Training loss at epoch 42: 1.243034541606903
[08/27/2025 16:41:08 INFO]: Training loss at epoch 27: 1.024336576461792
[08/27/2025 16:41:10 INFO]: New best epoch, val score: -0.6705105887381294
[08/27/2025 16:41:10 INFO]: Saving model to: blotchy-Amado_trial_36/model_best.pth
[08/27/2025 16:41:28 INFO]: Training loss at epoch 35: 0.99722820520401
[08/27/2025 16:41:58 INFO]: Training loss at epoch 27: 0.9631238579750061
[08/27/2025 16:42:04 INFO]: Training stats: {
    "score": -1.1666757934833656,
    "rmse": 1.1666757934833656
}
[08/27/2025 16:42:04 INFO]: Val stats: {
    "score": -0.7918395121065991,
    "rmse": 0.7918395121065991
}
[08/27/2025 16:42:04 INFO]: Test stats: {
    "score": -1.003617859105314,
    "rmse": 1.003617859105314
}
[08/27/2025 16:42:15 INFO]: Training loss at epoch 19: 0.9843282997608185
[08/27/2025 16:42:22 INFO]: Training loss at epoch 43: 0.8224593698978424
[08/27/2025 16:42:28 INFO]: Training loss at epoch 10: 1.0430973172187805
[08/27/2025 16:42:47 INFO]: Training loss at epoch 4: 1.1962117552757263
[08/27/2025 16:42:52 INFO]: Training stats: {
    "score": -0.9951272074530605,
    "rmse": 0.9951272074530605
}
[08/27/2025 16:42:52 INFO]: Val stats: {
    "score": -0.6960813100660171,
    "rmse": 0.6960813100660171
}
[08/27/2025 16:42:52 INFO]: Test stats: {
    "score": -0.881750617684957,
    "rmse": 0.881750617684957
}
[08/27/2025 16:42:54 INFO]: Training loss at epoch 5: 1.0004486739635468
[08/27/2025 16:43:02 INFO]: Training loss at epoch 46: 0.9294734597206116
[08/27/2025 16:43:15 INFO]: New best epoch, val score: -0.6571272859664491
[08/27/2025 16:43:15 INFO]: Saving model to: blotchy-Amado_trial_39/model_best.pth
[08/27/2025 16:43:20 INFO]: Training loss at epoch 15: 1.0628418326377869
[08/27/2025 16:43:21 INFO]: Training loss at epoch 37: 0.9506215155124664
[08/27/2025 16:43:46 INFO]: Training loss at epoch 44: 0.8924353420734406
[08/27/2025 16:44:01 INFO]: Training loss at epoch 21: 0.8658168017864227
[08/27/2025 16:44:18 INFO]: Training loss at epoch 28: 0.9817611277103424
[08/27/2025 16:44:18 INFO]: Training stats: {
    "score": -0.9810070324536615,
    "rmse": 0.9810070324536615
}
[08/27/2025 16:44:18 INFO]: Val stats: {
    "score": -0.7118014017952903,
    "rmse": 0.7118014017952903
}
[08/27/2025 16:44:18 INFO]: Test stats: {
    "score": -0.8929356006214042,
    "rmse": 0.8929356006214042
}
[08/27/2025 16:44:34 INFO]: Training loss at epoch 9: 0.9785292148590088
[08/27/2025 16:44:44 INFO]: Training loss at epoch 36: 1.0068824887275696
[08/27/2025 16:44:50 INFO]: Training loss at epoch 6: 1.0682697892189026
[08/27/2025 16:44:55 INFO]: Training loss at epoch 6: 0.9764147996902466
[08/27/2025 16:45:12 INFO]: Training loss at epoch 45: 0.9674611985683441
[08/27/2025 16:45:26 INFO]: Training loss at epoch 28: 0.9958425462245941
[08/27/2025 16:45:34 INFO]: Training loss at epoch 47: 0.9616454541683197
[08/27/2025 16:45:51 INFO]: Training loss at epoch 6: 1.1680506467819214
[08/27/2025 16:46:21 INFO]: Training loss at epoch 7: 1.2094287872314453
[08/27/2025 16:46:31 INFO]: Training loss at epoch 5: 1.297395408153534
[08/27/2025 16:46:36 INFO]: Training loss at epoch 38: 0.9894360899925232
[08/27/2025 16:46:43 INFO]: Training loss at epoch 46: 1.2967118918895721
[08/27/2025 16:46:48 INFO]: Training loss at epoch 29: 0.9520210325717926
[08/27/2025 16:47:02 INFO]: Training loss at epoch 16: 1.3416287302970886
[08/27/2025 16:47:06 INFO]: New best epoch, val score: -0.6669826754945263
[08/27/2025 16:47:06 INFO]: Saving model to: blotchy-Amado_trial_36/model_best.pth
[08/27/2025 16:47:35 INFO]: Training loss at epoch 10: 1.275078535079956
[08/27/2025 16:47:36 INFO]: Training loss at epoch 11: 0.8574435412883759
[08/27/2025 16:47:43 INFO]: Training stats: {
    "score": -1.0000283719550116,
    "rmse": 1.0000283719550116
}
[08/27/2025 16:47:43 INFO]: Val stats: {
    "score": -0.7005545728386098,
    "rmse": 0.7005545728386098
}
[08/27/2025 16:47:43 INFO]: Test stats: {
    "score": -0.8837008557238104,
    "rmse": 0.8837008557238104
}
[08/27/2025 16:48:14 INFO]: Training loss at epoch 47: 0.9029070436954498
[08/27/2025 16:48:14 INFO]: Training loss at epoch 5: 1.0791677236557007
[08/27/2025 16:48:15 INFO]: Training loss at epoch 48: 0.9307002127170563
[08/27/2025 16:48:15 INFO]: Training loss at epoch 37: 1.0460878610610962
[08/27/2025 16:48:15 INFO]: Training loss at epoch 5: 1.1066285371780396
[08/27/2025 16:48:18 INFO]: New best epoch, val score: -0.6717397753179536
[08/27/2025 16:48:18 INFO]: Saving model to: blotchy-Amado_trial_34/model_best.pth
[08/27/2025 16:48:57 INFO]: Training loss at epoch 7: 0.9884565472602844
[08/27/2025 16:49:04 INFO]: Training stats: {
    "score": -1.0263542589891386,
    "rmse": 1.0263542589891386
}
[08/27/2025 16:49:04 INFO]: Val stats: {
    "score": -0.8193720941340344,
    "rmse": 0.8193720941340344
}
[08/27/2025 16:49:04 INFO]: Test stats: {
    "score": -0.9544840532809179,
    "rmse": 0.9544840532809179
}
[08/27/2025 16:49:37 INFO]: New best epoch, val score: -0.6620317315337138
[08/27/2025 16:49:37 INFO]: Saving model to: blotchy-Amado_trial_31/model_best.pth
[08/27/2025 16:49:44 INFO]: Training loss at epoch 48: 1.1688599586486816
[08/27/2025 16:49:49 INFO]: Training loss at epoch 22: 0.9992880523204803
[08/27/2025 16:49:55 INFO]: Training loss at epoch 29: 0.9232777059078217
[08/27/2025 16:49:56 INFO]: Training loss at epoch 10: 1.218334823846817
[08/27/2025 16:49:58 INFO]: Training loss at epoch 39: 0.9356006681919098
[08/27/2025 16:50:13 INFO]: Training loss at epoch 30: 0.8659480512142181
[08/27/2025 16:50:33 INFO]: Training loss at epoch 7: 1.009496122598648
[08/27/2025 16:50:43 INFO]: Training loss at epoch 20: 0.916243314743042
[08/27/2025 16:50:54 INFO]: Training loss at epoch 17: 1.0879045724868774
[08/27/2025 16:50:55 INFO]: Training loss at epoch 49: 1.1342816948890686
[08/27/2025 16:51:06 INFO]: Training stats: {
    "score": -0.9850324104440601,
    "rmse": 0.9850324104440601
}
[08/27/2025 16:51:06 INFO]: Val stats: {
    "score": -0.6847309603652038,
    "rmse": 0.6847309603652038
}
[08/27/2025 16:51:06 INFO]: Test stats: {
    "score": -0.8901405990275107,
    "rmse": 0.8901405990275107
}
[08/27/2025 16:51:14 INFO]: Training loss at epoch 49: 1.0609359443187714
[08/27/2025 16:51:29 INFO]: Training stats: {
    "score": -0.9965147785699948,
    "rmse": 0.9965147785699948
}
[08/27/2025 16:51:29 INFO]: Val stats: {
    "score": -0.6941816940862234,
    "rmse": 0.6941816940862234
}
[08/27/2025 16:51:29 INFO]: Test stats: {
    "score": -0.8862977500842808,
    "rmse": 0.8862977500842808
}
[08/27/2025 16:51:46 INFO]: Training loss at epoch 38: 1.1963279247283936
[08/27/2025 16:51:48 INFO]: Training stats: {
    "score": -0.9967798265566815,
    "rmse": 0.9967798265566815
}
[08/27/2025 16:51:48 INFO]: Val stats: {
    "score": -0.6820410806147871,
    "rmse": 0.6820410806147871
}
[08/27/2025 16:51:48 INFO]: Test stats: {
    "score": -0.8801498355505822,
    "rmse": 0.8801498355505822
}
[08/27/2025 16:51:56 INFO]: Training stats: {
    "score": -1.000023279910638,
    "rmse": 1.000023279910638
}
[08/27/2025 16:51:56 INFO]: Val stats: {
    "score": -0.6692092798092538,
    "rmse": 0.6692092798092538
}
[08/27/2025 16:51:56 INFO]: Test stats: {
    "score": -0.8680141354810563,
    "rmse": 0.8680141354810563
}
[08/27/2025 16:52:06 INFO]: Training loss at epoch 8: 1.020016610622406
[08/27/2025 16:52:18 INFO]: Running Final Evaluation...
[08/27/2025 16:52:31 INFO]: Training loss at epoch 8: 1.326399326324463
[08/27/2025 16:52:43 INFO]: Training loss at epoch 31: 0.8720743656158447
[08/27/2025 16:52:52 INFO]: Training loss at epoch 12: 1.040459930896759
[08/27/2025 16:53:14 INFO]: New best epoch, val score: -0.6640524641077734
[08/27/2025 16:53:14 INFO]: Saving model to: blotchy-Amado_trial_36/model_best.pth
[08/27/2025 16:53:17 INFO]: Training loss at epoch 50: 1.088443100452423
[08/27/2025 16:53:18 INFO]: Training accuracy: {
    "score": -1.0148493843814042,
    "rmse": 1.0148493843814042
}
[08/27/2025 16:53:18 INFO]: Val accuracy: {
    "score": -0.6622464185474473,
    "rmse": 0.6622464185474473
}
[08/27/2025 16:53:18 INFO]: Test accuracy: {
    "score": -0.8696248842356572,
    "rmse": 0.8696248842356572
}
[08/27/2025 16:53:18 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_7",
    "best_epoch": 18,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8696248842356572,
        "rmse": 0.8696248842356572
    },
    "train_stats": {
        "score": -1.0148493843814042,
        "rmse": 1.0148493843814042
    },
    "val_stats": {
        "score": -0.6622464185474473,
        "rmse": 0.6622464185474473
    }
}
[08/27/2025 16:53:18 INFO]: Procewss finished for trial blotchy-Amado_trial_7
[08/27/2025 16:53:18 INFO]: 
_________________________________________________

[08/27/2025 16:53:18 INFO]: train_net_for_optune.py main() running.
[08/27/2025 16:53:18 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.5382704964111094
  attention_dropout: 0.09293986588810557
  ffn_dropout: 0.09293986588810557
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.351199122120034e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_40

[08/27/2025 16:53:18 INFO]: This ft_transformer has 5.497 million parameters.
[08/27/2025 16:53:18 INFO]: Training will start at epoch 0.
[08/27/2025 16:53:18 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 16:53:18 INFO]: Training loss at epoch 11: 0.8702487051486969
[08/27/2025 16:53:49 INFO]: Training loss at epoch 6: 0.9876539409160614
[08/27/2025 16:54:21 INFO]: Training loss at epoch 40: 1.0591776967048645
[08/27/2025 16:54:22 INFO]: Training loss at epoch 7: 1.0467103123664856
[08/27/2025 16:54:35 INFO]: Training loss at epoch 18: 0.9263812303543091
[08/27/2025 16:54:41 INFO]: Training loss at epoch 51: 1.055344820022583
[08/27/2025 16:55:02 INFO]: Training loss at epoch 9: 0.9281532764434814
[08/27/2025 16:55:04 INFO]: Training loss at epoch 39: 0.8095384836196899
[08/27/2025 16:55:05 INFO]: Training loss at epoch 32: 0.809420108795166
[08/27/2025 16:55:23 INFO]: Training loss at epoch 6: 1.302261769771576
[08/27/2025 16:55:27 INFO]: Training loss at epoch 23: 1.2568493783473969
[08/27/2025 16:55:43 INFO]: Training loss at epoch 30: 1.0022026598453522
[08/27/2025 16:55:49 INFO]: Training loss at epoch 0: 0.9385130405426025
[08/27/2025 16:55:58 INFO]: Training loss at epoch 8: 1.0656429827213287
[08/27/2025 16:56:00 INFO]: Training stats: {
    "score": -0.9999640735916137,
    "rmse": 0.9999640735916137
}
[08/27/2025 16:56:00 INFO]: Val stats: {
    "score": -0.6616759183272309,
    "rmse": 0.6616759183272309
}
[08/27/2025 16:56:00 INFO]: Test stats: {
    "score": -0.8677480742544472,
    "rmse": 0.8677480742544472
}
[08/27/2025 16:56:05 INFO]: Training loss at epoch 52: 0.8082825839519501
[08/27/2025 16:56:09 INFO]: Training stats: {
    "score": -1.0175219056320497,
    "rmse": 1.0175219056320497
}
[08/27/2025 16:56:09 INFO]: Val stats: {
    "score": -0.6596788509720803,
    "rmse": 0.6596788509720803
}
[08/27/2025 16:56:09 INFO]: Test stats: {
    "score": -0.8802224543282618,
    "rmse": 0.8802224543282618
}
[08/27/2025 16:56:10 INFO]: New best epoch, val score: -0.7991537297103734
[08/27/2025 16:56:10 INFO]: Saving model to: blotchy-Amado_trial_40/model_best.pth
[08/27/2025 16:56:47 INFO]: Training loss at epoch 21: 0.7766190767288208
[08/27/2025 16:56:49 INFO]: Training loss at epoch 11: 1.2647390961647034
[08/27/2025 16:57:25 INFO]: Training loss at epoch 33: 0.9106904864311218
[08/27/2025 16:57:27 INFO]: Training loss at epoch 41: 1.1399481892585754
[08/27/2025 16:57:30 INFO]: Training loss at epoch 53: 0.8525344133377075
[08/27/2025 16:57:47 INFO]: Training loss at epoch 13: 1.0585071444511414
[08/27/2025 16:58:07 INFO]: Training loss at epoch 19: 1.3868401050567627
[08/27/2025 16:58:14 INFO]: Training loss at epoch 9: 1.1734935641288757
[08/27/2025 16:58:38 INFO]: Training loss at epoch 12: 1.4958949089050293
[08/27/2025 16:58:47 INFO]: Training loss at epoch 1: 1.3083271980285645
[08/27/2025 16:58:59 INFO]: Training loss at epoch 54: 1.005334496498108
[08/27/2025 16:59:00 INFO]: Training loss at epoch 10: 0.9792039096355438
[08/27/2025 16:59:05 INFO]: Training loss at epoch 7: 0.8900299072265625
[08/27/2025 16:59:10 INFO]: New best epoch, val score: -0.6675472095250226
[08/27/2025 16:59:10 INFO]: Saving model to: blotchy-Amado_trial_40/model_best.pth
[08/27/2025 16:59:19 INFO]: Training loss at epoch 6: 0.8608242869377136
[08/27/2025 16:59:25 INFO]: Training stats: {
    "score": -0.969332295311051,
    "rmse": 0.969332295311051
}
[08/27/2025 16:59:25 INFO]: Val stats: {
    "score": -0.7437478486656844,
    "rmse": 0.7437478486656844
}
[08/27/2025 16:59:25 INFO]: Test stats: {
    "score": -0.9345050568655419,
    "rmse": 0.9345050568655419
}
[08/27/2025 16:59:32 INFO]: Training loss at epoch 40: 1.1239812970161438
[08/27/2025 16:59:57 INFO]: Training loss at epoch 34: 0.9860695600509644
[08/27/2025 17:00:04 INFO]: Training loss at epoch 31: 1.2475712299346924
[08/27/2025 17:00:21 INFO]: Training stats: {
    "score": -1.0201787827283353,
    "rmse": 1.0201787827283353
}
[08/27/2025 17:00:21 INFO]: Val stats: {
    "score": -0.6619641811863478,
    "rmse": 0.6619641811863478
}
[08/27/2025 17:00:21 INFO]: Test stats: {
    "score": -0.8750225888399981,
    "rmse": 0.8750225888399981
}
[08/27/2025 17:00:29 INFO]: Training loss at epoch 55: 1.1587193608283997
[08/27/2025 17:00:41 INFO]: New best epoch, val score: -0.661626732529226
[08/27/2025 17:00:41 INFO]: Saving model to: blotchy-Amado_trial_31/model_best.pth
[08/27/2025 17:00:50 INFO]: Training loss at epoch 42: 0.9417508840560913
[08/27/2025 17:01:06 INFO]: Training loss at epoch 24: 0.8099491894245148
[08/27/2025 17:01:07 INFO]: New best epoch, val score: -0.6619641811863478
[08/27/2025 17:01:07 INFO]: Saving model to: blotchy-Amado_trial_36/model_best.pth
[08/27/2025 17:01:34 INFO]: Training loss at epoch 9: 0.8069003820419312
[08/27/2025 17:01:57 INFO]: Training loss at epoch 2: 1.3373398184776306
[08/27/2025 17:01:59 INFO]: Training loss at epoch 56: 0.9639740288257599
[08/27/2025 17:02:08 INFO]: Training loss at epoch 11: 0.8882264494895935
[08/27/2025 17:02:10 INFO]: Training loss at epoch 10: 0.8984548151493073
[08/27/2025 17:02:28 INFO]: Training loss at epoch 35: 0.8488020896911621
[08/27/2025 17:03:05 INFO]: Training loss at epoch 14: 0.9877476692199707
[08/27/2025 17:03:05 INFO]: Training loss at epoch 41: 1.060997873544693
[08/27/2025 17:03:07 INFO]: Training loss at epoch 22: 1.090986043214798
[08/27/2025 17:03:18 INFO]: Training loss at epoch 20: 0.8395746052265167
[08/27/2025 17:03:27 INFO]: Training loss at epoch 57: 0.8418484032154083
[08/27/2025 17:03:28 INFO]: Training stats: {
    "score": -0.9663763409978567,
    "rmse": 0.9663763409978567
}
[08/27/2025 17:03:28 INFO]: Val stats: {
    "score": -0.6808743362449269,
    "rmse": 0.6808743362449269
}
[08/27/2025 17:03:28 INFO]: Test stats: {
    "score": -0.9118465251461529,
    "rmse": 0.9118465251461529
}
[08/27/2025 17:03:31 INFO]: New best epoch, val score: -0.658014910668982
[08/27/2025 17:03:31 INFO]: Saving model to: blotchy-Amado_trial_3/model_best.pth
[08/27/2025 17:03:42 INFO]: New best epoch, val score: -0.6594324508590762
[08/27/2025 17:03:42 INFO]: Saving model to: blotchy-Amado_trial_29/model_best.pth
[08/27/2025 17:03:43 INFO]: Training loss at epoch 8: 0.8645044267177582
[08/27/2025 17:03:47 INFO]: New best epoch, val score: -0.7038324787359718
[08/27/2025 17:03:47 INFO]: Saving model to: blotchy-Amado_trial_16/model_best.pth
[08/27/2025 17:04:01 INFO]: Training loss at epoch 12: 1.2664791345596313
[08/27/2025 17:04:14 INFO]: Training loss at epoch 43: 0.9192686975002289
[08/27/2025 17:04:19 INFO]: Training loss at epoch 7: 1.2723439931869507
[08/27/2025 17:04:20 INFO]: Training loss at epoch 13: 1.1286059319972992
[08/27/2025 17:04:35 INFO]: Training loss at epoch 32: 1.142126441001892
[08/27/2025 17:04:44 INFO]: Training loss at epoch 8: 1.106755554676056
[08/27/2025 17:05:01 INFO]: Training loss at epoch 58: 1.0806419551372528
[08/27/2025 17:05:04 INFO]: Training loss at epoch 36: 1.039549559354782
[08/27/2025 17:05:05 INFO]: Training loss at epoch 3: 0.9717998802661896
[08/27/2025 17:05:18 INFO]: Training loss at epoch 12: 0.8653610348701477
[08/27/2025 17:06:31 INFO]: Training loss at epoch 59: 0.989331066608429
[08/27/2025 17:06:33 INFO]: Training loss at epoch 10: 0.9786626398563385
[08/27/2025 17:06:40 INFO]: Training loss at epoch 42: 1.1311370730400085
[08/27/2025 17:06:57 INFO]: Training loss at epoch 25: 0.8797085285186768
[08/27/2025 17:07:03 INFO]: New best epoch, val score: -0.6572267770085529
[08/27/2025 17:07:03 INFO]: Saving model to: blotchy-Amado_trial_3/model_best.pth
[08/27/2025 17:07:03 INFO]: Training stats: {
    "score": -0.9980182907958413,
    "rmse": 0.9980182907958413
}
[08/27/2025 17:07:03 INFO]: Val stats: {
    "score": -0.6616591973787403,
    "rmse": 0.6616591973787403
}
[08/27/2025 17:07:03 INFO]: Test stats: {
    "score": -0.8718036905764645,
    "rmse": 0.8718036905764645
}
[08/27/2025 17:07:10 INFO]: Training loss at epoch 21: 0.882030725479126
[08/27/2025 17:07:31 INFO]: Training loss at epoch 37: 1.163564383983612
[08/27/2025 17:07:32 INFO]: Training loss at epoch 44: 0.9824813604354858
[08/27/2025 17:07:54 INFO]: Running Final Evaluation...
[08/27/2025 17:08:05 INFO]: Training loss at epoch 4: 0.8692159950733185
[08/27/2025 17:08:16 INFO]: Training loss at epoch 13: 0.8907762467861176
[08/27/2025 17:08:19 INFO]: Training loss at epoch 15: 1.0132806897163391
[08/27/2025 17:08:27 INFO]: Training loss at epoch 60: 1.3487316966056824
[08/27/2025 17:08:52 INFO]: Training loss at epoch 33: 0.9084263741970062
[08/27/2025 17:08:58 INFO]: Training accuracy: {
    "score": -1.0017321721056487,
    "rmse": 1.0017321721056487
}
[08/27/2025 17:08:58 INFO]: Val accuracy: {
    "score": -0.6496974889231703,
    "rmse": 0.6496974889231703
}
[08/27/2025 17:08:58 INFO]: Test accuracy: {
    "score": -0.8806101864362424,
    "rmse": 0.8806101864362424
}
[08/27/2025 17:08:58 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_18",
    "best_epoch": 13,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8806101864362424,
        "rmse": 0.8806101864362424
    },
    "train_stats": {
        "score": -1.0017321721056487,
        "rmse": 1.0017321721056487
    },
    "val_stats": {
        "score": -0.6496974889231703,
        "rmse": 0.6496974889231703
    }
}
[08/27/2025 17:08:58 INFO]: Procewss finished for trial blotchy-Amado_trial_18
[08/27/2025 17:08:58 INFO]: 
_________________________________________________

[08/27/2025 17:08:58 INFO]: train_net_for_optune.py main() running.
[08/27/2025 17:08:58 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.6513557419998754
  attention_dropout: 0.09366206861967344
  ffn_dropout: 0.09366206861967344
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.089568821513846e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_41

[08/27/2025 17:08:58 INFO]: This ft_transformer has 7.200 million parameters.
[08/27/2025 17:08:58 INFO]: Training will start at epoch 0.
[08/27/2025 17:08:58 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 17:09:00 INFO]: Training loss at epoch 10: 1.0579500794410706
[08/27/2025 17:09:21 INFO]: Training loss at epoch 23: 1.0364503264427185
[08/27/2025 17:09:46 INFO]: Training loss at epoch 14: 1.1246282160282135
[08/27/2025 17:09:51 INFO]: Training loss at epoch 38: 0.9237034916877747
[08/27/2025 17:09:51 INFO]: Training loss at epoch 61: 1.011299729347229
[08/27/2025 17:09:53 INFO]: Training loss at epoch 43: 0.771321177482605
[08/27/2025 17:10:09 INFO]: Training loss at epoch 9: 0.913085550069809
[08/27/2025 17:10:18 INFO]: New best epoch, val score: -0.6570391354600241
[08/27/2025 17:10:18 INFO]: Saving model to: blotchy-Amado_trial_3/model_best.pth
[08/27/2025 17:10:33 INFO]: Training loss at epoch 7: 1.1303351521492004
[08/27/2025 17:10:44 INFO]: Training loss at epoch 22: 1.2113198637962341
[08/27/2025 17:10:57 INFO]: Training loss at epoch 13: 0.9013679325580597
[08/27/2025 17:11:01 INFO]: Training loss at epoch 5: 0.9565710127353668
[08/27/2025 17:11:14 INFO]: Training loss at epoch 14: 0.8291811347007751
[08/27/2025 17:11:21 INFO]: Training loss at epoch 62: 0.9271901547908783
[08/27/2025 17:11:35 INFO]: New best epoch, val score: -0.6562910902986461
[08/27/2025 17:11:35 INFO]: Saving model to: blotchy-Amado_trial_39/model_best.pth
[08/27/2025 17:11:57 INFO]: New best epoch, val score: -0.661596013128384
[08/27/2025 17:11:57 INFO]: Saving model to: blotchy-Amado_trial_31/model_best.pth
[08/27/2025 17:12:06 INFO]: Training stats: {
    "score": -0.9864220012493846,
    "rmse": 0.9864220012493846
}
[08/27/2025 17:12:06 INFO]: Val stats: {
    "score": -0.6580379139827524,
    "rmse": 0.6580379139827524
}
[08/27/2025 17:12:06 INFO]: Test stats: {
    "score": -0.8912734855327528,
    "rmse": 0.8912734855327528
}
[08/27/2025 17:12:12 INFO]: Training loss at epoch 0: 1.157674789428711
[08/27/2025 17:12:21 INFO]: Training loss at epoch 11: 0.8543080389499664
[08/27/2025 17:12:22 INFO]: Training loss at epoch 39: 1.0056989192962646
[08/27/2025 17:12:30 INFO]: Training loss at epoch 26: 1.0807781219482422
[08/27/2025 17:12:37 INFO]: New best epoch, val score: -0.7043275079102632
[08/27/2025 17:12:37 INFO]: Saving model to: blotchy-Amado_trial_41/model_best.pth
[08/27/2025 17:12:43 INFO]: New best epoch, val score: -0.6580379139827524
[08/27/2025 17:12:43 INFO]: Saving model to: blotchy-Amado_trial_38/model_best.pth
[08/27/2025 17:12:55 INFO]: Training loss at epoch 63: 0.8810644745826721
[08/27/2025 17:12:58 INFO]: Training loss at epoch 9: 0.8856059014797211
[08/27/2025 17:13:06 INFO]: Training loss at epoch 8: 1.1314674615859985
[08/27/2025 17:13:13 INFO]: Training stats: {
    "score": -0.9993010046237903,
    "rmse": 0.9993010046237903
}
[08/27/2025 17:13:13 INFO]: Val stats: {
    "score": -0.6696473338055247,
    "rmse": 0.6696473338055247
}
[08/27/2025 17:13:13 INFO]: Test stats: {
    "score": -0.8697295797205207,
    "rmse": 0.8697295797205207
}
[08/27/2025 17:13:17 INFO]: Training loss at epoch 34: 0.8454672992229462
[08/27/2025 17:13:23 INFO]: Training loss at epoch 44: 1.1227805018424988
[08/27/2025 17:13:30 INFO]: Training loss at epoch 16: 0.9056572318077087
[08/27/2025 17:14:09 INFO]: Training loss at epoch 6: 1.1154756546020508
[08/27/2025 17:14:22 INFO]: Training loss at epoch 15: 1.1605279445648193
[08/27/2025 17:14:26 INFO]: Training loss at epoch 64: 0.974195808172226
[08/27/2025 17:14:31 INFO]: Training loss at epoch 23: 0.8680572211742401
[08/27/2025 17:14:34 INFO]: Training loss at epoch 11: 1.101882129907608
[08/27/2025 17:14:44 INFO]: New best epoch, val score: -0.6560653877249384
[08/27/2025 17:14:44 INFO]: Saving model to: blotchy-Amado_trial_39/model_best.pth
[08/27/2025 17:15:20 INFO]: Training loss at epoch 11: 1.0054623186588287
[08/27/2025 17:15:27 INFO]: Training loss at epoch 15: 1.2142675518989563
[08/27/2025 17:15:43 INFO]: Training loss at epoch 24: 0.9805191159248352
[08/27/2025 17:15:47 INFO]: Training loss at epoch 40: 1.0435957312583923
[08/27/2025 17:15:55 INFO]: Training loss at epoch 65: 0.8798805773258209
[08/27/2025 17:15:56 INFO]: Training loss at epoch 1: 1.0744237303733826
[08/27/2025 17:16:13 INFO]: Training stats: {
    "score": -1.0015795894511272,
    "rmse": 1.0015795894511272
}
[08/27/2025 17:16:13 INFO]: Val stats: {
    "score": -0.7036464439086532,
    "rmse": 0.7036464439086532
}
[08/27/2025 17:16:13 INFO]: Test stats: {
    "score": -0.886770568533496,
    "rmse": 0.886770568533496
}
[08/27/2025 17:16:55 INFO]: Training loss at epoch 45: 0.8212464451789856
[08/27/2025 17:16:57 INFO]: New best epoch, val score: -0.6809467188163403
[08/27/2025 17:16:57 INFO]: Saving model to: blotchy-Amado_trial_16/model_best.pth
[08/27/2025 17:17:18 INFO]: Training loss at epoch 7: 1.1003741025924683
[08/27/2025 17:17:25 INFO]: Training loss at epoch 66: 1.2641219794750214
[08/27/2025 17:17:30 INFO]: Training loss at epoch 16: 0.9418317377567291
[08/27/2025 17:17:43 INFO]: Training loss at epoch 10: 0.8030768632888794
[08/27/2025 17:17:48 INFO]: Training loss at epoch 35: 1.1612704396247864
[08/27/2025 17:18:08 INFO]: Training loss at epoch 14: 1.0200362801551819
[08/27/2025 17:18:16 INFO]: Training loss at epoch 41: 1.1138123273849487
[08/27/2025 17:18:17 INFO]: Training loss at epoch 27: 0.8872804939746857
[08/27/2025 17:18:17 INFO]: Training loss at epoch 24: 0.7876097857952118
[08/27/2025 17:18:25 INFO]: Training loss at epoch 12: 1.162549078464508
[08/27/2025 17:18:44 INFO]: Training loss at epoch 17: 0.8114841282367706
[08/27/2025 17:18:50 INFO]: Training loss at epoch 67: 1.1880492568016052
[08/27/2025 17:19:31 INFO]: Training loss at epoch 2: 1.2822161316871643
[08/27/2025 17:20:09 INFO]: Training loss at epoch 12: 1.4856715202331543
[08/27/2025 17:20:16 INFO]: Training loss at epoch 46: 0.9290288090705872
[08/27/2025 17:20:17 INFO]: Training loss at epoch 8: 1.0086892247200012
[08/27/2025 17:20:19 INFO]: Training loss at epoch 68: 1.115357667207718
[08/27/2025 17:20:29 INFO]: Training loss at epoch 17: 0.9221510887145996
[08/27/2025 17:20:43 INFO]: Training loss at epoch 42: 1.0944473147392273
[08/27/2025 17:21:00 INFO]: Training loss at epoch 16: 1.0531717240810394
[08/27/2025 17:21:48 INFO]: Training loss at epoch 69: 1.0943848192691803
[08/27/2025 17:21:56 INFO]: Training loss at epoch 8: 0.826354593038559
[08/27/2025 17:21:56 INFO]: Training loss at epoch 25: 1.0499056577682495
[08/27/2025 17:22:00 INFO]: Training loss at epoch 25: 1.0990219116210938
[08/27/2025 17:22:02 INFO]: Training loss at epoch 9: 1.1716736555099487
[08/27/2025 17:22:13 INFO]: Training loss at epoch 36: 0.8953283429145813
[08/27/2025 17:22:20 INFO]: Training stats: {
    "score": -0.9985754760086695,
    "rmse": 0.9985754760086695
}
[08/27/2025 17:22:20 INFO]: Val stats: {
    "score": -0.7009121705609289,
    "rmse": 0.7009121705609289
}
[08/27/2025 17:22:20 INFO]: Test stats: {
    "score": -0.8913584540514403,
    "rmse": 0.8913584540514403
}
[08/27/2025 17:23:09 INFO]: Training loss at epoch 11: 0.8569543957710266
[08/27/2025 17:23:11 INFO]: Training loss at epoch 3: 1.040903925895691
[08/27/2025 17:23:11 INFO]: Training loss at epoch 43: 0.8201625347137451
[08/27/2025 17:23:19 INFO]: Training loss at epoch 9: 0.8774599432945251
[08/27/2025 17:23:32 INFO]: Training loss at epoch 18: 1.3796917796134949
[08/27/2025 17:23:38 INFO]: New best epoch, val score: -0.6698499863412123
[08/27/2025 17:23:38 INFO]: Saving model to: blotchy-Amado_trial_41/model_best.pth
[08/27/2025 17:23:39 INFO]: Training loss at epoch 47: 0.8350491225719452
[08/27/2025 17:23:48 INFO]: Training loss at epoch 70: 0.9311788976192474
[08/27/2025 17:23:56 INFO]: Training loss at epoch 18: 0.9545072913169861
[08/27/2025 17:23:56 INFO]: Training loss at epoch 28: 1.079605221748352
[08/27/2025 17:24:22 INFO]: Training stats: {
    "score": -0.9963844112454685,
    "rmse": 0.9963844112454685
}
[08/27/2025 17:24:22 INFO]: Val stats: {
    "score": -0.6755660202245867,
    "rmse": 0.6755660202245867
}
[08/27/2025 17:24:22 INFO]: Test stats: {
    "score": -0.8736627276498153,
    "rmse": 0.8736627276498153
}
[08/27/2025 17:24:24 INFO]: Training loss at epoch 13: 0.9375406205654144
[08/27/2025 17:25:05 INFO]: Training stats: {
    "score": -1.051821774225108,
    "rmse": 1.051821774225108
}
[08/27/2025 17:25:05 INFO]: Val stats: {
    "score": -0.6802149742230333,
    "rmse": 0.6802149742230333
}
[08/27/2025 17:25:05 INFO]: Test stats: {
    "score": -0.8982001436893571,
    "rmse": 0.8982001436893571
}
[08/27/2025 17:25:12 INFO]: Training loss at epoch 15: 1.0153928995132446
[08/27/2025 17:25:22 INFO]: Training loss at epoch 71: 0.944320410490036
[08/27/2025 17:25:35 INFO]: Training loss at epoch 10: 0.9594780802726746
[08/27/2025 17:25:45 INFO]: Training loss at epoch 44: 0.9733123183250427
[08/27/2025 17:25:46 INFO]: Training loss at epoch 26: 0.8362753093242645
[08/27/2025 17:25:49 INFO]: Training loss at epoch 13: 1.3418893814086914
[08/27/2025 17:26:35 INFO]: Training loss at epoch 17: 1.0223871767520905
[08/27/2025 17:26:39 INFO]: Training loss at epoch 19: 0.8918593227863312
[08/27/2025 17:26:42 INFO]: Training loss at epoch 37: 0.8539935648441315
[08/27/2025 17:26:52 INFO]: Training loss at epoch 72: 1.166600912809372
[08/27/2025 17:26:54 INFO]: Training loss at epoch 4: 0.9587350785732269
[08/27/2025 17:27:09 INFO]: Training loss at epoch 48: 0.9266147613525391
[08/27/2025 17:27:13 INFO]: New best epoch, val score: -0.6673757356832771
[08/27/2025 17:27:13 INFO]: Saving model to: blotchy-Amado_trial_19/model_best.pth
[08/27/2025 17:27:27 INFO]: Training loss at epoch 10: 1.1349994540214539
[08/27/2025 17:27:39 INFO]: Training stats: {
    "score": -0.9954123597641147,
    "rmse": 0.9954123597641147
}
[08/27/2025 17:27:39 INFO]: Val stats: {
    "score": -0.6798965774310552,
    "rmse": 0.6798965774310552
}
[08/27/2025 17:27:39 INFO]: Test stats: {
    "score": -0.8746992270740335,
    "rmse": 0.8746992270740335
}
[08/27/2025 17:28:12 INFO]: Training loss at epoch 26: 0.903106302022934
[08/27/2025 17:28:16 INFO]: Training loss at epoch 45: 0.9681711196899414
[08/27/2025 17:28:18 INFO]: Training loss at epoch 73: 1.0988243222236633
[08/27/2025 17:28:34 INFO]: Training loss at epoch 12: 0.9477504193782806
[08/27/2025 17:28:43 INFO]: Training loss at epoch 12: 0.9557612240314484
[08/27/2025 17:29:10 INFO]: Training loss at epoch 19: 1.171507716178894
[08/27/2025 17:29:27 INFO]: Training loss at epoch 27: 0.9063593745231628
[08/27/2025 17:29:36 INFO]: Training loss at epoch 29: 0.9856094717979431
[08/27/2025 17:29:45 INFO]: Training loss at epoch 74: 1.0687480568885803
[08/27/2025 17:30:20 INFO]: Training loss at epoch 14: 1.007611095905304
[08/27/2025 17:30:24 INFO]: Training loss at epoch 11: 1.0574564933776855
[08/27/2025 17:30:24 INFO]: Training loss at epoch 5: 1.539475679397583
[08/27/2025 17:30:26 INFO]: Training loss at epoch 49: 1.2677077054977417
[08/27/2025 17:30:36 INFO]: Training loss at epoch 20: 1.146628737449646
[08/27/2025 17:30:39 INFO]: Training loss at epoch 46: 1.143865704536438
[08/27/2025 17:30:48 INFO]: Training stats: {
    "score": -0.9627454976066385,
    "rmse": 0.9627454976066385
}
[08/27/2025 17:30:48 INFO]: Val stats: {
    "score": -0.7227138834649041,
    "rmse": 0.7227138834649041
}
[08/27/2025 17:30:48 INFO]: Test stats: {
    "score": -0.9169485134801599,
    "rmse": 0.9169485134801599
}
[08/27/2025 17:30:56 INFO]: Training loss at epoch 38: 1.1616517901420593
[08/27/2025 17:31:09 INFO]: Training loss at epoch 75: 1.0987096428871155
[08/27/2025 17:31:12 INFO]: Training loss at epoch 14: 0.90462526679039
[08/27/2025 17:31:30 INFO]: Training stats: {
    "score": -1.012136495902862,
    "rmse": 1.012136495902862
}
[08/27/2025 17:31:30 INFO]: Val stats: {
    "score": -0.7391857769035731,
    "rmse": 0.7391857769035731
}
[08/27/2025 17:31:30 INFO]: Test stats: {
    "score": -0.908267893712541,
    "rmse": 0.908267893712541
}
[08/27/2025 17:31:31 INFO]: Training stats: {
    "score": -0.9994586565122229,
    "rmse": 0.9994586565122229
}
[08/27/2025 17:31:31 INFO]: Val stats: {
    "score": -0.6589800910952057,
    "rmse": 0.6589800910952057
}
[08/27/2025 17:31:31 INFO]: Test stats: {
    "score": -0.8731177768895251,
    "rmse": 0.8731177768895251
}
[08/27/2025 17:31:57 INFO]: Training loss at epoch 18: 1.1954355239868164
[08/27/2025 17:32:03 INFO]: Training loss at epoch 16: 1.096778392791748
[08/27/2025 17:32:34 INFO]: Training loss at epoch 76: 1.0215880572795868
[08/27/2025 17:32:35 INFO]: New best epoch, val score: -0.6645648868220897
[08/27/2025 17:32:35 INFO]: Saving model to: blotchy-Amado_trial_34/model_best.pth
[08/27/2025 17:32:52 INFO]: Training loss at epoch 9: 0.84328293800354
[08/27/2025 17:32:57 INFO]: Training loss at epoch 28: 0.9276851415634155
[08/27/2025 17:33:00 INFO]: Training loss at epoch 47: 1.1398033499717712
[08/27/2025 17:33:16 INFO]: Training loss at epoch 12: 0.9838475584983826
[08/27/2025 17:33:28 INFO]: Training loss at epoch 21: 1.197502762079239
[08/27/2025 17:33:39 INFO]: Training loss at epoch 10: 1.0420158505439758
[08/27/2025 17:33:49 INFO]: Training loss at epoch 6: 1.008811503648758
[08/27/2025 17:33:54 INFO]: Training loss at epoch 13: 1.030584454536438
[08/27/2025 17:34:00 INFO]: Training loss at epoch 77: 0.9342620968818665
[08/27/2025 17:34:10 INFO]: Training loss at epoch 27: 0.9108757078647614
[08/27/2025 17:34:38 INFO]: Training loss at epoch 11: 0.9913903474807739
[08/27/2025 17:34:44 INFO]: Training loss at epoch 50: 0.953209400177002
[08/27/2025 17:35:07 INFO]: Training loss at epoch 39: 1.0946218371391296
[08/27/2025 17:35:24 INFO]: Training loss at epoch 48: 0.8755099177360535
[08/27/2025 17:35:27 INFO]: Training loss at epoch 78: 0.9910806715488434
[08/27/2025 17:35:45 INFO]: Training loss at epoch 20: 0.9547916054725647
[08/27/2025 17:36:03 INFO]: Training loss at epoch 15: 0.9396193325519562
[08/27/2025 17:36:15 INFO]: Training loss at epoch 13: 0.9886142909526825
[08/27/2025 17:36:27 INFO]: Training loss at epoch 22: 0.7921264469623566
[08/27/2025 17:36:33 INFO]: Training loss at epoch 15: 0.921978771686554
[08/27/2025 17:36:35 INFO]: Training stats: {
    "score": -1.000771317032211,
    "rmse": 1.000771317032211
}
[08/27/2025 17:36:35 INFO]: Val stats: {
    "score": -0.6642101519101625,
    "rmse": 0.6642101519101625
}
[08/27/2025 17:36:35 INFO]: Test stats: {
    "score": -0.8689031514844684,
    "rmse": 0.8689031514844684
}
[08/27/2025 17:36:37 INFO]: Training loss at epoch 29: 0.8550913631916046
[08/27/2025 17:36:38 INFO]: New best epoch, val score: -0.6660266728375226
[08/27/2025 17:36:38 INFO]: Saving model to: blotchy-Amado_trial_40/model_best.pth
[08/27/2025 17:36:40 INFO]: Training stats: {
    "score": -1.0031948157835435,
    "rmse": 1.0031948157835435
}
[08/27/2025 17:36:40 INFO]: Val stats: {
    "score": -0.6672145029495326,
    "rmse": 0.6672145029495326
}
[08/27/2025 17:36:40 INFO]: Test stats: {
    "score": -0.8770149319171816,
    "rmse": 0.8770149319171816
}
[08/27/2025 17:36:57 INFO]: Training loss at epoch 79: 1.2831540405750275
[08/27/2025 17:37:03 INFO]: Training loss at epoch 30: 0.9677633345127106
[08/27/2025 17:37:13 INFO]: New best epoch, val score: -0.6672145029495326
[08/27/2025 17:37:13 INFO]: Saving model to: blotchy-Amado_trial_19/model_best.pth
[08/27/2025 17:37:26 INFO]: Training loss at epoch 19: 0.9425911009311676
[08/27/2025 17:37:29 INFO]: Training loss at epoch 7: 1.1535245180130005
[08/27/2025 17:37:32 INFO]: Training stats: {
    "score": -0.9953291826603275,
    "rmse": 0.9953291826603275
}
[08/27/2025 17:37:32 INFO]: Val stats: {
    "score": -0.6638060255263267,
    "rmse": 0.6638060255263267
}
[08/27/2025 17:37:32 INFO]: Test stats: {
    "score": -0.8728772987353571,
    "rmse": 0.8728772987353571
}
[08/27/2025 17:37:57 INFO]: Training stats: {
    "score": -0.9210657149762536,
    "rmse": 0.9210657149762536
}
[08/27/2025 17:37:57 INFO]: Val stats: {
    "score": -0.7422403957200975,
    "rmse": 0.7422403957200975
}
[08/27/2025 17:37:57 INFO]: Test stats: {
    "score": -0.9503112842251545,
    "rmse": 0.9503112842251545
}
[08/27/2025 17:37:57 INFO]: Training loss at epoch 49: 1.0117760300636292
[08/27/2025 17:38:11 INFO]: Training loss at epoch 51: 0.8697794377803802
[08/27/2025 17:38:51 INFO]: Training stats: {
    "score": -1.0004749260899388,
    "rmse": 1.0004749260899388
}
[08/27/2025 17:38:51 INFO]: Val stats: {
    "score": -0.7059807654118673,
    "rmse": 0.7059807654118673
}
[08/27/2025 17:38:51 INFO]: Test stats: {
    "score": -0.8874418411751928,
    "rmse": 0.8874418411751928
}
[08/27/2025 17:39:03 INFO]: Training loss at epoch 80: 1.154618740081787
[08/27/2025 17:39:04 INFO]: Training loss at epoch 17: 0.9781648516654968
[08/27/2025 17:39:09 INFO]: Running Final Evaluation...
[08/27/2025 17:39:26 INFO]: Training loss at epoch 14: 0.9656384885311127
[08/27/2025 17:39:28 INFO]: Training stats: {
    "score": -1.0186609964188331,
    "rmse": 1.0186609964188331
}
[08/27/2025 17:39:28 INFO]: Val stats: {
    "score": -0.6608854465526195,
    "rmse": 0.6608854465526195
}
[08/27/2025 17:39:28 INFO]: Test stats: {
    "score": -0.8734788387778772,
    "rmse": 0.8734788387778772
}
[08/27/2025 17:39:30 INFO]: Training loss at epoch 14: 0.8828964233398438
[08/27/2025 17:39:37 INFO]: Training loss at epoch 23: 1.1040140390396118
[08/27/2025 17:39:48 INFO]: New best epoch, val score: -0.6652807269481702
[08/27/2025 17:39:48 INFO]: Saving model to: blotchy-Amado_trial_40/model_best.pth
[08/27/2025 17:40:08 INFO]: Training accuracy: {
    "score": -1.0009145787976788,
    "rmse": 1.0009145787976788
}
[08/27/2025 17:40:08 INFO]: Val accuracy: {
    "score": -0.6680291916005244,
    "rmse": 0.6680291916005244
}
[08/27/2025 17:40:08 INFO]: Test accuracy: {
    "score": -0.8687903583438763,
    "rmse": 0.8687903583438763
}
[08/27/2025 17:40:08 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_28",
    "best_epoch": 18,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8687903583438763,
        "rmse": 0.8687903583438763
    },
    "train_stats": {
        "score": -1.0009145787976788,
        "rmse": 1.0009145787976788
    },
    "val_stats": {
        "score": -0.6680291916005244,
        "rmse": 0.6680291916005244
    }
}
[08/27/2025 17:40:08 INFO]: Procewss finished for trial blotchy-Amado_trial_28
[08/27/2025 17:40:08 INFO]: 
_________________________________________________

[08/27/2025 17:40:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 17:40:08 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.529160865425231
  attention_dropout: 0.09248302146808265
  ffn_dropout: 0.09248302146808265
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.4796081141219175e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_42

[08/27/2025 17:40:08 INFO]: This ft_transformer has 6.983 million parameters.
[08/27/2025 17:40:08 INFO]: Training will start at epoch 0.
[08/27/2025 17:40:08 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 17:40:09 INFO]: New best epoch, val score: -0.6608854465526195
[08/27/2025 17:40:09 INFO]: Saving model to: blotchy-Amado_trial_34/model_best.pth
[08/27/2025 17:40:29 INFO]: Training loss at epoch 28: 0.9249807596206665
[08/27/2025 17:40:33 INFO]: Training loss at epoch 81: 0.9989767968654633
[08/27/2025 17:41:05 INFO]: Training loss at epoch 21: 0.9436524510383606
[08/27/2025 17:41:10 INFO]: Training loss at epoch 40: 1.0632134675979614
[08/27/2025 17:41:11 INFO]: Training loss at epoch 8: 0.9803364276885986
[08/27/2025 17:41:31 INFO]: Training loss at epoch 13: 1.2929559648036957
[08/27/2025 17:41:39 INFO]: Training loss at epoch 52: 1.1270689070224762
[08/27/2025 17:41:48 INFO]: Training loss at epoch 30: 1.0773583054542542
[08/27/2025 17:42:04 INFO]: Training loss at epoch 82: 1.0939635932445526
[08/27/2025 17:42:13 INFO]: Training loss at epoch 16: 1.2226776778697968
[08/27/2025 17:42:20 INFO]: Training loss at epoch 16: 0.7957817912101746
[08/27/2025 17:42:34 INFO]: Training loss at epoch 15: 1.0330660045146942
[08/27/2025 17:42:40 INFO]: Training loss at epoch 11: 1.1078837215900421
[08/27/2025 17:42:46 INFO]: Training loss at epoch 24: 0.7718826681375504
[08/27/2025 17:42:54 INFO]: Training loss at epoch 31: 1.0152555704116821
[08/27/2025 17:42:58 INFO]: New best epoch, val score: -0.6605598774497623
[08/27/2025 17:42:58 INFO]: Saving model to: blotchy-Amado_trial_36/model_best.pth
[08/27/2025 17:43:25 INFO]: Training loss at epoch 0: 1.2409694194793701
[08/27/2025 17:43:37 INFO]: Training loss at epoch 83: 1.0945713520050049
[08/27/2025 17:43:52 INFO]: New best epoch, val score: -0.8438346399679546
[08/27/2025 17:43:52 INFO]: Saving model to: blotchy-Amado_trial_42/model_best.pth
[08/27/2025 17:44:15 INFO]: Training loss at epoch 12: 0.7704394310712814
[08/27/2025 17:45:00 INFO]: Training loss at epoch 9: 1.371416687965393
[08/27/2025 17:45:09 INFO]: Training loss at epoch 84: 0.8759650588035583
[08/27/2025 17:45:10 INFO]: Training loss at epoch 53: 0.9292407333850861
[08/27/2025 17:45:11 INFO]: Training loss at epoch 15: 1.147533893585205
[08/27/2025 17:45:13 INFO]: Training loss at epoch 20: 1.114746630191803
[08/27/2025 17:45:39 INFO]: Training loss at epoch 16: 0.9787866771221161
[08/27/2025 17:45:41 INFO]: Training loss at epoch 41: 0.9157598912715912
[08/27/2025 17:45:42 INFO]: Training loss at epoch 31: 0.8405948579311371
[08/27/2025 17:45:48 INFO]: New best epoch, val score: -0.6563285032907127
[08/27/2025 17:45:48 INFO]: Saving model to: blotchy-Amado_trial_38/model_best.pth
[08/27/2025 17:45:52 INFO]: Training loss at epoch 25: 0.9150209724903107
[08/27/2025 17:46:08 INFO]: Running Final Evaluation...
[08/27/2025 17:46:11 INFO]: Training stats: {
    "score": -1.0059502510791027,
    "rmse": 1.0059502510791027
}
[08/27/2025 17:46:11 INFO]: Val stats: {
    "score": -0.6686616448032581,
    "rmse": 0.6686616448032581
}
[08/27/2025 17:46:11 INFO]: Test stats: {
    "score": -0.8872245820082296,
    "rmse": 0.8872245820082296
}
[08/27/2025 17:46:14 INFO]: Training loss at epoch 18: 1.0758709907531738
[08/27/2025 17:46:22 INFO]: Training loss at epoch 22: 0.7844662666320801
[08/27/2025 17:46:34 INFO]: Training loss at epoch 85: 1.0913881659507751
[08/27/2025 17:46:35 INFO]: New best epoch, val score: -0.6686616448032581
[08/27/2025 17:46:35 INFO]: Saving model to: blotchy-Amado_trial_41/model_best.pth
[08/27/2025 17:46:48 INFO]: Training loss at epoch 29: 1.3278440833091736
[08/27/2025 17:46:58 INFO]: Training loss at epoch 1: 1.2748805284500122
[08/27/2025 17:47:22 INFO]: New best epoch, val score: -0.7128736863109945
[08/27/2025 17:47:22 INFO]: Saving model to: blotchy-Amado_trial_42/model_best.pth
[08/27/2025 17:47:23 INFO]: Training accuracy: {
    "score": -1.011521153905084,
    "rmse": 1.011521153905084
}
[08/27/2025 17:47:23 INFO]: Val accuracy: {
    "score": -0.6596721879223724,
    "rmse": 0.6596721879223724
}
[08/27/2025 17:47:23 INFO]: Test accuracy: {
    "score": -0.8948194545258044,
    "rmse": 0.8948194545258044
}
[08/27/2025 17:47:23 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_32",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8948194545258044,
        "rmse": 0.8948194545258044
    },
    "train_stats": {
        "score": -1.011521153905084,
        "rmse": 1.011521153905084
    },
    "val_stats": {
        "score": -0.6596721879223724,
        "rmse": 0.6596721879223724
    }
}
[08/27/2025 17:47:23 INFO]: Procewss finished for trial blotchy-Amado_trial_32
[08/27/2025 17:47:23 INFO]: 
_________________________________________________

[08/27/2025 17:47:23 INFO]: train_net_for_optune.py main() running.
[08/27/2025 17:47:23 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 1.7495371398422916
  attention_dropout: 0.0904558256404888
  ffn_dropout: 0.0904558256404888
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.4862581066802685e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_43

[08/27/2025 17:47:23 INFO]: This ft_transformer has 5.598 million parameters.
[08/27/2025 17:47:23 INFO]: Training will start at epoch 0.
[08/27/2025 17:47:23 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 17:47:49 INFO]: Training loss at epoch 17: 0.9999129176139832
[08/27/2025 17:47:53 INFO]: Training loss at epoch 10: 0.927362710237503
[08/27/2025 17:47:59 INFO]: Training loss at epoch 86: 0.9297723770141602
[08/27/2025 17:48:08 INFO]: Training loss at epoch 17: 1.022278606891632
[08/27/2025 17:48:23 INFO]: Training loss at epoch 54: 0.9751092195510864
[08/27/2025 17:48:32 INFO]: Training loss at epoch 32: 0.9430951178073883
[08/27/2025 17:48:35 INFO]: Training loss at epoch 17: 0.7973460257053375
[08/27/2025 17:48:48 INFO]: Training loss at epoch 26: 0.9273431599140167
[08/27/2025 17:48:57 INFO]: Training stats: {
    "score": -0.9668008201174473,
    "rmse": 0.9668008201174473
}
[08/27/2025 17:48:57 INFO]: Val stats: {
    "score": -0.7163282977288908,
    "rmse": 0.7163282977288908
}
[08/27/2025 17:48:57 INFO]: Test stats: {
    "score": -0.8990511621169885,
    "rmse": 0.8990511621169885
}
[08/27/2025 17:49:16 INFO]: Running Final Evaluation...
[08/27/2025 17:49:29 INFO]: Training loss at epoch 87: 1.00983065366745
[08/27/2025 17:49:44 INFO]: Training loss at epoch 10: 0.9832736551761627
[08/27/2025 17:49:58 INFO]: Training loss at epoch 42: 1.0021491348743439
[08/27/2025 17:50:07 INFO]: Training loss at epoch 0: 0.8983101546764374
[08/27/2025 17:50:30 INFO]: New best epoch, val score: -0.6730461472511389
[08/27/2025 17:50:30 INFO]: Saving model to: blotchy-Amado_trial_43/model_best.pth
[08/27/2025 17:50:31 INFO]: Training loss at epoch 2: 1.3120273351669312
[08/27/2025 17:50:32 INFO]: Training loss at epoch 16: 0.837570309638977
[08/27/2025 17:50:39 INFO]: Training loss at epoch 21: 0.9652419686317444
[08/27/2025 17:50:56 INFO]: Training loss at epoch 88: 0.8704920411109924
[08/27/2025 17:50:56 INFO]: New best epoch, val score: -0.6639008151246611
[08/27/2025 17:50:56 INFO]: Saving model to: blotchy-Amado_trial_42/model_best.pth
[08/27/2025 17:51:07 INFO]: Running Final Evaluation...
[08/27/2025 17:51:09 INFO]: New best epoch, val score: -0.6558027785632631
[08/27/2025 17:51:09 INFO]: Saving model to: blotchy-Amado_trial_38/model_best.pth
[08/27/2025 17:51:24 INFO]: Training loss at epoch 12: 1.2982651591300964
[08/27/2025 17:51:27 INFO]: Training loss at epoch 23: 0.8213948309421539
[08/27/2025 17:51:34 INFO]: Training accuracy: {
    "score": -1.017857105467924,
    "rmse": 1.017857105467924
}
[08/27/2025 17:51:34 INFO]: Val accuracy: {
    "score": -0.6564286867374425,
    "rmse": 0.6564286867374425
}
[08/27/2025 17:51:34 INFO]: Test accuracy: {
    "score": -0.8690546276082761,
    "rmse": 0.8690546276082761
}
[08/27/2025 17:51:34 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_2",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8690546276082761,
        "rmse": 0.8690546276082761
    },
    "train_stats": {
        "score": -1.017857105467924,
        "rmse": 1.017857105467924
    },
    "val_stats": {
        "score": -0.6564286867374425,
        "rmse": 0.6564286867374425
    }
}
[08/27/2025 17:51:34 INFO]: Procewss finished for trial blotchy-Amado_trial_2
[08/27/2025 17:51:34 INFO]: 
_________________________________________________

[08/27/2025 17:51:34 INFO]: train_net_for_optune.py main() running.
[08/27/2025 17:51:34 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 16
  d_ffn_factor: 2.5709825421042534
  attention_dropout: 0.40760020487689314
  ffn_dropout: 0.40760020487689314
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.726672561122996e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_44

[08/27/2025 17:51:35 INFO]: This ft_transformer has 13.986 million parameters.
[08/27/2025 17:51:35 INFO]: Training will start at epoch 0.
[08/27/2025 17:51:35 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 17:51:37 INFO]: Training loss at epoch 18: 0.9756691753864288
[08/27/2025 17:51:42 INFO]: Training accuracy: {
    "score": -0.9996321054116485,
    "rmse": 0.9996321054116485
}
[08/27/2025 17:51:42 INFO]: Val accuracy: {
    "score": -0.6594324508590762,
    "rmse": 0.6594324508590762
}
[08/27/2025 17:51:42 INFO]: Test accuracy: {
    "score": -0.8712443079188018,
    "rmse": 0.8712443079188018
}
[08/27/2025 17:51:42 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_29",
    "best_epoch": 57,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8712443079188018,
        "rmse": 0.8712443079188018
    },
    "train_stats": {
        "score": -0.9996321054116485,
        "rmse": 0.9996321054116485
    },
    "val_stats": {
        "score": -0.6594324508590762,
        "rmse": 0.6594324508590762
    }
}
[08/27/2025 17:51:42 INFO]: Procewss finished for trial blotchy-Amado_trial_29
[08/27/2025 17:51:43 INFO]: 
_________________________________________________

[08/27/2025 17:51:43 INFO]: train_net_for_optune.py main() running.
[08/27/2025 17:51:43 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 16
  d_ffn_factor: 1.7633949490227054
  attention_dropout: 0.39431341563533384
  ffn_dropout: 0.39431341563533384
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.011044067891117e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_45

[08/27/2025 17:51:43 INFO]: This ft_transformer has 11.124 million parameters.
[08/27/2025 17:51:43 INFO]: Training will start at epoch 0.
[08/27/2025 17:51:43 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 17:51:46 INFO]: Training loss at epoch 55: 1.127967655658722
[08/27/2025 17:51:50 INFO]: Training loss at epoch 27: 0.8609427213668823
[08/27/2025 17:53:15 INFO]: Training loss at epoch 19: 0.8995300829410553
[08/27/2025 17:53:16 INFO]: Training loss at epoch 1: 0.9876388311386108
[08/27/2025 17:53:23 INFO]: Training loss at epoch 11: 0.9669331610202789
[08/27/2025 17:53:26 INFO]: Training loss at epoch 18: 0.8254392147064209
[08/27/2025 17:53:31 INFO]: Training loss at epoch 13: 0.961140364408493
[08/27/2025 17:54:07 INFO]: New best epoch, val score: -0.6709596164392096
[08/27/2025 17:54:07 INFO]: Saving model to: blotchy-Amado_trial_37/model_best.pth
[08/27/2025 17:54:11 INFO]: Training loss at epoch 3: 1.0223812460899353
[08/27/2025 17:54:13 INFO]: Training loss at epoch 18: 1.10357666015625
[08/27/2025 17:54:27 INFO]: Training loss at epoch 43: 1.1128942370414734
[08/27/2025 17:54:46 INFO]: Training loss at epoch 14: 0.9578551650047302
[08/27/2025 17:54:46 INFO]: Training loss at epoch 19: 1.0347968935966492
[08/27/2025 17:55:00 INFO]: Training loss at epoch 28: 1.2838059663772583
[08/27/2025 17:55:15 INFO]: Training loss at epoch 30: 0.9361831247806549
[08/27/2025 17:55:16 INFO]: Training loss at epoch 56: 1.0815496444702148
[08/27/2025 17:55:40 INFO]: Training stats: {
    "score": -0.9983167319643586,
    "rmse": 0.9983167319643586
}
[08/27/2025 17:55:40 INFO]: Val stats: {
    "score": -0.716325892614122,
    "rmse": 0.716325892614122
}
[08/27/2025 17:55:40 INFO]: Test stats: {
    "score": -0.8935431785808876,
    "rmse": 0.8935431785808876
}
[08/27/2025 17:55:46 INFO]: Training stats: {
    "score": -0.9955603433912554,
    "rmse": 0.9955603433912554
}
[08/27/2025 17:55:46 INFO]: Val stats: {
    "score": -0.7005540140986548,
    "rmse": 0.7005540140986548
}
[08/27/2025 17:55:46 INFO]: Test stats: {
    "score": -0.8851601373633605,
    "rmse": 0.8851601373633605
}
[08/27/2025 17:56:04 INFO]: Training loss at epoch 17: 0.9316953718662262
[08/27/2025 17:56:14 INFO]: Training loss at epoch 22: 1.1035043597221375
[08/27/2025 17:56:22 INFO]: Training loss at epoch 2: 1.0708776414394379
[08/27/2025 17:56:40 INFO]: Training loss at epoch 24: 0.8659396469593048
[08/27/2025 17:56:40 INFO]: New best epoch, val score: -0.6540446901450585
[08/27/2025 17:56:40 INFO]: Saving model to: blotchy-Amado_trial_38/model_best.pth
[08/27/2025 17:56:58 INFO]: Training loss at epoch 12: 0.9401161074638367
[08/27/2025 17:57:40 INFO]: Training loss at epoch 4: 1.0839321613311768
[08/27/2025 17:57:52 INFO]: Training loss at epoch 0: 1.3220014572143555
[08/27/2025 17:57:54 INFO]: Training loss at epoch 29: 0.9354905486106873
[08/27/2025 17:58:29 INFO]: Training loss at epoch 57: 0.7770402729511261
[08/27/2025 17:58:40 INFO]: New best epoch, val score: -0.6651791882438978
[08/27/2025 17:58:40 INFO]: Saving model to: blotchy-Amado_trial_45/model_best.pth
[08/27/2025 17:58:40 INFO]: Training loss at epoch 20: 0.9919503927230835
[08/27/2025 17:58:40 INFO]: Training loss at epoch 44: 0.8600322902202606
[08/27/2025 17:58:43 INFO]: Training loss at epoch 0: 0.8984085321426392
[08/27/2025 17:58:51 INFO]: Training loss at epoch 19: 1.149572730064392
[08/27/2025 17:58:52 INFO]: Training stats: {
    "score": -0.997337605066687,
    "rmse": 0.997337605066687
}
[08/27/2025 17:58:52 INFO]: Val stats: {
    "score": -0.6604405788176364,
    "rmse": 0.6604405788176364
}
[08/27/2025 17:58:52 INFO]: Test stats: {
    "score": -0.8683789059317547,
    "rmse": 0.8683789059317547
}
[08/27/2025 17:58:57 INFO]: Training loss at epoch 11: 1.1814042329788208
[08/27/2025 17:59:18 INFO]: Training loss at epoch 3: 0.9142352044582367
[08/27/2025 17:59:37 INFO]: New best epoch, val score: -0.709220372046984
[08/27/2025 17:59:37 INFO]: Saving model to: blotchy-Amado_trial_44/model_best.pth
[08/27/2025 18:00:02 INFO]: Training loss at epoch 19: 0.9931494295597076
[08/27/2025 18:00:15 INFO]: Training loss at epoch 13: 1.4229304790496826
[08/27/2025 18:00:30 INFO]: Training loss at epoch 13: 1.0491864085197449
[08/27/2025 18:00:37 INFO]: Training stats: {
    "score": -0.9284453886555867,
    "rmse": 0.9284453886555867
}
[08/27/2025 18:00:37 INFO]: Val stats: {
    "score": -0.7064443083902973,
    "rmse": 0.7064443083902973
}
[08/27/2025 18:00:37 INFO]: Test stats: {
    "score": -0.9183750084884614,
    "rmse": 0.9183750084884614
}
[08/27/2025 18:01:09 INFO]: Training loss at epoch 5: 0.9737961292266846
[08/27/2025 18:01:17 INFO]: Training loss at epoch 18: 0.9329474866390228
[08/27/2025 18:01:19 INFO]: Training loss at epoch 31: 1.0496609508991241
[08/27/2025 18:01:36 INFO]: Training loss at epoch 25: 0.8075492084026337
[08/27/2025 18:01:38 INFO]: Training loss at epoch 23: 1.3454551994800568
[08/27/2025 18:01:40 INFO]: Training loss at epoch 21: 0.8230713903903961
[08/27/2025 18:01:50 INFO]: Training loss at epoch 58: 0.98504239320755
[08/27/2025 18:01:51 INFO]: Training loss at epoch 30: 1.1436084508895874
[08/27/2025 18:01:56 INFO]: New best epoch, val score: -0.6535679620634941
[08/27/2025 18:01:56 INFO]: Saving model to: blotchy-Amado_trial_38/model_best.pth
[08/27/2025 18:02:01 INFO]: Training stats: {
    "score": -1.0209412540030935,
    "rmse": 1.0209412540030935
}
[08/27/2025 18:02:01 INFO]: Val stats: {
    "score": -0.6619541090894429,
    "rmse": 0.6619541090894429
}
[08/27/2025 18:02:01 INFO]: Test stats: {
    "score": -0.8755120070930598,
    "rmse": 0.8755120070930598
}
[08/27/2025 18:02:04 INFO]: Running Final Evaluation...
[08/27/2025 18:02:25 INFO]: Training loss at epoch 4: 1.1302652955055237
[08/27/2025 18:02:33 INFO]: Training loss at epoch 20: 1.3824078440666199
[08/27/2025 18:02:41 INFO]: Training loss at epoch 14: 1.0225921869277954
[08/27/2025 18:03:01 INFO]: Training loss at epoch 45: 1.1592592000961304
[08/27/2025 18:03:51 INFO]: New best epoch, val score: -0.6937679527335298
[08/27/2025 18:03:51 INFO]: Saving model to: blotchy-Amado_trial_30/model_best.pth
[08/27/2025 18:04:12 INFO]: Training loss at epoch 14: 0.8649981915950775
[08/27/2025 18:04:27 INFO]: Training accuracy: {
    "score": -1.0216122134187582,
    "rmse": 1.0216122134187582
}
[08/27/2025 18:04:27 INFO]: Val accuracy: {
    "score": -0.6619580601065796,
    "rmse": 0.6619580601065796
}
[08/27/2025 18:04:27 INFO]: Test accuracy: {
    "score": -0.8952599513898578,
    "rmse": 0.8952599513898578
}
[08/27/2025 18:04:27 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_15",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8952599513898578,
        "rmse": 0.8952599513898578
    },
    "train_stats": {
        "score": -1.0216122134187582,
        "rmse": 1.0216122134187582
    },
    "val_stats": {
        "score": -0.6619580601065796,
        "rmse": 0.6619580601065796
    }
}
[08/27/2025 18:04:27 INFO]: Procewss finished for trial blotchy-Amado_trial_15
[08/27/2025 18:04:27 INFO]: 
_________________________________________________

[08/27/2025 18:04:27 INFO]: train_net_for_optune.py main() running.
[08/27/2025 18:04:27 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 16
  d_ffn_factor: 2.452163311664963
  attention_dropout: 0.41217126062271847
  ffn_dropout: 0.41217126062271847
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.4996217986298305e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_46

[08/27/2025 18:04:27 INFO]: This ft_transformer has 13.561 million parameters.
[08/27/2025 18:04:27 INFO]: Training will start at epoch 0.
[08/27/2025 18:04:27 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 18:04:49 INFO]: Training loss at epoch 22: 1.1062199771404266
[08/27/2025 18:04:51 INFO]: Training loss at epoch 6: 0.9714864194393158
[08/27/2025 18:04:52 INFO]: Training loss at epoch 1: 1.1520909070968628
[08/27/2025 18:05:00 INFO]: Training loss at epoch 31: 1.0173240303993225
[08/27/2025 18:05:21 INFO]: Training loss at epoch 59: 0.8637318909168243
[08/27/2025 18:05:38 INFO]: Training loss at epoch 5: 0.8991698622703552
[08/27/2025 18:06:16 INFO]: Training loss at epoch 20: 0.8048388361930847
[08/27/2025 18:06:31 INFO]: Training stats: {
    "score": -1.0223347267349447,
    "rmse": 1.0223347267349447
}
[08/27/2025 18:06:31 INFO]: Val stats: {
    "score": -0.6628712114276799,
    "rmse": 0.6628712114276799
}
[08/27/2025 18:06:31 INFO]: Test stats: {
    "score": -0.886585367021287,
    "rmse": 0.886585367021287
}
[08/27/2025 18:06:52 INFO]: Training loss at epoch 26: 1.0540510416030884
[08/27/2025 18:06:53 INFO]: Training loss at epoch 19: 1.2205474376678467
[08/27/2025 18:06:57 INFO]: Training loss at epoch 1: 0.9680034816265106
[08/27/2025 18:07:16 INFO]: Training loss at epoch 24: 1.1163291931152344
[08/27/2025 18:07:27 INFO]: Training loss at epoch 46: 0.9581822156906128
[08/27/2025 18:07:53 INFO]: Training loss at epoch 23: 1.0130342841148376
[08/27/2025 18:07:53 INFO]: Training loss at epoch 15: 0.9820461571216583
[08/27/2025 18:07:53 INFO]: Training loss at epoch 15: 1.0574150681495667
[08/27/2025 18:08:04 INFO]: Training loss at epoch 32: 1.0942340195178986
[08/27/2025 18:08:07 INFO]: Training loss at epoch 20: 1.0163263082504272
[08/27/2025 18:08:28 INFO]: Training loss at epoch 7: 1.3260970413684845
[08/27/2025 18:08:45 INFO]: Training loss at epoch 6: 0.8540688455104828
[08/27/2025 18:08:49 INFO]: Training stats: {
    "score": -0.9537142814631174,
    "rmse": 0.9537142814631174
}
[08/27/2025 18:08:49 INFO]: Val stats: {
    "score": -0.6541317958846544,
    "rmse": 0.6541317958846544
}
[08/27/2025 18:08:49 INFO]: Test stats: {
    "score": -0.8864034915170907,
    "rmse": 0.8864034915170907
}
[08/27/2025 18:09:20 INFO]: Training loss at epoch 14: 1.1170251965522766
[08/27/2025 18:09:46 INFO]: Training loss at epoch 21: 1.0013417601585388
[08/27/2025 18:10:02 INFO]: Training loss at epoch 60: 1.1191962659358978
[08/27/2025 18:10:17 INFO]: Training loss at epoch 12: 1.194716989994049
[08/27/2025 18:10:57 INFO]: Training loss at epoch 24: 1.1347402930259705
[08/27/2025 18:11:07 INFO]: Training loss at epoch 33: 0.8887448906898499
[08/27/2025 18:11:32 INFO]: Training loss at epoch 16: 1.302187204360962
[08/27/2025 18:11:32 INFO]: Training loss at epoch 0: 1.2826398015022278
[08/27/2025 18:11:47 INFO]: Training loss at epoch 7: 1.0307122468948364
[08/27/2025 18:11:48 INFO]: Training loss at epoch 47: 1.0685933232307434
[08/27/2025 18:11:51 INFO]: Training loss at epoch 21: 0.8772437572479248
[08/27/2025 18:11:56 INFO]: Training loss at epoch 2: 1.0822344422340393
[08/27/2025 18:12:00 INFO]: Training loss at epoch 8: 0.9546371400356293
[08/27/2025 18:12:03 INFO]: Training loss at epoch 27: 0.982422798871994
[08/27/2025 18:12:14 INFO]: Training loss at epoch 15: 1.019085317850113
[08/27/2025 18:12:26 INFO]: New best epoch, val score: -0.7211440471401473
[08/27/2025 18:12:26 INFO]: Saving model to: blotchy-Amado_trial_46/model_best.pth
[08/27/2025 18:12:45 INFO]: Training loss at epoch 25: 1.015471875667572
[08/27/2025 18:13:14 INFO]: Training loss at epoch 61: 0.9685769975185394
[08/27/2025 18:13:17 INFO]: New best epoch, val score: -0.6784939276887543
[08/27/2025 18:13:17 INFO]: Saving model to: blotchy-Amado_trial_30/model_best.pth
[08/27/2025 18:13:49 INFO]: Training loss at epoch 25: 1.1318398416042328
[08/27/2025 18:13:57 INFO]: Training loss at epoch 21: 0.9797452390193939
[08/27/2025 18:13:58 INFO]: Training loss at epoch 34: 0.9078464210033417
[08/27/2025 18:14:06 INFO]: Training loss at epoch 20: 0.8634446859359741
[08/27/2025 18:14:41 INFO]: Training loss at epoch 8: 0.9555173516273499
[08/27/2025 18:14:57 INFO]: Training loss at epoch 2: 1.004998654127121
[08/27/2025 18:14:57 INFO]: Training loss at epoch 17: 0.942351222038269
[08/27/2025 18:15:24 INFO]: Training loss at epoch 9: 0.9238454699516296
[08/27/2025 18:15:52 INFO]: New best epoch, val score: -0.6588680837708881
[08/27/2025 18:15:52 INFO]: Saving model to: blotchy-Amado_trial_44/model_best.pth
[08/27/2025 18:15:57 INFO]: Training loss at epoch 48: 1.1874338686466217
[08/27/2025 18:16:26 INFO]: Training loss at epoch 22: 0.995683491230011
[08/27/2025 18:16:26 INFO]: Training loss at epoch 62: 1.0421001315116882
[08/27/2025 18:16:32 INFO]: Training stats: {
    "score": -0.9856215233443472,
    "rmse": 0.9856215233443472
}
[08/27/2025 18:16:32 INFO]: Val stats: {
    "score": -0.6911283345017073,
    "rmse": 0.6911283345017073
}
[08/27/2025 18:16:32 INFO]: Test stats: {
    "score": -0.882346970571311,
    "rmse": 0.882346970571311
}
[08/27/2025 18:16:40 INFO]: Training loss at epoch 26: 0.8697727620601654
[08/27/2025 18:16:51 INFO]: Training loss at epoch 35: 1.1523323953151703
[08/27/2025 18:16:55 INFO]: Training loss at epoch 28: 0.814725935459137
[08/27/2025 18:17:03 INFO]: Training loss at epoch 22: 0.7583579421043396
[08/27/2025 18:17:34 INFO]: Training loss at epoch 9: 1.0950566232204437
[08/27/2025 18:17:48 INFO]: Training loss at epoch 15: 1.237008512020111
[08/27/2025 18:18:04 INFO]: Training loss at epoch 26: 0.8676549792289734
[08/27/2025 18:18:24 INFO]: Training loss at epoch 18: 1.062847375869751
[08/27/2025 18:18:36 INFO]: Training stats: {
    "score": -0.9693285404584374,
    "rmse": 0.9693285404584374
}
[08/27/2025 18:18:36 INFO]: Val stats: {
    "score": -0.6953781865295176,
    "rmse": 0.6953781865295176
}
[08/27/2025 18:18:36 INFO]: Test stats: {
    "score": -0.8860671172747172,
    "rmse": 0.8860671172747172
}
[08/27/2025 18:18:42 INFO]: Training loss at epoch 3: 1.1350383758544922
[08/27/2025 18:19:12 INFO]: Training loss at epoch 1: 0.9650426208972931
[08/27/2025 18:19:24 INFO]: Training loss at epoch 21: 0.9363182485103607
[08/27/2025 18:19:36 INFO]: Training loss at epoch 27: 1.1312322318553925
[08/27/2025 18:19:42 INFO]: Training loss at epoch 22: 0.8798098266124725
[08/27/2025 18:19:45 INFO]: Training loss at epoch 63: 1.1784716546535492
[08/27/2025 18:19:50 INFO]: Training loss at epoch 36: 0.929922342300415
[08/27/2025 18:20:03 INFO]: Training loss at epoch 10: 1.0582135319709778
[08/27/2025 18:20:14 INFO]: Training loss at epoch 49: 1.318642020225525
[08/27/2025 18:20:30 INFO]: New best epoch, val score: -0.6611582887897077
[08/27/2025 18:20:30 INFO]: Saving model to: blotchy-Amado_trial_42/model_best.pth
[08/27/2025 18:20:41 INFO]: Training loss at epoch 16: 1.0688302516937256
[08/27/2025 18:21:02 INFO]: Training loss at epoch 13: 0.7982521057128906
[08/27/2025 18:21:21 INFO]: Training loss at epoch 16: 0.8835163414478302
[08/27/2025 18:21:49 INFO]: Training loss at epoch 10: 0.936993807554245
[08/27/2025 18:21:50 INFO]: Training stats: {
    "score": -0.9937369090629997,
    "rmse": 0.9937369090629997
}
[08/27/2025 18:21:50 INFO]: Val stats: {
    "score": -0.6886463407356467,
    "rmse": 0.6886463407356467
}
[08/27/2025 18:21:50 INFO]: Test stats: {
    "score": -0.8831141589434054,
    "rmse": 0.8831141589434054
}
[08/27/2025 18:22:07 INFO]: Training loss at epoch 19: 1.0559173226356506
[08/27/2025 18:22:09 INFO]: Training loss at epoch 29: 0.7447910010814667
[08/27/2025 18:22:30 INFO]: New best epoch, val score: -0.6696052635426976
[08/27/2025 18:22:30 INFO]: Saving model to: blotchy-Amado_trial_30/model_best.pth
[08/27/2025 18:22:42 INFO]: Training loss at epoch 23: 0.8950805366039276
[08/27/2025 18:22:46 INFO]: Training loss at epoch 28: 0.9783715903759003
[08/27/2025 18:22:59 INFO]: Training loss at epoch 37: 0.9429241716861725
[08/27/2025 18:23:06 INFO]: Training loss at epoch 3: 1.0491761565208435
[08/27/2025 18:23:17 INFO]: Training loss at epoch 64: 1.1269468069076538
[08/27/2025 18:23:22 INFO]: Training stats: {
    "score": -0.9791071833325939,
    "rmse": 0.9791071833325939
}
[08/27/2025 18:23:22 INFO]: Val stats: {
    "score": -0.6683138474416037,
    "rmse": 0.6683138474416037
}
[08/27/2025 18:23:22 INFO]: Test stats: {
    "score": -0.8836152528712807,
    "rmse": 0.8836152528712807
}
[08/27/2025 18:23:30 INFO]: Training loss at epoch 23: 1.0995274186134338
[08/27/2025 18:23:47 INFO]: Training loss at epoch 11: 1.2431697845458984
[08/27/2025 18:23:48 INFO]: Training loss at epoch 27: 0.9210591018199921
[08/27/2025 18:23:50 INFO]: New best epoch, val score: -0.6683138474416037
[08/27/2025 18:23:50 INFO]: Saving model to: blotchy-Amado_trial_41/model_best.pth
[08/27/2025 18:23:59 INFO]: Training stats: {
    "score": -0.8814590191372796,
    "rmse": 0.8814590191372796
}
[08/27/2025 18:23:59 INFO]: Val stats: {
    "score": -0.7361375839581292,
    "rmse": 0.7361375839581292
}
[08/27/2025 18:23:59 INFO]: Test stats: {
    "score": -0.9544723661343795,
    "rmse": 0.9544723661343795
}
[08/27/2025 18:24:16 INFO]: New best epoch, val score: -0.6584429093820511
[08/27/2025 18:24:16 INFO]: Saving model to: blotchy-Amado_trial_42/model_best.pth
[08/27/2025 18:24:58 INFO]: Training loss at epoch 11: 0.8149077892303467
[08/27/2025 18:25:10 INFO]: Training loss at epoch 22: 0.8332915604114532
[08/27/2025 18:25:55 INFO]: Training loss at epoch 23: 0.8404134511947632
[08/27/2025 18:25:55 INFO]: Training loss at epoch 4: 0.856390118598938
[08/27/2025 18:25:56 INFO]: Training loss at epoch 29: 1.222731590270996
[08/27/2025 18:26:10 INFO]: Training loss at epoch 38: 1.011737197637558
[08/27/2025 18:26:22 INFO]: Training loss at epoch 50: 0.9639962911605835
[08/27/2025 18:26:53 INFO]: Training loss at epoch 65: 0.9954450726509094
[08/27/2025 18:26:58 INFO]: Training loss at epoch 16: 0.9151899814605713
[08/27/2025 18:27:01 INFO]: Training stats: {
    "score": -0.9995965445445829,
    "rmse": 0.9995965445445829
}
[08/27/2025 18:27:01 INFO]: Val stats: {
    "score": -0.6659966058741524,
    "rmse": 0.6659966058741524
}
[08/27/2025 18:27:01 INFO]: Test stats: {
    "score": -0.872139668781068,
    "rmse": 0.872139668781068
}
[08/27/2025 18:27:13 INFO]: Training loss at epoch 20: 0.9974550604820251
[08/27/2025 18:27:33 INFO]: Training loss at epoch 2: 0.9589519500732422
[08/27/2025 18:27:34 INFO]: Training loss at epoch 12: 0.9016813635826111
[08/27/2025 18:28:01 INFO]: New best epoch, val score: -0.667870792406553
[08/27/2025 18:28:01 INFO]: Saving model to: blotchy-Amado_trial_35/model_best.pth
[08/27/2025 18:28:08 INFO]: Training loss at epoch 12: 0.7268905937671661
[08/27/2025 18:28:25 INFO]: Training loss at epoch 24: 0.9464759826660156
[08/27/2025 18:29:02 INFO]: New best epoch, val score: -0.6658809932343849
[08/27/2025 18:29:02 INFO]: Saving model to: blotchy-Amado_trial_37/model_best.pth
[08/27/2025 18:29:11 INFO]: Training loss at epoch 39: 1.1771076321601868
[08/27/2025 18:29:12 INFO]: Training loss at epoch 30: 0.7779585421085358
[08/27/2025 18:29:25 INFO]: Training loss at epoch 28: 1.0286987125873566
[08/27/2025 18:29:55 INFO]: Training loss at epoch 30: 0.9166151583194733
[08/27/2025 18:30:10 INFO]: Training stats: {
    "score": -1.0001720570173718,
    "rmse": 1.0001720570173718
}
[08/27/2025 18:30:10 INFO]: Val stats: {
    "score": -0.6578599658095573,
    "rmse": 0.6578599658095573
}
[08/27/2025 18:30:10 INFO]: Test stats: {
    "score": -0.8696923989204778,
    "rmse": 0.8696923989204778
}
[08/27/2025 18:30:10 INFO]: Training loss at epoch 66: 0.9923458397388458
[08/27/2025 18:30:31 INFO]: Training loss at epoch 24: 0.9655964970588684
[08/27/2025 18:30:35 INFO]: Training loss at epoch 51: 1.2176353633403778
[08/27/2025 18:30:37 INFO]: Training loss at epoch 23: 0.7563936710357666
[08/27/2025 18:30:41 INFO]: Training loss at epoch 21: 0.9343196749687195
[08/27/2025 18:30:47 INFO]: Training loss at epoch 17: 1.0177579522132874
[08/27/2025 18:30:59 INFO]: Training loss at epoch 13: 0.8054399788379669
[08/27/2025 18:31:05 INFO]: Training loss at epoch 13: 0.8555786907672882
[08/27/2025 18:31:08 INFO]: New best epoch, val score: -0.66805771895145
[08/27/2025 18:31:08 INFO]: Saving model to: blotchy-Amado_trial_41/model_best.pth
[08/27/2025 18:31:20 INFO]: Training loss at epoch 4: 0.9102696180343628
[08/27/2025 18:31:47 INFO]: Training loss at epoch 24: 0.8712668418884277
[08/27/2025 18:32:22 INFO]: Training loss at epoch 14: 0.9201000928878784
[08/27/2025 18:32:56 INFO]: Training loss at epoch 5: 0.9986492097377777
[08/27/2025 18:32:59 INFO]: Training loss at epoch 31: 1.1342052221298218
[08/27/2025 18:33:16 INFO]: Training loss at epoch 40: 1.0236943364143372
[08/27/2025 18:33:36 INFO]: Training loss at epoch 67: 0.96877321600914
[08/27/2025 18:33:41 INFO]: New best epoch, val score: -0.6614609731176927
[08/27/2025 18:33:41 INFO]: Saving model to: blotchy-Amado_trial_31/model_best.pth
[08/27/2025 18:33:57 INFO]: Training loss at epoch 17: 1.098503202199936
[08/27/2025 18:34:02 INFO]: Training loss at epoch 25: 0.8402200639247894
[08/27/2025 18:34:15 INFO]: Training loss at epoch 14: 0.8163091540336609
[08/27/2025 18:34:21 INFO]: Training loss at epoch 31: 1.0662766695022583
[08/27/2025 18:34:25 INFO]: Training loss at epoch 22: 0.9747907817363739
[08/27/2025 18:34:42 INFO]: Training loss at epoch 14: 1.0437682569026947
[08/27/2025 18:34:57 INFO]: Training loss at epoch 29: 1.0307721495628357
[08/27/2025 18:35:01 INFO]: Training loss at epoch 52: 0.8296861052513123
[08/27/2025 18:35:29 INFO]: Training loss at epoch 3: 1.212923288345337
[08/27/2025 18:35:47 INFO]: Training loss at epoch 17: 0.8311795592308044
[08/27/2025 18:36:04 INFO]: Training loss at epoch 32: 1.0471526980400085
[08/27/2025 18:36:15 INFO]: Training loss at epoch 24: 1.059853732585907
[08/27/2025 18:36:21 INFO]: Training loss at epoch 41: 1.0160386860370636
[08/27/2025 18:36:54 INFO]: New best epoch, val score: -0.661701270948898
[08/27/2025 18:36:54 INFO]: Saving model to: blotchy-Amado_trial_35/model_best.pth
[08/27/2025 18:36:59 INFO]: Training stats: {
    "score": -1.0009377892445577,
    "rmse": 1.0009377892445577
}
[08/27/2025 18:36:59 INFO]: Val stats: {
    "score": -0.6959830482767764,
    "rmse": 0.6959830482767764
}
[08/27/2025 18:36:59 INFO]: Test stats: {
    "score": -0.881666507487158,
    "rmse": 0.881666507487158
}
[08/27/2025 18:37:07 INFO]: Training loss at epoch 68: 1.1299843490123749
[08/27/2025 18:37:26 INFO]: Training loss at epoch 15: 0.8621141314506531
[08/27/2025 18:37:44 INFO]: Training loss at epoch 25: 1.1172828078269958
[08/27/2025 18:37:55 INFO]: Training loss at epoch 25: 0.9509402811527252
[08/27/2025 18:38:11 INFO]: Training loss at epoch 23: 1.2306988537311554
[08/27/2025 18:38:24 INFO]: Training loss at epoch 15: 0.8881744146347046
[08/27/2025 18:39:06 INFO]: Training loss at epoch 33: 0.8219649195671082
[08/27/2025 18:39:22 INFO]: Training loss at epoch 42: 0.9367994368076324
[08/27/2025 18:39:25 INFO]: Training loss at epoch 53: 1.0216524004936218
[08/27/2025 18:39:34 INFO]: Training loss at epoch 32: 0.7776917815208435
[08/27/2025 18:39:35 INFO]: Training loss at epoch 5: 1.1447656154632568
[08/27/2025 18:39:41 INFO]: Training loss at epoch 26: 0.8234597444534302
[08/27/2025 18:39:57 INFO]: Training loss at epoch 6: 1.0164365768432617
[08/27/2025 18:40:14 INFO]: Training loss at epoch 18: 0.9943242967128754
[08/27/2025 18:40:24 INFO]: Training loss at epoch 69: 0.8330444395542145
[08/27/2025 18:40:24 INFO]: Training loss at epoch 16: 0.9127893447875977
[08/27/2025 18:41:38 INFO]: Training stats: {
    "score": -0.9922190407248164,
    "rmse": 0.9922190407248164
}
[08/27/2025 18:41:38 INFO]: Val stats: {
    "score": -0.6640755488665049,
    "rmse": 0.6640755488665049
}
[08/27/2025 18:41:38 INFO]: Test stats: {
    "score": -0.873843999476775,
    "rmse": 0.873843999476775
}
[08/27/2025 18:41:43 INFO]: Training loss at epoch 25: 0.8198359310626984
[08/27/2025 18:41:44 INFO]: Training loss at epoch 24: 0.8601776957511902
[08/27/2025 18:41:54 INFO]: Training loss at epoch 16: 1.1464905142784119
[08/27/2025 18:42:05 INFO]: Training loss at epoch 34: 0.911908894777298
[08/27/2025 18:42:23 INFO]: Training loss at epoch 43: 0.9382222592830658
[08/27/2025 18:42:28 INFO]: Training loss at epoch 30: 1.2557162046432495
[08/27/2025 18:43:33 INFO]: Training loss at epoch 4: 0.9169585108757019
[08/27/2025 18:43:33 INFO]: Training loss at epoch 17: 0.8528160750865936
[08/27/2025 18:43:39 INFO]: Training loss at epoch 15: 0.7875023484230042
[08/27/2025 18:43:46 INFO]: Training loss at epoch 54: 0.9704837203025818
[08/27/2025 18:43:50 INFO]: Training loss at epoch 26: 1.1178506016731262
[08/27/2025 18:44:39 INFO]: Training loss at epoch 18: 0.9841305017471313
[08/27/2025 18:44:41 INFO]: Training loss at epoch 26: 0.9383057951927185
[08/27/2025 18:44:45 INFO]: Training loss at epoch 33: 1.014796108007431
[08/27/2025 18:45:01 INFO]: Training loss at epoch 70: 1.0516630411148071
[08/27/2025 18:45:09 INFO]: Training loss at epoch 35: 0.8724199831485748
[08/27/2025 18:45:16 INFO]: Training loss at epoch 27: 0.7248432338237762
[08/27/2025 18:45:25 INFO]: Training loss at epoch 25: 1.0131830871105194
[08/27/2025 18:45:27 INFO]: Training loss at epoch 44: 0.9057111740112305
[08/27/2025 18:45:32 INFO]: Training loss at epoch 17: 0.8825770616531372
[08/27/2025 18:46:42 INFO]: Training loss at epoch 18: 0.850726991891861
[08/27/2025 18:47:02 INFO]: Training loss at epoch 7: 1.1101629734039307
[08/27/2025 18:47:13 INFO]: Training loss at epoch 18: 0.8217513561248779
[08/27/2025 18:47:17 INFO]: Training loss at epoch 26: 0.8146036863327026
[08/27/2025 18:47:48 INFO]: Training loss at epoch 6: 1.0058807134628296
[08/27/2025 18:48:00 INFO]: Training loss at epoch 31: 1.3071105480194092
[08/27/2025 18:48:08 INFO]: Training loss at epoch 55: 1.1121145486831665
[08/27/2025 18:48:10 INFO]: Training loss at epoch 36: 0.824945867061615
[08/27/2025 18:48:24 INFO]: Training loss at epoch 71: 0.9710096418857574
[08/27/2025 18:48:26 INFO]: Training loss at epoch 45: 0.7806672155857086
[08/27/2025 18:48:59 INFO]: Training loss at epoch 26: 0.9321148991584778
[08/27/2025 18:49:01 INFO]: Training loss at epoch 18: 0.9573713839054108
[08/27/2025 18:49:36 INFO]: Training loss at epoch 19: 0.9572248756885529
[08/27/2025 18:49:39 INFO]: Training loss at epoch 19: 1.024908035993576
[08/27/2025 18:49:47 INFO]: Training loss at epoch 27: 1.0150805711746216
[08/27/2025 18:49:56 INFO]: Training loss at epoch 34: 0.8440374135971069
[08/27/2025 18:50:44 INFO]: Training stats: {
    "score": -0.9440124721586771,
    "rmse": 0.9440124721586771
}
[08/27/2025 18:50:44 INFO]: Val stats: {
    "score": -0.6938201520707444,
    "rmse": 0.6938201520707444
}
[08/27/2025 18:50:44 INFO]: Test stats: {
    "score": -0.8943766903968091,
    "rmse": 0.8943766903968091
}
[08/27/2025 18:50:47 INFO]: Training loss at epoch 28: 0.8987630605697632
[08/27/2025 18:51:11 INFO]: Training loss at epoch 37: 1.0276138186454773
[08/27/2025 18:51:28 INFO]: Training loss at epoch 46: 0.9722313284873962
[08/27/2025 18:51:31 INFO]: Training loss at epoch 5: 1.0562352538108826
[08/27/2025 18:51:43 INFO]: Training loss at epoch 27: 1.082588016986847
[08/27/2025 18:51:48 INFO]: Training loss at epoch 72: 0.871488481760025
[08/27/2025 18:51:50 INFO]: Running Final Evaluation...
[08/27/2025 18:52:28 INFO]: Training loss at epoch 56: 0.9521874189376831
[08/27/2025 18:52:38 INFO]: Training loss at epoch 19: 0.9401952624320984
[08/27/2025 18:52:39 INFO]: Training loss at epoch 27: 0.8381209671497345
[08/27/2025 18:52:41 INFO]: Training loss at epoch 27: 0.842829555273056
[08/27/2025 18:52:46 INFO]: Training stats: {
    "score": -0.9995556275870483,
    "rmse": 0.9995556275870483
}
[08/27/2025 18:52:46 INFO]: Val stats: {
    "score": -0.6989265609079035,
    "rmse": 0.6989265609079035
}
[08/27/2025 18:52:46 INFO]: Test stats: {
    "score": -0.8842080675014985,
    "rmse": 0.8842080675014985
}
[08/27/2025 18:53:12 INFO]: Training accuracy: {
    "score": -1.0064311294084356,
    "rmse": 1.0064311294084356
}
[08/27/2025 18:53:12 INFO]: Val accuracy: {
    "score": -0.6560653877249384,
    "rmse": 0.6560653877249384
}
[08/27/2025 18:53:12 INFO]: Test accuracy: {
    "score": -0.8691397075343533,
    "rmse": 0.8691397075343533
}
[08/27/2025 18:53:12 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_39",
    "best_epoch": 15,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8691397075343533,
        "rmse": 0.8691397075343533
    },
    "train_stats": {
        "score": -1.0064311294084356,
        "rmse": 1.0064311294084356
    },
    "val_stats": {
        "score": -0.6560653877249384,
        "rmse": 0.6560653877249384
    }
}
[08/27/2025 18:53:12 INFO]: Procewss finished for trial blotchy-Amado_trial_39
[08/27/2025 18:53:12 INFO]: 
_________________________________________________

[08/27/2025 18:53:12 INFO]: train_net_for_optune.py main() running.
[08/27/2025 18:53:12 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 2.602613614947348
  attention_dropout: 0.4035715476666065
  ffn_dropout: 0.4035715476666065
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.481303937706872e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_47

[08/27/2025 18:53:13 INFO]: This ft_transformer has 11.020 million parameters.
[08/27/2025 18:53:13 INFO]: Training will start at epoch 0.
[08/27/2025 18:53:13 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 18:53:31 INFO]: Training loss at epoch 32: 0.9152444303035736
[08/27/2025 18:53:33 INFO]: Training loss at epoch 19: 1.0151369869709015
[08/27/2025 18:53:50 INFO]: Training loss at epoch 20: 0.8800305128097534
[08/27/2025 18:53:52 INFO]: Training stats: {
    "score": -0.9748182560423685,
    "rmse": 0.9748182560423685
}
[08/27/2025 18:53:52 INFO]: Val stats: {
    "score": -0.7091555315957299,
    "rmse": 0.7091555315957299
}
[08/27/2025 18:53:52 INFO]: Test stats: {
    "score": -0.8968227626621447,
    "rmse": 0.8968227626621447
}
[08/27/2025 18:53:56 INFO]: Training loss at epoch 8: 0.9346621036529541
[08/27/2025 18:54:14 INFO]: Training loss at epoch 38: 0.8710070550441742
[08/27/2025 18:54:50 INFO]: Training loss at epoch 16: 0.8706756830215454
[08/27/2025 18:55:10 INFO]: Training loss at epoch 35: 0.8325589001178741
[08/27/2025 18:55:14 INFO]: Training loss at epoch 73: 0.9686229526996613
[08/27/2025 18:55:48 INFO]: Training loss at epoch 28: 0.8655885457992554
[08/27/2025 18:56:00 INFO]: Training loss at epoch 7: 1.0863743424415588
[08/27/2025 18:56:17 INFO]: Training loss at epoch 28: 0.9976141154766083
[08/27/2025 18:56:21 INFO]: Training loss at epoch 29: 0.8403369188308716
[08/27/2025 18:56:34 INFO]: Training stats: {
    "score": -1.0475986326934008,
    "rmse": 1.0475986326934008
}
[08/27/2025 18:56:34 INFO]: Val stats: {
    "score": -0.6770721593314718,
    "rmse": 0.6770721593314718
}
[08/27/2025 18:56:34 INFO]: Test stats: {
    "score": -0.8947680195794782,
    "rmse": 0.8947680195794782
}
[08/27/2025 18:56:47 INFO]: Training loss at epoch 57: 0.9605206549167633
[08/27/2025 18:56:52 INFO]: Training loss at epoch 21: 0.9732929766178131
[08/27/2025 18:57:10 INFO]: Training loss at epoch 39: 0.9602997303009033
[08/27/2025 18:57:22 INFO]: Training loss at epoch 20: 0.8830615878105164
[08/27/2025 18:58:03 INFO]: Training loss at epoch 0: 0.8888440430164337
[08/27/2025 18:58:03 INFO]: Training loss at epoch 28: 0.8040406107902527
[08/27/2025 18:58:07 INFO]: Training stats: {
    "score": -0.9065156825625861,
    "rmse": 0.9065156825625861
}
[08/27/2025 18:58:07 INFO]: Val stats: {
    "score": -0.7235683966316219,
    "rmse": 0.7235683966316219
}
[08/27/2025 18:58:07 INFO]: Test stats: {
    "score": -0.9414995354601258,
    "rmse": 0.9414995354601258
}
[08/27/2025 18:58:09 INFO]: Training stats: {
    "score": -0.9924374607315442,
    "rmse": 0.9924374607315442
}
[08/27/2025 18:58:09 INFO]: Val stats: {
    "score": -0.704952849523869,
    "rmse": 0.704952849523869
}
[08/27/2025 18:58:09 INFO]: Test stats: {
    "score": -0.8877124737224629,
    "rmse": 0.8877124737224629
}
[08/27/2025 18:58:25 INFO]: Training loss at epoch 74: 1.09628564119339
[08/27/2025 18:58:34 INFO]: Training loss at epoch 28: 1.0232425928115845
[08/27/2025 18:58:39 INFO]: New best epoch, val score: -0.6601971190088424
[08/27/2025 18:58:39 INFO]: Saving model to: blotchy-Amado_trial_47/model_best.pth
[08/27/2025 18:58:48 INFO]: Running Final Evaluation...
[08/27/2025 18:58:53 INFO]: Training loss at epoch 33: 0.9140129387378693
[08/27/2025 18:59:19 INFO]: Training loss at epoch 6: 1.0287453830242157
[08/27/2025 18:59:42 INFO]: Training loss at epoch 29: 0.8796230256557465
[08/27/2025 18:59:45 INFO]: Training loss at epoch 22: 1.0709789395332336
[08/27/2025 18:59:59 INFO]: Training accuracy: {
    "score": -1.0089311333383009,
    "rmse": 1.0089311333383009
}
[08/27/2025 18:59:59 INFO]: Val accuracy: {
    "score": -0.6570391354600241,
    "rmse": 0.6570391354600241
}
[08/27/2025 18:59:59 INFO]: Test accuracy: {
    "score": -0.874584629402303,
    "rmse": 0.874584629402303
}
[08/27/2025 18:59:59 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_3",
    "best_epoch": 43,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.874584629402303,
        "rmse": 0.874584629402303
    },
    "train_stats": {
        "score": -1.0089311333383009,
        "rmse": 1.0089311333383009
    },
    "val_stats": {
        "score": -0.6570391354600241,
        "rmse": 0.6570391354600241
    }
}
[08/27/2025 18:59:59 INFO]: Procewss finished for trial blotchy-Amado_trial_3
[08/27/2025 18:59:59 INFO]: 
_________________________________________________

[08/27/2025 18:59:59 INFO]: train_net_for_optune.py main() running.
[08/27/2025 18:59:59 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 16
  d_ffn_factor: 2.5772004595902334
  attention_dropout: 0.3765712848210466
  ffn_dropout: 0.3765712848210466
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.592102591709099e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_48

[08/27/2025 19:00:00 INFO]: This ft_transformer has 10.958 million parameters.
[08/27/2025 19:00:00 INFO]: Training will start at epoch 0.
[08/27/2025 19:00:00 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:00:00 INFO]: Training loss at epoch 19: 0.9313176870346069
[08/27/2025 19:00:02 INFO]: Training loss at epoch 36: 0.9286763668060303
[08/27/2025 19:00:39 INFO]: Training loss at epoch 9: 1.0148562788963318
[08/27/2025 19:00:43 INFO]: Training loss at epoch 21: 0.863138347864151
[08/27/2025 19:00:51 INFO]: Training stats: {
    "score": -0.9573693462757727,
    "rmse": 0.9573693462757727
}
[08/27/2025 19:00:51 INFO]: Val stats: {
    "score": -0.7022742313861114,
    "rmse": 0.7022742313861114
}
[08/27/2025 19:00:51 INFO]: Test stats: {
    "score": -0.9009722066043673,
    "rmse": 0.9009722066043673
}
[08/27/2025 19:00:53 INFO]: Training loss at epoch 58: 0.9136446714401245
[08/27/2025 19:01:00 INFO]: Training loss at epoch 40: 1.0348663330078125
[08/27/2025 19:01:25 INFO]: Training loss at epoch 29: 0.835576981306076
[08/27/2025 19:01:46 INFO]: Training loss at epoch 20: 1.3438679575920105
[08/27/2025 19:02:42 INFO]: Training loss at epoch 23: 0.9262611865997314
[08/27/2025 19:02:57 INFO]: Training stats: {
    "score": -0.9963755228424914,
    "rmse": 0.9963755228424914
}
[08/27/2025 19:02:57 INFO]: Val stats: {
    "score": -0.6827839816527391,
    "rmse": 0.6827839816527391
}
[08/27/2025 19:02:57 INFO]: Test stats: {
    "score": -0.8676214597096377,
    "rmse": 0.8676214597096377
}
[08/27/2025 19:03:14 INFO]: Training loss at epoch 29: 0.9892636239528656
[08/27/2025 19:03:24 INFO]: Training loss at epoch 30: 0.7388029098510742
[08/27/2025 19:03:24 INFO]: Training loss at epoch 1: 0.9866637289524078
[08/27/2025 19:03:24 INFO]: Training stats: {
    "score": -0.9993700838306651,
    "rmse": 0.9993700838306651
}
[08/27/2025 19:03:24 INFO]: Val stats: {
    "score": -0.6776391860216519,
    "rmse": 0.6776391860216519
}
[08/27/2025 19:03:24 INFO]: Test stats: {
    "score": -0.8728373199605395,
    "rmse": 0.8728373199605395
}
[08/27/2025 19:03:49 INFO]: Training loss at epoch 8: 1.1189146041870117
[08/27/2025 19:03:57 INFO]: Training loss at epoch 41: 1.019271969795227
[08/27/2025 19:04:13 INFO]: Training loss at epoch 22: 0.7617192566394806
[08/27/2025 19:04:15 INFO]: Training loss at epoch 34: 1.0201732516288757
[08/27/2025 19:04:18 INFO]: Training stats: {
    "score": -0.9769538875078928,
    "rmse": 0.9769538875078928
}
[08/27/2025 19:04:18 INFO]: Val stats: {
    "score": -0.7648745555943106,
    "rmse": 0.7648745555943106
}
[08/27/2025 19:04:18 INFO]: Test stats: {
    "score": -0.9276225970123055,
    "rmse": 0.9276225970123055
}
[08/27/2025 19:04:27 INFO]: Training loss at epoch 30: 0.7678432166576385
[08/27/2025 19:05:07 INFO]: Training loss at epoch 20: 1.1839067935943604
[08/27/2025 19:05:09 INFO]: Training loss at epoch 37: 0.8641737997531891
[08/27/2025 19:05:12 INFO]: Training stats: {
    "score": -0.9032128927125602,
    "rmse": 0.9032128927125602
}
[08/27/2025 19:05:12 INFO]: Val stats: {
    "score": -0.7054981189879418,
    "rmse": 0.7054981189879418
}
[08/27/2025 19:05:12 INFO]: Test stats: {
    "score": -0.918111314480002,
    "rmse": 0.918111314480002
}
[08/27/2025 19:05:14 INFO]: Training loss at epoch 59: 1.0263969898223877
[08/27/2025 19:05:27 INFO]: Training loss at epoch 29: 1.1654859781265259
[08/27/2025 19:05:40 INFO]: Training loss at epoch 17: 0.9262928068637848
[08/27/2025 19:05:52 INFO]: Training loss at epoch 24: 0.847581684589386
[08/27/2025 19:06:06 INFO]: Training loss at epoch 0: 1.0007440149784088
[08/27/2025 19:06:49 INFO]: Training stats: {
    "score": -0.9965875679393488,
    "rmse": 0.9965875679393488
}
[08/27/2025 19:06:49 INFO]: Val stats: {
    "score": -0.671305181637461,
    "rmse": 0.671305181637461
}
[08/27/2025 19:06:49 INFO]: Test stats: {
    "score": -0.877265385383081,
    "rmse": 0.877265385383081
}
[08/27/2025 19:06:57 INFO]: New best epoch, val score: -0.760314799221815
[08/27/2025 19:06:57 INFO]: Saving model to: blotchy-Amado_trial_48/model_best.pth
[08/27/2025 19:07:04 INFO]: Training loss at epoch 42: 0.8249806761741638
[08/27/2025 19:07:16 INFO]: Training loss at epoch 7: 1.1139922738075256
[08/27/2025 19:07:52 INFO]: Training stats: {
    "score": -1.0086530910430824,
    "rmse": 1.0086530910430824
}
[08/27/2025 19:07:52 INFO]: Val stats: {
    "score": -0.7512452910647021,
    "rmse": 0.7512452910647021
}
[08/27/2025 19:07:52 INFO]: Test stats: {
    "score": -0.9143749953760317,
    "rmse": 0.9143749953760317
}
[08/27/2025 19:07:53 INFO]: Training loss at epoch 23: 0.941333532333374
[08/27/2025 19:08:11 INFO]: Training loss at epoch 31: 0.9240934550762177
[08/27/2025 19:08:58 INFO]: Training loss at epoch 25: 0.9660636782646179
[08/27/2025 19:09:07 INFO]: Training loss at epoch 31: 0.9459404945373535
[08/27/2025 19:09:11 INFO]: Training loss at epoch 2: 1.4510701894760132
[08/27/2025 19:09:32 INFO]: Training loss at epoch 30: 0.9707042574882507
[08/27/2025 19:09:55 INFO]: Training loss at epoch 35: 1.1198802590370178
[08/27/2025 19:10:06 INFO]: Training loss at epoch 10: 0.9059851467609406
[08/27/2025 19:10:11 INFO]: Training loss at epoch 43: 0.9738215804100037
[08/27/2025 19:10:28 INFO]: Training loss at epoch 38: 0.6809161007404327
[08/27/2025 19:10:49 INFO]: Training loss at epoch 30: 0.9541968405246735
[08/27/2025 19:11:07 INFO]: Running Final Evaluation...
[08/27/2025 19:11:15 INFO]: Training loss at epoch 60: 0.9808807671070099
[08/27/2025 19:11:18 INFO]: Training loss at epoch 21: 0.8443659842014313
[08/27/2025 19:11:36 INFO]: Training loss at epoch 24: 1.018443912267685
[08/27/2025 19:11:52 INFO]: Training loss at epoch 32: 0.8893334567546844
[08/27/2025 19:12:05 INFO]: Training loss at epoch 26: 0.8609638512134552
[08/27/2025 19:12:08 INFO]: Training loss at epoch 9: 0.908746063709259
[08/27/2025 19:12:48 INFO]: Training accuracy: {
    "score": -0.9984345372330063,
    "rmse": 0.9984345372330063
}
[08/27/2025 19:12:48 INFO]: Val accuracy: {
    "score": -0.6585974082350837,
    "rmse": 0.6585974082350837
}
[08/27/2025 19:12:48 INFO]: Test accuracy: {
    "score": -0.877311865509801,
    "rmse": 0.877311865509801
}
[08/27/2025 19:12:49 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_33",
    "best_epoch": 7,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.877311865509801,
        "rmse": 0.877311865509801
    },
    "train_stats": {
        "score": -0.9984345372330063,
        "rmse": 0.9984345372330063
    },
    "val_stats": {
        "score": -0.6585974082350837,
        "rmse": 0.6585974082350837
    }
}
[08/27/2025 19:12:49 INFO]: Procewss finished for trial blotchy-Amado_trial_33
[08/27/2025 19:12:50 INFO]: 
_________________________________________________

[08/27/2025 19:12:50 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:12:50 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 2.5812302222353365
  attention_dropout: 0.4025567198070835
  ffn_dropout: 0.4025567198070835
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.839875347645036e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_49

[08/27/2025 19:12:50 INFO]: This ft_transformer has 10.970 million parameters.
[08/27/2025 19:12:50 INFO]: Training will start at epoch 0.
[08/27/2025 19:12:50 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:13:04 INFO]: Training loss at epoch 1: 0.9374696910381317
[08/27/2025 19:13:10 INFO]: Training loss at epoch 44: 0.9718284904956818
[08/27/2025 19:13:52 INFO]: New best epoch, val score: -0.6657656733022957
[08/27/2025 19:13:52 INFO]: Saving model to: blotchy-Amado_trial_48/model_best.pth
[08/27/2025 19:14:00 INFO]: Training loss at epoch 21: 1.1232392191886902
[08/27/2025 19:14:36 INFO]: Training loss at epoch 32: 0.8174878358840942
[08/27/2025 19:14:39 INFO]: Training loss at epoch 3: 1.3172706365585327
[08/27/2025 19:14:45 INFO]: Training stats: {
    "score": -0.9932097282159488,
    "rmse": 0.9932097282159488
}
[08/27/2025 19:14:45 INFO]: Val stats: {
    "score": -0.6661670085513159,
    "rmse": 0.6661670085513159
}
[08/27/2025 19:14:45 INFO]: Test stats: {
    "score": -0.8869887900601444,
    "rmse": 0.8869887900601444
}
[08/27/2025 19:14:46 INFO]: Training loss at epoch 30: 0.9749984741210938
[08/27/2025 19:14:58 INFO]: Training loss at epoch 25: 0.9367092847824097
[08/27/2025 19:15:00 INFO]: Training loss at epoch 27: 0.9112941920757294
[08/27/2025 19:15:10 INFO]: Training loss at epoch 8: 1.2674714922904968
[08/27/2025 19:15:15 INFO]: Training loss at epoch 36: 1.2172526717185974
[08/27/2025 19:15:16 INFO]: Training loss at epoch 33: 0.9720025062561035
[08/27/2025 19:15:19 INFO]: Training loss at epoch 31: 0.9388818740844727
[08/27/2025 19:15:21 INFO]: Training loss at epoch 61: 1.082937777042389
[08/27/2025 19:16:00 INFO]: Training loss at epoch 45: 1.0148588120937347
[08/27/2025 19:16:00 INFO]: Training loss at epoch 31: 0.9150944948196411
[08/27/2025 19:16:21 INFO]: Running Final Evaluation...
[08/27/2025 19:16:37 INFO]: Training loss at epoch 18: 1.1898870468139648
[08/27/2025 19:16:47 INFO]: Training loss at epoch 11: 1.0797322988510132
[08/27/2025 19:17:19 INFO]: Training loss at epoch 20: 1.1241210103034973
[08/27/2025 19:17:24 INFO]: Training accuracy: {
    "score": -1.0033346449783611,
    "rmse": 1.0033346449783611
}
[08/27/2025 19:17:24 INFO]: Val accuracy: {
    "score": -0.6652807269481702,
    "rmse": 0.6652807269481702
}
[08/27/2025 19:17:24 INFO]: Test accuracy: {
    "score": -0.8720743081632886,
    "rmse": 0.8720743081632886
}
[08/27/2025 19:17:24 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_40",
    "best_epoch": 14,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8720743081632886,
        "rmse": 0.8720743081632886
    },
    "train_stats": {
        "score": -1.0033346449783611,
        "rmse": 1.0033346449783611
    },
    "val_stats": {
        "score": -0.6652807269481702,
        "rmse": 0.6652807269481702
    }
}
[08/27/2025 19:17:24 INFO]: Procewss finished for trial blotchy-Amado_trial_40
[08/27/2025 19:17:24 INFO]: 
_________________________________________________

[08/27/2025 19:17:24 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:17:24 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 2.4122062851566843
  attention_dropout: 0.4116750638541913
  ffn_dropout: 0.4116750638541913
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.580844133664217e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_50

[08/27/2025 19:17:25 INFO]: This ft_transformer has 10.570 million parameters.
[08/27/2025 19:17:25 INFO]: Training will start at epoch 0.
[08/27/2025 19:17:25 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:17:32 INFO]: Training loss at epoch 0: 1.41776043176651
[08/27/2025 19:17:56 INFO]: Training loss at epoch 28: 0.9402132034301758
[08/27/2025 19:18:09 INFO]: New best epoch, val score: -1.082973219644244
[08/27/2025 19:18:09 INFO]: Saving model to: blotchy-Amado_trial_49/model_best.pth
[08/27/2025 19:18:18 INFO]: New best epoch, val score: -0.671462707518134
[08/27/2025 19:18:18 INFO]: Saving model to: blotchy-Amado_trial_43/model_best.pth
[08/27/2025 19:18:23 INFO]: Training loss at epoch 26: 0.9379635155200958
[08/27/2025 19:18:46 INFO]: Training loss at epoch 34: 0.9375318884849548
[08/27/2025 19:19:38 INFO]: Training loss at epoch 62: 0.8966255784034729
[08/27/2025 19:19:48 INFO]: Training loss at epoch 2: 0.991907924413681
[08/27/2025 19:20:02 INFO]: Training loss at epoch 33: 0.8330641090869904
[08/27/2025 19:20:09 INFO]: Training loss at epoch 4: 1.0791966617107391
[08/27/2025 19:20:19 INFO]: Training loss at epoch 22: 0.761369913816452
[08/27/2025 19:20:44 INFO]: Training loss at epoch 37: 1.1965487003326416
[08/27/2025 19:21:06 INFO]: Training loss at epoch 29: 1.0248718857765198
[08/27/2025 19:21:14 INFO]: Training loss at epoch 32: 1.035681664943695
[08/27/2025 19:21:30 INFO]: Training loss at epoch 32: 1.13128063082695
[08/27/2025 19:21:44 INFO]: Training loss at epoch 31: 0.8702160120010376
[08/27/2025 19:22:03 INFO]: Training loss at epoch 27: 0.968540370464325
[08/27/2025 19:22:09 INFO]: Training stats: {
    "score": -0.9126625692367266,
    "rmse": 0.9126625692367266
}
[08/27/2025 19:22:09 INFO]: Val stats: {
    "score": -0.6760633717784872,
    "rmse": 0.6760633717784872
}
[08/27/2025 19:22:09 INFO]: Test stats: {
    "score": -0.9010345144004425,
    "rmse": 0.9010345144004425
}
[08/27/2025 19:22:15 INFO]: Training loss at epoch 0: 1.0386966466903687
[08/27/2025 19:22:30 INFO]: Training loss at epoch 35: 1.0262624025344849
[08/27/2025 19:22:45 INFO]: Training loss at epoch 22: 0.8918321132659912
[08/27/2025 19:22:50 INFO]: Training loss at epoch 10: 1.0340217351913452
[08/27/2025 19:22:55 INFO]: New best epoch, val score: -0.7325091229641311
[08/27/2025 19:22:55 INFO]: Saving model to: blotchy-Amado_trial_50/model_best.pth
[08/27/2025 19:23:11 INFO]: Training loss at epoch 9: 1.0065279603004456
[08/27/2025 19:23:16 INFO]: Training loss at epoch 1: 1.260752022266388
[08/27/2025 19:23:51 INFO]: Training loss at epoch 12: 0.9225670099258423
[08/27/2025 19:23:55 INFO]: New best epoch, val score: -0.9014149190313154
[08/27/2025 19:23:55 INFO]: Saving model to: blotchy-Amado_trial_49/model_best.pth
[08/27/2025 19:24:07 INFO]: Training loss at epoch 63: 1.002625435590744
[08/27/2025 19:25:21 INFO]: Training loss at epoch 30: 0.7962125539779663
[08/27/2025 19:25:46 INFO]: Training loss at epoch 28: 1.0549530684947968
[08/27/2025 19:25:47 INFO]: Training loss at epoch 34: 0.7494640350341797
[08/27/2025 19:25:56 INFO]: Training loss at epoch 5: 1.4112146198749542
[08/27/2025 19:26:01 INFO]: Training stats: {
    "score": -1.065975878315637,
    "rmse": 1.065975878315637
}
[08/27/2025 19:26:01 INFO]: Val stats: {
    "score": -0.8617936665816714,
    "rmse": 0.8617936665816714
}
[08/27/2025 19:26:01 INFO]: Test stats: {
    "score": -1.054699341075577,
    "rmse": 1.054699341075577
}
[08/27/2025 19:26:16 INFO]: Training loss at epoch 36: 1.1251408457756042
[08/27/2025 19:26:26 INFO]: Training loss at epoch 38: 0.9317212402820587
[08/27/2025 19:26:56 INFO]: Training loss at epoch 3: 1.13169926404953
[08/27/2025 19:27:06 INFO]: New best epoch, val score: -0.6604156931636443
[08/27/2025 19:27:06 INFO]: Saving model to: blotchy-Amado_trial_34/model_best.pth
[08/27/2025 19:27:06 INFO]: Training loss at epoch 33: 0.8084226250648499
[08/27/2025 19:27:20 INFO]: Training loss at epoch 33: 0.9313847720623016
[08/27/2025 19:27:46 INFO]: Training loss at epoch 1: 1.1795997619628906
[08/27/2025 19:27:55 INFO]: Training loss at epoch 19: 0.9427105486392975
[08/27/2025 19:28:23 INFO]: Training loss at epoch 31: 0.93000328540802
[08/27/2025 19:28:28 INFO]: Training loss at epoch 64: 1.0054208636283875
[08/27/2025 19:28:46 INFO]: Training loss at epoch 32: 0.9056132733821869
[08/27/2025 19:28:51 INFO]: Training loss at epoch 2: 1.1818835139274597
[08/27/2025 19:29:13 INFO]: Training loss at epoch 29: 0.9501484632492065
[08/27/2025 19:29:40 INFO]: Training loss at epoch 23: 0.9112045466899872
[08/27/2025 19:29:44 INFO]: Training loss at epoch 37: 0.9223731458187103
[08/27/2025 19:30:21 INFO]: Training stats: {
    "score": -0.9554135330908283,
    "rmse": 0.9554135330908283
}
[08/27/2025 19:30:21 INFO]: Val stats: {
    "score": -0.7121582661470613,
    "rmse": 0.7121582661470613
}
[08/27/2025 19:30:21 INFO]: Test stats: {
    "score": -0.9060827236501044,
    "rmse": 0.9060827236501044
}
[08/27/2025 19:30:28 INFO]: Training loss at epoch 21: 1.0851240754127502
[08/27/2025 19:30:42 INFO]: Training loss at epoch 13: 1.020832896232605
[08/27/2025 19:30:52 INFO]: Training loss at epoch 11: 1.1679200530052185
[08/27/2025 19:31:05 INFO]: Training loss at epoch 35: 0.9263689815998077
[08/27/2025 19:31:17 INFO]: Training loss at epoch 32: 0.8229974508285522
[08/27/2025 19:31:18 INFO]: Training loss at epoch 6: 0.9932064712047577
[08/27/2025 19:31:28 INFO]: Training stats: {
    "score": -0.9927009914970567,
    "rmse": 0.9927009914970567
}
[08/27/2025 19:31:28 INFO]: Val stats: {
    "score": -0.7012258406993956,
    "rmse": 0.7012258406993956
}
[08/27/2025 19:31:28 INFO]: Test stats: {
    "score": -0.8853135054073747,
    "rmse": 0.8853135054073747
}
[08/27/2025 19:31:29 INFO]: Training loss at epoch 23: 0.9974660575389862
[08/27/2025 19:31:41 INFO]: Training loss at epoch 39: 0.964080810546875
[08/27/2025 19:32:14 INFO]: Training loss at epoch 34: 0.8104217350482941
[08/27/2025 19:32:33 INFO]: Training loss at epoch 65: 0.9876025021076202
[08/27/2025 19:32:51 INFO]: Training loss at epoch 2: 1.1037372648715973
[08/27/2025 19:32:56 INFO]: Training loss at epoch 34: 0.9675366580486298
[08/27/2025 19:33:08 INFO]: Training loss at epoch 38: 0.849914163351059
[08/27/2025 19:33:29 INFO]: Training loss at epoch 4: 1.2433332204818726
[08/27/2025 19:33:31 INFO]: Training stats: {
    "score": -1.0159225963570286,
    "rmse": 1.0159225963570286
}
[08/27/2025 19:33:31 INFO]: Val stats: {
    "score": -0.6607267106819561,
    "rmse": 0.6607267106819561
}
[08/27/2025 19:33:31 INFO]: Test stats: {
    "score": -0.8725519831378955,
    "rmse": 0.8725519831378955
}
[08/27/2025 19:33:39 INFO]: Training loss at epoch 10: 1.0501424074172974
[08/27/2025 19:33:45 INFO]: Training loss at epoch 30: 1.0501829981803894
[08/27/2025 19:34:09 INFO]: Training loss at epoch 3: 1.403205692768097
[08/27/2025 19:34:13 INFO]: Training loss at epoch 33: 0.9341146647930145
[08/27/2025 19:34:34 INFO]: New best epoch, val score: -0.6630213377906302
[08/27/2025 19:34:34 INFO]: Saving model to: blotchy-Amado_trial_46/model_best.pth
[08/27/2025 19:34:34 INFO]: New best epoch, val score: -0.6667969890401269
[08/27/2025 19:34:34 INFO]: Saving model to: blotchy-Amado_trial_43/model_best.pth
[08/27/2025 19:34:46 INFO]: New best epoch, val score: -0.7462003104268103
[08/27/2025 19:34:46 INFO]: Saving model to: blotchy-Amado_trial_49/model_best.pth
[08/27/2025 19:35:26 INFO]: Training loss at epoch 33: 1.0224596858024597
[08/27/2025 19:36:29 INFO]: Training loss at epoch 36: 0.7692804634571075
[08/27/2025 19:36:46 INFO]: Training loss at epoch 39: 0.7341820597648621
[08/27/2025 19:36:46 INFO]: Training loss at epoch 7: 1.2940527200698853
[08/27/2025 19:36:52 INFO]: Training loss at epoch 66: 0.8875741362571716
[08/27/2025 19:37:20 INFO]: Training loss at epoch 31: 0.9201481938362122
[08/27/2025 19:37:21 INFO]: Training loss at epoch 34: 0.9297898411750793
[08/27/2025 19:37:32 INFO]: Training loss at epoch 14: 1.144330084323883
[08/27/2025 19:37:41 INFO]: Training loss at epoch 35: 0.8427165150642395
[08/27/2025 19:38:03 INFO]: Training stats: {
    "score": -0.9544168804398927,
    "rmse": 0.9544168804398927
}
[08/27/2025 19:38:03 INFO]: Val stats: {
    "score": -0.7413585407763852,
    "rmse": 0.7413585407763852
}
[08/27/2025 19:38:03 INFO]: Test stats: {
    "score": -0.933221977507031,
    "rmse": 0.933221977507031
}
[08/27/2025 19:38:18 INFO]: Training loss at epoch 3: 1.0820881128311157
[08/27/2025 19:38:48 INFO]: Training loss at epoch 24: 1.267016440629959
[08/27/2025 19:38:54 INFO]: New best epoch, val score: -0.6800936627099561
[08/27/2025 19:38:54 INFO]: Saving model to: blotchy-Amado_trial_50/model_best.pth
[08/27/2025 19:38:55 INFO]: Training loss at epoch 12: 0.9406864941120148
[08/27/2025 19:38:57 INFO]: Training loss at epoch 35: 1.0501692295074463
[08/27/2025 19:39:06 INFO]: Training loss at epoch 40: 1.3202128410339355
[08/27/2025 19:39:50 INFO]: Training loss at epoch 4: 0.9148333370685577
[08/27/2025 19:40:18 INFO]: Training loss at epoch 24: 0.8542517721652985
[08/27/2025 19:40:30 INFO]: Training loss at epoch 35: 0.7161029428243637
[08/27/2025 19:40:33 INFO]: Training loss at epoch 5: 1.1331759691238403
[08/27/2025 19:40:52 INFO]: New best epoch, val score: -0.6654210405674225
[08/27/2025 19:40:52 INFO]: Saving model to: blotchy-Amado_trial_43/model_best.pth
[08/27/2025 19:40:58 INFO]: Training loss at epoch 32: 1.0676280558109283
[08/27/2025 19:41:18 INFO]: Training loss at epoch 67: 0.9743807315826416
[08/27/2025 19:41:43 INFO]: Training loss at epoch 11: 0.9535020589828491
[08/27/2025 19:41:44 INFO]: Training loss at epoch 40: 0.9819146394729614
[08/27/2025 19:42:08 INFO]: Training loss at epoch 37: 0.9488265812397003
[08/27/2025 19:42:29 INFO]: Training loss at epoch 8: 1.088311493396759
[08/27/2025 19:42:34 INFO]: Training loss at epoch 34: 0.9266283512115479
[08/27/2025 19:42:35 INFO]: Training loss at epoch 20: 1.2527511417865753
[08/27/2025 19:42:40 INFO]: New best epoch, val score: -0.6496308546618419
[08/27/2025 19:42:40 INFO]: Saving model to: blotchy-Amado_trial_46/model_best.pth
[08/27/2025 19:43:12 INFO]: Training loss at epoch 36: 0.791510820388794
[08/27/2025 19:43:25 INFO]: Running Final Evaluation...
[08/27/2025 19:43:28 INFO]: Training loss at epoch 22: 0.9333782196044922
[08/27/2025 19:43:33 INFO]: Training loss at epoch 36: 0.8956358134746552
[08/27/2025 19:43:44 INFO]: Training loss at epoch 4: 0.9304229021072388
[08/27/2025 19:44:21 INFO]: New best epoch, val score: -0.66688937166095
[08/27/2025 19:44:21 INFO]: Saving model to: blotchy-Amado_trial_50/model_best.pth
[08/27/2025 19:44:30 INFO]: Training loss at epoch 33: 1.0243868827819824
[08/27/2025 19:44:33 INFO]: Training loss at epoch 15: 1.0912379622459412
[08/27/2025 19:44:39 INFO]: Training loss at epoch 41: 1.0114464163780212
[08/27/2025 19:44:55 INFO]: Training loss at epoch 36: 1.0533149242401123
[08/27/2025 19:45:17 INFO]: Training loss at epoch 41: 1.099323034286499
[08/27/2025 19:45:22 INFO]: Training loss at epoch 5: 1.358595371246338
[08/27/2025 19:45:36 INFO]: Training loss at epoch 68: 0.82982537150383
[08/27/2025 19:45:57 INFO]: Training accuracy: {
    "score": -1.0198037069507548,
    "rmse": 1.0198037069507548
}
[08/27/2025 19:45:57 INFO]: Val accuracy: {
    "score": -0.6636294269282988,
    "rmse": 0.6636294269282988
}
[08/27/2025 19:45:57 INFO]: Test accuracy: {
    "score": -0.8807240113957939,
    "rmse": 0.8807240113957939
}
[08/27/2025 19:45:57 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_27",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8807240113957939,
        "rmse": 0.8807240113957939
    },
    "train_stats": {
        "score": -1.0198037069507548,
        "rmse": 1.0198037069507548
    },
    "val_stats": {
        "score": -0.6636294269282988,
        "rmse": 0.6636294269282988
    }
}
[08/27/2025 19:45:57 INFO]: Procewss finished for trial blotchy-Amado_trial_27
[08/27/2025 19:45:57 INFO]: 
_________________________________________________

[08/27/2025 19:45:57 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:45:57 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 1.7065334739918945
  attention_dropout: 0.4129915440430638
  ffn_dropout: 0.4129915440430638
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.7695880964055785e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_51

[08/27/2025 19:45:57 INFO]: This ft_transformer has 8.903 million parameters.
[08/27/2025 19:45:57 INFO]: Training will start at epoch 0.
[08/27/2025 19:45:57 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:46:35 INFO]: Training loss at epoch 37: 0.8517997562885284
[08/27/2025 19:47:02 INFO]: Training loss at epoch 13: 1.11822509765625
[08/27/2025 19:47:23 INFO]: Training loss at epoch 6: 0.9072831869125366
[08/27/2025 19:47:36 INFO]: Training loss at epoch 38: 0.7135279178619385
[08/27/2025 19:47:57 INFO]: Training loss at epoch 34: 0.7483470439910889
[08/27/2025 19:47:57 INFO]: Training loss at epoch 9: 1.1751937568187714
[08/27/2025 19:48:03 INFO]: Training loss at epoch 25: 1.0000951290130615
[08/27/2025 19:48:32 INFO]: Training loss at epoch 37: 0.8939981162548065
[08/27/2025 19:48:50 INFO]: Training loss at epoch 42: 1.0315040946006775
[08/27/2025 19:49:00 INFO]: Training loss at epoch 25: 0.9401803612709045
[08/27/2025 19:49:04 INFO]: Training loss at epoch 5: 1.224794179201126
[08/27/2025 19:49:34 INFO]: Training loss at epoch 38: 0.8171353042125702
[08/27/2025 19:49:34 INFO]: Training loss at epoch 12: 1.0715388357639313
[08/27/2025 19:49:48 INFO]: Training stats: {
    "score": -0.997121517535276,
    "rmse": 0.997121517535276
}
[08/27/2025 19:49:48 INFO]: Val stats: {
    "score": -0.6829179235013397,
    "rmse": 0.6829179235013397
}
[08/27/2025 19:49:48 INFO]: Test stats: {
    "score": -0.8741967578489195,
    "rmse": 0.8741967578489195
}
[08/27/2025 19:49:50 INFO]: Training loss at epoch 69: 0.9339639842510223
[08/27/2025 19:50:01 INFO]: Training loss at epoch 0: 0.9955354630947113
[08/27/2025 19:50:07 INFO]: Training loss at epoch 42: 1.1413540244102478
[08/27/2025 19:50:35 INFO]: New best epoch, val score: -0.6561362446850612
[08/27/2025 19:50:35 INFO]: Saving model to: blotchy-Amado_trial_51/model_best.pth
[08/27/2025 19:50:46 INFO]: Training loss at epoch 37: 1.0858839452266693
[08/27/2025 19:50:51 INFO]: Training loss at epoch 6: 0.998936116695404
[08/27/2025 19:51:21 INFO]: Training stats: {
    "score": -0.9936922312970378,
    "rmse": 0.9936922312970378
}
[08/27/2025 19:51:21 INFO]: Val stats: {
    "score": -0.7038681558957656,
    "rmse": 0.7038681558957656
}
[08/27/2025 19:51:21 INFO]: Test stats: {
    "score": -0.8914879596643441,
    "rmse": 0.8914879596643441
}
[08/27/2025 19:51:24 INFO]: Training loss at epoch 16: 1.0497779846191406
[08/27/2025 19:51:29 INFO]: Training loss at epoch 35: 1.0176609754562378
[08/27/2025 19:52:26 INFO]: Training loss at epoch 43: 0.9178743958473206
[08/27/2025 19:52:35 INFO]: Training loss at epoch 39: 0.865431547164917
[08/27/2025 19:53:04 INFO]: Training loss at epoch 39: 0.8011342585086823
[08/27/2025 19:53:32 INFO]: Training loss at epoch 21: 1.2895047068595886
[08/27/2025 19:53:39 INFO]: Training stats: {
    "score": -0.9039543984019325,
    "rmse": 0.9039543984019325
}
[08/27/2025 19:53:39 INFO]: Val stats: {
    "score": -0.7200428260249223,
    "rmse": 0.7200428260249223
}
[08/27/2025 19:53:39 INFO]: Test stats: {
    "score": -0.9364737585761211,
    "rmse": 0.9364737585761211
}
[08/27/2025 19:53:59 INFO]: Training loss at epoch 38: 0.6459473967552185
[08/27/2025 19:54:18 INFO]: Training loss at epoch 7: 1.041655272245407
[08/27/2025 19:54:27 INFO]: Training loss at epoch 6: 0.9807302951812744
[08/27/2025 19:54:43 INFO]: Training loss at epoch 1: 0.9672732651233673
[08/27/2025 19:54:52 INFO]: Training stats: {
    "score": -0.8498420248488394,
    "rmse": 0.8498420248488394
}
[08/27/2025 19:54:52 INFO]: Val stats: {
    "score": -0.7179873181254146,
    "rmse": 0.7179873181254146
}
[08/27/2025 19:54:52 INFO]: Test stats: {
    "score": -0.956663347008188,
    "rmse": 0.956663347008188
}
[08/27/2025 19:54:58 INFO]: Training loss at epoch 36: 0.8652813136577606
[08/27/2025 19:55:02 INFO]: Training loss at epoch 14: 1.0793886482715607
[08/27/2025 19:55:05 INFO]: New best epoch, val score: -0.6653100027797602
[08/27/2025 19:55:05 INFO]: Saving model to: blotchy-Amado_trial_48/model_best.pth
[08/27/2025 19:55:19 INFO]: Training loss at epoch 10: 1.005389392375946
[08/27/2025 19:55:35 INFO]: Training loss at epoch 43: 1.0128357410430908
[08/27/2025 19:55:37 INFO]: Training loss at epoch 70: 0.8820608556270599
[08/27/2025 19:55:58 INFO]: Training loss at epoch 44: 0.9041389524936676
[08/27/2025 19:56:07 INFO]: Running Final Evaluation...
[08/27/2025 19:56:18 INFO]: Training loss at epoch 23: 0.8821220397949219
[08/27/2025 19:56:22 INFO]: Training loss at epoch 7: 1.0287628471851349
[08/27/2025 19:56:39 INFO]: Training loss at epoch 40: 0.9738602340221405
[08/27/2025 19:56:42 INFO]: Training loss at epoch 38: 1.0364826023578644
[08/27/2025 19:57:17 INFO]: Training loss at epoch 26: 0.9648822546005249
[08/27/2025 19:57:26 INFO]: Training loss at epoch 13: 1.0230491757392883
[08/27/2025 19:57:41 INFO]: Training accuracy: {
    "score": -1.0031948160606659,
    "rmse": 1.0031948160606659
}
[08/27/2025 19:57:41 INFO]: Val accuracy: {
    "score": -0.6672145029495326,
    "rmse": 0.6672145029495326
}
[08/27/2025 19:57:41 INFO]: Test accuracy: {
    "score": -0.8770149319171816,
    "rmse": 0.8770149319171816
}
[08/27/2025 19:57:41 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_19",
    "best_epoch": 39,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8770149319171816,
        "rmse": 0.8770149319171816
    },
    "train_stats": {
        "score": -1.0031948160606659,
        "rmse": 1.0031948160606659
    },
    "val_stats": {
        "score": -0.6672145029495326,
        "rmse": 0.6672145029495326
    }
}
[08/27/2025 19:57:41 INFO]: Procewss finished for trial blotchy-Amado_trial_19
[08/27/2025 19:57:41 INFO]: 
_________________________________________________

[08/27/2025 19:57:41 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:57:41 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 1.7516793585477095
  attention_dropout: 0.3913762098493244
  ffn_dropout: 0.3913762098493244
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.906924858968131e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_52

[08/27/2025 19:57:41 INFO]: This ft_transformer has 9.008 million parameters.
[08/27/2025 19:57:41 INFO]: Training will start at epoch 0.
[08/27/2025 19:57:41 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:57:44 INFO]: Training loss at epoch 26: 1.0762858986854553
[08/27/2025 19:58:18 INFO]: Training loss at epoch 17: 1.117042899131775
[08/27/2025 19:58:33 INFO]: Training loss at epoch 37: 0.9601578414440155
[08/27/2025 19:59:23 INFO]: Training loss at epoch 39: 0.7936330437660217
[08/27/2025 19:59:26 INFO]: Training loss at epoch 2: 1.1792618036270142
[08/27/2025 19:59:36 INFO]: Training loss at epoch 45: 0.8989370763301849
[08/27/2025 19:59:43 INFO]: Training loss at epoch 41: 0.6533707082271576
[08/27/2025 19:59:49 INFO]: Training loss at epoch 7: 1.2413618564605713
[08/27/2025 20:00:22 INFO]: Training loss at epoch 40: 0.7449670732021332
[08/27/2025 20:00:51 INFO]: Training loss at epoch 11: 0.8537225425243378
[08/27/2025 20:01:03 INFO]: Training loss at epoch 44: 1.1067784130573273
[08/27/2025 20:01:09 INFO]: Training loss at epoch 8: 0.963805764913559
[08/27/2025 20:01:11 INFO]: Training stats: {
    "score": -0.8571140168240746,
    "rmse": 0.8571140168240746
}
[08/27/2025 20:01:11 INFO]: Val stats: {
    "score": -0.6875577156722893,
    "rmse": 0.6875577156722893
}
[08/27/2025 20:01:11 INFO]: Test stats: {
    "score": -0.9433235452099283,
    "rmse": 0.9433235452099283
}
[08/27/2025 20:01:49 INFO]: Training loss at epoch 0: 0.9112406075000763
[08/27/2025 20:01:53 INFO]: Training loss at epoch 8: 1.2601152062416077
[08/27/2025 20:01:59 INFO]: New best epoch, val score: -0.6634139814154001
[08/27/2025 20:01:59 INFO]: Saving model to: blotchy-Amado_trial_48/model_best.pth
[08/27/2025 20:02:02 INFO]: Training loss at epoch 38: 0.9732806086540222
[08/27/2025 20:02:24 INFO]: New best epoch, val score: -0.6740570652019983
[08/27/2025 20:02:24 INFO]: Saving model to: blotchy-Amado_trial_52/model_best.pth
[08/27/2025 20:02:38 INFO]: Training loss at epoch 39: 0.8341670632362366
[08/27/2025 20:02:45 INFO]: Training loss at epoch 42: 1.001170575618744
[08/27/2025 20:03:07 INFO]: Training loss at epoch 15: 0.9375141561031342
[08/27/2025 20:03:09 INFO]: Training loss at epoch 46: 0.8507616221904755
[08/27/2025 20:04:04 INFO]: Training loss at epoch 3: 1.2129912972450256
[08/27/2025 20:04:31 INFO]: Training loss at epoch 22: 0.927170604467392
[08/27/2025 20:04:37 INFO]: New best epoch, val score: -0.6556217804877372
[08/27/2025 20:04:37 INFO]: Saving model to: blotchy-Amado_trial_51/model_best.pth
[08/27/2025 20:04:39 INFO]: Training stats: {
    "score": -1.0024917951809713,
    "rmse": 1.0024917951809713
}
[08/27/2025 20:04:39 INFO]: Val stats: {
    "score": -0.7063524648860069,
    "rmse": 0.7063524648860069
}
[08/27/2025 20:04:39 INFO]: Test stats: {
    "score": -0.8881342975570654,
    "rmse": 0.8881342975570654
}
[08/27/2025 20:05:09 INFO]: Training loss at epoch 18: 0.8561123311519623
[08/27/2025 20:05:09 INFO]: Training loss at epoch 8: 1.0226487219333649
[08/27/2025 20:05:19 INFO]: Training loss at epoch 14: 1.0201089084148407
[08/27/2025 20:05:33 INFO]: Training loss at epoch 39: 0.8499506115913391
[08/27/2025 20:05:43 INFO]: Training loss at epoch 43: 0.8590309917926788
[08/27/2025 20:05:49 INFO]: Training loss at epoch 41: 0.7937816977500916
[08/27/2025 20:06:23 INFO]: Training loss at epoch 12: 0.9965317249298096
[08/27/2025 20:06:25 INFO]: Training loss at epoch 27: 1.1931504309177399
[08/27/2025 20:06:28 INFO]: Training loss at epoch 45: 1.042662799358368
[08/27/2025 20:06:29 INFO]: Training loss at epoch 27: 0.9238340556621552
[08/27/2025 20:06:32 INFO]: Training loss at epoch 1: 0.9829021990299225
[08/27/2025 20:06:34 INFO]: Training loss at epoch 40: 0.9025834202766418
[08/27/2025 20:06:44 INFO]: Training loss at epoch 47: 0.8052701056003571
[08/27/2025 20:06:46 INFO]: Training stats: {
    "score": -0.9251909971891273,
    "rmse": 0.9251909971891273
}
[08/27/2025 20:06:46 INFO]: Val stats: {
    "score": -0.6760823592299804,
    "rmse": 0.6760823592299804
}
[08/27/2025 20:06:46 INFO]: Test stats: {
    "score": -0.8893437839472785,
    "rmse": 0.8893437839472785
}
[08/27/2025 20:07:26 INFO]: Training loss at epoch 9: 1.0636174082756042
[08/27/2025 20:08:01 INFO]: Training loss at epoch 9: 1.0679101347923279
[08/27/2025 20:08:49 INFO]: Training loss at epoch 44: 0.8651754856109619
[08/27/2025 20:08:49 INFO]: Training loss at epoch 4: 1.079653024673462
[08/27/2025 20:09:16 INFO]: Training stats: {
    "score": -1.0669200607732028,
    "rmse": 1.0669200607732028
}
[08/27/2025 20:09:16 INFO]: Val stats: {
    "score": -0.8526157393717891,
    "rmse": 0.8526157393717891
}
[08/27/2025 20:09:16 INFO]: Test stats: {
    "score": -0.988686732923799,
    "rmse": 0.988686732923799
}
[08/27/2025 20:09:16 INFO]: Training loss at epoch 24: 0.9313807487487793
[08/27/2025 20:10:17 INFO]: Training loss at epoch 40: 0.7850221693515778
[08/27/2025 20:10:20 INFO]: Training loss at epoch 48: 1.0036432147026062
[08/27/2025 20:10:21 INFO]: Training stats: {
    "score": -1.0147046356333589,
    "rmse": 1.0147046356333589
}
[08/27/2025 20:10:21 INFO]: Val stats: {
    "score": -0.6623077739791723,
    "rmse": 0.6623077739791723
}
[08/27/2025 20:10:21 INFO]: Test stats: {
    "score": -0.8771667446859585,
    "rmse": 0.8771667446859585
}
[08/27/2025 20:10:30 INFO]: Training loss at epoch 9: 0.9822489321231842
[08/27/2025 20:10:34 INFO]: Training loss at epoch 40: 1.1106666326522827
[08/27/2025 20:11:10 INFO]: New best epoch, val score: -0.6623077739791723
[08/27/2025 20:11:10 INFO]: Saving model to: blotchy-Amado_trial_48/model_best.pth
[08/27/2025 20:11:11 INFO]: Training loss at epoch 16: 1.0044901371002197
[08/27/2025 20:11:14 INFO]: Training loss at epoch 2: 1.1547473073005676
[08/27/2025 20:11:20 INFO]: Training loss at epoch 42: 0.8674938380718231
[08/27/2025 20:11:47 INFO]: New best epoch, val score: -0.6652318328183641
[08/27/2025 20:11:47 INFO]: Saving model to: blotchy-Amado_trial_52/model_best.pth
[08/27/2025 20:11:48 INFO]: Training loss at epoch 45: 0.7826085090637207
[08/27/2025 20:11:55 INFO]: Training loss at epoch 13: 1.0116381347179413
[08/27/2025 20:11:56 INFO]: Training loss at epoch 46: 1.0003259778022766
[08/27/2025 20:12:00 INFO]: Training loss at epoch 41: 0.7834498584270477
[08/27/2025 20:12:03 INFO]: Training loss at epoch 19: 0.8472022712230682
[08/27/2025 20:12:16 INFO]: Training stats: {
    "score": -0.9997228835508052,
    "rmse": 0.9997228835508052
}
[08/27/2025 20:12:16 INFO]: Val stats: {
    "score": -0.6804289888338132,
    "rmse": 0.6804289888338132
}
[08/27/2025 20:12:16 INFO]: Test stats: {
    "score": -0.8721930824157754,
    "rmse": 0.8721930824157754
}
[08/27/2025 20:13:11 INFO]: Training loss at epoch 15: 1.2671796679496765
[08/27/2025 20:13:26 INFO]: Training loss at epoch 5: 0.8240222334861755
[08/27/2025 20:13:46 INFO]: Training loss at epoch 41: 1.1798692047595978
[08/27/2025 20:13:51 INFO]: Training loss at epoch 49: 0.8715713024139404
[08/27/2025 20:14:24 INFO]: Training stats: {
    "score": -0.9861665053209132,
    "rmse": 0.9861665053209132
}
[08/27/2025 20:14:24 INFO]: Val stats: {
    "score": -0.7066281183879416,
    "rmse": 0.7066281183879416
}
[08/27/2025 20:14:24 INFO]: Test stats: {
    "score": -0.8790695267681627,
    "rmse": 0.8790695267681627
}
[08/27/2025 20:14:42 INFO]: Training loss at epoch 10: 1.2151201963424683
[08/27/2025 20:14:49 INFO]: Training loss at epoch 46: 0.9698268175125122
[08/27/2025 20:15:06 INFO]: Training stats: {
    "score": -0.9230277494411359,
    "rmse": 0.9230277494411359
}
[08/27/2025 20:15:06 INFO]: Val stats: {
    "score": -0.729362475121631,
    "rmse": 0.729362475121631
}
[08/27/2025 20:15:06 INFO]: Test stats: {
    "score": -0.931073051526725,
    "rmse": 0.931073051526725
}
[08/27/2025 20:15:10 INFO]: Training loss at epoch 28: 1.0172228813171387
[08/27/2025 20:15:21 INFO]: New best epoch, val score: -0.7344844690752317
[08/27/2025 20:15:21 INFO]: Saving model to: blotchy-Amado_trial_49/model_best.pth
[08/27/2025 20:15:30 INFO]: Training loss at epoch 23: 1.1471385061740875
[08/27/2025 20:15:42 INFO]: Training loss at epoch 28: 0.9639756381511688
[08/27/2025 20:15:55 INFO]: Training loss at epoch 3: 1.2951177954673767
[08/27/2025 20:16:26 INFO]: Training loss at epoch 41: 0.9840640127658844
[08/27/2025 20:16:46 INFO]: Training loss at epoch 43: 0.8890928626060486
[08/27/2025 20:16:47 INFO]: New best epoch, val score: -0.6639823131451781
[08/27/2025 20:16:47 INFO]: Saving model to: blotchy-Amado_trial_30/model_best.pth
[08/27/2025 20:17:11 INFO]: Training loss at epoch 10: 1.1560871005058289
[08/27/2025 20:17:19 INFO]: Training loss at epoch 42: 0.9027900397777557
[08/27/2025 20:17:20 INFO]: Training loss at epoch 47: 1.0053784251213074
[08/27/2025 20:17:24 INFO]: Training loss at epoch 42: 1.0400154292583466
[08/27/2025 20:17:24 INFO]: Training loss at epoch 14: 1.0727173686027527
[08/27/2025 20:17:36 INFO]: Training loss at epoch 10: 0.8986558318138123
[08/27/2025 20:17:45 INFO]: Running Final Evaluation...
[08/27/2025 20:17:50 INFO]: Training loss at epoch 47: 0.7008320838212967
[08/27/2025 20:18:07 INFO]: Training loss at epoch 6: 1.0000030398368835
[08/27/2025 20:18:39 INFO]: Training loss at epoch 50: 0.8865915238857269
[08/27/2025 20:19:00 INFO]: Training accuracy: {
    "score": -0.9975331416591904,
    "rmse": 0.9975331416591904
}
[08/27/2025 20:19:00 INFO]: Val accuracy: {
    "score": -0.6584429093820511,
    "rmse": 0.6584429093820511
}
[08/27/2025 20:19:00 INFO]: Test accuracy: {
    "score": -0.8704946268446544,
    "rmse": 0.8704946268446544
}
[08/27/2025 20:19:01 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_42",
    "best_epoch": 11,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8704946268446544,
        "rmse": 0.8704946268446544
    },
    "train_stats": {
        "score": -0.9975331416591904,
        "rmse": 0.9975331416591904
    },
    "val_stats": {
        "score": -0.6584429093820511,
        "rmse": 0.6584429093820511
    }
}
[08/27/2025 20:19:01 INFO]: Procewss finished for trial blotchy-Amado_trial_42
[08/27/2025 20:19:01 INFO]: 
_________________________________________________

[08/27/2025 20:19:01 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:19:01 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 2.3050857845827197
  attention_dropout: 0.39058754494991954
  ffn_dropout: 0.39058754494991954
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 7.476297528114388e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_53

[08/27/2025 20:19:01 INFO]: This ft_transformer has 10.318 million parameters.
[08/27/2025 20:19:01 INFO]: Training will start at epoch 0.
[08/27/2025 20:19:01 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:19:14 INFO]: Training loss at epoch 17: 0.8544799089431763
[08/27/2025 20:20:14 INFO]: Training loss at epoch 11: 1.342622697353363
[08/27/2025 20:20:38 INFO]: Training loss at epoch 4: 0.9724354147911072
[08/27/2025 20:20:53 INFO]: New best epoch, val score: -0.6692641760330781
[08/27/2025 20:20:53 INFO]: Saving model to: blotchy-Amado_trial_49/model_best.pth
[08/27/2025 20:20:55 INFO]: Training loss at epoch 48: 0.8615202903747559
[08/27/2025 20:21:03 INFO]: Training loss at epoch 16: 0.8684853613376617
[08/27/2025 20:21:17 INFO]: Training loss at epoch 20: 0.9381200075149536
[08/27/2025 20:22:06 INFO]: Training loss at epoch 25: 1.231969267129898
[08/27/2025 20:22:12 INFO]: Training loss at epoch 51: 0.8628110587596893
[08/27/2025 20:22:13 INFO]: Training loss at epoch 44: 0.5922295898199081
[08/27/2025 20:22:19 INFO]: Training loss at epoch 42: 0.917155385017395
[08/27/2025 20:22:42 INFO]: Training loss at epoch 48: 0.9664374887943268
[08/27/2025 20:22:42 INFO]: Training loss at epoch 7: 0.9305134117603302
[08/27/2025 20:22:44 INFO]: Training loss at epoch 43: 0.7512329518795013
[08/27/2025 20:22:51 INFO]: Training loss at epoch 15: 1.0505177974700928
[08/27/2025 20:22:55 INFO]: Training loss at epoch 11: 1.1462202668190002
[08/27/2025 20:23:35 INFO]: Training loss at epoch 0: 0.9614489376544952
[08/27/2025 20:23:48 INFO]: Training loss at epoch 29: 1.0432094931602478
[08/27/2025 20:23:51 INFO]: Training loss at epoch 49: 0.6592624187469482
[08/27/2025 20:23:58 INFO]: Training loss at epoch 11: 0.8612600564956665
[08/27/2025 20:24:12 INFO]: New best epoch, val score: -0.6816744947395772
[08/27/2025 20:24:12 INFO]: Saving model to: blotchy-Amado_trial_53/model_best.pth
[08/27/2025 20:24:52 INFO]: Training loss at epoch 29: 0.9052364230155945
[08/27/2025 20:24:53 INFO]: Training stats: {
    "score": -0.892096424461614,
    "rmse": 0.892096424461614
}
[08/27/2025 20:24:53 INFO]: Val stats: {
    "score": -0.6989177973207517,
    "rmse": 0.6989177973207517
}
[08/27/2025 20:24:53 INFO]: Test stats: {
    "score": -0.944984999821868,
    "rmse": 0.944984999821868
}
[08/27/2025 20:25:15 INFO]: Training loss at epoch 5: 1.1756342053413391
[08/27/2025 20:25:40 INFO]: Training loss at epoch 12: 0.9839732646942139
[08/27/2025 20:25:46 INFO]: Training loss at epoch 52: 0.9215708076953888
[08/27/2025 20:26:12 INFO]: Running Final Evaluation...
[08/27/2025 20:26:19 INFO]: New best epoch, val score: -0.6617025051766402
[08/27/2025 20:26:19 INFO]: Saving model to: blotchy-Amado_trial_49/model_best.pth
[08/27/2025 20:26:26 INFO]: Training loss at epoch 24: 0.8478196561336517
[08/27/2025 20:26:47 INFO]: Training stats: {
    "score": -1.0015480433648645,
    "rmse": 1.0015480433648645
}
[08/27/2025 20:26:47 INFO]: Val stats: {
    "score": -0.6980033791611617,
    "rmse": 0.6980033791611617
}
[08/27/2025 20:26:47 INFO]: Test stats: {
    "score": -0.8832595607867324,
    "rmse": 0.8832595607867324
}
[08/27/2025 20:27:11 INFO]: Training loss at epoch 18: 1.0875205993652344
[08/27/2025 20:27:20 INFO]: Training loss at epoch 8: 1.0819169282913208
[08/27/2025 20:27:25 INFO]: Training accuracy: {
    "score": -0.9773351361612725,
    "rmse": 0.9773351361612725
}
[08/27/2025 20:27:25 INFO]: Val accuracy: {
    "score": -0.66805771895145,
    "rmse": 0.66805771895145
}
[08/27/2025 20:27:25 INFO]: Test accuracy: {
    "score": -0.8850040588985488,
    "rmse": 0.8850040588985488
}
[08/27/2025 20:27:25 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_41",
    "best_epoch": 21,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8850040588985488,
        "rmse": 0.8850040588985488
    },
    "train_stats": {
        "score": -0.9773351361612725,
        "rmse": 0.9773351361612725
    },
    "val_stats": {
        "score": -0.66805771895145,
        "rmse": 0.66805771895145
    }
}
[08/27/2025 20:27:25 INFO]: Procewss finished for trial blotchy-Amado_trial_41
[08/27/2025 20:27:25 INFO]: 
_________________________________________________

[08/27/2025 20:27:25 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:27:25 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 1.6911824364902832
  attention_dropout: 0.39345871430832674
  ffn_dropout: 0.39345871430832674
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.529253779554126e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_54

[08/27/2025 20:27:25 INFO]: This ft_transformer has 8.866 million parameters.
[08/27/2025 20:27:25 INFO]: Training will start at epoch 0.
[08/27/2025 20:27:25 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:27:36 INFO]: Training loss at epoch 45: 0.6545730829238892
[08/27/2025 20:27:51 INFO]: Training loss at epoch 50: 1.0616421103477478
[08/27/2025 20:27:54 INFO]: Training stats: {
    "score": -0.9999074012633203,
    "rmse": 0.9999074012633203
}
[08/27/2025 20:27:54 INFO]: Val stats: {
    "score": -0.6664767260298434,
    "rmse": 0.6664767260298434
}
[08/27/2025 20:27:54 INFO]: Test stats: {
    "score": -0.8698084784293545,
    "rmse": 0.8698084784293545
}
[08/27/2025 20:28:03 INFO]: Training loss at epoch 21: 1.0535375773906708
[08/27/2025 20:28:06 INFO]: Training loss at epoch 44: 0.7681484818458557
[08/27/2025 20:28:07 INFO]: Training loss at epoch 49: 0.8447644710540771
[08/27/2025 20:28:09 INFO]: Training loss at epoch 43: 1.0278094410896301
[08/27/2025 20:28:13 INFO]: Training loss at epoch 12: 1.1491668224334717
[08/27/2025 20:28:20 INFO]: Training loss at epoch 16: 0.8257231712341309
[08/27/2025 20:28:48 INFO]: Training loss at epoch 17: 1.2424888908863068
[08/27/2025 20:28:49 INFO]: Training loss at epoch 1: 1.0121777653694153
[08/27/2025 20:29:45 INFO]: New best epoch, val score: -0.6366015019720523
[08/27/2025 20:29:45 INFO]: Saving model to: blotchy-Amado_trial_46/model_best.pth
[08/27/2025 20:29:57 INFO]: Training loss at epoch 6: 1.3676735162734985
[08/27/2025 20:30:01 INFO]: Training stats: {
    "score": -0.9992057376967629,
    "rmse": 0.9992057376967629
}
[08/27/2025 20:30:01 INFO]: Val stats: {
    "score": -0.6856225122134264,
    "rmse": 0.6856225122134264
}
[08/27/2025 20:30:01 INFO]: Test stats: {
    "score": -0.876532169726884,
    "rmse": 0.876532169726884
}
[08/27/2025 20:30:48 INFO]: Training loss at epoch 12: 1.215596616268158
[08/27/2025 20:30:56 INFO]: Training loss at epoch 51: 0.8990393280982971
[08/27/2025 20:31:11 INFO]: Training loss at epoch 13: 0.9264807999134064
[08/27/2025 20:31:34 INFO]: Training loss at epoch 0: 1.0487940311431885
[08/27/2025 20:32:04 INFO]: Training loss at epoch 9: 0.9284988939762115
[08/27/2025 20:32:08 INFO]: New best epoch, val score: -0.791820602518314
[08/27/2025 20:32:08 INFO]: Saving model to: blotchy-Amado_trial_54/model_best.pth
[08/27/2025 20:33:05 INFO]: Training loss at epoch 46: 1.0646610856056213
[08/27/2025 20:33:28 INFO]: Training loss at epoch 45: 0.8911464512348175
[08/27/2025 20:33:33 INFO]: Training loss at epoch 13: 1.342406451702118
[08/27/2025 20:33:33 INFO]: Training stats: {
    "score": -1.0290387184905638,
    "rmse": 1.0290387184905638
}
[08/27/2025 20:33:33 INFO]: Val stats: {
    "score": -0.6600095436759494,
    "rmse": 0.6600095436759494
}
[08/27/2025 20:33:33 INFO]: Test stats: {
    "score": -0.8805388653058724,
    "rmse": 0.8805388653058724
}
[08/27/2025 20:33:53 INFO]: Training loss at epoch 17: 0.9783485531806946
[08/27/2025 20:33:55 INFO]: Training loss at epoch 52: 0.7063997685909271
[08/27/2025 20:34:01 INFO]: Training loss at epoch 44: 0.9216422140598297
[08/27/2025 20:34:03 INFO]: Training loss at epoch 2: 1.1180108785629272
[08/27/2025 20:34:37 INFO]: Training loss at epoch 7: 1.0948049426078796
[08/27/2025 20:34:55 INFO]: Training loss at epoch 22: 1.1262821555137634
[08/27/2025 20:34:56 INFO]: Training loss at epoch 26: 0.9599406123161316
[08/27/2025 20:35:13 INFO]: Training loss at epoch 19: 0.8571878969669342
[08/27/2025 20:35:28 INFO]: Training loss at epoch 50: 0.7173630595207214
[08/27/2025 20:35:29 INFO]: Training loss at epoch 30: 0.9326838254928589
[08/27/2025 20:36:13 INFO]: Training loss at epoch 1: 1.210481345653534
[08/27/2025 20:36:40 INFO]: Training loss at epoch 18: 0.9351399540901184
[08/27/2025 20:36:43 INFO]: Training loss at epoch 14: 1.1391260623931885
[08/27/2025 20:36:58 INFO]: Training loss at epoch 53: 0.7308975160121918
[08/27/2025 20:37:09 INFO]: Training loss at epoch 30: 1.1585402488708496
[08/27/2025 20:37:24 INFO]: Training loss at epoch 25: 1.3066491484642029
[08/27/2025 20:37:39 INFO]: Training loss at epoch 13: 0.8450572490692139
[08/27/2025 20:37:56 INFO]: Training stats: {
    "score": -0.9801095890890829,
    "rmse": 0.9801095890890829
}
[08/27/2025 20:37:56 INFO]: Val stats: {
    "score": -0.6910442850214771,
    "rmse": 0.6910442850214771
}
[08/27/2025 20:37:56 INFO]: Test stats: {
    "score": -0.9021087441657747,
    "rmse": 0.9021087441657747
}
[08/27/2025 20:38:13 INFO]: Training loss at epoch 10: 1.0046446919441223
[08/27/2025 20:38:31 INFO]: Training loss at epoch 47: 0.7924573421478271
[08/27/2025 20:38:50 INFO]: Training loss at epoch 46: 0.800655335187912
[08/27/2025 20:38:52 INFO]: Training loss at epoch 14: 1.082287311553955
[08/27/2025 20:39:15 INFO]: Training loss at epoch 3: 1.1197100281715393
[08/27/2025 20:39:22 INFO]: Training loss at epoch 8: 0.863318920135498
[08/27/2025 20:39:22 INFO]: Training loss at epoch 18: 0.8115330636501312
[08/27/2025 20:39:30 INFO]: New best epoch, val score: -0.6635863575891224
[08/27/2025 20:39:30 INFO]: Saving model to: blotchy-Amado_trial_50/model_best.pth
[08/27/2025 20:39:53 INFO]: Training loss at epoch 45: 1.0048101544380188
[08/27/2025 20:39:53 INFO]: New best epoch, val score: -0.6707146049051751
[08/27/2025 20:39:54 INFO]: Saving model to: blotchy-Amado_trial_53/model_best.pth
[08/27/2025 20:39:58 INFO]: Training loss at epoch 54: 0.801121175289154
[08/27/2025 20:40:56 INFO]: Training loss at epoch 2: 1.1360145807266235
[08/27/2025 20:41:00 INFO]: Training loss at epoch 51: 0.9044695198535919
[08/27/2025 20:41:28 INFO]: New best epoch, val score: -0.6598341576351207
[08/27/2025 20:41:28 INFO]: Saving model to: blotchy-Amado_trial_54/model_best.pth
[08/27/2025 20:41:48 INFO]: Training loss at epoch 23: 1.0656988322734833
[08/27/2025 20:42:12 INFO]: Training loss at epoch 15: 1.084511160850525
[08/27/2025 20:42:53 INFO]: Training loss at epoch 11: 0.9961197972297668
[08/27/2025 20:42:59 INFO]: Training loss at epoch 55: 0.7918472290039062
[08/27/2025 20:43:58 INFO]: Training loss at epoch 48: 0.7694951295852661
[08/27/2025 20:44:05 INFO]: Training loss at epoch 9: 0.92880979180336
[08/27/2025 20:44:07 INFO]: Training loss at epoch 31: 1.0577791929244995
[08/27/2025 20:44:14 INFO]: Training loss at epoch 15: 0.8837412595748901
[08/27/2025 20:44:16 INFO]: Training loss at epoch 47: 0.834595650434494
[08/27/2025 20:44:27 INFO]: Training loss at epoch 14: 1.0175662338733673
[08/27/2025 20:44:29 INFO]: Training loss at epoch 19: 0.8001397252082825
[08/27/2025 20:44:30 INFO]: Training loss at epoch 4: 1.1476444005966187
[08/27/2025 20:44:52 INFO]: New best epoch, val score: -0.6629075605029644
[08/27/2025 20:44:52 INFO]: Saving model to: blotchy-Amado_trial_50/model_best.pth
[08/27/2025 20:44:56 INFO]: Training loss at epoch 19: 1.1582890450954437
[08/27/2025 20:45:09 INFO]: New best epoch, val score: -0.6644392019349246
[08/27/2025 20:45:09 INFO]: Saving model to: blotchy-Amado_trial_53/model_best.pth
[08/27/2025 20:45:33 INFO]: Training loss at epoch 3: 1.014054387807846
[08/27/2025 20:45:41 INFO]: Training stats: {
    "score": -1.0095808710771919,
    "rmse": 1.0095808710771919
}
[08/27/2025 20:45:41 INFO]: Val stats: {
    "score": -0.7295825906656396,
    "rmse": 0.7295825906656396
}
[08/27/2025 20:45:41 INFO]: Test stats: {
    "score": -0.9026027252257005,
    "rmse": 0.9026027252257005
}
[08/27/2025 20:45:48 INFO]: Training loss at epoch 46: 1.111295998096466
[08/27/2025 20:45:58 INFO]: Training loss at epoch 20: 0.9997406303882599
[08/27/2025 20:46:02 INFO]: Training loss at epoch 56: 0.6394851207733154
[08/27/2025 20:46:20 INFO]: Training loss at epoch 31: 1.3116301894187927
[08/27/2025 20:46:28 INFO]: Training loss at epoch 52: 0.9671544134616852
[08/27/2025 20:46:50 INFO]: Training stats: {
    "score": -0.9964087451495622,
    "rmse": 0.9964087451495622
}
[08/27/2025 20:46:50 INFO]: Val stats: {
    "score": -0.678029929338138,
    "rmse": 0.678029929338138
}
[08/27/2025 20:46:50 INFO]: Test stats: {
    "score": -0.871474792879481,
    "rmse": 0.871474792879481
}
[08/27/2025 20:47:17 INFO]: Training stats: {
    "score": -0.9717949825429159,
    "rmse": 0.9717949825429159
}
[08/27/2025 20:47:17 INFO]: Val stats: {
    "score": -0.672741776817208,
    "rmse": 0.672741776817208
}
[08/27/2025 20:47:17 INFO]: Test stats: {
    "score": -0.9073126919970049,
    "rmse": 0.9073126919970049
}
[08/27/2025 20:47:38 INFO]: Training loss at epoch 12: 1.265970528125763
[08/27/2025 20:47:46 INFO]: Training loss at epoch 16: 1.209408164024353
[08/27/2025 20:47:52 INFO]: Training loss at epoch 27: 1.1855152547359467
[08/27/2025 20:48:22 INFO]: New best epoch, val score: -0.659986825948019
[08/27/2025 20:48:22 INFO]: Saving model to: blotchy-Amado_trial_49/model_best.pth
[08/27/2025 20:48:22 INFO]: Training loss at epoch 26: 1.0751188397407532
[08/27/2025 20:48:42 INFO]: Training loss at epoch 24: 1.0750492811203003
[08/27/2025 20:49:04 INFO]: Training loss at epoch 57: 0.9701091945171356
[08/27/2025 20:49:28 INFO]: Training loss at epoch 49: 0.6596058011054993
[08/27/2025 20:49:36 INFO]: Training loss at epoch 16: 0.9755240678787231
[08/27/2025 20:49:39 INFO]: New best epoch, val score: -0.6611680577575183
[08/27/2025 20:49:39 INFO]: Saving model to: blotchy-Amado_trial_31/model_best.pth
[08/27/2025 20:49:44 INFO]: Training loss at epoch 48: 0.7937928140163422
[08/27/2025 20:49:52 INFO]: Training loss at epoch 5: 1.0716412663459778
[08/27/2025 20:50:17 INFO]: Training loss at epoch 4: 1.459447205066681
[08/27/2025 20:50:18 INFO]: New best epoch, val score: -0.6627776115450762
[08/27/2025 20:50:18 INFO]: Saving model to: blotchy-Amado_trial_50/model_best.pth
[08/27/2025 20:50:25 INFO]: Training loss at epoch 10: 0.9586242437362671
[08/27/2025 20:50:30 INFO]: New best epoch, val score: -0.6640425178148248
[08/27/2025 20:50:30 INFO]: Saving model to: blotchy-Amado_trial_53/model_best.pth
[08/27/2025 20:51:22 INFO]: Training stats: {
    "score": -0.7933150679700194,
    "rmse": 0.7933150679700194
}
[08/27/2025 20:51:22 INFO]: Val stats: {
    "score": -0.749812079896537,
    "rmse": 0.749812079896537
}
[08/27/2025 20:51:22 INFO]: Test stats: {
    "score": -0.9838777310763946,
    "rmse": 0.9838777310763946
}
[08/27/2025 20:51:23 INFO]: Training loss at epoch 15: 1.1587054133415222
[08/27/2025 20:51:40 INFO]: Training loss at epoch 47: 1.0095731317996979
[08/27/2025 20:51:55 INFO]: Training loss at epoch 53: 0.9287473559379578
[08/27/2025 20:52:08 INFO]: Training loss at epoch 58: 0.8205916583538055
[08/27/2025 20:52:18 INFO]: Training loss at epoch 13: 1.215156078338623
[08/27/2025 20:52:21 INFO]: Training loss at epoch 20: 0.9870187342166901
[08/27/2025 20:52:22 INFO]: Running Final Evaluation...
[08/27/2025 20:52:54 INFO]: Training loss at epoch 32: 0.8198463916778564
[08/27/2025 20:53:17 INFO]: Training loss at epoch 17: 1.0850791335105896
[08/27/2025 20:53:56 INFO]: New best epoch, val score: -0.6606131532355297
[08/27/2025 20:53:56 INFO]: Saving model to: blotchy-Amado_trial_35/model_best.pth
[08/27/2025 20:54:05 INFO]: Training loss at epoch 21: 0.9461137652397156
[08/27/2025 20:54:33 INFO]: Training accuracy: {
    "score": -1.014082454489063,
    "rmse": 1.014082454489063
}
[08/27/2025 20:54:33 INFO]: Val accuracy: {
    "score": -0.6605598774497623,
    "rmse": 0.6605598774497623
}
[08/27/2025 20:54:33 INFO]: Test accuracy: {
    "score": -0.8716768846505059,
    "rmse": 0.8716768846505059
}
[08/27/2025 20:54:33 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_36",
    "best_epoch": 16,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8716768846505059,
        "rmse": 0.8716768846505059
    },
    "train_stats": {
        "score": -1.014082454489063,
        "rmse": 1.014082454489063
    },
    "val_stats": {
        "score": -0.6605598774497623,
        "rmse": 0.6605598774497623
    }
}
[08/27/2025 20:54:33 INFO]: Procewss finished for trial blotchy-Amado_trial_36
[08/27/2025 20:54:33 INFO]: 
_________________________________________________

[08/27/2025 20:54:33 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:54:33 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 1.7596934597708445
  attention_dropout: 0.3954637499195786
  ffn_dropout: 0.3954637499195786
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 7.687080515632024e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_55

[08/27/2025 20:54:33 INFO]: This ft_transformer has 6.293 million parameters.
[08/27/2025 20:54:33 INFO]: Training will start at epoch 0.
[08/27/2025 20:54:33 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:54:55 INFO]: Training loss at epoch 5: 1.1429587006568909
[08/27/2025 20:54:59 INFO]: Training loss at epoch 17: 0.9530331194400787
[08/27/2025 20:55:05 INFO]: Training loss at epoch 11: 1.0187726616859436
[08/27/2025 20:55:07 INFO]: Training loss at epoch 6: 0.9969066381454468
[08/27/2025 20:55:07 INFO]: Training loss at epoch 20: 0.865044891834259
[08/27/2025 20:55:07 INFO]: Training loss at epoch 49: 0.8696503341197968
[08/27/2025 20:55:12 INFO]: Training loss at epoch 59: 0.7467467784881592
[08/27/2025 20:55:33 INFO]: Training loss at epoch 32: 1.1841658651828766
[08/27/2025 20:55:34 INFO]: Training loss at epoch 25: 1.0528503060340881
[08/27/2025 20:55:39 INFO]: New best epoch, val score: -0.6600191241143868
[08/27/2025 20:55:39 INFO]: Saving model to: blotchy-Amado_trial_52/model_best.pth
[08/27/2025 20:55:44 INFO]: New best epoch, val score: -0.6628723835133499
[08/27/2025 20:55:44 INFO]: Saving model to: blotchy-Amado_trial_53/model_best.pth
[08/27/2025 20:56:14 INFO]: Training stats: {
    "score": -0.8673794233092089,
    "rmse": 0.8673794233092089
}
[08/27/2025 20:56:14 INFO]: Val stats: {
    "score": -0.7391811063383302,
    "rmse": 0.7391811063383302
}
[08/27/2025 20:56:14 INFO]: Test stats: {
    "score": -0.9810468146590231,
    "rmse": 0.9810468146590231
}
[08/27/2025 20:56:45 INFO]: Training loss at epoch 50: 0.5944001823663712
[08/27/2025 20:56:57 INFO]: Training stats: {
    "score": -0.7981992857784764,
    "rmse": 0.7981992857784764
}
[08/27/2025 20:56:57 INFO]: Val stats: {
    "score": -0.7544354990651311,
    "rmse": 0.7544354990651311
}
[08/27/2025 20:56:57 INFO]: Test stats: {
    "score": -0.9902557615907964,
    "rmse": 0.9902557615907964
}
[08/27/2025 20:56:58 INFO]: Training loss at epoch 14: 1.0702964663505554
[08/27/2025 20:57:22 INFO]: Training loss at epoch 54: 0.7943432629108429
[08/27/2025 20:57:34 INFO]: Running Final Evaluation...
[08/27/2025 20:57:39 INFO]: Training loss at epoch 0: 0.9670988321304321
[08/27/2025 20:57:48 INFO]: Training loss at epoch 21: 0.9850980639457703
[08/27/2025 20:58:06 INFO]: New best epoch, val score: -0.7811901865297118
[08/27/2025 20:58:06 INFO]: Saving model to: blotchy-Amado_trial_55/model_best.pth
[08/27/2025 20:58:11 INFO]: Training loss at epoch 16: 1.1600558757781982
[08/27/2025 20:58:43 INFO]: Training loss at epoch 18: 1.1584337949752808
[08/27/2025 20:59:12 INFO]: Training loss at epoch 60: 0.7280254364013672
[08/27/2025 20:59:21 INFO]: Training loss at epoch 27: 1.1083192825317383
[08/27/2025 20:59:26 INFO]: Training accuracy: {
    "score": -0.9571497723112087,
    "rmse": 0.9571497723112087
}
[08/27/2025 20:59:26 INFO]: Val accuracy: {
    "score": -0.6535679620634941,
    "rmse": 0.6535679620634941
}
[08/27/2025 20:59:26 INFO]: Test accuracy: {
    "score": -0.8857225747902797,
    "rmse": 0.8857225747902797
}
[08/27/2025 20:59:27 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_38",
    "best_epoch": 18,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8857225747902797,
        "rmse": 0.8857225747902797
    },
    "train_stats": {
        "score": -0.9571497723112087,
        "rmse": 0.9571497723112087
    },
    "val_stats": {
        "score": -0.6535679620634941,
        "rmse": 0.6535679620634941
    }
}
[08/27/2025 20:59:27 INFO]: Procewss finished for trial blotchy-Amado_trial_38
[08/27/2025 20:59:27 INFO]: 
_________________________________________________

[08/27/2025 20:59:27 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:59:27 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 2.351841805281429
  attention_dropout: 0.3945396560820385
  ffn_dropout: 0.3945396560820385
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.309601404423079e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_56

[08/27/2025 20:59:27 INFO]: This ft_transformer has 7.262 million parameters.
[08/27/2025 20:59:27 INFO]: Training will start at epoch 0.
[08/27/2025 20:59:27 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:59:32 INFO]: Training loss at epoch 6: 0.8134653270244598
[08/27/2025 20:59:45 INFO]: Training loss at epoch 12: 1.4355974793434143
[08/27/2025 21:00:17 INFO]: Training loss at epoch 18: 1.373258113861084
[08/27/2025 21:00:20 INFO]: Training loss at epoch 7: 1.031544029712677
[08/27/2025 21:00:46 INFO]: Training loss at epoch 28: 0.8923884928226471
[08/27/2025 21:00:58 INFO]: New best epoch, val score: -0.661306170677831
[08/27/2025 21:00:58 INFO]: Saving model to: blotchy-Amado_trial_53/model_best.pth
[08/27/2025 21:01:16 INFO]: Training loss at epoch 1: 1.2710602283477783
[08/27/2025 21:01:33 INFO]: Training loss at epoch 33: 1.0368897318840027
[08/27/2025 21:01:36 INFO]: Training loss at epoch 15: 1.232677936553955
[08/27/2025 21:02:01 INFO]: Training loss at epoch 22: 1.0124459564685822
[08/27/2025 21:02:07 INFO]: New best epoch, val score: -0.6554564537094338
[08/27/2025 21:02:07 INFO]: Saving model to: blotchy-Amado_trial_51/model_best.pth
[08/27/2025 21:02:12 INFO]: Training loss at epoch 51: 0.5759115517139435
[08/27/2025 21:02:14 INFO]: Training loss at epoch 61: 0.728265255689621
[08/27/2025 21:02:21 INFO]: Training loss at epoch 26: 0.8500556647777557
[08/27/2025 21:02:44 INFO]: Training loss at epoch 55: 0.9941787719726562
[08/27/2025 21:02:55 INFO]: Training loss at epoch 21: 0.9240397214889526
[08/27/2025 21:02:55 INFO]: Training loss at epoch 0: 1.1517484784126282
[08/27/2025 21:03:16 INFO]: Training loss at epoch 22: 0.8608969748020172
[08/27/2025 21:03:24 INFO]: New best epoch, val score: -0.7249416645439593
[08/27/2025 21:03:24 INFO]: Saving model to: blotchy-Amado_trial_56/model_best.pth
[08/27/2025 21:04:07 INFO]: Training loss at epoch 7: 0.9651512503623962
[08/27/2025 21:04:08 INFO]: Training loss at epoch 19: 1.0974473357200623
[08/27/2025 21:04:23 INFO]: Training loss at epoch 13: 1.2248810529708862
[08/27/2025 21:04:39 INFO]: Training loss at epoch 33: 0.9435596764087677
[08/27/2025 21:04:47 INFO]: Training loss at epoch 2: 1.5244417786598206
[08/27/2025 21:04:57 INFO]: Training loss at epoch 17: 1.2570225596427917
[08/27/2025 21:05:13 INFO]: New best epoch, val score: -0.6695818044899323
[08/27/2025 21:05:13 INFO]: Saving model to: blotchy-Amado_trial_55/model_best.pth
[08/27/2025 21:05:16 INFO]: Training loss at epoch 62: 0.855705201625824
[08/27/2025 21:05:32 INFO]: Training loss at epoch 8: 1.1098347306251526
[08/27/2025 21:05:33 INFO]: Training loss at epoch 19: 0.9216397106647491
[08/27/2025 21:06:00 INFO]: Training stats: {
    "score": -1.0059978821792963,
    "rmse": 1.0059978821792963
}
[08/27/2025 21:06:00 INFO]: Val stats: {
    "score": -0.7208900197879761,
    "rmse": 0.7208900197879761
}
[08/27/2025 21:06:00 INFO]: Test stats: {
    "score": -0.8958309182289637,
    "rmse": 0.8958309182289637
}
[08/27/2025 21:06:10 INFO]: New best epoch, val score: -0.6605279712575003
[08/27/2025 21:06:10 INFO]: Saving model to: blotchy-Amado_trial_53/model_best.pth
[08/27/2025 21:06:16 INFO]: Training loss at epoch 16: 1.0530067682266235
[08/27/2025 21:06:57 INFO]: Training loss at epoch 1: 1.0824476778507233
[08/27/2025 21:07:20 INFO]: Training stats: {
    "score": -0.9996738387426612,
    "rmse": 0.9996738387426612
}
[08/27/2025 21:07:20 INFO]: Val stats: {
    "score": -0.6761804371167798,
    "rmse": 0.6761804371167798
}
[08/27/2025 21:07:20 INFO]: Test stats: {
    "score": -0.8702478336135987,
    "rmse": 0.8702478336135987
}
[08/27/2025 21:07:38 INFO]: Training loss at epoch 52: 0.8374808430671692
[08/27/2025 21:08:09 INFO]: Training loss at epoch 56: 0.9415348768234253
[08/27/2025 21:08:14 INFO]: Training loss at epoch 63: 0.8399408161640167
[08/27/2025 21:08:20 INFO]: Training loss at epoch 3: 0.9529624283313751
[08/27/2025 21:08:47 INFO]: Training loss at epoch 8: 1.0816037058830261
[08/27/2025 21:08:47 INFO]: Training loss at epoch 23: 0.921517938375473
[08/27/2025 21:09:05 INFO]: Training loss at epoch 14: 0.9736948013305664
[08/27/2025 21:09:11 INFO]: Training loss at epoch 27: 1.1028473377227783
[08/27/2025 21:10:02 INFO]: Training loss at epoch 23: 1.023060917854309
[08/27/2025 21:10:11 INFO]: Training loss at epoch 34: 1.2627264857292175
[08/27/2025 21:10:19 INFO]: Training loss at epoch 28: 1.1171507239341736
[08/27/2025 21:10:44 INFO]: Training loss at epoch 9: 0.8547980785369873
[08/27/2025 21:10:44 INFO]: Training loss at epoch 22: 0.9612127840518951
[08/27/2025 21:10:50 INFO]: Training loss at epoch 2: 1.251985490322113
[08/27/2025 21:10:52 INFO]: Training loss at epoch 17: 1.0912473797798157
[08/27/2025 21:11:15 INFO]: Training loss at epoch 64: 0.8703333139419556
[08/27/2025 21:11:27 INFO]: Training loss at epoch 20: 1.1251003444194794
[08/27/2025 21:11:41 INFO]: Training loss at epoch 18: 1.435233473777771
[08/27/2025 21:11:54 INFO]: Training loss at epoch 4: 1.1152597963809967
[08/27/2025 21:12:29 INFO]: Training stats: {
    "score": -1.004991367551737,
    "rmse": 1.004991367551737
}
[08/27/2025 21:12:29 INFO]: Val stats: {
    "score": -0.6615706622703411,
    "rmse": 0.6615706622703411
}
[08/27/2025 21:12:29 INFO]: Test stats: {
    "score": -0.8702351236787731,
    "rmse": 0.8702351236787731
}
[08/27/2025 21:12:36 INFO]: Training loss at epoch 20: 1.3022722005844116
[08/27/2025 21:13:04 INFO]: Training loss at epoch 53: 0.6447373628616333
[08/27/2025 21:13:30 INFO]: Training loss at epoch 9: 1.1364836692810059
[08/27/2025 21:13:34 INFO]: Training loss at epoch 29: 0.7320602834224701
[08/27/2025 21:13:36 INFO]: Training loss at epoch 57: 0.8384379744529724
[08/27/2025 21:13:49 INFO]: Training loss at epoch 15: 0.9210405051708221
[08/27/2025 21:13:50 INFO]: Training loss at epoch 34: 0.9364971220493317
[08/27/2025 21:14:17 INFO]: Training loss at epoch 24: 1.309253215789795
[08/27/2025 21:14:17 INFO]: Training loss at epoch 65: 0.8300392329692841
[08/27/2025 21:14:53 INFO]: Training loss at epoch 3: 1.126230537891388
[08/27/2025 21:15:03 INFO]: Training stats: {
    "score": -1.0222610727674013,
    "rmse": 1.0222610727674013
}
[08/27/2025 21:15:03 INFO]: Val stats: {
    "score": -0.6651122646637051,
    "rmse": 0.6651122646637051
}
[08/27/2025 21:15:03 INFO]: Test stats: {
    "score": -0.875931815977654,
    "rmse": 0.875931815977654
}
[08/27/2025 21:15:23 INFO]: New best epoch, val score: -0.6708647205117786
[08/27/2025 21:15:23 INFO]: Saving model to: blotchy-Amado_trial_56/model_best.pth
[08/27/2025 21:15:33 INFO]: Training loss at epoch 5: 0.9448224008083344
[08/27/2025 21:15:35 INFO]: Training loss at epoch 18: 1.2662821412086487
[08/27/2025 21:16:03 INFO]: Training loss at epoch 28: 1.085164725780487
[08/27/2025 21:17:01 INFO]: Training loss at epoch 21: 1.0226964354515076
[08/27/2025 21:17:23 INFO]: Training loss at epoch 66: 0.7924277484416962
[08/27/2025 21:17:45 INFO]: Running Final Evaluation...
[08/27/2025 21:17:47 INFO]: Training loss at epoch 10: 1.0057620704174042
[08/27/2025 21:17:55 INFO]: Training stats: {
    "score": -0.9212688052386138,
    "rmse": 0.9212688052386138
}
[08/27/2025 21:17:55 INFO]: Val stats: {
    "score": -0.7052546411556329,
    "rmse": 0.7052546411556329
}
[08/27/2025 21:17:55 INFO]: Test stats: {
    "score": -0.9098174525383076,
    "rmse": 0.9098174525383076
}
[08/27/2025 21:17:59 INFO]: Training loss at epoch 21: 1.0988713502883911
[08/27/2025 21:18:07 INFO]: Training loss at epoch 24: 0.8477743268013
[08/27/2025 21:18:35 INFO]: Training loss at epoch 16: 0.9203343987464905
[08/27/2025 21:18:35 INFO]: Training loss at epoch 54: 0.6637559831142426
[08/27/2025 21:18:41 INFO]: Training loss at epoch 19: 1.220984011888504
[08/27/2025 21:18:42 INFO]: Training loss at epoch 23: 1.2464330196380615
[08/27/2025 21:18:49 INFO]: Training accuracy: {
    "score": -0.908690138811273,
    "rmse": 0.908690138811273
}
[08/27/2025 21:18:49 INFO]: Val accuracy: {
    "score": -0.6654210405674225,
    "rmse": 0.6654210405674225
}
[08/27/2025 21:18:49 INFO]: Test accuracy: {
    "score": -0.904445305060505,
    "rmse": 0.904445305060505
}
[08/27/2025 21:18:49 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_43",
    "best_epoch": 35,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.904445305060505,
        "rmse": 0.904445305060505
    },
    "train_stats": {
        "score": -0.908690138811273,
        "rmse": 0.908690138811273
    },
    "val_stats": {
        "score": -0.6654210405674225,
        "rmse": 0.6654210405674225
    }
}
[08/27/2025 21:18:49 INFO]: Procewss finished for trial blotchy-Amado_trial_43
[08/27/2025 21:18:50 INFO]: 
_________________________________________________

[08/27/2025 21:18:50 INFO]: train_net_for_optune.py main() running.
[08/27/2025 21:18:50 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 2.3381030528505597
  attention_dropout: 0.39712606216239865
  ffn_dropout: 0.39712606216239865
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.438304991036628e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_57

[08/27/2025 21:18:50 INFO]: This ft_transformer has 7.242 million parameters.
[08/27/2025 21:18:50 INFO]: Training will start at epoch 0.
[08/27/2025 21:18:50 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 21:18:56 INFO]: Training loss at epoch 35: 1.019219994544983
[08/27/2025 21:18:58 INFO]: Training loss at epoch 4: 1.43348628282547
[08/27/2025 21:19:08 INFO]: Training loss at epoch 58: 0.9185586869716644
[08/27/2025 21:19:09 INFO]: Training loss at epoch 6: 1.157377541065216
[08/27/2025 21:19:43 INFO]: Training loss at epoch 10: 0.9010592699050903
[08/27/2025 21:19:54 INFO]: Training loss at epoch 25: 0.9821788966655731
[08/27/2025 21:20:00 INFO]: New best epoch, val score: -0.6605990469501009
[08/27/2025 21:20:00 INFO]: Saving model to: blotchy-Amado_trial_35/model_best.pth
[08/27/2025 21:20:16 INFO]: Training loss at epoch 19: 1.2898549437522888
[08/27/2025 21:21:01 INFO]: Training stats: {
    "score": -1.0007414254276366,
    "rmse": 1.0007414254276366
}
[08/27/2025 21:21:01 INFO]: Val stats: {
    "score": -0.6717369843397428,
    "rmse": 0.6717369843397428
}
[08/27/2025 21:21:01 INFO]: Test stats: {
    "score": -0.8749588404004639,
    "rmse": 0.8749588404004639
}
[08/27/2025 21:21:21 INFO]: Training loss at epoch 29: 1.0747689008712769
[08/27/2025 21:21:51 INFO]: Training stats: {
    "score": -1.0685909740233133,
    "rmse": 1.0685909740233133
}
[08/27/2025 21:21:51 INFO]: Val stats: {
    "score": -0.6905175739548031,
    "rmse": 0.6905175739548031
}
[08/27/2025 21:21:51 INFO]: Test stats: {
    "score": -0.9133565054025474,
    "rmse": 0.9133565054025474
}
[08/27/2025 21:22:22 INFO]: Training loss at epoch 0: 1.0041326582431793
[08/27/2025 21:22:34 INFO]: Training loss at epoch 22: 1.1643726229667664
[08/27/2025 21:22:46 INFO]: Training loss at epoch 7: 0.9685485064983368
[08/27/2025 21:22:48 INFO]: New best epoch, val score: -0.6976603802991747
[08/27/2025 21:22:48 INFO]: Saving model to: blotchy-Amado_trial_57/model_best.pth
[08/27/2025 21:22:56 INFO]: Training loss at epoch 5: 1.1152648329734802
[08/27/2025 21:22:58 INFO]: Training loss at epoch 29: 0.9577780961990356
[08/27/2025 21:23:01 INFO]: Training loss at epoch 11: 0.9129206836223602
[08/27/2025 21:23:02 INFO]: Training loss at epoch 35: 1.1237805783748627
[08/27/2025 21:23:16 INFO]: Training loss at epoch 17: 1.1106846332550049
[08/27/2025 21:23:21 INFO]: Training loss at epoch 22: 0.8325313329696655
[08/27/2025 21:24:02 INFO]: Training loss at epoch 55: 0.6527408361434937
[08/27/2025 21:24:08 INFO]: New best epoch, val score: -0.6629990508619463
[08/27/2025 21:24:08 INFO]: Saving model to: blotchy-Amado_trial_30/model_best.pth
[08/27/2025 21:24:24 INFO]: Training loss at epoch 11: 1.048348307609558
[08/27/2025 21:24:38 INFO]: Training loss at epoch 59: 0.8593947887420654
[08/27/2025 21:24:44 INFO]: Running Final Evaluation...
[08/27/2025 21:25:05 INFO]: Training stats: {
    "score": -0.9999550221678069,
    "rmse": 0.9999550221678069
}
[08/27/2025 21:25:05 INFO]: Val stats: {
    "score": -0.7401019368608996,
    "rmse": 0.7401019368608996
}
[08/27/2025 21:25:05 INFO]: Test stats: {
    "score": -0.9095216426190926,
    "rmse": 0.9095216426190926
}
[08/27/2025 21:25:21 INFO]: Training stats: {
    "score": -0.9788423011366312,
    "rmse": 0.9788423011366312
}
[08/27/2025 21:25:21 INFO]: Val stats: {
    "score": -0.681022561573155,
    "rmse": 0.681022561573155
}
[08/27/2025 21:25:21 INFO]: Test stats: {
    "score": -0.8653541470737145,
    "rmse": 0.8653541470737145
}
[08/27/2025 21:25:27 INFO]: Training loss at epoch 26: 1.2205581367015839
[08/27/2025 21:26:06 INFO]: Training loss at epoch 25: 1.0092075765132904
[08/27/2025 21:26:17 INFO]: Training loss at epoch 1: 1.20143324136734
[08/27/2025 21:26:19 INFO]: Training loss at epoch 8: 1.206553041934967
[08/27/2025 21:26:29 INFO]: Training loss at epoch 20: 1.0786818861961365
[08/27/2025 21:26:29 INFO]: Training stats: {
    "score": -1.0112675203428105,
    "rmse": 1.0112675203428105
}
[08/27/2025 21:26:29 INFO]: Val stats: {
    "score": -0.6603732135366944,
    "rmse": 0.6603732135366944
}
[08/27/2025 21:26:29 INFO]: Test stats: {
    "score": -0.8704819250855033,
    "rmse": 0.8704819250855033
}
[08/27/2025 21:26:32 INFO]: Training loss at epoch 24: 1.030859112739563
[08/27/2025 21:26:34 INFO]: Training accuracy: {
    "score": -0.9183855502290711,
    "rmse": 0.9183855502290711
}
[08/27/2025 21:26:34 INFO]: Val accuracy: {
    "score": -0.6658809932343849,
    "rmse": 0.6658809932343849
}
[08/27/2025 21:26:34 INFO]: Test accuracy: {
    "score": -0.8979637221255544,
    "rmse": 0.8979637221255544
}
[08/27/2025 21:26:34 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_37",
    "best_epoch": 24,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8979637221255544,
        "rmse": 0.8979637221255544
    },
    "train_stats": {
        "score": -0.9183855502290711,
        "rmse": 0.9183855502290711
    },
    "val_stats": {
        "score": -0.6658809932343849,
        "rmse": 0.6658809932343849
    }
}
[08/27/2025 21:26:34 INFO]: Procewss finished for trial blotchy-Amado_trial_37
[08/27/2025 21:26:34 INFO]: 
_________________________________________________

[08/27/2025 21:26:34 INFO]: train_net_for_optune.py main() running.
[08/27/2025 21:26:34 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 2.32218820706926
  attention_dropout: 0.3913098771242295
  ffn_dropout: 0.3913098771242295
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.747980112277845e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_58

[08/27/2025 21:26:34 INFO]: This ft_transformer has 7.216 million parameters.
[08/27/2025 21:26:34 INFO]: Training will start at epoch 0.
[08/27/2025 21:26:34 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 21:26:53 INFO]: Training loss at epoch 6: 1.0850704610347748
[08/27/2025 21:27:11 INFO]: New best epoch, val score: -0.6603732135366944
[08/27/2025 21:27:11 INFO]: Saving model to: blotchy-Amado_trial_34/model_best.pth
[08/27/2025 21:27:42 INFO]: Training loss at epoch 36: 1.2243182957172394
[08/27/2025 21:27:51 INFO]: Training loss at epoch 20: 0.9674144089221954
[08/27/2025 21:27:57 INFO]: Training loss at epoch 18: 0.8871117830276489
[08/27/2025 21:28:04 INFO]: Training loss at epoch 23: 0.9417247474193573
[08/27/2025 21:28:17 INFO]: Training loss at epoch 12: 0.9865175783634186
[08/27/2025 21:28:43 INFO]: Training loss at epoch 23: 0.9565920531749725
[08/27/2025 21:29:05 INFO]: Training loss at epoch 12: 1.007192313671112
[08/27/2025 21:29:56 INFO]: Training loss at epoch 9: 1.000439465045929
[08/27/2025 21:30:07 INFO]: Training loss at epoch 0: 1.2058001160621643
[08/27/2025 21:30:18 INFO]: Training loss at epoch 2: 0.8759463429450989
[08/27/2025 21:30:33 INFO]: New best epoch, val score: -0.8980295927774473
[08/27/2025 21:30:33 INFO]: Saving model to: blotchy-Amado_trial_58/model_best.pth
[08/27/2025 21:30:49 INFO]: Training loss at epoch 30: 0.7956035733222961
[08/27/2025 21:30:51 INFO]: Training loss at epoch 7: 1.482508897781372
[08/27/2025 21:30:55 INFO]: Training loss at epoch 27: 0.9418985247612
[08/27/2025 21:31:09 INFO]: Training stats: {
    "score": -1.0010726141348063,
    "rmse": 1.0010726141348063
}
[08/27/2025 21:31:09 INFO]: Val stats: {
    "score": -0.6951802692463356,
    "rmse": 0.6951802692463356
}
[08/27/2025 21:31:09 INFO]: Test stats: {
    "score": -0.8805954039814539,
    "rmse": 0.8805954039814539
}
[08/27/2025 21:31:11 INFO]: Training loss at epoch 21: 0.9979254603385925
[08/27/2025 21:32:00 INFO]: Training loss at epoch 60: 1.0200906991958618
[08/27/2025 21:32:12 INFO]: Training loss at epoch 30: 0.9911206960678101
[08/27/2025 21:32:16 INFO]: Training loss at epoch 36: 1.0476160049438477
[08/27/2025 21:32:38 INFO]: Training loss at epoch 19: 1.0886675417423248
[08/27/2025 21:33:28 INFO]: Training loss at epoch 13: 0.9083263278007507
[08/27/2025 21:33:31 INFO]: Training loss at epoch 24: 1.1406245827674866
[08/27/2025 21:33:44 INFO]: Training loss at epoch 13: 1.1109576225280762
[08/27/2025 21:33:58 INFO]: Training loss at epoch 24: 0.9782987534999847
[08/27/2025 21:34:02 INFO]: Training loss at epoch 1: 1.0753806829452515
[08/27/2025 21:34:06 INFO]: Training loss at epoch 26: 1.0854780077934265
[08/27/2025 21:34:11 INFO]: Training stats: {
    "score": -1.0063148430738635,
    "rmse": 1.0063148430738635
}
[08/27/2025 21:34:11 INFO]: Val stats: {
    "score": -0.7230148972393463,
    "rmse": 0.7230148972393463
}
[08/27/2025 21:34:11 INFO]: Test stats: {
    "score": -0.8978793429020857,
    "rmse": 0.8978793429020857
}
[08/27/2025 21:34:13 INFO]: Training loss at epoch 3: 1.1259508728981018
[08/27/2025 21:34:29 INFO]: Training loss at epoch 25: 0.9429031908512115
[08/27/2025 21:34:31 INFO]: New best epoch, val score: -0.6714885039837375
[08/27/2025 21:34:31 INFO]: Saving model to: blotchy-Amado_trial_58/model_best.pth
[08/27/2025 21:34:41 INFO]: Training loss at epoch 21: 1.254944384098053
[08/27/2025 21:34:43 INFO]: Training loss at epoch 10: 0.9734262824058533
[08/27/2025 21:34:51 INFO]: Training loss at epoch 8: 1.3229382038116455
[08/27/2025 21:35:07 INFO]: New best epoch, val score: -0.6640789259130673
[08/27/2025 21:35:07 INFO]: Saving model to: blotchy-Amado_trial_55/model_best.pth
[08/27/2025 21:35:49 INFO]: Training loss at epoch 22: 1.0664203763008118
[08/27/2025 21:36:03 INFO]: Training loss at epoch 30: 0.9989811182022095
[08/27/2025 21:36:26 INFO]: Training loss at epoch 37: 1.0350337624549866
[08/27/2025 21:36:27 INFO]: Training loss at epoch 28: 1.125472068786621
[08/27/2025 21:37:24 INFO]: Training loss at epoch 61: 0.9004537463188171
[08/27/2025 21:37:58 INFO]: Training loss at epoch 2: 1.492849349975586
[08/27/2025 21:38:07 INFO]: Training loss at epoch 4: 1.0649107098579407
[08/27/2025 21:38:14 INFO]: Training loss at epoch 11: 0.9074905812740326
[08/27/2025 21:38:21 INFO]: Training loss at epoch 14: 1.2012931108474731
[08/27/2025 21:38:25 INFO]: New best epoch, val score: -0.6700097466966674
[08/27/2025 21:38:25 INFO]: Saving model to: blotchy-Amado_trial_58/model_best.pth
[08/27/2025 21:38:35 INFO]: New best epoch, val score: -0.6679596165188798
[08/27/2025 21:38:35 INFO]: Saving model to: blotchy-Amado_trial_57/model_best.pth
[08/27/2025 21:38:42 INFO]: Training loss at epoch 14: 1.1148474216461182
[08/27/2025 21:38:49 INFO]: Training loss at epoch 9: 1.1745926141738892
[08/27/2025 21:38:51 INFO]: Training loss at epoch 20: 1.0147145986557007
[08/27/2025 21:39:00 INFO]: Training loss at epoch 25: 1.0967363715171814
[08/27/2025 21:39:00 INFO]: Training loss at epoch 31: 1.014729768037796
[08/27/2025 21:39:17 INFO]: Training loss at epoch 25: 0.9965729713439941
[08/27/2025 21:39:51 INFO]: Running Final Evaluation...
[08/27/2025 21:40:08 INFO]: Training stats: {
    "score": -1.146343651145867,
    "rmse": 1.146343651145867
}
[08/27/2025 21:40:08 INFO]: Val stats: {
    "score": -0.7708457175567806,
    "rmse": 0.7708457175567806
}
[08/27/2025 21:40:08 INFO]: Test stats: {
    "score": -0.9835988293780886,
    "rmse": 0.9835988293780886
}
[08/27/2025 21:40:26 INFO]: Training loss at epoch 23: 1.1155824065208435
[08/27/2025 21:41:21 INFO]: Training loss at epoch 37: 0.9196774363517761
[08/27/2025 21:41:28 INFO]: Training loss at epoch 22: 1.1265674829483032
[08/27/2025 21:41:49 INFO]: Training loss at epoch 12: 1.139444649219513
[08/27/2025 21:41:50 INFO]: Training loss at epoch 29: 1.107783317565918
[08/27/2025 21:41:54 INFO]: Training loss at epoch 3: 0.99515300989151
[08/27/2025 21:42:04 INFO]: Training loss at epoch 5: 1.241822063922882
[08/27/2025 21:42:04 INFO]: Training loss at epoch 27: 1.0624612271785736
[08/27/2025 21:42:17 INFO]: Training loss at epoch 26: 1.262173056602478
[08/27/2025 21:42:21 INFO]: Training accuracy: {
    "score": -1.0453837619235433,
    "rmse": 1.0453837619235433
}
[08/27/2025 21:42:21 INFO]: Val accuracy: {
    "score": -0.6651791882438978,
    "rmse": 0.6651791882438978
}
[08/27/2025 21:42:21 INFO]: Test accuracy: {
    "score": -0.896682064597687,
    "rmse": 0.896682064597687
}
[08/27/2025 21:42:21 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_45",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.896682064597687,
        "rmse": 0.896682064597687
    },
    "train_stats": {
        "score": -1.0453837619235433,
        "rmse": 1.0453837619235433
    },
    "val_stats": {
        "score": -0.6651791882438978,
        "rmse": 0.6651791882438978
    }
}
[08/27/2025 21:42:21 INFO]: Procewss finished for trial blotchy-Amado_trial_45
[08/27/2025 21:42:21 INFO]: 
_________________________________________________

[08/27/2025 21:42:21 INFO]: train_net_for_optune.py main() running.
[08/27/2025 21:42:21 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 2.2632391290774048
  attention_dropout: 0.4413050977221817
  ffn_dropout: 0.4413050977221817
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.462620072552021e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_59

[08/27/2025 21:42:21 INFO]: This ft_transformer has 7.119 million parameters.
[08/27/2025 21:42:21 INFO]: Training will start at epoch 0.
[08/27/2025 21:42:21 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 21:42:31 INFO]: New best epoch, val score: -0.6655509479368217
[08/27/2025 21:42:31 INFO]: Saving model to: blotchy-Amado_trial_57/model_best.pth
[08/27/2025 21:42:48 INFO]: Training loss at epoch 62: 0.9435259699821472
[08/27/2025 21:42:59 INFO]: Training loss at epoch 15: 0.9225268065929413
[08/27/2025 21:43:30 INFO]: Training loss at epoch 21: 1.1821105182170868
[08/27/2025 21:43:40 INFO]: Training loss at epoch 31: 1.0416803658008575
[08/27/2025 21:43:43 INFO]: Training stats: {
    "score": -1.0058050611133391,
    "rmse": 1.0058050611133391
}
[08/27/2025 21:43:43 INFO]: Val stats: {
    "score": -0.6618437677909657,
    "rmse": 0.6618437677909657
}
[08/27/2025 21:43:43 INFO]: Test stats: {
    "score": -0.8693718555600174,
    "rmse": 0.8693718555600174
}
[08/27/2025 21:43:54 INFO]: Training loss at epoch 15: 1.050932228565216
[08/27/2025 21:44:07 INFO]: Training loss at epoch 10: 1.120889663696289
[08/27/2025 21:44:28 INFO]: Training loss at epoch 26: 1.0080110430717468
[08/27/2025 21:44:33 INFO]: Training loss at epoch 26: 1.050560474395752
[08/27/2025 21:44:36 INFO]: New best epoch, val score: -0.6648329340830661
[08/27/2025 21:44:36 INFO]: Saving model to: blotchy-Amado_trial_56/model_best.pth
[08/27/2025 21:45:06 INFO]: Training loss at epoch 38: 0.8912471234798431
[08/27/2025 21:45:08 INFO]: Training loss at epoch 24: 1.0409936904907227
[08/27/2025 21:45:31 INFO]: Training loss at epoch 13: 0.9234848618507385
[08/27/2025 21:45:52 INFO]: Training loss at epoch 0: 0.8399122357368469
[08/27/2025 21:45:59 INFO]: Training loss at epoch 4: 0.9849773645401001
[08/27/2025 21:46:07 INFO]: Training loss at epoch 6: 1.1134478449821472
[08/27/2025 21:46:20 INFO]: New best epoch, val score: -0.6786055712799622
[08/27/2025 21:46:20 INFO]: Saving model to: blotchy-Amado_trial_59/model_best.pth
[08/27/2025 21:46:34 INFO]: New best epoch, val score: -0.6614595268290461
[08/27/2025 21:46:34 INFO]: Saving model to: blotchy-Amado_trial_57/model_best.pth
[08/27/2025 21:47:02 INFO]: Training loss at epoch 31: 1.0021497011184692
[08/27/2025 21:47:43 INFO]: Training loss at epoch 16: 1.0582327842712402
[08/27/2025 21:48:05 INFO]: Training loss at epoch 11: 1.0796223282814026
[08/27/2025 21:48:12 INFO]: Training loss at epoch 22: 0.9306858479976654
[08/27/2025 21:48:18 INFO]: Training loss at epoch 63: 0.8796103298664093
[08/27/2025 21:48:20 INFO]: Training loss at epoch 23: 1.1540850698947906
[08/27/2025 21:49:04 INFO]: Training loss at epoch 14: 1.1587249040603638
[08/27/2025 21:49:07 INFO]: Training loss at epoch 16: 0.8529352843761444
[08/27/2025 21:49:13 INFO]: Training loss at epoch 30: 1.1423358917236328
[08/27/2025 21:49:28 INFO]: New best epoch, val score: -0.6613260553290564
[08/27/2025 21:49:28 INFO]: Saving model to: blotchy-Amado_trial_55/model_best.pth
[08/27/2025 21:49:44 INFO]: Training loss at epoch 25: 0.9300603866577148
[08/27/2025 21:49:45 INFO]: Training loss at epoch 1: 1.1709086298942566
[08/27/2025 21:49:52 INFO]: Training loss at epoch 27: 1.3329696655273438
[08/27/2025 21:49:55 INFO]: Training loss at epoch 5: 1.1399718821048737
[08/27/2025 21:49:58 INFO]: Training loss at epoch 27: 1.1461750268936157
[08/27/2025 21:50:02 INFO]: Training loss at epoch 7: 1.2564905285835266
[08/27/2025 21:50:08 INFO]: Training loss at epoch 28: 0.9065192341804504
[08/27/2025 21:50:13 INFO]: Training loss at epoch 27: 1.015866458415985
[08/27/2025 21:50:31 INFO]: New best epoch, val score: -0.6576366203524888
[08/27/2025 21:50:31 INFO]: Saving model to: blotchy-Amado_trial_57/model_best.pth
[08/27/2025 21:50:35 INFO]: Training loss at epoch 38: 1.0833458304405212
[08/27/2025 21:52:03 INFO]: Training loss at epoch 12: 0.9441469013690948
[08/27/2025 21:52:20 INFO]: Training loss at epoch 17: 1.0546405911445618
[08/27/2025 21:52:39 INFO]: Training loss at epoch 15: 0.9391508400440216
[08/27/2025 21:52:51 INFO]: Training loss at epoch 23: 1.163189709186554
[08/27/2025 21:53:38 INFO]: Training loss at epoch 2: 1.0205405950546265
[08/27/2025 21:53:41 INFO]: Training loss at epoch 64: 1.1720688939094543
[08/27/2025 21:53:42 INFO]: Training loss at epoch 39: 1.04006227850914
[08/27/2025 21:53:47 INFO]: Training loss at epoch 6: 0.8676175773143768
[08/27/2025 21:53:53 INFO]: Training loss at epoch 8: 1.006408303976059
[08/27/2025 21:54:17 INFO]: Training loss at epoch 17: 1.1201433539390564
[08/27/2025 21:54:20 INFO]: Training loss at epoch 26: 0.8387092053890228
[08/27/2025 21:54:22 INFO]: New best epoch, val score: -0.6549509510706486
[08/27/2025 21:54:22 INFO]: Saving model to: blotchy-Amado_trial_57/model_best.pth
[08/27/2025 21:54:40 INFO]: Training loss at epoch 31: 1.2302840948104858
[08/27/2025 21:55:05 INFO]: Training loss at epoch 28: 1.0060954093933105
[08/27/2025 21:55:06 INFO]: Training loss at epoch 24: 0.92445108294487
[08/27/2025 21:55:18 INFO]: Running Final Evaluation...
[08/27/2025 21:55:24 INFO]: Training loss at epoch 28: 0.8469193577766418
[08/27/2025 21:55:56 INFO]: Training loss at epoch 13: 0.8870960474014282
[08/27/2025 21:56:09 INFO]: Training loss at epoch 16: 0.9618787467479706
[08/27/2025 21:56:28 INFO]: Training loss at epoch 32: 0.8500351011753082
[08/27/2025 21:56:38 INFO]: Training stats: {
    "score": -1.002119270854391,
    "rmse": 1.002119270854391
}
[08/27/2025 21:56:38 INFO]: Val stats: {
    "score": -0.7011265554923387,
    "rmse": 0.7011265554923387
}
[08/27/2025 21:56:38 INFO]: Test stats: {
    "score": -0.8850179443149515,
    "rmse": 0.8850179443149515
}
[08/27/2025 21:56:51 INFO]: Training loss at epoch 18: 1.0306820273399353
[08/27/2025 21:57:09 INFO]: Training accuracy: {
    "score": -1.0058032831436958,
    "rmse": 1.0058032831436958
}
[08/27/2025 21:57:09 INFO]: Val accuracy: {
    "score": -0.6601971190088424,
    "rmse": 0.6601971190088424
}
[08/27/2025 21:57:09 INFO]: Test accuracy: {
    "score": -0.869777742849043,
    "rmse": 0.869777742849043
}
[08/27/2025 21:57:09 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_47",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.869777742849043,
        "rmse": 0.869777742849043
    },
    "train_stats": {
        "score": -1.0058032831436958,
        "rmse": 1.0058032831436958
    },
    "val_stats": {
        "score": -0.6601971190088424,
        "rmse": 0.6601971190088424
    }
}
[08/27/2025 21:57:09 INFO]: Procewss finished for trial blotchy-Amado_trial_47
[08/27/2025 21:57:09 INFO]: 
_________________________________________________

[08/27/2025 21:57:09 INFO]: train_net_for_optune.py main() running.
[08/27/2025 21:57:09 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 2.357520290932271
  attention_dropout: 0.4425603910292486
  ffn_dropout: 0.4425603910292486
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 7.670606286269149e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_60

[08/27/2025 21:57:09 INFO]: This ft_transformer has 7.273 million parameters.
[08/27/2025 21:57:09 INFO]: Training will start at epoch 0.
[08/27/2025 21:57:09 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 21:57:25 INFO]: Training loss at epoch 24: 0.9373281896114349
[08/27/2025 21:57:30 INFO]: Training loss at epoch 3: 1.0261910259723663
[08/27/2025 21:57:41 INFO]: Training loss at epoch 7: 1.0804398953914642
[08/27/2025 21:57:48 INFO]: Training loss at epoch 9: 0.9900918900966644
[08/27/2025 21:57:49 INFO]: Training loss at epoch 32: 1.0467391610145569
[08/27/2025 21:57:56 INFO]: Training loss at epoch 28: 1.2918448746204376
[08/27/2025 21:57:59 INFO]: New best epoch, val score: -0.6690080566283116
[08/27/2025 21:57:59 INFO]: Saving model to: blotchy-Amado_trial_59/model_best.pth
[08/27/2025 21:58:01 INFO]: Training loss at epoch 29: 1.2059127390384674
[08/27/2025 21:58:52 INFO]: Training loss at epoch 27: 0.961266815662384
[08/27/2025 21:59:01 INFO]: Training loss at epoch 65: 1.0034388303756714
[08/27/2025 21:59:04 INFO]: Training stats: {
    "score": -1.0103354038409145,
    "rmse": 1.0103354038409145
}
[08/27/2025 21:59:04 INFO]: Val stats: {
    "score": -0.6542280560147022,
    "rmse": 0.6542280560147022
}
[08/27/2025 21:59:04 INFO]: Test stats: {
    "score": -0.8717448333425147,
    "rmse": 0.8717448333425147
}
[08/27/2025 21:59:27 INFO]: Training loss at epoch 18: 1.0337774753570557
[08/27/2025 21:59:32 INFO]: New best epoch, val score: -0.6542280560147022
[08/27/2025 21:59:32 INFO]: Saving model to: blotchy-Amado_trial_57/model_best.pth
[08/27/2025 21:59:35 INFO]: Training loss at epoch 39: 1.1291775703430176
[08/27/2025 21:59:40 INFO]: Training loss at epoch 17: 1.052072286605835
[08/27/2025 21:59:53 INFO]: Training loss at epoch 14: 1.5100976824760437
[08/27/2025 22:00:21 INFO]: Training loss at epoch 29: 0.9964315295219421
[08/27/2025 22:00:38 INFO]: Training loss at epoch 0: 1.1639742255210876
[08/27/2025 22:00:44 INFO]: Training stats: {
    "score": -0.9682680399221381,
    "rmse": 0.9682680399221381
}
[08/27/2025 22:00:44 INFO]: Val stats: {
    "score": -0.6899123499669244,
    "rmse": 0.6899123499669244
}
[08/27/2025 22:00:44 INFO]: Test stats: {
    "score": -0.9056525505887898,
    "rmse": 0.9056525505887898
}
[08/27/2025 22:00:49 INFO]: Training loss at epoch 29: 1.0763952136039734
[08/27/2025 22:01:08 INFO]: New best epoch, val score: -0.6785110225615091
[08/27/2025 22:01:08 INFO]: Saving model to: blotchy-Amado_trial_60/model_best.pth
[08/27/2025 22:01:26 INFO]: Training loss at epoch 4: 1.0461682677268982
[08/27/2025 22:01:28 INFO]: Training loss at epoch 19: 0.9729913473129272
[08/27/2025 22:01:39 INFO]: Training loss at epoch 8: 1.1410943269729614
[08/27/2025 22:01:53 INFO]: Training loss at epoch 25: 1.156372308731079
[08/27/2025 22:02:07 INFO]: Training loss at epoch 25: 1.0225538611412048
[08/27/2025 22:02:09 INFO]: Training stats: {
    "score": -1.0061648711340963,
    "rmse": 1.0061648711340963
}
[08/27/2025 22:02:09 INFO]: Val stats: {
    "score": -0.7243513713267626,
    "rmse": 0.7243513713267626
}
[08/27/2025 22:02:09 INFO]: Test stats: {
    "score": -0.8960526328831349,
    "rmse": 0.8960526328831349
}
[08/27/2025 22:02:38 INFO]: Training stats: {
    "score": -0.9981491631052859,
    "rmse": 0.9981491631052859
}
[08/27/2025 22:02:38 INFO]: Val stats: {
    "score": -0.6717509531871783,
    "rmse": 0.6717509531871783
}
[08/27/2025 22:02:38 INFO]: Test stats: {
    "score": -0.870480167508491,
    "rmse": 0.870480167508491
}
[08/27/2025 22:02:38 INFO]: Training stats: {
    "score": -0.9988085495313844,
    "rmse": 0.9988085495313844
}
[08/27/2025 22:02:38 INFO]: Val stats: {
    "score": -0.7110207615072714,
    "rmse": 0.7110207615072714
}
[08/27/2025 22:02:38 INFO]: Test stats: {
    "score": -0.8909657680764921,
    "rmse": 0.8909657680764921
}
[08/27/2025 22:03:02 INFO]: Training stats: {
    "score": -1.0010249076690485,
    "rmse": 1.0010249076690485
}
[08/27/2025 22:03:02 INFO]: Val stats: {
    "score": -0.665827718788405,
    "rmse": 0.665827718788405
}
[08/27/2025 22:03:02 INFO]: Test stats: {
    "score": -0.8671855997016137,
    "rmse": 0.8671855997016137
}
[08/27/2025 22:03:03 INFO]: Training loss at epoch 10: 1.1979336738586426
[08/27/2025 22:03:17 INFO]: Training loss at epoch 18: 1.1047195196151733
[08/27/2025 22:03:32 INFO]: Training loss at epoch 28: 0.9916634559631348
[08/27/2025 22:03:52 INFO]: Training loss at epoch 15: 1.0475112199783325
[08/27/2025 22:04:28 INFO]: Training loss at epoch 66: 1.0249656438827515
[08/27/2025 22:04:39 INFO]: Training loss at epoch 1: 0.9253499507904053
[08/27/2025 22:04:41 INFO]: Training loss at epoch 19: 1.0485126376152039
[08/27/2025 22:05:18 INFO]: Training loss at epoch 40: 0.9518028795719147
[08/27/2025 22:05:20 INFO]: Training loss at epoch 5: 1.0536892414093018
[08/27/2025 22:05:35 INFO]: Training loss at epoch 9: 0.9555202126502991
[08/27/2025 22:05:44 INFO]: Training loss at epoch 29: 1.1018043160438538
[08/27/2025 22:06:23 INFO]: Training stats: {
    "score": -0.9960845165192053,
    "rmse": 0.9960845165192053
}
[08/27/2025 22:06:23 INFO]: Val stats: {
    "score": -0.6805553471527103,
    "rmse": 0.6805553471527103
}
[08/27/2025 22:06:23 INFO]: Test stats: {
    "score": -0.8757500586346223,
    "rmse": 0.8757500586346223
}
[08/27/2025 22:06:48 INFO]: Training loss at epoch 26: 0.9909580647945404
[08/27/2025 22:06:49 INFO]: Training loss at epoch 19: 0.9668204486370087
[08/27/2025 22:06:52 INFO]: Training stats: {
    "score": -1.0009217521068856,
    "rmse": 1.0009217521068856
}
[08/27/2025 22:06:52 INFO]: Val stats: {
    "score": -0.6976442785684424,
    "rmse": 0.6976442785684424
}
[08/27/2025 22:06:52 INFO]: Test stats: {
    "score": -0.8824716526111237,
    "rmse": 0.8824716526111237
}
[08/27/2025 22:07:01 INFO]: Training loss at epoch 11: 0.9258034527301788
[08/27/2025 22:07:24 INFO]: Training loss at epoch 30: 1.0975807905197144
[08/27/2025 22:07:37 INFO]: Training loss at epoch 20: 0.9224779009819031
[08/27/2025 22:07:48 INFO]: Training loss at epoch 16: 1.292653203010559
[08/27/2025 22:08:01 INFO]: Training stats: {
    "score": -1.0034023992317265,
    "rmse": 1.0034023992317265
}
[08/27/2025 22:08:01 INFO]: Val stats: {
    "score": -0.7070971257396487,
    "rmse": 0.7070971257396487
}
[08/27/2025 22:08:01 INFO]: Test stats: {
    "score": -0.8878679547947974,
    "rmse": 0.8878679547947974
}
[08/27/2025 22:08:06 INFO]: Training loss at epoch 30: 0.9004140794277191
[08/27/2025 22:08:10 INFO]: Training loss at epoch 29: 1.1218727827072144
[08/27/2025 22:08:25 INFO]: Training stats: {
    "score": -0.9651492937080575,
    "rmse": 0.9651492937080575
}
[08/27/2025 22:08:25 INFO]: Val stats: {
    "score": -0.6452899491671502,
    "rmse": 0.6452899491671502
}
[08/27/2025 22:08:25 INFO]: Test stats: {
    "score": -0.9079031546376028,
    "rmse": 0.9079031546376028
}
[08/27/2025 22:08:39 INFO]: Training loss at epoch 2: 1.1658110320568085
[08/27/2025 22:08:40 INFO]: Training loss at epoch 26: 0.9184843003749847
[08/27/2025 22:08:45 INFO]: Training loss at epoch 30: 1.1959553062915802
[08/27/2025 22:08:46 INFO]: Training loss at epoch 33: 1.2721859216690063
[08/27/2025 22:09:14 INFO]: Training loss at epoch 6: 1.0029558539390564
[08/27/2025 22:09:18 INFO]: Training loss at epoch 33: 0.964564710855484
[08/27/2025 22:09:44 INFO]: Training stats: {
    "score": -1.0075455949454104,
    "rmse": 1.0075455949454104
}
[08/27/2025 22:09:44 INFO]: Val stats: {
    "score": -0.6566178948775474,
    "rmse": 0.6566178948775474
}
[08/27/2025 22:09:44 INFO]: Test stats: {
    "score": -0.8696029847176281,
    "rmse": 0.8696029847176281
}
[08/27/2025 22:09:54 INFO]: Training loss at epoch 67: 0.8481211364269257
[08/27/2025 22:10:48 INFO]: Training loss at epoch 10: 1.2524760961532593
[08/27/2025 22:10:57 INFO]: Training loss at epoch 12: 1.2167078256607056
[08/27/2025 22:11:28 INFO]: Training loss at epoch 27: 1.03340482711792
[08/27/2025 22:11:35 INFO]: Training loss at epoch 20: 0.9366302788257599
[08/27/2025 22:11:36 INFO]: Training loss at epoch 20: 0.993453860282898
[08/27/2025 22:11:44 INFO]: Training loss at epoch 17: 1.1515402793884277
[08/27/2025 22:11:46 INFO]: Training loss at epoch 40: 0.9437478482723236
[08/27/2025 22:12:17 INFO]: Training loss at epoch 21: 1.1537169814109802
[08/27/2025 22:12:35 INFO]: Training loss at epoch 3: 0.9926803708076477
[08/27/2025 22:12:43 INFO]: Training loss at epoch 31: 1.0431753993034363
[08/27/2025 22:13:04 INFO]: New best epoch, val score: -0.6621420321796098
[08/27/2025 22:13:04 INFO]: Saving model to: blotchy-Amado_trial_60/model_best.pth
[08/27/2025 22:13:06 INFO]: Training loss at epoch 7: 0.9796551465988159
[08/27/2025 22:13:35 INFO]: Training loss at epoch 31: 1.1714609861373901
[08/27/2025 22:13:58 INFO]: Training loss at epoch 41: 0.9734259247779846
[08/27/2025 22:14:22 INFO]: Training loss at epoch 30: 1.2590666711330414
[08/27/2025 22:14:44 INFO]: Training loss at epoch 11: 0.9137438535690308
[08/27/2025 22:14:52 INFO]: Training loss at epoch 13: 0.9238925576210022
[08/27/2025 22:15:13 INFO]: Training loss at epoch 21: 0.8321139812469482
[08/27/2025 22:15:13 INFO]: New best epoch, val score: -0.6686914408950189
[08/27/2025 22:15:13 INFO]: Saving model to: blotchy-Amado_trial_58/model_best.pth
[08/27/2025 22:15:18 INFO]: Training loss at epoch 68: 0.9828455150127411
[08/27/2025 22:15:27 INFO]: Training loss at epoch 27: 1.0438306331634521
[08/27/2025 22:15:37 INFO]: New best epoch, val score: -0.6606441111742754
[08/27/2025 22:15:37 INFO]: Saving model to: blotchy-Amado_trial_55/model_best.pth
[08/27/2025 22:15:41 INFO]: Training loss at epoch 18: 1.032514065504074
[08/27/2025 22:16:09 INFO]: Training loss at epoch 28: 1.095360904932022
[08/27/2025 22:16:15 INFO]: Training loss at epoch 30: 1.2676621675491333
[08/27/2025 22:16:37 INFO]: Training loss at epoch 4: 0.8931508958339691
[08/27/2025 22:16:43 INFO]: Training loss at epoch 31: 0.9120131433010101
[08/27/2025 22:16:50 INFO]: Training loss at epoch 21: 1.387777864933014
[08/27/2025 22:16:56 INFO]: Training loss at epoch 22: 0.9461157023906708
[08/27/2025 22:17:00 INFO]: Training loss at epoch 8: 0.8654870986938477
[08/27/2025 22:18:07 INFO]: Training loss at epoch 32: 1.08777916431427
[08/27/2025 22:18:43 INFO]: Training loss at epoch 12: 1.05430006980896
[08/27/2025 22:18:45 INFO]: Training loss at epoch 22: 0.9132272005081177
[08/27/2025 22:18:46 INFO]: Training loss at epoch 14: 1.0077919960021973
[08/27/2025 22:18:58 INFO]: Training loss at epoch 31: 0.9102393686771393
[08/27/2025 22:19:03 INFO]: Training loss at epoch 32: 0.8248521387577057
[08/27/2025 22:19:39 INFO]: Training loss at epoch 19: 1.0618205070495605
[08/27/2025 22:19:42 INFO]: Training loss at epoch 34: 1.1082993149757385
[08/27/2025 22:20:34 INFO]: Training loss at epoch 5: 0.839261919260025
[08/27/2025 22:20:44 INFO]: Training loss at epoch 69: 0.9690619111061096
[08/27/2025 22:20:49 INFO]: Training loss at epoch 29: 0.8111351430416107
[08/27/2025 22:20:51 INFO]: Training loss at epoch 9: 1.089533269405365
[08/27/2025 22:20:55 INFO]: Training loss at epoch 41: 0.9837729036808014
[08/27/2025 22:20:57 INFO]: Training stats: {
    "score": -1.0010777584537465,
    "rmse": 1.0010777584537465
}
[08/27/2025 22:20:57 INFO]: Val stats: {
    "score": -0.6685619418117762,
    "rmse": 0.6685619418117762
}
[08/27/2025 22:20:57 INFO]: Test stats: {
    "score": -0.8689136685137031,
    "rmse": 0.8689136685137031
}
[08/27/2025 22:21:32 INFO]: Training loss at epoch 23: 1.1105900406837463
[08/27/2025 22:22:02 INFO]: Training loss at epoch 22: 1.0720093548297882
[08/27/2025 22:22:04 INFO]: Training loss at epoch 34: 0.8925474286079407
[08/27/2025 22:22:08 INFO]: Training stats: {
    "score": -1.0185039232128628,
    "rmse": 1.0185039232128628
}
[08/27/2025 22:22:08 INFO]: Val stats: {
    "score": -0.7574194242433315,
    "rmse": 0.7574194242433315
}
[08/27/2025 22:22:08 INFO]: Test stats: {
    "score": -0.9170513554852147,
    "rmse": 0.9170513554852147
}
[08/27/2025 22:22:13 INFO]: Training loss at epoch 28: 1.2301976680755615
[08/27/2025 22:22:21 INFO]: Training loss at epoch 23: 1.0326462984085083
[08/27/2025 22:22:21 INFO]: Training stats: {
    "score": -0.9972357344419556,
    "rmse": 0.9972357344419556
}
[08/27/2025 22:22:21 INFO]: Val stats: {
    "score": -0.6895194473479441,
    "rmse": 0.6895194473479441
}
[08/27/2025 22:22:21 INFO]: Test stats: {
    "score": -0.8778674611331554,
    "rmse": 0.8778674611331554
}
[08/27/2025 22:22:37 INFO]: Training loss at epoch 42: 1.0514054894447327
[08/27/2025 22:22:37 INFO]: Training stats: {
    "score": -1.000234840704404,
    "rmse": 1.000234840704404
}
[08/27/2025 22:22:37 INFO]: Val stats: {
    "score": -0.6690016899118137,
    "rmse": 0.6690016899118137
}
[08/27/2025 22:22:37 INFO]: Test stats: {
    "score": -0.8694414466209215,
    "rmse": 0.8694414466209215
}
[08/27/2025 22:22:38 INFO]: New best epoch, val score: -0.6595921670313275
[08/27/2025 22:22:38 INFO]: Saving model to: blotchy-Amado_trial_53/model_best.pth
[08/27/2025 22:22:44 INFO]: Training loss at epoch 13: 1.0112415254116058
[08/27/2025 22:22:45 INFO]: Training loss at epoch 15: 0.9543631970882416
[08/27/2025 22:23:30 INFO]: Training loss at epoch 33: 0.9973965287208557
[08/27/2025 22:23:39 INFO]: Training loss at epoch 32: 1.10506933927536
[08/27/2025 22:24:07 INFO]: Training loss at epoch 31: 1.0069332420825958
[08/27/2025 22:24:34 INFO]: Training loss at epoch 6: 1.124480426311493
[08/27/2025 22:24:36 INFO]: Training loss at epoch 33: 1.222144216299057
[08/27/2025 22:24:44 INFO]: Training loss at epoch 32: 0.7862428426742554
[08/27/2025 22:24:55 INFO]: Training loss at epoch 20: 1.04197758436203
[08/27/2025 22:25:24 INFO]: New best epoch, val score: -0.6614641250220622
[08/27/2025 22:25:24 INFO]: Saving model to: blotchy-Amado_trial_56/model_best.pth
[08/27/2025 22:25:57 INFO]: Training loss at epoch 24: 1.0264764428138733
[08/27/2025 22:26:02 INFO]: Training loss at epoch 10: 1.0122568905353546
[08/27/2025 22:26:10 INFO]: Training loss at epoch 24: 0.9380836188793182
[08/27/2025 22:26:42 INFO]: Training loss at epoch 14: 0.9664285182952881
[08/27/2025 22:26:42 INFO]: Training loss at epoch 16: 1.3011688590049744
[08/27/2025 22:27:03 INFO]: Training loss at epoch 30: 0.9023351073265076
[08/27/2025 22:27:16 INFO]: Training loss at epoch 23: 1.0529431700706482
[08/27/2025 22:28:05 INFO]: Training loss at epoch 70: 0.764119952917099
[08/27/2025 22:28:20 INFO]: Training loss at epoch 33: 1.0345170497894287
[08/27/2025 22:28:30 INFO]: Training loss at epoch 7: 1.0167089700698853
[08/27/2025 22:28:46 INFO]: Training loss at epoch 34: 0.928846925497055
[08/27/2025 22:28:50 INFO]: Training loss at epoch 21: 1.05659818649292
[08/27/2025 22:29:01 INFO]: Training loss at epoch 29: 0.935007631778717
[08/27/2025 22:29:31 INFO]: Training loss at epoch 25: 1.0066836774349213
[08/27/2025 22:29:54 INFO]: Training loss at epoch 11: 1.0359854102134705
[08/27/2025 22:30:04 INFO]: Training loss at epoch 34: 1.025861144065857
[08/27/2025 22:30:06 INFO]: Training loss at epoch 42: 1.0244366228580475
[08/27/2025 22:30:37 INFO]: Training loss at epoch 17: 1.2874301373958588
[08/27/2025 22:30:39 INFO]: Training loss at epoch 15: 0.8310061395168304
[08/27/2025 22:30:41 INFO]: Training loss at epoch 35: 1.024311602115631
[08/27/2025 22:30:50 INFO]: Training loss at epoch 25: 0.9174531698226929
[08/27/2025 22:31:19 INFO]: Training loss at epoch 43: 1.0137226283550262
[08/27/2025 22:31:22 INFO]: Training stats: {
    "score": -1.0187030395708643,
    "rmse": 1.0187030395708643
}
[08/27/2025 22:31:22 INFO]: Val stats: {
    "score": -0.7530960395791988,
    "rmse": 0.7530960395791988
}
[08/27/2025 22:31:22 INFO]: Test stats: {
    "score": -0.9214486423832324,
    "rmse": 0.9214486423832324
}
[08/27/2025 22:31:45 INFO]: Training loss at epoch 31: 1.0339415669441223
[08/27/2025 22:32:03 INFO]: Training loss at epoch 32: 0.8960227966308594
[08/27/2025 22:32:29 INFO]: Training loss at epoch 8: 1.1996889114379883
[08/27/2025 22:32:31 INFO]: Training loss at epoch 24: 0.9964662492275238
[08/27/2025 22:32:44 INFO]: Training loss at epoch 33: 0.8659591972827911
[08/27/2025 22:32:50 INFO]: Training loss at epoch 22: 1.0425902009010315
[08/27/2025 22:33:01 INFO]: Training loss at epoch 34: 0.9665015637874603
[08/27/2025 22:33:07 INFO]: Training loss at epoch 26: 0.8180700838565826
[08/27/2025 22:33:32 INFO]: Training loss at epoch 71: 0.9586426317691803
[08/27/2025 22:33:42 INFO]: Running Final Evaluation...
[08/27/2025 22:33:50 INFO]: Training loss at epoch 12: 1.0347238183021545
[08/27/2025 22:34:07 INFO]: Training loss at epoch 35: 1.1108636856079102
[08/27/2025 22:34:34 INFO]: Training loss at epoch 18: 0.9924673736095428
[08/27/2025 22:34:37 INFO]: Training loss at epoch 16: 1.0957730412483215
[08/27/2025 22:34:58 INFO]: Training loss at epoch 35: 0.622909277677536
[08/27/2025 22:35:30 INFO]: Training loss at epoch 26: 1.1418108344078064
[08/27/2025 22:35:36 INFO]: Training loss at epoch 35: 0.9333119988441467
[08/27/2025 22:36:24 INFO]: Training loss at epoch 32: 0.9449185729026794
[08/27/2025 22:36:26 INFO]: Training loss at epoch 9: 0.9617826044559479
[08/27/2025 22:36:26 INFO]: Training accuracy: {
    "score": -1.0129107282533671,
    "rmse": 1.0129107282533671
}
[08/27/2025 22:36:26 INFO]: Val accuracy: {
    "score": -0.6588680837708881,
    "rmse": 0.6588680837708881
}
[08/27/2025 22:36:26 INFO]: Test accuracy: {
    "score": -0.8938189890377228,
    "rmse": 0.8938189890377228
}
[08/27/2025 22:36:26 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_44",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8938189890377228,
        "rmse": 0.8938189890377228
    },
    "train_stats": {
        "score": -1.0129107282533671,
        "rmse": 1.0129107282533671
    },
    "val_stats": {
        "score": -0.6588680837708881,
        "rmse": 0.6588680837708881
    }
}
[08/27/2025 22:36:26 INFO]: Procewss finished for trial blotchy-Amado_trial_44
[08/27/2025 22:36:26 INFO]: 
_________________________________________________

[08/27/2025 22:36:26 INFO]: train_net_for_optune.py main() running.
[08/27/2025 22:36:26 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 2.413611150237597
  attention_dropout: 0.43902165643727337
  ffn_dropout: 0.43902165643727337
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 7.686131349310538e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_61

[08/27/2025 22:36:26 INFO]: This ft_transformer has 5.550 million parameters.
[08/27/2025 22:36:26 INFO]: Training will start at epoch 0.
[08/27/2025 22:36:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 22:36:43 INFO]: Training loss at epoch 27: 0.9594694077968597
[08/27/2025 22:36:47 INFO]: Training loss at epoch 23: 1.1669755578041077
[08/27/2025 22:37:43 INFO]: Training loss at epoch 13: 1.0015449523925781
[08/27/2025 22:37:45 INFO]: Training loss at epoch 25: 1.3232007026672363
[08/27/2025 22:37:45 INFO]: Training loss at epoch 35: 0.9073688089847565
[08/27/2025 22:37:46 INFO]: Training stats: {
    "score": -1.0050066629365129,
    "rmse": 1.0050066629365129
}
[08/27/2025 22:37:46 INFO]: Val stats: {
    "score": -0.7145404661951077,
    "rmse": 0.7145404661951077
}
[08/27/2025 22:37:46 INFO]: Test stats: {
    "score": -0.8921631743432664,
    "rmse": 0.8921631743432664
}
[08/27/2025 22:38:11 INFO]: Training loss at epoch 30: 0.8694087862968445
[08/27/2025 22:38:31 INFO]: Training loss at epoch 19: 0.9396852254867554
[08/27/2025 22:38:34 INFO]: Training loss at epoch 17: 0.9249347448348999
[08/27/2025 22:38:58 INFO]: Training loss at epoch 72: 1.0870150029659271
[08/27/2025 22:39:01 INFO]: Training loss at epoch 0: 1.2938671112060547
[08/27/2025 22:39:16 INFO]: Training loss at epoch 43: 1.0014971494674683
[08/27/2025 22:39:20 INFO]: New best epoch, val score: -0.6971192890489866
[08/27/2025 22:39:20 INFO]: Saving model to: blotchy-Amado_trial_61/model_best.pth
[08/27/2025 22:39:26 INFO]: Training loss at epoch 36: 0.9052640199661255
[08/27/2025 22:39:50 INFO]: Training stats: {
    "score": -0.9990758970451852,
    "rmse": 0.9990758970451852
}
[08/27/2025 22:39:50 INFO]: Val stats: {
    "score": -0.6633096967334737,
    "rmse": 0.6633096967334737
}
[08/27/2025 22:39:50 INFO]: Test stats: {
    "score": -0.8712447726156548,
    "rmse": 0.8712447726156548
}
[08/27/2025 22:39:55 INFO]: Training loss at epoch 33: 1.1599583625793457
[08/27/2025 22:40:06 INFO]: Training loss at epoch 44: 0.8118329644203186
[08/27/2025 22:40:07 INFO]: Training loss at epoch 27: 1.0756623446941376
[08/27/2025 22:40:16 INFO]: Training loss at epoch 28: 1.0279878973960876
[08/27/2025 22:40:41 INFO]: Training loss at epoch 24: 0.9419963657855988
[08/27/2025 22:41:03 INFO]: Training loss at epoch 36: 1.0974762439727783
[08/27/2025 22:41:06 INFO]: Training loss at epoch 33: 0.7297273874282837
[08/27/2025 22:41:37 INFO]: Training loss at epoch 14: 1.111951470375061
[08/27/2025 22:41:38 INFO]: Training loss at epoch 36: 1.0790640711784363
[08/27/2025 22:41:44 INFO]: Training loss at epoch 10: 0.9769323468208313
[08/27/2025 22:41:55 INFO]: Training loss at epoch 1: 1.0613538026809692
[08/27/2025 22:42:27 INFO]: Training loss at epoch 36: 1.232601284980774
[08/27/2025 22:42:36 INFO]: Training loss at epoch 18: 1.0077209174633026
[08/27/2025 22:43:03 INFO]: Training loss at epoch 26: 1.0140442550182343
[08/27/2025 22:43:46 INFO]: Training loss at epoch 20: 1.2164793610572815
[08/27/2025 22:43:54 INFO]: Training loss at epoch 29: 0.792575478553772
[08/27/2025 22:44:23 INFO]: Training loss at epoch 73: 0.9920071363449097
[08/27/2025 22:44:39 INFO]: Training loss at epoch 25: 1.015443593263626
[08/27/2025 22:44:43 INFO]: Training loss at epoch 28: 1.0962727665901184
[08/27/2025 22:44:44 INFO]: Training loss at epoch 37: 1.0125247240066528
[08/27/2025 22:44:46 INFO]: Training loss at epoch 2: 1.1994192004203796
[08/27/2025 22:45:00 INFO]: Training loss at epoch 31: 0.8890885710716248
[08/27/2025 22:45:05 INFO]: Training stats: {
    "score": -1.0004847006109059,
    "rmse": 1.0004847006109059
}
[08/27/2025 22:45:05 INFO]: Val stats: {
    "score": -0.6695110395059082,
    "rmse": 0.6695110395059082
}
[08/27/2025 22:45:05 INFO]: Test stats: {
    "score": -0.8684092054363179,
    "rmse": 0.8684092054363179
}
[08/27/2025 22:45:30 INFO]: Training loss at epoch 15: 1.200340449810028
[08/27/2025 22:45:41 INFO]: Training loss at epoch 11: 1.0748931169509888
[08/27/2025 22:45:50 INFO]: Training loss at epoch 34: 0.8462170958518982
[08/27/2025 22:46:32 INFO]: Training loss at epoch 19: 1.0679191946983337
[08/27/2025 22:46:34 INFO]: Training loss at epoch 37: 1.0055910050868988
[08/27/2025 22:47:08 INFO]: Training loss at epoch 37: 0.8755293190479279
[08/27/2025 22:47:41 INFO]: Training loss at epoch 3: 0.9021114706993103
[08/27/2025 22:47:43 INFO]: Training loss at epoch 21: 1.2676770687103271
[08/27/2025 22:47:47 INFO]: Training loss at epoch 34: 1.0268880426883698
[08/27/2025 22:47:50 INFO]: Training stats: {
    "score": -1.0164294701178915,
    "rmse": 1.0164294701178915
}
[08/27/2025 22:47:50 INFO]: Val stats: {
    "score": -0.7481856638239668,
    "rmse": 0.7481856638239668
}
[08/27/2025 22:47:50 INFO]: Test stats: {
    "score": -0.9159590797485042,
    "rmse": 0.9159590797485042
}
[08/27/2025 22:47:52 INFO]: Training loss at epoch 36: 0.8541219532489777
[08/27/2025 22:48:13 INFO]: Training loss at epoch 27: 1.0049640238285065
[08/27/2025 22:48:27 INFO]: Training loss at epoch 44: 1.059008151292801
[08/27/2025 22:48:36 INFO]: Training loss at epoch 26: 1.1255401074886322
[08/27/2025 22:48:41 INFO]: Training loss at epoch 30: 0.9924394488334656
[08/27/2025 22:48:50 INFO]: Training loss at epoch 45: 0.9790016710758209
[08/27/2025 22:49:05 INFO]: New best epoch, val score: -0.6613613306451657
[08/27/2025 22:49:05 INFO]: Saving model to: blotchy-Amado_trial_56/model_best.pth
[08/27/2025 22:49:23 INFO]: Training loss at epoch 29: 1.1383537650108337
[08/27/2025 22:49:24 INFO]: Training loss at epoch 16: 0.919445276260376
[08/27/2025 22:49:41 INFO]: Training loss at epoch 12: 1.3210465013980865
[08/27/2025 22:49:51 INFO]: Training loss at epoch 74: 0.9793151319026947
[08/27/2025 22:50:05 INFO]: Training loss at epoch 38: 0.8677580952644348
[08/27/2025 22:50:30 INFO]: Training loss at epoch 35: 1.229498565196991
[08/27/2025 22:50:33 INFO]: Training loss at epoch 4: 1.1618409156799316
[08/27/2025 22:50:54 INFO]: New best epoch, val score: -0.6677806101091456
[08/27/2025 22:50:54 INFO]: Saving model to: blotchy-Amado_trial_61/model_best.pth
[08/27/2025 22:50:58 INFO]: Training stats: {
    "score": -0.9969573940089902,
    "rmse": 0.9969573940089902
}
[08/27/2025 22:50:58 INFO]: Val stats: {
    "score": -0.6786324459376233,
    "rmse": 0.6786324459376233
}
[08/27/2025 22:50:58 INFO]: Test stats: {
    "score": -0.872675066790009,
    "rmse": 0.872675066790009
}
[08/27/2025 22:51:39 INFO]: Training loss at epoch 22: 1.0350375771522522
[08/27/2025 22:51:49 INFO]: Training loss at epoch 20: 0.7692036330699921
[08/27/2025 22:51:49 INFO]: Training loss at epoch 38: 0.967322438955307
[08/27/2025 22:51:51 INFO]: Training loss at epoch 32: 1.0817782282829285
[08/27/2025 22:52:03 INFO]: Training loss at epoch 38: 1.0807277858257294
[08/27/2025 22:52:18 INFO]: Training loss at epoch 31: 1.2453693747520447
[08/27/2025 22:52:38 INFO]: Training loss at epoch 27: 0.94957035779953
[08/27/2025 22:52:38 INFO]: Training loss at epoch 37: 0.9309044182300568
[08/27/2025 22:53:21 INFO]: Training loss at epoch 17: 1.09092116355896
[08/27/2025 22:53:29 INFO]: Training loss at epoch 28: 1.190058946609497
[08/27/2025 22:53:30 INFO]: Training loss at epoch 5: 1.2445915937423706
[08/27/2025 22:53:42 INFO]: Training loss at epoch 13: 0.9981081485748291
[08/27/2025 22:53:52 INFO]: New best epoch, val score: -0.6648526932004313
[08/27/2025 22:53:52 INFO]: Saving model to: blotchy-Amado_trial_61/model_best.pth
[08/27/2025 22:55:15 INFO]: Training loss at epoch 36: 1.084816813468933
[08/27/2025 22:55:19 INFO]: Training loss at epoch 75: 0.9030049741268158
[08/27/2025 22:55:26 INFO]: Training loss at epoch 39: 0.8920823633670807
[08/27/2025 22:55:37 INFO]: Training loss at epoch 23: 1.0613440871238708
[08/27/2025 22:55:38 INFO]: Training loss at epoch 30: 0.8902005553245544
[08/27/2025 22:55:40 INFO]: Training loss at epoch 35: 0.9798609614372253
[08/27/2025 22:55:47 INFO]: Training loss at epoch 21: 1.0426441431045532
[08/27/2025 22:55:54 INFO]: Training loss at epoch 32: 0.9882246553897858
[08/27/2025 22:56:24 INFO]: Training loss at epoch 6: 1.1888499855995178
[08/27/2025 22:56:30 INFO]: Training loss at epoch 39: 0.9757311642169952
[08/27/2025 22:56:35 INFO]: Training loss at epoch 28: 1.0256038308143616
[08/27/2025 22:56:44 INFO]: New best epoch, val score: -0.6619110409493418
[08/27/2025 22:56:44 INFO]: Saving model to: blotchy-Amado_trial_61/model_best.pth
[08/27/2025 22:57:13 INFO]: Training stats: {
    "score": -1.0011402907947866,
    "rmse": 1.0011402907947866
}
[08/27/2025 22:57:13 INFO]: Val stats: {
    "score": -0.7082079882517176,
    "rmse": 0.7082079882517176
}
[08/27/2025 22:57:13 INFO]: Test stats: {
    "score": -0.8866931904345065,
    "rmse": 0.8866931904345065
}
[08/27/2025 22:57:17 INFO]: Training loss at epoch 18: 1.1367193460464478
[08/27/2025 22:57:37 INFO]: Training loss at epoch 39: 1.0456081628799438
[08/27/2025 22:57:38 INFO]: Training loss at epoch 46: 0.9483544528484344
[08/27/2025 22:57:41 INFO]: Training loss at epoch 14: 1.2910799384117126
[08/27/2025 22:57:43 INFO]: Training loss at epoch 45: 0.8398295938968658
[08/27/2025 22:58:07 INFO]: Training stats: {
    "score": -1.0182912167256313,
    "rmse": 1.0182912167256313
}
[08/27/2025 22:58:07 INFO]: Val stats: {
    "score": -0.6587204129158981,
    "rmse": 0.6587204129158981
}
[08/27/2025 22:58:07 INFO]: Test stats: {
    "score": -0.8750344587352834,
    "rmse": 0.8750344587352834
}
[08/27/2025 22:58:10 INFO]: New best epoch, val score: -0.6605289981082006
[08/27/2025 22:58:10 INFO]: Saving model to: blotchy-Amado_trial_60/model_best.pth
[08/27/2025 22:58:42 INFO]: Training loss at epoch 33: 0.8452068865299225
[08/27/2025 22:58:45 INFO]: Training loss at epoch 29: 1.4627858102321625
[08/27/2025 22:59:19 INFO]: Training loss at epoch 7: 1.1274012923240662
[08/27/2025 22:59:27 INFO]: Training stats: {
    "score": -0.9974758074904665,
    "rmse": 0.9974758074904665
}
[08/27/2025 22:59:27 INFO]: Val stats: {
    "score": -0.6839097403372235,
    "rmse": 0.6839097403372235
}
[08/27/2025 22:59:27 INFO]: Test stats: {
    "score": -0.876426622219362,
    "rmse": 0.876426622219362
}
[08/27/2025 22:59:30 INFO]: Training loss at epoch 33: 1.0409650802612305
[08/27/2025 22:59:37 INFO]: Training loss at epoch 24: 1.352654606103897
[08/27/2025 22:59:39 INFO]: New best epoch, val score: -0.6596938281947804
[08/27/2025 22:59:39 INFO]: Saving model to: blotchy-Amado_trial_61/model_best.pth
[08/27/2025 22:59:49 INFO]: Training loss at epoch 22: 0.8129071295261383
[08/27/2025 22:59:59 INFO]: Training loss at epoch 37: 0.8577646315097809
[08/27/2025 23:00:20 INFO]: Training loss at epoch 31: 1.1766738891601562
[08/27/2025 23:00:30 INFO]: Training stats: {
    "score": -1.0024445258794161,
    "rmse": 1.0024445258794161
}
[08/27/2025 23:00:30 INFO]: Val stats: {
    "score": -0.7182872402539546,
    "rmse": 0.7182872402539546
}
[08/27/2025 23:00:30 INFO]: Test stats: {
    "score": -0.8973086903842767,
    "rmse": 0.8973086903842767
}
[08/27/2025 23:00:35 INFO]: Training loss at epoch 29: 0.9926237165927887
[08/27/2025 23:00:48 INFO]: Training loss at epoch 76: 1.0097299814224243
[08/27/2025 23:00:50 INFO]: Training loss at epoch 37: 0.8362274765968323
[08/27/2025 23:01:13 INFO]: Training loss at epoch 19: 1.2000518441200256
[08/27/2025 23:01:42 INFO]: Training loss at epoch 15: 0.8919506669044495
[08/27/2025 23:01:53 INFO]: Training stats: {
    "score": -0.9995909088829461,
    "rmse": 0.9995909088829461
}
[08/27/2025 23:01:53 INFO]: Val stats: {
    "score": -0.6913207185952409,
    "rmse": 0.6913207185952409
}
[08/27/2025 23:01:53 INFO]: Test stats: {
    "score": -0.8789448028423034,
    "rmse": 0.8789448028423034
}
[08/27/2025 23:02:13 INFO]: Training loss at epoch 8: 1.091156005859375
[08/27/2025 23:02:29 INFO]: Training stats: {
    "score": -1.002751327655284,
    "rmse": 1.002751327655284
}
[08/27/2025 23:02:29 INFO]: Val stats: {
    "score": -0.7146531097959361,
    "rmse": 0.7146531097959361
}
[08/27/2025 23:02:29 INFO]: Test stats: {
    "score": -0.8913842272981167,
    "rmse": 0.8913842272981167
}
[08/27/2025 23:02:33 INFO]: New best epoch, val score: -0.6584443051031972
[08/27/2025 23:02:33 INFO]: Saving model to: blotchy-Amado_trial_61/model_best.pth
[08/27/2025 23:02:33 INFO]: Training loss at epoch 40: 1.0383747220039368
[08/27/2025 23:02:47 INFO]: Training loss at epoch 40: 1.0344641208648682
[08/27/2025 23:03:04 INFO]: Training loss at epoch 34: 1.1154300570487976
[08/27/2025 23:03:34 INFO]: Training loss at epoch 36: 0.7900101244449615
[08/27/2025 23:03:35 INFO]: Training loss at epoch 25: 1.0119043588638306
[08/27/2025 23:03:40 INFO]: Training loss at epoch 38: 0.8736589848995209
[08/27/2025 23:03:48 INFO]: Training loss at epoch 23: 1.1030051112174988
[08/27/2025 23:04:16 INFO]: New best epoch, val score: -0.6682164186298337
[08/27/2025 23:04:16 INFO]: Saving model to: blotchy-Amado_trial_58/model_best.pth
[08/27/2025 23:04:38 INFO]: Training loss at epoch 38: 1.1471197009086609
[08/27/2025 23:04:57 INFO]: Training loss at epoch 40: 1.1701040863990784
[08/27/2025 23:04:57 INFO]: Training loss at epoch 32: 1.2562730312347412
[08/27/2025 23:05:05 INFO]: Training loss at epoch 9: 1.0850706696510315
[08/27/2025 23:05:32 INFO]: Training loss at epoch 34: 0.8504690825939178
[08/27/2025 23:05:44 INFO]: Training loss at epoch 30: 0.9014723598957062
[08/27/2025 23:05:45 INFO]: Training loss at epoch 16: 1.0506795048713684
[08/27/2025 23:05:52 INFO]: Training loss at epoch 30: 0.8620930314064026
[08/27/2025 23:06:03 INFO]: Training stats: {
    "score": -1.0116763574702505,
    "rmse": 1.0116763574702505
}
[08/27/2025 23:06:03 INFO]: Val stats: {
    "score": -0.6580954068728997,
    "rmse": 0.6580954068728997
}
[08/27/2025 23:06:03 INFO]: Test stats: {
    "score": -0.8717967968476042,
    "rmse": 0.8717967968476042
}
[08/27/2025 23:06:14 INFO]: Training loss at epoch 77: 0.9312094449996948
[08/27/2025 23:06:22 INFO]: Training loss at epoch 47: 1.2976109981536865
[08/27/2025 23:06:24 INFO]: Training loss at epoch 20: 0.9296830296516418
[08/27/2025 23:06:24 INFO]: New best epoch, val score: -0.6580954068728997
[08/27/2025 23:06:24 INFO]: Saving model to: blotchy-Amado_trial_61/model_best.pth
[08/27/2025 23:06:42 INFO]: Training loss at epoch 35: 1.1902683675289154
[08/27/2025 23:06:56 INFO]: Training loss at epoch 46: 0.9770697355270386
[08/27/2025 23:07:31 INFO]: Training loss at epoch 41: 0.9317440688610077
[08/27/2025 23:07:35 INFO]: Training loss at epoch 26: 0.8799828886985779
[08/27/2025 23:07:50 INFO]: Training loss at epoch 24: 1.124849796295166
[08/27/2025 23:07:56 INFO]: Training loss at epoch 41: 1.2530236840248108
[08/27/2025 23:09:02 INFO]: Training loss at epoch 10: 1.064994752407074
[08/27/2025 23:09:23 INFO]: Training loss at epoch 39: 1.1645811796188354
[08/27/2025 23:09:41 INFO]: Training loss at epoch 33: 1.0057756006717682
[08/27/2025 23:09:47 INFO]: Training loss at epoch 17: 1.0319373607635498
[08/27/2025 23:09:53 INFO]: Training loss at epoch 31: 0.9078386425971985
[08/27/2025 23:10:15 INFO]: Running Final Evaluation...
[08/27/2025 23:10:18 INFO]: Training loss at epoch 36: 0.911605566740036
[08/27/2025 23:10:21 INFO]: Training loss at epoch 21: 1.082560956478119
[08/27/2025 23:10:32 INFO]: Training loss at epoch 41: 1.0106933116912842
[08/27/2025 23:10:58 INFO]: Training stats: {
    "score": -0.9983662908505523,
    "rmse": 0.9983662908505523
}
[08/27/2025 23:10:58 INFO]: Val stats: {
    "score": -0.7008266317630216,
    "rmse": 0.7008266317630216
}
[08/27/2025 23:10:58 INFO]: Test stats: {
    "score": -0.8850806548979956,
    "rmse": 0.8850806548979956
}
[08/27/2025 23:11:05 INFO]: Training loss at epoch 31: 1.103533297777176
[08/27/2025 23:11:29 INFO]: Training loss at epoch 37: 0.9854068160057068
[08/27/2025 23:11:31 INFO]: Training loss at epoch 27: 1.0179014801979065
[08/27/2025 23:11:43 INFO]: Training loss at epoch 78: 1.052036315202713
[08/27/2025 23:11:46 INFO]: Training loss at epoch 25: 0.846343606710434
[08/27/2025 23:11:54 INFO]: Training loss at epoch 11: 1.0946112275123596
[08/27/2025 23:12:08 INFO]: Training accuracy: {
    "score": -1.0163133815460967,
    "rmse": 1.0163133815460967
}
[08/27/2025 23:12:08 INFO]: Val accuracy: {
    "score": -0.6598341576351207,
    "rmse": 0.6598341576351207
}
[08/27/2025 23:12:08 INFO]: Test accuracy: {
    "score": -0.8716244175071495,
    "rmse": 0.8716244175071495
}
[08/27/2025 23:12:08 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_54",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8716244175071495,
        "rmse": 0.8716244175071495
    },
    "train_stats": {
        "score": -1.0163133815460967,
        "rmse": 1.0163133815460967
    },
    "val_stats": {
        "score": -0.6598341576351207,
        "rmse": 0.6598341576351207
    }
}
[08/27/2025 23:12:08 INFO]: Procewss finished for trial blotchy-Amado_trial_54
[08/27/2025 23:12:09 INFO]: 
_________________________________________________

[08/27/2025 23:12:09 INFO]: train_net_for_optune.py main() running.
[08/27/2025 23:12:09 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 2.3318558302970227
  attention_dropout: 0.49979336895756393
  ffn_dropout: 0.49979336895756393
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 7.557312990929644e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_62

[08/27/2025 23:12:09 INFO]: This ft_transformer has 5.450 million parameters.
[08/27/2025 23:12:09 INFO]: Training will start at epoch 0.
[08/27/2025 23:12:09 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 23:12:12 INFO]: Training loss at epoch 42: 0.9435533881187439
[08/27/2025 23:12:25 INFO]: Training loss at epoch 35: 0.948138564825058
[08/27/2025 23:13:15 INFO]: Training loss at epoch 42: 0.8861146569252014
[08/27/2025 23:13:43 INFO]: Training loss at epoch 38: 0.7885439395904541
[08/27/2025 23:13:48 INFO]: Training loss at epoch 18: 0.9963418841362
[08/27/2025 23:13:53 INFO]: Training loss at epoch 32: 0.9731523394584656
[08/27/2025 23:13:53 INFO]: Training loss at epoch 37: 0.9898945689201355
[08/27/2025 23:14:15 INFO]: Training loss at epoch 22: 0.9582451283931732
[08/27/2025 23:14:39 INFO]: Training loss at epoch 0: 1.1816759705543518
[08/27/2025 23:14:47 INFO]: Training loss at epoch 12: 1.0210039019584656
[08/27/2025 23:14:51 INFO]: Training loss at epoch 39: 1.039486050605774
[08/27/2025 23:15:01 INFO]: New best epoch, val score: -0.6637019514139156
[08/27/2025 23:15:01 INFO]: Saving model to: blotchy-Amado_trial_62/model_best.pth
[08/27/2025 23:15:07 INFO]: Training loss at epoch 48: 0.9151991903781891
[08/27/2025 23:15:28 INFO]: Training loss at epoch 28: 1.1564210057258606
[08/27/2025 23:15:40 INFO]: Training loss at epoch 40: 0.9944061636924744
[08/27/2025 23:15:45 INFO]: Training loss at epoch 26: 1.097524642944336
[08/27/2025 23:16:07 INFO]: Training loss at epoch 42: 1.2392178773880005
[08/27/2025 23:16:15 INFO]: Training loss at epoch 47: 0.8443773984909058
[08/27/2025 23:16:24 INFO]: Training loss at epoch 32: 1.1428343057632446
[08/27/2025 23:16:53 INFO]: Training loss at epoch 43: 0.7930620312690735
[08/27/2025 23:17:17 INFO]: Training loss at epoch 79: 0.9074903428554535
[08/27/2025 23:17:35 INFO]: Training loss at epoch 1: 0.9080284535884857
[08/27/2025 23:17:35 INFO]: Training loss at epoch 38: 1.1227328181266785
[08/27/2025 23:17:46 INFO]: Training loss at epoch 13: 0.8098637014627457
[08/27/2025 23:17:49 INFO]: Training loss at epoch 19: 0.9355496764183044
[08/27/2025 23:17:50 INFO]: Training loss at epoch 33: 1.0409238934516907
[08/27/2025 23:18:09 INFO]: Training loss at epoch 23: 0.9652315974235535
[08/27/2025 23:18:37 INFO]: Training loss at epoch 43: 1.3094069361686707
[08/27/2025 23:18:41 INFO]: Training stats: {
    "score": -0.9847217266829146,
    "rmse": 0.9847217266829146
}
[08/27/2025 23:18:41 INFO]: Val stats: {
    "score": -0.6610074921201476,
    "rmse": 0.6610074921201476
}
[08/27/2025 23:18:41 INFO]: Test stats: {
    "score": -0.8718348038364506,
    "rmse": 0.8718348038364506
}
[08/27/2025 23:19:11 INFO]: Training stats: {
    "score": -0.9985037675098016,
    "rmse": 0.9985037675098016
}
[08/27/2025 23:19:11 INFO]: Val stats: {
    "score": -0.6773735687800158,
    "rmse": 0.6773735687800158
}
[08/27/2025 23:19:11 INFO]: Test stats: {
    "score": -0.8732593341159071,
    "rmse": 0.8732593341159071
}
[08/27/2025 23:19:16 INFO]: Training stats: {
    "score": -0.9998262663273757,
    "rmse": 0.9998262663273757
}
[08/27/2025 23:19:16 INFO]: Val stats: {
    "score": -0.6695777874634411,
    "rmse": 0.6695777874634411
}
[08/27/2025 23:19:16 INFO]: Test stats: {
    "score": -0.8696160771535538,
    "rmse": 0.8696160771535538
}
[08/27/2025 23:19:16 INFO]: Training loss at epoch 36: 1.167649120092392
[08/27/2025 23:19:26 INFO]: Training loss at epoch 38: 1.1360423564910889
[08/27/2025 23:19:27 INFO]: Training loss at epoch 29: 0.9221444725990295
[08/27/2025 23:19:44 INFO]: Training loss at epoch 27: 0.9849824607372284
[08/27/2025 23:19:59 INFO]: New best epoch, val score: -0.6610074921201476
[08/27/2025 23:19:59 INFO]: Saving model to: blotchy-Amado_trial_31/model_best.pth
[08/27/2025 23:20:23 INFO]: Training loss at epoch 41: 1.2214168906211853
[08/27/2025 23:20:27 INFO]: Training loss at epoch 2: 1.184468150138855
[08/27/2025 23:20:42 INFO]: Training loss at epoch 14: 1.3187281489372253
[08/27/2025 23:20:46 INFO]: Training stats: {
    "score": -1.0013881213702027,
    "rmse": 1.0013881213702027
}
[08/27/2025 23:20:46 INFO]: Val stats: {
    "score": -0.7031910624406337,
    "rmse": 0.7031910624406337
}
[08/27/2025 23:20:46 INFO]: Test stats: {
    "score": -0.8913968968733231,
    "rmse": 0.8913968968733231
}
[08/27/2025 23:20:46 INFO]: New best epoch, val score: -0.6631268032234486
[08/27/2025 23:20:46 INFO]: Saving model to: blotchy-Amado_trial_62/model_best.pth
[08/27/2025 23:21:11 INFO]: Training loss at epoch 39: 1.1618369221687317
[08/27/2025 23:21:44 INFO]: Training loss at epoch 44: 1.0457693338394165
[08/27/2025 23:21:46 INFO]: Training loss at epoch 33: 1.0125588476657867
[08/27/2025 23:21:47 INFO]: Training loss at epoch 43: 0.9419175386428833
[08/27/2025 23:21:51 INFO]: Training loss at epoch 34: 1.007528841495514
[08/27/2025 23:22:09 INFO]: Training loss at epoch 24: 0.908417284488678
[08/27/2025 23:22:26 INFO]: Training stats: {
    "score": -0.9985037459626017,
    "rmse": 0.9985037459626017
}
[08/27/2025 23:22:26 INFO]: Val stats: {
    "score": -0.683018417460131,
    "rmse": 0.683018417460131
}
[08/27/2025 23:22:26 INFO]: Test stats: {
    "score": -0.8740851374573961,
    "rmse": 0.8740851374573961
}
[08/27/2025 23:23:15 INFO]: Training loss at epoch 20: 1.0613492131233215
[08/27/2025 23:23:21 INFO]: Training loss at epoch 3: 1.114389955997467
[08/27/2025 23:23:39 INFO]: Training loss at epoch 15: 1.0638291239738464
[08/27/2025 23:23:40 INFO]: New best epoch, val score: -0.6574653669179417
[08/27/2025 23:23:40 INFO]: Saving model to: blotchy-Amado_trial_62/model_best.pth
[08/27/2025 23:23:52 INFO]: Training loss at epoch 28: 1.0451886057853699
[08/27/2025 23:23:59 INFO]: Training loss at epoch 49: 1.1121535897254944
[08/27/2025 23:24:05 INFO]: Training loss at epoch 44: 0.8826644122600555
[08/27/2025 23:24:48 INFO]: Training loss at epoch 80: 1.0689624845981598
[08/27/2025 23:24:57 INFO]: Training loss at epoch 30: 1.178067684173584
[08/27/2025 23:25:05 INFO]: Training loss at epoch 42: 0.9627854824066162
[08/27/2025 23:25:27 INFO]: Training loss at epoch 48: 0.9612533152103424
[08/27/2025 23:25:37 INFO]: Running Final Evaluation...
[08/27/2025 23:25:52 INFO]: Training loss at epoch 35: 0.8828719854354858
[08/27/2025 23:26:03 INFO]: Training loss at epoch 37: 0.9178513884544373
[08/27/2025 23:26:04 INFO]: Training loss at epoch 40: 0.861748993396759
[08/27/2025 23:26:06 INFO]: Training loss at epoch 25: 0.9623145759105682
[08/27/2025 23:26:07 INFO]: Training loss at epoch 4: 1.0249158143997192
[08/27/2025 23:26:21 INFO]: Training loss at epoch 45: 0.9658052325248718
[08/27/2025 23:26:27 INFO]: New best epoch, val score: -0.6574552222370085
[08/27/2025 23:26:27 INFO]: Saving model to: blotchy-Amado_trial_62/model_best.pth
[08/27/2025 23:26:28 INFO]: Training loss at epoch 16: 0.9359868168830872
[08/27/2025 23:26:38 INFO]: Training loss at epoch 39: 0.7148925960063934
[08/27/2025 23:26:55 INFO]: Training stats: {
    "score": -1.0004697515536425,
    "rmse": 1.0004697515536425
}
[08/27/2025 23:26:55 INFO]: Val stats: {
    "score": -0.6715700950282518,
    "rmse": 0.6715700950282518
}
[08/27/2025 23:26:55 INFO]: Test stats: {
    "score": -0.8703919757525511,
    "rmse": 0.8703919757525511
}
[08/27/2025 23:26:56 INFO]: Training loss at epoch 34: 0.8820534944534302
[08/27/2025 23:27:13 INFO]: Training loss at epoch 21: 1.014925867319107
[08/27/2025 23:27:16 INFO]: Training loss at epoch 44: 0.9999341368675232
[08/27/2025 23:27:19 INFO]: Training loss at epoch 39: 0.9173227548599243
[08/27/2025 23:27:56 INFO]: Training loss at epoch 29: 1.021308034658432
[08/27/2025 23:28:05 INFO]: Training accuracy: {
    "score": -1.0128232769498637,
    "rmse": 1.0128232769498637
}
[08/27/2025 23:28:05 INFO]: Val accuracy: {
    "score": -0.6600191241143868,
    "rmse": 0.6600191241143868
}
[08/27/2025 23:28:05 INFO]: Test accuracy: {
    "score": -0.8710118345309408,
    "rmse": 0.8710118345309408
}
[08/27/2025 23:28:05 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_52",
    "best_epoch": 11,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8710118345309408,
        "rmse": 0.8710118345309408
    },
    "train_stats": {
        "score": -1.0128232769498637,
        "rmse": 1.0128232769498637
    },
    "val_stats": {
        "score": -0.6600191241143868,
        "rmse": 0.6600191241143868
    }
}
[08/27/2025 23:28:05 INFO]: Procewss finished for trial blotchy-Amado_trial_52
[08/27/2025 23:28:05 INFO]: 
_________________________________________________

[08/27/2025 23:28:05 INFO]: train_net_for_optune.py main() running.
[08/27/2025 23:28:05 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 2.4070197761656256
  attention_dropout: 0.4981379576996268
  ffn_dropout: 0.4981379576996268
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 6.404182703292873e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_63

[08/27/2025 23:28:05 INFO]: This ft_transformer has 5.543 million parameters.
[08/27/2025 23:28:05 INFO]: Training will start at epoch 0.
[08/27/2025 23:28:05 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 23:28:51 INFO]: Training loss at epoch 31: 0.9546087682247162
[08/27/2025 23:29:02 INFO]: Training loss at epoch 5: 0.9996882975101471
[08/27/2025 23:29:14 INFO]: Training stats: {
    "score": -0.9991768703541367,
    "rmse": 0.9991768703541367
}
[08/27/2025 23:29:14 INFO]: Val stats: {
    "score": -0.6868424962990222,
    "rmse": 0.6868424962990222
}
[08/27/2025 23:29:14 INFO]: Test stats: {
    "score": -0.8782490485234883,
    "rmse": 0.8782490485234883
}
[08/27/2025 23:29:27 INFO]: Training loss at epoch 17: 1.0379506349563599
[08/27/2025 23:29:33 INFO]: Training loss at epoch 45: 0.8817262053489685
[08/27/2025 23:29:42 INFO]: Training loss at epoch 40: 0.8885500729084015
[08/27/2025 23:29:44 INFO]: Training loss at epoch 41: 1.1471143960952759
[08/27/2025 23:29:47 INFO]: Training loss at epoch 36: 1.0220319032669067
[08/27/2025 23:29:59 INFO]: Training stats: {
    "score": -0.9425804957183355,
    "rmse": 0.9425804957183355
}
[08/27/2025 23:29:59 INFO]: Val stats: {
    "score": -0.6760663090820842,
    "rmse": 0.6760663090820842
}
[08/27/2025 23:29:59 INFO]: Test stats: {
    "score": -0.9275008489572882,
    "rmse": 0.9275008489572882
}
[08/27/2025 23:30:02 INFO]: Training loss at epoch 26: 1.0857398509979248
[08/27/2025 23:30:10 INFO]: Training loss at epoch 81: 1.1830894649028778
[08/27/2025 23:30:41 INFO]: Training loss at epoch 0: 1.3151264786720276
[08/27/2025 23:31:01 INFO]: Training stats: {
    "score": -0.858822629625154,
    "rmse": 0.858822629625154
}
[08/27/2025 23:31:01 INFO]: Val stats: {
    "score": -0.7603675513068874,
    "rmse": 0.7603675513068874
}
[08/27/2025 23:31:01 INFO]: Test stats: {
    "score": -0.9510125147825615,
    "rmse": 0.9510125147825615
}
[08/27/2025 23:31:02 INFO]: New best epoch, val score: -0.6609929428389674
[08/27/2025 23:31:02 INFO]: Saving model to: blotchy-Amado_trial_31/model_best.pth
[08/27/2025 23:31:08 INFO]: New best epoch, val score: -0.8683569404797954
[08/27/2025 23:31:08 INFO]: Saving model to: blotchy-Amado_trial_63/model_best.pth
[08/27/2025 23:31:14 INFO]: Training loss at epoch 46: 1.4186687171459198
[08/27/2025 23:31:14 INFO]: Training loss at epoch 22: 1.287845253944397
[08/27/2025 23:31:48 INFO]: Running Final Evaluation...
[08/27/2025 23:31:53 INFO]: Training loss at epoch 6: 1.1253659427165985
[08/27/2025 23:32:10 INFO]: Training loss at epoch 35: 1.0163599848747253
[08/27/2025 23:32:25 INFO]: Training loss at epoch 18: 0.9691289365291595
[08/27/2025 23:32:42 INFO]: Training loss at epoch 45: 0.9554644227027893
[08/27/2025 23:32:44 INFO]: New best epoch, val score: -0.6580836070454326
[08/27/2025 23:32:44 INFO]: Saving model to: blotchy-Amado_trial_61/model_best.pth
[08/27/2025 23:32:47 INFO]: Training loss at epoch 32: 0.9412905275821686
[08/27/2025 23:32:55 INFO]: Training loss at epoch 38: 1.1482039093971252
[08/27/2025 23:33:09 INFO]: Training loss at epoch 30: 1.046801745891571
[08/27/2025 23:33:18 INFO]: Training loss at epoch 42: 0.8489961922168732
[08/27/2025 23:33:39 INFO]: Training loss at epoch 1: 0.8941885530948639
[08/27/2025 23:33:44 INFO]: Training loss at epoch 37: 1.0977454781532288
[08/27/2025 23:33:53 INFO]: Training loss at epoch 27: 0.8446681797504425
[08/27/2025 23:33:54 INFO]: Training accuracy: {
    "score": -1.010865916033224,
    "rmse": 1.010865916033224
}
[08/27/2025 23:33:54 INFO]: Val accuracy: {
    "score": -0.6554564537094338,
    "rmse": 0.6554564537094338
}
[08/27/2025 23:33:54 INFO]: Test accuracy: {
    "score": -0.8696475633374671,
    "rmse": 0.8696475633374671
}
[08/27/2025 23:33:54 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_51",
    "best_epoch": 15,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8696475633374671,
        "rmse": 0.8696475633374671
    },
    "train_stats": {
        "score": -1.010865916033224,
        "rmse": 1.010865916033224
    },
    "val_stats": {
        "score": -0.6554564537094338,
        "rmse": 0.6554564537094338
    }
}
[08/27/2025 23:33:54 INFO]: Procewss finished for trial blotchy-Amado_trial_51
[08/27/2025 23:33:54 INFO]: 
_________________________________________________

[08/27/2025 23:33:54 INFO]: train_net_for_optune.py main() running.
[08/27/2025 23:33:54 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 0.6693204725723276
  attention_dropout: 0.43603746070398985
  ffn_dropout: 0.43603746070398985
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 7.524260567173692e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_64

[08/27/2025 23:33:54 INFO]: This ft_transformer has 3.404 million parameters.
[08/27/2025 23:33:54 INFO]: Training will start at epoch 0.
[08/27/2025 23:33:54 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 23:34:35 INFO]: Training loss at epoch 49: 0.8786087930202484
[08/27/2025 23:34:42 INFO]: Training loss at epoch 7: 1.2233842611312866
[08/27/2025 23:34:56 INFO]: Training loss at epoch 46: 0.8987790942192078
[08/27/2025 23:35:12 INFO]: Training loss at epoch 23: 0.8993574678897858
[08/27/2025 23:35:18 INFO]: Training loss at epoch 19: 1.073006272315979
[08/27/2025 23:35:40 INFO]: Training loss at epoch 50: 0.9142171740531921
[08/27/2025 23:35:41 INFO]: Training loss at epoch 82: 0.9282128512859344
[08/27/2025 23:35:49 INFO]: Training loss at epoch 0: 1.2604364156723022
[08/27/2025 23:36:03 INFO]: New best epoch, val score: -0.6608713643858036
[08/27/2025 23:36:03 INFO]: Saving model to: blotchy-Amado_trial_64/model_best.pth
[08/27/2025 23:36:17 INFO]: Training stats: {
    "score": -1.0060556405869148,
    "rmse": 1.0060556405869148
}
[08/27/2025 23:36:17 INFO]: Val stats: {
    "score": -0.6577321812184274,
    "rmse": 0.6577321812184274
}
[08/27/2025 23:36:17 INFO]: Test stats: {
    "score": -0.8693720924207924,
    "rmse": 0.8693720924207924
}
[08/27/2025 23:36:36 INFO]: Training loss at epoch 2: 1.2610350251197815
[08/27/2025 23:36:39 INFO]: New best epoch, val score: -0.6577321812184274
[08/27/2025 23:36:39 INFO]: Saving model to: blotchy-Amado_trial_61/model_best.pth
[08/27/2025 23:36:47 INFO]: Training loss at epoch 33: 1.1588667631149292
[08/27/2025 23:36:55 INFO]: Training loss at epoch 43: 1.4192448258399963
[08/27/2025 23:37:07 INFO]: Training loss at epoch 31: 0.9874421060085297
[08/27/2025 23:37:25 INFO]: Training loss at epoch 36: 1.2531038522720337
[08/27/2025 23:37:33 INFO]: Training loss at epoch 8: 0.8492565751075745
[08/27/2025 23:37:39 INFO]: Training stats: {
    "score": -1.013644097165405,
    "rmse": 1.013644097165405
}
[08/27/2025 23:37:39 INFO]: Val stats: {
    "score": -0.6653691251924887,
    "rmse": 0.6653691251924887
}
[08/27/2025 23:37:39 INFO]: Test stats: {
    "score": -0.8784496994626455,
    "rmse": 0.8784496994626455
}
[08/27/2025 23:37:43 INFO]: Training loss at epoch 38: 0.9211392104625702
[08/27/2025 23:37:52 INFO]: Training loss at epoch 40: 0.9036674797534943
[08/27/2025 23:37:53 INFO]: Training loss at epoch 28: 1.1101508140563965
[08/27/2025 23:37:53 INFO]: Training loss at epoch 1: 1.0666536688804626
[08/27/2025 23:38:11 INFO]: Training loss at epoch 46: 1.1447124183177948
[08/27/2025 23:39:12 INFO]: Training loss at epoch 20: 0.9004631042480469
[08/27/2025 23:39:12 INFO]: Training loss at epoch 24: 0.8196605443954468
[08/27/2025 23:39:29 INFO]: Training loss at epoch 3: 1.3648420572280884
[08/27/2025 23:39:42 INFO]: Training loss at epoch 39: 1.0039460957050323
[08/27/2025 23:39:48 INFO]: New best epoch, val score: -0.6650851504903889
[08/27/2025 23:39:48 INFO]: Saving model to: blotchy-Amado_trial_63/model_best.pth
[08/27/2025 23:39:59 INFO]: Training loss at epoch 2: 1.1092050671577454
[08/27/2025 23:40:25 INFO]: Training loss at epoch 47: 1.087455689907074
[08/27/2025 23:40:25 INFO]: Training loss at epoch 9: 1.001720279455185
[08/27/2025 23:40:30 INFO]: Training loss at epoch 44: 1.1363498270511627
[08/27/2025 23:40:43 INFO]: Training loss at epoch 41: 1.0132316946983337
[08/27/2025 23:40:46 INFO]: Training loss at epoch 34: 0.9770242869853973
[08/27/2025 23:41:05 INFO]: Running Final Evaluation...
[08/27/2025 23:41:06 INFO]: Training loss at epoch 32: 1.179549127817154
[08/27/2025 23:41:08 INFO]: Training loss at epoch 83: 1.0423206686973572
[08/27/2025 23:41:24 INFO]: Training stats: {
    "score": -1.0069941251920203,
    "rmse": 1.0069941251920203
}
[08/27/2025 23:41:24 INFO]: Val stats: {
    "score": -0.657526980590517,
    "rmse": 0.657526980590517
}
[08/27/2025 23:41:24 INFO]: Test stats: {
    "score": -0.8707230215483408,
    "rmse": 0.8707230215483408
}
[08/27/2025 23:41:40 INFO]: Training loss at epoch 39: 0.9859669804573059
[08/27/2025 23:41:46 INFO]: Training loss at epoch 29: 1.286788284778595
[08/27/2025 23:42:00 INFO]: Training loss at epoch 3: 1.0384477972984314
[08/27/2025 23:42:02 INFO]: Training stats: {
    "score": -1.0281049734009007,
    "rmse": 1.0281049734009007
}
[08/27/2025 23:42:02 INFO]: Val stats: {
    "score": -0.7751474349958487,
    "rmse": 0.7751474349958487
}
[08/27/2025 23:42:02 INFO]: Test stats: {
    "score": -0.9363025782652467,
    "rmse": 0.9363025782652467
}
[08/27/2025 23:42:03 INFO]: Training loss at epoch 21: 1.0139758884906769
[08/27/2025 23:42:25 INFO]: Training loss at epoch 4: 0.8973100483417511
[08/27/2025 23:42:41 INFO]: Training loss at epoch 37: 1.1810206174850464
[08/27/2025 23:43:01 INFO]: Training stats: {
    "score": -1.0006935075269634,
    "rmse": 1.0006935075269634
}
[08/27/2025 23:43:01 INFO]: Val stats: {
    "score": -0.6682668382856252,
    "rmse": 0.6682668382856252
}
[08/27/2025 23:43:01 INFO]: Test stats: {
    "score": -0.8683683354237689,
    "rmse": 0.8683683354237689
}
[08/27/2025 23:43:07 INFO]: Training stats: {
    "score": -1.006374104478012,
    "rmse": 1.006374104478012
}
[08/27/2025 23:43:07 INFO]: Val stats: {
    "score": -0.7266772217170077,
    "rmse": 0.7266772217170077
}
[08/27/2025 23:43:07 INFO]: Test stats: {
    "score": -0.900019918417511,
    "rmse": 0.900019918417511
}
[08/27/2025 23:43:16 INFO]: Training loss at epoch 25: 1.0482746362686157
[08/27/2025 23:43:36 INFO]: Training accuracy: {
    "score": -1.0123259933472764,
    "rmse": 1.0123259933472764
}
[08/27/2025 23:43:36 INFO]: Val accuracy: {
    "score": -0.6627776115450762,
    "rmse": 0.6627776115450762
}
[08/27/2025 23:43:36 INFO]: Test accuracy: {
    "score": -0.8697262697664726,
    "rmse": 0.8697262697664726
}
[08/27/2025 23:43:36 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_50",
    "best_epoch": 16,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8697262697664726,
        "rmse": 0.8697262697664726
    },
    "train_stats": {
        "score": -1.0123259933472764,
        "rmse": 1.0123259933472764
    },
    "val_stats": {
        "score": -0.6627776115450762,
        "rmse": 0.6627776115450762
    }
}
[08/27/2025 23:43:36 INFO]: Procewss finished for trial blotchy-Amado_trial_50
[08/27/2025 23:43:37 INFO]: 
_________________________________________________

[08/27/2025 23:43:37 INFO]: train_net_for_optune.py main() running.
[08/27/2025 23:43:37 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 0.6777625846999511
  attention_dropout: 0.44875785157138276
  ffn_dropout: 0.44875785157138276
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 7.802425511364345e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_65

[08/27/2025 23:43:37 INFO]: This ft_transformer has 3.412 million parameters.
[08/27/2025 23:43:37 INFO]: Training will start at epoch 0.
[08/27/2025 23:43:37 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 23:43:49 INFO]: Training loss at epoch 47: 1.1655620336532593
[08/27/2025 23:43:52 INFO]: Training loss at epoch 40: 0.8341703414916992
[08/27/2025 23:44:05 INFO]: Training loss at epoch 4: 1.154198169708252
[08/27/2025 23:44:12 INFO]: Training loss at epoch 45: 1.0209113955497742
[08/27/2025 23:44:23 INFO]: Training loss at epoch 10: 0.9741742610931396
[08/27/2025 23:44:26 INFO]: Running Final Evaluation...
[08/27/2025 23:44:28 INFO]: Training loss at epoch 51: 1.2318379878997803
[08/27/2025 23:44:40 INFO]: Training loss at epoch 35: 1.1306432485580444
[08/27/2025 23:44:56 INFO]: Training loss at epoch 22: 0.9773343801498413
[08/27/2025 23:45:01 INFO]: Training loss at epoch 33: 1.1203411519527435
[08/27/2025 23:45:19 INFO]: Training loss at epoch 5: 1.5035646557807922
[08/27/2025 23:45:21 INFO]: Training loss at epoch 0: 1.094576358795166
[08/27/2025 23:45:38 INFO]: New best epoch, val score: -0.686367939642789
[08/27/2025 23:45:38 INFO]: Saving model to: blotchy-Amado_trial_65/model_best.pth
[08/27/2025 23:45:43 INFO]: Training loss at epoch 41: 0.8671484589576721
[08/27/2025 23:46:14 INFO]: Training loss at epoch 5: 1.0549314618110657
[08/27/2025 23:46:34 INFO]: Training loss at epoch 84: 0.9261151552200317
[08/27/2025 23:46:46 INFO]: Training loss at epoch 50: 0.9677663445472717
[08/27/2025 23:46:53 INFO]: Training loss at epoch 40: 0.9432145059108734
[08/27/2025 23:46:54 INFO]: Training loss at epoch 30: 1.0639505982398987
[08/27/2025 23:47:00 INFO]: Training accuracy: {
    "score": -1.0138472805298402,
    "rmse": 1.0138472805298402
}
[08/27/2025 23:47:00 INFO]: Val accuracy: {
    "score": -0.659986825948019,
    "rmse": 0.659986825948019
}
[08/27/2025 23:47:00 INFO]: Test accuracy: {
    "score": -0.870764038770904,
    "rmse": 0.870764038770904
}
[08/27/2025 23:47:00 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_49",
    "best_epoch": 16,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.870764038770904,
        "rmse": 0.870764038770904
    },
    "train_stats": {
        "score": -1.0138472805298402,
        "rmse": 1.0138472805298402
    },
    "val_stats": {
        "score": -0.659986825948019,
        "rmse": 0.659986825948019
    }
}
[08/27/2025 23:47:00 INFO]: Procewss finished for trial blotchy-Amado_trial_49
[08/27/2025 23:47:00 INFO]: 
_________________________________________________

[08/27/2025 23:47:00 INFO]: train_net_for_optune.py main() running.
[08/27/2025 23:47:00 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 2.3217413245697838
  attention_dropout: 0.49613766427735223
  ffn_dropout: 0.49613766427735223
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 7.195818307404729e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_66

[08/27/2025 23:47:01 INFO]: This ft_transformer has 8.987 million parameters.
[08/27/2025 23:47:01 INFO]: Training will start at epoch 0.
[08/27/2025 23:47:01 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 23:47:09 INFO]: Training loss at epoch 26: 1.0809395909309387
[08/27/2025 23:47:11 INFO]: Training loss at epoch 11: 1.0805331468582153
[08/27/2025 23:47:22 INFO]: Training loss at epoch 1: 1.1401117444038391
[08/27/2025 23:47:38 INFO]: New best epoch, val score: -0.6603126347961807
[08/27/2025 23:47:38 INFO]: Saving model to: blotchy-Amado_trial_65/model_best.pth
[08/27/2025 23:47:43 INFO]: Training loss at epoch 46: 1.2377904653549194
[08/27/2025 23:47:47 INFO]: Training loss at epoch 23: 1.0044366717338562
[08/27/2025 23:48:07 INFO]: Training loss at epoch 38: 0.8737481236457825
[08/27/2025 23:48:14 INFO]: Training loss at epoch 6: 1.0524964034557343
[08/27/2025 23:48:17 INFO]: Training loss at epoch 6: 1.2218408286571503
[08/27/2025 23:48:34 INFO]: Training loss at epoch 36: 0.9119996428489685
[08/27/2025 23:48:50 INFO]: Training loss at epoch 40: 0.9610595107078552
[08/27/2025 23:48:59 INFO]: Training loss at epoch 34: 0.8402236998081207
[08/27/2025 23:49:35 INFO]: Training loss at epoch 2: 1.401257038116455
[08/27/2025 23:49:39 INFO]: Running Final Evaluation...
[08/27/2025 23:49:50 INFO]: New best epoch, val score: -0.6599241610013995
[08/27/2025 23:49:50 INFO]: Saving model to: blotchy-Amado_trial_65/model_best.pth
[08/27/2025 23:50:05 INFO]: Training loss at epoch 12: 1.0133139491081238
[08/27/2025 23:50:17 INFO]: Training loss at epoch 7: 1.339680552482605
[08/27/2025 23:50:48 INFO]: Training loss at epoch 31: 0.8916005790233612
[08/27/2025 23:50:48 INFO]: Training loss at epoch 41: 0.90981125831604
[08/27/2025 23:50:50 INFO]: Training loss at epoch 24: 1.1911138892173767
[08/27/2025 23:51:08 INFO]: Training loss at epoch 27: 1.0945173501968384
[08/27/2025 23:51:13 INFO]: Training loss at epoch 7: 1.3476080298423767
[08/27/2025 23:51:17 INFO]: Training loss at epoch 47: 0.8311190605163574
[08/27/2025 23:51:29 INFO]: Training loss at epoch 0: 1.1057493686676025
[08/27/2025 23:51:36 INFO]: Training loss at epoch 42: 1.2937831282615662
[08/27/2025 23:51:40 INFO]: Training loss at epoch 3: 0.9870579242706299
[08/27/2025 23:52:03 INFO]: Training loss at epoch 85: 1.0932746529579163
[08/27/2025 23:52:05 INFO]: New best epoch, val score: -0.6635138249153022
[08/27/2025 23:52:05 INFO]: Saving model to: blotchy-Amado_trial_66/model_best.pth
[08/27/2025 23:52:20 INFO]: Training loss at epoch 8: 1.204468309879303
[08/27/2025 23:52:29 INFO]: Training loss at epoch 37: 0.8909460604190826
[08/27/2025 23:52:49 INFO]: Training accuracy: {
    "score": -1.0147046356540972,
    "rmse": 1.0147046356540972
}
[08/27/2025 23:52:49 INFO]: Val accuracy: {
    "score": -0.6623077739791723,
    "rmse": 0.6623077739791723
}
[08/27/2025 23:52:49 INFO]: Test accuracy: {
    "score": -0.8771667446859585,
    "rmse": 0.8771667446859585
}
[08/27/2025 23:52:54 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_48",
    "best_epoch": 9,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8771667446859585,
        "rmse": 0.8771667446859585
    },
    "train_stats": {
        "score": -1.0147046356540972,
        "rmse": 1.0147046356540972
    },
    "val_stats": {
        "score": -0.6623077739791723,
        "rmse": 0.6623077739791723
    }
}
[08/27/2025 23:52:54 INFO]: Procewss finished for trial blotchy-Amado_trial_48
[08/27/2025 23:52:54 INFO]: 
_________________________________________________

[08/27/2025 23:52:54 INFO]: train_net_for_optune.py main() running.
[08/27/2025 23:52:54 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 2.209159773925707
  attention_dropout: 0.4517533785959516
  ffn_dropout: 0.4517533785959516
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 7.115486067405606e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_67

[08/27/2025 23:52:55 INFO]: This ft_transformer has 8.757 million parameters.
[08/27/2025 23:52:55 INFO]: Training will start at epoch 0.
[08/27/2025 23:52:55 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 23:52:56 INFO]: Training loss at epoch 13: 1.1462043225765228
[08/27/2025 23:53:05 INFO]: Training loss at epoch 35: 0.9022199511528015
[08/27/2025 23:53:05 INFO]: Training loss at epoch 52: 0.9091641306877136
[08/27/2025 23:53:25 INFO]: Training loss at epoch 39: 0.8784126043319702
[08/27/2025 23:53:31 INFO]: Training loss at epoch 42: 0.9811009466648102
[08/27/2025 23:53:44 INFO]: Training loss at epoch 4: 0.8581950962543488
[08/27/2025 23:53:46 INFO]: Training loss at epoch 25: 0.8890233933925629
[08/27/2025 23:54:08 INFO]: Training loss at epoch 8: 1.3951472640037537
[08/27/2025 23:54:22 INFO]: Training loss at epoch 9: 0.9940879642963409
[08/27/2025 23:54:45 INFO]: Training loss at epoch 32: 0.8754251897335052
[08/27/2025 23:54:58 INFO]: Training loss at epoch 42: 0.8489656150341034
[08/27/2025 23:54:58 INFO]: Training loss at epoch 48: 0.9419765770435333
[08/27/2025 23:55:04 INFO]: Training stats: {
    "score": -1.0508186068748915,
    "rmse": 1.0508186068748915
}
[08/27/2025 23:55:04 INFO]: Val stats: {
    "score": -0.6789323105484806,
    "rmse": 0.6789323105484806
}
[08/27/2025 23:55:04 INFO]: Test stats: {
    "score": -0.896022325523908,
    "rmse": 0.896022325523908
}
[08/27/2025 23:55:12 INFO]: Training stats: {
    "score": -0.9949226278332041,
    "rmse": 0.9949226278332041
}
[08/27/2025 23:55:12 INFO]: Val stats: {
    "score": -0.6797130032419425,
    "rmse": 0.6797130032419425
}
[08/27/2025 23:55:12 INFO]: Test stats: {
    "score": -0.8753505364950374,
    "rmse": 0.8753505364950374
}
[08/27/2025 23:55:12 INFO]: Training loss at epoch 28: 1.2267667055130005
[08/27/2025 23:55:45 INFO]: Training loss at epoch 14: 1.1315686702728271
[08/27/2025 23:55:46 INFO]: Training loss at epoch 5: 1.0598569512367249
[08/27/2025 23:55:57 INFO]: Training loss at epoch 51: 1.0115403532981873
[08/27/2025 23:56:25 INFO]: Training loss at epoch 38: 0.9277566373348236
[08/27/2025 23:56:41 INFO]: Training loss at epoch 26: 0.9133225083351135
[08/27/2025 23:56:41 INFO]: Training loss at epoch 1: 0.9820004403591156
[08/27/2025 23:56:42 INFO]: Training loss at epoch 41: 0.7414663434028625
[08/27/2025 23:57:02 INFO]: Training loss at epoch 36: 1.0570823550224304
[08/27/2025 23:57:02 INFO]: Training loss at epoch 9: 1.1525442004203796
[08/27/2025 23:57:07 INFO]: Training loss at epoch 10: 1.035227745771408
[08/27/2025 23:57:19 INFO]: Training loss at epoch 0: 0.9340080916881561
[08/27/2025 23:57:30 INFO]: Training loss at epoch 86: 0.8818970322608948
[08/27/2025 23:58:01 INFO]: New best epoch, val score: -0.6580510321291317
[08/27/2025 23:58:01 INFO]: Saving model to: blotchy-Amado_trial_67/model_best.pth
[08/27/2025 23:58:01 INFO]: Training loss at epoch 6: 0.9556820094585419
[08/27/2025 23:58:08 INFO]: Training stats: {
    "score": -1.1278671676690226,
    "rmse": 1.1278671676690226
}
[08/27/2025 23:58:08 INFO]: Val stats: {
    "score": -0.9560715497880758,
    "rmse": 0.9560715497880758
}
[08/27/2025 23:58:08 INFO]: Test stats: {
    "score": -1.0690145685825418,
    "rmse": 1.0690145685825418
}
[08/27/2025 23:58:35 INFO]: Training loss at epoch 49: 0.9970181584358215
[08/27/2025 23:58:39 INFO]: Training loss at epoch 15: 1.1922316551208496
[08/27/2025 23:58:43 INFO]: Training loss at epoch 33: 0.9630205035209656
[08/27/2025 23:58:57 INFO]: Training loss at epoch 43: 1.0631088614463806
[08/27/2025 23:59:14 INFO]: Training loss at epoch 11: 1.1848906874656677
[08/27/2025 23:59:15 INFO]: Training loss at epoch 29: 1.0769421458244324
[08/27/2025 23:59:41 INFO]: Training loss at epoch 27: 1.26481431722641
[08/27/2025 23:59:49 INFO]: Training stats: {
    "score": -0.9987530508711957,
    "rmse": 0.9987530508711957
}
[08/27/2025 23:59:49 INFO]: Val stats: {
    "score": -0.6718438433255766,
    "rmse": 0.6718438433255766
}
[08/27/2025 23:59:49 INFO]: Test stats: {
    "score": -0.8698636925623974,
    "rmse": 0.8698636925623974
}
[08/28/2025 00:00:08 INFO]: Training loss at epoch 7: 1.5654597878456116
[08/28/2025 00:00:31 INFO]: Training loss at epoch 40: 1.0598012208938599
[08/28/2025 00:00:36 INFO]: Training stats: {
    "score": -1.0078749756173262,
    "rmse": 1.0078749756173262
}
[08/28/2025 00:00:36 INFO]: Val stats: {
    "score": -0.6622502740696842,
    "rmse": 0.6622502740696842
}
[08/28/2025 00:00:36 INFO]: Test stats: {
    "score": -0.8705726186276403,
    "rmse": 0.8705726186276403
}
[08/28/2025 00:00:36 INFO]: Training loss at epoch 39: 1.0343343615531921
[08/28/2025 00:01:05 INFO]: Training loss at epoch 10: 1.1260635554790497
[08/28/2025 00:01:07 INFO]: Training loss at epoch 37: 0.8760325312614441
[08/28/2025 00:01:19 INFO]: Training loss at epoch 12: 1.2153700292110443
[08/28/2025 00:01:30 INFO]: Training loss at epoch 43: 1.1003903448581696
[08/28/2025 00:01:40 INFO]: Training loss at epoch 16: 1.1286453008651733
[08/28/2025 00:01:51 INFO]: Training loss at epoch 53: 1.334280788898468
[08/28/2025 00:01:54 INFO]: Training stats: {
    "score": -1.0053307385111747,
    "rmse": 1.0053307385111747
}
[08/28/2025 00:01:54 INFO]: Val stats: {
    "score": -0.7176994620962215,
    "rmse": 0.7176994620962215
}
[08/28/2025 00:01:54 INFO]: Test stats: {
    "score": -0.900127451772773,
    "rmse": 0.900127451772773
}
[08/28/2025 00:01:58 INFO]: Training loss at epoch 2: 0.9107802510261536
[08/28/2025 00:02:12 INFO]: Training loss at epoch 8: 1.0073351860046387
[08/28/2025 00:02:41 INFO]: Training loss at epoch 1: 1.1684659123420715
[08/28/2025 00:02:41 INFO]: Training loss at epoch 43: 1.0257966816425323
[08/28/2025 00:02:51 INFO]: Training loss at epoch 28: 1.0663346946239471
[08/28/2025 00:02:55 INFO]: Training loss at epoch 34: 0.8395978212356567
[08/28/2025 00:02:56 INFO]: Training loss at epoch 44: 1.0322499871253967
[08/28/2025 00:02:59 INFO]: Training loss at epoch 87: 0.9025036096572876
[08/28/2025 00:03:22 INFO]: Running Final Evaluation...
[08/28/2025 00:03:24 INFO]: Training loss at epoch 50: 1.104066163301468
[08/28/2025 00:03:28 INFO]: Training loss at epoch 13: 0.9576783180236816
[08/28/2025 00:03:57 INFO]: Training loss at epoch 11: 1.0399715602397919
[08/28/2025 00:04:18 INFO]: New best epoch, val score: -0.6639442549158482
[08/28/2025 00:04:18 INFO]: Saving model to: blotchy-Amado_trial_63/model_best.pth
[08/28/2025 00:04:30 INFO]: Training loss at epoch 9: 1.0324026346206665
[08/28/2025 00:04:32 INFO]: Training loss at epoch 17: 1.0094668865203857
[08/28/2025 00:04:34 INFO]: Training loss at epoch 30: 0.9464031755924225
[08/28/2025 00:05:06 INFO]: Training accuracy: {
    "score": -1.0026248852804296,
    "rmse": 1.0026248852804296
}
[08/28/2025 00:05:06 INFO]: Val accuracy: {
    "score": -0.6690080566283116,
    "rmse": 0.6690080566283116
}
[08/28/2025 00:05:06 INFO]: Test accuracy: {
    "score": -0.8679832878095216,
    "rmse": 0.8679832878095216
}
[08/28/2025 00:05:06 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_59",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8679832878095216,
        "rmse": 0.8679832878095216
    },
    "train_stats": {
        "score": -1.0026248852804296,
        "rmse": 1.0026248852804296
    },
    "val_stats": {
        "score": -0.6690080566283116,
        "rmse": 0.6690080566283116
    }
}
[08/28/2025 00:05:06 INFO]: Procewss finished for trial blotchy-Amado_trial_59
[08/28/2025 00:05:06 INFO]: 
_________________________________________________

[08/28/2025 00:05:06 INFO]: train_net_for_optune.py main() running.
[08/28/2025 00:05:06 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 2.0829867474148664
  attention_dropout: 0.4576283307532407
  ffn_dropout: 0.4576283307532407
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 6.584307475448861e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_68

[08/28/2025 00:05:07 INFO]: This ft_transformer has 8.500 million parameters.
[08/28/2025 00:05:07 INFO]: Training will start at epoch 0.
[08/28/2025 00:05:07 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 00:05:11 INFO]: Training loss at epoch 38: 0.8305686712265015
[08/28/2025 00:05:13 INFO]: Training stats: {
    "score": -1.0203910563236196,
    "rmse": 1.0203910563236196
}
[08/28/2025 00:05:13 INFO]: Val stats: {
    "score": -0.7610651598627718,
    "rmse": 0.7610651598627718
}
[08/28/2025 00:05:13 INFO]: Test stats: {
    "score": -0.9230344856469191,
    "rmse": 0.9230344856469191
}
[08/28/2025 00:05:17 INFO]: Training loss at epoch 52: 1.0210385620594025
[08/28/2025 00:05:34 INFO]: Training loss at epoch 14: 1.2147065997123718
[08/28/2025 00:05:50 INFO]: Training loss at epoch 41: 0.9140927791595459
[08/28/2025 00:05:51 INFO]: Training loss at epoch 29: 1.1099519729614258
[08/28/2025 00:05:55 INFO]: Training loss at epoch 40: 0.8557004928588867
[08/28/2025 00:06:23 INFO]: Running Final Evaluation...
[08/28/2025 00:06:48 INFO]: Training stats: {
    "score": -0.997890122655023,
    "rmse": 0.997890122655023
}
[08/28/2025 00:06:48 INFO]: Val stats: {
    "score": -0.6719798840443179,
    "rmse": 0.6719798840443179
}
[08/28/2025 00:06:48 INFO]: Test stats: {
    "score": -0.8727245229001483,
    "rmse": 0.8727245229001483
}
[08/28/2025 00:06:55 INFO]: Training loss at epoch 12: 1.028319537639618
[08/28/2025 00:07:00 INFO]: Training loss at epoch 45: 1.085820496082306
[08/28/2025 00:07:03 INFO]: Training loss at epoch 51: 0.9127781093120575
[08/28/2025 00:07:15 INFO]: Training loss at epoch 3: 0.935942143201828
[08/28/2025 00:07:15 INFO]: Training loss at epoch 10: 0.9037725627422333
[08/28/2025 00:07:25 INFO]: Training loss at epoch 18: 1.0863026976585388
[08/28/2025 00:07:37 INFO]: Training loss at epoch 15: 1.293434739112854
[08/28/2025 00:07:44 INFO]: Training loss at epoch 2: 1.2332749962806702
[08/28/2025 00:08:16 INFO]: Training accuracy: {
    "score": -1.0103354036664725,
    "rmse": 1.0103354036664725
}
[08/28/2025 00:08:16 INFO]: Val accuracy: {
    "score": -0.6542280560147022,
    "rmse": 0.6542280560147022
}
[08/28/2025 00:08:16 INFO]: Test accuracy: {
    "score": -0.8717448333425147,
    "rmse": 0.8717448333425147
}
[08/28/2025 00:08:16 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_57",
    "best_epoch": 9,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8717448333425147,
        "rmse": 0.8717448333425147
    },
    "train_stats": {
        "score": -1.0103354036664725,
        "rmse": 1.0103354036664725
    },
    "val_stats": {
        "score": -0.6542280560147022,
        "rmse": 0.6542280560147022
    }
}
[08/28/2025 00:08:16 INFO]: Procewss finished for trial blotchy-Amado_trial_57
[08/28/2025 00:08:16 INFO]: 
_________________________________________________

[08/28/2025 00:08:16 INFO]: train_net_for_optune.py main() running.
[08/28/2025 00:08:16 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 2.0806411435960537
  attention_dropout: 0.49603727098630335
  ffn_dropout: 0.49603727098630335
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 7.545005736695967e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_69

[08/28/2025 00:08:16 INFO]: This ft_transformer has 5.139 million parameters.
[08/28/2025 00:08:16 INFO]: Training will start at epoch 0.
[08/28/2025 00:08:16 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 00:08:29 INFO]: Training loss at epoch 88: 1.0515892505645752
[08/28/2025 00:08:35 INFO]: Training loss at epoch 31: 0.9834725260734558
[08/28/2025 00:09:09 INFO]: Training loss at epoch 39: 1.0940289497375488
[08/28/2025 00:09:18 INFO]: Training loss at epoch 11: 0.8798176050186157
[08/28/2025 00:09:24 INFO]: Training loss at epoch 0: 1.1241825222969055
[08/28/2025 00:09:40 INFO]: Training loss at epoch 16: 1.1101132035255432
[08/28/2025 00:09:40 INFO]: Training loss at epoch 44: 0.9185509085655212
[08/28/2025 00:09:41 INFO]: Training loss at epoch 30: 1.0864852666854858
[08/28/2025 00:09:46 INFO]: Training loss at epoch 42: 0.7443836331367493
[08/28/2025 00:09:49 INFO]: Training loss at epoch 13: 1.004590928554535
[08/28/2025 00:09:57 INFO]: New best epoch, val score: -0.8006037311385252
[08/28/2025 00:09:57 INFO]: Saving model to: blotchy-Amado_trial_68/model_best.pth
[08/28/2025 00:10:18 INFO]: Training loss at epoch 19: 1.0347660183906555
[08/28/2025 00:10:30 INFO]: Training stats: {
    "score": -1.0070163539512231,
    "rmse": 1.0070163539512231
}
[08/28/2025 00:10:30 INFO]: Val stats: {
    "score": -0.7243505086976814,
    "rmse": 0.7243505086976814
}
[08/28/2025 00:10:30 INFO]: Test stats: {
    "score": -0.8991931603877641,
    "rmse": 0.8991931603877641
}
[08/28/2025 00:10:36 INFO]: Training loss at epoch 54: 0.8832530379295349
[08/28/2025 00:10:39 INFO]: Training loss at epoch 52: 1.054324209690094
[08/28/2025 00:10:41 INFO]: Training loss at epoch 0: 1.0795743465423584
[08/28/2025 00:11:00 INFO]: Training loss at epoch 46: 0.7890578806400299
[08/28/2025 00:11:01 INFO]: New best epoch, val score: -0.6671438819821899
[08/28/2025 00:11:01 INFO]: Saving model to: blotchy-Amado_trial_69/model_best.pth
[08/28/2025 00:11:06 INFO]: Running Final Evaluation...
[08/28/2025 00:11:07 INFO]: Training loss at epoch 42: 1.0982457399368286
[08/28/2025 00:11:15 INFO]: Training stats: {
    "score": -0.9961679667964931,
    "rmse": 0.9961679667964931
}
[08/28/2025 00:11:15 INFO]: Val stats: {
    "score": -0.6726963639850748,
    "rmse": 0.6726963639850748
}
[08/28/2025 00:11:15 INFO]: Test stats: {
    "score": -0.872100422957346,
    "rmse": 0.872100422957346
}
[08/28/2025 00:11:19 INFO]: Running Final Evaluation...
[08/28/2025 00:11:23 INFO]: Training loss at epoch 12: 1.0483068823814392
[08/28/2025 00:11:45 INFO]: Training loss at epoch 17: 0.9131668508052826
[08/28/2025 00:12:26 INFO]: Training loss at epoch 4: 1.1137930154800415
[08/28/2025 00:12:27 INFO]: Training accuracy: {
    "score": -1.010134376137948,
    "rmse": 1.010134376137948
}
[08/28/2025 00:12:27 INFO]: Val accuracy: {
    "score": -0.6606441111742754,
    "rmse": 0.6606441111742754
}
[08/28/2025 00:12:27 INFO]: Test accuracy: {
    "score": -0.8691132311614861,
    "rmse": 0.8691132311614861
}
[08/28/2025 00:12:27 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_55",
    "best_epoch": 21,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8691132311614861,
        "rmse": 0.8691132311614861
    },
    "train_stats": {
        "score": -1.010134376137948,
        "rmse": 1.010134376137948
    },
    "val_stats": {
        "score": -0.6606441111742754,
        "rmse": 0.6606441111742754
    }
}
[08/28/2025 00:12:27 INFO]: Procewss finished for trial blotchy-Amado_trial_55
[08/28/2025 00:12:27 INFO]: 
_________________________________________________

[08/28/2025 00:12:27 INFO]: train_net_for_optune.py main() running.
[08/28/2025 00:12:27 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 2.418894913137287
  attention_dropout: 0.46953718239598846
  ffn_dropout: 0.46953718239598846
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 6.871470799427067e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_70

[08/28/2025 00:12:27 INFO]: This ft_transformer has 5.558 million parameters.
[08/28/2025 00:12:27 INFO]: Training will start at epoch 0.
[08/28/2025 00:12:27 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 00:12:36 INFO]: Training loss at epoch 32: 0.8990389704704285
[08/28/2025 00:12:38 INFO]: Training loss at epoch 31: 0.9193036556243896
[08/28/2025 00:12:46 INFO]: Training loss at epoch 14: 1.2361120581626892
[08/28/2025 00:12:47 INFO]: Training loss at epoch 3: 0.957942932844162
[08/28/2025 00:13:27 INFO]: Training loss at epoch 13: 1.0932440757751465
[08/28/2025 00:13:28 INFO]: Training loss at epoch 1: 1.1610113382339478
[08/28/2025 00:13:47 INFO]: Training loss at epoch 44: 1.0792060196399689
[08/28/2025 00:13:49 INFO]: Training loss at epoch 18: 1.0511151552200317
[08/28/2025 00:14:00 INFO]: Training loss at epoch 89: 1.121811181306839
[08/28/2025 00:14:10 INFO]: Training loss at epoch 20: 1.0277647376060486
[08/28/2025 00:14:19 INFO]: Training loss at epoch 1: 1.2201355695724487
[08/28/2025 00:14:30 INFO]: Training loss at epoch 40: 1.0802003741264343
[08/28/2025 00:14:33 INFO]: Training loss at epoch 53: 1.1229354739189148
[08/28/2025 00:15:00 INFO]: Training loss at epoch 47: 1.0134950578212738
[08/28/2025 00:15:04 INFO]: Training loss at epoch 0: 0.8801441788673401
[08/28/2025 00:15:23 INFO]: New best epoch, val score: -0.7311061153552189
[08/28/2025 00:15:23 INFO]: Saving model to: blotchy-Amado_trial_70/model_best.pth
[08/28/2025 00:15:30 INFO]: Training loss at epoch 14: 1.3851542472839355
[08/28/2025 00:15:31 INFO]: Training loss at epoch 32: 1.1057229042053223
[08/28/2025 00:15:42 INFO]: Training loss at epoch 15: 1.4720622301101685
[08/28/2025 00:15:53 INFO]: Training loss at epoch 19: 1.1206190288066864
[08/28/2025 00:15:55 INFO]: Training stats: {
    "score": -0.9979258336953458,
    "rmse": 0.9979258336953458
}
[08/28/2025 00:15:55 INFO]: Val stats: {
    "score": -0.680284762417052,
    "rmse": 0.680284762417052
}
[08/28/2025 00:15:55 INFO]: Test stats: {
    "score": -0.8741468507279399,
    "rmse": 0.8741468507279399
}
[08/28/2025 00:15:57 INFO]: Training accuracy: {
    "score": -0.9913328271355332,
    "rmse": 0.9913328271355332
}
[08/28/2025 00:15:57 INFO]: Val accuracy: {
    "score": -0.6809467188163403,
    "rmse": 0.6809467188163403
}
[08/28/2025 00:15:57 INFO]: Test accuracy: {
    "score": -0.8702493503035352,
    "rmse": 0.8702493503035352
}
[08/28/2025 00:15:57 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_16",
    "best_epoch": 11,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8702493503035352,
        "rmse": 0.8702493503035352
    },
    "train_stats": {
        "score": -0.9913328271355332,
        "rmse": 0.9913328271355332
    },
    "val_stats": {
        "score": -0.6809467188163403,
        "rmse": 0.6809467188163403
    }
}
[08/28/2025 00:15:57 INFO]: Procewss finished for trial blotchy-Amado_trial_16
[08/28/2025 00:15:57 INFO]: 
_________________________________________________

[08/28/2025 00:15:57 INFO]: train_net_for_optune.py main() running.
[08/28/2025 00:15:57 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 2.4502333366356903
  attention_dropout: 0.499236398399924
  ffn_dropout: 0.499236398399924
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 6.67056286214323e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_71

[08/28/2025 00:15:57 INFO]: This ft_transformer has 5.596 million parameters.
[08/28/2025 00:15:57 INFO]: Training will start at epoch 0.
[08/28/2025 00:15:57 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 00:16:15 INFO]: Training loss at epoch 2: 0.9607674479484558
[08/28/2025 00:16:26 INFO]: Training loss at epoch 43: 0.9350159466266632
[08/28/2025 00:16:34 INFO]: New best epoch, val score: -0.6618899000619612
[08/28/2025 00:16:34 INFO]: Saving model to: blotchy-Amado_trial_69/model_best.pth
[08/28/2025 00:16:36 INFO]: Training stats: {
    "score": -1.0024135400313083,
    "rmse": 1.0024135400313083
}
[08/28/2025 00:16:36 INFO]: Val stats: {
    "score": -0.6658221323530973,
    "rmse": 0.6658221323530973
}
[08/28/2025 00:16:36 INFO]: Test stats: {
    "score": -0.8681024179943221,
    "rmse": 0.8681024179943221
}
[08/28/2025 00:16:37 INFO]: Training loss at epoch 33: 1.111797571182251
[08/28/2025 00:17:02 INFO]: Training loss at epoch 21: 0.9388269186019897
[08/28/2025 00:17:35 INFO]: Training loss at epoch 15: 1.0003564655780792
[08/28/2025 00:17:36 INFO]: Training loss at epoch 5: 0.9754112958908081
[08/28/2025 00:17:38 INFO]: Training loss at epoch 45: 1.0894244313240051
[08/28/2025 00:17:49 INFO]: Training loss at epoch 4: 1.0893299579620361
[08/28/2025 00:18:02 INFO]: Training loss at epoch 1: 1.380641222000122
[08/28/2025 00:18:23 INFO]: New best epoch, val score: -0.6632164868803938
[08/28/2025 00:18:23 INFO]: Saving model to: blotchy-Amado_trial_70/model_best.pth
[08/28/2025 00:18:27 INFO]: Training loss at epoch 33: 0.8577607274055481
[08/28/2025 00:18:28 INFO]: Training loss at epoch 41: 1.092310905456543
[08/28/2025 00:18:31 INFO]: Training loss at epoch 0: 1.3240122199058533
[08/28/2025 00:18:37 INFO]: Training loss at epoch 20: 1.1065583527088165
[08/28/2025 00:18:39 INFO]: Training loss at epoch 16: 1.435919463634491
[08/28/2025 00:18:53 INFO]: New best epoch, val score: -0.6638941345995018
[08/28/2025 00:18:53 INFO]: Saving model to: blotchy-Amado_trial_71/model_best.pth
[08/28/2025 00:18:59 INFO]: Training loss at epoch 48: 0.994191974401474
[08/28/2025 00:19:04 INFO]: Training loss at epoch 3: 1.3096316456794739
[08/28/2025 00:19:10 INFO]: Training loss at epoch 2: 1.235495686531067
[08/28/2025 00:19:24 INFO]: Training loss at epoch 55: 0.783053070306778
[08/28/2025 00:19:37 INFO]: Training loss at epoch 16: 1.0511307120323181
[08/28/2025 00:19:45 INFO]: New best epoch, val score: -0.6606457528289631
[08/28/2025 00:19:45 INFO]: Saving model to: blotchy-Amado_trial_68/model_best.pth
[08/28/2025 00:19:57 INFO]: Training loss at epoch 22: 0.8909675478935242
[08/28/2025 00:20:36 INFO]: Training loss at epoch 34: 1.1573999524116516
[08/28/2025 00:20:42 INFO]: Training loss at epoch 21: 0.920565664768219
[08/28/2025 00:20:59 INFO]: Training loss at epoch 2: 1.092446744441986
[08/28/2025 00:21:19 INFO]: Training loss at epoch 90: 0.9690446257591248
[08/28/2025 00:21:22 INFO]: Training loss at epoch 34: 0.8609345853328705
[08/28/2025 00:21:27 INFO]: Training loss at epoch 1: 0.9622178077697754
[08/28/2025 00:21:33 INFO]: Training loss at epoch 17: 1.2809104919433594
[08/28/2025 00:21:39 INFO]: Training loss at epoch 17: 1.1997857987880707
[08/28/2025 00:21:40 INFO]: Training loss at epoch 44: 1.004173219203949
[08/28/2025 00:21:48 INFO]: Training loss at epoch 4: 1.2090368270874023
[08/28/2025 00:22:00 INFO]: Running Final Evaluation...
[08/28/2025 00:22:26 INFO]: Training loss at epoch 42: 0.9986517131328583
[08/28/2025 00:22:39 INFO]: Training loss at epoch 6: 1.305637240409851
[08/28/2025 00:22:43 INFO]: Training loss at epoch 23: 1.221518486738205
[08/28/2025 00:22:44 INFO]: Training loss at epoch 22: 1.0074063837528229
[08/28/2025 00:22:47 INFO]: Training loss at epoch 5: 1.1241013407707214
[08/28/2025 00:22:54 INFO]: Training loss at epoch 49: 0.9707346260547638
[08/28/2025 00:23:41 INFO]: Training loss at epoch 18: 0.9945102334022522
[08/28/2025 00:23:43 INFO]: Training loss at epoch 54: 0.8022244870662689
[08/28/2025 00:23:52 INFO]: Training loss at epoch 3: 1.1620709300041199
[08/28/2025 00:24:01 INFO]: Training accuracy: {
    "score": -1.011267520465282,
    "rmse": 1.011267520465282
}
[08/28/2025 00:24:01 INFO]: Val accuracy: {
    "score": -0.6603732135366944,
    "rmse": 0.6603732135366944
}
[08/28/2025 00:24:01 INFO]: Test accuracy: {
    "score": -0.8704819250855033,
    "rmse": 0.8704819250855033
}
[08/28/2025 00:24:01 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_34",
    "best_epoch": 59,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8704819250855033,
        "rmse": 0.8704819250855033
    },
    "train_stats": {
        "score": -1.011267520465282,
        "rmse": 1.011267520465282
    },
    "val_stats": {
        "score": -0.6603732135366944,
        "rmse": 0.6603732135366944
    }
}
[08/28/2025 00:24:01 INFO]: Procewss finished for trial blotchy-Amado_trial_34
[08/28/2025 00:24:01 INFO]: 
_________________________________________________

[08/28/2025 00:24:01 INFO]: train_net_for_optune.py main() running.
[08/28/2025 00:24:01 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 0.7029883741628524
  attention_dropout: 0.4792031711805624
  ffn_dropout: 0.4792031711805624
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 6.306621745160388e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_72

[08/28/2025 00:24:01 INFO]: This ft_transformer has 3.443 million parameters.
[08/28/2025 00:24:01 INFO]: Training will start at epoch 0.
[08/28/2025 00:24:01 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 00:24:02 INFO]: Training loss at epoch 3: 1.0331307649612427
[08/28/2025 00:24:12 INFO]: Training stats: {
    "score": -0.9977463436709663,
    "rmse": 0.9977463436709663
}
[08/28/2025 00:24:12 INFO]: Val stats: {
    "score": -0.6874891550964316,
    "rmse": 0.6874891550964316
}
[08/28/2025 00:24:12 INFO]: Test stats: {
    "score": -0.8770541428242365,
    "rmse": 0.8770541428242365
}
[08/28/2025 00:24:17 INFO]: Training loss at epoch 35: 1.0532590746879578
[08/28/2025 00:24:22 INFO]: Training loss at epoch 2: 1.177815556526184
[08/28/2025 00:24:29 INFO]: Training loss at epoch 18: 1.126309871673584
[08/28/2025 00:24:33 INFO]: Training loss at epoch 5: 1.2618456482887268
[08/28/2025 00:24:33 INFO]: Training loss at epoch 35: 1.0164411664009094
[08/28/2025 00:24:46 INFO]: Training loss at epoch 45: 0.9447524845600128
[08/28/2025 00:24:49 INFO]: Training loss at epoch 23: 0.9703598618507385
[08/28/2025 00:25:28 INFO]: Training loss at epoch 46: 0.8897846043109894
[08/28/2025 00:25:40 INFO]: Training loss at epoch 24: 0.9589144885540009
[08/28/2025 00:25:49 INFO]: Training loss at epoch 19: 0.9981415271759033
[08/28/2025 00:25:55 INFO]: Training loss at epoch 0: 1.17876398563385
[08/28/2025 00:26:09 INFO]: New best epoch, val score: -0.7304635590224781
[08/28/2025 00:26:09 INFO]: Saving model to: blotchy-Amado_trial_72/model_best.pth
[08/28/2025 00:26:28 INFO]: Training stats: {
    "score": -1.0455160683976077,
    "rmse": 1.0455160683976077
}
[08/28/2025 00:26:28 INFO]: Val stats: {
    "score": -0.8164589318180007,
    "rmse": 0.8164589318180007
}
[08/28/2025 00:26:28 INFO]: Test stats: {
    "score": -0.9618865020713507,
    "rmse": 0.9618865020713507
}
[08/28/2025 00:26:28 INFO]: Training loss at epoch 43: 1.1781956255435944
[08/28/2025 00:26:50 INFO]: Training loss at epoch 4: 1.0788365602493286
[08/28/2025 00:26:53 INFO]: Training loss at epoch 24: 0.9578152894973755
[08/28/2025 00:26:59 INFO]: Training loss at epoch 45: 1.0073291659355164
[08/28/2025 00:27:14 INFO]: Training loss at epoch 36: 1.185312271118164
[08/28/2025 00:27:18 INFO]: Training loss at epoch 6: 1.3637568354606628
[08/28/2025 00:27:20 INFO]: Training loss at epoch 3: 1.0658079385757446
[08/28/2025 00:27:24 INFO]: Training loss at epoch 19: 1.0367488265037537
[08/28/2025 00:27:46 INFO]: Training loss at epoch 7: 0.9452332556247711
[08/28/2025 00:27:52 INFO]: Training loss at epoch 6: 1.268269121646881
[08/28/2025 00:27:59 INFO]: Training loss at epoch 1: 1.211138129234314
[08/28/2025 00:28:09 INFO]: Training loss at epoch 56: 0.8594045341014862
[08/28/2025 00:28:12 INFO]: Training loss at epoch 50: 1.238479882478714
[08/28/2025 00:28:13 INFO]: New best epoch, val score: -0.670392727339844
[08/28/2025 00:28:13 INFO]: Saving model to: blotchy-Amado_trial_72/model_best.pth
[08/28/2025 00:28:23 INFO]: Training stats: {
    "score": -1.0096208251258194,
    "rmse": 1.0096208251258194
}
[08/28/2025 00:28:23 INFO]: Val stats: {
    "score": -0.6626233813448719,
    "rmse": 0.6626233813448719
}
[08/28/2025 00:28:23 INFO]: Test stats: {
    "score": -0.8707474647402182,
    "rmse": 0.8707474647402182
}
[08/28/2025 00:28:31 INFO]: Training loss at epoch 25: 1.015586942434311
[08/28/2025 00:28:32 INFO]: Training loss at epoch 20: 0.9729955792427063
[08/28/2025 00:28:35 INFO]: Training loss at epoch 36: 0.9696394801139832
[08/28/2025 00:28:44 INFO]: New best epoch, val score: -0.6626233813448719
[08/28/2025 00:28:44 INFO]: Saving model to: blotchy-Amado_trial_63/model_best.pth
[08/28/2025 00:28:57 INFO]: Training loss at epoch 4: 1.1861588954925537
[08/28/2025 00:28:57 INFO]: Training loss at epoch 25: 0.9853612184524536
[08/28/2025 00:29:49 INFO]: Training loss at epoch 5: 1.2807600498199463
[08/28/2025 00:30:06 INFO]: Training loss at epoch 2: 1.0534545183181763
[08/28/2025 00:30:06 INFO]: Training loss at epoch 7: 0.9882681369781494
[08/28/2025 00:30:10 INFO]: Training loss at epoch 37: 0.8417197167873383
[08/28/2025 00:30:18 INFO]: Training loss at epoch 4: 1.2551249265670776
[08/28/2025 00:30:20 INFO]: New best epoch, val score: -0.661479476247227
[08/28/2025 00:30:20 INFO]: Saving model to: blotchy-Amado_trial_72/model_best.pth
[08/28/2025 00:30:29 INFO]: Training loss at epoch 44: 1.1434879899024963
[08/28/2025 00:30:36 INFO]: Training loss at epoch 21: 0.8715406954288483
[08/28/2025 00:31:02 INFO]: Training loss at epoch 26: 1.1032198071479797
[08/28/2025 00:31:20 INFO]: Training loss at epoch 20: 1.1884222030639648
[08/28/2025 00:31:25 INFO]: Training loss at epoch 26: 1.03189355134964
[08/28/2025 00:32:07 INFO]: Training loss at epoch 3: 0.8317815959453583
[08/28/2025 00:32:10 INFO]: Training loss at epoch 51: 0.8519316613674164
[08/28/2025 00:32:17 INFO]: Training loss at epoch 46: 1.0098347663879395
[08/28/2025 00:32:32 INFO]: Training loss at epoch 37: 0.9734765887260437
[08/28/2025 00:32:35 INFO]: Training loss at epoch 22: 1.055703043937683
[08/28/2025 00:32:40 INFO]: Training loss at epoch 6: 1.4281127452850342
[08/28/2025 00:32:45 INFO]: Training loss at epoch 8: 1.1895052790641785
[08/28/2025 00:32:51 INFO]: Training loss at epoch 8: 1.0230953097343445
[08/28/2025 00:32:52 INFO]: Training loss at epoch 7: 1.0151756405830383
[08/28/2025 00:32:59 INFO]: Training loss at epoch 55: 1.1097017526626587
[08/28/2025 00:33:01 INFO]: Training loss at epoch 38: 1.0271777510643005
[08/28/2025 00:33:02 INFO]: Training loss at epoch 27: 0.9129438102245331
[08/28/2025 00:33:11 INFO]: Training loss at epoch 5: 0.9351843893527985
[08/28/2025 00:33:21 INFO]: Training loss at epoch 47: 0.7743326425552368
[08/28/2025 00:33:50 INFO]: Training loss at epoch 5: 1.1656742095947266
[08/28/2025 00:34:11 INFO]: Training loss at epoch 4: 1.0137842893600464
[08/28/2025 00:34:11 INFO]: Training loss at epoch 21: 1.2561296224594116
[08/28/2025 00:34:14 INFO]: Training loss at epoch 27: 1.2079485356807709
[08/28/2025 00:34:26 INFO]: Training loss at epoch 45: 0.968761146068573
[08/28/2025 00:34:41 INFO]: Training loss at epoch 23: 0.982162743806839
[08/28/2025 00:35:07 INFO]: Training loss at epoch 28: 1.0264038741588593
[08/28/2025 00:35:33 INFO]: Training loss at epoch 9: 1.2768008708953857
[08/28/2025 00:35:39 INFO]: Training loss at epoch 7: 1.052841603755951
[08/28/2025 00:35:48 INFO]: Training loss at epoch 46: 1.0794349312782288
[08/28/2025 00:35:55 INFO]: Training loss at epoch 39: 0.8689566850662231
[08/28/2025 00:35:58 INFO]: New best epoch, val score: -0.6631060288381734
[08/28/2025 00:35:58 INFO]: Saving model to: blotchy-Amado_trial_70/model_best.pth
[08/28/2025 00:36:07 INFO]: Training loss at epoch 6: 0.9594883024692535
[08/28/2025 00:36:08 INFO]: Training loss at epoch 52: 0.9138166010379791
[08/28/2025 00:36:15 INFO]: Training loss at epoch 5: 0.9350049197673798
[08/28/2025 00:36:27 INFO]: Training stats: {
    "score": -1.0842161974943503,
    "rmse": 1.0842161974943503
}
[08/28/2025 00:36:27 INFO]: Val stats: {
    "score": -0.8846988328148133,
    "rmse": 0.8846988328148133
}
[08/28/2025 00:36:27 INFO]: Test stats: {
    "score": -1.0138928575593396,
    "rmse": 1.0138928575593396
}
[08/28/2025 00:36:34 INFO]: Training loss at epoch 38: 1.177955448627472
[08/28/2025 00:36:43 INFO]: Training loss at epoch 24: 0.992744505405426
[08/28/2025 00:36:51 INFO]: Training loss at epoch 57: 0.8619409501552582
[08/28/2025 00:36:53 INFO]: Training stats: {
    "score": -0.9970285149953877,
    "rmse": 0.9970285149953877
}
[08/28/2025 00:36:53 INFO]: Val stats: {
    "score": -0.6785829765596917,
    "rmse": 0.6785829765596917
}
[08/28/2025 00:36:53 INFO]: Test stats: {
    "score": -0.8753756433218012,
    "rmse": 0.8753756433218012
}
[08/28/2025 00:37:06 INFO]: Training loss at epoch 22: 1.0686259865760803
[08/28/2025 00:37:06 INFO]: Training loss at epoch 28: 0.9595016539096832
[08/28/2025 00:37:09 INFO]: Training loss at epoch 29: 1.210663080215454
[08/28/2025 00:37:37 INFO]: Training loss at epoch 47: 1.035422682762146
[08/28/2025 00:37:51 INFO]: Training stats: {
    "score": -1.02586020686722,
    "rmse": 1.02586020686722
}
[08/28/2025 00:37:51 INFO]: Val stats: {
    "score": -0.7758261425668632,
    "rmse": 0.7758261425668632
}
[08/28/2025 00:37:51 INFO]: Test stats: {
    "score": -0.9330314455334727,
    "rmse": 0.9330314455334727
}
[08/28/2025 00:37:55 INFO]: Training loss at epoch 8: 1.1319290399551392
[08/28/2025 00:37:59 INFO]: Training loss at epoch 9: 1.3100739121437073
[08/28/2025 00:38:19 INFO]: Training loss at epoch 6: 1.2054163217544556
[08/28/2025 00:38:22 INFO]: Training loss at epoch 46: 0.7568255215883255
[08/28/2025 00:38:35 INFO]: Training loss at epoch 8: 0.9847837388515472
[08/28/2025 00:38:39 INFO]: Training loss at epoch 6: 1.1609312891960144
[08/28/2025 00:38:43 INFO]: Training loss at epoch 25: 0.9075188338756561
[08/28/2025 00:38:56 INFO]: New best epoch, val score: -0.6612826738827612
[08/28/2025 00:38:56 INFO]: Saving model to: blotchy-Amado_trial_70/model_best.pth
[08/28/2025 00:39:02 INFO]: Training loss at epoch 7: 1.2162464559078217
[08/28/2025 00:39:10 INFO]: Training loss at epoch 10: 1.1468477249145508
[08/28/2025 00:39:22 INFO]: New best epoch, val score: -0.6637997336844139
[08/28/2025 00:39:22 INFO]: Saving model to: blotchy-Amado_trial_71/model_best.pth
[08/28/2025 00:39:39 INFO]: Training stats: {
    "score": -1.0220028792413691,
    "rmse": 1.0220028792413691
}
[08/28/2025 00:39:39 INFO]: Val stats: {
    "score": -0.7603390004456407,
    "rmse": 0.7603390004456407
}
[08/28/2025 00:39:39 INFO]: Test stats: {
    "score": -0.9236577665139883,
    "rmse": 0.9236577665139883
}
[08/28/2025 00:39:46 INFO]: Training loss at epoch 40: 1.128652572631836
[08/28/2025 00:39:53 INFO]: Training loss at epoch 30: 0.9316883981227875
[08/28/2025 00:39:57 INFO]: Training loss at epoch 29: 1.1016793251037598
[08/28/2025 00:39:59 INFO]: Training loss at epoch 23: 1.079131305217743
[08/28/2025 00:40:05 INFO]: Training loss at epoch 53: 1.1053956747055054
[08/28/2025 00:40:24 INFO]: Training loss at epoch 7: 0.9407587945461273
[08/28/2025 00:40:32 INFO]: Training loss at epoch 39: 0.8969627320766449
[08/28/2025 00:40:50 INFO]: Training loss at epoch 26: 1.1270246505737305
[08/28/2025 00:40:55 INFO]: Training stats: {
    "score": -1.0264176709517803,
    "rmse": 1.0264176709517803
}
[08/28/2025 00:40:55 INFO]: Val stats: {
    "score": -0.6642322917947736,
    "rmse": 0.6642322917947736
}
[08/28/2025 00:40:55 INFO]: Test stats: {
    "score": -0.8829213678075473,
    "rmse": 0.8829213678075473
}
[08/28/2025 00:41:16 INFO]: Training loss at epoch 48: 1.0571078658103943
[08/28/2025 00:41:35 INFO]: Training loss at epoch 9: 1.0779061317443848
[08/28/2025 00:41:52 INFO]: Training stats: {
    "score": -1.001716760921249,
    "rmse": 1.001716760921249
}
[08/28/2025 00:41:52 INFO]: Val stats: {
    "score": -0.6646506959451974,
    "rmse": 0.6646506959451974
}
[08/28/2025 00:41:52 INFO]: Test stats: {
    "score": -0.8687443677148972,
    "rmse": 0.8687443677148972
}
[08/28/2025 00:41:57 INFO]: Training loss at epoch 11: 1.0533525347709656
[08/28/2025 00:41:59 INFO]: Training loss at epoch 31: 1.2190791964530945
[08/28/2025 00:42:01 INFO]: Training loss at epoch 8: 1.02802973985672
[08/28/2025 00:42:13 INFO]: Running Final Evaluation...
[08/28/2025 00:42:14 INFO]: Training loss at epoch 56: 1.2026466131210327
[08/28/2025 00:42:14 INFO]: Running Final Evaluation...
[08/28/2025 00:42:21 INFO]: New best epoch, val score: -0.6623919888986471
[08/28/2025 00:42:21 INFO]: Saving model to: blotchy-Amado_trial_71/model_best.pth
[08/28/2025 00:42:23 INFO]: Training loss at epoch 47: 0.9957396984100342
[08/28/2025 00:42:31 INFO]: Training loss at epoch 8: 1.1177101135253906
[08/28/2025 00:42:33 INFO]: Training stats: {
    "score": -1.0152456579314078,
    "rmse": 1.0152456579314078
}
[08/28/2025 00:42:33 INFO]: Val stats: {
    "score": -0.6601546477406647,
    "rmse": 0.6601546477406647
}
[08/28/2025 00:42:33 INFO]: Test stats: {
    "score": -0.8699706157446249,
    "rmse": 0.8699706157446249
}
[08/28/2025 00:42:42 INFO]: Training loss at epoch 41: 1.143536388874054
[08/28/2025 00:42:53 INFO]: Training loss at epoch 27: 1.059375524520874
[08/28/2025 00:42:53 INFO]: New best epoch, val score: -0.6601546477406647
[08/28/2025 00:42:53 INFO]: Saving model to: blotchy-Amado_trial_70/model_best.pth
[08/28/2025 00:42:54 INFO]: Training loss at epoch 48: 1.2039144039154053
[08/28/2025 00:42:57 INFO]: Training loss at epoch 24: 0.9771174192428589
[08/28/2025 00:42:57 INFO]: Training loss at epoch 9: 1.1237279176712036
[08/28/2025 00:42:59 INFO]: Training accuracy: {
    "score": -1.0200028711775464,
    "rmse": 1.0200028711775464
}
[08/28/2025 00:42:59 INFO]: Val accuracy: {
    "score": -0.6608713643858036,
    "rmse": 0.6608713643858036
}
[08/28/2025 00:42:59 INFO]: Test accuracy: {
    "score": -0.8748254439899449,
    "rmse": 0.8748254439899449
}
[08/28/2025 00:42:59 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_64",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8748254439899449,
        "rmse": 0.8748254439899449
    },
    "train_stats": {
        "score": -1.0200028711775464,
        "rmse": 1.0200028711775464
    },
    "val_stats": {
        "score": -0.6608713643858036,
        "rmse": 0.6608713643858036
    }
}
[08/28/2025 00:42:59 INFO]: Procewss finished for trial blotchy-Amado_trial_64
[08/28/2025 00:42:59 INFO]: 
_________________________________________________

[08/28/2025 00:42:59 INFO]: train_net_for_optune.py main() running.
[08/28/2025 00:42:59 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 2.1336765543226317
  attention_dropout: 0.47365680764024265
  ffn_dropout: 0.47365680764024265
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 5.5779708866031584e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_73

[08/28/2025 00:42:59 INFO]: This ft_transformer has 8.603 million parameters.
[08/28/2025 00:42:59 INFO]: Training will start at epoch 0.
[08/28/2025 00:42:59 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 00:43:37 INFO]: Training loss at epoch 7: 1.1125735640525818
[08/28/2025 00:43:52 INFO]: Training loss at epoch 30: 1.1864836812019348
[08/28/2025 00:44:08 INFO]: Training loss at epoch 54: 1.299014151096344
[08/28/2025 00:44:38 INFO]: Training loss at epoch 9: 1.0060401856899261
[08/28/2025 00:44:40 INFO]: Training stats: {
    "score": -1.070757908311575,
    "rmse": 1.070757908311575
}
[08/28/2025 00:44:40 INFO]: Val stats: {
    "score": -0.8605738443831714,
    "rmse": 0.8605738443831714
}
[08/28/2025 00:44:40 INFO]: Test stats: {
    "score": -0.9918241249235841,
    "rmse": 0.9918241249235841
}
[08/28/2025 00:44:43 INFO]: Training loss at epoch 12: 0.8745491802692413
[08/28/2025 00:44:49 INFO]: Training loss at epoch 10: 0.9660459160804749
[08/28/2025 00:44:59 INFO]: Training loss at epoch 28: 1.0765069127082825
[08/28/2025 00:44:59 INFO]: Training loss at epoch 9: 1.206786572933197
[08/28/2025 00:45:05 INFO]: Training accuracy: {
    "score": -0.990501762284952,
    "rmse": 0.990501762284952
}
[08/28/2025 00:45:05 INFO]: Val accuracy: {
    "score": -0.6366015019720523,
    "rmse": 0.6366015019720523
}
[08/28/2025 00:45:05 INFO]: Test accuracy: {
    "score": -0.9014234337108907,
    "rmse": 0.9014234337108907
}
[08/28/2025 00:45:05 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_46",
    "best_epoch": 17,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9014234337108907,
        "rmse": 0.9014234337108907
    },
    "train_stats": {
        "score": -0.990501762284952,
        "rmse": 0.990501762284952
    },
    "val_stats": {
        "score": -0.6366015019720523,
        "rmse": 0.6366015019720523
    }
}
[08/28/2025 00:45:05 INFO]: Procewss finished for trial blotchy-Amado_trial_46
[08/28/2025 00:45:05 INFO]: 
_________________________________________________

[08/28/2025 00:45:05 INFO]: train_net_for_optune.py main() running.
[08/28/2025 00:45:05 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 2.4166012973625737
  attention_dropout: 0.4657128493635591
  ffn_dropout: 0.4657128493635591
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 5.830234910601782e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_74

[08/28/2025 00:45:05 INFO]: This ft_transformer has 7.043 million parameters.
[08/28/2025 00:45:05 INFO]: Training will start at epoch 0.
[08/28/2025 00:45:05 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 00:45:21 INFO]: Training stats: {
    "score": -1.0236874037674573,
    "rmse": 1.0236874037674573
}
[08/28/2025 00:45:21 INFO]: Val stats: {
    "score": -0.6636140050914124,
    "rmse": 0.6636140050914124
}
[08/28/2025 00:45:21 INFO]: Test stats: {
    "score": -0.8726042583525045,
    "rmse": 0.8726042583525045
}
[08/28/2025 00:45:32 INFO]: Training loss at epoch 10: 1.0601609349250793
[08/28/2025 00:45:39 INFO]: Training loss at epoch 58: 0.9111124575138092
[08/28/2025 00:45:41 INFO]: Training loss at epoch 42: 1.0486835837364197
[08/28/2025 00:45:55 INFO]: Training loss at epoch 25: 1.114022433757782
[08/28/2025 00:45:56 INFO]: Training loss at epoch 40: 0.9008970558643341
[08/28/2025 00:45:59 INFO]: Training stats: {
    "score": -1.0107271343104256,
    "rmse": 1.0107271343104256
}
[08/28/2025 00:45:59 INFO]: Val stats: {
    "score": -0.6620822668253862,
    "rmse": 0.6620822668253862
}
[08/28/2025 00:45:59 INFO]: Test stats: {
    "score": -0.8697469317140348,
    "rmse": 0.8697469317140348
}
[08/28/2025 00:46:20 INFO]: New best epoch, val score: -0.6620822668253862
[08/28/2025 00:46:20 INFO]: Saving model to: blotchy-Amado_trial_71/model_best.pth
[08/28/2025 00:46:26 INFO]: Training loss at epoch 48: 0.9425491690635681
[08/28/2025 00:46:44 INFO]: Training loss at epoch 31: 1.0183069109916687
[08/28/2025 00:46:54 INFO]: Training loss at epoch 47: 0.9530790150165558
[08/28/2025 00:47:02 INFO]: Training loss at epoch 29: 0.8586789071559906
[08/28/2025 00:47:22 INFO]: Training loss at epoch 0: 1.3178499341011047
[08/28/2025 00:47:26 INFO]: Training loss at epoch 10: 0.8269087970256805
[08/28/2025 00:47:29 INFO]: Training loss at epoch 13: 1.339432418346405
[08/28/2025 00:47:44 INFO]: Training stats: {
    "score": -1.0141897877650898,
    "rmse": 1.0141897877650898
}
[08/28/2025 00:47:44 INFO]: Val stats: {
    "score": -0.7519768414562414,
    "rmse": 0.7519768414562414
}
[08/28/2025 00:47:44 INFO]: Test stats: {
    "score": -0.9160404409397378,
    "rmse": 0.9160404409397378
}
[08/28/2025 00:47:56 INFO]: New best epoch, val score: -0.7278462351007247
[08/28/2025 00:47:56 INFO]: Saving model to: blotchy-Amado_trial_73/model_best.pth
[08/28/2025 00:48:05 INFO]: Training loss at epoch 55: 1.1151752471923828
[08/28/2025 00:48:12 INFO]: Training loss at epoch 0: 1.3667346835136414
[08/28/2025 00:48:13 INFO]: Training loss at epoch 49: 1.0156620740890503
[08/28/2025 00:48:26 INFO]: Training loss at epoch 11: 1.2869203686714172
[08/28/2025 00:48:28 INFO]: Training loss at epoch 8: 1.1202317476272583
[08/28/2025 00:48:32 INFO]: Training loss at epoch 43: 1.0873435139656067
[08/28/2025 00:48:37 INFO]: New best epoch, val score: -0.7181561435132713
[08/28/2025 00:48:37 INFO]: Saving model to: blotchy-Amado_trial_74/model_best.pth
[08/28/2025 00:48:49 INFO]: Training loss at epoch 26: 1.0882688164710999
[08/28/2025 00:48:54 INFO]: Training loss at epoch 10: 1.0592917203903198
[08/28/2025 00:49:31 INFO]: Training loss at epoch 11: 1.02572900056839
[08/28/2025 00:49:36 INFO]: Training loss at epoch 32: 0.9573054909706116
[08/28/2025 00:49:44 INFO]: Training loss at epoch 10: 0.9406073689460754
[08/28/2025 00:49:46 INFO]: Training loss at epoch 30: 1.0579707026481628
[08/28/2025 00:49:56 INFO]: Training loss at epoch 41: 0.9932390749454498
[08/28/2025 00:49:56 INFO]: Training loss at epoch 11: 0.9591006934642792
[08/28/2025 00:49:59 INFO]: Training stats: {
    "score": -1.0030615287493414,
    "rmse": 1.0030615287493414
}
[08/28/2025 00:49:59 INFO]: Val stats: {
    "score": -0.6614726547955926,
    "rmse": 0.6614726547955926
}
[08/28/2025 00:49:59 INFO]: Test stats: {
    "score": -0.8713888549734721,
    "rmse": 0.8713888549734721
}
[08/28/2025 00:50:14 INFO]: Training loss at epoch 14: 1.324847161769867
[08/28/2025 00:50:24 INFO]: Training loss at epoch 49: 1.029101550579071
[08/28/2025 00:51:26 INFO]: Training loss at epoch 12: 1.1571218967437744
[08/28/2025 00:51:28 INFO]: Training loss at epoch 44: 0.8625894486904144
[08/28/2025 00:51:31 INFO]: Training loss at epoch 57: 1.0142961740493774
[08/28/2025 00:51:34 INFO]: Training loss at epoch 12: 1.0320099592208862
[08/28/2025 00:51:40 INFO]: Training stats: {
    "score": -0.999492212945881,
    "rmse": 0.999492212945881
}
[08/28/2025 00:51:40 INFO]: Val stats: {
    "score": -0.6965418017802444,
    "rmse": 0.6965418017802444
}
[08/28/2025 00:51:40 INFO]: Test stats: {
    "score": -0.8830928368587884,
    "rmse": 0.8830928368587884
}
[08/28/2025 00:51:43 INFO]: Training loss at epoch 27: 1.187524437904358
[08/28/2025 00:51:45 INFO]: Training loss at epoch 1: 1.2189928889274597
[08/28/2025 00:51:50 INFO]: Training loss at epoch 31: 0.9591482281684875
[08/28/2025 00:51:54 INFO]: Training loss at epoch 11: 0.9466440081596375
[08/28/2025 00:52:04 INFO]: Training loss at epoch 56: 0.88739874958992
[08/28/2025 00:52:05 INFO]: New best epoch, val score: -0.6621847269149814
[08/28/2025 00:52:05 INFO]: Saving model to: blotchy-Amado_trial_63/model_best.pth
[08/28/2025 00:52:19 INFO]: Training loss at epoch 1: 0.977502852678299
[08/28/2025 00:52:30 INFO]: Training loss at epoch 33: 0.987494170665741
[08/28/2025 00:53:01 INFO]: Training loss at epoch 15: 1.1716607809066772
[08/28/2025 00:53:30 INFO]: Training loss at epoch 9: 0.9023900330066681
[08/28/2025 00:53:40 INFO]: Training loss at epoch 13: 0.9239886701107025
[08/28/2025 00:53:54 INFO]: Training loss at epoch 32: 0.9792190194129944
[08/28/2025 00:53:56 INFO]: Training loss at epoch 42: 0.7999685704708099
[08/28/2025 00:54:24 INFO]: Training loss at epoch 45: 1.0108160376548767
[08/28/2025 00:54:25 INFO]: Training loss at epoch 59: 0.7760849446058273
[08/28/2025 00:54:25 INFO]: Training loss at epoch 13: 0.9607239365577698
[08/28/2025 00:54:40 INFO]: Training loss at epoch 28: 1.157516062259674
[08/28/2025 00:54:47 INFO]: Training loss at epoch 11: 1.0996697545051575
[08/28/2025 00:54:52 INFO]: Training loss at epoch 12: 0.841269314289093
[08/28/2025 00:55:01 INFO]: Training loss at epoch 12: 1.190040946006775
[08/28/2025 00:55:06 INFO]: Training stats: {
    "score": -1.0097773623402957,
    "rmse": 1.0097773623402957
}
[08/28/2025 00:55:06 INFO]: Val stats: {
    "score": -0.6617217105846263,
    "rmse": 0.6617217105846263
}
[08/28/2025 00:55:06 INFO]: Test stats: {
    "score": -0.8720726137450149,
    "rmse": 0.8720726137450149
}
[08/28/2025 00:55:14 INFO]: Training loss at epoch 50: 1.0842174887657166
[08/28/2025 00:55:19 INFO]: Training loss at epoch 2: 0.9876976609230042
[08/28/2025 00:55:23 INFO]: Training loss at epoch 34: 1.2096751928329468
[08/28/2025 00:55:39 INFO]: Training loss at epoch 50: 0.9296049773693085
[08/28/2025 00:55:43 INFO]: Training loss at epoch 16: 1.164490520954132
[08/28/2025 00:55:43 INFO]: Training loss at epoch 14: 1.0046096444129944
[08/28/2025 00:55:57 INFO]: Training loss at epoch 33: 0.912315309047699
[08/28/2025 00:56:04 INFO]: Training loss at epoch 57: 0.9886762797832489
[08/28/2025 00:56:13 INFO]: Running Final Evaluation...
[08/28/2025 00:56:31 INFO]: Running Final Evaluation...
[08/28/2025 00:56:59 INFO]: Training accuracy: {
    "score": -1.010830355789304,
    "rmse": 1.010830355789304
}
[08/28/2025 00:56:59 INFO]: Val accuracy: {
    "score": -0.6599241610013995,
    "rmse": 0.6599241610013995
}
[08/28/2025 00:56:59 INFO]: Test accuracy: {
    "score": -0.8699093193779122,
    "rmse": 0.8699093193779122
}
[08/28/2025 00:56:59 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_65",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8699093193779122,
        "rmse": 0.8699093193779122
    },
    "train_stats": {
        "score": -1.010830355789304,
        "rmse": 1.010830355789304
    },
    "val_stats": {
        "score": -0.6599241610013995,
        "rmse": 0.6599241610013995
    }
}
[08/28/2025 00:56:59 INFO]: Procewss finished for trial blotchy-Amado_trial_65
[08/28/2025 00:56:59 INFO]: 
_________________________________________________

[08/28/2025 00:56:59 INFO]: train_net_for_optune.py main() running.
[08/28/2025 00:56:59 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 2.4626552407128766
  attention_dropout: 0.46731210193295547
  ffn_dropout: 0.46731210193295547
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 5.846064537300753e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_75

[08/28/2025 00:56:59 INFO]: This ft_transformer has 7.129 million parameters.
[08/28/2025 00:56:59 INFO]: Training will start at epoch 0.
[08/28/2025 00:56:59 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 00:57:13 INFO]: Training loss at epoch 2: 0.9446118772029877
[08/28/2025 00:57:18 INFO]: Training loss at epoch 46: 1.150565356016159
[08/28/2025 00:57:21 INFO]: Training loss at epoch 14: 0.8648878037929535
[08/28/2025 00:57:24 INFO]: Training stats: {
    "score": -1.000727559950702,
    "rmse": 1.000727559950702
}
[08/28/2025 00:57:24 INFO]: Val stats: {
    "score": -0.6948880213996625,
    "rmse": 0.6948880213996625
}
[08/28/2025 00:57:24 INFO]: Test stats: {
    "score": -0.8814861652825299,
    "rmse": 0.8814861652825299
}
[08/28/2025 00:57:35 INFO]: Training loss at epoch 29: 1.1044751405715942
[08/28/2025 00:57:48 INFO]: New best epoch, val score: -0.6809526692218266
[08/28/2025 00:57:48 INFO]: Saving model to: blotchy-Amado_trial_73/model_best.pth
[08/28/2025 00:57:49 INFO]: Training loss at epoch 15: 0.9584283530712128
[08/28/2025 00:57:50 INFO]: Training loss at epoch 13: 1.0332542657852173
[08/28/2025 00:57:57 INFO]: Training loss at epoch 43: 1.12735977768898
[08/28/2025 00:58:01 INFO]: Training accuracy: {
    "score": -1.0106771507006145,
    "rmse": 1.0106771507006145
}
[08/28/2025 00:58:01 INFO]: Val accuracy: {
    "score": -0.6613613306451657,
    "rmse": 0.6613613306451657
}
[08/28/2025 00:58:01 INFO]: Test accuracy: {
    "score": -0.8698346984719633,
    "rmse": 0.8698346984719633
}
[08/28/2025 00:58:01 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_56",
    "best_epoch": 26,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8698346984719633,
        "rmse": 0.8698346984719633
    },
    "train_stats": {
        "score": -1.0106771507006145,
        "rmse": 1.0106771507006145
    },
    "val_stats": {
        "score": -0.6613613306451657,
        "rmse": 0.6613613306451657
    }
}
[08/28/2025 00:58:01 INFO]: Procewss finished for trial blotchy-Amado_trial_56
[08/28/2025 00:58:01 INFO]: 
_________________________________________________

[08/28/2025 00:58:01 INFO]: train_net_for_optune.py main() running.
[08/28/2025 00:58:01 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 2.431839351313746
  attention_dropout: 0.4834973845726133
  ffn_dropout: 0.4834973845726133
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 5.319373596256151e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_76

[08/28/2025 00:58:01 INFO]: This ft_transformer has 1.163 million parameters.
[08/28/2025 00:58:01 INFO]: Training will start at epoch 0.
[08/28/2025 00:58:01 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 00:58:02 INFO]: Training loss at epoch 48: 1.1529817581176758
[08/28/2025 00:58:21 INFO]: Training loss at epoch 35: 0.9164749085903168
[08/28/2025 00:58:32 INFO]: Training loss at epoch 17: 0.8716637790203094
[08/28/2025 00:58:35 INFO]: Training stats: {
    "score": -1.0227071863646962,
    "rmse": 1.0227071863646962
}
[08/28/2025 00:58:35 INFO]: Val stats: {
    "score": -0.6644763109816243,
    "rmse": 0.6644763109816243
}
[08/28/2025 00:58:35 INFO]: Test stats: {
    "score": -0.8778640789715698,
    "rmse": 0.8778640789715698
}
[08/28/2025 00:58:41 INFO]: Running Final Evaluation...
[08/28/2025 00:58:54 INFO]: Training loss at epoch 3: 1.3457450866699219
[08/28/2025 00:58:55 INFO]: Training loss at epoch 0: 1.150147557258606
[08/28/2025 00:59:02 INFO]: New best epoch, val score: -0.8711830663824132
[08/28/2025 00:59:02 INFO]: Saving model to: blotchy-Amado_trial_76/model_best.pth
[08/28/2025 00:59:38 INFO]: Training loss at epoch 51: 1.2107852101325989
[08/28/2025 00:59:40 INFO]: Training accuracy: {
    "score": -1.0071506907944792,
    "rmse": 1.0071506907944792
}
[08/28/2025 00:59:40 INFO]: Val accuracy: {
    "score": -0.6574552222370085,
    "rmse": 0.6574552222370085
}
[08/28/2025 00:59:40 INFO]: Test accuracy: {
    "score": -0.8708124098585583,
    "rmse": 0.8708124098585583
}
[08/28/2025 00:59:40 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_62",
    "best_epoch": 4,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8708124098585583,
        "rmse": 0.8708124098585583
    },
    "train_stats": {
        "score": -1.0071506907944792,
        "rmse": 1.0071506907944792
    },
    "val_stats": {
        "score": -0.6574552222370085,
        "rmse": 0.6574552222370085
    }
}
[08/28/2025 00:59:40 INFO]: Procewss finished for trial blotchy-Amado_trial_62
[08/28/2025 00:59:40 INFO]: 
_________________________________________________

[08/28/2025 00:59:40 INFO]: train_net_for_optune.py main() running.
[08/28/2025 00:59:40 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 2.4555588402877193
  attention_dropout: 0.4633876819242486
  ffn_dropout: 0.4633876819242486
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 5.73972301657915e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_77

[08/28/2025 00:59:40 INFO]: This ft_transformer has 1.170 million parameters.
[08/28/2025 00:59:40 INFO]: Training will start at epoch 0.
[08/28/2025 00:59:40 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 00:59:50 INFO]: Training loss at epoch 12: 0.9651389718055725
[08/28/2025 00:59:51 INFO]: Training loss at epoch 16: 0.944462239742279
[08/28/2025 00:59:56 INFO]: Training loss at epoch 1: 1.4242609739303589
[08/28/2025 01:00:00 INFO]: Training loss at epoch 10: 0.8026637434959412
[08/28/2025 01:00:03 INFO]: New best epoch, val score: -0.743230511851981
[08/28/2025 01:00:03 INFO]: Saving model to: blotchy-Amado_trial_76/model_best.pth
[08/28/2025 01:00:06 INFO]: Training loss at epoch 13: 1.0450032949447632
[08/28/2025 01:00:06 INFO]: Training loss at epoch 0: 1.219680666923523
[08/28/2025 01:00:12 INFO]: Training loss at epoch 47: 0.8941875696182251
[08/28/2025 01:00:16 INFO]: Training loss at epoch 15: 1.0657570362091064
[08/28/2025 01:00:32 INFO]: New best epoch, val score: -0.7780688079785438
[08/28/2025 01:00:32 INFO]: Saving model to: blotchy-Amado_trial_75/model_best.pth
[08/28/2025 01:00:34 INFO]: Training loss at epoch 51: 1.0742545425891876
[08/28/2025 01:00:37 INFO]: Training loss at epoch 0: 1.3360778093338013
[08/28/2025 01:00:43 INFO]: New best epoch, val score: -0.9409413062810948
[08/28/2025 01:00:43 INFO]: Saving model to: blotchy-Amado_trial_77/model_best.pth
[08/28/2025 01:00:45 INFO]: Training loss at epoch 14: 1.0582827925682068
[08/28/2025 01:00:49 INFO]: Training loss at epoch 58: 0.9864461421966553
[08/28/2025 01:00:59 INFO]: Training loss at epoch 2: 1.2596123218536377
[08/28/2025 01:01:06 INFO]: New best epoch, val score: -0.6740859656379409
[08/28/2025 01:01:06 INFO]: Saving model to: blotchy-Amado_trial_76/model_best.pth
[08/28/2025 01:01:18 INFO]: Training loss at epoch 18: 1.1277531385421753
[08/28/2025 01:01:32 INFO]: Training loss at epoch 30: 0.978396862745285
[08/28/2025 01:01:40 INFO]: Training loss at epoch 1: 1.6043941378593445
[08/28/2025 01:01:47 INFO]: New best epoch, val score: -0.7586529113343999
[08/28/2025 01:01:47 INFO]: Saving model to: blotchy-Amado_trial_77/model_best.pth
[08/28/2025 01:01:59 INFO]: Training loss at epoch 44: 1.0420290231704712
[08/28/2025 01:01:59 INFO]: Training loss at epoch 17: 0.9424235820770264
[08/28/2025 01:02:02 INFO]: Training loss at epoch 3: 1.494212806224823
[08/28/2025 01:02:12 INFO]: Training loss at epoch 3: 1.0292738676071167
[08/28/2025 01:02:32 INFO]: Training loss at epoch 4: 1.2754572033882141
[08/28/2025 01:02:43 INFO]: Training loss at epoch 2: 0.9673749208450317
[08/28/2025 01:02:48 INFO]: New best epoch, val score: -0.6718767638135669
[08/28/2025 01:02:48 INFO]: Saving model to: blotchy-Amado_trial_73/model_best.pth
[08/28/2025 01:02:50 INFO]: New best epoch, val score: -0.6576396340653747
[08/28/2025 01:02:50 INFO]: Saving model to: blotchy-Amado_trial_77/model_best.pth
[08/28/2025 01:02:57 INFO]: New best epoch, val score: -0.694580548166725
[08/28/2025 01:02:57 INFO]: Saving model to: blotchy-Amado_trial_74/model_best.pth
[08/28/2025 01:03:04 INFO]: Training loss at epoch 4: 1.1966236233711243
[08/28/2025 01:03:12 INFO]: Training loss at epoch 48: 1.1585850417613983
[08/28/2025 01:03:18 INFO]: Training loss at epoch 16: 0.9881285727024078
[08/28/2025 01:03:43 INFO]: Training loss at epoch 52: 0.985693484544754
[08/28/2025 01:03:45 INFO]: Training loss at epoch 3: 0.9484810829162598
[08/28/2025 01:03:45 INFO]: Training loss at epoch 1: 1.0567151308059692
[08/28/2025 01:03:46 INFO]: Training loss at epoch 15: 1.082890272140503
[08/28/2025 01:04:04 INFO]: Training loss at epoch 19: 1.1472474336624146
[08/28/2025 01:04:05 INFO]: Training loss at epoch 5: 0.9990250468254089
[08/28/2025 01:04:05 INFO]: Training loss at epoch 18: 1.0803306102752686
[08/28/2025 01:04:29 INFO]: Training loss at epoch 31: 1.0366284251213074
[08/28/2025 01:04:46 INFO]: Training loss at epoch 4: 0.9730440080165863
[08/28/2025 01:05:00 INFO]: Training loss at epoch 11: 0.9772951900959015
[08/28/2025 01:05:00 INFO]: Training loss at epoch 13: 1.260526418685913
[08/28/2025 01:05:02 INFO]: Training stats: {
    "score": -1.0406405592720491,
    "rmse": 1.0406405592720491
}
[08/28/2025 01:05:02 INFO]: Val stats: {
    "score": -0.8040882748247867,
    "rmse": 0.8040882748247867
}
[08/28/2025 01:05:02 INFO]: Test stats: {
    "score": -0.953194961089227,
    "rmse": 0.953194961089227
}
[08/28/2025 01:05:08 INFO]: Training loss at epoch 6: 0.9432222247123718
[08/28/2025 01:05:18 INFO]: Training loss at epoch 14: 1.394053041934967
[08/28/2025 01:05:47 INFO]: Training loss at epoch 5: 0.9969441592693329
[08/28/2025 01:05:56 INFO]: Training loss at epoch 52: 0.8860196173191071
[08/28/2025 01:06:03 INFO]: Training loss at epoch 45: 0.8507752418518066
[08/28/2025 01:06:07 INFO]: Training loss at epoch 5: 0.9544249773025513
[08/28/2025 01:06:08 INFO]: Training loss at epoch 7: 1.1023569107055664
[08/28/2025 01:06:09 INFO]: Training loss at epoch 49: 0.9685637652873993
[08/28/2025 01:06:11 INFO]: Training loss at epoch 19: 1.0553398132324219
[08/28/2025 01:06:15 INFO]: Training loss at epoch 60: 0.8744861781597137
[08/28/2025 01:06:15 INFO]: Training loss at epoch 17: 1.223221480846405
[08/28/2025 01:06:30 INFO]: Running Final Evaluation...
[08/28/2025 01:06:41 INFO]: Training loss at epoch 16: 1.2081387042999268
[08/28/2025 01:06:47 INFO]: Training loss at epoch 6: 1.2717783451080322
[08/28/2025 01:06:52 INFO]: Training stats: {
    "score": -1.012236396551415,
    "rmse": 1.012236396551415
}
[08/28/2025 01:06:52 INFO]: Val stats: {
    "score": -0.6620312209829555,
    "rmse": 0.6620312209829555
}
[08/28/2025 01:06:52 INFO]: Test stats: {
    "score": -0.8678403160405438,
    "rmse": 0.8678403160405438
}
[08/28/2025 01:07:03 INFO]: New best epoch, val score: -0.6606322632851434
[08/28/2025 01:07:03 INFO]: Saving model to: blotchy-Amado_trial_71/model_best.pth
[08/28/2025 01:07:06 INFO]: Training stats: {
    "score": -0.9971277651972092,
    "rmse": 0.9971277651972092
}
[08/28/2025 01:07:06 INFO]: Val stats: {
    "score": -0.6828191453005718,
    "rmse": 0.6828191453005718
}
[08/28/2025 01:07:06 INFO]: Test stats: {
    "score": -0.878035244086555,
    "rmse": 0.878035244086555
}
[08/28/2025 01:07:09 INFO]: Training loss at epoch 8: 1.0922651290893555
[08/28/2025 01:07:10 INFO]: Training loss at epoch 4: 1.1617613434791565
[08/28/2025 01:07:19 INFO]: Training loss at epoch 2: 1.072354793548584
[08/28/2025 01:07:25 INFO]: Training loss at epoch 32: 1.1857945919036865
[08/28/2025 01:07:45 INFO]: Training loss at epoch 53: 0.8716265857219696
[08/28/2025 01:07:48 INFO]: Training loss at epoch 20: 1.0650777220726013
[08/28/2025 01:07:52 INFO]: Training loss at epoch 7: 0.9861969351768494
[08/28/2025 01:07:58 INFO]: Training accuracy: {
    "score": -1.0081391922245022,
    "rmse": 1.0081391922245022
}
[08/28/2025 01:07:58 INFO]: Val accuracy: {
    "score": -0.6605289981082006,
    "rmse": 0.6605289981082006
}
[08/28/2025 01:07:58 INFO]: Test accuracy: {
    "score": -0.8687942081681049,
    "rmse": 0.8687942081681049
}
[08/28/2025 01:07:58 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_60",
    "best_epoch": 14,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8687942081681049,
        "rmse": 0.8687942081681049
    },
    "train_stats": {
        "score": -1.0081391922245022,
        "rmse": 1.0081391922245022
    },
    "val_stats": {
        "score": -0.6605289981082006,
        "rmse": 0.6605289981082006
    }
}
[08/28/2025 01:07:58 INFO]: Procewss finished for trial blotchy-Amado_trial_60
[08/28/2025 01:07:58 INFO]: 
_________________________________________________

[08/28/2025 01:07:58 INFO]: train_net_for_optune.py main() running.
[08/28/2025 01:07:58 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 2.1348798736604113
  attention_dropout: 0.46797331880092236
  ffn_dropout: 0.46797331880092236
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 5.562646088167718e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_78

[08/28/2025 01:07:58 INFO]: This ft_transformer has 1.075 million parameters.
[08/28/2025 01:07:58 INFO]: Training will start at epoch 0.
[08/28/2025 01:07:58 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 01:08:12 INFO]: Training loss at epoch 9: 1.1830224394798279
[08/28/2025 01:08:32 INFO]: Training stats: {
    "score": -1.0500955057963297,
    "rmse": 1.0500955057963297
}
[08/28/2025 01:08:32 INFO]: Val stats: {
    "score": -0.6867784361173598,
    "rmse": 0.6867784361173598
}
[08/28/2025 01:08:32 INFO]: Test stats: {
    "score": -0.8987661003646294,
    "rmse": 0.8987661003646294
}
[08/28/2025 01:08:48 INFO]: Training loss at epoch 0: 1.273676872253418
[08/28/2025 01:08:51 INFO]: Training loss at epoch 8: 1.1520628333091736
[08/28/2025 01:08:55 INFO]: New best epoch, val score: -0.8292680692969625
[08/28/2025 01:08:55 INFO]: Saving model to: blotchy-Amado_trial_78/model_best.pth
[08/28/2025 01:08:57 INFO]: Training loss at epoch 20: 1.0160622596740723
[08/28/2025 01:09:10 INFO]: Training loss at epoch 49: 0.7186237871646881
[08/28/2025 01:09:10 INFO]: Training loss at epoch 18: 1.1402747631072998
[08/28/2025 01:09:32 INFO]: Training loss at epoch 10: 1.1997807621955872
[08/28/2025 01:09:39 INFO]: New best epoch, val score: -0.6724634072249237
[08/28/2025 01:09:39 INFO]: Saving model to: blotchy-Amado_trial_76/model_best.pth
[08/28/2025 01:09:41 INFO]: Training loss at epoch 17: 0.9189139902591705
[08/28/2025 01:09:41 INFO]: Training loss at epoch 6: 1.2112233638763428
[08/28/2025 01:09:48 INFO]: Training loss at epoch 1: 1.1301676034927368
[08/28/2025 01:09:54 INFO]: Training loss at epoch 9: 1.1230034828186035
[08/28/2025 01:09:54 INFO]: Training loss at epoch 12: 1.0279206037521362
[08/28/2025 01:09:55 INFO]: New best epoch, val score: -0.6824852788546539
[08/28/2025 01:09:55 INFO]: Saving model to: blotchy-Amado_trial_78/model_best.pth
[08/28/2025 01:10:02 INFO]: Training loss at epoch 50: 0.8022466003894806
[08/28/2025 01:10:02 INFO]: Training loss at epoch 14: 1.418272614479065
[08/28/2025 01:10:12 INFO]: Training loss at epoch 59: 1.058285504579544
[08/28/2025 01:10:16 INFO]: Training stats: {
    "score": -1.1729211096574057,
    "rmse": 1.1729211096574057
}
[08/28/2025 01:10:16 INFO]: Val stats: {
    "score": -0.7880018617831627,
    "rmse": 0.7880018617831627
}
[08/28/2025 01:10:16 INFO]: Test stats: {
    "score": -0.9883844726645981,
    "rmse": 0.9883844726645981
}
[08/28/2025 01:10:22 INFO]: Training loss at epoch 33: 1.0667908787727356
[08/28/2025 01:10:23 INFO]: Running Final Evaluation...
[08/28/2025 01:10:26 INFO]: Training loss at epoch 15: 0.9941551685333252
[08/28/2025 01:10:34 INFO]: Training loss at epoch 21: 0.9934372901916504
[08/28/2025 01:10:36 INFO]: Training loss at epoch 11: 0.9359249174594879
[08/28/2025 01:10:43 INFO]: New best epoch, val score: -0.6695015576179723
[08/28/2025 01:10:43 INFO]: Saving model to: blotchy-Amado_trial_76/model_best.pth
[08/28/2025 01:10:47 INFO]: Training loss at epoch 2: 1.0071771740913391
[08/28/2025 01:10:53 INFO]: Training loss at epoch 3: 1.2631405591964722
[08/28/2025 01:11:05 INFO]: Training loss at epoch 21: 1.102491319179535
[08/28/2025 01:11:14 INFO]: Training loss at epoch 53: 0.9225011169910431
[08/28/2025 01:11:19 INFO]: New best epoch, val score: -0.7756389562630357
[08/28/2025 01:11:19 INFO]: Saving model to: blotchy-Amado_trial_75/model_best.pth
[08/28/2025 01:11:20 INFO]: Training loss at epoch 10: 0.8609020113945007
[08/28/2025 01:11:25 INFO]: Training accuracy: {
    "score": -1.0060556408184103,
    "rmse": 1.0060556408184103
}
[08/28/2025 01:11:25 INFO]: Val accuracy: {
    "score": -0.6577321812184274,
    "rmse": 0.6577321812184274
}
[08/28/2025 01:11:25 INFO]: Test accuracy: {
    "score": -0.8693720924207924,
    "rmse": 0.8693720924207924
}
[08/28/2025 01:11:25 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_61",
    "best_epoch": 19,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8693720924207924,
        "rmse": 0.8693720924207924
    },
    "train_stats": {
        "score": -1.0060556408184103,
        "rmse": 1.0060556408184103
    },
    "val_stats": {
        "score": -0.6577321812184274,
        "rmse": 0.6577321812184274
    }
}
[08/28/2025 01:11:25 INFO]: Procewss finished for trial blotchy-Amado_trial_61
[08/28/2025 01:11:26 INFO]: 
_________________________________________________

[08/28/2025 01:11:26 INFO]: train_net_for_optune.py main() running.
[08/28/2025 01:11:26 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 1.8772026379751559
  attention_dropout: 0.4637394832654855
  ffn_dropout: 0.4637394832654855
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 5.7072957924878165e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_79

[08/28/2025 01:11:26 INFO]: This ft_transformer has 0.680 million parameters.
[08/28/2025 01:11:26 INFO]: Training will start at epoch 0.
[08/28/2025 01:11:26 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 01:11:41 INFO]: Training loss at epoch 12: 0.9962778687477112
[08/28/2025 01:11:47 INFO]: Training loss at epoch 54: 0.9132646024227142
[08/28/2025 01:11:48 INFO]: Training loss at epoch 3: 1.049234926700592
[08/28/2025 01:11:52 INFO]: Running Final Evaluation...
[08/28/2025 01:12:12 INFO]: Training loss at epoch 5: 1.0853229761123657
[08/28/2025 01:12:14 INFO]: Training loss at epoch 19: 0.8206595480442047
[08/28/2025 01:12:16 INFO]: Running Final Evaluation...
[08/28/2025 01:12:22 INFO]: Training loss at epoch 11: 1.2138015031814575
[08/28/2025 01:12:25 INFO]: Training loss at epoch 0: 1.4698404669761658
[08/28/2025 01:12:33 INFO]: New best epoch, val score: -0.6653479459385244
[08/28/2025 01:12:33 INFO]: Saving model to: blotchy-Amado_trial_79/model_best.pth
[08/28/2025 01:12:41 INFO]: Training loss at epoch 18: 1.2042008638381958
[08/28/2025 01:12:44 INFO]: Training loss at epoch 13: 0.995559573173523
[08/28/2025 01:12:47 INFO]: Training loss at epoch 4: 1.0627719163894653
[08/28/2025 01:13:01 INFO]: Training stats: {
    "score": -0.9265426328259831,
    "rmse": 0.9265426328259831
}
[08/28/2025 01:13:01 INFO]: Val stats: {
    "score": -0.6831565168211116,
    "rmse": 0.6831565168211116
}
[08/28/2025 01:13:01 INFO]: Test stats: {
    "score": -0.8986361565183096,
    "rmse": 0.8986361565183096
}
[08/28/2025 01:13:02 INFO]: New best epoch, val score: -0.6601875155862301
[08/28/2025 01:13:02 INFO]: Saving model to: blotchy-Amado_trial_71/model_best.pth
[08/28/2025 01:13:12 INFO]: Training stats: {
    "score": -0.9981671228681864,
    "rmse": 0.9981671228681864
}
[08/28/2025 01:13:12 INFO]: Val stats: {
    "score": -0.6772767081580052,
    "rmse": 0.6772767081580052
}
[08/28/2025 01:13:12 INFO]: Test stats: {
    "score": -0.8718424856290985,
    "rmse": 0.8718424856290985
}
[08/28/2025 01:13:14 INFO]: Training loss at epoch 22: 0.8764393329620361
[08/28/2025 01:13:17 INFO]: Training loss at epoch 7: 1.0665318369865417
[08/28/2025 01:13:20 INFO]: Training loss at epoch 34: 1.111911118030548
[08/28/2025 01:13:21 INFO]: Training stats: {
    "score": -0.9921918212737212,
    "rmse": 0.9921918212737212
}
[08/28/2025 01:13:21 INFO]: Val stats: {
    "score": -0.7109858609412094,
    "rmse": 0.7109858609412094
}
[08/28/2025 01:13:21 INFO]: Test stats: {
    "score": -0.892355743344503,
    "rmse": 0.892355743344503
}
[08/28/2025 01:13:23 INFO]: Training loss at epoch 22: 1.0717706680297852
[08/28/2025 01:13:24 INFO]: Training loss at epoch 12: 0.9369127154350281
[08/28/2025 01:13:34 INFO]: Training loss at epoch 1: 1.0350509583950043
[08/28/2025 01:13:40 INFO]: Training accuracy: {
    "score": -1.001452701668047,
    "rmse": 1.001452701668047
}
[08/28/2025 01:13:40 INFO]: Val accuracy: {
    "score": -0.6682164186298337,
    "rmse": 0.6682164186298337
}
[08/28/2025 01:13:40 INFO]: Test accuracy: {
    "score": -0.8706284038469143,
    "rmse": 0.8706284038469143
}
[08/28/2025 01:13:40 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_58",
    "best_epoch": 23,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8706284038469143,
        "rmse": 0.8706284038469143
    },
    "train_stats": {
        "score": -1.001452701668047,
        "rmse": 1.001452701668047
    },
    "val_stats": {
        "score": -0.6682164186298337,
        "rmse": 0.6682164186298337
    }
}
[08/28/2025 01:13:40 INFO]: Procewss finished for trial blotchy-Amado_trial_58
[08/28/2025 01:13:40 INFO]: 
_________________________________________________

[08/28/2025 01:13:40 INFO]: train_net_for_optune.py main() running.
[08/28/2025 01:13:40 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 16
  d_ffn_factor: 2.13098723507308
  attention_dropout: 0.47443522981157726
  ffn_dropout: 0.47443522981157726
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 5.7905975510819966e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_80

[08/28/2025 01:13:40 INFO]: This ft_transformer has 1.759 million parameters.
[08/28/2025 01:13:40 INFO]: Training will start at epoch 0.
[08/28/2025 01:13:40 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 01:13:44 INFO]: New best epoch, val score: -0.694307135419072
[08/28/2025 01:13:44 INFO]: Saving model to: blotchy-Amado_trial_74/model_best.pth
[08/28/2025 01:13:45 INFO]: Training accuracy: {
    "score": -1.007639038726162,
    "rmse": 1.007639038726162
}
[08/28/2025 01:13:45 INFO]: Val accuracy: {
    "score": -0.6595921670313275,
    "rmse": 0.6595921670313275
}
[08/28/2025 01:13:45 INFO]: Test accuracy: {
    "score": -0.8714852097182757,
    "rmse": 0.8714852097182757
}
[08/28/2025 01:13:45 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_53",
    "best_epoch": 22,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8714852097182757,
        "rmse": 0.8714852097182757
    },
    "train_stats": {
        "score": -1.007639038726162,
        "rmse": 1.007639038726162
    },
    "val_stats": {
        "score": -0.6595921670313275,
        "rmse": 0.6595921670313275
    }
}
[08/28/2025 01:13:45 INFO]: Procewss finished for trial blotchy-Amado_trial_53
[08/28/2025 01:13:45 INFO]: 
_________________________________________________

[08/28/2025 01:13:45 INFO]: train_net_for_optune.py main() running.
[08/28/2025 01:13:45 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 16
  d_ffn_factor: 2.1434069955316515
  attention_dropout: 0.478125913842194
  ffn_dropout: 0.478125913842194
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 5.6102668312748755e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_81

[08/28/2025 01:13:45 INFO]: This ft_transformer has 1.767 million parameters.
[08/28/2025 01:13:45 INFO]: Training will start at epoch 0.
[08/28/2025 01:13:45 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 01:13:48 INFO]: Training loss at epoch 5: 1.1315572261810303
[08/28/2025 01:13:49 INFO]: Training loss at epoch 14: 1.208465337753296
[08/28/2025 01:14:26 INFO]: Training loss at epoch 13: 1.084620714187622
[08/28/2025 01:14:33 INFO]: Training loss at epoch 4: 0.8988732695579529
[08/28/2025 01:14:42 INFO]: Training loss at epoch 2: 0.9594121277332306
[08/28/2025 01:14:47 INFO]: Training loss at epoch 6: 1.0926003456115723
[08/28/2025 01:14:51 INFO]: Training loss at epoch 15: 0.9042525291442871
[08/28/2025 01:14:52 INFO]: Training loss at epoch 13: 1.1963698863983154
[08/28/2025 01:14:57 INFO]: New best epoch, val score: -0.7670252279817437
[08/28/2025 01:14:57 INFO]: Saving model to: blotchy-Amado_trial_75/model_best.pth
[08/28/2025 01:15:06 INFO]: Training loss at epoch 61: 0.9511078894138336
[08/28/2025 01:15:10 INFO]: Training loss at epoch 15: 0.9846580028533936
[08/28/2025 01:15:19 INFO]: Training loss at epoch 23: 1.1466466188430786
[08/28/2025 01:15:26 INFO]: Training loss at epoch 14: 1.2309774160385132
[08/28/2025 01:15:36 INFO]: Training loss at epoch 16: 1.1241430938243866
[08/28/2025 01:15:38 INFO]: Training loss at epoch 19: 1.0489757061004639
[08/28/2025 01:15:43 INFO]: Training loss at epoch 7: 1.1424092054367065
[08/28/2025 01:15:49 INFO]: Training loss at epoch 3: 0.8883325755596161
[08/28/2025 01:15:51 INFO]: Training loss at epoch 16: 1.0497376918792725
[08/28/2025 01:16:09 INFO]: Training loss at epoch 23: 1.124609887599945
[08/28/2025 01:16:09 INFO]: Training loss at epoch 20: 1.0083053708076477
[08/28/2025 01:16:17 INFO]: Training loss at epoch 35: 0.9055074453353882
[08/28/2025 01:16:29 INFO]: Training loss at epoch 15: 0.9878343939781189
[08/28/2025 01:16:36 INFO]: New best epoch, val score: -0.6548800347397449
[08/28/2025 01:16:36 INFO]: Saving model to: blotchy-Amado_trial_77/model_best.pth
[08/28/2025 01:16:39 INFO]: Training stats: {
    "score": -1.0008347542787062,
    "rmse": 1.0008347542787062
}
[08/28/2025 01:16:39 INFO]: Val stats: {
    "score": -0.6660233199024084,
    "rmse": 0.6660233199024084
}
[08/28/2025 01:16:39 INFO]: Test stats: {
    "score": -0.86819137595307,
    "rmse": 0.86819137595307
}
[08/28/2025 01:16:42 INFO]: Training loss at epoch 0: 1.0230322480201721
[08/28/2025 01:16:44 INFO]: Training loss at epoch 8: 1.5992201566696167
[08/28/2025 01:16:47 INFO]: Training loss at epoch 0: 0.9141241014003754
[08/28/2025 01:16:53 INFO]: Training loss at epoch 8: 1.0590635240077972
[08/28/2025 01:16:54 INFO]: Training loss at epoch 17: 1.0512278079986572
[08/28/2025 01:17:00 INFO]: Training loss at epoch 4: 0.8675261437892914
[08/28/2025 01:17:09 INFO]: New best epoch, val score: -0.7496574366238167
[08/28/2025 01:17:09 INFO]: Saving model to: blotchy-Amado_trial_80/model_best.pth
[08/28/2025 01:17:10 INFO]: Training loss at epoch 6: 1.1340493559837341
[08/28/2025 01:17:15 INFO]: New best epoch, val score: -0.7233650044374534
[08/28/2025 01:17:15 INFO]: Saving model to: blotchy-Amado_trial_81/model_best.pth
[08/28/2025 01:17:20 INFO]: New best epoch, val score: -0.6884265294057279
[08/28/2025 01:17:20 INFO]: Saving model to: blotchy-Amado_trial_74/model_best.pth
[08/28/2025 01:17:28 INFO]: Training loss at epoch 24: 1.1465376019477844
[08/28/2025 01:17:33 INFO]: Training loss at epoch 16: 1.053327739238739
[08/28/2025 01:17:45 INFO]: Training loss at epoch 9: 1.0287761092185974
[08/28/2025 01:17:59 INFO]: Training loss at epoch 18: 1.1508248448371887
[08/28/2025 01:18:07 INFO]: Training stats: {
    "score": -1.1822315489751556,
    "rmse": 1.1822315489751556
}
[08/28/2025 01:18:07 INFO]: Val stats: {
    "score": -0.8092043380998781,
    "rmse": 0.8092043380998781
}
[08/28/2025 01:18:07 INFO]: Test stats: {
    "score": -1.0187940794404267,
    "rmse": 1.0187940794404267
}
[08/28/2025 01:18:11 INFO]: Training loss at epoch 5: 1.137222945690155
[08/28/2025 01:18:11 INFO]: Training loss at epoch 5: 1.3293455839157104
[08/28/2025 01:18:36 INFO]: Training loss at epoch 17: 1.1008298993110657
[08/28/2025 01:18:37 INFO]: New best epoch, val score: -0.7659155290861287
[08/28/2025 01:18:37 INFO]: Saving model to: blotchy-Amado_trial_75/model_best.pth
[08/28/2025 01:19:01 INFO]: Training loss at epoch 24: 1.0672357678413391
[08/28/2025 01:19:02 INFO]: Training loss at epoch 19: 1.4266566634178162
[08/28/2025 01:19:07 INFO]: Training loss at epoch 10: 1.0966781377792358
[08/28/2025 01:19:14 INFO]: Training loss at epoch 21: 1.026540219783783
[08/28/2025 01:19:20 INFO]: Training loss at epoch 36: 1.3188007473945618
[08/28/2025 01:19:21 INFO]: Training loss at epoch 6: 1.0460494756698608
[08/28/2025 01:19:23 INFO]: Training stats: {
    "score": -0.9965083233567803,
    "rmse": 0.9965083233567803
}
[08/28/2025 01:19:23 INFO]: Val stats: {
    "score": -0.7043429464713792,
    "rmse": 0.7043429464713792
}
[08/28/2025 01:19:23 INFO]: Test stats: {
    "score": -0.8839596952516767,
    "rmse": 0.8839596952516767
}
[08/28/2025 01:19:36 INFO]: Training loss at epoch 25: 1.020150065422058
[08/28/2025 01:19:38 INFO]: Training loss at epoch 18: 1.0152934193611145
[08/28/2025 01:19:42 INFO]: Training loss at epoch 20: 1.3836665153503418
[08/28/2025 01:19:54 INFO]: Training loss at epoch 14: 1.166731595993042
[08/28/2025 01:20:06 INFO]: Training loss at epoch 11: 0.9955630898475647
[08/28/2025 01:20:15 INFO]: Training loss at epoch 1: 1.1843786239624023
[08/28/2025 01:20:22 INFO]: Training loss at epoch 1: 0.9283662438392639
[08/28/2025 01:20:25 INFO]: Training loss at epoch 16: 1.2782532274723053
[08/28/2025 01:20:27 INFO]: Training loss at epoch 20: 1.0180761218070984
[08/28/2025 01:20:31 INFO]: Training loss at epoch 7: 1.0909698009490967
[08/28/2025 01:20:36 INFO]: Training loss at epoch 9: 1.2284329533576965
[08/28/2025 01:20:41 INFO]: Training loss at epoch 19: 1.0498329997062683
[08/28/2025 01:20:42 INFO]: New best epoch, val score: -0.6602578732013671
[08/28/2025 01:20:42 INFO]: Saving model to: blotchy-Amado_trial_80/model_best.pth
[08/28/2025 01:20:48 INFO]: New best epoch, val score: -0.680690405208279
[08/28/2025 01:20:48 INFO]: Saving model to: blotchy-Amado_trial_81/model_best.pth
[08/28/2025 01:20:54 INFO]: Training loss at epoch 17: 1.1254726648330688
[08/28/2025 01:21:01 INFO]: Training stats: {
    "score": -1.000292452636697,
    "rmse": 1.000292452636697
}
[08/28/2025 01:21:01 INFO]: Val stats: {
    "score": -0.6693560019578035,
    "rmse": 0.6693560019578035
}
[08/28/2025 01:21:01 INFO]: Test stats: {
    "score": -0.8578630370103075,
    "rmse": 0.8578630370103075
}
[08/28/2025 01:21:05 INFO]: Training loss at epoch 12: 0.9029842615127563
[08/28/2025 01:21:11 INFO]: New best epoch, val score: -0.6760109519623496
[08/28/2025 01:21:11 INFO]: Saving model to: blotchy-Amado_trial_78/model_best.pth
[08/28/2025 01:21:28 INFO]: Training loss at epoch 21: 0.8589881658554077
[08/28/2025 01:21:38 INFO]: Training loss at epoch 8: 1.3341526985168457
[08/28/2025 01:21:42 INFO]: Training loss at epoch 26: 0.930135190486908
[08/28/2025 01:21:46 INFO]: Training stats: {
    "score": -1.0453264882248503,
    "rmse": 1.0453264882248503
}
[08/28/2025 01:21:46 INFO]: Val stats: {
    "score": -0.6819750943702528,
    "rmse": 0.6819750943702528
}
[08/28/2025 01:21:46 INFO]: Test stats: {
    "score": -0.9013428465479327,
    "rmse": 0.9013428465479327
}
[08/28/2025 01:21:48 INFO]: Training loss at epoch 6: 1.096935510635376
[08/28/2025 01:21:49 INFO]: Training loss at epoch 25: 1.1546395421028137
[08/28/2025 01:22:02 INFO]: Training loss at epoch 20: 1.2553947567939758
[08/28/2025 01:22:03 INFO]: Training loss at epoch 13: 1.1064172983169556
[08/28/2025 01:22:11 INFO]: New best epoch, val score: -0.6819750943702528
[08/28/2025 01:22:11 INFO]: Saving model to: blotchy-Amado_trial_74/model_best.pth
[08/28/2025 01:22:11 INFO]: New best epoch, val score: -0.6627957907283616
[08/28/2025 01:22:11 INFO]: Saving model to: blotchy-Amado_trial_78/model_best.pth
[08/28/2025 01:22:13 INFO]: Training loss at epoch 22: 1.195439875125885
[08/28/2025 01:22:14 INFO]: New best epoch, val score: -0.7581863470490289
[08/28/2025 01:22:14 INFO]: Saving model to: blotchy-Amado_trial_75/model_best.pth
[08/28/2025 01:22:17 INFO]: Training loss at epoch 7: 0.9378464221954346
[08/28/2025 01:22:17 INFO]: Training loss at epoch 37: 0.8937745094299316
[08/28/2025 01:22:31 INFO]: Training loss at epoch 22: 0.9934051632881165
[08/28/2025 01:22:41 INFO]: Training loss at epoch 21: 0.9322108626365662
[08/28/2025 01:22:48 INFO]: Training loss at epoch 60: 1.0044608116149902
[08/28/2025 01:22:48 INFO]: Training loss at epoch 9: 0.9795062839984894
[08/28/2025 01:23:04 INFO]: Training loss at epoch 14: 1.1598199605941772
[08/28/2025 01:23:05 INFO]: Training loss at epoch 21: 1.1717934608459473
[08/28/2025 01:23:11 INFO]: New best epoch, val score: -0.6600370803256835
[08/28/2025 01:23:11 INFO]: Saving model to: blotchy-Amado_trial_78/model_best.pth
[08/28/2025 01:23:13 INFO]: Training stats: {
    "score": -1.1940554250429576,
    "rmse": 1.1940554250429576
}
[08/28/2025 01:23:13 INFO]: Val stats: {
    "score": -1.0528503204002064,
    "rmse": 1.0528503204002064
}
[08/28/2025 01:23:13 INFO]: Test stats: {
    "score": -1.1418846367781568,
    "rmse": 1.1418846367781568
}
[08/28/2025 01:23:34 INFO]: Training loss at epoch 23: 0.8598112761974335
[08/28/2025 01:23:46 INFO]: Training loss at epoch 2: 0.8415637910366058
[08/28/2025 01:23:51 INFO]: Training loss at epoch 27: 0.8886563777923584
[08/28/2025 01:23:54 INFO]: Training loss at epoch 2: 1.0649996399879456
[08/28/2025 01:24:04 INFO]: Training loss at epoch 62: 0.8968068957328796
[08/28/2025 01:24:05 INFO]: Training loss at epoch 15: 1.1694018840789795
[08/28/2025 01:24:09 INFO]: Training loss at epoch 22: 0.8816126585006714
[08/28/2025 01:24:18 INFO]: Training loss at epoch 50: 1.0344933569431305
[08/28/2025 01:24:19 INFO]: New best epoch, val score: -0.66740205858609
[08/28/2025 01:24:19 INFO]: Saving model to: blotchy-Amado_trial_81/model_best.pth
[08/28/2025 01:24:22 INFO]: Training loss at epoch 10: 0.912913054227829
[08/28/2025 01:24:36 INFO]: Training loss at epoch 24: 0.9811923503875732
[08/28/2025 01:24:38 INFO]: Training loss at epoch 26: 1.1280041933059692
[08/28/2025 01:24:54 INFO]: Training loss at epoch 15: 1.1519086360931396
[08/28/2025 01:25:03 INFO]: Training loss at epoch 16: 1.0970919132232666
[08/28/2025 01:25:10 INFO]: Training loss at epoch 23: 0.9276966452598572
[08/28/2025 01:25:15 INFO]: Training loss at epoch 23: 0.9335311651229858
[08/28/2025 01:25:17 INFO]: Training loss at epoch 38: 0.8071893155574799
[08/28/2025 01:25:26 INFO]: Training loss at epoch 10: 0.9631538391113281
[08/28/2025 01:25:28 INFO]: Training loss at epoch 7: 1.032120406627655
[08/28/2025 01:25:33 INFO]: Training loss at epoch 11: 1.1005781888961792
[08/28/2025 01:25:36 INFO]: Training loss at epoch 17: 0.9356576800346375
[08/28/2025 01:25:40 INFO]: Training loss at epoch 25: 1.1014752388000488
[08/28/2025 01:25:40 INFO]: Training loss at epoch 22: 1.175317108631134
[08/28/2025 01:25:50 INFO]: New best epoch, val score: -0.6671844894695276
[08/28/2025 01:25:50 INFO]: Saving model to: blotchy-Amado_trial_74/model_best.pth
[08/28/2025 01:25:53 INFO]: New best epoch, val score: -0.7474814836862603
[08/28/2025 01:25:53 INFO]: Saving model to: blotchy-Amado_trial_75/model_best.pth
[08/28/2025 01:25:58 INFO]: Training loss at epoch 28: 0.9080804884433746
[08/28/2025 01:26:04 INFO]: Training loss at epoch 17: 1.0014752447605133
[08/28/2025 01:26:08 INFO]: Training loss at epoch 18: 1.0946593284606934
[08/28/2025 01:26:13 INFO]: Training loss at epoch 24: 1.1238767504692078
[08/28/2025 01:26:43 INFO]: Training loss at epoch 12: 0.9518715441226959
[08/28/2025 01:26:44 INFO]: Training loss at epoch 26: 1.1257413029670715
[08/28/2025 01:27:03 INFO]: Training loss at epoch 18: 1.2123275995254517
[08/28/2025 01:27:14 INFO]: Training loss at epoch 3: 1.0179191827774048
[08/28/2025 01:27:15 INFO]: Training loss at epoch 25: 1.2600347399711609
[08/28/2025 01:27:19 INFO]: Training loss at epoch 8: 1.1911437511444092
[08/28/2025 01:27:23 INFO]: Training loss at epoch 3: 1.093848705291748
[08/28/2025 01:27:30 INFO]: Training loss at epoch 27: 1.2684726119041443
[08/28/2025 01:27:46 INFO]: Training loss at epoch 27: 1.1337533593177795
[08/28/2025 01:27:48 INFO]: New best epoch, val score: -0.6607062813596292
[08/28/2025 01:27:48 INFO]: Saving model to: blotchy-Amado_trial_69/model_best.pth
[08/28/2025 01:27:49 INFO]: New best epoch, val score: -0.6645973047695454
[08/28/2025 01:27:49 INFO]: Saving model to: blotchy-Amado_trial_81/model_best.pth
[08/28/2025 01:27:51 INFO]: Training loss at epoch 13: 0.9186189472675323
[08/28/2025 01:28:01 INFO]: Training loss at epoch 19: 1.1874374151229858
[08/28/2025 01:28:03 INFO]: Training loss at epoch 29: 1.1134889721870422
[08/28/2025 01:28:14 INFO]: Training loss at epoch 24: 0.9028143882751465
[08/28/2025 01:28:14 INFO]: Training loss at epoch 39: 1.1530625224113464
[08/28/2025 01:28:18 INFO]: Training loss at epoch 26: 1.036055862903595
[08/28/2025 01:28:22 INFO]: Training stats: {
    "score": -0.9950587326008237,
    "rmse": 0.9950587326008237
}
[08/28/2025 01:28:22 INFO]: Val stats: {
    "score": -0.6709690646672609,
    "rmse": 0.6709690646672609
}
[08/28/2025 01:28:22 INFO]: Test stats: {
    "score": -0.8769288696613535,
    "rmse": 0.8769288696613535
}
[08/28/2025 01:28:40 INFO]: Training loss at epoch 23: 1.2439666390419006
[08/28/2025 01:28:48 INFO]: Training stats: {
    "score": -1.0004841907223683,
    "rmse": 1.0004841907223683
}
[08/28/2025 01:28:48 INFO]: Val stats: {
    "score": -0.6699714626899476,
    "rmse": 0.6699714626899476
}
[08/28/2025 01:28:48 INFO]: Test stats: {
    "score": -0.8679636939583621,
    "rmse": 0.8679636939583621
}
[08/28/2025 01:28:50 INFO]: Training loss at epoch 28: 0.7854320406913757
[08/28/2025 01:29:01 INFO]: Training loss at epoch 14: 0.8494130373001099
[08/28/2025 01:29:03 INFO]: Training loss at epoch 11: 1.3500244915485382
[08/28/2025 01:29:07 INFO]: Training loss at epoch 8: 1.1672223210334778
[08/28/2025 01:29:15 INFO]: Training stats: {
    "score": -0.9996300001462095,
    "rmse": 0.9996300001462095
}
[08/28/2025 01:29:15 INFO]: Val stats: {
    "score": -0.6702119765538251,
    "rmse": 0.6702119765538251
}
[08/28/2025 01:29:15 INFO]: Test stats: {
    "score": -0.8707315396054669,
    "rmse": 0.8707315396054669
}
[08/28/2025 01:29:21 INFO]: Training loss at epoch 27: 1.002343237400055
[08/28/2025 01:29:23 INFO]: Training loss at epoch 20: 0.9769228994846344
[08/28/2025 01:29:33 INFO]: New best epoch, val score: -0.7356084498375667
[08/28/2025 01:29:33 INFO]: Saving model to: blotchy-Amado_trial_75/model_best.pth
[08/28/2025 01:29:53 INFO]: Training loss at epoch 29: 0.9871605932712555
[08/28/2025 01:29:57 INFO]: Training loss at epoch 16: 1.314546287059784
[08/28/2025 01:30:10 INFO]: Training loss at epoch 15: 0.9399183690547943
[08/28/2025 01:30:13 INFO]: Training stats: {
    "score": -0.9970163307582411,
    "rmse": 0.9970163307582411
}
[08/28/2025 01:30:13 INFO]: Val stats: {
    "score": -0.6757292742543844,
    "rmse": 0.6757292742543844
}
[08/28/2025 01:30:13 INFO]: Test stats: {
    "score": -0.8722877980321472,
    "rmse": 0.8722877980321472
}
[08/28/2025 01:30:19 INFO]: Training loss at epoch 28: 1.3937247395515442
[08/28/2025 01:30:22 INFO]: Training loss at epoch 21: 1.3248077034950256
[08/28/2025 01:30:23 INFO]: Training loss at epoch 28: 0.910624235868454
[08/28/2025 01:30:43 INFO]: Training loss at epoch 4: 1.0951111912727356
[08/28/2025 01:30:46 INFO]: Training loss at epoch 18: 0.934930682182312
[08/28/2025 01:30:53 INFO]: Training loss at epoch 4: 1.0359053015708923
[08/28/2025 01:30:55 INFO]: Training loss at epoch 30: 1.3153277933597565
[08/28/2025 01:31:15 INFO]: Training loss at epoch 25: 0.8345803320407867
[08/28/2025 01:31:15 INFO]: Training loss at epoch 30: 0.8939278721809387
[08/28/2025 01:31:18 INFO]: Training loss at epoch 16: 1.0596119165420532
[08/28/2025 01:31:20 INFO]: Training loss at epoch 19: 1.1690983176231384
[08/28/2025 01:31:20 INFO]: Training loss at epoch 22: 1.040240466594696
[08/28/2025 01:31:26 INFO]: Training loss at epoch 29: 1.409205824136734
[08/28/2025 01:31:38 INFO]: Training loss at epoch 24: 1.0608287453651428
[08/28/2025 01:31:47 INFO]: Training stats: {
    "score": -1.005529250383362,
    "rmse": 1.005529250383362
}
[08/28/2025 01:31:47 INFO]: Val stats: {
    "score": -0.6583050817652052,
    "rmse": 0.6583050817652052
}
[08/28/2025 01:31:47 INFO]: Test stats: {
    "score": -0.8596921055211972,
    "rmse": 0.8596921055211972
}
[08/28/2025 01:32:08 INFO]: Training loss at epoch 40: 1.1649039387702942
[08/28/2025 01:32:09 INFO]: Training loss at epoch 61: 1.0288715362548828
[08/28/2025 01:32:14 INFO]: Training loss at epoch 9: 1.1508727669715881
[08/28/2025 01:32:14 INFO]: Training loss at epoch 31: 0.8487237989902496
[08/28/2025 01:32:16 INFO]: Training loss at epoch 23: 0.769133061170578
[08/28/2025 01:32:25 INFO]: Training loss at epoch 17: 1.4028054773807526
[08/28/2025 01:32:37 INFO]: Training loss at epoch 12: 0.8945035040378571
[08/28/2025 01:32:41 INFO]: Training loss at epoch 9: 1.01085963845253
[08/28/2025 01:32:51 INFO]: Training loss at epoch 30: 1.1447823643684387
[08/28/2025 01:32:55 INFO]: Training loss at epoch 63: 0.8364286422729492
[08/28/2025 01:32:57 INFO]: Training loss at epoch 31: 1.361097812652588
[08/28/2025 01:33:01 INFO]: Training stats: {
    "score": -0.9985784072468641,
    "rmse": 0.9985784072468641
}
[08/28/2025 01:33:01 INFO]: Val stats: {
    "score": -0.6784142478949595,
    "rmse": 0.6784142478949595
}
[08/28/2025 01:33:01 INFO]: Test stats: {
    "score": -0.8740781549879669,
    "rmse": 0.8740781549879669
}
[08/28/2025 01:33:06 INFO]: Training loss at epoch 29: 1.0089357793331146
[08/28/2025 01:33:16 INFO]: Training loss at epoch 24: 1.08773672580719
[08/28/2025 01:33:19 INFO]: Training loss at epoch 32: 1.3475453853607178
[08/28/2025 01:33:38 INFO]: Training loss at epoch 18: 1.244812697172165
[08/28/2025 01:33:54 INFO]: Training stats: {
    "score": -0.9945555757664998,
    "rmse": 0.9945555757664998
}
[08/28/2025 01:33:54 INFO]: Val stats: {
    "score": -0.7235375215563263,
    "rmse": 0.7235375215563263
}
[08/28/2025 01:33:54 INFO]: Test stats: {
    "score": -0.8895553848651505,
    "rmse": 0.8895553848651505
}
[08/28/2025 01:33:56 INFO]: Training stats: {
    "score": -1.019257998758815,
    "rmse": 1.019257998758815
}
[08/28/2025 01:33:56 INFO]: Val stats: {
    "score": -0.7589813608820997,
    "rmse": 0.7589813608820997
}
[08/28/2025 01:33:56 INFO]: Test stats: {
    "score": -0.9205149820809518,
    "rmse": 0.9205149820809518
}
[08/28/2025 01:33:59 INFO]: Training loss at epoch 31: 1.0982317328453064
[08/28/2025 01:34:04 INFO]: Training stats: {
    "score": -1.0152593010562971,
    "rmse": 1.0152593010562971
}
[08/28/2025 01:34:04 INFO]: Val stats: {
    "score": -0.6606494673003696,
    "rmse": 0.6606494673003696
}
[08/28/2025 01:34:04 INFO]: Test stats: {
    "score": -0.8736679788635527,
    "rmse": 0.8736679788635527
}
[08/28/2025 01:34:11 INFO]: Training loss at epoch 5: 0.9837989509105682
[08/28/2025 01:34:14 INFO]: Training loss at epoch 26: 0.9934184551239014
[08/28/2025 01:34:17 INFO]: Training loss at epoch 25: 1.0857497453689575
[08/28/2025 01:34:20 INFO]: New best epoch, val score: -0.7235375215563263
[08/28/2025 01:34:20 INFO]: Saving model to: blotchy-Amado_trial_75/model_best.pth
[08/28/2025 01:34:21 INFO]: Training loss at epoch 5: 1.269012689590454
[08/28/2025 01:34:22 INFO]: Training loss at epoch 33: 1.032818078994751
[08/28/2025 01:34:24 INFO]: New best epoch, val score: -0.6606494673003696
[08/28/2025 01:34:24 INFO]: Saving model to: blotchy-Amado_trial_69/model_best.pth
[08/28/2025 01:34:35 INFO]: Training loss at epoch 25: 0.9230014383792877
[08/28/2025 01:34:46 INFO]: Training loss at epoch 19: 1.2402629256248474
[08/28/2025 01:34:53 INFO]: Training loss at epoch 17: 1.1877017319202423
[08/28/2025 01:35:03 INFO]: Training loss at epoch 32: 0.9802758991718292
[08/28/2025 01:35:05 INFO]: Training loss at epoch 32: 1.0973730087280273
[08/28/2025 01:35:08 INFO]: Training loss at epoch 41: 1.1283946633338928
[08/28/2025 01:35:12 INFO]: Training stats: {
    "score": -1.02181537842036,
    "rmse": 1.02181537842036
}
[08/28/2025 01:35:12 INFO]: Val stats: {
    "score": -0.7496871336401567,
    "rmse": 0.7496871336401567
}
[08/28/2025 01:35:12 INFO]: Test stats: {
    "score": -0.9077525730052332,
    "rmse": 0.9077525730052332
}
[08/28/2025 01:35:15 INFO]: Training loss at epoch 26: 1.0388804972171783
[08/28/2025 01:35:24 INFO]: Training loss at epoch 34: 1.2311346530914307
[08/28/2025 01:35:29 INFO]: Training loss at epoch 51: 0.8475859463214874
[08/28/2025 01:35:31 INFO]: New best epoch, val score: -0.669254746827872
[08/28/2025 01:35:31 INFO]: Saving model to: blotchy-Amado_trial_76/model_best.pth
[08/28/2025 01:35:54 INFO]: Training loss at epoch 19: 1.3094589710235596
[08/28/2025 01:36:05 INFO]: Training loss at epoch 33: 0.8686763346195221
[08/28/2025 01:36:16 INFO]: Training loss at epoch 13: 0.8706201910972595
[08/28/2025 01:36:17 INFO]: Training loss at epoch 27: 0.9858985543251038
[08/28/2025 01:36:21 INFO]: Training loss at epoch 20: 0.9747266471385956
[08/28/2025 01:36:25 INFO]: Training loss at epoch 35: 0.9953964948654175
[08/28/2025 01:36:51 INFO]: Training loss at epoch 30: 0.9789210259914398
[08/28/2025 01:37:06 INFO]: Training loss at epoch 34: 0.8758982419967651
[08/28/2025 01:37:10 INFO]: Training loss at epoch 33: 1.0079386830329895
[08/28/2025 01:37:11 INFO]: New best epoch, val score: -0.6600227844538308
[08/28/2025 01:37:11 INFO]: Saving model to: blotchy-Amado_trial_69/model_best.pth
[08/28/2025 01:37:11 INFO]: Training loss at epoch 27: 0.9667079746723175
[08/28/2025 01:37:16 INFO]: Training loss at epoch 28: 0.8808131217956543
[08/28/2025 01:37:26 INFO]: Running Final Evaluation...
[08/28/2025 01:37:28 INFO]: Training loss at epoch 36: 0.9226803779602051
[08/28/2025 01:37:30 INFO]: Training loss at epoch 21: 1.068298101425171
[08/28/2025 01:37:31 INFO]: Training loss at epoch 10: 1.0973694324493408
[08/28/2025 01:37:35 INFO]: Training loss at epoch 26: 0.9325129389762878
[08/28/2025 01:37:38 INFO]: Training stats: {
    "score": -1.022897146087498,
    "rmse": 1.022897146087498
}
[08/28/2025 01:37:38 INFO]: Val stats: {
    "score": -0.765574540209097,
    "rmse": 0.765574540209097
}
[08/28/2025 01:37:38 INFO]: Test stats: {
    "score": -0.9238868626667847,
    "rmse": 0.9238868626667847
}
[08/28/2025 01:37:41 INFO]: Training loss at epoch 6: 1.0303429961204529
[08/28/2025 01:37:50 INFO]: Training loss at epoch 6: 1.1056960821151733
[08/28/2025 01:37:55 INFO]: New best epoch, val score: -0.6777385843785337
[08/28/2025 01:37:55 INFO]: Saving model to: blotchy-Amado_trial_75/model_best.pth
[08/28/2025 01:38:04 INFO]: Training loss at epoch 42: 0.9969940781593323
[08/28/2025 01:38:08 INFO]: Training loss at epoch 35: 0.8837071359157562
[08/28/2025 01:38:09 INFO]: Training accuracy: {
    "score": -1.0186904465210496,
    "rmse": 1.0186904465210496
}
[08/28/2025 01:38:09 INFO]: Val accuracy: {
    "score": -0.661479476247227,
    "rmse": 0.661479476247227
}
[08/28/2025 01:38:09 INFO]: Test accuracy: {
    "score": -0.8706925645280045,
    "rmse": 0.8706925645280045
}
[08/28/2025 01:38:09 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_72",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8706925645280045,
        "rmse": 0.8706925645280045
    },
    "train_stats": {
        "score": -1.0186904465210496,
        "rmse": 1.0186904465210496
    },
    "val_stats": {
        "score": -0.661479476247227,
        "rmse": 0.661479476247227
    }
}
[08/28/2025 01:38:09 INFO]: Procewss finished for trial blotchy-Amado_trial_72
[08/28/2025 01:38:10 INFO]: 
_________________________________________________

[08/28/2025 01:38:10 INFO]: train_net_for_optune.py main() running.
[08/28/2025 01:38:10 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 16
  d_ffn_factor: 2.471520165098916
  attention_dropout: 0.4603318650127034
  ffn_dropout: 0.4603318650127034
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 5.343258483152334e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_82

[08/28/2025 01:38:10 INFO]: This ft_transformer has 1.929 million parameters.
[08/28/2025 01:38:10 INFO]: Training will start at epoch 0.
[08/28/2025 01:38:10 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 01:38:10 INFO]: Training loss at epoch 20: 1.020133376121521
[08/28/2025 01:38:15 INFO]: Training loss at epoch 29: 0.8979793787002563
[08/28/2025 01:38:29 INFO]: Training loss at epoch 37: 0.9613162875175476
[08/28/2025 01:38:37 INFO]: Training stats: {
    "score": -1.0557609470881424,
    "rmse": 1.0557609470881424
}
[08/28/2025 01:38:37 INFO]: Val stats: {
    "score": -0.6876723709717041,
    "rmse": 0.6876723709717041
}
[08/28/2025 01:38:37 INFO]: Test stats: {
    "score": -0.908472631302212,
    "rmse": 0.908472631302212
}
[08/28/2025 01:38:38 INFO]: Training loss at epoch 22: 1.0525729656219482
[08/28/2025 01:38:47 INFO]: New best epoch, val score: -0.6605059883505614
[08/28/2025 01:38:47 INFO]: Saving model to: blotchy-Amado_trial_66/model_best.pth
[08/28/2025 01:38:53 INFO]: Training loss at epoch 10: 0.9691888391971588
[08/28/2025 01:39:09 INFO]: Training loss at epoch 36: 1.0010533928871155
[08/28/2025 01:39:31 INFO]: Training loss at epoch 38: 1.147343397140503
[08/28/2025 01:39:35 INFO]: Training loss at epoch 30: 0.974672943353653
[08/28/2025 01:39:39 INFO]: Training loss at epoch 31: 1.0171721875667572
[08/28/2025 01:39:46 INFO]: Training loss at epoch 23: 1.1785825490951538
[08/28/2025 01:39:48 INFO]: Training loss at epoch 18: 0.9396078586578369
[08/28/2025 01:39:50 INFO]: Training loss at epoch 14: 1.0087384283542633
[08/28/2025 01:40:08 INFO]: Training loss at epoch 28: 0.8989466726779938
[08/28/2025 01:40:10 INFO]: Training loss at epoch 37: 0.9622281193733215
[08/28/2025 01:40:31 INFO]: Training loss at epoch 27: 1.0454648733139038
[08/28/2025 01:40:33 INFO]: Training loss at epoch 39: 1.1682553887367249
[08/28/2025 01:40:34 INFO]: Training loss at epoch 31: 0.9404950439929962
[08/28/2025 01:40:53 INFO]: Training stats: {
    "score": -0.9939511025472764,
    "rmse": 0.9939511025472764
}
[08/28/2025 01:40:53 INFO]: Val stats: {
    "score": -0.6841790366063908,
    "rmse": 0.6841790366063908
}
[08/28/2025 01:40:53 INFO]: Test stats: {
    "score": -0.8741927213416493,
    "rmse": 0.8741927213416493
}
[08/28/2025 01:40:54 INFO]: Training loss at epoch 24: 0.9007337093353271
[08/28/2025 01:41:00 INFO]: Training loss at epoch 43: 1.151864767074585
[08/28/2025 01:41:06 INFO]: Training loss at epoch 11: 1.0919569730758667
[08/28/2025 01:41:06 INFO]: Training loss at epoch 7: 1.0318138003349304
[08/28/2025 01:41:13 INFO]: Training loss at epoch 38: 1.14347705245018
[08/28/2025 01:41:16 INFO]: Training loss at epoch 7: 0.9928127527236938
[08/28/2025 01:41:18 INFO]: Training loss at epoch 0: 1.2199377417564392
[08/28/2025 01:41:33 INFO]: Training loss at epoch 62: 0.8525579571723938
[08/28/2025 01:41:33 INFO]: Training loss at epoch 32: 1.2394751906394958
[08/28/2025 01:41:44 INFO]: New best epoch, val score: -0.7083799915477405
[08/28/2025 01:41:44 INFO]: Saving model to: blotchy-Amado_trial_82/model_best.pth
[08/28/2025 01:41:45 INFO]: Training loss at epoch 64: 0.8307522237300873
[08/28/2025 01:41:55 INFO]: Training loss at epoch 40: 0.9395492076873779
[08/28/2025 01:42:04 INFO]: Training loss at epoch 25: 0.943292647600174
[08/28/2025 01:42:15 INFO]: Training loss at epoch 39: 1.04800146818161
[08/28/2025 01:42:26 INFO]: Training loss at epoch 32: 1.1025363206863403
[08/28/2025 01:42:31 INFO]: Training loss at epoch 33: 1.0719959735870361
[08/28/2025 01:42:34 INFO]: Training stats: {
    "score": -1.0058950732172907,
    "rmse": 1.0058950732172907
}
[08/28/2025 01:42:34 INFO]: Val stats: {
    "score": -0.6563443273497434,
    "rmse": 0.6563443273497434
}
[08/28/2025 01:42:34 INFO]: Test stats: {
    "score": -0.8610298420205916,
    "rmse": 0.8610298420205916
}
[08/28/2025 01:42:43 INFO]: Training loss at epoch 20: 0.9989012479782104
[08/28/2025 01:42:56 INFO]: Training loss at epoch 41: 1.1910333633422852
[08/28/2025 01:43:05 INFO]: Training loss at epoch 29: 1.053123116493225
[08/28/2025 01:43:10 INFO]: Training loss at epoch 26: 0.7901478707790375
[08/28/2025 01:43:17 INFO]: Training loss at epoch 21: 0.8536447584629059
[08/28/2025 01:43:22 INFO]: Training loss at epoch 15: 1.140242576599121
[08/28/2025 01:43:24 INFO]: Training loss at epoch 28: 1.1892832517623901
[08/28/2025 01:43:27 INFO]: Training loss at epoch 34: 0.8563653230667114
[08/28/2025 01:43:34 INFO]: Training loss at epoch 40: 1.1438454985618591
[08/28/2025 01:43:48 INFO]: Training loss at epoch 11: 1.1341887712478638
[08/28/2025 01:43:55 INFO]: Training loss at epoch 44: 1.2144731283187866
[08/28/2025 01:43:58 INFO]: Training loss at epoch 42: 0.9828985929489136
[08/28/2025 01:44:05 INFO]: Training stats: {
    "score": -1.0122351950361967,
    "rmse": 1.0122351950361967
}
[08/28/2025 01:44:05 INFO]: Val stats: {
    "score": -0.7406699533752172,
    "rmse": 0.7406699533752172
}
[08/28/2025 01:44:05 INFO]: Test stats: {
    "score": -0.908819463665691,
    "rmse": 0.908819463665691
}
[08/28/2025 01:44:20 INFO]: Training loss at epoch 27: 0.9856151938438416
[08/28/2025 01:44:29 INFO]: Training loss at epoch 35: 0.9803205728530884
[08/28/2025 01:44:32 INFO]: Training loss at epoch 8: 0.9059942066669464
[08/28/2025 01:44:36 INFO]: Training loss at epoch 41: 1.0704207718372345
[08/28/2025 01:44:38 INFO]: Training loss at epoch 12: 0.8884598016738892
[08/28/2025 01:44:43 INFO]: Training loss at epoch 8: 0.9034941792488098
[08/28/2025 01:44:44 INFO]: Training loss at epoch 19: 0.9283934235572815
[08/28/2025 01:44:56 INFO]: Training loss at epoch 1: 0.966631680727005
[08/28/2025 01:45:02 INFO]: Training loss at epoch 43: 1.190974473953247
[08/28/2025 01:45:14 INFO]: Training loss at epoch 33: 0.9126822054386139
[08/28/2025 01:45:24 INFO]: New best epoch, val score: -0.6920259120357335
[08/28/2025 01:45:24 INFO]: Saving model to: blotchy-Amado_trial_82/model_best.pth
[08/28/2025 01:45:30 INFO]: Training loss at epoch 36: 0.8491820394992828
[08/28/2025 01:45:30 INFO]: Training loss at epoch 28: 1.2018702924251556
[08/28/2025 01:45:40 INFO]: Training loss at epoch 42: 1.1321961283683777
[08/28/2025 01:46:05 INFO]: Training loss at epoch 44: 1.1150698065757751
[08/28/2025 01:46:28 INFO]: Training loss at epoch 29: 1.3416067957878113
[08/28/2025 01:46:29 INFO]: Training stats: {
    "score": -1.0070116148146635,
    "rmse": 1.0070116148146635
}
[08/28/2025 01:46:29 INFO]: Val stats: {
    "score": -0.6617961190022187,
    "rmse": 0.6617961190022187
}
[08/28/2025 01:46:29 INFO]: Test stats: {
    "score": -0.8689413537502209,
    "rmse": 0.8689413537502209
}
[08/28/2025 01:46:30 INFO]: Training loss at epoch 37: 0.9948790669441223
[08/28/2025 01:46:40 INFO]: Training loss at epoch 29: 1.0996321439743042
[08/28/2025 01:46:43 INFO]: Training loss at epoch 52: 0.6939072608947754
[08/28/2025 01:46:43 INFO]: Training loss at epoch 43: 1.1710546016693115
[08/28/2025 01:46:58 INFO]: Training loss at epoch 45: 0.9473593235015869
[08/28/2025 01:47:05 INFO]: Training loss at epoch 16: 0.9773726761341095
[08/28/2025 01:47:06 INFO]: Training stats: {
    "score": -1.0325860393033235,
    "rmse": 1.0325860393033235
}
[08/28/2025 01:47:06 INFO]: Val stats: {
    "score": -0.7812356446916657,
    "rmse": 0.7812356446916657
}
[08/28/2025 01:47:06 INFO]: Test stats: {
    "score": -0.9291077773985368,
    "rmse": 0.9291077773985368
}
[08/28/2025 01:47:08 INFO]: Training loss at epoch 30: 1.1805816292762756
[08/28/2025 01:47:09 INFO]: Training loss at epoch 45: 0.9887439608573914
[08/28/2025 01:47:27 INFO]: Training stats: {
    "score": -1.0000710696289457,
    "rmse": 1.0000710696289457
}
[08/28/2025 01:47:27 INFO]: Val stats: {
    "score": -0.7031310296460331,
    "rmse": 0.7031310296460331
}
[08/28/2025 01:47:27 INFO]: Test stats: {
    "score": -0.8856651773554545,
    "rmse": 0.8856651773554545
}
[08/28/2025 01:47:29 INFO]: Training loss at epoch 38: 1.1000052094459534
[08/28/2025 01:47:45 INFO]: Training loss at epoch 44: 0.9280029535293579
[08/28/2025 01:47:52 INFO]: Training loss at epoch 21: 0.8830490112304688
[08/28/2025 01:48:01 INFO]: Training loss at epoch 9: 1.1901251077651978
[08/28/2025 01:48:02 INFO]: Training loss at epoch 34: 1.0387157201766968
[08/28/2025 01:48:10 INFO]: Training loss at epoch 46: 0.982089102268219
[08/28/2025 01:48:14 INFO]: Training loss at epoch 30: 1.054164707660675
[08/28/2025 01:48:15 INFO]: Training loss at epoch 9: 1.1803854703903198
[08/28/2025 01:48:18 INFO]: Training loss at epoch 13: 1.1108229160308838
[08/28/2025 01:48:28 INFO]: Training loss at epoch 39: 0.8610771894454956
[08/28/2025 01:48:32 INFO]: Training loss at epoch 2: 1.3891213536262512
[08/28/2025 01:48:33 INFO]: Training loss at epoch 22: 1.085312306880951
[08/28/2025 01:48:48 INFO]: Training loss at epoch 45: 0.8970101177692413
[08/28/2025 01:48:49 INFO]: Training stats: {
    "score": -0.9998665864625975,
    "rmse": 0.9998665864625975
}
[08/28/2025 01:48:49 INFO]: Val stats: {
    "score": -0.6612046709704774,
    "rmse": 0.6612046709704774
}
[08/28/2025 01:48:49 INFO]: Test stats: {
    "score": -0.8760181342801678,
    "rmse": 0.8760181342801678
}
[08/28/2025 01:48:50 INFO]: Training loss at epoch 12: 0.9440364837646484
[08/28/2025 01:49:08 INFO]: New best epoch, val score: -0.6604344542036932
[08/28/2025 01:49:08 INFO]: Saving model to: blotchy-Amado_trial_66/model_best.pth
[08/28/2025 01:49:12 INFO]: Training loss at epoch 47: 1.119852900505066
[08/28/2025 01:49:16 INFO]: Training stats: {
    "score": -1.005748984191137,
    "rmse": 1.005748984191137
}
[08/28/2025 01:49:16 INFO]: Val stats: {
    "score": -0.6620024293453821,
    "rmse": 0.6620024293453821
}
[08/28/2025 01:49:16 INFO]: Test stats: {
    "score": -0.8751234226351815,
    "rmse": 0.8751234226351815
}
[08/28/2025 01:49:22 INFO]: Training loss at epoch 31: 1.0067570507526398
[08/28/2025 01:49:23 INFO]: New best epoch, val score: -0.6670146221992874
[08/28/2025 01:49:23 INFO]: Saving model to: blotchy-Amado_trial_73/model_best.pth
[08/28/2025 01:49:28 INFO]: Training stats: {
    "score": -1.004625123487227,
    "rmse": 1.004625123487227
}
[08/28/2025 01:49:28 INFO]: Val stats: {
    "score": -0.6747638758348611,
    "rmse": 0.6747638758348611
}
[08/28/2025 01:49:28 INFO]: Test stats: {
    "score": -0.8683875936512949,
    "rmse": 0.8683875936512949
}
[08/28/2025 01:49:30 INFO]: Running Final Evaluation...
[08/28/2025 01:49:45 INFO]: Training loss at epoch 40: 1.0159419775009155
[08/28/2025 01:49:47 INFO]: Training loss at epoch 46: 0.9649081230163574
[08/28/2025 01:49:51 INFO]: Training loss at epoch 46: 1.1153348088264465
[08/28/2025 01:49:54 INFO]: Running Final Evaluation...
[08/28/2025 01:49:55 INFO]: Training accuracy: {
    "score": -1.028290116675855,
    "rmse": 1.028290116675855
}
[08/28/2025 01:49:55 INFO]: Val accuracy: {
    "score": -0.6653479459385244,
    "rmse": 0.6653479459385244
}
[08/28/2025 01:49:55 INFO]: Test accuracy: {
    "score": -0.8707896661829149,
    "rmse": 0.8707896661829149
}
[08/28/2025 01:49:55 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_79",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8707896661829149,
        "rmse": 0.8707896661829149
    },
    "train_stats": {
        "score": -1.028290116675855,
        "rmse": 1.028290116675855
    },
    "val_stats": {
        "score": -0.6653479459385244,
        "rmse": 0.6653479459385244
    }
}
[08/28/2025 01:49:55 INFO]: Procewss finished for trial blotchy-Amado_trial_79
[08/28/2025 01:49:55 INFO]: 
_________________________________________________

[08/28/2025 01:49:55 INFO]: train_net_for_optune.py main() running.
[08/28/2025 01:49:55 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 16
  d_ffn_factor: 2.173609896877601
  attention_dropout: 0.11750108444671399
  ffn_dropout: 0.11750108444671399
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.1961376339887007e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_83

[08/28/2025 01:49:55 INFO]: This ft_transformer has 7.011 million parameters.
[08/28/2025 01:49:55 INFO]: Training will start at epoch 0.
[08/28/2025 01:49:55 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 01:50:03 INFO]: Training loss at epoch 31: 0.99543496966362
[08/28/2025 01:50:11 INFO]: Training loss at epoch 48: 0.9592779278755188
[08/28/2025 01:50:15 INFO]: Training accuracy: {
    "score": -1.0152107092023661,
    "rmse": 1.0152107092023661
}
[08/28/2025 01:50:15 INFO]: Val accuracy: {
    "score": -0.6548800347397449,
    "rmse": 0.6548800347397449
}
[08/28/2025 01:50:15 INFO]: Test accuracy: {
    "score": -0.8540729120286925,
    "rmse": 0.8540729120286925
}
[08/28/2025 01:50:15 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_77",
    "best_epoch": 15,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8540729120286925,
        "rmse": 0.8540729120286925
    },
    "train_stats": {
        "score": -1.0152107092023661,
        "rmse": 1.0152107092023661
    },
    "val_stats": {
        "score": -0.6548800347397449,
        "rmse": 0.6548800347397449
    }
}
[08/28/2025 01:50:15 INFO]: Procewss finished for trial blotchy-Amado_trial_77
[08/28/2025 01:50:15 INFO]: 
_________________________________________________

[08/28/2025 01:50:15 INFO]: train_net_for_optune.py main() running.
[08/28/2025 01:50:15 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 2.1050531622982422
  attention_dropout: 0.356441492438638
  ffn_dropout: 0.356441492438638
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 5.2796204483628835e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_84

[08/28/2025 01:50:15 INFO]: This ft_transformer has 1.066 million parameters.
[08/28/2025 01:50:15 INFO]: Training will start at epoch 0.
[08/28/2025 01:50:15 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 01:50:22 INFO]: Training loss at epoch 30: 1.0177676975727081
[08/28/2025 01:50:36 INFO]: Training loss at epoch 17: 1.022593468427658
[08/28/2025 01:50:36 INFO]: Training loss at epoch 65: 0.9484315812587738
[08/28/2025 01:50:44 INFO]: Training loss at epoch 41: 1.2604557573795319
[08/28/2025 01:50:49 INFO]: Training loss at epoch 35: 1.01636803150177
[08/28/2025 01:50:55 INFO]: Training loss at epoch 63: 1.0499254763126373
[08/28/2025 01:51:09 INFO]: Training loss at epoch 0: 1.1250473260879517
[08/28/2025 01:51:15 INFO]: Training loss at epoch 49: 1.0990593433380127
[08/28/2025 01:51:16 INFO]: New best epoch, val score: -0.886086913678616
[08/28/2025 01:51:16 INFO]: Saving model to: blotchy-Amado_trial_84/model_best.pth
[08/28/2025 01:51:24 INFO]: Training loss at epoch 20: 1.0775107741355896
[08/28/2025 01:51:37 INFO]: Training stats: {
    "score": -0.9948600095349854,
    "rmse": 0.9948600095349854
}
[08/28/2025 01:51:37 INFO]: Val stats: {
    "score": -0.7003451161084591,
    "rmse": 0.7003451161084591
}
[08/28/2025 01:51:37 INFO]: Test stats: {
    "score": -0.8832943260962601,
    "rmse": 0.8832943260962601
}
[08/28/2025 01:51:44 INFO]: Training loss at epoch 42: 1.295178234577179
[08/28/2025 01:51:52 INFO]: Training loss at epoch 14: 1.2744920253753662
[08/28/2025 01:52:03 INFO]: Training loss at epoch 3: 0.9152956604957581
[08/28/2025 01:52:10 INFO]: Training loss at epoch 1: 1.0290580093860626
[08/28/2025 01:52:17 INFO]: New best epoch, val score: -0.6728573002165802
[08/28/2025 01:52:17 INFO]: Saving model to: blotchy-Amado_trial_84/model_best.pth
[08/28/2025 01:52:39 INFO]: Training loss at epoch 50: 1.0005890727043152
[08/28/2025 01:52:43 INFO]: Training loss at epoch 10: 0.9930371642112732
[08/28/2025 01:52:44 INFO]: Training loss at epoch 43: 1.1160079836845398
[08/28/2025 01:52:51 INFO]: Training loss at epoch 47: 1.1587084531784058
[08/28/2025 01:52:57 INFO]: Training loss at epoch 10: 0.9436701238155365
[08/28/2025 01:53:01 INFO]: Training loss at epoch 22: 1.022739291191101
[08/28/2025 01:53:05 INFO]: Training loss at epoch 32: 1.121397227048874
[08/28/2025 01:53:09 INFO]: Training loss at epoch 2: 1.2886542677879333
[08/28/2025 01:53:23 INFO]: Training loss at epoch 31: 0.8571022748947144
[08/28/2025 01:53:40 INFO]: Training loss at epoch 36: 0.8955239951610565
[08/28/2025 01:53:42 INFO]: Training loss at epoch 51: 0.9351805150508881
[08/28/2025 01:53:43 INFO]: Training loss at epoch 23: 0.9712749421596527
[08/28/2025 01:53:44 INFO]: Training loss at epoch 44: 1.1350667476654053
[08/28/2025 01:53:51 INFO]: Training loss at epoch 13: 1.2055031657218933
[08/28/2025 01:54:09 INFO]: Training loss at epoch 3: 1.3119890689849854
[08/28/2025 01:54:16 INFO]: Training loss at epoch 18: 1.1131420135498047
[08/28/2025 01:54:44 INFO]: Training loss at epoch 45: 0.7767904847860336
[08/28/2025 01:54:46 INFO]: Training loss at epoch 52: 1.164805680513382
[08/28/2025 01:54:51 INFO]: Running Final Evaluation...
[08/28/2025 01:55:08 INFO]: Training loss at epoch 4: 1.049609124660492
[08/28/2025 01:55:11 INFO]: Training accuracy: {
    "score": -1.0068181102294214,
    "rmse": 1.0068181102294214
}
[08/28/2025 01:55:11 INFO]: Val accuracy: {
    "score": -0.6600370803256835,
    "rmse": 0.6600370803256835
}
[08/28/2025 01:55:11 INFO]: Test accuracy: {
    "score": -0.8759461633161113,
    "rmse": 0.8759461633161113
}
[08/28/2025 01:55:11 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_78",
    "best_epoch": 14,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8759461633161113,
        "rmse": 0.8759461633161113
    },
    "train_stats": {
        "score": -1.0068181102294214,
        "rmse": 1.0068181102294214
    },
    "val_stats": {
        "score": -0.6600370803256835,
        "rmse": 0.6600370803256835
    }
}
[08/28/2025 01:55:11 INFO]: Procewss finished for trial blotchy-Amado_trial_78
[08/28/2025 01:55:11 INFO]: 
_________________________________________________

[08/28/2025 01:55:11 INFO]: train_net_for_optune.py main() running.
[08/28/2025 01:55:11 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 16
  d_ffn_factor: 2.4680903809002284
  attention_dropout: 0.1368756629140086
  ffn_dropout: 0.1368756629140086
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.849011512353707e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_85

[08/28/2025 01:55:11 INFO]: This ft_transformer has 1.925 million parameters.
[08/28/2025 01:55:11 INFO]: Training will start at epoch 0.
[08/28/2025 01:55:11 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 01:55:14 INFO]: Training loss at epoch 0: 1.4220096468925476
[08/28/2025 01:55:31 INFO]: Training loss at epoch 15: 1.0697622299194336
[08/28/2025 01:55:38 INFO]: Training loss at epoch 4: 0.824578046798706
[08/28/2025 01:55:48 INFO]: Training loss at epoch 53: 0.9413029551506042
[08/28/2025 01:55:51 INFO]: Training loss at epoch 48: 1.066221833229065
[08/28/2025 01:56:00 INFO]: New best epoch, val score: -0.7572542067688119
[08/28/2025 01:56:00 INFO]: Saving model to: blotchy-Amado_trial_83/model_best.pth
[08/28/2025 01:56:07 INFO]: Training loss at epoch 33: 1.072349190711975
[08/28/2025 01:56:08 INFO]: Training loss at epoch 5: 1.0504936575889587
[08/28/2025 01:56:12 INFO]: New best epoch, val score: -0.661752351129538
[08/28/2025 01:56:12 INFO]: Saving model to: blotchy-Amado_trial_63/model_best.pth
[08/28/2025 01:56:13 INFO]: Training loss at epoch 11: 1.214200496673584
[08/28/2025 01:56:24 INFO]: Training loss at epoch 32: 0.9667434692382812
[08/28/2025 01:56:26 INFO]: Training loss at epoch 21: 0.9733185768127441
[08/28/2025 01:56:26 INFO]: Training loss at epoch 11: 0.9291056394577026
[08/28/2025 01:56:29 INFO]: Training loss at epoch 37: 1.059815376996994
[08/28/2025 01:56:38 INFO]: New best epoch, val score: -0.6598009258603086
[08/28/2025 01:56:38 INFO]: Saving model to: blotchy-Amado_trial_80/model_best.pth
[08/28/2025 01:56:49 INFO]: Training loss at epoch 54: 0.8402684032917023
[08/28/2025 01:57:05 INFO]: Training loss at epoch 6: 1.1045416593551636
[08/28/2025 01:57:52 INFO]: Training loss at epoch 55: 0.9698846340179443
[08/28/2025 01:57:53 INFO]: Training loss at epoch 19: 1.0413331389427185
[08/28/2025 01:57:57 INFO]: Training loss at epoch 53: 0.8064707517623901
[08/28/2025 01:58:05 INFO]: Training loss at epoch 7: 1.025596261024475
[08/28/2025 01:58:10 INFO]: Training loss at epoch 23: 1.1551544666290283
[08/28/2025 01:58:17 INFO]: Training loss at epoch 0: 0.9714669287204742
[08/28/2025 01:58:42 INFO]: New best epoch, val score: -0.6951877771194819
[08/28/2025 01:58:42 INFO]: Saving model to: blotchy-Amado_trial_85/model_best.pth
[08/28/2025 01:58:46 INFO]: Training loss at epoch 49: 0.9389898478984833
[08/28/2025 01:58:47 INFO]: Training loss at epoch 14: 1.0233487486839294
[08/28/2025 01:58:51 INFO]: Training loss at epoch 56: 0.9175642728805542
[08/28/2025 01:58:51 INFO]: Training loss at epoch 24: 1.036201298236847
[08/28/2025 01:59:01 INFO]: Training stats: {
    "score": -1.019173760071573,
    "rmse": 1.019173760071573
}
[08/28/2025 01:59:01 INFO]: Val stats: {
    "score": -0.7827166551353794,
    "rmse": 0.7827166551353794
}
[08/28/2025 01:59:01 INFO]: Test stats: {
    "score": -0.9500489702519643,
    "rmse": 0.9500489702519643
}
[08/28/2025 01:59:01 INFO]: Training loss at epoch 8: 1.1861931681632996
[08/28/2025 01:59:02 INFO]: Training loss at epoch 34: 0.9176787734031677
[08/28/2025 01:59:05 INFO]: Training loss at epoch 16: 1.0027644336223602
[08/28/2025 01:59:10 INFO]: Training loss at epoch 5: 1.2046711444854736
[08/28/2025 01:59:15 INFO]: Training loss at epoch 38: 0.9539784491062164
[08/28/2025 01:59:20 INFO]: Training loss at epoch 33: 1.1080041527748108
[08/28/2025 01:59:30 INFO]: Training loss at epoch 66: 1.1310694813728333
[08/28/2025 01:59:38 INFO]: Training loss at epoch 12: 0.941891998052597
[08/28/2025 01:59:46 INFO]: Training stats: {
    "score": -1.004873934820852,
    "rmse": 1.004873934820852
}
[08/28/2025 01:59:46 INFO]: Val stats: {
    "score": -0.6630240804245278,
    "rmse": 0.6630240804245278
}
[08/28/2025 01:59:46 INFO]: Test stats: {
    "score": -0.8697339658455142,
    "rmse": 0.8697339658455142
}
[08/28/2025 01:59:53 INFO]: Training loss at epoch 12: 0.9597686231136322
[08/28/2025 01:59:55 INFO]: Training loss at epoch 57: 1.0483454465866089
[08/28/2025 02:00:01 INFO]: Training loss at epoch 9: 0.9544983208179474
[08/28/2025 02:00:04 INFO]: New best epoch, val score: -0.659227971505381
[08/28/2025 02:00:04 INFO]: Saving model to: blotchy-Amado_trial_80/model_best.pth
[08/28/2025 02:00:20 INFO]: Training loss at epoch 64: 1.022652804851532
[08/28/2025 02:00:22 INFO]: Training stats: {
    "score": -1.0007060458577532,
    "rmse": 1.0007060458577532
}
[08/28/2025 02:00:22 INFO]: Val stats: {
    "score": -0.6790811777187377,
    "rmse": 0.6790811777187377
}
[08/28/2025 02:00:22 INFO]: Test stats: {
    "score": -0.8751218808884306,
    "rmse": 0.8751218808884306
}
[08/28/2025 02:00:36 INFO]: Running Final Evaluation...
[08/28/2025 02:00:57 INFO]: Training loss at epoch 58: 1.2160910069942474
[08/28/2025 02:01:15 INFO]: Training loss at epoch 1: 1.154653549194336
[08/28/2025 02:01:21 INFO]: Training loss at epoch 10: 0.9730061590671539
[08/28/2025 02:01:23 INFO]: Training loss at epoch 22: 1.0243948698043823
[08/28/2025 02:01:51 INFO]: Training loss at epoch 1: 1.305246353149414
[08/28/2025 02:01:59 INFO]: New best epoch, val score: -0.7103333552208061
[08/28/2025 02:01:59 INFO]: Saving model to: blotchy-Amado_trial_83/model_best.pth
[08/28/2025 02:01:59 INFO]: Training loss at epoch 59: 0.8304060101509094
[08/28/2025 02:02:04 INFO]: Training loss at epoch 39: 0.963804304599762
[08/28/2025 02:02:04 INFO]: Training loss at epoch 35: 1.1685054898262024
[08/28/2025 02:02:16 INFO]: New best epoch, val score: -0.6718403384457194
[08/28/2025 02:02:16 INFO]: Saving model to: blotchy-Amado_trial_85/model_best.pth
[08/28/2025 02:02:19 INFO]: Training stats: {
    "score": -0.9934122862859137,
    "rmse": 0.9934122862859137
}
[08/28/2025 02:02:19 INFO]: Val stats: {
    "score": -0.6788203957399598,
    "rmse": 0.6788203957399598
}
[08/28/2025 02:02:19 INFO]: Test stats: {
    "score": -0.8737424933223267,
    "rmse": 0.8737424933223267
}
[08/28/2025 02:02:19 INFO]: Training loss at epoch 11: 0.9363684952259064
[08/28/2025 02:02:20 INFO]: Training loss at epoch 34: 1.0987064242362976
[08/28/2025 02:02:38 INFO]: Training loss at epoch 20: 0.9705932140350342
[08/28/2025 02:02:42 INFO]: Training loss at epoch 17: 0.9705643057823181
[08/28/2025 02:02:44 INFO]: Training loss at epoch 6: 0.8981091380119324
[08/28/2025 02:02:45 INFO]: Training loss at epoch 50: 0.8271231353282928
[08/28/2025 02:02:59 INFO]: Training stats: {
    "score": -1.011116079735806,
    "rmse": 1.011116079735806
}
[08/28/2025 02:02:59 INFO]: Val stats: {
    "score": -0.7381021093752486,
    "rmse": 0.7381021093752486
}
[08/28/2025 02:02:59 INFO]: Test stats: {
    "score": -0.9079027227755061,
    "rmse": 0.9079027227755061
}
[08/28/2025 02:03:05 INFO]: Training loss at epoch 13: 0.8629831075668335
[08/28/2025 02:03:16 INFO]: Training loss at epoch 24: 1.2546835541725159
[08/28/2025 02:03:18 INFO]: Training loss at epoch 12: 1.068821668624878
[08/28/2025 02:03:20 INFO]: Training loss at epoch 13: 0.9413353800773621
[08/28/2025 02:03:21 INFO]: Training loss at epoch 60: 0.9804154336452484
[08/28/2025 02:03:31 INFO]: New best epoch, val score: -0.6586041443359811
[08/28/2025 02:03:31 INFO]: Saving model to: blotchy-Amado_trial_80/model_best.pth
[08/28/2025 02:03:48 INFO]: Training loss at epoch 15: 0.9541987776756287
[08/28/2025 02:03:52 INFO]: Training accuracy: {
    "score": -1.0124928332221523,
    "rmse": 1.0124928332221523
}
[08/28/2025 02:03:52 INFO]: Val accuracy: {
    "score": -0.6605990469501009,
    "rmse": 0.6605990469501009
}
[08/28/2025 02:03:52 INFO]: Test accuracy: {
    "score": -0.8708699674807128,
    "rmse": 0.8708699674807128
}
[08/28/2025 02:03:53 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_35",
    "best_epoch": 35,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8708699674807128,
        "rmse": 0.8708699674807128
    },
    "train_stats": {
        "score": -1.0124928332221523,
        "rmse": 1.0124928332221523
    },
    "val_stats": {
        "score": -0.6605990469501009,
        "rmse": 0.6605990469501009
    }
}
[08/28/2025 02:03:53 INFO]: Procewss finished for trial blotchy-Amado_trial_35
[08/28/2025 02:03:53 INFO]: 
_________________________________________________

[08/28/2025 02:03:53 INFO]: train_net_for_optune.py main() running.
[08/28/2025 02:03:53 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 2.454541335710796
  attention_dropout: 0.3473758724457849
  ffn_dropout: 0.3473758724457849
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.9999897114570927e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_86

[08/28/2025 02:03:54 INFO]: This ft_transformer has 1.170 million parameters.
[08/28/2025 02:03:54 INFO]: Training will start at epoch 0.
[08/28/2025 02:03:54 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 02:04:04 INFO]: Training loss at epoch 25: 1.0596292614936829
[08/28/2025 02:04:19 INFO]: Training loss at epoch 13: 1.0473730564117432
[08/28/2025 02:04:26 INFO]: Training loss at epoch 61: 0.8401445150375366
[08/28/2025 02:04:50 INFO]: Training loss at epoch 0: 1.2925978899002075
[08/28/2025 02:04:57 INFO]: New best epoch, val score: -1.0278310015394228
[08/28/2025 02:04:57 INFO]: Saving model to: blotchy-Amado_trial_86/model_best.pth
[08/28/2025 02:05:05 INFO]: Training loss at epoch 36: 0.9537761807441711
[08/28/2025 02:05:18 INFO]: Training loss at epoch 14: 1.2088505923748016
[08/28/2025 02:05:24 INFO]: Training loss at epoch 35: 1.1472334265708923
[08/28/2025 02:05:25 INFO]: Training loss at epoch 2: 1.2169969975948334
[08/28/2025 02:05:27 INFO]: Training loss at epoch 62: 1.001566767692566
[08/28/2025 02:05:43 INFO]: Training loss at epoch 51: 0.9389967322349548
[08/28/2025 02:05:50 INFO]: Training loss at epoch 40: 1.2012128233909607
[08/28/2025 02:05:52 INFO]: Training loss at epoch 1: 0.9649164080619812
[08/28/2025 02:05:59 INFO]: New best epoch, val score: -0.8976847764751269
[08/28/2025 02:05:59 INFO]: Saving model to: blotchy-Amado_trial_86/model_best.pth
[08/28/2025 02:06:16 INFO]: Training loss at epoch 21: 1.085833728313446
[08/28/2025 02:06:17 INFO]: Training loss at epoch 7: 1.29630708694458
[08/28/2025 02:06:18 INFO]: Training loss at epoch 15: 1.2990873456001282
[08/28/2025 02:06:19 INFO]: Training loss at epoch 18: 0.9274484813213348
[08/28/2025 02:06:21 INFO]: Training loss at epoch 23: 0.9209273159503937
[08/28/2025 02:06:31 INFO]: Training loss at epoch 63: 1.1757349073886871
[08/28/2025 02:06:33 INFO]: Training loss at epoch 14: 1.056997835636139
[08/28/2025 02:06:43 INFO]: New best epoch, val score: -0.6775040452455698
[08/28/2025 02:06:43 INFO]: Saving model to: blotchy-Amado_trial_75/model_best.pth
[08/28/2025 02:06:49 INFO]: Training loss at epoch 14: 0.9207293391227722
[08/28/2025 02:06:54 INFO]: Training loss at epoch 2: 1.1509824395179749
[08/28/2025 02:07:00 INFO]: New best epoch, val score: -0.7453903353848536
[08/28/2025 02:07:00 INFO]: Saving model to: blotchy-Amado_trial_86/model_best.pth
[08/28/2025 02:07:13 INFO]: Training loss at epoch 2: 1.1141214966773987
[08/28/2025 02:07:14 INFO]: Training loss at epoch 16: 0.8941984176635742
[08/28/2025 02:07:30 INFO]: Training loss at epoch 64: 1.0651994943618774
[08/28/2025 02:07:54 INFO]: Training loss at epoch 3: 0.9678974449634552
[08/28/2025 02:08:02 INFO]: New best epoch, val score: -0.6543000441218239
[08/28/2025 02:08:02 INFO]: Saving model to: blotchy-Amado_trial_86/model_best.pth
[08/28/2025 02:08:02 INFO]: Training loss at epoch 37: 1.207610160112381
[08/28/2025 02:08:12 INFO]: Training loss at epoch 17: 0.9359055161476135
[08/28/2025 02:08:20 INFO]: Training loss at epoch 36: 1.0570178627967834
[08/28/2025 02:08:22 INFO]: Training loss at epoch 25: 0.8233867287635803
[08/28/2025 02:08:31 INFO]: Training loss at epoch 65: 1.0747994184494019
[08/28/2025 02:08:35 INFO]: Training loss at epoch 41: 0.9699669480323792
[08/28/2025 02:08:36 INFO]: Training loss at epoch 52: 1.0294247269630432
[08/28/2025 02:08:38 INFO]: Running Final Evaluation...
[08/28/2025 02:08:43 INFO]: Training loss at epoch 16: 1.1483280062675476
[08/28/2025 02:08:53 INFO]: Training loss at epoch 3: 0.9969368577003479
[08/28/2025 02:08:56 INFO]: Training loss at epoch 4: 1.5273675918579102
[08/28/2025 02:09:00 INFO]: Training accuracy: {
    "score": -1.001843040696677,
    "rmse": 1.001843040696677
}
[08/28/2025 02:09:00 INFO]: Val accuracy: {
    "score": -0.669254746827872,
    "rmse": 0.669254746827872
}
[08/28/2025 02:09:00 INFO]: Test accuracy: {
    "score": -0.8714281787624992,
    "rmse": 0.8714281787624992
}
[08/28/2025 02:09:00 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_76",
    "best_epoch": 34,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8714281787624992,
        "rmse": 0.8714281787624992
    },
    "train_stats": {
        "score": -1.001843040696677,
        "rmse": 1.001843040696677
    },
    "val_stats": {
        "score": -0.669254746827872,
        "rmse": 0.669254746827872
    }
}
[08/28/2025 02:09:00 INFO]: Procewss finished for trial blotchy-Amado_trial_76
[08/28/2025 02:09:00 INFO]: 
_________________________________________________

[08/28/2025 02:09:00 INFO]: train_net_for_optune.py main() running.
[08/28/2025 02:09:00 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.4830999702086904
  attention_dropout: 0.13196941976736437
  ffn_dropout: 0.13196941976736437
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.068489868821463e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_87

[08/28/2025 02:09:00 INFO]: This ft_transformer has 3.099 million parameters.
[08/28/2025 02:09:00 INFO]: Training will start at epoch 0.
[08/28/2025 02:09:00 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 02:09:06 INFO]: Training loss at epoch 54: 0.770172119140625
[08/28/2025 02:09:09 INFO]: Training loss at epoch 26: 1.0348618626594543
[08/28/2025 02:09:11 INFO]: Training loss at epoch 18: 0.9981242716312408
[08/28/2025 02:09:43 INFO]: Training loss at epoch 65: 0.901077538728714
[08/28/2025 02:09:47 INFO]: Training loss at epoch 8: 0.9349188208580017
[08/28/2025 02:09:50 INFO]: Training loss at epoch 22: 1.0825408697128296
[08/28/2025 02:09:54 INFO]: Training loss at epoch 19: 0.9200815260410309
[08/28/2025 02:09:59 INFO]: Training loss at epoch 15: 1.2215555906295776
[08/28/2025 02:10:01 INFO]: Training loss at epoch 5: 1.2399187684059143
[08/28/2025 02:10:12 INFO]: Training loss at epoch 19: 1.044406235218048
[08/28/2025 02:10:16 INFO]: Training loss at epoch 15: 1.0044986605644226
[08/28/2025 02:10:26 INFO]: New best epoch, val score: -0.6578776435241721
[08/28/2025 02:10:26 INFO]: Saving model to: blotchy-Amado_trial_80/model_best.pth
[08/28/2025 02:10:33 INFO]: Training stats: {
    "score": -0.9983915323137658,
    "rmse": 0.9983915323137658
}
[08/28/2025 02:10:33 INFO]: Val stats: {
    "score": -0.6758659529350984,
    "rmse": 0.6758659529350984
}
[08/28/2025 02:10:33 INFO]: Test stats: {
    "score": -0.8728921671161961,
    "rmse": 0.8728921671161961
}
[08/28/2025 02:10:57 INFO]: Training loss at epoch 0: 1.099210500717163
[08/28/2025 02:11:02 INFO]: Training loss at epoch 6: 1.0005938112735748
[08/28/2025 02:11:02 INFO]: Training loss at epoch 38: 0.7973696887493134
[08/28/2025 02:11:05 INFO]: Training stats: {
    "score": -0.9941442122265864,
    "rmse": 0.9941442122265864
}
[08/28/2025 02:11:05 INFO]: Val stats: {
    "score": -0.6796894913265376,
    "rmse": 0.6796894913265376
}
[08/28/2025 02:11:05 INFO]: Test stats: {
    "score": -0.872225338395435,
    "rmse": 0.872225338395435
}
[08/28/2025 02:11:14 INFO]: New best epoch, val score: -0.7901200254247474
[08/28/2025 02:11:14 INFO]: Saving model to: blotchy-Amado_trial_87/model_best.pth
[08/28/2025 02:11:18 INFO]: Training loss at epoch 24: 0.9228483140468597
[08/28/2025 02:11:22 INFO]: Training loss at epoch 37: 0.9028103351593018
[08/28/2025 02:11:27 INFO]: Training loss at epoch 42: 1.3692052364349365
[08/28/2025 02:11:32 INFO]: Training loss at epoch 20: 1.1204944252967834
[08/28/2025 02:11:39 INFO]: Training loss at epoch 53: 0.9980553090572357
[08/28/2025 02:12:07 INFO]: Training loss at epoch 7: 1.0164862275123596
[08/28/2025 02:12:30 INFO]: Training loss at epoch 21: 1.0705840587615967
[08/28/2025 02:12:35 INFO]: Training loss at epoch 4: 0.9369877874851227
[08/28/2025 02:13:05 INFO]: Training loss at epoch 1: 1.1374384760856628
[08/28/2025 02:13:06 INFO]: Training loss at epoch 8: 0.9387432634830475
[08/28/2025 02:13:11 INFO]: Training loss at epoch 3: 1.2563977241516113
[08/28/2025 02:13:17 INFO]: Training loss at epoch 9: 1.2699041366577148
[08/28/2025 02:13:20 INFO]: New best epoch, val score: -0.672942369716418
[08/28/2025 02:13:20 INFO]: Saving model to: blotchy-Amado_trial_87/model_best.pth
[08/28/2025 02:13:22 INFO]: Training loss at epoch 23: 0.8577514886856079
[08/28/2025 02:13:23 INFO]: Training loss at epoch 16: 1.2039821147918701
[08/28/2025 02:13:26 INFO]: Training loss at epoch 22: 1.2230269312858582
[08/28/2025 02:13:29 INFO]: Training loss at epoch 26: 0.8823806643486023
[08/28/2025 02:13:40 INFO]: Training loss at epoch 16: 0.8691646456718445
[08/28/2025 02:13:43 INFO]: Training loss at epoch 17: 1.077234923839569
[08/28/2025 02:13:48 INFO]: New best epoch, val score: -0.6566793603056257
[08/28/2025 02:13:48 INFO]: Saving model to: blotchy-Amado_trial_80/model_best.pth
[08/28/2025 02:13:56 INFO]: Training loss at epoch 39: 1.019045352935791
[08/28/2025 02:14:05 INFO]: Training loss at epoch 9: 0.895357757806778
[08/28/2025 02:14:08 INFO]: Training loss at epoch 43: 0.8083604574203491
[08/28/2025 02:14:15 INFO]: Training loss at epoch 38: 0.9735336303710938
[08/28/2025 02:14:16 INFO]: Training loss at epoch 27: 0.9854870438575745
[08/28/2025 02:14:26 INFO]: Training loss at epoch 23: 0.8704684674739838
[08/28/2025 02:14:27 INFO]: Training stats: {
    "score": -1.0421944663408935,
    "rmse": 1.0421944663408935
}
[08/28/2025 02:14:27 INFO]: Val stats: {
    "score": -0.6604861081797513,
    "rmse": 0.6604861081797513
}
[08/28/2025 02:14:27 INFO]: Test stats: {
    "score": -0.8891152478650927,
    "rmse": 0.8891152478650927
}
[08/28/2025 02:14:31 INFO]: Training loss at epoch 54: 1.1471917629241943
[08/28/2025 02:14:31 INFO]: Training stats: {
    "score": -1.1026532631727701,
    "rmse": 1.1026532631727701
}
[08/28/2025 02:14:31 INFO]: Val stats: {
    "score": -0.9037820622914575,
    "rmse": 0.9037820622914575
}
[08/28/2025 02:14:31 INFO]: Test stats: {
    "score": -1.042947606971563,
    "rmse": 1.042947606971563
}
[08/28/2025 02:14:36 INFO]: Training loss at epoch 20: 0.9162607192993164
[08/28/2025 02:14:57 INFO]: Training stats: {
    "score": -0.9977509045990628,
    "rmse": 0.9977509045990628
}
[08/28/2025 02:14:57 INFO]: Val stats: {
    "score": -0.6752696014514014,
    "rmse": 0.6752696014514014
}
[08/28/2025 02:14:57 INFO]: Test stats: {
    "score": -0.8704682486788657,
    "rmse": 0.8704682486788657
}
[08/28/2025 02:15:13 INFO]: Training loss at epoch 2: 0.7999755591154099
[08/28/2025 02:15:26 INFO]: Training loss at epoch 24: 1.432090938091278
[08/28/2025 02:15:31 INFO]: Training loss at epoch 10: 1.1107736825942993
[08/28/2025 02:16:08 INFO]: Training loss at epoch 5: 0.8601191341876984
[08/28/2025 02:16:12 INFO]: Training loss at epoch 25: 0.9420954287052155
[08/28/2025 02:16:25 INFO]: Training loss at epoch 25: 1.0263332724571228
[08/28/2025 02:16:32 INFO]: Training loss at epoch 11: 0.9103430509567261
[08/28/2025 02:16:39 INFO]: New best epoch, val score: -0.6522972678111685
[08/28/2025 02:16:39 INFO]: Saving model to: blotchy-Amado_trial_86/model_best.pth
[08/28/2025 02:16:50 INFO]: Training loss at epoch 17: 1.0893369913101196
[08/28/2025 02:16:59 INFO]: Training loss at epoch 44: 0.9475592076778412
[08/28/2025 02:16:59 INFO]: Training loss at epoch 24: 0.8652430474758148
[08/28/2025 02:17:08 INFO]: Training loss at epoch 17: 1.073241949081421
[08/28/2025 02:17:18 INFO]: New best epoch, val score: -0.6561962083899898
[08/28/2025 02:17:18 INFO]: Saving model to: blotchy-Amado_trial_80/model_best.pth
[08/28/2025 02:17:18 INFO]: Training loss at epoch 39: 1.1406771540641785
[08/28/2025 02:17:21 INFO]: Training loss at epoch 3: 0.908841997385025
[08/28/2025 02:17:25 INFO]: Training loss at epoch 26: 1.1897039413452148
[08/28/2025 02:17:32 INFO]: Training loss at epoch 55: 0.939375638961792
[08/28/2025 02:17:35 INFO]: Training loss at epoch 12: 1.0518582463264465
[08/28/2025 02:17:42 INFO]: New best epoch, val score: -0.6517189297874612
[08/28/2025 02:17:42 INFO]: Saving model to: blotchy-Amado_trial_86/model_best.pth
[08/28/2025 02:17:58 INFO]: Training loss at epoch 40: 0.7766030728816986
[08/28/2025 02:18:06 INFO]: Training loss at epoch 10: 1.0274342894554138
[08/28/2025 02:18:15 INFO]: Training loss at epoch 21: 1.0752509832382202
[08/28/2025 02:18:17 INFO]: Training stats: {
    "score": -0.9974349303793314,
    "rmse": 0.9974349303793314
}
[08/28/2025 02:18:17 INFO]: Val stats: {
    "score": -0.6851415580290321,
    "rmse": 0.6851415580290321
}
[08/28/2025 02:18:17 INFO]: Test stats: {
    "score": -0.8768576883877839,
    "rmse": 0.8768576883877839
}
[08/28/2025 02:18:19 INFO]: Running Final Evaluation...
[08/28/2025 02:18:28 INFO]: Training loss at epoch 27: 1.0756704211235046
[08/28/2025 02:18:40 INFO]: Training loss at epoch 13: 1.2759951949119568
[08/28/2025 02:18:40 INFO]: Training loss at epoch 27: 0.7855412662029266
[08/28/2025 02:18:46 INFO]: Training loss at epoch 18: 1.2592000365257263
[08/28/2025 02:19:05 INFO]: Training loss at epoch 66: 1.0136843919754028
[08/28/2025 02:19:11 INFO]: Training loss at epoch 4: 0.9352397918701172
[08/28/2025 02:19:20 INFO]: Training accuracy: {
    "score": -1.015245657993232,
    "rmse": 1.015245657993232
}
[08/28/2025 02:19:20 INFO]: Val accuracy: {
    "score": -0.6601546477406647,
    "rmse": 0.6601546477406647
}
[08/28/2025 02:19:20 INFO]: Test accuracy: {
    "score": -0.8699706157446249,
    "rmse": 0.8699706157446249
}
[08/28/2025 02:19:20 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_70",
    "best_epoch": 9,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8699706157446249,
        "rmse": 0.8699706157446249
    },
    "train_stats": {
        "score": -1.015245657993232,
        "rmse": 1.015245657993232
    },
    "val_stats": {
        "score": -0.6601546477406647,
        "rmse": 0.6601546477406647
    }
}
[08/28/2025 02:19:20 INFO]: Procewss finished for trial blotchy-Amado_trial_70
[08/28/2025 02:19:20 INFO]: 
_________________________________________________

[08/28/2025 02:19:20 INFO]: train_net_for_optune.py main() running.
[08/28/2025 02:19:20 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 16
  d_ffn_factor: 2.4815989330825845
  attention_dropout: 0.1262417672671307
  ffn_dropout: 0.1262417672671307
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.2318768683193014e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_88

[08/28/2025 02:19:20 INFO]: This ft_transformer has 1.592 million parameters.
[08/28/2025 02:19:20 INFO]: Training will start at epoch 0.
[08/28/2025 02:19:20 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 02:19:27 INFO]: Training loss at epoch 28: 0.9190107583999634
[08/28/2025 02:19:28 INFO]: Training loss at epoch 4: 1.2255403995513916
[08/28/2025 02:19:30 INFO]: Training loss at epoch 28: 1.1018016934394836
[08/28/2025 02:19:41 INFO]: Training loss at epoch 14: 1.2008822560310364
[08/28/2025 02:19:43 INFO]: Training loss at epoch 6: 0.8783943355083466
[08/28/2025 02:19:47 INFO]: Training loss at epoch 45: 0.987591415643692
[08/28/2025 02:19:55 INFO]: New best epoch, val score: -0.6810827884532249
[08/28/2025 02:19:55 INFO]: Saving model to: blotchy-Amado_trial_83/model_best.pth
[08/28/2025 02:20:02 INFO]: Training loss at epoch 0: 0.9088870882987976
[08/28/2025 02:20:08 INFO]: New best epoch, val score: -0.6597289797568969
[08/28/2025 02:20:08 INFO]: Saving model to: blotchy-Amado_trial_88/model_best.pth
[08/28/2025 02:20:12 INFO]: Running Final Evaluation...
[08/28/2025 02:20:19 INFO]: Training loss at epoch 55: 0.7470227181911469
[08/28/2025 02:20:19 INFO]: Training loss at epoch 18: 0.9110118448734283
[08/28/2025 02:20:26 INFO]: Training loss at epoch 29: 0.8807461261749268
[08/28/2025 02:20:32 INFO]: Training loss at epoch 56: 1.1029747128486633
[08/28/2025 02:20:36 INFO]: Training loss at epoch 25: 0.9830952286720276
[08/28/2025 02:20:42 INFO]: Training loss at epoch 18: 1.050833821296692
[08/28/2025 02:20:43 INFO]: Training loss at epoch 15: 0.95133838057518
[08/28/2025 02:20:45 INFO]: Training stats: {
    "score": -0.9967174648669664,
    "rmse": 0.9967174648669664
}
[08/28/2025 02:20:45 INFO]: Val stats: {
    "score": -0.6727245978386058,
    "rmse": 0.6727245978386058
}
[08/28/2025 02:20:45 INFO]: Test stats: {
    "score": -0.8721179682713904,
    "rmse": 0.8721179682713904
}
[08/28/2025 02:20:47 INFO]: Training loss at epoch 1: 1.0598604679107666
[08/28/2025 02:20:52 INFO]: New best epoch, val score: -0.6727245978386058
[08/28/2025 02:20:52 INFO]: Saving model to: blotchy-Amado_trial_84/model_best.pth
[08/28/2025 02:21:10 INFO]: Training loss at epoch 26: 1.0501863956451416
[08/28/2025 02:21:16 INFO]: Training loss at epoch 40: 0.991687685251236
[08/28/2025 02:21:33 INFO]: Training loss at epoch 2: 1.1133756041526794
[08/28/2025 02:21:35 INFO]: Training loss at epoch 5: 1.019371747970581
[08/28/2025 02:21:37 INFO]: Training loss at epoch 11: 1.2192716002464294
[08/28/2025 02:21:45 INFO]: Training loss at epoch 30: 0.854879230260849
[08/28/2025 02:21:46 INFO]: Training loss at epoch 16: 1.2480748295783997
[08/28/2025 02:21:52 INFO]: Training loss at epoch 22: 1.101078987121582
[08/28/2025 02:21:53 INFO]: New best epoch, val score: -0.6697123353755886
[08/28/2025 02:21:53 INFO]: Saving model to: blotchy-Amado_trial_84/model_best.pth
[08/28/2025 02:22:20 INFO]: Training loss at epoch 3: 1.059818595647812
[08/28/2025 02:22:36 INFO]: Training loss at epoch 46: 1.0564576387405396
[08/28/2025 02:22:46 INFO]: Training loss at epoch 31: 1.086211621761322
[08/28/2025 02:22:48 INFO]: Training loss at epoch 17: 0.8098923861980438
[08/28/2025 02:22:54 INFO]: New best epoch, val score: -0.6678443629494942
[08/28/2025 02:22:54 INFO]: Saving model to: blotchy-Amado_trial_84/model_best.pth
[08/28/2025 02:23:07 INFO]: Training loss at epoch 4: 1.1043308973312378
[08/28/2025 02:23:18 INFO]: Training loss at epoch 7: 0.9318364858627319
[08/28/2025 02:23:26 INFO]: Training accuracy: {
    "score": -1.0108034101647922,
    "rmse": 1.0108034101647922
}
[08/28/2025 02:23:26 INFO]: Val accuracy: {
    "score": -0.6629990508619463,
    "rmse": 0.6629990508619463
}
[08/28/2025 02:23:26 INFO]: Test accuracy: {
    "score": -0.8735801132772518,
    "rmse": 0.8735801132772518
}
[08/28/2025 02:23:26 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_30",
    "best_epoch": 35,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8735801132772518,
        "rmse": 0.8735801132772518
    },
    "train_stats": {
        "score": -1.0108034101647922,
        "rmse": 1.0108034101647922
    },
    "val_stats": {
        "score": -0.6629990508619463,
        "rmse": 0.6629990508619463
    }
}
[08/28/2025 02:23:26 INFO]: Procewss finished for trial blotchy-Amado_trial_30
[08/28/2025 02:23:27 INFO]: 
_________________________________________________

[08/28/2025 02:23:27 INFO]: train_net_for_optune.py main() running.
[08/28/2025 02:23:27 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 16
  d_ffn_factor: 2.4795001712191893
  attention_dropout: 0.3543425094206437
  ffn_dropout: 0.3543425094206437
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 9.623794345526393e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_89

[08/28/2025 02:23:27 INFO]: This ft_transformer has 1.590 million parameters.
[08/28/2025 02:23:27 INFO]: Training will start at epoch 0.
[08/28/2025 02:23:27 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 02:23:31 INFO]: Training loss at epoch 57: 1.0494777858257294
[08/28/2025 02:23:44 INFO]: Training loss at epoch 6: 0.9299188852310181
[08/28/2025 02:23:44 INFO]: Training loss at epoch 19: 0.9754239916801453
[08/28/2025 02:23:46 INFO]: Training loss at epoch 19: 1.0902737379074097
[08/28/2025 02:23:47 INFO]: Training loss at epoch 32: 0.966698557138443
[08/28/2025 02:23:47 INFO]: Training loss at epoch 28: 1.1228227019309998
[08/28/2025 02:23:52 INFO]: Training loss at epoch 5: 1.1922267973423004
[08/28/2025 02:23:52 INFO]: Training loss at epoch 18: 0.9957819283008575
[08/28/2025 02:23:54 INFO]: New best epoch, val score: -0.6677152051329315
[08/28/2025 02:23:54 INFO]: Saving model to: blotchy-Amado_trial_84/model_best.pth
[08/28/2025 02:24:05 INFO]: Training loss at epoch 0: 0.9602681994438171
[08/28/2025 02:24:10 INFO]: New best epoch, val score: -0.7648591932772886
[08/28/2025 02:24:10 INFO]: Saving model to: blotchy-Amado_trial_89/model_best.pth
[08/28/2025 02:24:12 INFO]: Training loss at epoch 26: 0.7856179922819138
[08/28/2025 02:24:13 INFO]: Training loss at epoch 19: 1.0389507412910461
[08/28/2025 02:24:15 INFO]: Training loss at epoch 41: 1.003825306892395
[08/28/2025 02:24:37 INFO]: Training loss at epoch 6: 0.9632498025894165
[08/28/2025 02:24:39 INFO]: Training loss at epoch 29: 1.2006608247756958
[08/28/2025 02:24:46 INFO]: Training loss at epoch 33: 0.9870659708976746
[08/28/2025 02:24:51 INFO]: Training loss at epoch 1: 1.085216224193573
[08/28/2025 02:24:55 INFO]: Training loss at epoch 19: 0.8747543394565582
[08/28/2025 02:24:56 INFO]: New best epoch, val score: -0.6752381720698746
[08/28/2025 02:24:56 INFO]: Saving model to: blotchy-Amado_trial_89/model_best.pth
[08/28/2025 02:25:00 INFO]: Training stats: {
    "score": -0.9971790395892737,
    "rmse": 0.9971790395892737
}
[08/28/2025 02:25:00 INFO]: Val stats: {
    "score": -0.6579893672581936,
    "rmse": 0.6579893672581936
}
[08/28/2025 02:25:00 INFO]: Test stats: {
    "score": -0.877477322362983,
    "rmse": 0.877477322362983
}
[08/28/2025 02:25:10 INFO]: Training loss at epoch 12: 1.219559669494629
[08/28/2025 02:25:11 INFO]: Training loss at epoch 5: 0.868338018655777
[08/28/2025 02:25:18 INFO]: Training stats: {
    "score": -1.0020021634372247,
    "rmse": 1.0020021634372247
}
[08/28/2025 02:25:18 INFO]: Val stats: {
    "score": -0.679208774377688,
    "rmse": 0.679208774377688
}
[08/28/2025 02:25:18 INFO]: Test stats: {
    "score": -0.8779271490458413,
    "rmse": 0.8779271490458413
}
[08/28/2025 02:25:22 INFO]: Training loss at epoch 47: 1.074376106262207
[08/28/2025 02:25:23 INFO]: Training stats: {
    "score": -1.0453518030420939,
    "rmse": 1.0453518030420939
}
[08/28/2025 02:25:23 INFO]: Val stats: {
    "score": -0.8177690244271825,
    "rmse": 0.8177690244271825
}
[08/28/2025 02:25:23 INFO]: Test stats: {
    "score": -0.9610701925798323,
    "rmse": 0.9610701925798323
}
[08/28/2025 02:25:24 INFO]: Training loss at epoch 7: 1.0186514258384705
[08/28/2025 02:25:27 INFO]: Training loss at epoch 23: 1.0604819059371948
[08/28/2025 02:25:29 INFO]: Training stats: {
    "score": -1.0022711111025662,
    "rmse": 1.0022711111025662
}
[08/28/2025 02:25:29 INFO]: Val stats: {
    "score": -0.6757764601157514,
    "rmse": 0.6757764601157514
}
[08/28/2025 02:25:29 INFO]: Test stats: {
    "score": -0.8699787107923078,
    "rmse": 0.8699787107923078
}
[08/28/2025 02:25:36 INFO]: Training loss at epoch 2: 1.3027872443199158
[08/28/2025 02:25:41 INFO]: New best epoch, val score: -0.6696177142572723
[08/28/2025 02:25:41 INFO]: Saving model to: blotchy-Amado_trial_89/model_best.pth
[08/28/2025 02:25:44 INFO]: Training loss at epoch 34: 1.156931310892105
[08/28/2025 02:25:50 INFO]: Training loss at epoch 7: 1.152059257030487
[08/28/2025 02:25:53 INFO]: New best epoch, val score: -0.6791512433941883
[08/28/2025 02:25:54 INFO]: Saving model to: blotchy-Amado_trial_83/model_best.pth
[08/28/2025 02:26:07 INFO]: Training loss at epoch 27: 0.8301035165786743
[08/28/2025 02:26:09 INFO]: Training loss at epoch 8: 1.11160409450531
[08/28/2025 02:26:21 INFO]: Training stats: {
    "score": -0.998645543249512,
    "rmse": 0.998645543249512
}
[08/28/2025 02:26:21 INFO]: Val stats: {
    "score": -0.6739052432709021,
    "rmse": 0.6739052432709021
}
[08/28/2025 02:26:21 INFO]: Test stats: {
    "score": -0.8727733759944261,
    "rmse": 0.8727733759944261
}
[08/28/2025 02:26:21 INFO]: Training loss at epoch 20: 1.0185232162475586
[08/28/2025 02:26:22 INFO]: Training loss at epoch 3: 1.1901291012763977
[08/28/2025 02:26:27 INFO]: Training loss at epoch 58: 1.175093173980713
[08/28/2025 02:26:45 INFO]: Training loss at epoch 35: 1.252482682466507
[08/28/2025 02:26:48 INFO]: New best epoch, val score: -0.6617445856498102
[08/28/2025 02:26:48 INFO]: Saving model to: blotchy-Amado_trial_63/model_best.pth
[08/28/2025 02:26:50 INFO]: Training loss at epoch 8: 1.0150120854377747
[08/28/2025 02:26:56 INFO]: Training loss at epoch 9: 1.1233040690422058
[08/28/2025 02:27:09 INFO]: Training loss at epoch 4: 0.8275308012962341
[08/28/2025 02:27:13 INFO]: Training stats: {
    "score": -0.9977341824686791,
    "rmse": 0.9977341824686791
}
[08/28/2025 02:27:13 INFO]: Val stats: {
    "score": -0.6873365852321622,
    "rmse": 0.6873365852321622
}
[08/28/2025 02:27:13 INFO]: Test stats: {
    "score": -0.8830516350472803,
    "rmse": 0.8830516350472803
}
[08/28/2025 02:27:16 INFO]: Training loss at epoch 42: 1.0923390984535217
[08/28/2025 02:27:26 INFO]: Training loss at epoch 21: 1.1015328168869019
[08/28/2025 02:27:46 INFO]: Training loss at epoch 36: 0.9354590773582458
[08/28/2025 02:27:50 INFO]: Training loss at epoch 27: 0.9722282588481903
[08/28/2025 02:27:56 INFO]: Training loss at epoch 5: 1.1830198764801025
[08/28/2025 02:28:00 INFO]: Training loss at epoch 10: 0.8989070057868958
[08/28/2025 02:28:02 INFO]: Training loss at epoch 8: 0.8448736667633057
[08/28/2025 02:28:11 INFO]: Training loss at epoch 48: 1.0586768984794617
[08/28/2025 02:28:29 INFO]: Training loss at epoch 22: 0.9879871904850006
[08/28/2025 02:28:31 INFO]: Training loss at epoch 20: 1.0321064591407776
[08/28/2025 02:28:42 INFO]: Training loss at epoch 6: 1.0150263011455536
[08/28/2025 02:28:44 INFO]: Training loss at epoch 13: 1.0844581723213196
[08/28/2025 02:28:45 INFO]: Training loss at epoch 37: 0.9665781557559967
[08/28/2025 02:28:46 INFO]: Training loss at epoch 11: 0.9516535103321075
[08/28/2025 02:28:58 INFO]: Training loss at epoch 29: 1.1398475766181946
[08/28/2025 02:29:00 INFO]: Training loss at epoch 20: 0.9218391180038452
[08/28/2025 02:29:05 INFO]: Training loss at epoch 24: 1.0518738627433777
[08/28/2025 02:29:25 INFO]: Training loss at epoch 7: 1.199441134929657
[08/28/2025 02:29:26 INFO]: Training loss at epoch 59: 0.8939450979232788
[08/28/2025 02:29:29 INFO]: Training loss at epoch 23: 1.129162609577179
[08/28/2025 02:29:30 INFO]: Training loss at epoch 12: 0.9958876371383667
[08/28/2025 02:29:41 INFO]: Training loss at epoch 38: 1.0617996454238892
[08/28/2025 02:30:06 INFO]: Training loss at epoch 9: 0.9658677279949188
[08/28/2025 02:30:08 INFO]: Training loss at epoch 8: 1.09449964761734
[08/28/2025 02:30:11 INFO]: Training loss at epoch 43: 0.9313020706176758
[08/28/2025 02:30:14 INFO]: Training loss at epoch 13: 0.9865788221359253
[08/28/2025 02:30:20 INFO]: Training loss at epoch 20: 1.0866960883140564
[08/28/2025 02:30:22 INFO]: Training stats: {
    "score": -1.0118183996066599,
    "rmse": 1.0118183996066599
}
[08/28/2025 02:30:22 INFO]: Val stats: {
    "score": -0.6617192954612199,
    "rmse": 0.6617192954612199
}
[08/28/2025 02:30:22 INFO]: Test stats: {
    "score": -0.8726513589140243,
    "rmse": 0.8726513589140243
}
[08/28/2025 02:30:22 INFO]: Training loss at epoch 9: 1.09430730342865
[08/28/2025 02:30:30 INFO]: Training loss at epoch 24: 1.0214442610740662
[08/28/2025 02:30:38 INFO]: Training stats: {
    "score": -1.0014800089993923,
    "rmse": 1.0014800089993923
}
[08/28/2025 02:30:38 INFO]: Val stats: {
    "score": -0.703522553318741,
    "rmse": 0.703522553318741
}
[08/28/2025 02:30:38 INFO]: Test stats: {
    "score": -0.8842200006572003,
    "rmse": 0.8842200006572003
}
[08/28/2025 02:30:40 INFO]: Training loss at epoch 39: 0.9605457186698914
[08/28/2025 02:30:43 INFO]: New best epoch, val score: -0.6617192954612199
[08/28/2025 02:30:43 INFO]: Saving model to: blotchy-Amado_trial_63/model_best.pth
[08/28/2025 02:30:50 INFO]: Training stats: {
    "score": -0.9955158013141676,
    "rmse": 0.9955158013141676
}
[08/28/2025 02:30:50 INFO]: Val stats: {
    "score": -0.7016929687472983,
    "rmse": 0.7016929687472983
}
[08/28/2025 02:30:50 INFO]: Test stats: {
    "score": -0.8868827445792562,
    "rmse": 0.8868827445792562
}
[08/28/2025 02:30:55 INFO]: Training loss at epoch 9: 1.051807940006256
[08/28/2025 02:30:56 INFO]: Training loss at epoch 49: 0.9645829498767853
[08/28/2025 02:31:01 INFO]: Training stats: {
    "score": -0.9939433340998614,
    "rmse": 0.9939433340998614
}
[08/28/2025 02:31:01 INFO]: Val stats: {
    "score": -0.6902437482156495,
    "rmse": 0.6902437482156495
}
[08/28/2025 02:31:01 INFO]: Test stats: {
    "score": -0.8804415671233574,
    "rmse": 0.8804415671233574
}
[08/28/2025 02:31:02 INFO]: Training loss at epoch 14: 1.0002168715000153
[08/28/2025 02:31:05 INFO]: Training loss at epoch 28: 1.2443285882472992
[08/28/2025 02:31:11 INFO]: Training stats: {
    "score": -1.064010213721596,
    "rmse": 1.064010213721596
}
[08/28/2025 02:31:11 INFO]: Val stats: {
    "score": -0.8619515580801028,
    "rmse": 0.8619515580801028
}
[08/28/2025 02:31:11 INFO]: Test stats: {
    "score": -1.0027006564372465,
    "rmse": 1.0027006564372465
}
[08/28/2025 02:31:11 INFO]: Training loss at epoch 6: 0.9510530829429626
[08/28/2025 02:31:24 INFO]: Training loss at epoch 28: 1.2663748264312744
[08/28/2025 02:31:31 INFO]: Training loss at epoch 56: 0.770451158285141
[08/28/2025 02:31:31 INFO]: Training loss at epoch 30: 1.0155676007270813
[08/28/2025 02:31:34 INFO]: Training loss at epoch 25: 1.3960902094841003
[08/28/2025 02:31:42 INFO]: Training stats: {
    "score": -1.001517270726715,
    "rmse": 1.001517270726715
}
[08/28/2025 02:31:42 INFO]: Val stats: {
    "score": -0.6849121505861941,
    "rmse": 0.6849121505861941
}
[08/28/2025 02:31:42 INFO]: Test stats: {
    "score": -0.8820311980433565,
    "rmse": 0.8820311980433565
}
[08/28/2025 02:31:49 INFO]: Training loss at epoch 15: 1.0895318984985352
[08/28/2025 02:31:53 INFO]: Training stats: {
    "score": -0.9973293583352154,
    "rmse": 0.9973293583352154
}
[08/28/2025 02:31:53 INFO]: Val stats: {
    "score": -0.6766650991425608,
    "rmse": 0.6766650991425608
}
[08/28/2025 02:31:53 INFO]: Test stats: {
    "score": -0.8718700668098264,
    "rmse": 0.8718700668098264
}
[08/28/2025 02:31:55 INFO]: New best epoch, val score: -0.6776859504160515
[08/28/2025 02:31:55 INFO]: Saving model to: blotchy-Amado_trial_83/model_best.pth
[08/28/2025 02:31:55 INFO]: Training loss at epoch 21: 0.9200917482376099
[08/28/2025 02:31:57 INFO]: Training loss at epoch 10: 1.0788164734840393
[08/28/2025 02:32:00 INFO]: Training loss at epoch 40: 0.9698531925678253
[08/28/2025 02:32:14 INFO]: Training loss at epoch 14: 1.3661587834358215
[08/28/2025 02:32:27 INFO]: Training loss at epoch 21: 0.9494705200195312
[08/28/2025 02:32:35 INFO]: Training loss at epoch 16: 0.9804244637489319
[08/28/2025 02:32:36 INFO]: Training loss at epoch 26: 0.9664489924907684
[08/28/2025 02:32:39 INFO]: Training loss at epoch 25: 1.1980869770050049
[08/28/2025 02:32:43 INFO]: Training loss at epoch 11: 0.842719554901123
[08/28/2025 02:32:49 INFO]: New best epoch, val score: -0.6683340569681447
[08/28/2025 02:32:49 INFO]: Saving model to: blotchy-Amado_trial_89/model_best.pth
[08/28/2025 02:33:01 INFO]: Training loss at epoch 10: 1.115982174873352
[08/28/2025 02:33:01 INFO]: Training loss at epoch 41: 1.0010760724544525
[08/28/2025 02:33:13 INFO]: Training loss at epoch 44: 1.2033767104148865
[08/28/2025 02:33:22 INFO]: Training loss at epoch 17: 0.9065430164337158
[08/28/2025 02:33:25 INFO]: Training loss at epoch 60: 1.3224686086177826
[08/28/2025 02:33:30 INFO]: Training loss at epoch 12: 0.9924705922603607
[08/28/2025 02:33:36 INFO]: New best epoch, val score: -0.6632469382964605
[08/28/2025 02:33:36 INFO]: Saving model to: blotchy-Amado_trial_89/model_best.pth
[08/28/2025 02:33:40 INFO]: Training loss at epoch 27: 0.9809085428714752
[08/28/2025 02:34:00 INFO]: Training loss at epoch 42: 0.8302767872810364
[08/28/2025 02:34:10 INFO]: Training loss at epoch 18: 0.8085620105266571
[08/28/2025 02:34:17 INFO]: Training loss at epoch 13: 1.03477942943573
[08/28/2025 02:34:41 INFO]: Training loss at epoch 28: 0.8641520142555237
[08/28/2025 02:34:42 INFO]: Training loss at epoch 50: 0.9491665959358215
[08/28/2025 02:34:54 INFO]: Training loss at epoch 19: 1.1161460876464844
[08/28/2025 02:34:58 INFO]: Training loss at epoch 43: 0.8735618889331818
[08/28/2025 02:35:00 INFO]: Training loss at epoch 14: 1.0390272736549377
[08/28/2025 02:35:00 INFO]: Training loss at epoch 29: 0.9759169220924377
[08/28/2025 02:35:07 INFO]: Training loss at epoch 11: 1.088141143321991
[08/28/2025 02:35:10 INFO]: Training stats: {
    "score": -0.9949194767276488,
    "rmse": 0.9949194767276488
}
[08/28/2025 02:35:10 INFO]: Val stats: {
    "score": -0.6857751561940592,
    "rmse": 0.6857751561940592
}
[08/28/2025 02:35:10 INFO]: Test stats: {
    "score": -0.8832195429602064,
    "rmse": 0.8832195429602064
}
[08/28/2025 02:35:17 INFO]: Training loss at epoch 10: 1.009313941001892
[08/28/2025 02:35:23 INFO]: Training loss at epoch 21: 1.0070293843746185
[08/28/2025 02:35:24 INFO]: Training loss at epoch 22: 1.170871615409851
[08/28/2025 02:35:43 INFO]: Training loss at epoch 29: 1.219188630580902
[08/28/2025 02:35:45 INFO]: Training loss at epoch 15: 0.8460962176322937
[08/28/2025 02:35:45 INFO]: Training loss at epoch 15: 1.013040840625763
[08/28/2025 02:35:47 INFO]: Training loss at epoch 30: 0.953685998916626
[08/28/2025 02:35:54 INFO]: Training loss at epoch 22: 0.910663515329361
[08/28/2025 02:35:55 INFO]: Training loss at epoch 20: 1.0167204141616821
[08/28/2025 02:35:56 INFO]: Training loss at epoch 44: 1.0799568891525269
[08/28/2025 02:36:02 INFO]: Training stats: {
    "score": -1.000267377998529,
    "rmse": 1.000267377998529
}
[08/28/2025 02:36:02 INFO]: Val stats: {
    "score": -0.6769834031381009,
    "rmse": 0.6769834031381009
}
[08/28/2025 02:36:02 INFO]: Test stats: {
    "score": -0.8768791798235522,
    "rmse": 0.8768791798235522
}
[08/28/2025 02:36:04 INFO]: Training loss at epoch 29: 1.0986378192901611
[08/28/2025 02:36:11 INFO]: Training stats: {
    "score": -1.0111024202888415,
    "rmse": 1.0111024202888415
}
[08/28/2025 02:36:11 INFO]: Val stats: {
    "score": -0.7747925965932269,
    "rmse": 0.7747925965932269
}
[08/28/2025 02:36:11 INFO]: Test stats: {
    "score": -0.9458551775158085,
    "rmse": 0.9458551775158085
}
[08/28/2025 02:36:12 INFO]: New best epoch, val score: -0.6886954584569176
[08/28/2025 02:36:12 INFO]: Training loss at epoch 45: 1.0593262016773224
[08/28/2025 02:36:12 INFO]: Saving model to: blotchy-Amado_trial_82/model_best.pth
[08/28/2025 02:36:16 INFO]: Training loss at epoch 26: 1.1107358932495117
[08/28/2025 02:36:26 INFO]: Training loss at epoch 61: 1.0772311091423035
[08/28/2025 02:36:31 INFO]: Training loss at epoch 16: 0.9953159391880035
[08/28/2025 02:36:33 INFO]: New best epoch, val score: -0.6599611897026566
[08/28/2025 02:36:33 INFO]: Saving model to: blotchy-Amado_trial_71/model_best.pth
[08/28/2025 02:36:37 INFO]: New best epoch, val score: -0.6626254135361369
[08/28/2025 02:36:37 INFO]: Saving model to: blotchy-Amado_trial_89/model_best.pth
[08/28/2025 02:36:42 INFO]: New best epoch, val score: -0.6737457093202743
[08/28/2025 02:36:42 INFO]: Saving model to: blotchy-Amado_trial_75/model_best.pth
[08/28/2025 02:36:42 INFO]: Training loss at epoch 31: 1.1357544660568237
[08/28/2025 02:36:43 INFO]: Training loss at epoch 21: 0.9200110137462616
[08/28/2025 02:36:57 INFO]: Training loss at epoch 45: 0.9556286931037903
[08/28/2025 02:37:06 INFO]: Training loss at epoch 30: 0.8589089214801788
[08/28/2025 02:37:11 INFO]: Training loss at epoch 7: 1.0104185342788696
[08/28/2025 02:37:16 INFO]: Training loss at epoch 12: 1.1668003797531128
[08/28/2025 02:37:17 INFO]: Training loss at epoch 17: 1.0032735466957092
[08/28/2025 02:37:22 INFO]: New best epoch, val score: -0.6617656918883532
[08/28/2025 02:37:22 INFO]: Saving model to: blotchy-Amado_trial_89/model_best.pth
[08/28/2025 02:37:28 INFO]: Training loss at epoch 51: 0.9684678912162781
[08/28/2025 02:37:28 INFO]: Training loss at epoch 22: 0.9125843644142151
[08/28/2025 02:37:42 INFO]: Training stats: {
    "score": -1.0118321245409032,
    "rmse": 1.0118321245409032
}
[08/28/2025 02:37:42 INFO]: Val stats: {
    "score": -0.6608143647042777,
    "rmse": 0.6608143647042777
}
[08/28/2025 02:37:42 INFO]: Test stats: {
    "score": -0.8709596827595562,
    "rmse": 0.8709596827595562
}
[08/28/2025 02:37:53 INFO]: New best epoch, val score: -0.6767717275434354
[08/28/2025 02:37:53 INFO]: Saving model to: blotchy-Amado_trial_83/model_best.pth
[08/28/2025 02:37:55 INFO]: Training loss at epoch 46: 1.0647766590118408
[08/28/2025 02:38:03 INFO]: Training loss at epoch 18: 0.9810315072536469
[08/28/2025 02:38:09 INFO]: Training loss at epoch 31: 1.0375486612319946
[08/28/2025 02:38:15 INFO]: Training loss at epoch 23: 0.9246776401996613
[08/28/2025 02:38:49 INFO]: Training loss at epoch 19: 1.2347863614559174
[08/28/2025 02:38:49 INFO]: Training loss at epoch 11: 1.1092565059661865
[08/28/2025 02:38:51 INFO]: Training loss at epoch 23: 0.9543889462947845
[08/28/2025 02:38:57 INFO]: Training loss at epoch 47: 1.0428670644760132
[08/28/2025 02:39:03 INFO]: Training loss at epoch 24: 0.9935781955718994
[08/28/2025 02:39:06 INFO]: Training stats: {
    "score": -0.9888730355340144,
    "rmse": 0.9888730355340144
}
[08/28/2025 02:39:06 INFO]: Val stats: {
    "score": -0.6886690048503237,
    "rmse": 0.6886690048503237
}
[08/28/2025 02:39:06 INFO]: Test stats: {
    "score": -0.8833054974803638,
    "rmse": 0.8833054974803638
}
[08/28/2025 02:39:13 INFO]: Training loss at epoch 32: 1.0355299711227417
[08/28/2025 02:39:13 INFO]: Training loss at epoch 46: 0.8799213171005249
[08/28/2025 02:39:17 INFO]: New best epoch, val score: -0.6689284740903351
[08/28/2025 02:39:17 INFO]: Saving model to: blotchy-Amado_trial_85/model_best.pth
[08/28/2025 02:39:18 INFO]: Training loss at epoch 16: 1.038753867149353
[08/28/2025 02:39:25 INFO]: Training loss at epoch 13: 0.9236409664154053
[08/28/2025 02:39:25 INFO]: Training loss at epoch 23: 1.1517381072044373
[08/28/2025 02:39:26 INFO]: Training loss at epoch 62: 0.9783181250095367
[08/28/2025 02:39:39 INFO]: New best epoch, val score: -0.6717729312834745
[08/28/2025 02:39:39 INFO]: Saving model to: blotchy-Amado_trial_87/model_best.pth
[08/28/2025 02:39:44 INFO]: New best epoch, val score: -0.6848039549954774
[08/28/2025 02:39:44 INFO]: Saving model to: blotchy-Amado_trial_82/model_best.pth
[08/28/2025 02:39:48 INFO]: Training loss at epoch 30: 1.013793706893921
[08/28/2025 02:39:48 INFO]: Training loss at epoch 25: 1.0840128064155579
[08/28/2025 02:39:49 INFO]: Training loss at epoch 20: 0.8950276076793671
[08/28/2025 02:39:52 INFO]: Training loss at epoch 27: 0.8533179759979248
[08/28/2025 02:39:54 INFO]: Training loss at epoch 48: 0.8086635768413544
[08/28/2025 02:40:15 INFO]: Training loss at epoch 33: 0.8687889575958252
[08/28/2025 02:40:17 INFO]: Training loss at epoch 52: 1.0788041353225708
[08/28/2025 02:40:21 INFO]: Training loss at epoch 22: 0.848063588142395
[08/28/2025 02:40:35 INFO]: Training loss at epoch 26: 0.8705251514911652
[08/28/2025 02:40:36 INFO]: Training loss at epoch 21: 0.9788627326488495
[08/28/2025 02:40:55 INFO]: Training loss at epoch 49: 1.0632805824279785
[08/28/2025 02:40:58 INFO]: Training loss at epoch 31: 0.994504302740097
[08/28/2025 02:41:16 INFO]: Training stats: {
    "score": -0.9916939259753261,
    "rmse": 0.9916939259753261
}
[08/28/2025 02:41:16 INFO]: Val stats: {
    "score": -0.6791474822209574,
    "rmse": 0.6791474822209574
}
[08/28/2025 02:41:16 INFO]: Test stats: {
    "score": -0.8752156752724867,
    "rmse": 0.8752156752724867
}
[08/28/2025 02:41:19 INFO]: Training loss at epoch 34: 1.1106452941894531
[08/28/2025 02:41:22 INFO]: Training loss at epoch 27: 0.9634920656681061
[08/28/2025 02:41:23 INFO]: Training loss at epoch 22: 1.181300938129425
[08/28/2025 02:41:33 INFO]: Training loss at epoch 14: 1.0285491347312927
[08/28/2025 02:41:35 INFO]: Running Final Evaluation...
[08/28/2025 02:41:54 INFO]: Training loss at epoch 32: 0.9543536305427551
[08/28/2025 02:42:09 INFO]: Training loss at epoch 23: 0.9548488259315491
[08/28/2025 02:42:09 INFO]: Training loss at epoch 28: 1.0121609568595886
[08/28/2025 02:42:15 INFO]: Training loss at epoch 50: 1.0412236452102661
[08/28/2025 02:42:15 INFO]: Training loss at epoch 47: 1.2353230714797974
[08/28/2025 02:42:21 INFO]: Training loss at epoch 35: 1.1107515692710876
[08/28/2025 02:42:25 INFO]: Training loss at epoch 24: 1.0996006727218628
[08/28/2025 02:42:26 INFO]: Training loss at epoch 12: 1.0054436922073364
[08/28/2025 02:42:27 INFO]: Training loss at epoch 63: 0.8969503343105316
[08/28/2025 02:42:45 INFO]: Training loss at epoch 30: 1.0821783542633057
[08/28/2025 02:42:51 INFO]: Training loss at epoch 17: 1.0302384495735168
[08/28/2025 02:42:53 INFO]: Training loss at epoch 57: 0.9354643821716309
[08/28/2025 02:42:53 INFO]: New best epoch, val score: -0.6658147791743776
[08/28/2025 02:42:53 INFO]: Saving model to: blotchy-Amado_trial_85/model_best.pth
[08/28/2025 02:42:55 INFO]: Training loss at epoch 24: 0.9657191038131714
[08/28/2025 02:42:55 INFO]: Training loss at epoch 29: 1.290554165840149
[08/28/2025 02:42:55 INFO]: Training loss at epoch 24: 0.8262232542037964
[08/28/2025 02:43:06 INFO]: Training loss at epoch 53: 1.145872950553894
[08/28/2025 02:43:10 INFO]: Training stats: {
    "score": -0.9907217577541988,
    "rmse": 0.9907217577541988
}
[08/28/2025 02:43:10 INFO]: Val stats: {
    "score": -0.6739549019973268,
    "rmse": 0.6739549019973268
}
[08/28/2025 02:43:10 INFO]: Test stats: {
    "score": -0.8784869402243095,
    "rmse": 0.8784869402243095
}
[08/28/2025 02:43:13 INFO]: Training loss at epoch 51: 1.1393187642097473
[08/28/2025 02:43:13 INFO]: Training loss at epoch 8: 1.1164411902427673
[08/28/2025 02:43:20 INFO]: New best epoch, val score: -0.6630566193145225
[08/28/2025 02:43:20 INFO]: Saving model to: blotchy-Amado_trial_81/model_best.pth
[08/28/2025 02:43:25 INFO]: Training loss at epoch 36: 0.9391669034957886
[08/28/2025 02:43:26 INFO]: Training loss at epoch 31: 0.8863645195960999
[08/28/2025 02:43:27 INFO]: Training accuracy: {
    "score": -1.0133853540597997,
    "rmse": 1.0133853540597997
}
[08/28/2025 02:43:27 INFO]: Val accuracy: {
    "score": -0.6580510321291317,
    "rmse": 0.6580510321291317
}
[08/28/2025 02:43:27 INFO]: Test accuracy: {
    "score": -0.8644620550017998,
    "rmse": 0.8644620550017998
}
[08/28/2025 02:43:27 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_67",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8644620550017998,
        "rmse": 0.8644620550017998
    },
    "train_stats": {
        "score": -1.0133853540597997,
        "rmse": 1.0133853540597997
    },
    "val_stats": {
        "score": -0.6580510321291317,
        "rmse": 0.6580510321291317
    }
}
[08/28/2025 02:43:27 INFO]: Procewss finished for trial blotchy-Amado_trial_67
[08/28/2025 02:43:28 INFO]: 
_________________________________________________

[08/28/2025 02:43:28 INFO]: train_net_for_optune.py main() running.
[08/28/2025 02:43:28 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 16
  d_ffn_factor: 2.505385409474115
  attention_dropout: 0.13436169712502719
  ffn_dropout: 0.13436169712502719
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.1079011493959826e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_90

[08/28/2025 02:43:28 INFO]: This ft_transformer has 1.601 million parameters.
[08/28/2025 02:43:28 INFO]: Training will start at epoch 0.
[08/28/2025 02:43:28 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 02:43:31 INFO]: Training loss at epoch 28: 1.0445747375488281
[08/28/2025 02:43:39 INFO]: Training loss at epoch 25: 0.8606706857681274
[08/28/2025 02:43:40 INFO]: Training loss at epoch 15: 1.2260654270648956
[08/28/2025 02:43:57 INFO]: Training loss at epoch 30: 0.9332920014858246
[08/28/2025 02:43:58 INFO]: New best epoch, val score: -0.6766451839426563
[08/28/2025 02:43:58 INFO]: Saving model to: blotchy-Amado_trial_83/model_best.pth
[08/28/2025 02:44:10 INFO]: Training loss at epoch 0: 1.4750523567199707
[08/28/2025 02:44:14 INFO]: Training loss at epoch 52: 0.9915144145488739
[08/28/2025 02:44:16 INFO]: New best epoch, val score: -1.0175613708698477
[08/28/2025 02:44:16 INFO]: Saving model to: blotchy-Amado_trial_90/model_best.pth
[08/28/2025 02:44:27 INFO]: Training loss at epoch 26: 0.9397168755531311
[08/28/2025 02:44:31 INFO]: Training loss at epoch 37: 0.8103348016738892
[08/28/2025 02:44:43 INFO]: Training loss at epoch 31: 0.9667705595493317
[08/28/2025 02:44:49 INFO]: Running Final Evaluation...
[08/28/2025 02:44:56 INFO]: Training loss at epoch 1: 1.5327481031417847
[08/28/2025 02:45:01 INFO]: New best epoch, val score: -0.8079286054665805
[08/28/2025 02:45:01 INFO]: Saving model to: blotchy-Amado_trial_90/model_best.pth
[08/28/2025 02:45:04 INFO]: Training accuracy: {
    "score": -1.0210501001122263,
    "rmse": 1.0210501001122263
}
[08/28/2025 02:45:04 INFO]: Val accuracy: {
    "score": -0.6597289797568969,
    "rmse": 0.6597289797568969
}
[08/28/2025 02:45:04 INFO]: Test accuracy: {
    "score": -0.8773526402206216,
    "rmse": 0.8773526402206216
}
[08/28/2025 02:45:04 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_88",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8773526402206216,
        "rmse": 0.8773526402206216
    },
    "train_stats": {
        "score": -1.0210501001122263,
        "rmse": 1.0210501001122263
    },
    "val_stats": {
        "score": -0.6597289797568969,
        "rmse": 0.6597289797568969
    }
}
[08/28/2025 02:45:04 INFO]: Procewss finished for trial blotchy-Amado_trial_88
[08/28/2025 02:45:04 INFO]: 
_________________________________________________

[08/28/2025 02:45:04 INFO]: train_net_for_optune.py main() running.
[08/28/2025 02:45:04 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 16
  d_ffn_factor: 1.4031855623883935
  attention_dropout: 0.35695647900647287
  ffn_dropout: 0.35695647900647287
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.8733755870614988e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_91

[08/28/2025 02:45:04 INFO]: This ft_transformer has 21.745 million parameters.
[08/28/2025 02:45:04 INFO]: Training will start at epoch 0.
[08/28/2025 02:45:04 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 02:45:10 INFO]: Training loss at epoch 27: 1.4233366250991821
[08/28/2025 02:45:12 INFO]: Training loss at epoch 53: 1.132197916507721
[08/28/2025 02:45:15 INFO]: Training loss at epoch 48: 0.8350962102413177
[08/28/2025 02:45:20 INFO]: Training loss at epoch 23: 0.8780237138271332
[08/28/2025 02:45:22 INFO]: Training loss at epoch 64: 0.9063831567764282
[08/28/2025 02:45:33 INFO]: Training loss at epoch 38: 0.9506708979606628
[08/28/2025 02:45:43 INFO]: Training loss at epoch 2: 1.1322768926620483
[08/28/2025 02:45:44 INFO]: New best epoch, val score: -0.6613974847607322
[08/28/2025 02:45:44 INFO]: Saving model to: blotchy-Amado_trial_63/model_best.pth
[08/28/2025 02:45:48 INFO]: Training loss at epoch 16: 1.0377970337867737
[08/28/2025 02:45:48 INFO]: New best epoch, val score: -0.6657098883037977
[08/28/2025 02:45:48 INFO]: Saving model to: blotchy-Amado_trial_90/model_best.pth
[08/28/2025 02:45:52 INFO]: Training loss at epoch 25: 0.9919221997261047
[08/28/2025 02:45:54 INFO]: Training loss at epoch 54: 1.3538635075092316
[08/28/2025 02:45:56 INFO]: Training loss at epoch 28: 1.1197336614131927
[08/28/2025 02:46:04 INFO]: Training loss at epoch 13: 1.2778304815292358
[08/28/2025 02:46:11 INFO]: Training loss at epoch 54: 1.0494708120822906
[08/28/2025 02:46:20 INFO]: Training loss at epoch 18: 1.108497440814972
[08/28/2025 02:46:21 INFO]: Training loss at epoch 25: 0.8806548714637756
[08/28/2025 02:46:27 INFO]: Training loss at epoch 3: 1.1070684790611267
[08/28/2025 02:46:30 INFO]: New best epoch, val score: -0.6626138317295103
[08/28/2025 02:46:30 INFO]: Saving model to: blotchy-Amado_trial_85/model_best.pth
[08/28/2025 02:46:36 INFO]: Training loss at epoch 39: 1.1761642098426819
[08/28/2025 02:46:39 INFO]: Training loss at epoch 29: 1.2189823389053345
[08/28/2025 02:46:46 INFO]: New best epoch, val score: -0.6620512838106926
[08/28/2025 02:46:46 INFO]: Saving model to: blotchy-Amado_trial_81/model_best.pth
[08/28/2025 02:46:54 INFO]: Training stats: {
    "score": -0.9862995598480406,
    "rmse": 0.9862995598480406
}
[08/28/2025 02:46:54 INFO]: Val stats: {
    "score": -0.6717382757026197,
    "rmse": 0.6717382757026197
}
[08/28/2025 02:46:54 INFO]: Test stats: {
    "score": -0.8775141998479102,
    "rmse": 0.8775141998479102
}
[08/28/2025 02:46:57 INFO]: Training stats: {
    "score": -0.9993943492734059,
    "rmse": 0.9993943492734059
}
[08/28/2025 02:46:57 INFO]: Val stats: {
    "score": -0.6798709532988478,
    "rmse": 0.6798709532988478
}
[08/28/2025 02:46:57 INFO]: Test stats: {
    "score": -0.8785769759264851,
    "rmse": 0.8785769759264851
}
[08/28/2025 02:46:59 INFO]: Training loss at epoch 32: 1.2132784724235535
[08/28/2025 02:47:01 INFO]: Training loss at epoch 33: 0.8362409472465515
[08/28/2025 02:47:07 INFO]: Training loss at epoch 29: 0.9568284451961517
[08/28/2025 02:47:09 INFO]: Training loss at epoch 55: 1.2269341945648193
[08/28/2025 02:47:14 INFO]: Training loss at epoch 4: 1.1871213912963867
[08/28/2025 02:47:38 INFO]: Training loss at epoch 31: 0.8944412469863892
[08/28/2025 02:47:40 INFO]: Training loss at epoch 30: 0.800991952419281
[08/28/2025 02:47:52 INFO]: Training loss at epoch 17: 1.0474418997764587
[08/28/2025 02:48:00 INFO]: Training loss at epoch 5: 1.412604808807373
[08/28/2025 02:48:02 INFO]: Training loss at epoch 40: 1.0210745632648468
[08/28/2025 02:48:08 INFO]: Training loss at epoch 56: 0.9584547877311707
[08/28/2025 02:48:12 INFO]: Training loss at epoch 49: 1.0037284195423126
[08/28/2025 02:48:18 INFO]: Training stats: {
    "score": -1.0130217345296522,
    "rmse": 1.0130217345296522
}
[08/28/2025 02:48:18 INFO]: Val stats: {
    "score": -0.6762674475565287,
    "rmse": 0.6762674475565287
}
[08/28/2025 02:48:18 INFO]: Test stats: {
    "score": -0.8812719716564207,
    "rmse": 0.8812719716564207
}
[08/28/2025 02:48:18 INFO]: Training loss at epoch 65: 1.0135207772254944
[08/28/2025 02:48:25 INFO]: Training loss at epoch 31: 1.3765269815921783
[08/28/2025 02:48:39 INFO]: Training loss at epoch 55: 0.9134939908981323
[08/28/2025 02:48:48 INFO]: Training loss at epoch 6: 1.452555537223816
[08/28/2025 02:49:06 INFO]: Training loss at epoch 41: 0.8639380633831024
[08/28/2025 02:49:08 INFO]: Training loss at epoch 57: 1.020763337612152
[08/28/2025 02:49:13 INFO]: Training loss at epoch 32: 0.8445632755756378
[08/28/2025 02:49:13 INFO]: Training loss at epoch 9: 0.8797376751899719
[08/28/2025 02:49:13 INFO]: Training stats: {
    "score": -1.0013437991851213,
    "rmse": 1.0013437991851213
}
[08/28/2025 02:49:13 INFO]: Val stats: {
    "score": -0.7084913376940157,
    "rmse": 0.7084913376940157
}
[08/28/2025 02:49:13 INFO]: Test stats: {
    "score": -0.8897459917164751,
    "rmse": 0.8897459917164751
}
[08/28/2025 02:49:17 INFO]: Training loss at epoch 26: 1.1012665033340454
[08/28/2025 02:49:35 INFO]: Training loss at epoch 7: 1.0830954909324646
[08/28/2025 02:49:37 INFO]: Training loss at epoch 14: 0.9423068463802338
[08/28/2025 02:49:48 INFO]: Training loss at epoch 26: 1.3448405265808105
[08/28/2025 02:49:51 INFO]: Training loss at epoch 19: 1.1234996318817139
[08/28/2025 02:49:57 INFO]: Training loss at epoch 33: 0.7695086747407913
[08/28/2025 02:50:01 INFO]: Training loss at epoch 18: 1.1221522390842438
[08/28/2025 02:50:02 INFO]: New best epoch, val score: -0.6602733927316172
[08/28/2025 02:50:02 INFO]: Saving model to: blotchy-Amado_trial_85/model_best.pth
[08/28/2025 02:50:05 INFO]: Training loss at epoch 58: 0.8601750731468201
[08/28/2025 02:50:06 INFO]: Training loss at epoch 42: 0.9889937937259674
[08/28/2025 02:50:16 INFO]: Training loss at epoch 24: 0.8866392374038696
[08/28/2025 02:50:19 INFO]: Training loss at epoch 8: 1.2642474174499512
[08/28/2025 02:50:32 INFO]: Training loss at epoch 33: 1.2755736112594604
[08/28/2025 02:50:42 INFO]: Training loss at epoch 34: 1.0493888556957245
[08/28/2025 02:51:04 INFO]: Training loss at epoch 59: 1.0152376294136047
[08/28/2025 02:51:06 INFO]: Training loss at epoch 9: 1.407583236694336
[08/28/2025 02:51:06 INFO]: Training stats: {
    "score": -0.9989196810772979,
    "rmse": 0.9989196810772979
}
[08/28/2025 02:51:06 INFO]: Val stats: {
    "score": -0.6902671498979472,
    "rmse": 0.6902671498979472
}
[08/28/2025 02:51:06 INFO]: Test stats: {
    "score": -0.8871982349592343,
    "rmse": 0.8871982349592343
}
[08/28/2025 02:51:08 INFO]: Training loss at epoch 43: 1.1434597373008728
[08/28/2025 02:51:14 INFO]: Training stats: {
    "score": -0.9945110403979817,
    "rmse": 0.9945110403979817
}
[08/28/2025 02:51:14 INFO]: Val stats: {
    "score": -0.676631723462214,
    "rmse": 0.676631723462214
}
[08/28/2025 02:51:14 INFO]: Test stats: {
    "score": -0.8795926771950935,
    "rmse": 0.8795926771950935
}
[08/28/2025 02:51:16 INFO]: Running Final Evaluation...
[08/28/2025 02:51:16 INFO]: Training loss at epoch 66: 1.2189915776252747
[08/28/2025 02:51:24 INFO]: Training stats: {
    "score": -1.115272919722009,
    "rmse": 1.115272919722009
}
[08/28/2025 02:51:24 INFO]: Val stats: {
    "score": -0.7422881756785441,
    "rmse": 0.7422881756785441
}
[08/28/2025 02:51:24 INFO]: Test stats: {
    "score": -0.9516956204853257,
    "rmse": 0.9516956204853257
}
[08/28/2025 02:51:25 INFO]: Training stats: {
    "score": -0.9888834752944307,
    "rmse": 0.9888834752944307
}
[08/28/2025 02:51:25 INFO]: Val stats: {
    "score": -0.682148811818221,
    "rmse": 0.682148811818221
}
[08/28/2025 02:51:25 INFO]: Test stats: {
    "score": -0.8753930424067257,
    "rmse": 0.8753930424067257
}
[08/28/2025 02:51:26 INFO]: Training loss at epoch 56: 1.0062048435211182
[08/28/2025 02:51:28 INFO]: Training loss at epoch 35: 1.0327081680297852
[08/28/2025 02:51:37 INFO]: Training accuracy: {
    "score": -1.0169050298162614,
    "rmse": 1.0169050298162614
}
[08/28/2025 02:51:37 INFO]: Val accuracy: {
    "score": -0.6517189297874612,
    "rmse": 0.6517189297874612
}
[08/28/2025 02:51:37 INFO]: Test accuracy: {
    "score": -0.8725981874294325,
    "rmse": 0.8725981874294325
}
[08/28/2025 02:51:37 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_86",
    "best_epoch": 12,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8725981874294325,
        "rmse": 0.8725981874294325
    },
    "train_stats": {
        "score": -1.0169050298162614,
        "rmse": 1.0169050298162614
    },
    "val_stats": {
        "score": -0.6517189297874612,
        "rmse": 0.6517189297874612
    }
}
[08/28/2025 02:51:37 INFO]: Procewss finished for trial blotchy-Amado_trial_86
[08/28/2025 02:51:37 INFO]: 
_________________________________________________

[08/28/2025 02:51:37 INFO]: train_net_for_optune.py main() running.
[08/28/2025 02:51:37 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 2.480536021504447
  attention_dropout: 0.3513852108659868
  ffn_dropout: 0.3513852108659868
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.892609135379422e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_92

[08/28/2025 02:51:37 INFO]: This ft_transformer has 0.799 million parameters.
[08/28/2025 02:51:37 INFO]: Training will start at epoch 0.
[08/28/2025 02:51:37 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 02:51:53 INFO]: Training loss at epoch 30: 0.938957154750824
[08/28/2025 02:51:57 INFO]: New best epoch, val score: -0.676631723462214
[08/28/2025 02:51:57 INFO]: Saving model to: blotchy-Amado_trial_83/model_best.pth
[08/28/2025 02:52:08 INFO]: Training loss at epoch 19: 0.9949166774749756
[08/28/2025 02:52:10 INFO]: Training loss at epoch 50: 0.9302161037921906
[08/28/2025 02:52:10 INFO]: Training loss at epoch 34: 1.0927135944366455
[08/28/2025 02:52:10 INFO]: Training loss at epoch 10: 1.298383355140686
[08/28/2025 02:52:11 INFO]: Training loss at epoch 0: 1.207732915878296
[08/28/2025 02:52:13 INFO]: Training loss at epoch 36: 0.9026523232460022
[08/28/2025 02:52:16 INFO]: New best epoch, val score: -0.6591338637007663
[08/28/2025 02:52:16 INFO]: Saving model to: blotchy-Amado_trial_92/model_best.pth
[08/28/2025 02:52:24 INFO]: Training loss at epoch 60: 1.2900427877902985
[08/28/2025 02:52:37 INFO]: Training loss at epoch 32: 0.9241551756858826
[08/28/2025 02:52:42 INFO]: Training loss at epoch 27: 1.1682089567184448
[08/28/2025 02:52:50 INFO]: Training loss at epoch 1: 1.0976139307022095
[08/28/2025 02:52:53 INFO]: Training stats: {
    "score": -0.9861569093236434,
    "rmse": 0.9861569093236434
}
[08/28/2025 02:52:53 INFO]: Val stats: {
    "score": -0.6906942999108112,
    "rmse": 0.6906942999108112
}
[08/28/2025 02:52:53 INFO]: Test stats: {
    "score": -0.8795758482050821,
    "rmse": 0.8795758482050821
}
[08/28/2025 02:52:57 INFO]: Training loss at epoch 11: 1.0570002794265747
[08/28/2025 02:52:59 INFO]: Training loss at epoch 37: 0.9543237388134003
[08/28/2025 02:53:07 INFO]: Training loss at epoch 15: 0.8889634013175964
[08/28/2025 02:53:14 INFO]: Training loss at epoch 27: 0.920823335647583
[08/28/2025 02:53:23 INFO]: Training loss at epoch 61: 0.8552858531475067
[08/28/2025 02:53:27 INFO]: Training loss at epoch 2: 0.9225801229476929
[08/28/2025 02:53:33 INFO]: New best epoch, val score: -0.6598261159571849
[08/28/2025 02:53:33 INFO]: Saving model to: blotchy-Amado_trial_85/model_best.pth
[08/28/2025 02:53:42 INFO]: Training loss at epoch 12: 0.8416534066200256
[08/28/2025 02:53:43 INFO]: Training loss at epoch 38: 0.9071477949619293
[08/28/2025 02:53:48 INFO]: New best epoch, val score: -0.6643009245898863
[08/28/2025 02:53:48 INFO]: Saving model to: blotchy-Amado_trial_90/model_best.pth
[08/28/2025 02:54:04 INFO]: Training loss at epoch 58: 0.6629561185836792
[08/28/2025 02:54:06 INFO]: Training loss at epoch 3: 1.1173818707466125
[08/28/2025 02:54:09 INFO]: Training loss at epoch 34: 0.9183225631713867
[08/28/2025 02:54:13 INFO]: Training loss at epoch 67: 1.3601445257663727
[08/28/2025 02:54:13 INFO]: Training loss at epoch 57: 1.0189713835716248
[08/28/2025 02:54:23 INFO]: Training loss at epoch 62: 1.1469574868679047
[08/28/2025 02:54:31 INFO]: Training loss at epoch 39: 1.1604632139205933
[08/28/2025 02:54:31 INFO]: Training loss at epoch 13: 0.9536032974720001
[08/28/2025 02:54:37 INFO]: New best epoch, val score: -0.6630680107278665
[08/28/2025 02:54:37 INFO]: Saving model to: blotchy-Amado_trial_90/model_best.pth
[08/28/2025 02:54:40 INFO]: Training loss at epoch 20: 0.9350159764289856
[08/28/2025 02:54:45 INFO]: Training loss at epoch 4: 0.941448450088501
[08/28/2025 02:54:45 INFO]: Training stats: {
    "score": -0.9869128705393728,
    "rmse": 0.9869128705393728
}
[08/28/2025 02:54:45 INFO]: Val stats: {
    "score": -0.7122018536027699,
    "rmse": 0.7122018536027699
}
[08/28/2025 02:54:45 INFO]: Test stats: {
    "score": -0.8989634323318889,
    "rmse": 0.8989634323318889
}
[08/28/2025 02:55:01 INFO]: Training loss at epoch 20: 0.8233802616596222
[08/28/2025 02:55:10 INFO]: Training loss at epoch 51: 0.9209198653697968
[08/28/2025 02:55:17 INFO]: Training loss at epoch 25: 1.0340222716331482
[08/28/2025 02:55:18 INFO]: Training loss at epoch 14: 0.909881204366684
[08/28/2025 02:55:23 INFO]: Training loss at epoch 5: 1.3138819932937622
[08/28/2025 02:55:23 INFO]: Training loss at epoch 63: 0.9089899361133575
[08/28/2025 02:55:30 INFO]: Running Final Evaluation...
[08/28/2025 02:55:31 INFO]: Training loss at epoch 31: 0.9887626767158508
[08/28/2025 02:55:31 INFO]: Training loss at epoch 40: 0.9287833869457245
[08/28/2025 02:55:46 INFO]: Training loss at epoch 0: 1.1675564646720886
[08/28/2025 02:55:50 INFO]: Training accuracy: {
    "score": -0.9981890137751815,
    "rmse": 0.9981890137751815
}
[08/28/2025 02:55:50 INFO]: Val accuracy: {
    "score": -0.6677152051329315,
    "rmse": 0.6677152051329315
}
[08/28/2025 02:55:50 INFO]: Test accuracy: {
    "score": -0.8705560417020118,
    "rmse": 0.8705560417020118
}
[08/28/2025 02:55:50 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_84",
    "best_epoch": 32,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8705560417020118,
        "rmse": 0.8705560417020118
    },
    "train_stats": {
        "score": -0.9981890137751815,
        "rmse": 0.9981890137751815
    },
    "val_stats": {
        "score": -0.6677152051329315,
        "rmse": 0.6677152051329315
    }
}
[08/28/2025 02:55:50 INFO]: Procewss finished for trial blotchy-Amado_trial_84
[08/28/2025 02:55:50 INFO]: 
_________________________________________________

[08/28/2025 02:55:50 INFO]: train_net_for_optune.py main() running.
[08/28/2025 02:55:50 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 2.485056023002024
  attention_dropout: 0.03763439276425412
  ffn_dropout: 0.03763439276425412
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 9.159700196223448e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_93

[08/28/2025 02:55:50 INFO]: This ft_transformer has 0.800 million parameters.
[08/28/2025 02:55:50 INFO]: Training will start at epoch 0.
[08/28/2025 02:55:50 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 02:56:01 INFO]: Training loss at epoch 6: 1.0155833661556244
[08/28/2025 02:56:04 INFO]: Training loss at epoch 15: 0.9182201027870178
[08/28/2025 02:56:08 INFO]: Training loss at epoch 28: 0.8345429003238678
[08/28/2025 02:56:17 INFO]: Training loss at epoch 41: 1.145215094089508
[08/28/2025 02:56:24 INFO]: Training loss at epoch 0: 1.0192394256591797
[08/28/2025 02:56:28 INFO]: New best epoch, val score: -0.6723033787312505
[08/28/2025 02:56:28 INFO]: Saving model to: blotchy-Amado_trial_93/model_best.pth
[08/28/2025 02:56:39 INFO]: Training loss at epoch 7: 1.097282588481903
[08/28/2025 02:56:40 INFO]: Training loss at epoch 16: 1.028563380241394
[08/28/2025 02:56:41 INFO]: Training loss at epoch 28: 1.0015680491924286
[08/28/2025 02:56:52 INFO]: Training loss at epoch 16: 1.2031351029872894
[08/28/2025 02:57:02 INFO]: Training loss at epoch 58: 0.9684908986091614
[08/28/2025 02:57:03 INFO]: Training loss at epoch 1: 0.9950624108314514
[08/28/2025 02:57:04 INFO]: Training loss at epoch 42: 0.9505486488342285
[08/28/2025 02:57:08 INFO]: Training loss at epoch 21: 0.9414737820625305
[08/28/2025 02:57:11 INFO]: Training loss at epoch 68: 1.0832909941673279
[08/28/2025 02:57:13 INFO]: Training loss at epoch 10: 0.8941110670566559
[08/28/2025 02:57:14 INFO]: New best epoch, val score: -0.68674903628246
[08/28/2025 02:57:14 INFO]: Saving model to: blotchy-Amado_trial_91/model_best.pth
[08/28/2025 02:57:18 INFO]: Training loss at epoch 8: 1.128963828086853
[08/28/2025 02:57:19 INFO]: Training loss at epoch 35: 0.939669132232666
[08/28/2025 02:57:34 INFO]: Training loss at epoch 33: 1.0459968447685242
[08/28/2025 02:57:39 INFO]: Training loss at epoch 17: 0.9643217921257019
[08/28/2025 02:57:42 INFO]: Training loss at epoch 2: 1.0312055945396423
[08/28/2025 02:57:47 INFO]: Training loss at epoch 35: 1.1578379273414612
[08/28/2025 02:57:50 INFO]: Training loss at epoch 43: 0.8417932093143463
[08/28/2025 02:57:56 INFO]: Training loss at epoch 9: 1.0971070528030396
[08/28/2025 02:58:09 INFO]: Running Final Evaluation...
[08/28/2025 02:58:11 INFO]: Training stats: {
    "score": -1.026542358344982,
    "rmse": 1.026542358344982
}
[08/28/2025 02:58:11 INFO]: Val stats: {
    "score": -0.7578850794023165,
    "rmse": 0.7578850794023165
}
[08/28/2025 02:58:11 INFO]: Test stats: {
    "score": -0.9221656365017407,
    "rmse": 0.9221656365017407
}
[08/28/2025 02:58:11 INFO]: Training loss at epoch 52: 1.2379015684127808
[08/28/2025 02:58:15 INFO]: Training loss at epoch 21: 0.8980163335800171
[08/28/2025 02:58:21 INFO]: Training loss at epoch 3: 0.8464871644973755
[08/28/2025 02:58:25 INFO]: New best epoch, val score: -0.6711676457579877
[08/28/2025 02:58:25 INFO]: Saving model to: blotchy-Amado_trial_93/model_best.pth
[08/28/2025 02:58:27 INFO]: Training loss at epoch 18: 1.063604861497879
[08/28/2025 02:58:35 INFO]: Training loss at epoch 44: 1.0312628746032715
[08/28/2025 02:58:47 INFO]: Training loss at epoch 10: 1.097184658050537
[08/28/2025 02:58:57 INFO]: Training loss at epoch 4: 0.9822848737239838
[08/28/2025 02:59:07 INFO]: Training loss at epoch 32: 1.1105163097381592
[08/28/2025 02:59:12 INFO]: Training loss at epoch 19: 0.9825106263160706
[08/28/2025 02:59:17 INFO]: Training loss at epoch 22: 1.1575855612754822
[08/28/2025 02:59:20 INFO]: Training loss at epoch 45: 1.1138925552368164
[08/28/2025 02:59:25 INFO]: Training loss at epoch 11: 1.1713647842407227
[08/28/2025 02:59:29 INFO]: Training stats: {
    "score": -1.001325853008819,
    "rmse": 1.001325853008819
}
[08/28/2025 02:59:29 INFO]: Val stats: {
    "score": -0.7038713202924713,
    "rmse": 0.7038713202924713
}
[08/28/2025 02:59:29 INFO]: Test stats: {
    "score": -0.8830476208132491,
    "rmse": 0.8830476208132491
}
[08/28/2025 02:59:35 INFO]: Training loss at epoch 5: 0.8317798376083374
[08/28/2025 02:59:36 INFO]: Training loss at epoch 29: 1.1948482394218445
[08/28/2025 02:59:52 INFO]: Training loss at epoch 59: 1.2217537462711334
[08/28/2025 03:00:02 INFO]: Training accuracy: {
    "score": -1.0144232948305858,
    "rmse": 1.0144232948305858
}
[08/28/2025 03:00:02 INFO]: Val accuracy: {
    "score": -0.6606457528289631,
    "rmse": 0.6606457528289631
}
[08/28/2025 03:00:02 INFO]: Test accuracy: {
    "score": -0.8742791596689178,
    "rmse": 0.8742791596689178
}
[08/28/2025 03:00:02 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_68",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8742791596689178,
        "rmse": 0.8742791596689178
    },
    "train_stats": {
        "score": -1.0144232948305858,
        "rmse": 1.0144232948305858
    },
    "val_stats": {
        "score": -0.6606457528289631,
        "rmse": 0.6606457528289631
    }
}
[08/28/2025 03:00:02 INFO]: Procewss finished for trial blotchy-Amado_trial_68
[08/28/2025 03:00:02 INFO]: 
_________________________________________________

[08/28/2025 03:00:02 INFO]: train_net_for_optune.py main() running.
[08/28/2025 03:00:02 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.5107298630191828
  attention_dropout: 0.04462497429463473
  ffn_dropout: 0.04462497429463473
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.8395849579027482e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_94

[08/28/2025 03:00:02 INFO]: This ft_transformer has 0.805 million parameters.
[08/28/2025 03:00:02 INFO]: Training will start at epoch 0.
[08/28/2025 03:00:02 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 03:00:06 INFO]: Training loss at epoch 12: 1.1303817927837372
[08/28/2025 03:00:06 INFO]: Training loss at epoch 46: 1.3217573761940002
[08/28/2025 03:00:10 INFO]: Training loss at epoch 29: 0.9903763234615326
[08/28/2025 03:00:11 INFO]: Training loss at epoch 69: 1.1376565992832184
[08/28/2025 03:00:13 INFO]: Training loss at epoch 6: 0.9577541351318359
[08/28/2025 03:00:15 INFO]: Training loss at epoch 17: 0.997908353805542
[08/28/2025 03:00:16 INFO]: Training loss at epoch 20: 1.0419762134552002
[08/28/2025 03:00:17 INFO]: Training loss at epoch 26: 0.8592483997344971
[08/28/2025 03:00:42 INFO]: Training loss at epoch 13: 1.0331461429595947
[08/28/2025 03:00:44 INFO]: Training stats: {
    "score": -0.9971709901648302,
    "rmse": 0.9971709901648302
}
[08/28/2025 03:00:44 INFO]: Val stats: {
    "score": -0.6868492600764579,
    "rmse": 0.6868492600764579
}
[08/28/2025 03:00:44 INFO]: Test stats: {
    "score": -0.876845072851827,
    "rmse": 0.876845072851827
}
[08/28/2025 03:00:50 INFO]: Training loss at epoch 47: 0.8375687003135681
[08/28/2025 03:00:50 INFO]: Training loss at epoch 7: 1.1973540782928467
[08/28/2025 03:00:50 INFO]: Training stats: {
    "score": -0.9946450169168224,
    "rmse": 0.9946450169168224
}
[08/28/2025 03:00:50 INFO]: Val stats: {
    "score": -0.6564435736534272,
    "rmse": 0.6564435736534272
}
[08/28/2025 03:00:50 INFO]: Test stats: {
    "score": -0.8787563312204565,
    "rmse": 0.8787563312204565
}
[08/28/2025 03:01:00 INFO]: Training loss at epoch 21: 1.2521289885044098
[08/28/2025 03:01:02 INFO]: Training loss at epoch 0: 1.0241245031356812
[08/28/2025 03:01:06 INFO]: Training loss at epoch 53: 1.197494238615036
[08/28/2025 03:01:07 INFO]: Training stats: {
    "score": -1.0058266814993595,
    "rmse": 1.0058266814993595
}
[08/28/2025 03:01:07 INFO]: Val stats: {
    "score": -0.6621440697019233,
    "rmse": 0.6621440697019233
}
[08/28/2025 03:01:07 INFO]: Test stats: {
    "score": -0.8705497963924141,
    "rmse": 0.8705497963924141
}
[08/28/2025 03:01:11 INFO]: New best epoch, val score: -0.6623614714286682
[08/28/2025 03:01:11 INFO]: Saving model to: blotchy-Amado_trial_94/model_best.pth
[08/28/2025 03:01:19 INFO]: Training loss at epoch 14: 1.3657608032226562
[08/28/2025 03:01:19 INFO]: Training loss at epoch 36: 1.2518313527107239
[08/28/2025 03:01:21 INFO]: Training stats: {
    "score": -1.0020643259668185,
    "rmse": 1.0020643259668185
}
[08/28/2025 03:01:21 INFO]: Val stats: {
    "score": -0.6656654299562521,
    "rmse": 0.6656654299562521
}
[08/28/2025 03:01:21 INFO]: Test stats: {
    "score": -0.8650664116479612,
    "rmse": 0.8650664116479612
}
[08/28/2025 03:01:22 INFO]: Training loss at epoch 23: 1.1111444532871246
[08/28/2025 03:01:26 INFO]: Training loss at epoch 8: 1.0862568318843842
[08/28/2025 03:01:33 INFO]: Training loss at epoch 48: 1.0170259177684784
[08/28/2025 03:01:38 INFO]: Running Final Evaluation...
[08/28/2025 03:01:46 INFO]: Training loss at epoch 22: 0.8783372640609741
[08/28/2025 03:01:48 INFO]: Training loss at epoch 22: 1.2751544713974
[08/28/2025 03:01:56 INFO]: Training accuracy: {
    "score": -1.0014196551316588,
    "rmse": 1.0014196551316588
}
[08/28/2025 03:01:56 INFO]: Val accuracy: {
    "score": -0.6617656918883532,
    "rmse": 0.6617656918883532
}
[08/28/2025 03:01:56 INFO]: Test accuracy: {
    "score": -0.8746752958689458,
    "rmse": 0.8746752958689458
}
[08/28/2025 03:01:56 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_89",
    "best_epoch": 17,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8746752958689458,
        "rmse": 0.8746752958689458
    },
    "train_stats": {
        "score": -1.0014196551316588,
        "rmse": 1.0014196551316588
    },
    "val_stats": {
        "score": -0.6617656918883532,
        "rmse": 0.6617656918883532
    }
}
[08/28/2025 03:01:56 INFO]: Procewss finished for trial blotchy-Amado_trial_89
[08/28/2025 03:01:56 INFO]: 
_________________________________________________

[08/28/2025 03:01:56 INFO]: train_net_for_optune.py main() running.
[08/28/2025 03:01:56 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 2.251486354275264
  attention_dropout: 0.4247955240167774
  ffn_dropout: 0.4247955240167774
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.0354715151860326e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_95

[08/28/2025 03:01:56 INFO]: This ft_transformer has 0.754 million parameters.
[08/28/2025 03:01:56 INFO]: Training will start at epoch 0.
[08/28/2025 03:01:56 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 03:01:58 INFO]: Training loss at epoch 15: 1.178392469882965
[08/28/2025 03:02:04 INFO]: Training loss at epoch 9: 1.0518355071544647
[08/28/2025 03:02:15 INFO]: Training loss at epoch 1: 1.39002126455307
[08/28/2025 03:02:18 INFO]: Training stats: {
    "score": -0.9908440795450434,
    "rmse": 0.9908440795450434
}
[08/28/2025 03:02:18 INFO]: Val stats: {
    "score": -0.6758044119047474,
    "rmse": 0.6758044119047474
}
[08/28/2025 03:02:18 INFO]: Test stats: {
    "score": -0.8805527146911352,
    "rmse": 0.8805527146911352
}
[08/28/2025 03:02:25 INFO]: New best epoch, val score: -0.6565876780082175
[08/28/2025 03:02:25 INFO]: Saving model to: blotchy-Amado_trial_94/model_best.pth
[08/28/2025 03:02:27 INFO]: Training loss at epoch 36: 0.9486944675445557
[08/28/2025 03:02:31 INFO]: Training loss at epoch 0: 1.4674394726753235
[08/28/2025 03:02:35 INFO]: Training loss at epoch 23: 0.8436226546764374
[08/28/2025 03:02:35 INFO]: New best epoch, val score: -0.7226969849782382
[08/28/2025 03:02:35 INFO]: Saving model to: blotchy-Amado_trial_95/model_best.pth
[08/28/2025 03:02:37 INFO]: Training loss at epoch 16: 0.92132967710495
[08/28/2025 03:02:43 INFO]: Training loss at epoch 33: 1.150998294353485
[08/28/2025 03:02:57 INFO]: Training loss at epoch 10: 1.0203807950019836
[08/28/2025 03:03:09 INFO]: Training loss at epoch 1: 1.0868983268737793
[08/28/2025 03:03:13 INFO]: Training loss at epoch 11: 0.9941318333148956
[08/28/2025 03:03:13 INFO]: New best epoch, val score: -0.6664198355030839
[08/28/2025 03:03:13 INFO]: Saving model to: blotchy-Amado_trial_95/model_best.pth
[08/28/2025 03:03:16 INFO]: Training loss at epoch 17: 0.8971820771694183
[08/28/2025 03:03:22 INFO]: Training loss at epoch 24: 1.1465765833854675
[08/28/2025 03:03:31 INFO]: Training loss at epoch 2: 0.9274874329566956
[08/28/2025 03:03:33 INFO]: Training loss at epoch 24: 0.842409074306488
[08/28/2025 03:03:33 INFO]: Training loss at epoch 60: 0.9130630791187286
[08/28/2025 03:03:36 INFO]: Training loss at epoch 11: 1.1297024488449097
[08/28/2025 03:03:41 INFO]: New best epoch, val score: -0.6541032817121915
[08/28/2025 03:03:41 INFO]: Saving model to: blotchy-Amado_trial_94/model_best.pth
[08/28/2025 03:03:47 INFO]: Training loss at epoch 2: 1.2746391296386719
[08/28/2025 03:03:52 INFO]: Training loss at epoch 18: 1.0684590637683868
[08/28/2025 03:03:55 INFO]: Training loss at epoch 18: 0.9528218507766724
[08/28/2025 03:04:07 INFO]: Training loss at epoch 54: 0.9135348796844482
[08/28/2025 03:04:08 INFO]: Training loss at epoch 70: 1.246447741985321
[08/28/2025 03:04:10 INFO]: Training loss at epoch 25: 1.2326507866382599
[08/28/2025 03:04:14 INFO]: Training loss at epoch 12: 0.9675359427928925
[08/28/2025 03:04:21 INFO]: Training loss at epoch 30: 1.0249828100204468
[08/28/2025 03:04:26 INFO]: Training loss at epoch 3: 0.8968587815761566
[08/28/2025 03:04:33 INFO]: Training loss at epoch 19: 1.3031314611434937
[08/28/2025 03:04:45 INFO]: Training loss at epoch 3: 0.849949836730957
[08/28/2025 03:04:48 INFO]: Training stats: {
    "score": -1.003847633559575,
    "rmse": 1.003847633559575
}
[08/28/2025 03:04:48 INFO]: Val stats: {
    "score": -0.681724884914434,
    "rmse": 0.681724884914434
}
[08/28/2025 03:04:48 INFO]: Test stats: {
    "score": -0.8757826971846092,
    "rmse": 0.8757826971846092
}
[08/28/2025 03:04:48 INFO]: New best epoch, val score: -0.6545011544969308
[08/28/2025 03:04:49 INFO]: Saving model to: blotchy-Amado_trial_80/model_best.pth
[08/28/2025 03:04:53 INFO]: Training loss at epoch 13: 0.9779278039932251
[08/28/2025 03:04:57 INFO]: Training loss at epoch 30: 1.021911233663559
[08/28/2025 03:04:58 INFO]: Training loss at epoch 26: 0.9648107886314392
[08/28/2025 03:05:02 INFO]: Training loss at epoch 37: 0.8690285086631775
[08/28/2025 03:05:03 INFO]: Training loss at epoch 4: 1.1204678416252136
[08/28/2025 03:05:18 INFO]: Training loss at epoch 27: 0.8379395604133606
[08/28/2025 03:05:26 INFO]: Training loss at epoch 59: 0.8853646814823151
[08/28/2025 03:05:27 INFO]: Training loss at epoch 20: 0.8907243311405182
[08/28/2025 03:05:27 INFO]: Training loss at epoch 23: 0.970037430524826
[08/28/2025 03:05:31 INFO]: Training loss at epoch 14: 0.9978415071964264
[08/28/2025 03:05:40 INFO]: Training loss at epoch 5: 1.0930918455123901
[08/28/2025 03:05:42 INFO]: Training loss at epoch 25: 0.8956761956214905
[08/28/2025 03:05:44 INFO]: Training loss at epoch 27: 0.8573473989963531
[08/28/2025 03:06:00 INFO]: Training loss at epoch 4: 1.0475678443908691
[08/28/2025 03:06:05 INFO]: Training loss at epoch 21: 0.9123543202877045
[08/28/2025 03:06:10 INFO]: Training loss at epoch 15: 0.7006552517414093
[08/28/2025 03:06:17 INFO]: Training loss at epoch 6: 1.1303207278251648
[08/28/2025 03:06:23 INFO]: Training loss at epoch 61: 1.02400404214859
[08/28/2025 03:06:24 INFO]: Training loss at epoch 34: 0.8950505554676056
[08/28/2025 03:06:32 INFO]: Training loss at epoch 28: 0.8219027519226074
[08/28/2025 03:06:43 INFO]: Training loss at epoch 22: 1.196811318397522
[08/28/2025 03:06:44 INFO]: Running Final Evaluation...
[08/28/2025 03:06:48 INFO]: Training loss at epoch 16: 1.1194461584091187
[08/28/2025 03:06:54 INFO]: Training loss at epoch 7: 1.1084779500961304
[08/28/2025 03:07:10 INFO]: Training loss at epoch 71: 0.927435427904129
[08/28/2025 03:07:10 INFO]: Training loss at epoch 55: 0.8156566917896271
[08/28/2025 03:07:14 INFO]: Training loss at epoch 5: 0.908828467130661
[08/28/2025 03:07:20 INFO]: Training loss at epoch 29: 0.8981474041938782
[08/28/2025 03:07:22 INFO]: Training loss at epoch 23: 1.0953169465065002
[08/28/2025 03:07:26 INFO]: Training loss at epoch 17: 1.099532574415207
[08/28/2025 03:07:29 INFO]: Training loss at epoch 19: 0.8764412999153137
[08/28/2025 03:07:31 INFO]: Training loss at epoch 8: 1.3280184268951416
[08/28/2025 03:07:37 INFO]: Training stats: {
    "score": -0.9976575112826646,
    "rmse": 0.9976575112826646
}
[08/28/2025 03:07:37 INFO]: Val stats: {
    "score": -0.6704838300478747,
    "rmse": 0.6704838300478747
}
[08/28/2025 03:07:37 INFO]: Test stats: {
    "score": -0.8667480619888446,
    "rmse": 0.8667480619888446
}
[08/28/2025 03:07:44 INFO]: Training loss at epoch 37: 1.0715293884277344
[08/28/2025 03:07:45 INFO]: Training accuracy: {
    "score": -1.011764259248708,
    "rmse": 1.011764259248708
}
[08/28/2025 03:07:45 INFO]: Val accuracy: {
    "score": -0.6600227844538308,
    "rmse": 0.6600227844538308
}
[08/28/2025 03:07:45 INFO]: Test accuracy: {
    "score": -0.8716394421725426,
    "rmse": 0.8716394421725426
}
[08/28/2025 03:07:45 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_69",
    "best_epoch": 30,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8716394421725426,
        "rmse": 0.8716394421725426
    },
    "train_stats": {
        "score": -1.011764259248708,
        "rmse": 1.011764259248708
    },
    "val_stats": {
        "score": -0.6600227844538308,
        "rmse": 0.6600227844538308
    }
}
[08/28/2025 03:07:45 INFO]: Procewss finished for trial blotchy-Amado_trial_69
[08/28/2025 03:07:45 INFO]: 
_________________________________________________

[08/28/2025 03:07:45 INFO]: train_net_for_optune.py main() running.
[08/28/2025 03:07:45 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.4929258183883105
  attention_dropout: 0.4193609068089302
  ffn_dropout: 0.4193609068089302
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.897459872414478e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_96

[08/28/2025 03:07:45 INFO]: This ft_transformer has 0.802 million parameters.
[08/28/2025 03:07:45 INFO]: Training will start at epoch 0.
[08/28/2025 03:07:45 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 03:07:53 INFO]: Training loss at epoch 31: 0.9701937437057495
[08/28/2025 03:07:54 INFO]: Training loss at epoch 26: 0.9699451625347137
[08/28/2025 03:08:01 INFO]: Training loss at epoch 24: 1.13712078332901
[08/28/2025 03:08:05 INFO]: Training loss at epoch 18: 1.0389192402362823
[08/28/2025 03:08:07 INFO]: Training loss at epoch 1: 1.0396934151649475
[08/28/2025 03:08:08 INFO]: Training loss at epoch 9: 0.9045383036136627
[08/28/2025 03:08:19 INFO]: New best epoch, val score: -0.6529623133649511
[08/28/2025 03:08:19 INFO]: Saving model to: blotchy-Amado_trial_80/model_best.pth
[08/28/2025 03:08:23 INFO]: Training stats: {
    "score": -1.0691698151253828,
    "rmse": 1.0691698151253828
}
[08/28/2025 03:08:23 INFO]: Val stats: {
    "score": -0.8520224619287224,
    "rmse": 0.8520224619287224
}
[08/28/2025 03:08:23 INFO]: Test stats: {
    "score": -0.981834524637633,
    "rmse": 0.981834524637633
}
[08/28/2025 03:08:24 INFO]: Training loss at epoch 30: 1.258954018354416
[08/28/2025 03:08:28 INFO]: Training loss at epoch 6: 0.9970875382423401
[08/28/2025 03:08:29 INFO]: Training loss at epoch 31: 1.1989660263061523
[08/28/2025 03:08:41 INFO]: Training loss at epoch 25: 1.16147780418396
[08/28/2025 03:08:42 INFO]: Training loss at epoch 38: 1.1038847267627716
[08/28/2025 03:08:46 INFO]: Training loss at epoch 19: 1.0957597494125366
[08/28/2025 03:08:47 INFO]: Training stats: {
    "score": -0.9950216306812054,
    "rmse": 0.9950216306812054
}
[08/28/2025 03:08:47 INFO]: Val stats: {
    "score": -0.6695031338226803,
    "rmse": 0.6695031338226803
}
[08/28/2025 03:08:47 INFO]: Test stats: {
    "score": -0.8755685202852729,
    "rmse": 0.8755685202852729
}
[08/28/2025 03:08:50 INFO]: Training loss at epoch 0: 1.1159793138504028
[08/28/2025 03:08:58 INFO]: Training stats: {
    "score": -0.975586421678982,
    "rmse": 0.975586421678982
}
[08/28/2025 03:08:58 INFO]: Val stats: {
    "score": -0.694195447958488,
    "rmse": 0.694195447958488
}
[08/28/2025 03:08:58 INFO]: Test stats: {
    "score": -0.8894336617887904,
    "rmse": 0.8894336617887904
}
[08/28/2025 03:08:58 INFO]: Training loss at epoch 10: 1.223822832107544
[08/28/2025 03:08:59 INFO]: New best epoch, val score: -0.7839038533151572
[08/28/2025 03:08:59 INFO]: Saving model to: blotchy-Amado_trial_96/model_best.pth
[08/28/2025 03:09:05 INFO]: Training loss at epoch 24: 1.0481582880020142
[08/28/2025 03:09:09 INFO]: Training loss at epoch 31: 1.136264830827713
[08/28/2025 03:09:14 INFO]: Training stats: {
    "score": -0.9663251311721452,
    "rmse": 0.9663251311721452
}
[08/28/2025 03:09:14 INFO]: Val stats: {
    "score": -1.0963427157604646,
    "rmse": 1.0963427157604646
}
[08/28/2025 03:09:14 INFO]: Test stats: {
    "score": -1.2592266600659208,
    "rmse": 1.2592266600659208
}
[08/28/2025 03:09:17 INFO]: Training loss at epoch 12: 1.1997780799865723
[08/28/2025 03:09:17 INFO]: Training loss at epoch 26: 0.9728725254535675
[08/28/2025 03:09:33 INFO]: Training loss at epoch 11: 1.1510809063911438
[08/28/2025 03:09:34 INFO]: Training loss at epoch 20: 0.9263586401939392
[08/28/2025 03:09:39 INFO]: Training loss at epoch 7: 0.9363628625869751
[08/28/2025 03:09:56 INFO]: Training loss at epoch 27: 1.035690724849701
[08/28/2025 03:09:56 INFO]: Training loss at epoch 32: 1.0596697330474854
[08/28/2025 03:10:02 INFO]: Training loss at epoch 1: 1.1896924376487732
[08/28/2025 03:10:03 INFO]: Training loss at epoch 27: 1.0895383656024933
[08/28/2025 03:10:03 INFO]: Training loss at epoch 35: 0.9012908339500427
[08/28/2025 03:10:09 INFO]: Training loss at epoch 72: 1.0409032106399536
[08/28/2025 03:10:11 INFO]: Training loss at epoch 56: 1.0415526628494263
[08/28/2025 03:10:12 INFO]: New best epoch, val score: -0.7299164375101354
[08/28/2025 03:10:12 INFO]: Saving model to: blotchy-Amado_trial_96/model_best.pth
[08/28/2025 03:10:13 INFO]: Training loss at epoch 12: 1.3375165164470673
[08/28/2025 03:10:14 INFO]: Training loss at epoch 21: 0.8685665726661682
[08/28/2025 03:10:21 INFO]: Training loss at epoch 28: 0.8516354560852051
[08/28/2025 03:10:36 INFO]: Training loss at epoch 28: 1.2317620515823364
[08/28/2025 03:10:46 INFO]: Training loss at epoch 33: 1.019469141960144
[08/28/2025 03:10:50 INFO]: Training loss at epoch 13: 1.0419543981552124
[08/28/2025 03:10:53 INFO]: Training loss at epoch 8: 0.958111971616745
[08/28/2025 03:10:54 INFO]: Training loss at epoch 22: 0.815398097038269
[08/28/2025 03:11:16 INFO]: Training loss at epoch 29: 0.8939596712589264
[08/28/2025 03:11:17 INFO]: Training loss at epoch 2: 1.064071238040924
[08/28/2025 03:11:23 INFO]: Training loss at epoch 32: 1.181060016155243
[08/28/2025 03:11:26 INFO]: New best epoch, val score: -0.6887389349386448
[08/28/2025 03:11:26 INFO]: Saving model to: blotchy-Amado_trial_96/model_best.pth
[08/28/2025 03:11:28 INFO]: Training loss at epoch 14: 1.2178625166416168
[08/28/2025 03:11:28 INFO]: Training stats: {
    "score": -1.0032709598502758,
    "rmse": 1.0032709598502758
}
[08/28/2025 03:11:28 INFO]: Val stats: {
    "score": -0.6756583904732982,
    "rmse": 0.6756583904732982
}
[08/28/2025 03:11:28 INFO]: Test stats: {
    "score": -0.8725774965543741,
    "rmse": 0.8725774965543741
}
[08/28/2025 03:11:32 INFO]: Training loss at epoch 23: 0.9886254966259003
[08/28/2025 03:11:33 INFO]: Training loss at epoch 34: 1.0549719333648682
[08/28/2025 03:11:48 INFO]: New best epoch, val score: -0.6521002356679267
[08/28/2025 03:11:48 INFO]: Saving model to: blotchy-Amado_trial_80/model_best.pth
[08/28/2025 03:11:57 INFO]: Training loss at epoch 32: 1.0327233374118805
[08/28/2025 03:12:04 INFO]: Training loss at epoch 15: 0.932613343000412
[08/28/2025 03:12:06 INFO]: Training loss at epoch 30: 1.0151702761650085
[08/28/2025 03:12:06 INFO]: Training loss at epoch 9: 0.9340513944625854
[08/28/2025 03:12:11 INFO]: Training loss at epoch 24: 1.2188116908073425
[08/28/2025 03:12:14 INFO]: Training loss at epoch 28: 0.9213399291038513
[08/28/2025 03:12:20 INFO]: Training loss at epoch 35: 0.8019760847091675
[08/28/2025 03:12:22 INFO]: Training loss at epoch 20: 0.9578071534633636
[08/28/2025 03:12:23 INFO]: Training loss at epoch 39: 1.169058918952942
[08/28/2025 03:12:29 INFO]: Training loss at epoch 3: 1.2154789566993713
[08/28/2025 03:12:32 INFO]: Training stats: {
    "score": -0.9946226371400645,
    "rmse": 0.9946226371400645
}
[08/28/2025 03:12:32 INFO]: Val stats: {
    "score": -0.6637707202366219,
    "rmse": 0.6637707202366219
}
[08/28/2025 03:12:32 INFO]: Test stats: {
    "score": -0.8602092274047947,
    "rmse": 0.8602092274047947
}
[08/28/2025 03:12:38 INFO]: New best epoch, val score: -0.6723261981424884
[08/28/2025 03:12:38 INFO]: Saving model to: blotchy-Amado_trial_96/model_best.pth
[08/28/2025 03:12:40 INFO]: Training loss at epoch 16: 1.0417534708976746
[08/28/2025 03:12:41 INFO]: Training loss at epoch 25: 1.2365668416023254
[08/28/2025 03:12:44 INFO]: Training loss at epoch 31: 1.2132403254508972
[08/28/2025 03:12:47 INFO]: Training loss at epoch 25: 0.8394210040569305
[08/28/2025 03:12:48 INFO]: Running Final Evaluation...
[08/28/2025 03:12:54 INFO]: Training loss at epoch 38: 0.9845767617225647
[08/28/2025 03:13:00 INFO]: Training accuracy: {
    "score": -1.0257737175688257,
    "rmse": 1.0257737175688257
}
[08/28/2025 03:13:00 INFO]: Val accuracy: {
    "score": -0.6591338637007663,
    "rmse": 0.6591338637007663
}
[08/28/2025 03:13:00 INFO]: Test accuracy: {
    "score": -0.8755477855607003,
    "rmse": 0.8755477855607003
}
[08/28/2025 03:13:00 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_92",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8755477855607003,
        "rmse": 0.8755477855607003
    },
    "train_stats": {
        "score": -1.0257737175688257,
        "rmse": 1.0257737175688257
    },
    "val_stats": {
        "score": -0.6591338637007663,
        "rmse": 0.6591338637007663
    }
}
[08/28/2025 03:13:00 INFO]: Procewss finished for trial blotchy-Amado_trial_92
[08/28/2025 03:13:00 INFO]: 
_________________________________________________

[08/28/2025 03:13:00 INFO]: train_net_for_optune.py main() running.
[08/28/2025 03:13:00 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.20183819518915
  attention_dropout: 0.03700941847977268
  ffn_dropout: 0.03700941847977268
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.8376820251734076e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_97

[08/28/2025 03:13:00 INFO]: This ft_transformer has 11.316 million parameters.
[08/28/2025 03:13:00 INFO]: Training will start at epoch 0.
[08/28/2025 03:13:00 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 03:13:06 INFO]: Training loss at epoch 36: 1.0692089200019836
[08/28/2025 03:13:07 INFO]: Training loss at epoch 73: 0.9126048684120178
[08/28/2025 03:13:14 INFO]: Training loss at epoch 57: 1.1848329603672028
[08/28/2025 03:13:17 INFO]: Training loss at epoch 17: 0.9628362357616425
[08/28/2025 03:13:26 INFO]: Training loss at epoch 26: 0.8383945822715759
[08/28/2025 03:13:35 INFO]: Training stats: {
    "score": -0.9791183859881429,
    "rmse": 0.9791183859881429
}
[08/28/2025 03:13:35 INFO]: Val stats: {
    "score": -0.7079773374324109,
    "rmse": 0.7079773374324109
}
[08/28/2025 03:13:35 INFO]: Test stats: {
    "score": -0.8958663425429615,
    "rmse": 0.8958663425429615
}
[08/28/2025 03:13:42 INFO]: Training loss at epoch 4: 0.9677743017673492
[08/28/2025 03:13:44 INFO]: Training loss at epoch 36: 0.9198898673057556
[08/28/2025 03:13:46 INFO]: Training loss at epoch 10: 1.0636267066001892
[08/28/2025 03:13:51 INFO]: New best epoch, val score: -0.6699228269464497
[08/28/2025 03:13:51 INFO]: Saving model to: blotchy-Amado_trial_96/model_best.pth
[08/28/2025 03:13:54 INFO]: Training loss at epoch 37: 1.0728011429309845
[08/28/2025 03:13:54 INFO]: Training loss at epoch 18: 0.8836343586444855
[08/28/2025 03:14:04 INFO]: Training loss at epoch 27: 1.4420996606349945
[08/28/2025 03:14:23 INFO]: Training loss at epoch 29: 0.8359906375408173
[08/28/2025 03:14:32 INFO]: Training loss at epoch 19: 0.9140781164169312
[08/28/2025 03:14:41 INFO]: Training loss at epoch 38: 0.9820927679538727
[08/28/2025 03:14:43 INFO]: Training loss at epoch 28: 0.9204130172729492
[08/28/2025 03:14:44 INFO]: Training stats: {
    "score": -0.9987212624500308,
    "rmse": 0.9987212624500308
}
[08/28/2025 03:14:44 INFO]: Val stats: {
    "score": -0.690891159108253,
    "rmse": 0.690891159108253
}
[08/28/2025 03:14:44 INFO]: Test stats: {
    "score": -0.8795021143166567,
    "rmse": 0.8795021143166567
}
[08/28/2025 03:14:53 INFO]: Training loss at epoch 33: 0.8298179507255554
[08/28/2025 03:14:55 INFO]: Training loss at epoch 5: 1.0200253129005432
[08/28/2025 03:15:01 INFO]: Training loss at epoch 11: 0.9299544394016266
[08/28/2025 03:15:04 INFO]: New best epoch, val score: -0.669912321010207
[08/28/2025 03:15:04 INFO]: Saving model to: blotchy-Amado_trial_96/model_best.pth
[08/28/2025 03:15:06 INFO]: Training stats: {
    "score": -0.9804885636417219,
    "rmse": 0.9804885636417219
}
[08/28/2025 03:15:06 INFO]: Val stats: {
    "score": -0.6836212382775819,
    "rmse": 0.6836212382775819
}
[08/28/2025 03:15:06 INFO]: Test stats: {
    "score": -0.8730888729722573,
    "rmse": 0.8730888729722573
}
[08/28/2025 03:15:18 INFO]: New best epoch, val score: -0.6517107463850329
[08/28/2025 03:15:18 INFO]: Saving model to: blotchy-Amado_trial_80/model_best.pth
[08/28/2025 03:15:19 INFO]: Training loss at epoch 20: 1.347061812877655
[08/28/2025 03:15:20 INFO]: Training loss at epoch 29: 0.7282822728157043
[08/28/2025 03:15:21 INFO]: Training loss at epoch 29: 1.1313735842704773
[08/28/2025 03:15:21 INFO]: Training loss at epoch 13: 1.1494338810443878
[08/28/2025 03:15:26 INFO]: Training loss at epoch 39: 0.9525731801986694
[08/28/2025 03:15:29 INFO]: Training loss at epoch 33: 1.0380742251873016
[08/28/2025 03:15:32 INFO]: Training stats: {
    "score": -0.9601664003063886,
    "rmse": 0.9601664003063886
}
[08/28/2025 03:15:32 INFO]: Val stats: {
    "score": -0.6995269247966244,
    "rmse": 0.6995269247966244
}
[08/28/2025 03:15:32 INFO]: Test stats: {
    "score": -0.9033467554793625,
    "rmse": 0.9033467554793625
}
[08/28/2025 03:15:44 INFO]: Training stats: {
    "score": -1.0017192343556516,
    "rmse": 1.0017192343556516
}
[08/28/2025 03:15:44 INFO]: Val stats: {
    "score": -0.6621060355168459,
    "rmse": 0.6621060355168459
}
[08/28/2025 03:15:44 INFO]: Test stats: {
    "score": -0.8666877864027195,
    "rmse": 0.8666877864027195
}
[08/28/2025 03:15:49 INFO]: New best epoch, val score: -0.6621060355168459
[08/28/2025 03:15:49 INFO]: Saving model to: blotchy-Amado_trial_90/model_best.pth
[08/28/2025 03:15:56 INFO]: Training loss at epoch 21: 1.175490379333496
[08/28/2025 03:16:05 INFO]: Training loss at epoch 21: 1.0035904347896576
[08/28/2025 03:16:07 INFO]: Training loss at epoch 6: 1.1740394830703735
[08/28/2025 03:16:10 INFO]: Training loss at epoch 30: 0.860309362411499
[08/28/2025 03:16:11 INFO]: Training loss at epoch 74: 1.0372035503387451
[08/28/2025 03:16:12 INFO]: Training loss at epoch 58: 0.966379702091217
[08/28/2025 03:16:16 INFO]: New best epoch, val score: -0.6698392710414149
[08/28/2025 03:16:16 INFO]: Saving model to: blotchy-Amado_trial_96/model_best.pth
[08/28/2025 03:16:16 INFO]: Training loss at epoch 12: 1.138369381427765
[08/28/2025 03:16:27 INFO]: Training loss at epoch 26: 1.3338626027107239
[08/28/2025 03:16:29 INFO]: Training loss at epoch 40: 0.988315761089325
[08/28/2025 03:16:31 INFO]: Training loss at epoch 22: 1.0401687622070312
[08/28/2025 03:16:34 INFO]: New best epoch, val score: -0.6620499848441096
[08/28/2025 03:16:34 INFO]: Saving model to: blotchy-Amado_trial_90/model_best.pth
[08/28/2025 03:16:47 INFO]: Training loss at epoch 31: 1.0424407720565796
[08/28/2025 03:17:00 INFO]: Training stats: {
    "score": -1.005133425949466,
    "rmse": 1.005133425949466
}
[08/28/2025 03:17:00 INFO]: Val stats: {
    "score": -0.7259493503142425,
    "rmse": 0.7259493503142425
}
[08/28/2025 03:17:00 INFO]: Test stats: {
    "score": -0.8990117926390928,
    "rmse": 0.8990117926390928
}
[08/28/2025 03:17:08 INFO]: Training loss at epoch 23: 1.0304324924945831
[08/28/2025 03:17:12 INFO]: Training loss at epoch 40: 1.0605472326278687
[08/28/2025 03:17:12 INFO]: Training loss at epoch 30: 0.9530547261238098
[08/28/2025 03:17:17 INFO]: Training loss at epoch 41: 0.9792643189430237
[08/28/2025 03:17:19 INFO]: Training loss at epoch 37: 1.1686419248580933
[08/28/2025 03:17:19 INFO]: Training loss at epoch 7: 1.3538926243782043
[08/28/2025 03:17:27 INFO]: Training loss at epoch 32: 1.0753903985023499
[08/28/2025 03:17:29 INFO]: New best epoch, val score: -0.6697635605098442
[08/28/2025 03:17:29 INFO]: Saving model to: blotchy-Amado_trial_96/model_best.pth
[08/28/2025 03:17:30 INFO]: Training loss at epoch 13: 1.0408002734184265
[08/28/2025 03:17:32 INFO]: Training loss at epoch 0: 1.194330632686615
[08/28/2025 03:17:45 INFO]: Training loss at epoch 24: 0.915103018283844
[08/28/2025 03:18:03 INFO]: Training loss at epoch 42: 0.9471195936203003
[08/28/2025 03:18:03 INFO]: Training loss at epoch 33: 0.959197461605072
[08/28/2025 03:18:07 INFO]: Training loss at epoch 39: 0.9890443980693817
[08/28/2025 03:18:07 INFO]: New best epoch, val score: -0.7249399181851984
[08/28/2025 03:18:07 INFO]: Saving model to: blotchy-Amado_trial_97/model_best.pth
[08/28/2025 03:18:20 INFO]: Training loss at epoch 34: 0.9308517575263977
[08/28/2025 03:18:22 INFO]: Training loss at epoch 25: 1.0835500359535217
[08/28/2025 03:18:32 INFO]: Training loss at epoch 8: 0.7507846802473068
[08/28/2025 03:18:41 INFO]: New best epoch, val score: -0.6696738939614664
[08/28/2025 03:18:41 INFO]: Saving model to: blotchy-Amado_trial_96/model_best.pth
[08/28/2025 03:18:42 INFO]: Training loss at epoch 34: 1.1681556403636932
[08/28/2025 03:18:43 INFO]: Training loss at epoch 14: 1.0386435985565186
[08/28/2025 03:18:47 INFO]: Running Final Evaluation...
[08/28/2025 03:18:47 INFO]: New best epoch, val score: -0.6514380783238921
[08/28/2025 03:18:47 INFO]: Saving model to: blotchy-Amado_trial_80/model_best.pth
[08/28/2025 03:18:50 INFO]: Training loss at epoch 43: 0.9447241127490997
[08/28/2025 03:18:59 INFO]: Training loss at epoch 34: 1.1306567192077637
[08/28/2025 03:18:59 INFO]: Training loss at epoch 26: 1.035375714302063
[08/28/2025 03:19:00 INFO]: Training accuracy: {
    "score": -1.0027883939733062,
    "rmse": 1.0027883939733062
}
[08/28/2025 03:19:00 INFO]: Val accuracy: {
    "score": -0.6711676457579877,
    "rmse": 0.6711676457579877
}
[08/28/2025 03:19:00 INFO]: Test accuracy: {
    "score": -0.8838928733889625,
    "rmse": 0.8838928733889625
}
[08/28/2025 03:19:00 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_93",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8838928733889625,
        "rmse": 0.8838928733889625
    },
    "train_stats": {
        "score": -1.0027883939733062,
        "rmse": 1.0027883939733062
    },
    "val_stats": {
        "score": -0.6711676457579877,
        "rmse": 0.6711676457579877
    }
}
[08/28/2025 03:19:00 INFO]: Procewss finished for trial blotchy-Amado_trial_93
[08/28/2025 03:19:00 INFO]: 
_________________________________________________

[08/28/2025 03:19:00 INFO]: train_net_for_optune.py main() running.
[08/28/2025 03:19:00 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 2.235629382874536
  attention_dropout: 0.32440573235756
  ffn_dropout: 0.32440573235756
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.0242320973802183e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_98

[08/28/2025 03:19:01 INFO]: This ft_transformer has 17.046 million parameters.
[08/28/2025 03:19:01 INFO]: Training will start at epoch 0.
[08/28/2025 03:19:01 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 03:19:12 INFO]: Training loss at epoch 59: 1.22089022397995
[08/28/2025 03:19:14 INFO]: Training loss at epoch 75: 0.9836675524711609
[08/28/2025 03:19:22 INFO]: Training loss at epoch 31: 0.9369716644287109
[08/28/2025 03:19:37 INFO]: Training loss at epoch 27: 1.3116042017936707
[08/28/2025 03:19:39 INFO]: Training loss at epoch 44: 0.9260555505752563
[08/28/2025 03:19:45 INFO]: Training loss at epoch 22: 1.1756506860256195
[08/28/2025 03:19:45 INFO]: Training loss at epoch 9: 0.9600025117397308
[08/28/2025 03:19:52 INFO]: Training stats: {
    "score": -1.0019642128949522,
    "rmse": 1.0019642128949522
}
[08/28/2025 03:19:52 INFO]: Val stats: {
    "score": -0.6642487061586876,
    "rmse": 0.6642487061586876
}
[08/28/2025 03:19:52 INFO]: Test stats: {
    "score": -0.869834531774707,
    "rmse": 0.869834531774707
}
[08/28/2025 03:19:58 INFO]: Training loss at epoch 15: 1.0004837214946747
[08/28/2025 03:20:06 INFO]: Training loss at epoch 27: 0.9755060076713562
[08/28/2025 03:20:10 INFO]: Training stats: {
    "score": -1.0141862875004382,
    "rmse": 1.0141862875004382
}
[08/28/2025 03:20:10 INFO]: Val stats: {
    "score": -0.6695788871003923,
    "rmse": 0.6695788871003923
}
[08/28/2025 03:20:10 INFO]: Test stats: {
    "score": -0.8810570029121242,
    "rmse": 0.8810570029121242
}
[08/28/2025 03:20:11 INFO]: Training stats: {
    "score": -1.0003514778465212,
    "rmse": 1.0003514778465212
}
[08/28/2025 03:20:11 INFO]: Val stats: {
    "score": -0.6648755007246195,
    "rmse": 0.6648755007246195
}
[08/28/2025 03:20:11 INFO]: Test stats: {
    "score": -0.8694254315460471,
    "rmse": 0.8694254315460471
}
[08/28/2025 03:20:12 INFO]: Training loss at epoch 28: 1.2566614747047424
[08/28/2025 03:20:19 INFO]: New best epoch, val score: -0.6695788871003923
[08/28/2025 03:20:19 INFO]: Saving model to: blotchy-Amado_trial_96/model_best.pth
[08/28/2025 03:20:23 INFO]: Training loss at epoch 45: 0.8940187394618988
[08/28/2025 03:20:33 INFO]: Training loss at epoch 2: 0.9219309389591217
[08/28/2025 03:20:38 INFO]: Training loss at epoch 60: 0.7122519612312317
[08/28/2025 03:20:49 INFO]: Training loss at epoch 29: 1.0119409263134003
[08/28/2025 03:20:50 INFO]: Training loss at epoch 41: 1.1875940561294556
[08/28/2025 03:20:56 INFO]: Training loss at epoch 38: 0.867086797952652
[08/28/2025 03:21:02 INFO]: Training stats: {
    "score": -0.9970465774329509,
    "rmse": 0.9970465774329509
}
[08/28/2025 03:21:02 INFO]: Val stats: {
    "score": -0.6887179900690121,
    "rmse": 0.6887179900690121
}
[08/28/2025 03:21:02 INFO]: Test stats: {
    "score": -0.8772566116696618,
    "rmse": 0.8772566116696618
}
[08/28/2025 03:21:11 INFO]: Training loss at epoch 16: 0.9894283413887024
[08/28/2025 03:21:12 INFO]: Training loss at epoch 46: 1.0814828276634216
[08/28/2025 03:21:16 INFO]: Running Final Evaluation...
[08/28/2025 03:21:23 INFO]: Training loss at epoch 10: 0.9146377444267273
[08/28/2025 03:21:26 INFO]: Training loss at epoch 14: 1.134596824645996
[08/28/2025 03:21:32 INFO]: Training loss at epoch 32: 1.1377328038215637
[08/28/2025 03:21:33 INFO]: New best epoch, val score: -0.6693532424668035
[08/28/2025 03:21:33 INFO]: Saving model to: blotchy-Amado_trial_96/model_best.pth
[08/28/2025 03:21:39 INFO]: Training loss at epoch 30: 1.057927668094635
[08/28/2025 03:21:51 INFO]: Training loss at epoch 35: 0.959758460521698
[08/28/2025 03:21:59 INFO]: Training loss at epoch 47: 0.9720601737499237
[08/28/2025 03:22:01 INFO]: Training loss at epoch 30: 1.2794153988361359
[08/28/2025 03:22:15 INFO]: Training loss at epoch 31: 0.8341409862041473
[08/28/2025 03:22:16 INFO]: Training loss at epoch 76: 1.064900517463684
[08/28/2025 03:22:16 INFO]: New best epoch, val score: -0.6509842184065382
[08/28/2025 03:22:16 INFO]: Saving model to: blotchy-Amado_trial_80/model_best.pth
[08/28/2025 03:22:24 INFO]: Training loss at epoch 17: 0.8191011846065521
[08/28/2025 03:22:30 INFO]: Training loss at epoch 35: 1.0076309144496918
[08/28/2025 03:22:30 INFO]: Training accuracy: {
    "score": -1.0016121497562018,
    "rmse": 1.0016121497562018
}
[08/28/2025 03:22:30 INFO]: Val accuracy: {
    "score": -0.6671844894695276,
    "rmse": 0.6671844894695276
}
[08/28/2025 03:22:30 INFO]: Test accuracy: {
    "score": -0.8787008154196417,
    "rmse": 0.8787008154196417
}
[08/28/2025 03:22:30 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_74",
    "best_epoch": 10,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8787008154196417,
        "rmse": 0.8787008154196417
    },
    "train_stats": {
        "score": -1.0016121497562018,
        "rmse": 1.0016121497562018
    },
    "val_stats": {
        "score": -0.6671844894695276,
        "rmse": 0.6671844894695276
    }
}
[08/28/2025 03:22:30 INFO]: Procewss finished for trial blotchy-Amado_trial_74
[08/28/2025 03:22:30 INFO]: 
_________________________________________________

[08/28/2025 03:22:30 INFO]: train_net_for_optune.py main() running.
[08/28/2025 03:22:30 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.243391230445143
  attention_dropout: 0.4254396448522544
  ffn_dropout: 0.4254396448522544
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.9812729503466026e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_99

[08/28/2025 03:22:31 INFO]: This ft_transformer has 11.445 million parameters.
[08/28/2025 03:22:31 INFO]: Training will start at epoch 0.
[08/28/2025 03:22:31 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 03:22:36 INFO]: Training loss at epoch 11: 1.113880693912506
[08/28/2025 03:22:41 INFO]: Training loss at epoch 1: 1.3137748837471008
[08/28/2025 03:22:45 INFO]: New best epoch, val score: -0.6692777194675054
[08/28/2025 03:22:45 INFO]: Saving model to: blotchy-Amado_trial_96/model_best.pth
[08/28/2025 03:22:45 INFO]: Training loss at epoch 48: 0.8843463659286499
[08/28/2025 03:22:52 INFO]: Training loss at epoch 32: 1.1247477531433105
[08/28/2025 03:22:57 INFO]: Running Final Evaluation...
[08/28/2025 03:23:10 INFO]: Training accuracy: {
    "score": -1.0420112084281377,
    "rmse": 1.0420112084281377
}
[08/28/2025 03:23:10 INFO]: Val accuracy: {
    "score": -0.6664198355030839,
    "rmse": 0.6664198355030839
}
[08/28/2025 03:23:10 INFO]: Test accuracy: {
    "score": -0.8902693146873436,
    "rmse": 0.8902693146873436
}
[08/28/2025 03:23:10 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_95",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8902693146873436,
        "rmse": 0.8902693146873436
    },
    "train_stats": {
        "score": -1.0420112084281377,
        "rmse": 1.0420112084281377
    },
    "val_stats": {
        "score": -0.6664198355030839,
        "rmse": 0.6664198355030839
    }
}
[08/28/2025 03:23:10 INFO]: Procewss finished for trial blotchy-Amado_trial_95
[08/28/2025 03:23:10 INFO]: 
_________________________________________________

[08/28/2025 03:23:10 INFO]: train_net_for_optune.py main() running.
[08/28/2025 03:23:10 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 2.1591827068953466
  attention_dropout: 0.3773384114759296
  ffn_dropout: 0.3773384114759296
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.215614153757459e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_100

[08/28/2025 03:23:10 INFO]: This ft_transformer has 16.686 million parameters.
[08/28/2025 03:23:10 INFO]: Training will start at epoch 0.
[08/28/2025 03:23:10 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 03:23:13 INFO]: Training loss at epoch 60: 0.9958273768424988
[08/28/2025 03:23:21 INFO]: Training loss at epoch 23: 1.0704404711723328
[08/28/2025 03:23:32 INFO]: Training loss at epoch 49: 0.8649351298809052
[08/28/2025 03:23:37 INFO]: Training loss at epoch 18: 0.9476954340934753
[08/28/2025 03:23:41 INFO]: Training loss at epoch 33: 1.1339009702205658
[08/28/2025 03:23:42 INFO]: Training loss at epoch 28: 0.898916482925415
[08/28/2025 03:23:49 INFO]: Training loss at epoch 12: 0.9303930699825287
[08/28/2025 03:23:50 INFO]: Training stats: {
    "score": -0.994337776727975,
    "rmse": 0.994337776727975
}
[08/28/2025 03:23:50 INFO]: Val stats: {
    "score": -0.6693651431724295,
    "rmse": 0.6693651431724295
}
[08/28/2025 03:23:50 INFO]: Test stats: {
    "score": -0.8669163044422685,
    "rmse": 0.8669163044422685
}
[08/28/2025 03:24:35 INFO]: Training loss at epoch 39: 0.975493848323822
[08/28/2025 03:24:36 INFO]: Training loss at epoch 50: 0.9619617164134979
[08/28/2025 03:24:49 INFO]: Training loss at epoch 19: 1.0686768889427185
[08/28/2025 03:25:01 INFO]: Training loss at epoch 13: 1.0723475813865662
[08/28/2025 03:25:06 INFO]: Training loss at epoch 40: 1.091244101524353
[08/28/2025 03:25:17 INFO]: Training stats: {
    "score": -0.9897901906967089,
    "rmse": 0.9897901906967089
}
[08/28/2025 03:25:17 INFO]: Val stats: {
    "score": -0.6650459399015786,
    "rmse": 0.6650459399015786
}
[08/28/2025 03:25:17 INFO]: Test stats: {
    "score": -0.8615901464520568,
    "rmse": 0.8615901464520568
}
[08/28/2025 03:25:18 INFO]: Training loss at epoch 77: 0.9227145612239838
[08/28/2025 03:25:21 INFO]: Training loss at epoch 36: 1.2513157427310944
[08/28/2025 03:25:23 INFO]: Training loss at epoch 51: 0.9961725771427155
[08/28/2025 03:25:45 INFO]: Training stats: {
    "score": -1.0004958075437542,
    "rmse": 1.0004958075437542
}
[08/28/2025 03:25:45 INFO]: Val stats: {
    "score": -0.6728086753762773,
    "rmse": 0.6728086753762773
}
[08/28/2025 03:25:45 INFO]: Test stats: {
    "score": -0.879698721521062,
    "rmse": 0.879698721521062
}
[08/28/2025 03:25:46 INFO]: New best epoch, val score: -0.6506937335466737
[08/28/2025 03:25:46 INFO]: Saving model to: blotchy-Amado_trial_80/model_best.pth
[08/28/2025 03:25:49 INFO]: Training loss at epoch 34: 1.0058381259441376
[08/28/2025 03:26:00 INFO]: Training loss at epoch 36: 0.8311552107334137
[08/28/2025 03:26:10 INFO]: Training loss at epoch 52: 0.8529094457626343
[08/28/2025 03:26:11 INFO]: New best epoch, val score: -0.6728086753762773
[08/28/2025 03:26:11 INFO]: Saving model to: blotchy-Amado_trial_75/model_best.pth
[08/28/2025 03:26:12 INFO]: Training loss at epoch 61: 1.3640851378440857
[08/28/2025 03:26:12 INFO]: Training loss at epoch 14: 0.9498392343521118
[08/28/2025 03:26:29 INFO]: Training loss at epoch 0: 1.1819431781768799
[08/28/2025 03:26:29 INFO]: Training loss at epoch 20: 0.916518896818161
[08/28/2025 03:26:55 INFO]: Training loss at epoch 24: 1.2539567649364471
[08/28/2025 03:26:58 INFO]: Training loss at epoch 53: 1.064263790845871
[08/28/2025 03:27:04 INFO]: Training loss at epoch 31: 0.868022233247757
[08/28/2025 03:27:06 INFO]: Training loss at epoch 0: 1.0759662687778473
[08/28/2025 03:27:17 INFO]: Training loss at epoch 29: 1.192750334739685
[08/28/2025 03:27:25 INFO]: Training loss at epoch 15: 1.2871598601341248
[08/28/2025 03:27:30 INFO]: New best epoch, val score: -0.8618681388619063
[08/28/2025 03:27:30 INFO]: Saving model to: blotchy-Amado_trial_98/model_best.pth
[08/28/2025 03:27:30 INFO]: Training loss at epoch 15: 1.3621833324432373
[08/28/2025 03:27:44 INFO]: New best epoch, val score: -1.0432380106776182
[08/28/2025 03:27:44 INFO]: Saving model to: blotchy-Amado_trial_99/model_best.pth
[08/28/2025 03:27:44 INFO]: Training loss at epoch 21: 1.015074759721756
[08/28/2025 03:27:46 INFO]: Training loss at epoch 54: 0.8066479563713074
[08/28/2025 03:27:52 INFO]: Training loss at epoch 2: 1.31205815076828
[08/28/2025 03:27:59 INFO]: Training loss at epoch 35: 1.1825332641601562
[08/28/2025 03:28:18 INFO]: Training loss at epoch 78: 0.9383285641670227
[08/28/2025 03:28:31 INFO]: Training loss at epoch 55: 1.0027025938034058
[08/28/2025 03:28:33 INFO]: Training stats: {
    "score": -1.0060737566669993,
    "rmse": 1.0060737566669993
}
[08/28/2025 03:28:33 INFO]: Val stats: {
    "score": -0.7254888463731662,
    "rmse": 0.7254888463731662
}
[08/28/2025 03:28:33 INFO]: Test stats: {
    "score": -0.9103002230452474,
    "rmse": 0.9103002230452474
}
[08/28/2025 03:28:37 INFO]: Training loss at epoch 16: 1.5229243636131287
[08/28/2025 03:28:49 INFO]: Training loss at epoch 37: 0.7947290539741516
[08/28/2025 03:28:55 INFO]: Training loss at epoch 22: 1.086905598640442
[08/28/2025 03:29:11 INFO]: Training loss at epoch 62: 0.9313332438468933
[08/28/2025 03:29:18 INFO]: Training loss at epoch 56: 0.9199877381324768
[08/28/2025 03:29:23 INFO]: Training loss at epoch 40: 1.0193933546543121
[08/28/2025 03:29:27 INFO]: Training loss at epoch 37: 0.8443825244903564
[08/28/2025 03:29:46 INFO]: New best epoch, val score: -0.6727716314167428
[08/28/2025 03:29:46 INFO]: Saving model to: blotchy-Amado_trial_75/model_best.pth
[08/28/2025 03:29:48 INFO]: Training loss at epoch 17: 1.1404833793640137
[08/28/2025 03:30:05 INFO]: Training loss at epoch 57: 0.9061270356178284
[08/28/2025 03:30:05 INFO]: Training loss at epoch 36: 1.135294884443283
[08/28/2025 03:30:07 INFO]: Training loss at epoch 23: 1.0424532890319824
[08/28/2025 03:30:15 INFO]: Training loss at epoch 41: 0.8368771970272064
[08/28/2025 03:30:27 INFO]: Training loss at epoch 0: 1.1900088787078857
[08/28/2025 03:30:28 INFO]: Training loss at epoch 25: 0.9412977397441864
[08/28/2025 03:30:51 INFO]: Training loss at epoch 58: 1.016512006521225
[08/28/2025 03:31:01 INFO]: Training loss at epoch 18: 0.9863274097442627
[08/28/2025 03:31:14 INFO]: Training loss at epoch 79: 0.9621936082839966
[08/28/2025 03:31:19 INFO]: Training loss at epoch 24: 0.964890867471695
[08/28/2025 03:31:26 INFO]: New best epoch, val score: -0.6537080419180912
[08/28/2025 03:31:26 INFO]: Saving model to: blotchy-Amado_trial_100/model_best.pth
[08/28/2025 03:31:39 INFO]: Training loss at epoch 59: 1.1671326160430908
[08/28/2025 03:31:54 INFO]: Training stats: {
    "score": -0.9905939745555844,
    "rmse": 0.9905939745555844
}
[08/28/2025 03:31:54 INFO]: Val stats: {
    "score": -0.6819458733644278,
    "rmse": 0.6819458733644278
}
[08/28/2025 03:31:54 INFO]: Test stats: {
    "score": -0.8727064656792508,
    "rmse": 0.8727064656792508
}
[08/28/2025 03:31:58 INFO]: Training loss at epoch 61: 0.8570627272129059
[08/28/2025 03:32:03 INFO]: Training loss at epoch 32: 1.234839677810669
[08/28/2025 03:32:07 INFO]: Training loss at epoch 30: 0.881990522146225
[08/28/2025 03:32:10 INFO]: Training loss at epoch 63: 1.0834141075611115
[08/28/2025 03:32:14 INFO]: Training loss at epoch 19: 1.1673387289047241
[08/28/2025 03:32:14 INFO]: Training loss at epoch 37: 0.9707417488098145
[08/28/2025 03:32:15 INFO]: Training stats: {
    "score": -1.0038950448642439,
    "rmse": 1.0038950448642439
}
[08/28/2025 03:32:15 INFO]: Val stats: {
    "score": -0.6635698456874992,
    "rmse": 0.6635698456874992
}
[08/28/2025 03:32:15 INFO]: Test stats: {
    "score": -0.8703091509297928,
    "rmse": 0.8703091509297928
}
[08/28/2025 03:32:15 INFO]: Training loss at epoch 1: 1.1647270321846008
[08/28/2025 03:32:17 INFO]: Training loss at epoch 38: 1.229948103427887
[08/28/2025 03:32:32 INFO]: Training loss at epoch 25: 0.8541505932807922
[08/28/2025 03:32:39 INFO]: Training stats: {
    "score": -1.0020557629913547,
    "rmse": 1.0020557629913547
}
[08/28/2025 03:32:39 INFO]: Val stats: {
    "score": -0.6737301008854789,
    "rmse": 0.6737301008854789
}
[08/28/2025 03:32:39 INFO]: Test stats: {
    "score": -0.880962301884168,
    "rmse": 0.880962301884168
}
[08/28/2025 03:32:41 INFO]: Training loss at epoch 60: 0.8282439112663269
[08/28/2025 03:32:50 INFO]: New best epoch, val score: -0.7045314320574806
[08/28/2025 03:32:50 INFO]: Saving model to: blotchy-Amado_trial_99/model_best.pth
[08/28/2025 03:32:53 INFO]: Training loss at epoch 3: 0.8384169638156891
[08/28/2025 03:32:56 INFO]: Training loss at epoch 3: 0.9882740080356598
[08/28/2025 03:32:58 INFO]: Training loss at epoch 41: 0.9605438709259033
[08/28/2025 03:32:59 INFO]: Training loss at epoch 38: 1.0061129927635193
[08/28/2025 03:33:28 INFO]: Training loss at epoch 61: 0.9845452606678009
[08/28/2025 03:33:29 INFO]: Training loss at epoch 16: 1.045656979084015
[08/28/2025 03:33:33 INFO]: New best epoch, val score: -0.7190776384275851
[08/28/2025 03:33:33 INFO]: Saving model to: blotchy-Amado_trial_97/model_best.pth
[08/28/2025 03:33:45 INFO]: Training loss at epoch 26: 0.9024142920970917
[08/28/2025 03:33:52 INFO]: Training loss at epoch 20: 1.0912979245185852
[08/28/2025 03:34:01 INFO]: Training loss at epoch 26: 0.9722363352775574
[08/28/2025 03:34:13 INFO]: New best epoch, val score: -0.6712979420359151
[08/28/2025 03:34:13 INFO]: Saving model to: blotchy-Amado_trial_83/model_best.pth
[08/28/2025 03:34:15 INFO]: Training loss at epoch 62: 0.8860251307487488
[08/28/2025 03:34:21 INFO]: Training loss at epoch 38: 0.9983372390270233
[08/28/2025 03:34:57 INFO]: Training loss at epoch 27: 1.1097734868526459
[08/28/2025 03:34:59 INFO]: Training loss at epoch 1: 1.2358351945877075
[08/28/2025 03:35:04 INFO]: Training loss at epoch 63: 1.0664023458957672
[08/28/2025 03:35:05 INFO]: Training loss at epoch 21: 1.0088850557804108
[08/28/2025 03:35:10 INFO]: Training loss at epoch 64: 1.0788384079933167
[08/28/2025 03:35:14 INFO]: Training loss at epoch 80: 0.932953417301178
[08/28/2025 03:35:25 INFO]: Training loss at epoch 42: 0.9442163407802582
[08/28/2025 03:35:39 INFO]: Training loss at epoch 31: 0.9473845958709717
[08/28/2025 03:35:43 INFO]: Training loss at epoch 39: 0.8928882777690887
[08/28/2025 03:35:49 INFO]: Training loss at epoch 64: 0.966607391834259
[08/28/2025 03:36:07 INFO]: Training loss at epoch 28: 0.8722712695598602
[08/28/2025 03:36:14 INFO]: Training loss at epoch 22: 1.353398859500885
[08/28/2025 03:36:28 INFO]: Training loss at epoch 39: 0.902332216501236
[08/28/2025 03:36:29 INFO]: Training loss at epoch 39: 0.8295031785964966
[08/28/2025 03:36:35 INFO]: Training loss at epoch 65: 0.9573394954204559
[08/28/2025 03:36:36 INFO]: Training loss at epoch 42: 1.0120099186897278
[08/28/2025 03:36:57 INFO]: Training stats: {
    "score": -1.0100604518493348,
    "rmse": 1.0100604518493348
}
[08/28/2025 03:36:57 INFO]: Val stats: {
    "score": -0.6518915794903547,
    "rmse": 0.6518915794903547
}
[08/28/2025 03:36:57 INFO]: Test stats: {
    "score": -0.8850622377087771,
    "rmse": 0.8850622377087771
}
[08/28/2025 03:37:00 INFO]: Training loss at epoch 33: 0.9686761498451233
[08/28/2025 03:37:13 INFO]: Training stats: {
    "score": -0.9770457418841925,
    "rmse": 0.9770457418841925
}
[08/28/2025 03:37:13 INFO]: Val stats: {
    "score": -0.6788231621766371,
    "rmse": 0.6788231621766371
}
[08/28/2025 03:37:13 INFO]: Test stats: {
    "score": -0.8719579434825087,
    "rmse": 0.8719579434825087
}
[08/28/2025 03:37:20 INFO]: Training loss at epoch 29: 0.8727157115936279
[08/28/2025 03:37:22 INFO]: Training loss at epoch 66: 1.1626628637313843
[08/28/2025 03:37:23 INFO]: Training loss at epoch 2: 1.188168704509735
[08/28/2025 03:37:27 INFO]: Training loss at epoch 23: 0.981795459985733
[08/28/2025 03:37:33 INFO]: Training loss at epoch 27: 1.0993964672088623
[08/28/2025 03:37:44 INFO]: Training stats: {
    "score": -1.000723481313592,
    "rmse": 1.000723481313592
}
[08/28/2025 03:37:44 INFO]: Val stats: {
    "score": -0.6657499074657,
    "rmse": 0.6657499074657
}
[08/28/2025 03:37:44 INFO]: Test stats: {
    "score": -0.8628905807158853,
    "rmse": 0.8628905807158853
}
[08/28/2025 03:37:45 INFO]: Training stats: {
    "score": -0.9860712076399287,
    "rmse": 0.9860712076399287
}
[08/28/2025 03:37:45 INFO]: Val stats: {
    "score": -0.6678397775097993,
    "rmse": 0.6678397775097993
}
[08/28/2025 03:37:45 INFO]: Test stats: {
    "score": -0.8636420428248963,
    "rmse": 0.8636420428248963
}
[08/28/2025 03:37:58 INFO]: New best epoch, val score: -0.658999141394759
[08/28/2025 03:37:58 INFO]: Saving model to: blotchy-Amado_trial_85/model_best.pth
[08/28/2025 03:38:02 INFO]: Training loss at epoch 4: 1.0869361758232117
[08/28/2025 03:38:04 INFO]: Training loss at epoch 65: 1.2077483236789703
[08/28/2025 03:38:06 INFO]: Training loss at epoch 67: 1.0201746225357056
[08/28/2025 03:38:07 INFO]: Training loss at epoch 81: 1.123193621635437
[08/28/2025 03:38:38 INFO]: Training loss at epoch 24: 0.851830005645752
[08/28/2025 03:38:39 INFO]: New best epoch, val score: -0.6907670510888985
[08/28/2025 03:38:39 INFO]: Saving model to: blotchy-Amado_trial_97/model_best.pth
[08/28/2025 03:38:42 INFO]: Training loss at epoch 1: 1.3576166033744812
[08/28/2025 03:38:54 INFO]: Training loss at epoch 68: 0.9661804735660553
[08/28/2025 03:38:59 INFO]: Training loss at epoch 30: 0.954616367816925
[08/28/2025 03:39:09 INFO]: Training loss at epoch 32: 0.9044165909290314
[08/28/2025 03:39:18 INFO]: Training loss at epoch 40: 0.8033190965652466
[08/28/2025 03:39:27 INFO]: Training loss at epoch 17: 0.9237704575061798
[08/28/2025 03:39:39 INFO]: Training loss at epoch 69: 1.002950131893158
[08/28/2025 03:39:48 INFO]: Training loss at epoch 25: 1.1130973100662231
[08/28/2025 03:39:56 INFO]: Training stats: {
    "score": -0.9894536893273789,
    "rmse": 0.9894536893273789
}
[08/28/2025 03:39:56 INFO]: Val stats: {
    "score": -0.6694768412007276,
    "rmse": 0.6694768412007276
}
[08/28/2025 03:39:56 INFO]: Test stats: {
    "score": -0.8676422164889679,
    "rmse": 0.8676422164889679
}
[08/28/2025 03:40:08 INFO]: Training loss at epoch 43: 0.9268468916416168
[08/28/2025 03:40:10 INFO]: New best epoch, val score: -0.6656390575109823
[08/28/2025 03:40:10 INFO]: Saving model to: blotchy-Amado_trial_83/model_best.pth
[08/28/2025 03:40:11 INFO]: Training loss at epoch 31: 0.8808881342411041
[08/28/2025 03:40:24 INFO]: Training loss at epoch 40: 1.27411687374115
[08/28/2025 03:40:29 INFO]: Training loss at epoch 43: 0.8916831612586975
[08/28/2025 03:40:40 INFO]: Training loss at epoch 70: 0.9158200919628143
[08/28/2025 03:40:59 INFO]: Training loss at epoch 66: 1.0153233706951141
[08/28/2025 03:40:59 INFO]: Training loss at epoch 26: 1.131667137145996
[08/28/2025 03:41:02 INFO]: Training loss at epoch 82: 1.1748343706130981
[08/28/2025 03:41:07 INFO]: Training loss at epoch 28: 0.8649848103523254
[08/28/2025 03:41:08 INFO]: Training loss at epoch 40: 1.0671411156654358
[08/28/2025 03:41:20 INFO]: Training loss at epoch 32: 1.0447852313518524
[08/28/2025 03:41:22 INFO]: Training loss at epoch 41: 0.8481373488903046
[08/28/2025 03:41:24 INFO]: Training loss at epoch 71: 1.065861701965332
[08/28/2025 03:41:29 INFO]: Running Final Evaluation...
[08/28/2025 03:41:32 INFO]: New best epoch, val score: -0.6574247034765026
[08/28/2025 03:41:32 INFO]: Saving model to: blotchy-Amado_trial_85/model_best.pth
[08/28/2025 03:41:45 INFO]: Training accuracy: {
    "score": -1.0018201882017992,
    "rmse": 1.0018201882017992
}
[08/28/2025 03:41:45 INFO]: Val accuracy: {
    "score": -0.6620499848441096,
    "rmse": 0.6620499848441096
}
[08/28/2025 03:41:45 INFO]: Test accuracy: {
    "score": -0.8667152373105524,
    "rmse": 0.8667152373105524
}
[08/28/2025 03:41:45 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_90",
    "best_epoch": 40,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8667152373105524,
        "rmse": 0.8667152373105524
    },
    "train_stats": {
        "score": -1.0018201882017992,
        "rmse": 1.0018201882017992
    },
    "val_stats": {
        "score": -0.6620499848441096,
        "rmse": 0.6620499848441096
    }
}
[08/28/2025 03:41:45 INFO]: Procewss finished for trial blotchy-Amado_trial_90
[08/28/2025 03:41:45 INFO]: 
_________________________________________________

[08/28/2025 03:41:45 INFO]: train_net_for_optune.py main() running.
[08/28/2025 03:41:45 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 16
  d_ffn_factor: 1.4391424086838986
  attention_dropout: 0.4244843132741854
  ffn_dropout: 0.4244843132741854
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.6425102390245705e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_101

[08/28/2025 03:41:45 INFO]: This ft_transformer has 22.022 million parameters.
[08/28/2025 03:41:45 INFO]: Training will start at epoch 0.
[08/28/2025 03:41:45 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 03:41:51 INFO]: Training loss at epoch 34: 1.0457537770271301
[08/28/2025 03:42:10 INFO]: Training loss at epoch 27: 1.227483332157135
[08/28/2025 03:42:23 INFO]: Training loss at epoch 3: 0.9689990282058716
[08/28/2025 03:42:31 INFO]: Training loss at epoch 33: 0.8805221915245056
[08/28/2025 03:42:36 INFO]: Training loss at epoch 33: 0.9806979298591614
[08/28/2025 03:42:41 INFO]: Running Final Evaluation...
[08/28/2025 03:43:01 INFO]: Training loss at epoch 62: 0.6531753242015839
[08/28/2025 03:43:03 INFO]: Training loss at epoch 5: 0.966689258813858
[08/28/2025 03:43:07 INFO]: Training accuracy: {
    "score": -1.0082132356569944,
    "rmse": 1.0082132356569944
}
[08/28/2025 03:43:07 INFO]: Val accuracy: {
    "score": -0.6541032817121915,
    "rmse": 0.6541032817121915
}
[08/28/2025 03:43:07 INFO]: Test accuracy: {
    "score": -0.8592367826689263,
    "rmse": 0.8592367826689263
}
[08/28/2025 03:43:07 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_94",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8592367826689263,
        "rmse": 0.8592367826689263
    },
    "train_stats": {
        "score": -1.0082132356569944,
        "rmse": 1.0082132356569944
    },
    "val_stats": {
        "score": -0.6541032817121915,
        "rmse": 0.6541032817121915
    }
}
[08/28/2025 03:43:07 INFO]: Procewss finished for trial blotchy-Amado_trial_94
[08/28/2025 03:43:07 INFO]: 
_________________________________________________

[08/28/2025 03:43:07 INFO]: train_net_for_optune.py main() running.
[08/28/2025 03:43:07 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 1.5533403066446145
  attention_dropout: 0.03726276027927312
  ffn_dropout: 0.03726276027927312
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.2265670413291197e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_102

[08/28/2025 03:43:07 INFO]: This ft_transformer has 9.274 million parameters.
[08/28/2025 03:43:07 INFO]: Training will start at epoch 0.
[08/28/2025 03:43:07 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 03:43:16 INFO]: Training loss at epoch 2: 1.4583991765975952
[08/28/2025 03:43:23 INFO]: Training loss at epoch 28: 1.3698067665100098
[08/28/2025 03:43:28 INFO]: Training loss at epoch 42: 1.1732203662395477
[08/28/2025 03:43:38 INFO]: Training loss at epoch 44: 0.9106167554855347
[08/28/2025 03:43:49 INFO]: Training loss at epoch 41: 0.9225205183029175
[08/28/2025 03:43:53 INFO]: Training loss at epoch 67: 1.026817113161087
[08/28/2025 03:43:55 INFO]: Training loss at epoch 83: 1.137936532497406
[08/28/2025 03:44:17 INFO]: New best epoch, val score: -0.6500542783284797
[08/28/2025 03:44:17 INFO]: Saving model to: blotchy-Amado_trial_80/model_best.pth
[08/28/2025 03:44:33 INFO]: Training loss at epoch 41: 0.9824098348617554
[08/28/2025 03:44:35 INFO]: Training loss at epoch 29: 0.923736184835434
[08/28/2025 03:44:40 INFO]: Training loss at epoch 29: 0.8399302363395691
[08/28/2025 03:45:00 INFO]: Training stats: {
    "score": -0.9994299393811277,
    "rmse": 0.9994299393811277
}
[08/28/2025 03:45:00 INFO]: Val stats: {
    "score": -0.6859238623774013,
    "rmse": 0.6859238623774013
}
[08/28/2025 03:45:00 INFO]: Test stats: {
    "score": -0.8876938051282777,
    "rmse": 0.8876938051282777
}
[08/28/2025 03:45:03 INFO]: Training loss at epoch 4: 0.9489249885082245
[08/28/2025 03:45:20 INFO]: Training loss at epoch 18: 1.038152426481247
[08/28/2025 03:45:33 INFO]: Training loss at epoch 44: 1.4066322147846222
[08/28/2025 03:45:36 INFO]: Training loss at epoch 43: 1.0718713104724884
[08/28/2025 03:45:56 INFO]: Training stats: {
    "score": -0.994080275779302,
    "rmse": 0.994080275779302
}
[08/28/2025 03:45:56 INFO]: Val stats: {
    "score": -0.6581225445228259,
    "rmse": 0.6581225445228259
}
[08/28/2025 03:45:56 INFO]: Test stats: {
    "score": -0.8722722460359381,
    "rmse": 0.8722722460359381
}
[08/28/2025 03:46:04 INFO]: New best epoch, val score: -0.6644064597336625
[08/28/2025 03:46:04 INFO]: Saving model to: blotchy-Amado_trial_83/model_best.pth
[08/28/2025 03:46:07 INFO]: Training loss at epoch 34: 0.8767210245132446
[08/28/2025 03:46:13 INFO]: Training loss at epoch 30: 1.2933649718761444
[08/28/2025 03:46:50 INFO]: Training loss at epoch 2: 0.9952934682369232
[08/28/2025 03:46:51 INFO]: Training loss at epoch 35: 1.1149475574493408
[08/28/2025 03:46:54 INFO]: Training loss at epoch 68: 0.8696806132793427
[08/28/2025 03:46:55 INFO]: Training loss at epoch 84: 1.397287279367447
[08/28/2025 03:47:00 INFO]: Training loss at epoch 0: 0.9794564843177795
[08/28/2025 03:47:17 INFO]: Training loss at epoch 45: 1.188325047492981
[08/28/2025 03:47:20 INFO]: Training loss at epoch 42: 1.1248286962509155
[08/28/2025 03:47:25 INFO]: Training loss at epoch 31: 0.9784879684448242
[08/28/2025 03:47:33 INFO]: New best epoch, val score: -0.6852360809220743
[08/28/2025 03:47:33 INFO]: Saving model to: blotchy-Amado_trial_102/model_best.pth
[08/28/2025 03:47:33 INFO]: Training loss at epoch 4: 1.1699652671813965
[08/28/2025 03:47:44 INFO]: Training loss at epoch 44: 0.9515590667724609
[08/28/2025 03:48:01 INFO]: Training loss at epoch 42: 0.8932738304138184
[08/28/2025 03:48:01 INFO]: Running Final Evaluation...
[08/28/2025 03:48:10 INFO]: Training loss at epoch 6: 1.0569684207439423
[08/28/2025 03:48:36 INFO]: Training loss at epoch 32: 1.0523670315742493
[08/28/2025 03:48:44 INFO]: Training accuracy: {
    "score": -0.9951482094590663,
    "rmse": 0.9951482094590663
}
[08/28/2025 03:48:44 INFO]: Val accuracy: {
    "score": -0.6717729312834745,
    "rmse": 0.6717729312834745
}
[08/28/2025 03:48:44 INFO]: Test accuracy: {
    "score": -0.8741643239789358,
    "rmse": 0.8741643239789358
}
[08/28/2025 03:48:44 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_87",
    "best_epoch": 13,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8741643239789358,
        "rmse": 0.8741643239789358
    },
    "train_stats": {
        "score": -0.9951482094590663,
        "rmse": 0.9951482094590663
    },
    "val_stats": {
        "score": -0.6717729312834745,
        "rmse": 0.6717729312834745
    }
}
[08/28/2025 03:48:44 INFO]: Procewss finished for trial blotchy-Amado_trial_87
[08/28/2025 03:48:44 INFO]: 
_________________________________________________

[08/28/2025 03:48:44 INFO]: train_net_for_optune.py main() running.
[08/28/2025 03:48:44 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.198173973423705
  attention_dropout: 0.03317796417223756
  ffn_dropout: 0.03317796417223756
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.2629806133310301e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_103

[08/28/2025 03:48:44 INFO]: This ft_transformer has 11.304 million parameters.
[08/28/2025 03:48:44 INFO]: Training will start at epoch 0.
[08/28/2025 03:48:44 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 03:49:29 INFO]: Training loss at epoch 30: 1.2280964851379395
[08/28/2025 03:49:41 INFO]: Training loss at epoch 35: 1.2872113585472107
[08/28/2025 03:49:46 INFO]: Training loss at epoch 33: 1.144132912158966
[08/28/2025 03:49:49 INFO]: Training loss at epoch 69: 0.9205923080444336
[08/28/2025 03:49:51 INFO]: Training loss at epoch 85: 1.1681465804576874
[08/28/2025 03:50:39 INFO]: Training loss at epoch 45: 1.1377984285354614
[08/28/2025 03:50:49 INFO]: Training stats: {
    "score": -0.9969286745370333,
    "rmse": 0.9969286745370333
}
[08/28/2025 03:50:49 INFO]: Val stats: {
    "score": -0.6715574953036504,
    "rmse": 0.6715574953036504
}
[08/28/2025 03:50:49 INFO]: Test stats: {
    "score": -0.870368894925574,
    "rmse": 0.870368894925574
}
[08/28/2025 03:50:50 INFO]: Training loss at epoch 43: 1.0483251810073853
[08/28/2025 03:50:50 INFO]: Training loss at epoch 46: 0.9631442427635193
[08/28/2025 03:50:59 INFO]: Training loss at epoch 34: 1.2956531047821045
[08/28/2025 03:51:25 INFO]: Training loss at epoch 43: 0.8924974799156189
[08/28/2025 03:51:26 INFO]: Training loss at epoch 19: 1.036022663116455
[08/28/2025 03:51:28 INFO]: Training loss at epoch 1: 1.1278694868087769
[08/28/2025 03:51:37 INFO]: Training loss at epoch 3: 1.1701070368289948
[08/28/2025 03:51:48 INFO]: Training loss at epoch 36: 1.131590723991394
[08/28/2025 03:52:10 INFO]: Training loss at epoch 35: 0.9926881492137909
[08/28/2025 03:52:36 INFO]: New best epoch, val score: -0.6938447618377798
[08/28/2025 03:52:36 INFO]: Saving model to: blotchy-Amado_trial_98/model_best.pth
[08/28/2025 03:52:37 INFO]: Training loss at epoch 0: 0.9028660356998444
[08/28/2025 03:52:41 INFO]: Training loss at epoch 5: 1.0824357271194458
[08/28/2025 03:52:48 INFO]: Training loss at epoch 86: 1.0054675340652466
[08/28/2025 03:53:06 INFO]: Training loss at epoch 31: 0.9638882279396057
[08/28/2025 03:53:12 INFO]: Training loss at epoch 36: 1.2912091612815857
[08/28/2025 03:53:12 INFO]: Training loss at epoch 0: 0.8285614252090454
[08/28/2025 03:53:13 INFO]: Training loss at epoch 7: 0.7888587415218353
[08/28/2025 03:53:23 INFO]: Training loss at epoch 36: 1.0604711174964905
[08/28/2025 03:53:29 INFO]: Training stats: {
    "score": -0.9964128659269827,
    "rmse": 0.9964128659269827
}
[08/28/2025 03:53:29 INFO]: Val stats: {
    "score": -0.6645857578934551,
    "rmse": 0.6645857578934551
}
[08/28/2025 03:53:29 INFO]: Test stats: {
    "score": -0.8810387356068233,
    "rmse": 0.8810387356068233
}
[08/28/2025 03:53:46 INFO]: Training loss at epoch 70: 0.9240497648715973
[08/28/2025 03:53:47 INFO]: New best epoch, val score: -0.6716731645619874
[08/28/2025 03:53:47 INFO]: Saving model to: blotchy-Amado_trial_103/model_best.pth
[08/28/2025 03:54:08 INFO]: New best epoch, val score: -0.7211421636442439
[08/28/2025 03:54:08 INFO]: Saving model to: blotchy-Amado_trial_101/model_best.pth
[08/28/2025 03:54:15 INFO]: Training loss at epoch 63: 0.5752230286598206
[08/28/2025 03:54:22 INFO]: Training loss at epoch 44: 1.0254017114639282
[08/28/2025 03:54:26 INFO]: Training loss at epoch 47: 0.9037451446056366
[08/28/2025 03:54:35 INFO]: Training loss at epoch 37: 1.152860701084137
[08/28/2025 03:54:53 INFO]: Training loss at epoch 44: 1.2101581394672394
[08/28/2025 03:55:02 INFO]: Training loss at epoch 3: 1.1283929347991943
[08/28/2025 03:55:46 INFO]: Training loss at epoch 38: 1.1268631219863892
[08/28/2025 03:55:47 INFO]: Training loss at epoch 87: 0.8259584605693817
[08/28/2025 03:55:49 INFO]: Training loss at epoch 46: 0.9438859820365906
[08/28/2025 03:55:52 INFO]: Training loss at epoch 2: 0.9905555546283722
[08/28/2025 03:56:40 INFO]: Training loss at epoch 32: 0.9199364185333252
[08/28/2025 03:56:42 INFO]: Training loss at epoch 37: 0.9998137950897217
[08/28/2025 03:56:47 INFO]: Training loss at epoch 71: 1.039516568183899
[08/28/2025 03:56:48 INFO]: Training loss at epoch 37: 1.0232086181640625
[08/28/2025 03:56:58 INFO]: Training loss at epoch 39: 1.1273152828216553
[08/28/2025 03:57:17 INFO]: Training loss at epoch 5: 0.9858386516571045
[08/28/2025 03:57:23 INFO]: Training stats: {
    "score": -0.9988024587531787,
    "rmse": 0.9988024587531787
}
[08/28/2025 03:57:23 INFO]: Val stats: {
    "score": -0.6850677967119272,
    "rmse": 0.6850677967119272
}
[08/28/2025 03:57:23 INFO]: Test stats: {
    "score": -0.8878081912076772,
    "rmse": 0.8878081912076772
}
[08/28/2025 03:57:47 INFO]: Training loss at epoch 6: 1.1002704203128815
[08/28/2025 03:57:48 INFO]: Training loss at epoch 45: 0.8619407415390015
[08/28/2025 03:57:59 INFO]: Training loss at epoch 48: 1.148317575454712
[08/28/2025 03:58:19 INFO]: Training loss at epoch 45: 0.9664925336837769
[08/28/2025 03:58:19 INFO]: Training loss at epoch 1: 0.993769109249115
[08/28/2025 03:58:19 INFO]: Training loss at epoch 8: 0.8875087797641754
[08/28/2025 03:58:34 INFO]: Training loss at epoch 40: 1.0117606222629547
[08/28/2025 03:58:44 INFO]: Training loss at epoch 88: 1.29880753159523
[08/28/2025 03:59:26 INFO]: Training loss at epoch 20: 0.847183883190155
[08/28/2025 03:59:45 INFO]: Training loss at epoch 72: 0.8178110718727112
[08/28/2025 03:59:46 INFO]: Training loss at epoch 41: 1.0151883661746979
[08/28/2025 03:59:57 INFO]: Training loss at epoch 4: 1.1465359926223755
[08/28/2025 04:00:13 INFO]: Training loss at epoch 33: 0.8697617053985596
[08/28/2025 04:00:13 INFO]: Training loss at epoch 38: 1.0044077634811401
[08/28/2025 04:00:17 INFO]: Training loss at epoch 3: 0.9504845440387726
[08/28/2025 04:00:58 INFO]: Training loss at epoch 47: 1.1050282716751099
[08/28/2025 04:00:58 INFO]: Training loss at epoch 42: 0.8432841300964355
[08/28/2025 04:01:07 INFO]: Running Final Evaluation...
[08/28/2025 04:01:14 INFO]: Training loss at epoch 46: 1.0059386789798737
[08/28/2025 04:01:33 INFO]: Training accuracy: {
    "score": -1.010818625564747,
    "rmse": 1.010818625564747
}
[08/28/2025 04:01:33 INFO]: Val accuracy: {
    "score": -0.6692777194675054,
    "rmse": 0.6692777194675054
}
[08/28/2025 04:01:33 INFO]: Test accuracy: {
    "score": -0.8800938572199326,
    "rmse": 0.8800938572199326
}
[08/28/2025 04:01:33 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_96",
    "best_epoch": 11,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8800938572199326,
        "rmse": 0.8800938572199326
    },
    "train_stats": {
        "score": -1.010818625564747,
        "rmse": 1.010818625564747
    },
    "val_stats": {
        "score": -0.6692777194675054,
        "rmse": 0.6692777194675054
    }
}
[08/28/2025 04:01:33 INFO]: Procewss finished for trial blotchy-Amado_trial_96
[08/28/2025 04:01:33 INFO]: 
_________________________________________________

[08/28/2025 04:01:33 INFO]: train_net_for_optune.py main() running.
[08/28/2025 04:01:33 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 1.370972139213052
  attention_dropout: 0.036523102134392874
  ffn_dropout: 0.036523102134392874
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.1465737704655585e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_104

[08/28/2025 04:01:33 INFO]: This ft_transformer has 12.958 million parameters.
[08/28/2025 04:01:33 INFO]: Training will start at epoch 0.
[08/28/2025 04:01:33 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 04:01:38 INFO]: Training loss at epoch 49: 1.0341469943523407
[08/28/2025 04:01:42 INFO]: Training loss at epoch 89: 1.1608633399009705
[08/28/2025 04:01:46 INFO]: Training loss at epoch 46: 0.9380376935005188
[08/28/2025 04:01:46 INFO]: Training loss at epoch 38: 0.9992301762104034
[08/28/2025 04:02:40 INFO]: Training stats: {
    "score": -1.0234975475791692,
    "rmse": 1.0234975475791692
}
[08/28/2025 04:02:40 INFO]: Val stats: {
    "score": -0.6655081995269945,
    "rmse": 0.6655081995269945
}
[08/28/2025 04:02:40 INFO]: Test stats: {
    "score": -0.8801469874485549,
    "rmse": 0.8801469874485549
}
[08/28/2025 04:02:44 INFO]: Training loss at epoch 73: 1.061643898487091
[08/28/2025 04:02:49 INFO]: Training stats: {
    "score": -0.9837879183159964,
    "rmse": 0.9837879183159964
}
[08/28/2025 04:02:49 INFO]: Val stats: {
    "score": -0.6758776185845343,
    "rmse": 0.6758776185845343
}
[08/28/2025 04:02:49 INFO]: Test stats: {
    "score": -0.8824696675799436,
    "rmse": 0.8824696675799436
}
[08/28/2025 04:02:56 INFO]: Training loss at epoch 7: 0.9835239350795746
[08/28/2025 04:03:17 INFO]: Training loss at epoch 4: 0.9707205295562744
[08/28/2025 04:03:27 INFO]: Training loss at epoch 2: 1.1225454807281494
[08/28/2025 04:03:28 INFO]: Training loss at epoch 9: 1.0457855463027954
[08/28/2025 04:03:49 INFO]: Training loss at epoch 39: 1.003001868724823
[08/28/2025 04:03:50 INFO]: Training loss at epoch 34: 1.024224191904068
[08/28/2025 04:04:40 INFO]: Training loss at epoch 47: 1.0805488228797913
[08/28/2025 04:04:43 INFO]: Training loss at epoch 4: 0.92031529545784
[08/28/2025 04:05:00 INFO]: Training loss at epoch 1: 1.320939302444458
[08/28/2025 04:05:04 INFO]: Training stats: {
    "score": -1.0020383847108318,
    "rmse": 1.0020383847108318
}
[08/28/2025 04:05:04 INFO]: Val stats: {
    "score": -0.7215822987112265,
    "rmse": 0.7215822987112265
}
[08/28/2025 04:05:04 INFO]: Test stats: {
    "score": -0.9044999133825872,
    "rmse": 0.9044999133825872
}
[08/28/2025 04:05:08 INFO]: Training stats: {
    "score": -0.9716912562993436,
    "rmse": 0.9716912562993436
}
[08/28/2025 04:05:08 INFO]: Val stats: {
    "score": -0.7083346475180156,
    "rmse": 0.7083346475180156
}
[08/28/2025 04:05:08 INFO]: Test stats: {
    "score": -0.8934572374311222,
    "rmse": 0.8934572374311222
}
[08/28/2025 04:05:10 INFO]: Training loss at epoch 47: 1.2806839346885681
[08/28/2025 04:05:13 INFO]: New best epoch, val score: -0.6694849609685222
[08/28/2025 04:05:13 INFO]: Saving model to: blotchy-Amado_trial_102/model_best.pth
[08/28/2025 04:05:23 INFO]: Training loss at epoch 64: 0.5576895773410797
[08/28/2025 04:05:27 INFO]: Training loss at epoch 21: 1.2733953297138214
[08/28/2025 04:05:32 INFO]: Training loss at epoch 90: 1.1122415661811829
[08/28/2025 04:05:37 INFO]: Training loss at epoch 74: 1.063364326953888
[08/28/2025 04:06:06 INFO]: Training loss at epoch 48: 0.973144918680191
[08/28/2025 04:06:21 INFO]: Training loss at epoch 50: 1.156866729259491
[08/28/2025 04:06:31 INFO]: New best epoch, val score: -0.6779990191287657
[08/28/2025 04:06:31 INFO]: Saving model to: blotchy-Amado_trial_101/model_best.pth
[08/28/2025 04:06:41 INFO]: Training loss at epoch 39: 0.9196591675281525
[08/28/2025 04:07:20 INFO]: Training loss at epoch 35: 0.8230662941932678
[08/28/2025 04:07:32 INFO]: Training loss at epoch 0: 1.0180207192897797
[08/28/2025 04:08:01 INFO]: Training loss at epoch 8: 1.1232715249061584
[08/28/2025 04:08:02 INFO]: Training loss at epoch 48: 0.8837850689888
[08/28/2025 04:08:20 INFO]: Training stats: {
    "score": -0.9984261440964192,
    "rmse": 0.9984261440964192
}
[08/28/2025 04:08:20 INFO]: Val stats: {
    "score": -0.7017032454917346,
    "rmse": 0.7017032454917346
}
[08/28/2025 04:08:20 INFO]: Test stats: {
    "score": -0.8844231662683518,
    "rmse": 0.8844231662683518
}
[08/28/2025 04:08:21 INFO]: New best epoch, val score: -0.6481733996311175
[08/28/2025 04:08:21 INFO]: Saving model to: blotchy-Amado_trial_104/model_best.pth
[08/28/2025 04:08:22 INFO]: Training loss at epoch 5: 1.245321810245514
[08/28/2025 04:08:28 INFO]: Training loss at epoch 3: 0.9287537038326263
[08/28/2025 04:08:30 INFO]: Training loss at epoch 91: 1.0053217709064484
[08/28/2025 04:08:34 INFO]: Training loss at epoch 40: 1.077117145061493
[08/28/2025 04:08:34 INFO]: Training loss at epoch 48: 1.057065725326538
[08/28/2025 04:08:35 INFO]: Training loss at epoch 75: 1.0536325573921204
[08/28/2025 04:09:05 INFO]: Training loss at epoch 5: 0.9103581607341766
[08/28/2025 04:09:35 INFO]: Training loss at epoch 6: 0.940505176782608
[08/28/2025 04:09:38 INFO]: New best epoch, val score: -0.6686928489143797
[08/28/2025 04:09:38 INFO]: Saving model to: blotchy-Amado_trial_102/model_best.pth
[08/28/2025 04:09:57 INFO]: Training loss at epoch 51: 0.9477089047431946
[08/28/2025 04:10:15 INFO]: Training loss at epoch 10: 0.8436453640460968
[08/28/2025 04:10:51 INFO]: Training loss at epoch 36: 1.0203863978385925
[08/28/2025 04:11:16 INFO]: Training loss at epoch 49: 1.102233111858368
[08/28/2025 04:11:27 INFO]: Training loss at epoch 5: 1.1470233798027039
[08/28/2025 04:11:27 INFO]: Training loss at epoch 22: 0.9829919636249542
[08/28/2025 04:11:28 INFO]: Training loss at epoch 92: 1.306691586971283
[08/28/2025 04:11:29 INFO]: Training loss at epoch 49: 1.1097131967544556
[08/28/2025 04:11:35 INFO]: Training loss at epoch 76: 1.250320941209793
[08/28/2025 04:11:56 INFO]: Running Final Evaluation...
[08/28/2025 04:12:02 INFO]: Training loss at epoch 49: 1.1055026054382324
[08/28/2025 04:12:06 INFO]: Training loss at epoch 41: 0.9307748973369598
[08/28/2025 04:12:42 INFO]: Training stats: {
    "score": -0.9810469079582099,
    "rmse": 0.9810469079582099
}
[08/28/2025 04:12:42 INFO]: Val stats: {
    "score": -0.6797947226307581,
    "rmse": 0.6797947226307581
}
[08/28/2025 04:12:42 INFO]: Test stats: {
    "score": -0.8844261285384951,
    "rmse": 0.8844261285384951
}
[08/28/2025 04:12:56 INFO]: Training accuracy: {
    "score": -1.00944716594845,
    "rmse": 1.00944716594845
}
[08/28/2025 04:12:56 INFO]: Val accuracy: {
    "score": -0.6599611897026566,
    "rmse": 0.6599611897026566
}
[08/28/2025 04:12:56 INFO]: Test accuracy: {
    "score": -0.8710451172631247,
    "rmse": 0.8710451172631247
}
[08/28/2025 04:12:56 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_71",
    "best_epoch": 45,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8710451172631247,
        "rmse": 0.8710451172631247
    },
    "train_stats": {
        "score": -1.00944716594845,
        "rmse": 1.00944716594845
    },
    "val_stats": {
        "score": -0.6599611897026566,
        "rmse": 0.6599611897026566
    }
}
[08/28/2025 04:12:56 INFO]: Procewss finished for trial blotchy-Amado_trial_71
[08/28/2025 04:12:56 INFO]: 
_________________________________________________

[08/28/2025 04:12:56 INFO]: train_net_for_optune.py main() running.
[08/28/2025 04:12:56 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 1.566398583645506
  attention_dropout: 0.04034382513623507
  ffn_dropout: 0.04034382513623507
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.6710178565135427e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_105

[08/28/2025 04:12:56 INFO]: This ft_transformer has 9.310 million parameters.
[08/28/2025 04:12:56 INFO]: Training will start at epoch 0.
[08/28/2025 04:12:56 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 04:12:58 INFO]: Training stats: {
    "score": -1.0049850158505458,
    "rmse": 1.0049850158505458
}
[08/28/2025 04:12:58 INFO]: Val stats: {
    "score": -0.7191324709692133,
    "rmse": 0.7191324709692133
}
[08/28/2025 04:12:58 INFO]: Test stats: {
    "score": -0.8980930429342171,
    "rmse": 0.8980930429342171
}
[08/28/2025 04:13:10 INFO]: Training loss at epoch 9: 1.1461463570594788
[08/28/2025 04:13:15 INFO]: Training stats: {
    "score": -0.998535662816366,
    "rmse": 0.998535662816366
}
[08/28/2025 04:13:15 INFO]: Val stats: {
    "score": -0.70423015702825,
    "rmse": 0.70423015702825
}
[08/28/2025 04:13:15 INFO]: Test stats: {
    "score": -0.8810404069115829,
    "rmse": 0.8810404069115829
}
[08/28/2025 04:13:16 INFO]: Training loss at epoch 40: 0.8798391222953796
[08/28/2025 04:13:29 INFO]: Training loss at epoch 6: 0.9907858371734619
[08/28/2025 04:13:30 INFO]: Training loss at epoch 52: 1.1469261646270752
[08/28/2025 04:13:32 INFO]: Training loss at epoch 4: 0.9917734861373901
[08/28/2025 04:14:09 INFO]: New best epoch, val score: -0.6692094011362667
[08/28/2025 04:14:09 INFO]: Saving model to: blotchy-Amado_trial_103/model_best.pth
[08/28/2025 04:14:20 INFO]: Training loss at epoch 1: 0.8700847625732422
[08/28/2025 04:14:20 INFO]: Training loss at epoch 37: 0.8467550575733185
[08/28/2025 04:14:24 INFO]: Training loss at epoch 93: 1.0122371912002563
[08/28/2025 04:14:57 INFO]: Training stats: {
    "score": -1.2027424670858344,
    "rmse": 1.2027424670858344
}
[08/28/2025 04:14:57 INFO]: Val stats: {
    "score": -1.0901333021181505,
    "rmse": 1.0901333021181505
}
[08/28/2025 04:14:57 INFO]: Test stats: {
    "score": -1.1723436179902231,
    "rmse": 1.1723436179902231
}
[08/28/2025 04:15:21 INFO]: Training loss at epoch 11: 1.0076777935028076
[08/28/2025 04:15:38 INFO]: Training loss at epoch 42: 1.1666181087493896
[08/28/2025 04:16:10 INFO]: Training loss at epoch 50: 0.9906600117683411
[08/28/2025 04:16:37 INFO]: Training loss at epoch 65: 0.7155115604400635
[08/28/2025 04:16:43 INFO]: Training loss at epoch 50: 0.97602778673172
[08/28/2025 04:16:46 INFO]: Training loss at epoch 6: 1.1960610151290894
[08/28/2025 04:16:53 INFO]: Training loss at epoch 0: 0.9992124438285828
[08/28/2025 04:17:09 INFO]: Training loss at epoch 53: 0.9331478178501129
[08/28/2025 04:17:23 INFO]: New best epoch, val score: -0.6614596171729609
[08/28/2025 04:17:23 INFO]: Saving model to: blotchy-Amado_trial_105/model_best.pth
[08/28/2025 04:17:23 INFO]: Training loss at epoch 94: 1.165554940700531
[08/28/2025 04:17:27 INFO]: Training loss at epoch 2: 1.2728208303451538
[08/28/2025 04:17:27 INFO]: Training loss at epoch 23: 0.9439129829406738
[08/28/2025 04:17:59 INFO]: Training loss at epoch 38: 0.9318345189094543
[08/28/2025 04:18:02 INFO]: Training loss at epoch 7: 0.9567549228668213
[08/28/2025 04:18:09 INFO]: Training loss at epoch 50: 0.9808067679405212
[08/28/2025 04:18:16 INFO]: Training loss at epoch 41: 1.137843668460846
[08/28/2025 04:18:44 INFO]: Training loss at epoch 5: 1.012377381324768
[08/28/2025 04:19:12 INFO]: Training loss at epoch 43: 1.1181697249412537
[08/28/2025 04:19:21 INFO]: New best epoch, val score: -0.6673938298751846
[08/28/2025 04:19:21 INFO]: Saving model to: blotchy-Amado_trial_103/model_best.pth
[08/28/2025 04:19:36 INFO]: Training loss at epoch 51: 1.0369333028793335
[08/28/2025 04:19:43 INFO]: Training loss at epoch 6: 1.5735702514648438
[08/28/2025 04:20:10 INFO]: Training loss at epoch 10: 0.9422593712806702
[08/28/2025 04:20:10 INFO]: Training loss at epoch 51: 0.9676325619220734
[08/28/2025 04:20:24 INFO]: Training loss at epoch 95: 0.887912392616272
[08/28/2025 04:20:31 INFO]: Training loss at epoch 12: 0.8293455839157104
[08/28/2025 04:20:46 INFO]: Running Final Evaluation...
[08/28/2025 04:20:47 INFO]: Training loss at epoch 54: 0.9263869225978851
[08/28/2025 04:21:19 INFO]: Training loss at epoch 2: 1.2707130312919617
[08/28/2025 04:21:24 INFO]: Training loss at epoch 1: 1.1182571649551392
[08/28/2025 04:21:34 INFO]: Training loss at epoch 39: 0.9033806324005127
[08/28/2025 04:21:47 INFO]: Training accuracy: {
    "score": -1.0125866265903023,
    "rmse": 1.0125866265903023
}
[08/28/2025 04:21:47 INFO]: Val accuracy: {
    "score": -0.6613974847607322,
    "rmse": 0.6613974847607322
}
[08/28/2025 04:21:47 INFO]: Test accuracy: {
    "score": -0.8734041906454064,
    "rmse": 0.8734041906454064
}
[08/28/2025 04:21:47 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_63",
    "best_epoch": 64,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8734041906454064,
        "rmse": 0.8734041906454064
    },
    "train_stats": {
        "score": -1.0125866265903023,
        "rmse": 1.0125866265903023
    },
    "val_stats": {
        "score": -0.6613974847607322,
        "rmse": 0.6613974847607322
    }
}
[08/28/2025 04:21:47 INFO]: Procewss finished for trial blotchy-Amado_trial_63
[08/28/2025 04:21:47 INFO]: 
_________________________________________________

[08/28/2025 04:21:47 INFO]: train_net_for_optune.py main() running.
[08/28/2025 04:21:47 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 1.5607660683072022
  attention_dropout: 0.37399266836270983
  ffn_dropout: 0.37399266836270983
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.7030013516477635e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_106

[08/28/2025 04:21:48 INFO]: This ft_transformer has 9.298 million parameters.
[08/28/2025 04:21:48 INFO]: Training will start at epoch 0.
[08/28/2025 04:21:48 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 04:21:58 INFO]: Training loss at epoch 7: 0.8570985198020935
[08/28/2025 04:22:33 INFO]: Training loss at epoch 8: 0.9238406717777252
[08/28/2025 04:22:46 INFO]: Training loss at epoch 44: 1.076912522315979
[08/28/2025 04:22:50 INFO]: Training stats: {
    "score": -0.9844839162862403,
    "rmse": 0.9844839162862403
}
[08/28/2025 04:22:50 INFO]: Val stats: {
    "score": -0.6702943219406416,
    "rmse": 0.6702943219406416
}
[08/28/2025 04:22:50 INFO]: Test stats: {
    "score": -0.8794061315350984,
    "rmse": 0.8794061315350984
}
[08/28/2025 04:23:03 INFO]: Training loss at epoch 52: 0.9481887519359589
[08/28/2025 04:23:18 INFO]: Training loss at epoch 42: 0.9836772382259369
[08/28/2025 04:23:21 INFO]: Training loss at epoch 51: 0.8859715163707733
[08/28/2025 04:23:28 INFO]: Training loss at epoch 24: 1.0076351463794708
[08/28/2025 04:23:36 INFO]: Training loss at epoch 52: 0.9523611068725586
[08/28/2025 04:23:53 INFO]: Training loss at epoch 6: 1.106263279914856
[08/28/2025 04:24:22 INFO]: Training loss at epoch 55: 0.8932871520519257
[08/28/2025 04:24:27 INFO]: New best epoch, val score: -0.6662331720302509
[08/28/2025 04:24:27 INFO]: Saving model to: blotchy-Amado_trial_103/model_best.pth
[08/28/2025 04:25:09 INFO]: Training loss at epoch 7: 1.451797902584076
[08/28/2025 04:25:17 INFO]: Training loss at epoch 11: 0.9243817627429962
[08/28/2025 04:25:38 INFO]: Training loss at epoch 0: 1.073758602142334
[08/28/2025 04:25:40 INFO]: Training loss at epoch 13: 1.0026338994503021
[08/28/2025 04:25:46 INFO]: Training loss at epoch 2: 0.9396165013313293
[08/28/2025 04:26:09 INFO]: New best epoch, val score: -0.7749613209972126
[08/28/2025 04:26:09 INFO]: Saving model to: blotchy-Amado_trial_106/model_best.pth
[08/28/2025 04:26:14 INFO]: Training loss at epoch 45: 1.2195222675800323
[08/28/2025 04:26:21 INFO]: Training loss at epoch 40: 0.8976318836212158
[08/28/2025 04:26:27 INFO]: Training loss at epoch 53: 0.7103054374456406
[08/28/2025 04:26:56 INFO]: Training loss at epoch 9: 0.9258540272712708
[08/28/2025 04:27:00 INFO]: Training loss at epoch 53: 0.953628420829773
[08/28/2025 04:27:48 INFO]: Training loss at epoch 66: 0.8160676956176758
[08/28/2025 04:27:55 INFO]: Training loss at epoch 7: 1.0745392143726349
[08/28/2025 04:27:55 INFO]: Training loss at epoch 56: 1.2218959331512451
[08/28/2025 04:28:04 INFO]: Training loss at epoch 3: 1.020106941461563
[08/28/2025 04:28:12 INFO]: Training loss at epoch 43: 1.081180989742279
[08/28/2025 04:28:25 INFO]: Training loss at epoch 52: 1.056217074394226
[08/28/2025 04:28:25 INFO]: Training stats: {
    "score": -0.9770972432366691,
    "rmse": 0.9770972432366691
}
[08/28/2025 04:28:25 INFO]: Val stats: {
    "score": -0.6776946846977409,
    "rmse": 0.6776946846977409
}
[08/28/2025 04:28:25 INFO]: Test stats: {
    "score": -0.871671788762256,
    "rmse": 0.871671788762256
}
[08/28/2025 04:28:46 INFO]: Running Final Evaluation...
[08/28/2025 04:28:54 INFO]: Training loss at epoch 7: 0.9774525165557861
[08/28/2025 04:29:22 INFO]: Training loss at epoch 25: 0.8837777674198151
[08/28/2025 04:29:43 INFO]: Training loss at epoch 46: 0.9969412088394165
[08/28/2025 04:29:48 INFO]: Training loss at epoch 54: 0.8526843786239624
[08/28/2025 04:29:48 INFO]: Training loss at epoch 41: 1.198497474193573
[08/28/2025 04:29:49 INFO]: Training loss at epoch 3: 1.0013151168823242
[08/28/2025 04:30:00 INFO]: Training loss at epoch 1: 1.2316308617591858
[08/28/2025 04:30:06 INFO]: Training loss at epoch 3: 1.02434903383255
[08/28/2025 04:30:22 INFO]: Training loss at epoch 12: 1.0573922395706177
[08/28/2025 04:30:24 INFO]: Training loss at epoch 54: 0.8519225120544434
[08/28/2025 04:30:28 INFO]: Training accuracy: {
    "score": -1.0058837662906934,
    "rmse": 1.0058837662906934
}
[08/28/2025 04:30:28 INFO]: Val accuracy: {
    "score": -0.6670146221992874,
    "rmse": 0.6670146221992874
}
[08/28/2025 04:30:28 INFO]: Test accuracy: {
    "score": -0.8713094610652987,
    "rmse": 0.8713094610652987
}
[08/28/2025 04:30:28 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_73",
    "best_epoch": 12,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8713094610652987,
        "rmse": 0.8713094610652987
    },
    "train_stats": {
        "score": -1.0058837662906934,
        "rmse": 1.0058837662906934
    },
    "val_stats": {
        "score": -0.6670146221992874,
        "rmse": 0.6670146221992874
    }
}
[08/28/2025 04:30:28 INFO]: Procewss finished for trial blotchy-Amado_trial_73
[08/28/2025 04:30:28 INFO]: 
_________________________________________________

[08/28/2025 04:30:28 INFO]: train_net_for_optune.py main() running.
[08/28/2025 04:30:28 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 1.6082487061088204
  attention_dropout: 0.04205039329158825
  ffn_dropout: 0.04205039329158825
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.624202212937321e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_107

[08/28/2025 04:30:29 INFO]: This ft_transformer has 9.446 million parameters.
[08/28/2025 04:30:29 INFO]: Training will start at epoch 0.
[08/28/2025 04:30:29 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 04:30:33 INFO]: New best epoch, val score: -0.7440104919607137
[08/28/2025 04:30:33 INFO]: Saving model to: blotchy-Amado_trial_106/model_best.pth
[08/28/2025 04:30:42 INFO]: Training loss at epoch 14: 0.9868196547031403
[08/28/2025 04:31:19 INFO]: New best epoch, val score: -0.6675065940911815
[08/28/2025 04:31:19 INFO]: Saving model to: blotchy-Amado_trial_101/model_best.pth
[08/28/2025 04:31:30 INFO]: Training loss at epoch 57: 1.2054495811462402
[08/28/2025 04:32:50 INFO]: Training loss at epoch 10: 0.8997151851654053
[08/28/2025 04:33:13 INFO]: Training loss at epoch 55: 0.887161374092102
[08/28/2025 04:33:17 INFO]: Training loss at epoch 47: 0.8894870579242706
[08/28/2025 04:33:24 INFO]: Training loss at epoch 42: 0.9904634356498718
[08/28/2025 04:33:26 INFO]: Training loss at epoch 8: 1.34649658203125
[08/28/2025 04:33:32 INFO]: Training loss at epoch 53: 1.1701138615608215
[08/28/2025 04:33:43 INFO]: Running Final Evaluation...
[08/28/2025 04:33:49 INFO]: Training loss at epoch 55: 1.0552060008049011
[08/28/2025 04:33:57 INFO]: Training loss at epoch 8: 0.9139432609081268
[08/28/2025 04:34:03 INFO]: Training loss at epoch 8: 1.0048620104789734
[08/28/2025 04:34:09 INFO]: Running Final Evaluation...
[08/28/2025 04:34:22 INFO]: Training loss at epoch 0: 0.9990561604499817
[08/28/2025 04:34:30 INFO]: Training loss at epoch 2: 0.9925822913646698
[08/28/2025 04:34:33 INFO]: Training loss at epoch 4: 1.007434606552124
[08/28/2025 04:34:53 INFO]: Training loss at epoch 4: 1.2817219495773315
[08/28/2025 04:34:55 INFO]: New best epoch, val score: -0.6800842629720046
[08/28/2025 04:34:55 INFO]: Saving model to: blotchy-Amado_trial_107/model_best.pth
[08/28/2025 04:35:01 INFO]: Training accuracy: {
    "score": -0.9986354090239706,
    "rmse": 0.9986354090239706
}
[08/28/2025 04:35:01 INFO]: Val accuracy: {
    "score": -0.6848039549954774,
    "rmse": 0.6848039549954774
}
[08/28/2025 04:35:01 INFO]: Test accuracy: {
    "score": -0.8851001143665699,
    "rmse": 0.8851001143665699
}
[08/28/2025 04:35:01 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_82",
    "best_epoch": 16,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8851001143665699,
        "rmse": 0.8851001143665699
    },
    "train_stats": {
        "score": -0.9986354090239706,
        "rmse": 0.9986354090239706
    },
    "val_stats": {
        "score": -0.6848039549954774,
        "rmse": 0.6848039549954774
    }
}
[08/28/2025 04:35:01 INFO]: Procewss finished for trial blotchy-Amado_trial_82
[08/28/2025 04:35:01 INFO]: 
_________________________________________________

[08/28/2025 04:35:01 INFO]: train_net_for_optune.py main() running.
[08/28/2025 04:35:01 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 1.5959653440377863
  attention_dropout: 0.04542280629319266
  ffn_dropout: 0.04542280629319266
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.8041107519601595e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_108

[08/28/2025 04:35:01 INFO]: This ft_transformer has 9.409 million parameters.
[08/28/2025 04:35:01 INFO]: Training will start at epoch 0.
[08/28/2025 04:35:01 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 04:35:06 INFO]: Training loss at epoch 58: 0.9341146945953369
[08/28/2025 04:35:23 INFO]: Training loss at epoch 26: 1.0204308927059174
[08/28/2025 04:35:33 INFO]: Training loss at epoch 13: 1.083058774471283
[08/28/2025 04:35:49 INFO]: Training loss at epoch 15: 0.9539624154567719
[08/28/2025 04:35:58 INFO]: Training accuracy: {
    "score": -1.0145555028119029,
    "rmse": 1.0145555028119029
}
[08/28/2025 04:35:58 INFO]: Val accuracy: {
    "score": -0.6604344542036932,
    "rmse": 0.6604344542036932
}
[08/28/2025 04:35:58 INFO]: Test accuracy: {
    "score": -0.873615120247475,
    "rmse": 0.873615120247475
}
[08/28/2025 04:35:58 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_66",
    "best_epoch": 22,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.873615120247475,
        "rmse": 0.873615120247475
    },
    "train_stats": {
        "score": -1.0145555028119029,
        "rmse": 1.0145555028119029
    },
    "val_stats": {
        "score": -0.6604344542036932,
        "rmse": 0.6604344542036932
    }
}
[08/28/2025 04:35:58 INFO]: Procewss finished for trial blotchy-Amado_trial_66
[08/28/2025 04:35:58 INFO]: 
_________________________________________________

[08/28/2025 04:35:58 INFO]: train_net_for_optune.py main() running.
[08/28/2025 04:35:58 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.5461108687290004
  attention_dropout: 0.11405246055004503
  ffn_dropout: 0.11405246055004503
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.2337286741665645e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_109

[08/28/2025 04:35:58 INFO]: This ft_transformer has 12.399 million parameters.
[08/28/2025 04:35:58 INFO]: Training will start at epoch 0.
[08/28/2025 04:35:58 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 04:36:10 INFO]: Training loss at epoch 8: 1.1785154938697815
[08/28/2025 04:36:12 INFO]: New best epoch, val score: -0.6871062018547925
[08/28/2025 04:36:12 INFO]: Saving model to: blotchy-Amado_trial_99/model_best.pth
[08/28/2025 04:36:41 INFO]: Training loss at epoch 56: 0.9726357758045197
[08/28/2025 04:36:59 INFO]: Training loss at epoch 43: 0.8216513693332672
[08/28/2025 04:37:19 INFO]: Training loss at epoch 11: 0.8594169318675995
[08/28/2025 04:37:20 INFO]: Training loss at epoch 56: 0.9458379447460175
[08/28/2025 04:37:45 INFO]: Running Final Evaluation...
[08/28/2025 04:38:46 INFO]: Training loss at epoch 59: 1.1488394141197205
[08/28/2025 04:38:55 INFO]: Training loss at epoch 1: 1.2118810415267944
[08/28/2025 04:39:00 INFO]: Training loss at epoch 3: 0.9791330099105835
[08/28/2025 04:39:01 INFO]: Training accuracy: {
    "score": -1.0077292080685394,
    "rmse": 1.0077292080685394
}
[08/28/2025 04:39:01 INFO]: Val accuracy: {
    "score": -0.6620512838106926,
    "rmse": 0.6620512838106926
}
[08/28/2025 04:39:01 INFO]: Test accuracy: {
    "score": -0.8657548300056129,
    "rmse": 0.8657548300056129
}
[08/28/2025 04:39:01 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_81",
    "best_epoch": 25,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8657548300056129,
        "rmse": 0.8657548300056129
    },
    "train_stats": {
        "score": -1.0077292080685394,
        "rmse": 1.0077292080685394
    },
    "val_stats": {
        "score": -0.6620512838106926,
        "rmse": 0.6620512838106926
    }
}
[08/28/2025 04:39:01 INFO]: Procewss finished for trial blotchy-Amado_trial_81
[08/28/2025 04:39:01 INFO]: 
_________________________________________________

[08/28/2025 04:39:01 INFO]: train_net_for_optune.py main() running.
[08/28/2025 04:39:01 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.559596525780892
  attention_dropout: 0.029873548726619993
  ffn_dropout: 0.029873548726619993
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.3656337152367808e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_110

[08/28/2025 04:39:01 INFO]: This ft_transformer has 12.442 million parameters.
[08/28/2025 04:39:01 INFO]: Training will start at epoch 0.
[08/28/2025 04:39:01 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 04:39:01 INFO]: Training loss at epoch 0: 0.9716358184814453
[08/28/2025 04:39:03 INFO]: Training loss at epoch 67: 0.6079218685626984
[08/28/2025 04:39:04 INFO]: Training loss at epoch 5: 1.1431106328964233
[08/28/2025 04:39:08 INFO]: Training loss at epoch 9: 0.8819544315338135
[08/28/2025 04:39:34 INFO]: New best epoch, val score: -0.6542289093349163
[08/28/2025 04:39:34 INFO]: Saving model to: blotchy-Amado_trial_108/model_best.pth
[08/28/2025 04:39:56 INFO]: Training stats: {
    "score": -0.980770664797395,
    "rmse": 0.980770664797395
}
[08/28/2025 04:39:56 INFO]: Val stats: {
    "score": -0.6748479013524339,
    "rmse": 0.6748479013524339
}
[08/28/2025 04:39:56 INFO]: Test stats: {
    "score": -0.8841133766035489,
    "rmse": 0.8841133766035489
}
[08/28/2025 04:40:05 INFO]: Training loss at epoch 57: 0.8386533558368683
[08/28/2025 04:40:34 INFO]: Training loss at epoch 44: 0.9224071502685547
[08/28/2025 04:40:44 INFO]: Training loss at epoch 14: 1.1918071508407593
[08/28/2025 04:40:50 INFO]: Training loss at epoch 0: 1.1457980275154114
[08/28/2025 04:40:51 INFO]: Training stats: {
    "score": -0.9753721713164905,
    "rmse": 0.9753721713164905
}
[08/28/2025 04:40:51 INFO]: Val stats: {
    "score": -0.6720628938923138,
    "rmse": 0.6720628938923138
}
[08/28/2025 04:40:51 INFO]: Test stats: {
    "score": -0.883063100920769,
    "rmse": 0.883063100920769
}
[08/28/2025 04:40:56 INFO]: Training loss at epoch 16: 0.998047947883606
[08/28/2025 04:41:19 INFO]: New best epoch, val score: -0.6852026562682214
[08/28/2025 04:41:19 INFO]: Saving model to: blotchy-Amado_trial_99/model_best.pth
[08/28/2025 04:41:24 INFO]: Training loss at epoch 27: 1.1490185260772705
[08/28/2025 04:41:27 INFO]: New best epoch, val score: -0.7096955242664851
[08/28/2025 04:41:27 INFO]: Saving model to: blotchy-Amado_trial_109/model_best.pth
[08/28/2025 04:41:42 INFO]: Training loss at epoch 12: 0.9443501830101013
[08/28/2025 04:41:46 INFO]: Training loss at epoch 5: 0.854363888502121
[08/28/2025 04:41:48 INFO]: Training loss at epoch 9: 1.298218309879303
[08/28/2025 04:42:14 INFO]: Training loss at epoch 4: 0.9415623843669891
[08/28/2025 04:43:20 INFO]: Training loss at epoch 2: 1.093211829662323
[08/28/2025 04:43:23 INFO]: Training loss at epoch 4: 1.0016630589962006
[08/28/2025 04:43:27 INFO]: Training loss at epoch 1: 0.7921297550201416
[08/28/2025 04:43:28 INFO]: Training loss at epoch 6: 1.0642191171646118
[08/28/2025 04:43:28 INFO]: Training loss at epoch 60: 0.9766886532306671
[08/28/2025 04:43:29 INFO]: Training loss at epoch 58: 0.8453115820884705
[08/28/2025 04:43:49 INFO]: Training loss at epoch 0: 1.2503849267959595
[08/28/2025 04:43:52 INFO]: New best epoch, val score: -0.6674505858995845
[08/28/2025 04:43:52 INFO]: Saving model to: blotchy-Amado_trial_107/model_best.pth
[08/28/2025 04:44:06 INFO]: Training loss at epoch 45: 1.0255571603775024
[08/28/2025 04:44:21 INFO]: Training loss at epoch 9: 1.1375563740730286
[08/28/2025 04:44:26 INFO]: New best epoch, val score: -0.6757585421041126
[08/28/2025 04:44:26 INFO]: Saving model to: blotchy-Amado_trial_110/model_best.pth
[08/28/2025 04:44:38 INFO]: Training stats: {
    "score": -1.1513855686696446,
    "rmse": 1.1513855686696446
}
[08/28/2025 04:44:38 INFO]: Val stats: {
    "score": -0.813134378962119,
    "rmse": 0.813134378962119
}
[08/28/2025 04:44:38 INFO]: Test stats: {
    "score": -1.016854573786271,
    "rmse": 1.016854573786271
}
[08/28/2025 04:45:52 INFO]: Training loss at epoch 15: 0.9393256306648254
[08/28/2025 04:45:54 INFO]: Training loss at epoch 10: 0.921739250421524
[08/28/2025 04:45:59 INFO]: Training loss at epoch 17: 0.8378797471523285
[08/28/2025 04:46:06 INFO]: Training loss at epoch 13: 0.814350813627243
[08/28/2025 04:46:17 INFO]: Training loss at epoch 1: 1.0563029050827026
[08/28/2025 04:46:18 INFO]: Training loss at epoch 9: 0.8297322392463684
[08/28/2025 04:46:52 INFO]: Training loss at epoch 59: 0.8052866756916046
[08/28/2025 04:47:02 INFO]: Training loss at epoch 61: 1.2377465069293976
[08/28/2025 04:47:07 INFO]: Training stats: {
    "score": -1.1020205817553614,
    "rmse": 1.1020205817553614
}
[08/28/2025 04:47:07 INFO]: Val stats: {
    "score": -0.741441228232782,
    "rmse": 0.741441228232782
}
[08/28/2025 04:47:07 INFO]: Test stats: {
    "score": -0.9698556109613471,
    "rmse": 0.9698556109613471
}
[08/28/2025 04:47:19 INFO]: Training loss at epoch 28: 0.9276874959468842
[08/28/2025 04:47:33 INFO]: Training loss at epoch 46: 0.814956784248352
[08/28/2025 04:47:44 INFO]: Training loss at epoch 3: 1.12589693069458
[08/28/2025 04:47:46 INFO]: Training loss at epoch 5: 1.2171221375465393
[08/28/2025 04:47:50 INFO]: Training loss at epoch 7: 1.0022637248039246
[08/28/2025 04:47:51 INFO]: Training loss at epoch 2: 1.0051419138908386
[08/28/2025 04:48:06 INFO]: Training stats: {
    "score": -0.98059412663806,
    "rmse": 0.98059412663806
}
[08/28/2025 04:48:06 INFO]: Val stats: {
    "score": -0.667888662459914,
    "rmse": 0.667888662459914
}
[08/28/2025 04:48:06 INFO]: Test stats: {
    "score": -0.873354333529705,
    "rmse": 0.873354333529705
}
[08/28/2025 04:48:15 INFO]: New best epoch, val score: -0.6615486098315309
[08/28/2025 04:48:15 INFO]: Saving model to: blotchy-Amado_trial_107/model_best.pth
[08/28/2025 04:48:34 INFO]: Training loss at epoch 6: 0.9509319365024567
[08/28/2025 04:49:16 INFO]: Training loss at epoch 1: 0.9332869946956635
[08/28/2025 04:50:09 INFO]: Training loss at epoch 68: 0.6167155206203461
[08/28/2025 04:50:30 INFO]: Training stats: {
    "score": -0.9675558433023296,
    "rmse": 0.9675558433023296
}
[08/28/2025 04:50:30 INFO]: Val stats: {
    "score": -0.7174762955717936,
    "rmse": 0.7174762955717936
}
[08/28/2025 04:50:30 INFO]: Test stats: {
    "score": -0.8815422305490077,
    "rmse": 0.8815422305490077
}
[08/28/2025 04:50:30 INFO]: Training loss at epoch 14: 0.8205346167087555
[08/28/2025 04:50:37 INFO]: Training loss at epoch 62: 1.3089709281921387
[08/28/2025 04:50:57 INFO]: Training loss at epoch 11: 0.865489661693573
[08/28/2025 04:50:59 INFO]: Training loss at epoch 16: 1.2298704385757446
[08/28/2025 04:51:04 INFO]: Training loss at epoch 18: 0.9774872064590454
[08/28/2025 04:51:05 INFO]: Training loss at epoch 47: 0.9432864189147949
[08/28/2025 04:51:31 INFO]: Training loss at epoch 60: 0.863981693983078
[08/28/2025 04:51:39 INFO]: New best epoch, val score: -0.6896581589103737
[08/28/2025 04:51:39 INFO]: Saving model to: blotchy-Amado_trial_97/model_best.pth
[08/28/2025 04:51:46 INFO]: Training loss at epoch 2: 1.0076739192008972
[08/28/2025 04:52:10 INFO]: Training loss at epoch 6: 1.217930555343628
[08/28/2025 04:52:11 INFO]: Training loss at epoch 4: 0.9199098348617554
[08/28/2025 04:52:15 INFO]: Training loss at epoch 8: 1.0795811414718628
[08/28/2025 04:52:19 INFO]: Training loss at epoch 3: 1.0084696412086487
[08/28/2025 04:52:57 INFO]: Training loss at epoch 10: 1.08578622341156
[08/28/2025 04:53:16 INFO]: Training loss at epoch 29: 0.894366055727005
[08/28/2025 04:53:58 INFO]: New best epoch, val score: -0.6844712588527734
[08/28/2025 04:53:58 INFO]: Saving model to: blotchy-Amado_trial_98/model_best.pth
[08/28/2025 04:54:14 INFO]: Training loss at epoch 63: 0.9662433862686157
[08/28/2025 04:54:36 INFO]: Training loss at epoch 48: 0.8224284052848816
[08/28/2025 04:54:38 INFO]: Training loss at epoch 5: 1.1603281497955322
[08/28/2025 04:54:48 INFO]: Training loss at epoch 2: 1.1856564283370972
[08/28/2025 04:54:58 INFO]: Training loss at epoch 15: 1.0316868424415588
[08/28/2025 04:54:58 INFO]: Training loss at epoch 61: 0.905042290687561
[08/28/2025 04:55:20 INFO]: Training stats: {
    "score": -0.9730829544257702,
    "rmse": 0.9730829544257702
}
[08/28/2025 04:55:20 INFO]: Val stats: {
    "score": -0.7051582426859817,
    "rmse": 0.7051582426859817
}
[08/28/2025 04:55:20 INFO]: Test stats: {
    "score": -0.8972132818019684,
    "rmse": 0.8972132818019684
}
[08/28/2025 04:55:22 INFO]: Training loss at epoch 10: 0.995712399482727
[08/28/2025 04:55:26 INFO]: Training loss at epoch 7: 0.9995347261428833
[08/28/2025 04:55:28 INFO]: New best epoch, val score: -0.6650643907267274
[08/28/2025 04:55:28 INFO]: Saving model to: blotchy-Amado_trial_102/model_best.pth
[08/28/2025 04:56:05 INFO]: Training loss at epoch 12: 0.9433520138263702
[08/28/2025 04:56:10 INFO]: Training loss at epoch 17: 0.9880131185054779
[08/28/2025 04:56:13 INFO]: Training loss at epoch 19: 0.9205999374389648
[08/28/2025 04:56:40 INFO]: Training loss at epoch 7: 0.9416240453720093
[08/28/2025 04:56:42 INFO]: Training loss at epoch 5: 1.0539075434207916
[08/28/2025 04:56:43 INFO]: Training loss at epoch 9: 1.0235758125782013
[08/28/2025 04:56:49 INFO]: Training loss at epoch 4: 0.8129924237728119
[08/28/2025 04:57:17 INFO]: Training loss at epoch 3: 0.8793617486953735
[08/28/2025 04:57:49 INFO]: Training loss at epoch 64: 0.9977223575115204
[08/28/2025 04:57:55 INFO]: Training stats: {
    "score": -0.9555540135193797,
    "rmse": 0.9555540135193797
}
[08/28/2025 04:57:55 INFO]: Val stats: {
    "score": -0.6900304350155607,
    "rmse": 0.6900304350155607
}
[08/28/2025 04:57:55 INFO]: Test stats: {
    "score": -0.896091853201507,
    "rmse": 0.896091853201507
}
[08/28/2025 04:57:57 INFO]: New best epoch, val score: -0.6839898112779479
[08/28/2025 04:57:57 INFO]: Saving model to: blotchy-Amado_trial_109/model_best.pth
[08/28/2025 04:58:07 INFO]: Training loss at epoch 49: 0.8085854351520538
[08/28/2025 04:58:12 INFO]: Training stats: {
    "score": -1.0073041749284217,
    "rmse": 1.0073041749284217
}
[08/28/2025 04:58:12 INFO]: Val stats: {
    "score": -0.7629017198859038,
    "rmse": 0.7629017198859038
}
[08/28/2025 04:58:12 INFO]: Test stats: {
    "score": -0.9530218029327296,
    "rmse": 0.9530218029327296
}
[08/28/2025 04:58:21 INFO]: Training loss at epoch 62: 0.9504857361316681
[08/28/2025 04:59:18 INFO]: Training loss at epoch 16: 1.0208775401115417
[08/28/2025 04:59:26 INFO]: Training stats: {
    "score": -0.9809941334952574,
    "rmse": 0.9809941334952574
}
[08/28/2025 04:59:26 INFO]: Val stats: {
    "score": -0.6800511748605036,
    "rmse": 0.6800511748605036
}
[08/28/2025 04:59:26 INFO]: Test stats: {
    "score": -0.8922782920548842,
    "rmse": 0.8922782920548842
}
[08/28/2025 04:59:50 INFO]: New best epoch, val score: -0.6644789065956218
[08/28/2025 04:59:50 INFO]: Saving model to: blotchy-Amado_trial_102/model_best.pth
[08/28/2025 05:00:12 INFO]: Training loss at epoch 3: 0.9678078889846802
[08/28/2025 05:01:07 INFO]: Training loss at epoch 6: 1.1163522601127625
[08/28/2025 05:01:09 INFO]: Training loss at epoch 8: 1.0868641138076782
[08/28/2025 05:01:09 INFO]: Training loss at epoch 13: 0.859309196472168
[08/28/2025 05:01:17 INFO]: Training loss at epoch 18: 1.047168791294098
[08/28/2025 05:01:19 INFO]: Training loss at epoch 11: 1.0811488032341003
[08/28/2025 05:01:20 INFO]: Training loss at epoch 5: 0.9266786873340607
[08/28/2025 05:01:21 INFO]: Training loss at epoch 69: 0.6975013613700867
[08/28/2025 05:01:23 INFO]: Training loss at epoch 65: 0.8958262205123901
[08/28/2025 05:01:23 INFO]: Training loss at epoch 30: 1.051731437444687
[08/28/2025 05:01:44 INFO]: Training loss at epoch 63: 0.8764642477035522
[08/28/2025 05:02:11 INFO]: Training loss at epoch 8: 0.9613754451274872
[08/28/2025 05:02:35 INFO]: Training loss at epoch 10: 0.90000119805336
[08/28/2025 05:02:39 INFO]: Training loss at epoch 10: 0.8850591778755188
[08/28/2025 05:02:47 INFO]: Training loss at epoch 4: 1.027284324169159
[08/28/2025 05:02:56 INFO]: Training loss at epoch 50: 0.9097147583961487
[08/28/2025 05:02:59 INFO]: Training loss at epoch 20: 0.8136360049247742
[08/28/2025 05:03:26 INFO]: New best epoch, val score: -0.6730040379926387
[08/28/2025 05:03:26 INFO]: Saving model to: blotchy-Amado_trial_109/model_best.pth
[08/28/2025 05:03:32 INFO]: Training loss at epoch 11: 0.8207394182682037
[08/28/2025 05:03:40 INFO]: Training loss at epoch 17: 0.7684237360954285
[08/28/2025 05:04:56 INFO]: Training loss at epoch 66: 0.9296842813491821
[08/28/2025 05:05:05 INFO]: Training stats: {
    "score": -0.9290240767731324,
    "rmse": 0.9290240767731324
}
[08/28/2025 05:05:05 INFO]: Val stats: {
    "score": -1.1014623966064947,
    "rmse": 1.1014623966064947
}
[08/28/2025 05:05:05 INFO]: Test stats: {
    "score": -1.2867265346875691,
    "rmse": 1.2867265346875691
}
[08/28/2025 05:05:07 INFO]: Training loss at epoch 64: 0.9191054999828339
[08/28/2025 05:05:31 INFO]: Training loss at epoch 9: 1.1489550471305847
[08/28/2025 05:05:33 INFO]: Training loss at epoch 7: 0.8520015180110931
[08/28/2025 05:05:39 INFO]: Training loss at epoch 4: 1.082969605922699
[08/28/2025 05:05:44 INFO]: Training loss at epoch 6: 0.8849413394927979
[08/28/2025 05:06:12 INFO]: Training loss at epoch 14: 0.7215086668729782
[08/28/2025 05:06:24 INFO]: Training loss at epoch 19: 1.1137880086898804
[08/28/2025 05:06:27 INFO]: Training loss at epoch 51: 1.0890687108039856
[08/28/2025 05:06:59 INFO]: Training loss at epoch 11: 1.144708514213562
[08/28/2025 05:07:02 INFO]: Training stats: {
    "score": -1.1013168049172875,
    "rmse": 1.1013168049172875
}
[08/28/2025 05:07:02 INFO]: Val stats: {
    "score": -0.7424513360517059,
    "rmse": 0.7424513360517059
}
[08/28/2025 05:07:02 INFO]: Test stats: {
    "score": -0.9575654668370327,
    "rmse": 0.9575654668370327
}
[08/28/2025 05:07:03 INFO]: Training loss at epoch 6: 1.10199773311615
[08/28/2025 05:07:16 INFO]: Training loss at epoch 31: 1.0071550607681274
[08/28/2025 05:07:32 INFO]: New best epoch, val score: -0.7424513360517059
[08/28/2025 05:07:32 INFO]: Saving model to: blotchy-Amado_trial_106/model_best.pth
[08/28/2025 05:08:03 INFO]: Training loss at epoch 21: 0.8928932547569275
[08/28/2025 05:08:07 INFO]: Training loss at epoch 18: 0.9577069878578186
[08/28/2025 05:08:09 INFO]: Training stats: {
    "score": -1.0096924776545213,
    "rmse": 1.0096924776545213
}
[08/28/2025 05:08:09 INFO]: Val stats: {
    "score": -0.7989231623453541,
    "rmse": 0.7989231623453541
}
[08/28/2025 05:08:09 INFO]: Test stats: {
    "score": -0.9427717853130672,
    "rmse": 0.9427717853130672
}
[08/28/2025 05:08:16 INFO]: Training loss at epoch 5: 0.9577938914299011
[08/28/2025 05:08:31 INFO]: Training loss at epoch 67: 0.9454975128173828
[08/28/2025 05:08:31 INFO]: Training loss at epoch 65: 0.9049328565597534
[08/28/2025 05:09:02 INFO]: Training loss at epoch 9: 0.8790993690490723
[08/28/2025 05:09:41 INFO]: Training loss at epoch 12: 1.24570232629776
[08/28/2025 05:09:59 INFO]: Training loss at epoch 52: 1.0942200124263763
[08/28/2025 05:10:01 INFO]: Training loss at epoch 8: 1.0935212969779968
[08/28/2025 05:10:15 INFO]: Training loss at epoch 7: 1.0600536465644836
[08/28/2025 05:11:07 INFO]: Training loss at epoch 5: 1.1917860507965088
[08/28/2025 05:11:15 INFO]: Training loss at epoch 15: 0.9465445280075073
[08/28/2025 05:11:22 INFO]: Training stats: {
    "score": -0.9705961386789508,
    "rmse": 0.9705961386789508
}
[08/28/2025 05:11:22 INFO]: Val stats: {
    "score": -0.6659161939203235,
    "rmse": 0.6659161939203235
}
[08/28/2025 05:11:22 INFO]: Test stats: {
    "score": -0.8860809970191135,
    "rmse": 0.8860809970191135
}
[08/28/2025 05:11:23 INFO]: Training loss at epoch 12: 1.0038422346115112
[08/28/2025 05:11:27 INFO]: Training loss at epoch 10: 1.3903042078018188
[08/28/2025 05:11:45 INFO]: Training loss at epoch 12: 0.881296306848526
[08/28/2025 05:11:55 INFO]: Training loss at epoch 66: 1.0172103643417358
[08/28/2025 05:11:59 INFO]: New best epoch, val score: -0.7177839263515756
[08/28/2025 05:11:59 INFO]: Saving model to: blotchy-Amado_trial_106/model_best.pth
[08/28/2025 05:12:05 INFO]: Training loss at epoch 68: 1.0425708293914795
[08/28/2025 05:12:30 INFO]: Training loss at epoch 19: 0.8962820768356323
[08/28/2025 05:13:06 INFO]: Training loss at epoch 22: 0.9470248222351074
[08/28/2025 05:13:15 INFO]: Training loss at epoch 32: 0.8683147430419922
[08/28/2025 05:13:15 INFO]: Training loss at epoch 20: 1.0258616209030151
[08/28/2025 05:13:33 INFO]: Training loss at epoch 53: 0.9860867857933044
[08/28/2025 05:13:48 INFO]: Training loss at epoch 6: 0.9071940183639526
[08/28/2025 05:14:00 INFO]: Training stats: {
    "score": -0.959235331115577,
    "rmse": 0.959235331115577
}
[08/28/2025 05:14:00 INFO]: Val stats: {
    "score": -0.690559569672465,
    "rmse": 0.690559569672465
}
[08/28/2025 05:14:00 INFO]: Test stats: {
    "score": -0.8826297169237778,
    "rmse": 0.8826297169237778
}
[08/28/2025 05:14:25 INFO]: Training loss at epoch 9: 0.9355987012386322
[08/28/2025 05:14:44 INFO]: Training loss at epoch 8: 0.8471494019031525
[08/28/2025 05:14:48 INFO]: Training loss at epoch 11: 1.084890902042389
[08/28/2025 05:15:22 INFO]: Training loss at epoch 67: 0.9564211964607239
[08/28/2025 05:15:39 INFO]: Training loss at epoch 69: 0.9203929007053375
[08/28/2025 05:15:48 INFO]: Training loss at epoch 13: 1.1876957416534424
[08/28/2025 05:15:54 INFO]: Training loss at epoch 11: 1.2166429162025452
[08/28/2025 05:15:56 INFO]: Training stats: {
    "score": -0.9685877477470624,
    "rmse": 0.9685877477470624
}
[08/28/2025 05:15:56 INFO]: Val stats: {
    "score": -0.7003709518770604,
    "rmse": 0.7003709518770604
}
[08/28/2025 05:15:56 INFO]: Test stats: {
    "score": -0.8978463694422977,
    "rmse": 0.8978463694422977
}
[08/28/2025 05:16:19 INFO]: Training loss at epoch 70: 0.8444409668445587
[08/28/2025 05:16:21 INFO]: Training loss at epoch 16: 1.0608200430870056
[08/28/2025 05:16:26 INFO]: New best epoch, val score: -0.7054232210656913
[08/28/2025 05:16:26 INFO]: Saving model to: blotchy-Amado_trial_106/model_best.pth
[08/28/2025 05:16:35 INFO]: Training loss at epoch 6: 1.2365139722824097
[08/28/2025 05:16:50 INFO]: Training stats: {
    "score": -1.008673465054501,
    "rmse": 1.008673465054501
}
[08/28/2025 05:16:50 INFO]: Val stats: {
    "score": -0.6879447190734558,
    "rmse": 0.6879447190734558
}
[08/28/2025 05:16:50 INFO]: Test stats: {
    "score": -0.9030010248800965,
    "rmse": 0.9030010248800965
}
[08/28/2025 05:17:06 INFO]: Training loss at epoch 54: 1.1086037158966064
[08/28/2025 05:18:03 INFO]: Training loss at epoch 13: 1.4177678227424622
[08/28/2025 05:18:11 INFO]: Training loss at epoch 10: 0.8823256194591522
[08/28/2025 05:18:12 INFO]: Training loss at epoch 23: 0.8103276789188385
[08/28/2025 05:18:25 INFO]: Training loss at epoch 21: 1.0342134237289429
[08/28/2025 05:18:26 INFO]: Training loss at epoch 20: 1.0705578923225403
[08/28/2025 05:18:47 INFO]: Training loss at epoch 68: 0.8326043784618378
[08/28/2025 05:19:13 INFO]: Training loss at epoch 33: 0.9794476628303528
[08/28/2025 05:19:14 INFO]: Training loss at epoch 9: 0.924479067325592
[08/28/2025 05:19:19 INFO]: Training loss at epoch 7: 0.9801456332206726
[08/28/2025 05:19:30 INFO]: Training loss at epoch 7: 0.7812647968530655
[08/28/2025 05:19:58 INFO]: Training loss at epoch 13: 1.1564385890960693
[08/28/2025 05:20:16 INFO]: Training loss at epoch 14: 0.9856843650341034
[08/28/2025 05:20:22 INFO]: Training loss at epoch 12: 0.9708875119686127
[08/28/2025 05:20:27 INFO]: Training loss at epoch 10: 1.0090036988258362
[08/28/2025 05:20:28 INFO]: Training loss at epoch 70: 1.1387713551521301
[08/28/2025 05:20:38 INFO]: Training loss at epoch 55: 1.1303855776786804
[08/28/2025 05:20:48 INFO]: Training stats: {
    "score": -0.9653564904588742,
    "rmse": 0.9653564904588742
}
[08/28/2025 05:20:48 INFO]: Val stats: {
    "score": -0.7068246130503857,
    "rmse": 0.7068246130503857
}
[08/28/2025 05:20:48 INFO]: Test stats: {
    "score": -0.8936922434161975,
    "rmse": 0.8936922434161975
}
[08/28/2025 05:20:55 INFO]: New best epoch, val score: -0.6990422500397849
[08/28/2025 05:20:55 INFO]: Saving model to: blotchy-Amado_trial_106/model_best.pth
[08/28/2025 05:21:05 INFO]: New best epoch, val score: -0.6559112988425342
[08/28/2025 05:21:05 INFO]: Saving model to: blotchy-Amado_trial_85/model_best.pth
[08/28/2025 05:21:30 INFO]: Training loss at epoch 17: 1.1091495752334595
[08/28/2025 05:22:05 INFO]: Training loss at epoch 7: 0.9389045536518097
[08/28/2025 05:22:14 INFO]: Training loss at epoch 69: 1.0355662107467651
[08/28/2025 05:22:51 INFO]: Training loss at epoch 21: 0.9221077263355255
[08/28/2025 05:23:19 INFO]: Training loss at epoch 24: 0.9964552819728851
[08/28/2025 05:23:32 INFO]: Training stats: {
    "score": -0.9795431969993866,
    "rmse": 0.9795431969993866
}
[08/28/2025 05:23:32 INFO]: Val stats: {
    "score": -0.6660655679443028,
    "rmse": 0.6660655679443028
}
[08/28/2025 05:23:32 INFO]: Test stats: {
    "score": -0.8717195833458832,
    "rmse": 0.8717195833458832
}
[08/28/2025 05:23:33 INFO]: Training loss at epoch 22: 1.0409055948257446
[08/28/2025 05:24:00 INFO]: Training loss at epoch 71: 0.9649774134159088
[08/28/2025 05:24:23 INFO]: Running Final Evaluation...
[08/28/2025 05:24:25 INFO]: Training loss at epoch 56: 0.8595103323459625
[08/28/2025 05:24:35 INFO]: Training loss at epoch 15: 0.9519097805023193
[08/28/2025 05:24:49 INFO]: Training loss at epoch 11: 0.7633196711540222
[08/28/2025 05:24:50 INFO]: Training loss at epoch 8: 1.0753750503063202
[08/28/2025 05:24:52 INFO]: New best epoch, val score: -0.6546187561578269
[08/28/2025 05:24:52 INFO]: Saving model to: blotchy-Amado_trial_85/model_best.pth
[08/28/2025 05:24:57 INFO]: Training loss at epoch 13: 1.0373088419437408
[08/28/2025 05:25:00 INFO]: Training loss at epoch 11: 0.9515156447887421
[08/28/2025 05:25:23 INFO]: New best epoch, val score: -0.6608194074069031
[08/28/2025 05:25:23 INFO]: Saving model to: blotchy-Amado_trial_107/model_best.pth
[08/28/2025 05:25:24 INFO]: Training loss at epoch 10: 1.0762205719947815
[08/28/2025 05:25:30 INFO]: Training loss at epoch 34: 0.8168314695358276
[08/28/2025 05:25:39 INFO]: Training accuracy: {
    "score": -1.000278899248928,
    "rmse": 1.000278899248928
}
[08/28/2025 05:25:39 INFO]: Val accuracy: {
    "score": -0.6727716314167428,
    "rmse": 0.6727716314167428
}
[08/28/2025 05:25:39 INFO]: Test accuracy: {
    "score": -0.8796611397586601,
    "rmse": 0.8796611397586601
}
[08/28/2025 05:25:39 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_75",
    "best_epoch": 40,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8796611397586601,
        "rmse": 0.8796611397586601
    },
    "train_stats": {
        "score": -1.000278899248928,
        "rmse": 1.000278899248928
    },
    "val_stats": {
        "score": -0.6727716314167428,
        "rmse": 0.6727716314167428
    }
}
[08/28/2025 05:25:39 INFO]: Procewss finished for trial blotchy-Amado_trial_75
[08/28/2025 05:25:39 INFO]: 
_________________________________________________

[08/28/2025 05:25:39 INFO]: train_net_for_optune.py main() running.
[08/28/2025 05:25:39 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.555763486429945
  attention_dropout: 0.04509336240997216
  ffn_dropout: 0.04509336240997216
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.7295519969833946e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_111

[08/28/2025 05:25:39 INFO]: This ft_transformer has 9.746 million parameters.
[08/28/2025 05:25:39 INFO]: Training will start at epoch 0.
[08/28/2025 05:25:39 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 05:26:29 INFO]: Training loss at epoch 18: 1.311091423034668
[08/28/2025 05:26:34 INFO]: Training loss at epoch 14: 1.1634178757667542
[08/28/2025 05:26:57 INFO]: Training loss at epoch 70: 1.1584852933883667
[08/28/2025 05:27:04 INFO]: Training loss at epoch 12: 0.7574598789215088
[08/28/2025 05:27:13 INFO]: Training loss at epoch 22: 0.6984068900346756
[08/28/2025 05:27:31 INFO]: Training loss at epoch 8: 0.9990291595458984
[08/28/2025 05:27:35 INFO]: Training loss at epoch 71: 0.6733973026275635
[08/28/2025 05:27:57 INFO]: Training loss at epoch 57: 0.8853811919689178
[08/28/2025 05:28:10 INFO]: Training loss at epoch 14: 1.1694547533988953
[08/28/2025 05:28:11 INFO]: New best epoch, val score: -0.6742849094222422
[08/28/2025 05:28:11 INFO]: Saving model to: blotchy-Amado_trial_110/model_best.pth
[08/28/2025 05:28:23 INFO]: Training loss at epoch 25: 1.0874667465686798
[08/28/2025 05:28:41 INFO]: Training loss at epoch 23: 0.943754106760025
[08/28/2025 05:28:55 INFO]: Running Final Evaluation...
[08/28/2025 05:29:02 INFO]: Training loss at epoch 16: 0.8540197610855103
[08/28/2025 05:29:21 INFO]: Training loss at epoch 12: 0.9133999347686768
[08/28/2025 05:29:26 INFO]: Training loss at epoch 14: 0.926743894815445
[08/28/2025 05:29:48 INFO]: Training loss at epoch 0: 1.303592324256897
[08/28/2025 05:29:55 INFO]: Training loss at epoch 11: 0.9925562739372253
[08/28/2025 05:30:21 INFO]: New best epoch, val score: -0.8501742985769642
[08/28/2025 05:30:21 INFO]: Saving model to: blotchy-Amado_trial_111/model_best.pth
[08/28/2025 05:30:23 INFO]: Training loss at epoch 9: 0.9462644159793854
[08/28/2025 05:30:24 INFO]: Training loss at epoch 71: 1.3059214651584625
[08/28/2025 05:31:28 INFO]: Training loss at epoch 35: 0.9540321230888367
[08/28/2025 05:31:30 INFO]: Training loss at epoch 58: 0.9145469963550568
[08/28/2025 05:31:38 INFO]: Training loss at epoch 19: 0.9932355284690857
[08/28/2025 05:31:40 INFO]: Training loss at epoch 23: 0.7040597498416901
[08/28/2025 05:31:52 INFO]: Training loss at epoch 12: 1.033861219882965
[08/28/2025 05:32:01 INFO]: Training loss at epoch 8: 1.0504646301269531
[08/28/2025 05:32:11 INFO]: Training stats: {
    "score": -0.9753139188161143,
    "rmse": 0.9753139188161143
}
[08/28/2025 05:32:11 INFO]: Val stats: {
    "score": -0.6799817283868003,
    "rmse": 0.6799817283868003
}
[08/28/2025 05:32:11 INFO]: Test stats: {
    "score": -0.8826326505501295,
    "rmse": 0.8826326505501295
}
[08/28/2025 05:32:48 INFO]: Training accuracy: {
    "score": -0.9847252571266547,
    "rmse": 0.9847252571266547
}
[08/28/2025 05:32:48 INFO]: Val accuracy: {
    "score": -0.6609929428389674,
    "rmse": 0.6609929428389674
}
[08/28/2025 05:32:48 INFO]: Test accuracy: {
    "score": -0.871947081376698,
    "rmse": 0.871947081376698
}
[08/28/2025 05:32:48 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_31",
    "best_epoch": 40,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.871947081376698,
        "rmse": 0.871947081376698
    },
    "train_stats": {
        "score": -0.9847252571266547,
        "rmse": 0.9847252571266547
    },
    "val_stats": {
        "score": -0.6609929428389674,
        "rmse": 0.6609929428389674
    }
}
[08/28/2025 05:32:48 INFO]: Procewss finished for trial blotchy-Amado_trial_31
[08/28/2025 05:32:48 INFO]: 
_________________________________________________

[08/28/2025 05:32:48 INFO]: train_net_for_optune.py main() running.
[08/28/2025 05:32:48 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 1.6175672326536432
  attention_dropout: 0.03863771278357625
  ffn_dropout: 0.03863771278357625
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.1981896710393103e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_112

[08/28/2025 05:32:48 INFO]: This ft_transformer has 7.778 million parameters.
[08/28/2025 05:32:48 INFO]: Training will start at epoch 0.
[08/28/2025 05:32:48 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 05:32:59 INFO]: Training loss at epoch 9: 0.9384787082672119
[08/28/2025 05:33:18 INFO]: Training stats: {
    "score": -0.9522990236595249,
    "rmse": 0.9522990236595249
}
[08/28/2025 05:33:18 INFO]: Val stats: {
    "score": -0.680217070037672,
    "rmse": 0.680217070037672
}
[08/28/2025 05:33:18 INFO]: Test stats: {
    "score": -0.8863005587032466,
    "rmse": 0.8863005587032466
}
[08/28/2025 05:33:27 INFO]: Training loss at epoch 17: 1.0639757812023163
[08/28/2025 05:33:28 INFO]: Training loss at epoch 26: 0.901626855134964
[08/28/2025 05:33:45 INFO]: Training loss at epoch 72: 0.8954636454582214
[08/28/2025 05:33:47 INFO]: Training loss at epoch 13: 1.1046420335769653
[08/28/2025 05:33:47 INFO]: Training loss at epoch 24: 1.0681977272033691
[08/28/2025 05:33:49 INFO]: Training loss at epoch 15: 1.14181649684906
[08/28/2025 05:34:12 INFO]: Running Final Evaluation...
[08/28/2025 05:34:19 INFO]: Training loss at epoch 12: 0.8990916907787323
[08/28/2025 05:34:21 INFO]: Training loss at epoch 1: 1.529536783695221
[08/28/2025 05:34:52 INFO]: Training stats: {
    "score": -0.9846171388487321,
    "rmse": 0.9846171388487321
}
[08/28/2025 05:34:52 INFO]: Val stats: {
    "score": -0.66819571756982,
    "rmse": 0.66819571756982
}
[08/28/2025 05:34:52 INFO]: Test stats: {
    "score": -0.9104192648795341,
    "rmse": 0.9104192648795341
}
[08/28/2025 05:34:54 INFO]: New best epoch, val score: -0.8169675721927137
[08/28/2025 05:34:54 INFO]: Saving model to: blotchy-Amado_trial_111/model_best.pth
[08/28/2025 05:34:57 INFO]: Training loss at epoch 15: 1.0107156932353973
[08/28/2025 05:35:00 INFO]: Training loss at epoch 59: 0.9173471331596375
[08/28/2025 05:35:26 INFO]: Training accuracy: {
    "score": -1.0012328640446981,
    "rmse": 1.0012328640446981
}
[08/28/2025 05:35:26 INFO]: Val accuracy: {
    "score": -0.6500542783284797,
    "rmse": 0.6500542783284797
}
[08/28/2025 05:35:26 INFO]: Test accuracy: {
    "score": -0.8802266091379936,
    "rmse": 0.8802266091379936
}
[08/28/2025 05:35:26 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_80",
    "best_epoch": 41,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8802266091379936,
        "rmse": 0.8802266091379936
    },
    "train_stats": {
        "score": -1.0012328640446981,
        "rmse": 1.0012328640446981
    },
    "val_stats": {
        "score": -0.6500542783284797,
        "rmse": 0.6500542783284797
    }
}
[08/28/2025 05:35:26 INFO]: Procewss finished for trial blotchy-Amado_trial_80
[08/28/2025 05:35:27 INFO]: 
_________________________________________________

[08/28/2025 05:35:27 INFO]: train_net_for_optune.py main() running.
[08/28/2025 05:35:27 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.5561434732466575
  attention_dropout: 0.03709369672314094
  ffn_dropout: 0.03709369672314094
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.2557481019112024e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_113

[08/28/2025 05:35:27 INFO]: This ft_transformer has 12.430 million parameters.
[08/28/2025 05:35:27 INFO]: Training will start at epoch 0.
[08/28/2025 05:35:27 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 05:35:28 INFO]: New best epoch, val score: -0.66819571756982
[08/28/2025 05:35:28 INFO]: Saving model to: blotchy-Amado_trial_110/model_best.pth
[08/28/2025 05:36:01 INFO]: Training loss at epoch 24: 0.8744502067565918
[08/28/2025 05:36:14 INFO]: Training stats: {
    "score": -0.9739401601736518,
    "rmse": 0.9739401601736518
}
[08/28/2025 05:36:14 INFO]: Val stats: {
    "score": -0.6671315644559955,
    "rmse": 0.6671315644559955
}
[08/28/2025 05:36:14 INFO]: Test stats: {
    "score": -0.8889025522780073,
    "rmse": 0.8889025522780073
}
[08/28/2025 05:36:15 INFO]: Training loss at epoch 0: 1.2255083322525024
[08/28/2025 05:36:20 INFO]: Training loss at epoch 15: 1.147920846939087
[08/28/2025 05:36:44 INFO]: New best epoch, val score: -0.660136891092942
[08/28/2025 05:36:44 INFO]: Saving model to: blotchy-Amado_trial_112/model_best.pth
[08/28/2025 05:37:20 INFO]: Training loss at epoch 36: 1.1505859196186066
[08/28/2025 05:37:40 INFO]: Training loss at epoch 10: 0.99204221367836
[08/28/2025 05:37:50 INFO]: Training loss at epoch 18: 1.2178484499454498
[08/28/2025 05:38:10 INFO]: Training loss at epoch 14: 1.0386714041233063
[08/28/2025 05:38:13 INFO]: Training loss at epoch 16: 0.9394056499004364
[08/28/2025 05:38:20 INFO]: Training loss at epoch 20: 0.9799172878265381
[08/28/2025 05:38:31 INFO]: Training loss at epoch 27: 1.0481118261814117
[08/28/2025 05:38:37 INFO]: Training loss at epoch 13: 1.1247339844703674
[08/28/2025 05:38:44 INFO]: Training loss at epoch 13: 1.0374800860881805
[08/28/2025 05:38:53 INFO]: Training loss at epoch 25: 1.0506903529167175
[08/28/2025 05:38:55 INFO]: Training loss at epoch 2: 1.2726944088935852
[08/28/2025 05:39:17 INFO]: Training loss at epoch 13: 1.0689601004123688
[08/28/2025 05:39:27 INFO]: New best epoch, val score: -0.6771193752106344
[08/28/2025 05:39:27 INFO]: Saving model to: blotchy-Amado_trial_111/model_best.pth
[08/28/2025 05:39:46 INFO]: Training loss at epoch 60: 0.8134844601154327
[08/28/2025 05:40:10 INFO]: Training loss at epoch 1: 0.9663501381874084
[08/28/2025 05:40:14 INFO]: Training loss at epoch 0: 1.4025165438652039
[08/28/2025 05:40:16 INFO]: Training loss at epoch 10: 1.0459606051445007
[08/28/2025 05:40:24 INFO]: Training loss at epoch 25: 0.7444911897182465
[08/28/2025 05:40:51 INFO]: New best epoch, val score: -0.7431317331555732
[08/28/2025 05:40:51 INFO]: Saving model to: blotchy-Amado_trial_113/model_best.pth
[08/28/2025 05:42:11 INFO]: Training loss at epoch 19: 1.2020983695983887
[08/28/2025 05:42:35 INFO]: Training loss at epoch 15: 0.9703661203384399
[08/28/2025 05:42:37 INFO]: Training loss at epoch 17: 1.0794482231140137
[08/28/2025 05:43:06 INFO]: Training loss at epoch 11: 0.9644457697868347
[08/28/2025 05:43:09 INFO]: Training loss at epoch 14: 0.947781503200531
[08/28/2025 05:43:14 INFO]: Training loss at epoch 16: 0.931172251701355
[08/28/2025 05:43:14 INFO]: Training loss at epoch 61: 0.9130081236362457
[08/28/2025 05:43:15 INFO]: Training loss at epoch 37: 0.84489905834198
[08/28/2025 05:43:25 INFO]: Training loss at epoch 21: 0.7800244390964508
[08/28/2025 05:43:28 INFO]: Training loss at epoch 3: 1.128761351108551
[08/28/2025 05:43:35 INFO]: Training loss at epoch 28: 0.9021249413490295
[08/28/2025 05:43:41 INFO]: Training stats: {
    "score": -0.9924675094832451,
    "rmse": 0.9924675094832451
}
[08/28/2025 05:43:41 INFO]: Val stats: {
    "score": -0.7568198811758167,
    "rmse": 0.7568198811758167
}
[08/28/2025 05:43:41 INFO]: Test stats: {
    "score": -0.9481618097730817,
    "rmse": 0.9481618097730817
}
[08/28/2025 05:43:56 INFO]: Training loss at epoch 26: 0.9805809557437897
[08/28/2025 05:44:04 INFO]: Training loss at epoch 2: 1.0092368125915527
[08/28/2025 05:44:16 INFO]: Training loss at epoch 9: 1.0188071131706238
[08/28/2025 05:44:28 INFO]: Training loss at epoch 16: 0.9809873700141907
[08/28/2025 05:44:45 INFO]: Training loss at epoch 26: 0.9872131943702698
[08/28/2025 05:45:23 INFO]: Training loss at epoch 14: 0.9790901839733124
[08/28/2025 05:45:39 INFO]: Training loss at epoch 1: 1.1026855409145355
[08/28/2025 05:45:41 INFO]: Training loss at epoch 11: 1.0127865970134735
[08/28/2025 05:46:44 INFO]: Training loss at epoch 62: 0.8026175200939178
[08/28/2025 05:47:01 INFO]: Training loss at epoch 16: 0.9240528643131256
[08/28/2025 05:47:01 INFO]: Training loss at epoch 18: 1.3004388213157654
[08/28/2025 05:47:34 INFO]: Training loss at epoch 15: 0.9855021238327026
[08/28/2025 05:47:59 INFO]: Training loss at epoch 3: 1.096289575099945
[08/28/2025 05:48:01 INFO]: Training loss at epoch 4: 1.0259768068790436
[08/28/2025 05:48:05 INFO]: Training loss at epoch 20: 0.715783104300499
[08/28/2025 05:48:27 INFO]: Training loss at epoch 22: 0.9657336473464966
[08/28/2025 05:48:31 INFO]: Training stats: {
    "score": -0.9834530569802094,
    "rmse": 0.9834530569802094
}
[08/28/2025 05:48:31 INFO]: Val stats: {
    "score": -0.6961493259888127,
    "rmse": 0.6961493259888127
}
[08/28/2025 05:48:31 INFO]: Test stats: {
    "score": -0.8826980845634655,
    "rmse": 0.8826980845634655
}
[08/28/2025 05:48:33 INFO]: Training loss at epoch 12: 0.8886384069919586
[08/28/2025 05:48:37 INFO]: Training loss at epoch 29: 0.9745575189590454
[08/28/2025 05:49:03 INFO]: Training loss at epoch 27: 1.1596751809120178
[08/28/2025 05:49:10 INFO]: Training loss at epoch 38: 0.7890995442867279
[08/28/2025 05:49:12 INFO]: Training loss at epoch 27: 0.9442909359931946
[08/28/2025 05:50:14 INFO]: Training loss at epoch 63: 1.010472595691681
[08/28/2025 05:50:22 INFO]: Training stats: {
    "score": -0.9176777142979695,
    "rmse": 0.9176777142979695
}
[08/28/2025 05:50:22 INFO]: Val stats: {
    "score": -0.7121845763293366,
    "rmse": 0.7121845763293366
}
[08/28/2025 05:50:22 INFO]: Test stats: {
    "score": -0.9069855730010825,
    "rmse": 0.9069855730010825
}
[08/28/2025 05:51:06 INFO]: Training loss at epoch 2: 1.4684850573539734
[08/28/2025 05:51:08 INFO]: Training loss at epoch 12: 1.1107285618782043
[08/28/2025 05:51:26 INFO]: Training loss at epoch 14: 0.8390036821365356
[08/28/2025 05:51:26 INFO]: Training loss at epoch 19: 1.0126934349536896
[08/28/2025 05:51:27 INFO]: Training loss at epoch 17: 0.8597602844238281
[08/28/2025 05:51:32 INFO]: Training loss at epoch 17: 1.4876132607460022
[08/28/2025 05:51:53 INFO]: Training loss at epoch 4: 0.8465084433555603
[08/28/2025 05:52:01 INFO]: Training loss at epoch 16: 0.9071312248706818
[08/28/2025 05:52:10 INFO]: Training loss at epoch 15: 0.9253404438495636
[08/28/2025 05:52:28 INFO]: Training loss at epoch 21: 1.0013337135314941
[08/28/2025 05:52:35 INFO]: Training loss at epoch 5: 1.1090978384017944
[08/28/2025 05:52:39 INFO]: Training loss at epoch 17: 0.7149323523044586
[08/28/2025 05:52:58 INFO]: Training stats: {
    "score": -1.082663182191135,
    "rmse": 1.082663182191135
}
[08/28/2025 05:52:58 INFO]: Val stats: {
    "score": -0.7340677106226066,
    "rmse": 0.7340677106226066
}
[08/28/2025 05:52:58 INFO]: Test stats: {
    "score": -0.9509457112266568,
    "rmse": 0.9509457112266568
}
[08/28/2025 05:53:31 INFO]: Training loss at epoch 23: 1.1841143369674683
[08/28/2025 05:53:36 INFO]: Training loss at epoch 28: 0.7262703031301498
[08/28/2025 05:53:45 INFO]: Training loss at epoch 64: 0.7572041153907776
[08/28/2025 05:54:04 INFO]: Training loss at epoch 13: 1.0222561359405518
[08/28/2025 05:54:10 INFO]: Training loss at epoch 28: 1.0492055416107178
[08/28/2025 05:55:07 INFO]: Training loss at epoch 39: 0.9099164307117462
[08/28/2025 05:55:27 INFO]: Training loss at epoch 30: 0.7789500653743744
[08/28/2025 05:55:49 INFO]: Training loss at epoch 5: 0.8820666968822479
[08/28/2025 05:55:55 INFO]: Training loss at epoch 18: 0.7192207723855972
[08/28/2025 05:56:29 INFO]: Training loss at epoch 17: 0.8284540176391602
[08/28/2025 05:56:34 INFO]: Training loss at epoch 3: 1.3019559979438782
[08/28/2025 05:56:35 INFO]: Training loss at epoch 13: 0.83736851811409
[08/28/2025 05:56:51 INFO]: Training loss at epoch 22: 0.9149209558963776
[08/28/2025 05:57:07 INFO]: Training loss at epoch 6: 0.9276702702045441
[08/28/2025 05:57:08 INFO]: Training stats: {
    "score": -0.9674148968438493,
    "rmse": 0.9674148968438493
}
[08/28/2025 05:57:08 INFO]: Val stats: {
    "score": -0.7322739888936268,
    "rmse": 0.7322739888936268
}
[08/28/2025 05:57:08 INFO]: Test stats: {
    "score": -0.9222121502524818,
    "rmse": 0.9222121502524818
}
[08/28/2025 05:57:12 INFO]: New best epoch, val score: -0.6637024179219582
[08/28/2025 05:57:12 INFO]: Saving model to: blotchy-Amado_trial_113/model_best.pth
[08/28/2025 05:57:15 INFO]: Training loss at epoch 65: 0.9197142422199249
[08/28/2025 05:57:24 INFO]: Training loss at epoch 20: 1.0094877779483795
[08/28/2025 05:57:58 INFO]: Training loss at epoch 29: 0.8979296386241913
[08/28/2025 05:58:34 INFO]: Training loss at epoch 24: 0.895027369260788
[08/28/2025 05:58:59 INFO]: Training loss at epoch 16: 1.141081303358078
[08/28/2025 05:59:14 INFO]: Training loss at epoch 29: 0.9499214887619019
[08/28/2025 05:59:28 INFO]: Training stats: {
    "score": -0.9448280817163494,
    "rmse": 0.9448280817163494
}
[08/28/2025 05:59:28 INFO]: Val stats: {
    "score": -0.7108927552448305,
    "rmse": 0.7108927552448305
}
[08/28/2025 05:59:28 INFO]: Test stats: {
    "score": -0.899454789842642,
    "rmse": 0.899454789842642
}
[08/28/2025 05:59:31 INFO]: Training loss at epoch 14: 0.8964549601078033
[08/28/2025 05:59:43 INFO]: Training loss at epoch 6: 0.8239130675792694
[08/28/2025 05:59:53 INFO]: Training loss at epoch 18: 1.033687710762024
[08/28/2025 06:00:24 INFO]: Training loss at epoch 19: 0.8948647975921631
[08/28/2025 06:00:28 INFO]: Training loss at epoch 31: 0.7772710621356964
[08/28/2025 06:00:45 INFO]: Training loss at epoch 66: 1.0125029981136322
[08/28/2025 06:00:49 INFO]: Training loss at epoch 18: 0.8869999051094055
[08/28/2025 06:00:51 INFO]: Training loss at epoch 10: 0.9253551661968231
[08/28/2025 06:00:52 INFO]: New best epoch, val score: -0.6840852720387198
[08/28/2025 06:00:52 INFO]: Saving model to: blotchy-Amado_trial_98/model_best.pth
[08/28/2025 06:00:54 INFO]: Training loss at epoch 18: 0.8443502485752106
[08/28/2025 06:00:58 INFO]: Training stats: {
    "score": -0.98946031574272,
    "rmse": 0.98946031574272
}
[08/28/2025 06:00:58 INFO]: Val stats: {
    "score": -0.7738570131191695,
    "rmse": 0.7738570131191695
}
[08/28/2025 06:00:58 INFO]: Test stats: {
    "score": -0.9237036742522303,
    "rmse": 0.9237036742522303
}
[08/28/2025 06:01:14 INFO]: Training loss at epoch 23: 1.027626395225525
[08/28/2025 06:01:40 INFO]: Training loss at epoch 7: 1.1487848162651062
[08/28/2025 06:01:47 INFO]: Training loss at epoch 21: 0.9046926498413086
[08/28/2025 06:01:55 INFO]: Training stats: {
    "score": -0.9565342582451157,
    "rmse": 0.9565342582451157
}
[08/28/2025 06:01:55 INFO]: Val stats: {
    "score": -0.7327458565026899,
    "rmse": 0.7327458565026899
}
[08/28/2025 06:01:55 INFO]: Test stats: {
    "score": -0.9165856823406354,
    "rmse": 0.9165856823406354
}
[08/28/2025 06:02:01 INFO]: Training loss at epoch 14: 1.0087834596633911
[08/28/2025 06:02:01 INFO]: Training loss at epoch 4: 1.2055431008338928
[08/28/2025 06:03:10 INFO]: Training loss at epoch 40: 0.9512487947940826
[08/28/2025 06:03:38 INFO]: Training loss at epoch 7: 1.1388521790504456
[08/28/2025 06:03:38 INFO]: Training loss at epoch 25: 1.0184742510318756
[08/28/2025 06:03:39 INFO]: Training loss at epoch 15: 0.9581019878387451
[08/28/2025 06:03:51 INFO]: Training loss at epoch 30: 0.79954594373703
[08/28/2025 06:04:15 INFO]: Training loss at epoch 67: 1.136813223361969
[08/28/2025 06:05:00 INFO]: Training loss at epoch 15: 1.1782636046409607
[08/28/2025 06:05:24 INFO]: Training loss at epoch 19: 0.9063380360603333
[08/28/2025 06:05:34 INFO]: Training loss at epoch 32: 0.9644753336906433
[08/28/2025 06:05:40 INFO]: Training loss at epoch 24: 0.8674843609333038
[08/28/2025 06:05:50 INFO]: Training loss at epoch 17: 0.9598792195320129
[08/28/2025 06:06:08 INFO]: Training loss at epoch 30: 0.9544877707958221
[08/28/2025 06:06:15 INFO]: Training loss at epoch 22: 1.2543712258338928
[08/28/2025 06:06:17 INFO]: Training loss at epoch 8: 1.1077076196670532
[08/28/2025 06:06:25 INFO]: Training loss at epoch 20: 0.9664667248725891
[08/28/2025 06:06:55 INFO]: Training stats: {
    "score": -0.9275974550182848,
    "rmse": 0.9275974550182848
}
[08/28/2025 06:06:55 INFO]: Val stats: {
    "score": -0.686357435016354,
    "rmse": 0.686357435016354
}
[08/28/2025 06:06:55 INFO]: Test stats: {
    "score": -0.8885693101239925,
    "rmse": 0.8885693101239925
}
[08/28/2025 06:07:34 INFO]: Training loss at epoch 15: 1.1264711320400238
[08/28/2025 06:07:34 INFO]: Training loss at epoch 5: 1.293498456478119
[08/28/2025 06:07:35 INFO]: Training loss at epoch 8: 0.7982776761054993
[08/28/2025 06:07:47 INFO]: Training loss at epoch 68: 0.9933080673217773
[08/28/2025 06:08:17 INFO]: Training loss at epoch 19: 0.9425147771835327
[08/28/2025 06:08:17 INFO]: Training loss at epoch 31: 0.894870400428772
[08/28/2025 06:08:46 INFO]: Training loss at epoch 26: 0.9332465529441833
[08/28/2025 06:09:04 INFO]: Training loss at epoch 19: 1.1256103515625
[08/28/2025 06:09:10 INFO]: Training loss at epoch 41: 0.9405847191810608
[08/28/2025 06:10:07 INFO]: Training loss at epoch 25: 0.9073120951652527
[08/28/2025 06:10:30 INFO]: Training loss at epoch 16: 0.8915931880474091
[08/28/2025 06:10:42 INFO]: Training loss at epoch 23: 0.9905266165733337
[08/28/2025 06:10:43 INFO]: Training loss at epoch 33: 0.8837021887302399
[08/28/2025 06:10:53 INFO]: Training loss at epoch 9: 1.029951810836792
[08/28/2025 06:10:54 INFO]: Training loss at epoch 21: 0.9640623331069946
[08/28/2025 06:11:07 INFO]: Training stats: {
    "score": -0.9820812531254982,
    "rmse": 0.9820812531254982
}
[08/28/2025 06:11:07 INFO]: Val stats: {
    "score": -0.6738430530187692,
    "rmse": 0.6738430530187692
}
[08/28/2025 06:11:07 INFO]: Test stats: {
    "score": -0.8717761102980436,
    "rmse": 0.8717761102980436
}
[08/28/2025 06:11:16 INFO]: Training loss at epoch 31: 1.0044061541557312
[08/28/2025 06:11:18 INFO]: Training loss at epoch 69: 0.8628268241882324
[08/28/2025 06:11:21 INFO]: Training loss at epoch 20: 0.8011772036552429
[08/28/2025 06:11:28 INFO]: Training loss at epoch 9: 1.0132420063018799
[08/28/2025 06:11:48 INFO]: Training stats: {
    "score": -0.9721028782988778,
    "rmse": 0.9721028782988778
}
[08/28/2025 06:11:48 INFO]: Val stats: {
    "score": -0.6721470993665691,
    "rmse": 0.6721470993665691
}
[08/28/2025 06:11:48 INFO]: Test stats: {
    "score": -0.8806454210948585,
    "rmse": 0.8806454210948585
}
[08/28/2025 06:12:03 INFO]: New best epoch, val score: -0.6738430530187692
[08/28/2025 06:12:03 INFO]: Saving model to: blotchy-Amado_trial_98/model_best.pth
[08/28/2025 06:12:22 INFO]: Training stats: {
    "score": -1.0188279145815646,
    "rmse": 1.0188279145815646
}
[08/28/2025 06:12:22 INFO]: Val stats: {
    "score": -0.7552057469149114,
    "rmse": 0.7552057469149114
}
[08/28/2025 06:12:22 INFO]: Test stats: {
    "score": -0.9228788211883415,
    "rmse": 0.9228788211883415
}
[08/28/2025 06:12:29 INFO]: Training stats: {
    "score": -0.9673762515051023,
    "rmse": 0.9673762515051023
}
[08/28/2025 06:12:29 INFO]: Val stats: {
    "score": -0.6752477501732898,
    "rmse": 0.6752477501732898
}
[08/28/2025 06:12:29 INFO]: Test stats: {
    "score": -0.8957126627392952,
    "rmse": 0.8957126627392952
}
[08/28/2025 06:12:35 INFO]: Training loss at epoch 32: 0.8810541033744812
[08/28/2025 06:12:37 INFO]: Training loss at epoch 18: 1.0069687962532043
[08/28/2025 06:12:44 INFO]: Training stats: {
    "score": -0.9894826095970919,
    "rmse": 0.9894826095970919
}
[08/28/2025 06:12:44 INFO]: Val stats: {
    "score": -0.6777264593072824,
    "rmse": 0.6777264593072824
}
[08/28/2025 06:12:44 INFO]: Test stats: {
    "score": -0.87358262221398,
    "rmse": 0.87358262221398
}
[08/28/2025 06:12:56 INFO]: Training loss at epoch 16: 0.9594303667545319
[08/28/2025 06:12:56 INFO]: Training loss at epoch 6: 1.1711269617080688
[08/28/2025 06:13:08 INFO]: Training loss at epoch 11: 1.0754673480987549
[08/28/2025 06:13:45 INFO]: Training loss at epoch 27: 1.053497701883316
[08/28/2025 06:14:27 INFO]: Training loss at epoch 26: 0.9504640102386475
[08/28/2025 06:15:01 INFO]: Training loss at epoch 42: 0.9617373645305634
[08/28/2025 06:15:02 INFO]: Training loss at epoch 24: 0.9668702483177185
[08/28/2025 06:15:19 INFO]: Training loss at epoch 22: 0.7902886867523193
[08/28/2025 06:15:46 INFO]: Training loss at epoch 34: 1.0033059120178223
[08/28/2025 06:15:47 INFO]: Training loss at epoch 21: 0.8865837752819061
[08/28/2025 06:15:50 INFO]: Training loss at epoch 16: 1.1647446751594543
[08/28/2025 06:15:57 INFO]: Training loss at epoch 17: 0.8515369296073914
[08/28/2025 06:16:02 INFO]: Training loss at epoch 70: 0.8606634140014648
[08/28/2025 06:16:20 INFO]: Training loss at epoch 32: 0.9407006204128265
[08/28/2025 06:16:41 INFO]: Training loss at epoch 10: 1.1224026679992676
[08/28/2025 06:16:58 INFO]: Training loss at epoch 10: 1.0423743724822998
[08/28/2025 06:17:01 INFO]: Training loss at epoch 33: 0.7566896378993988
[08/28/2025 06:17:29 INFO]: New best epoch, val score: -0.6640861862204802
[08/28/2025 06:17:29 INFO]: Saving model to: blotchy-Amado_trial_111/model_best.pth
[08/28/2025 06:18:26 INFO]: Training loss at epoch 17: 1.094260334968567
[08/28/2025 06:18:26 INFO]: Training loss at epoch 7: 1.0797547399997711
[08/28/2025 06:18:50 INFO]: Training loss at epoch 28: 0.7636712491512299
[08/28/2025 06:18:50 INFO]: Training loss at epoch 27: 1.2523613572120667
[08/28/2025 06:19:05 INFO]: New best epoch, val score: -0.6647941479339562
[08/28/2025 06:19:05 INFO]: Saving model to: blotchy-Amado_trial_110/model_best.pth
[08/28/2025 06:19:22 INFO]: Training loss at epoch 20: 0.7913662195205688
[08/28/2025 06:19:24 INFO]: Training loss at epoch 25: 0.9257357716560364
[08/28/2025 06:19:28 INFO]: Training loss at epoch 19: 1.0001411437988281
[08/28/2025 06:19:29 INFO]: Training loss at epoch 71: 0.9172378480434418
[08/28/2025 06:19:40 INFO]: Training loss at epoch 23: 0.8213832974433899
[08/28/2025 06:19:56 INFO]: New best epoch, val score: -0.6949011612923804
[08/28/2025 06:19:56 INFO]: Saving model to: blotchy-Amado_trial_106/model_best.pth
[08/28/2025 06:19:58 INFO]: Training loss at epoch 20: 0.9819798469543457
[08/28/2025 06:20:09 INFO]: Training loss at epoch 22: 0.7329563498497009
[08/28/2025 06:20:21 INFO]: New best epoch, val score: -0.6730664967795442
[08/28/2025 06:20:21 INFO]: Saving model to: blotchy-Amado_trial_98/model_best.pth
[08/28/2025 06:20:32 INFO]: Training loss at epoch 11: 0.8564557433128357
[08/28/2025 06:20:48 INFO]: Training loss at epoch 35: 1.077953577041626
[08/28/2025 06:20:54 INFO]: Training loss at epoch 43: 0.7614460289478302
[08/28/2025 06:21:21 INFO]: Training loss at epoch 18: 1.0974899232387543
[08/28/2025 06:21:23 INFO]: Training loss at epoch 34: 0.8061299622058868
[08/28/2025 06:21:26 INFO]: Training loss at epoch 33: 0.9866925179958344
[08/28/2025 06:21:31 INFO]: Training loss at epoch 11: 0.9441021084785461
[08/28/2025 06:21:50 INFO]: Training stats: {
    "score": -0.9501890712855942,
    "rmse": 0.9501890712855942
}
[08/28/2025 06:21:50 INFO]: Val stats: {
    "score": -0.6744011390761339,
    "rmse": 0.6744011390761339
}
[08/28/2025 06:21:50 INFO]: Test stats: {
    "score": -0.8953972241963171,
    "rmse": 0.8953972241963171
}
[08/28/2025 06:23:01 INFO]: Training loss at epoch 72: 0.9821273386478424
[08/28/2025 06:23:17 INFO]: Training loss at epoch 28: 1.0899732410907745
[08/28/2025 06:23:50 INFO]: Training loss at epoch 26: 1.0590556859970093
[08/28/2025 06:23:54 INFO]: Training loss at epoch 29: 1.0504016280174255
[08/28/2025 06:23:56 INFO]: Training loss at epoch 8: 1.0835716724395752
[08/28/2025 06:23:56 INFO]: Training loss at epoch 18: 0.8931588232517242
[08/28/2025 06:24:10 INFO]: Training loss at epoch 24: 0.9336309731006622
[08/28/2025 06:24:22 INFO]: New best epoch, val score: -0.6895465893524578
[08/28/2025 06:24:22 INFO]: Saving model to: blotchy-Amado_trial_106/model_best.pth
[08/28/2025 06:24:28 INFO]: Training loss at epoch 12: 1.1637136340141296
[08/28/2025 06:24:40 INFO]: Training loss at epoch 23: 0.9031396210193634
[08/28/2025 06:25:28 INFO]: Training loss at epoch 12: 1.2051826417446136
[08/28/2025 06:25:38 INFO]: Training stats: {
    "score": -0.9259931385676092,
    "rmse": 0.9259931385676092
}
[08/28/2025 06:25:38 INFO]: Val stats: {
    "score": -0.6926045188813839,
    "rmse": 0.6926045188813839
}
[08/28/2025 06:25:38 INFO]: Test stats: {
    "score": -0.8990278370400545,
    "rmse": 0.8990278370400545
}
[08/28/2025 06:25:48 INFO]: Training loss at epoch 35: 0.8469081819057465
[08/28/2025 06:25:54 INFO]: Training loss at epoch 36: 0.7208288609981537
[08/28/2025 06:26:10 INFO]: Training loss at epoch 12: 1.216339886188507
[08/28/2025 06:26:30 INFO]: Training loss at epoch 73: 1.052158147096634
[08/28/2025 06:26:32 INFO]: Training loss at epoch 34: 1.0127829313278198
[08/28/2025 06:26:49 INFO]: Training loss at epoch 44: 0.9060340821743011
[08/28/2025 06:26:49 INFO]: Training loss at epoch 19: 0.9971795082092285
[08/28/2025 06:27:37 INFO]: Training loss at epoch 29: 0.8417839109897614
[08/28/2025 06:27:40 INFO]: Training loss at epoch 21: 1.0553029775619507
[08/28/2025 06:27:51 INFO]: Training loss at epoch 17: 1.1206844449043274
[08/28/2025 06:28:08 INFO]: Training loss at epoch 21: 0.9765815734863281
[08/28/2025 06:28:13 INFO]: Training loss at epoch 27: 0.9392810463905334
[08/28/2025 06:28:19 INFO]: Training loss at epoch 13: 0.7705726623535156
[08/28/2025 06:28:34 INFO]: Training loss at epoch 20: 0.9494313597679138
[08/28/2025 06:28:36 INFO]: Training stats: {
    "score": -0.9670623871091021,
    "rmse": 0.9670623871091021
}
[08/28/2025 06:28:36 INFO]: Val stats: {
    "score": -0.6768055730728513,
    "rmse": 0.6768055730728513
}
[08/28/2025 06:28:36 INFO]: Test stats: {
    "score": -0.8857106687686211,
    "rmse": 0.8857106687686211
}
[08/28/2025 06:28:37 INFO]: Training loss at epoch 25: 0.8820410668849945
[08/28/2025 06:28:43 INFO]: New best epoch, val score: -0.6852525421970987
[08/28/2025 06:28:43 INFO]: Saving model to: blotchy-Amado_trial_106/model_best.pth
[08/28/2025 06:29:03 INFO]: Training loss at epoch 24: 0.8052146434783936
[08/28/2025 06:29:07 INFO]: Training stats: {
    "score": -0.958014917777907,
    "rmse": 0.958014917777907
}
[08/28/2025 06:29:07 INFO]: Val stats: {
    "score": -0.7039840699103003,
    "rmse": 0.7039840699103003
}
[08/28/2025 06:29:07 INFO]: Test stats: {
    "score": -0.9189618467553483,
    "rmse": 0.9189618467553483
}
[08/28/2025 06:29:21 INFO]: Training loss at epoch 19: 0.8777757585048676
[08/28/2025 06:29:23 INFO]: Training loss at epoch 9: 1.508439540863037
[08/28/2025 06:29:58 INFO]: Training loss at epoch 74: 1.049254834651947
[08/28/2025 06:30:08 INFO]: Training loss at epoch 36: 0.8856304287910461
[08/28/2025 06:30:40 INFO]: Training loss at epoch 30: 0.8397596180438995
[08/28/2025 06:30:44 INFO]: Training loss at epoch 13: 1.1404060125350952
[08/28/2025 06:30:57 INFO]: Training loss at epoch 37: 0.8301480114459991
[08/28/2025 06:31:16 INFO]: Training stats: {
    "score": -0.959708931998543,
    "rmse": 0.959708931998543
}
[08/28/2025 06:31:16 INFO]: Val stats: {
    "score": -0.6669719794381198,
    "rmse": 0.6669719794381198
}
[08/28/2025 06:31:16 INFO]: Test stats: {
    "score": -0.9097223384073541,
    "rmse": 0.9097223384073541
}
[08/28/2025 06:31:17 INFO]: Training stats: {
    "score": -1.0336915491385548,
    "rmse": 1.0336915491385548
}
[08/28/2025 06:31:17 INFO]: Val stats: {
    "score": -0.688665321812761,
    "rmse": 0.688665321812761
}
[08/28/2025 06:31:17 INFO]: Test stats: {
    "score": -0.9112968217458454,
    "rmse": 0.9112968217458454
}
[08/28/2025 06:31:41 INFO]: Training loss at epoch 35: 0.9539963006973267
[08/28/2025 06:32:16 INFO]: Training loss at epoch 14: 1.0399900674819946
[08/28/2025 06:32:40 INFO]: Training loss at epoch 28: 1.0471356511116028
[08/28/2025 06:32:41 INFO]: Training loss at epoch 45: 1.1650202870368958
[08/28/2025 06:33:04 INFO]: Training loss at epoch 26: 1.0514985620975494
[08/28/2025 06:33:09 INFO]: New best epoch, val score: -0.6850228004310952
[08/28/2025 06:33:09 INFO]: Saving model to: blotchy-Amado_trial_106/model_best.pth
[08/28/2025 06:33:32 INFO]: Training loss at epoch 25: 0.7999198734760284
[08/28/2025 06:33:32 INFO]: Training loss at epoch 30: 0.80629101395607
[08/28/2025 06:33:33 INFO]: Training loss at epoch 75: 0.797937273979187
[08/28/2025 06:34:06 INFO]: Training loss at epoch 20: 0.916644424200058
[08/28/2025 06:34:34 INFO]: Training loss at epoch 37: 0.7845460772514343
[08/28/2025 06:35:21 INFO]: Training loss at epoch 14: 0.9317674040794373
[08/28/2025 06:35:25 INFO]: Training loss at epoch 21: 0.7163861840963364
[08/28/2025 06:35:42 INFO]: Training loss at epoch 31: 1.05250883102417
[08/28/2025 06:35:53 INFO]: New best epoch, val score: -0.6582277989395023
[08/28/2025 06:35:53 INFO]: Saving model to: blotchy-Amado_trial_111/model_best.pth
[08/28/2025 06:36:00 INFO]: Training loss at epoch 38: 0.818513810634613
[08/28/2025 06:36:01 INFO]: Training loss at epoch 22: 1.262556403875351
[08/28/2025 06:36:07 INFO]: Training loss at epoch 15: 0.8518365323543549
[08/28/2025 06:36:17 INFO]: Training loss at epoch 22: 1.0153900980949402
[08/28/2025 06:36:38 INFO]: Training loss at epoch 20: 0.8178900182247162
[08/28/2025 06:36:40 INFO]: Training loss at epoch 36: 0.9071585834026337
[08/28/2025 06:36:42 INFO]: Training loss at epoch 10: 1.0996888279914856
[08/28/2025 06:37:00 INFO]: Training loss at epoch 76: 0.9149457812309265
[08/28/2025 06:37:00 INFO]: Training loss at epoch 29: 0.917746901512146
[08/28/2025 06:37:28 INFO]: Training loss at epoch 27: 0.9527392983436584
[08/28/2025 06:37:46 INFO]: Training loss at epoch 13: 0.9463646709918976
[08/28/2025 06:37:55 INFO]: Training loss at epoch 31: 0.8682622015476227
[08/28/2025 06:37:56 INFO]: Training loss at epoch 26: 0.9532515406608582
[08/28/2025 06:38:26 INFO]: Running Final Evaluation...
[08/28/2025 06:38:33 INFO]: Training loss at epoch 46: 0.7039745450019836
[08/28/2025 06:38:33 INFO]: Training stats: {
    "score": -1.0224119403303513,
    "rmse": 1.0224119403303513
}
[08/28/2025 06:38:33 INFO]: Val stats: {
    "score": -0.6885518488478982,
    "rmse": 0.6885518488478982
}
[08/28/2025 06:38:33 INFO]: Test stats: {
    "score": -0.9059276730861667,
    "rmse": 0.9059276730861667
}
[08/28/2025 06:38:55 INFO]: Training loss at epoch 38: 1.0178771018981934
[08/28/2025 06:39:30 INFO]: Training loss at epoch 21: 0.8595415949821472
[08/28/2025 06:39:57 INFO]: Training loss at epoch 15: 0.928549736738205
[08/28/2025 06:39:58 INFO]: Training loss at epoch 18: 1.0307863056659698
[08/28/2025 06:40:01 INFO]: Training accuracy: {
    "score": -1.0148092990067314,
    "rmse": 1.0148092990067314
}
[08/28/2025 06:40:01 INFO]: Val accuracy: {
    "score": -0.6614596171729609,
    "rmse": 0.6614596171729609
}
[08/28/2025 06:40:01 INFO]: Test accuracy: {
    "score": -0.8848205142392197,
    "rmse": 0.8848205142392197
}
[08/28/2025 06:40:01 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_105",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8848205142392197,
        "rmse": 0.8848205142392197
    },
    "train_stats": {
        "score": -1.0148092990067314,
        "rmse": 1.0148092990067314
    },
    "val_stats": {
        "score": -0.6614596171729609,
        "rmse": 0.6614596171729609
    }
}
[08/28/2025 06:40:01 INFO]: Procewss finished for trial blotchy-Amado_trial_105
[08/28/2025 06:40:01 INFO]: 
_________________________________________________

[08/28/2025 06:40:01 INFO]: train_net_for_optune.py main() running.
[08/28/2025 06:40:01 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.5449774397550717
  attention_dropout: 0.03092018490438724
  ffn_dropout: 0.03092018490438724
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.1980780933804575e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_114

[08/28/2025 06:40:01 INFO]: This ft_transformer has 0.811 million parameters.
[08/28/2025 06:40:01 INFO]: Training will start at epoch 0.
[08/28/2025 06:40:01 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 06:40:01 INFO]: Training loss at epoch 16: 0.8110932111740112
[08/28/2025 06:40:36 INFO]: Training loss at epoch 77: 0.8047533333301544
[08/28/2025 06:40:48 INFO]: Training loss at epoch 32: 0.899419754743576
[08/28/2025 06:41:03 INFO]: Training loss at epoch 39: 0.7753610908985138
[08/28/2025 06:41:05 INFO]: Training loss at epoch 0: 1.0556316673755646
[08/28/2025 06:41:14 INFO]: New best epoch, val score: -0.6961938832018533
[08/28/2025 06:41:14 INFO]: Saving model to: blotchy-Amado_trial_114/model_best.pth
[08/28/2025 06:41:48 INFO]: Training loss at epoch 37: 0.8676020503044128
[08/28/2025 06:41:54 INFO]: Training loss at epoch 28: 0.933985024690628
[08/28/2025 06:42:07 INFO]: Training loss at epoch 21: 0.9493581652641296
[08/28/2025 06:42:13 INFO]: Training loss at epoch 11: 1.1036138534545898
[08/28/2025 06:42:14 INFO]: Training loss at epoch 22: 0.8627040386199951
[08/28/2025 06:42:19 INFO]: Training loss at epoch 1: 0.8650467991828918
[08/28/2025 06:42:23 INFO]: Training loss at epoch 27: 0.889252781867981
[08/28/2025 06:42:28 INFO]: New best epoch, val score: -0.6741873045180491
[08/28/2025 06:42:28 INFO]: Saving model to: blotchy-Amado_trial_114/model_best.pth
[08/28/2025 06:42:46 INFO]: Training stats: {
    "score": -0.9381203362648685,
    "rmse": 0.9381203362648685
}
[08/28/2025 06:42:46 INFO]: Val stats: {
    "score": -0.8681611253192468,
    "rmse": 0.8681611253192468
}
[08/28/2025 06:42:46 INFO]: Test stats: {
    "score": -1.0181611889141258,
    "rmse": 1.0181611889141258
}
[08/28/2025 06:42:56 INFO]: Training loss at epoch 30: 0.9488365650177002
[08/28/2025 06:43:20 INFO]: Training loss at epoch 39: 0.7567335963249207
[08/28/2025 06:43:31 INFO]: Training loss at epoch 2: 1.141129195690155
[08/28/2025 06:43:41 INFO]: New best epoch, val score: -0.6640360587765576
[08/28/2025 06:43:41 INFO]: Saving model to: blotchy-Amado_trial_114/model_best.pth
[08/28/2025 06:43:56 INFO]: Training loss at epoch 17: 0.8645898401737213
[08/28/2025 06:44:10 INFO]: Training loss at epoch 78: 0.8166650533676147
[08/28/2025 06:44:22 INFO]: Training loss at epoch 23: 0.9835014641284943
[08/28/2025 06:44:30 INFO]: Training loss at epoch 23: 1.213714063167572
[08/28/2025 06:44:33 INFO]: Training loss at epoch 16: 1.0498071312904358
[08/28/2025 06:44:34 INFO]: Training loss at epoch 47: 1.1266362071037292
[08/28/2025 06:44:44 INFO]: Training loss at epoch 3: 1.1968112587928772
[08/28/2025 06:44:52 INFO]: Training stats: {
    "score": -0.9244592428518331,
    "rmse": 0.9244592428518331
}
[08/28/2025 06:44:52 INFO]: Val stats: {
    "score": -0.7096997934384286,
    "rmse": 0.7096997934384286
}
[08/28/2025 06:44:52 INFO]: Test stats: {
    "score": -0.910424114746345,
    "rmse": 0.910424114746345
}
[08/28/2025 06:45:01 INFO]: Training loss at epoch 22: 0.8852096796035767
[08/28/2025 06:45:50 INFO]: Training loss at epoch 33: 0.8175911903381348
[08/28/2025 06:45:54 INFO]: Training loss at epoch 4: 0.8827739059925079
[08/28/2025 06:46:22 INFO]: Training loss at epoch 29: 0.9154384434223175
[08/28/2025 06:46:47 INFO]: Training loss at epoch 28: 0.6966515183448792
[08/28/2025 06:46:54 INFO]: Training loss at epoch 38: 0.9304898679256439
[08/28/2025 06:47:07 INFO]: Training loss at epoch 5: 1.0736252069473267
[08/28/2025 06:47:20 INFO]: Training loss at epoch 31: 1.2422534227371216
[08/28/2025 06:47:33 INFO]: Training loss at epoch 22: 0.8413548171520233
[08/28/2025 06:47:38 INFO]: Training loss at epoch 79: 0.944111555814743
[08/28/2025 06:47:39 INFO]: Training loss at epoch 12: 0.9579682946205139
[08/28/2025 06:47:49 INFO]: Training loss at epoch 18: 1.1990846395492554
[08/28/2025 06:47:52 INFO]: Training loss at epoch 40: 0.7598526775836945
[08/28/2025 06:47:53 INFO]: Training stats: {
    "score": -0.9190206685743237,
    "rmse": 0.9190206685743237
}
[08/28/2025 06:47:53 INFO]: Val stats: {
    "score": -0.6768374735743848,
    "rmse": 0.6768374735743848
}
[08/28/2025 06:47:53 INFO]: Test stats: {
    "score": -0.8940358576183237,
    "rmse": 0.8940358576183237
}
[08/28/2025 06:48:18 INFO]: Training loss at epoch 6: 0.8933215737342834
[08/28/2025 06:48:56 INFO]: Training stats: {
    "score": -0.9649573596229323,
    "rmse": 0.9649573596229323
}
[08/28/2025 06:48:56 INFO]: Val stats: {
    "score": -0.7069003735815402,
    "rmse": 0.7069003735815402
}
[08/28/2025 06:48:56 INFO]: Test stats: {
    "score": -0.9213669853683922,
    "rmse": 0.9213669853683922
}
[08/28/2025 06:49:03 INFO]: Training loss at epoch 23: 0.9795833826065063
[08/28/2025 06:49:08 INFO]: Training loss at epoch 17: 1.2772593200206757
[08/28/2025 06:49:13 INFO]: Training loss at epoch 40: 0.9235292971134186
[08/28/2025 06:49:30 INFO]: Training loss at epoch 7: 1.2019303441047668
[08/28/2025 06:49:39 INFO]: New best epoch, val score: -0.6639878808624455
[08/28/2025 06:49:39 INFO]: Saving model to: blotchy-Amado_trial_114/model_best.pth
[08/28/2025 06:50:04 INFO]: Training loss at epoch 14: 0.9519698917865753
[08/28/2025 06:50:26 INFO]: Training loss at epoch 23: 1.056088149547577
[08/28/2025 06:50:28 INFO]: Training loss at epoch 48: 0.7583786249160767
[08/28/2025 06:50:43 INFO]: Training loss at epoch 8: 0.9691558182239532
[08/28/2025 06:50:51 INFO]: New best epoch, val score: -0.6639184774839031
[08/28/2025 06:50:51 INFO]: Saving model to: blotchy-Amado_trial_114/model_best.pth
[08/28/2025 06:50:56 INFO]: Training loss at epoch 34: 0.9166187644004822
[08/28/2025 06:51:13 INFO]: Training loss at epoch 29: 0.9419267177581787
[08/28/2025 06:51:42 INFO]: Training loss at epoch 19: 0.9623714983463287
[08/28/2025 06:51:45 INFO]: Training loss at epoch 32: 1.2137656211853027
[08/28/2025 06:51:53 INFO]: Training loss at epoch 9: 1.0934390723705292
[08/28/2025 06:52:01 INFO]: Training loss at epoch 39: 0.9361941516399384
[08/28/2025 06:52:06 INFO]: Training loss at epoch 19: 0.7629746198654175
[08/28/2025 06:52:17 INFO]: Training stats: {
    "score": -0.9980982035830682,
    "rmse": 0.9980982035830682
}
[08/28/2025 06:52:17 INFO]: Val stats: {
    "score": -0.6639508784150978,
    "rmse": 0.6639508784150978
}
[08/28/2025 06:52:17 INFO]: Test stats: {
    "score": -0.8733198049348374,
    "rmse": 0.8733198049348374
}
[08/28/2025 06:52:18 INFO]: Training loss at epoch 30: 0.9337443113327026
[08/28/2025 06:52:22 INFO]: Training loss at epoch 80: 0.901935338973999
[08/28/2025 06:52:35 INFO]: Training loss at epoch 24: 1.1786958575248718
[08/28/2025 06:52:38 INFO]: Training loss at epoch 24: 0.9902356564998627
[08/28/2025 06:52:42 INFO]: Training stats: {
    "score": -0.8940478260632334,
    "rmse": 0.8940478260632334
}
[08/28/2025 06:52:42 INFO]: Val stats: {
    "score": -0.7341505402101288,
    "rmse": 0.7341505402101288
}
[08/28/2025 06:52:42 INFO]: Test stats: {
    "score": -0.9324528118450308,
    "rmse": 0.9324528118450308
}
[08/28/2025 06:52:54 INFO]: Training loss at epoch 41: 0.7747328877449036
[08/28/2025 06:52:58 INFO]: Training loss at epoch 23: 0.788983553647995
[08/28/2025 06:53:01 INFO]: Training stats: {
    "score": -0.9861888855621931,
    "rmse": 0.9861888855621931
}
[08/28/2025 06:53:01 INFO]: Val stats: {
    "score": -0.6724740907512391,
    "rmse": 0.6724740907512391
}
[08/28/2025 06:53:01 INFO]: Test stats: {
    "score": -0.870506941071317,
    "rmse": 0.870506941071317
}
[08/28/2025 06:53:04 INFO]: Training loss at epoch 13: 1.0873134136199951
[08/28/2025 06:53:30 INFO]: Training loss at epoch 10: 1.0865391194820404
[08/28/2025 06:53:34 INFO]: Training loss at epoch 41: 0.852771669626236
[08/28/2025 06:53:36 INFO]: New best epoch, val score: -0.6699559786355497
[08/28/2025 06:53:36 INFO]: Saving model to: blotchy-Amado_trial_98/model_best.pth
[08/28/2025 06:53:39 INFO]: Training loss at epoch 18: 0.9380978047847748
[08/28/2025 06:53:45 INFO]: Training stats: {
    "score": -1.037830685206817,
    "rmse": 1.037830685206817
}
[08/28/2025 06:53:45 INFO]: Val stats: {
    "score": -0.8729385399851626,
    "rmse": 0.8729385399851626
}
[08/28/2025 06:53:45 INFO]: Test stats: {
    "score": -0.9978111392586545,
    "rmse": 0.9978111392586545
}
[08/28/2025 06:54:39 INFO]: Training loss at epoch 11: 0.9750394821166992
[08/28/2025 06:55:47 INFO]: Training loss at epoch 24: 0.815353125333786
[08/28/2025 06:55:50 INFO]: Training loss at epoch 24: 0.9261458516120911
[08/28/2025 06:55:52 INFO]: Training loss at epoch 12: 1.0292809009552002
[08/28/2025 06:55:53 INFO]: Training loss at epoch 81: 0.980634331703186
[08/28/2025 06:55:57 INFO]: Training loss at epoch 35: 0.7057235240936279
[08/28/2025 06:56:08 INFO]: Training loss at epoch 33: 1.1253140568733215
[08/28/2025 06:56:16 INFO]: Training stats: {
    "score": -0.9476862601012987,
    "rmse": 0.9476862601012987
}
[08/28/2025 06:56:16 INFO]: Val stats: {
    "score": -0.7375339584878694,
    "rmse": 0.7375339584878694
}
[08/28/2025 06:56:16 INFO]: Test stats: {
    "score": -0.9001022440681328,
    "rmse": 0.9001022440681328
}
[08/28/2025 06:56:22 INFO]: Training loss at epoch 49: 0.9331498444080353
[08/28/2025 06:56:43 INFO]: Training loss at epoch 31: 0.8136379420757294
[08/28/2025 06:56:55 INFO]: Training loss at epoch 20: 1.2108996510505676
[08/28/2025 06:57:04 INFO]: Training loss at epoch 13: 0.8917006254196167
[08/28/2025 06:57:09 INFO]: Training loss at epoch 30: 0.7965284585952759
[08/28/2025 06:57:55 INFO]: Training loss at epoch 42: 0.8262861669063568
[08/28/2025 06:58:00 INFO]: Training loss at epoch 42: 0.7402834892272949
[08/28/2025 06:58:14 INFO]: Training loss at epoch 19: 0.9030018746852875
[08/28/2025 06:58:17 INFO]: Training loss at epoch 14: 1.0462172031402588
[08/28/2025 06:58:25 INFO]: Training stats: {
    "score": -0.9440234845288099,
    "rmse": 0.9440234845288099
}
[08/28/2025 06:58:25 INFO]: Val stats: {
    "score": -0.7130358643882501,
    "rmse": 0.7130358643882501
}
[08/28/2025 06:58:25 INFO]: Test stats: {
    "score": -0.9203406395113887,
    "rmse": 0.9203406395113887
}
[08/28/2025 06:58:27 INFO]: Training loss at epoch 24: 0.8088112473487854
[08/28/2025 06:58:32 INFO]: Training loss at epoch 14: 1.1435703039169312
[08/28/2025 06:58:53 INFO]: Training loss at epoch 40: 1.187618374824524
[08/28/2025 06:59:08 INFO]: Running Final Evaluation...
[08/28/2025 06:59:23 INFO]: Training loss at epoch 82: 0.984294056892395
[08/28/2025 06:59:27 INFO]: Training loss at epoch 15: 1.0202959477901459
[08/28/2025 06:59:45 INFO]: Training stats: {
    "score": -1.0138504855520099,
    "rmse": 1.0138504855520099
}
[08/28/2025 06:59:45 INFO]: Val stats: {
    "score": -0.7482596009767585,
    "rmse": 0.7482596009767585
}
[08/28/2025 06:59:45 INFO]: Test stats: {
    "score": -0.9179371281885973,
    "rmse": 0.9179371281885973
}
[08/28/2025 07:00:32 INFO]: Training loss at epoch 34: 1.0156245827674866
[08/28/2025 07:00:37 INFO]: Training loss at epoch 16: 1.1734585464000702
[08/28/2025 07:00:46 INFO]: Training loss at epoch 21: 0.9654106497764587
[08/28/2025 07:00:48 INFO]: Training loss at epoch 25: 0.9333190023899078
[08/28/2025 07:00:54 INFO]: Training loss at epoch 25: 0.9388112723827362
[08/28/2025 07:01:00 INFO]: Training loss at epoch 36: 0.9416464269161224
[08/28/2025 07:01:10 INFO]: Training loss at epoch 32: 0.756850004196167
[08/28/2025 07:01:11 INFO]: Training accuracy: {
    "score": -0.9953045426829489,
    "rmse": 0.9953045426829489
}
[08/28/2025 07:01:11 INFO]: Val accuracy: {
    "score": -0.6644064597336625,
    "rmse": 0.6644064597336625
}
[08/28/2025 07:01:11 INFO]: Test accuracy: {
    "score": -0.8796425508059196,
    "rmse": 0.8796425508059196
}
[08/28/2025 07:01:11 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_83",
    "best_epoch": 18,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8796425508059196,
        "rmse": 0.8796425508059196
    },
    "train_stats": {
        "score": -0.9953045426829489,
        "rmse": 0.9953045426829489
    },
    "val_stats": {
        "score": -0.6644064597336625,
        "rmse": 0.6644064597336625
    }
}
[08/28/2025 07:01:11 INFO]: Procewss finished for trial blotchy-Amado_trial_83
[08/28/2025 07:01:11 INFO]: 
_________________________________________________

[08/28/2025 07:01:11 INFO]: train_net_for_optune.py main() running.
[08/28/2025 07:01:11 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.665975346896533
  attention_dropout: 0.37872092136125746
  ffn_dropout: 0.37872092136125746
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.2581634362993796e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_115

[08/28/2025 07:01:11 INFO]: This ft_transformer has 0.836 million parameters.
[08/28/2025 07:01:11 INFO]: Training will start at epoch 0.
[08/28/2025 07:01:11 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 07:01:17 INFO]: Training loss at epoch 25: 1.201523333787918
[08/28/2025 07:01:33 INFO]: Training loss at epoch 31: 0.8124909400939941
[08/28/2025 07:01:49 INFO]: Training loss at epoch 17: 1.1252512633800507
[08/28/2025 07:02:05 INFO]: Running Final Evaluation...
[08/28/2025 07:02:15 INFO]: Training loss at epoch 0: 1.295275092124939
[08/28/2025 07:02:17 INFO]: Training loss at epoch 43: 0.799114465713501
[08/28/2025 07:02:22 INFO]: Training loss at epoch 15: 1.1681617498397827
[08/28/2025 07:02:23 INFO]: New best epoch, val score: -0.8089791719255757
[08/28/2025 07:02:23 INFO]: Saving model to: blotchy-Amado_trial_115/model_best.pth
[08/28/2025 07:02:38 INFO]: Training loss at epoch 25: 0.7752357125282288
[08/28/2025 07:02:52 INFO]: Training loss at epoch 83: 0.9830417037010193
[08/28/2025 07:03:00 INFO]: Training loss at epoch 18: 0.8522902429103851
[08/28/2025 07:03:04 INFO]: Training loss at epoch 43: 0.7580981552600861
[08/28/2025 07:03:28 INFO]: Training loss at epoch 1: 1.037131428718567
[08/28/2025 07:03:37 INFO]: New best epoch, val score: -0.7790996275184257
[08/28/2025 07:03:37 INFO]: Saving model to: blotchy-Amado_trial_115/model_best.pth
[08/28/2025 07:03:52 INFO]: Training accuracy: {
    "score": -1.0037584809515976,
    "rmse": 1.0037584809515976
}
[08/28/2025 07:03:52 INFO]: Val accuracy: {
    "score": -0.6542289093349163,
    "rmse": 0.6542289093349163
}
[08/28/2025 07:03:52 INFO]: Test accuracy: {
    "score": -0.883292016708527,
    "rmse": 0.883292016708527
}
[08/28/2025 07:03:53 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_108",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.883292016708527,
        "rmse": 0.883292016708527
    },
    "train_stats": {
        "score": -1.0037584809515976,
        "rmse": 1.0037584809515976
    },
    "val_stats": {
        "score": -0.6542289093349163,
        "rmse": 0.6542289093349163
    }
}
[08/28/2025 07:03:53 INFO]: Procewss finished for trial blotchy-Amado_trial_108
[08/28/2025 07:03:53 INFO]: 
_________________________________________________

[08/28/2025 07:03:53 INFO]: train_net_for_optune.py main() running.
[08/28/2025 07:03:53 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 1.6626418985117697
  attention_dropout: 0.37628827180801405
  ffn_dropout: 0.37628827180801405
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.2484957203644597e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_116

[08/28/2025 07:03:53 INFO]: This ft_transformer has 9.618 million parameters.
[08/28/2025 07:03:53 INFO]: Training will start at epoch 0.
[08/28/2025 07:03:53 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 07:03:54 INFO]: Training loss at epoch 25: 0.7012299597263336
[08/28/2025 07:03:58 INFO]: Training loss at epoch 41: 0.9173872768878937
[08/28/2025 07:04:00 INFO]: Training loss at epoch 15: 1.0296375751495361
[08/28/2025 07:04:13 INFO]: Training loss at epoch 19: 1.0797098875045776
[08/28/2025 07:04:20 INFO]: Training loss at epoch 20: 0.9326848983764648
[08/28/2025 07:04:39 INFO]: Training stats: {
    "score": -0.9894804244307986,
    "rmse": 0.9894804244307986
}
[08/28/2025 07:04:39 INFO]: Val stats: {
    "score": -0.7083422640350506,
    "rmse": 0.7083422640350506
}
[08/28/2025 07:04:39 INFO]: Test stats: {
    "score": -0.8969248460229565,
    "rmse": 0.8969248460229565
}
[08/28/2025 07:04:41 INFO]: Training loss at epoch 2: 0.9301815927028656
[08/28/2025 07:04:41 INFO]: Training loss at epoch 22: 1.034902036190033
[08/28/2025 07:04:50 INFO]: New best epoch, val score: -0.7507587721765575
[08/28/2025 07:04:50 INFO]: Saving model to: blotchy-Amado_trial_115/model_best.pth
[08/28/2025 07:04:59 INFO]: Training loss at epoch 35: 1.1749430894851685
[08/28/2025 07:05:38 INFO]: Training loss at epoch 33: 0.8762867450714111
[08/28/2025 07:05:51 INFO]: Training loss at epoch 20: 0.8707868754863739
[08/28/2025 07:05:55 INFO]: Training loss at epoch 3: 0.9633018672466278
[08/28/2025 07:06:04 INFO]: New best epoch, val score: -0.726855020281157
[08/28/2025 07:06:04 INFO]: Saving model to: blotchy-Amado_trial_115/model_best.pth
[08/28/2025 07:06:08 INFO]: Training loss at epoch 37: 0.8574414551258087
[08/28/2025 07:06:23 INFO]: Training loss at epoch 84: 0.8174565136432648
[08/28/2025 07:06:41 INFO]: Training loss at epoch 44: 0.9186842143535614
[08/28/2025 07:06:42 INFO]: Running Final Evaluation...
[08/28/2025 07:06:46 INFO]: Training loss at epoch 26: 1.1342844069004059
[08/28/2025 07:07:00 INFO]: Training loss at epoch 21: 0.9097772538661957
[08/28/2025 07:07:04 INFO]: Training loss at epoch 4: 0.880411297082901
[08/28/2025 07:07:13 INFO]: New best epoch, val score: -0.709573784432632
[08/28/2025 07:07:13 INFO]: Saving model to: blotchy-Amado_trial_115/model_best.pth
[08/28/2025 07:07:49 INFO]: Training loss at epoch 0: 1.2677937150001526
[08/28/2025 07:08:07 INFO]: Training loss at epoch 44: 0.6544061005115509
[08/28/2025 07:08:11 INFO]: Training loss at epoch 22: 0.9787949621677399
[08/28/2025 07:08:15 INFO]: Training loss at epoch 5: 0.963800698518753
[08/28/2025 07:08:19 INFO]: New best epoch, val score: -1.0120009562000105
[08/28/2025 07:08:19 INFO]: Saving model to: blotchy-Amado_trial_116/model_best.pth
[08/28/2025 07:08:21 INFO]: Training loss at epoch 20: 1.0682474970817566
[08/28/2025 07:08:24 INFO]: New best epoch, val score: -0.7088087905590175
[08/28/2025 07:08:24 INFO]: Saving model to: blotchy-Amado_trial_115/model_best.pth
[08/28/2025 07:08:27 INFO]: Training accuracy: {
    "score": -0.9861374013820281,
    "rmse": 0.9861374013820281
}
[08/28/2025 07:08:27 INFO]: Val accuracy: {
    "score": -0.6662331720302509,
    "rmse": 0.6662331720302509
}
[08/28/2025 07:08:27 INFO]: Test accuracy: {
    "score": -0.8849938270397438,
    "rmse": 0.8849938270397438
}
[08/28/2025 07:08:27 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_103",
    "best_epoch": 6,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8849938270397438,
        "rmse": 0.8849938270397438
    },
    "train_stats": {
        "score": -0.9861374013820281,
        "rmse": 0.9861374013820281
    },
    "val_stats": {
        "score": -0.6662331720302509,
        "rmse": 0.6662331720302509
    }
}
[08/28/2025 07:08:27 INFO]: Procewss finished for trial blotchy-Amado_trial_103
[08/28/2025 07:08:27 INFO]: 
_________________________________________________

[08/28/2025 07:08:27 INFO]: train_net_for_optune.py main() running.
[08/28/2025 07:08:27 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 16
  d_ffn_factor: 1.389232331484068
  attention_dropout: 0.37692186420612117
  ffn_dropout: 0.37692186420612117
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.684435892862162e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_117

[08/28/2025 07:08:28 INFO]: This ft_transformer has 17.344 million parameters.
[08/28/2025 07:08:28 INFO]: Training will start at epoch 0.
[08/28/2025 07:08:28 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 07:08:31 INFO]: Training loss at epoch 23: 0.8620874583721161
[08/28/2025 07:08:51 INFO]: Training loss at epoch 21: 0.9697195589542389
[08/28/2025 07:08:58 INFO]: Training loss at epoch 26: 0.817737877368927
[08/28/2025 07:09:01 INFO]: Training loss at epoch 42: 1.000497817993164
[08/28/2025 07:09:14 INFO]: Training loss at epoch 26: 0.9071967303752899
[08/28/2025 07:09:19 INFO]: Training loss at epoch 26: 0.871196836233139
[08/28/2025 07:09:23 INFO]: Training loss at epoch 36: 0.9237702786922455
[08/28/2025 07:09:24 INFO]: Training loss at epoch 23: 0.7856459617614746
[08/28/2025 07:09:24 INFO]: Training loss at epoch 16: 1.0412443280220032
[08/28/2025 07:09:26 INFO]: Training loss at epoch 26: 0.7650494575500488
[08/28/2025 07:09:29 INFO]: Training loss at epoch 6: 1.3128777146339417
[08/28/2025 07:09:38 INFO]: New best epoch, val score: -0.7082397858381173
[08/28/2025 07:09:38 INFO]: Saving model to: blotchy-Amado_trial_115/model_best.pth
[08/28/2025 07:09:49 INFO]: Training loss at epoch 85: 0.9674651324748993
[08/28/2025 07:09:53 INFO]: New best epoch, val score: -0.6761884278489837
[08/28/2025 07:09:53 INFO]: Saving model to: blotchy-Amado_trial_106/model_best.pth
[08/28/2025 07:10:02 INFO]: Training loss at epoch 34: 0.8813965618610382
[08/28/2025 07:10:36 INFO]: Training loss at epoch 24: 0.9109797775745392
[08/28/2025 07:10:43 INFO]: Training loss at epoch 7: 0.9901537895202637
[08/28/2025 07:10:52 INFO]: New best epoch, val score: -0.7081058480817166
[08/28/2025 07:10:52 INFO]: Saving model to: blotchy-Amado_trial_115/model_best.pth
[08/28/2025 07:11:04 INFO]: Training loss at epoch 45: 0.7795089185237885
[08/28/2025 07:11:48 INFO]: Training loss at epoch 25: 0.9691178202629089
[08/28/2025 07:11:56 INFO]: Training loss at epoch 8: 1.0503752827644348
[08/28/2025 07:12:14 INFO]: Training loss at epoch 27: 1.0416622161865234
[08/28/2025 07:12:21 INFO]: Training loss at epoch 1: 1.5220332741737366
[08/28/2025 07:12:29 INFO]: Training loss at epoch 24: 1.1709170937538147
[08/28/2025 07:12:54 INFO]: New best epoch, val score: -0.663823589725255
[08/28/2025 07:12:54 INFO]: Saving model to: blotchy-Amado_trial_116/model_best.pth
[08/28/2025 07:13:01 INFO]: Training loss at epoch 26: 0.9016350507736206
[08/28/2025 07:13:08 INFO]: Training loss at epoch 9: 1.1166385412216187
[08/28/2025 07:13:13 INFO]: Training loss at epoch 45: 0.8255756199359894
[08/28/2025 07:13:22 INFO]: Training loss at epoch 86: 0.8271203637123108
[08/28/2025 07:13:28 INFO]: Training loss at epoch 22: 1.0819909572601318
[08/28/2025 07:13:35 INFO]: Training stats: {
    "score": -1.0085060578335756,
    "rmse": 1.0085060578335756
}
[08/28/2025 07:13:35 INFO]: Val stats: {
    "score": -0.7106970479879298,
    "rmse": 0.7106970479879298
}
[08/28/2025 07:13:35 INFO]: Test stats: {
    "score": -0.9066514475337409,
    "rmse": 0.9066514475337409
}
[08/28/2025 07:13:49 INFO]: Training loss at epoch 37: 0.8155246078968048
[08/28/2025 07:14:09 INFO]: Training loss at epoch 43: 1.051732063293457
[08/28/2025 07:14:11 INFO]: Training loss at epoch 27: 0.9378633797168732
[08/28/2025 07:14:19 INFO]: New best epoch, val score: -0.6737539317735863
[08/28/2025 07:14:19 INFO]: Saving model to: blotchy-Amado_trial_106/model_best.pth
[08/28/2025 07:14:29 INFO]: Training loss at epoch 35: 0.8134020864963531
[08/28/2025 07:14:43 INFO]: Training loss at epoch 16: 0.9040287137031555
[08/28/2025 07:14:46 INFO]: Training loss at epoch 10: 1.0529455542564392
[08/28/2025 07:14:47 INFO]: Training loss at epoch 27: 0.8382727205753326
[08/28/2025 07:14:52 INFO]: Training loss at epoch 17: 0.9701767265796661
[08/28/2025 07:15:24 INFO]: Training loss at epoch 28: 0.7720014452934265
[08/28/2025 07:15:25 INFO]: Training loss at epoch 46: 0.8923702538013458
[08/28/2025 07:15:59 INFO]: Training loss at epoch 11: 1.4109694361686707
[08/28/2025 07:16:16 INFO]: Training loss at epoch 27: 1.1625160872936249
[08/28/2025 07:16:21 INFO]: Training loss at epoch 25: 0.9063914716243744
[08/28/2025 07:16:35 INFO]: Training loss at epoch 29: 1.1146590113639832
[08/28/2025 07:16:50 INFO]: Training loss at epoch 2: 0.9038423299789429
[08/28/2025 07:16:50 INFO]: Training loss at epoch 87: 0.9285220801830292
[08/28/2025 07:16:51 INFO]: Training loss at epoch 0: 0.9817380309104919
[08/28/2025 07:16:59 INFO]: Training stats: {
    "score": -0.9826983264951087,
    "rmse": 0.9826983264951087
}
[08/28/2025 07:16:59 INFO]: Val stats: {
    "score": -0.6761837287320688,
    "rmse": 0.6761837287320688
}
[08/28/2025 07:16:59 INFO]: Test stats: {
    "score": -0.8782381882377406,
    "rmse": 0.8782381882377406
}
[08/28/2025 07:17:09 INFO]: Training loss at epoch 12: 1.1123435497283936
[08/28/2025 07:17:09 INFO]: Training loss at epoch 27: 0.820772111415863
[08/28/2025 07:17:15 INFO]: Running Final Evaluation...
[08/28/2025 07:17:31 INFO]: Training loss at epoch 27: 1.0498199164867401
[08/28/2025 07:17:39 INFO]: Training loss at epoch 28: 0.983186811208725
[08/28/2025 07:17:58 INFO]: Training loss at epoch 23: 1.1560417711734772
[08/28/2025 07:17:58 INFO]: New best epoch, val score: -0.776969105345667
[08/28/2025 07:17:58 INFO]: Saving model to: blotchy-Amado_trial_117/model_best.pth
[08/28/2025 07:18:10 INFO]: Training loss at epoch 30: 0.954916387796402
[08/28/2025 07:18:10 INFO]: Training loss at epoch 38: 1.0075135827064514
[08/28/2025 07:18:15 INFO]: Training loss at epoch 46: 0.8884499669075012
[08/28/2025 07:18:22 INFO]: Training loss at epoch 13: 0.9759979844093323
[08/28/2025 07:18:33 INFO]: Training accuracy: {
    "score": -0.9795937686171611,
    "rmse": 0.9795937686171611
}
[08/28/2025 07:18:33 INFO]: Val accuracy: {
    "score": -0.6546187561578269,
    "rmse": 0.6546187561578269
}
[08/28/2025 07:18:33 INFO]: Test accuracy: {
    "score": -0.8832385570258156,
    "rmse": 0.8832385570258156
}
[08/28/2025 07:18:33 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_85",
    "best_epoch": 56,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8832385570258156,
        "rmse": 0.8832385570258156
    },
    "train_stats": {
        "score": -0.9795937686171611,
        "rmse": 0.9795937686171611
    },
    "val_stats": {
        "score": -0.6546187561578269,
        "rmse": 0.6546187561578269
    }
}
[08/28/2025 07:18:33 INFO]: Procewss finished for trial blotchy-Amado_trial_85
[08/28/2025 07:18:33 INFO]: 
_________________________________________________

[08/28/2025 07:18:33 INFO]: train_net_for_optune.py main() running.
[08/28/2025 07:18:33 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 1.5823055999987459
  attention_dropout: 0.2572016289998749
  ffn_dropout: 0.2572016289998749
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.7720048617191147e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_118

[08/28/2025 07:18:33 INFO]: This ft_transformer has 9.366 million parameters.
[08/28/2025 07:18:33 INFO]: Training will start at epoch 0.
[08/28/2025 07:18:33 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 07:18:53 INFO]: Training loss at epoch 36: 1.1616663038730621
[08/28/2025 07:19:15 INFO]: Training loss at epoch 44: 1.1501609086990356
[08/28/2025 07:19:22 INFO]: Training loss at epoch 31: 0.9818567037582397
[08/28/2025 07:19:34 INFO]: Training loss at epoch 14: 1.033368468284607
[08/28/2025 07:19:47 INFO]: Training loss at epoch 47: 0.8604012429714203
[08/28/2025 07:20:12 INFO]: Training loss at epoch 28: 0.8270577788352966
[08/28/2025 07:20:12 INFO]: Training loss at epoch 26: 0.9459949731826782
[08/28/2025 07:20:17 INFO]: Running Final Evaluation...
[08/28/2025 07:20:20 INFO]: Training loss at epoch 18: 0.814550369977951
[08/28/2025 07:20:30 INFO]: Training loss at epoch 21: 1.1555653512477875
[08/28/2025 07:20:35 INFO]: Training loss at epoch 32: 0.888526439666748
[08/28/2025 07:20:47 INFO]: Training loss at epoch 15: 0.8983028531074524
[08/28/2025 07:21:21 INFO]: Training loss at epoch 3: 1.5802929401397705
[08/28/2025 07:21:46 INFO]: Training loss at epoch 33: 1.0238773226737976
[08/28/2025 07:21:49 INFO]: Training accuracy: {
    "score": -0.9778053996165367,
    "rmse": 0.9778053996165367
}
[08/28/2025 07:21:49 INFO]: Val accuracy: {
    "score": -0.6644789065956218,
    "rmse": 0.6644789065956218
}
[08/28/2025 07:21:49 INFO]: Test accuracy: {
    "score": -0.875549933760132,
    "rmse": 0.875549933760132
}
[08/28/2025 07:21:49 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_102",
    "best_epoch": 16,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.875549933760132,
        "rmse": 0.875549933760132
    },
    "train_stats": {
        "score": -0.9778053996165367,
        "rmse": 0.9778053996165367
    },
    "val_stats": {
        "score": -0.6644789065956218,
        "rmse": 0.6644789065956218
    }
}
[08/28/2025 07:21:49 INFO]: Procewss finished for trial blotchy-Amado_trial_102
[08/28/2025 07:21:49 INFO]: 
_________________________________________________

[08/28/2025 07:21:49 INFO]: train_net_for_optune.py main() running.
[08/28/2025 07:21:49 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.571227304795812
  attention_dropout: 0.21423155830950336
  ffn_dropout: 0.21423155830950336
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.7308183619701668e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_119

[08/28/2025 07:21:49 INFO]: This ft_transformer has 0.817 million parameters.
[08/28/2025 07:21:49 INFO]: Training will start at epoch 0.
[08/28/2025 07:21:49 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 07:21:58 INFO]: Training loss at epoch 16: 0.8730244934558868
[08/28/2025 07:22:26 INFO]: Training loss at epoch 0: 1.058902621269226
[08/28/2025 07:22:32 INFO]: Training loss at epoch 24: 1.0003501176834106
[08/28/2025 07:22:35 INFO]: Training loss at epoch 39: 1.068495750427246
[08/28/2025 07:22:52 INFO]: Training loss at epoch 0: 1.0778260231018066
[08/28/2025 07:22:58 INFO]: New best epoch, val score: -0.671818822724529
[08/28/2025 07:22:58 INFO]: Saving model to: blotchy-Amado_trial_118/model_best.pth
[08/28/2025 07:22:58 INFO]: Training loss at epoch 34: 1.0032235085964203
[08/28/2025 07:23:02 INFO]: Training loss at epoch 28: 0.9019125699996948
[08/28/2025 07:23:02 INFO]: New best epoch, val score: -0.662466311421831
[08/28/2025 07:23:02 INFO]: Saving model to: blotchy-Amado_trial_119/model_best.pth
[08/28/2025 07:23:06 INFO]: Training loss at epoch 29: 1.1106529831886292
[08/28/2025 07:23:11 INFO]: Training loss at epoch 17: 1.0505390167236328
[08/28/2025 07:23:19 INFO]: Training loss at epoch 47: 0.7331590950489044
[08/28/2025 07:23:21 INFO]: Training loss at epoch 37: 0.8537603616714478
[08/28/2025 07:24:06 INFO]: Training loss at epoch 1: 1.090835988521576
[08/28/2025 07:24:09 INFO]: Training stats: {
    "score": -0.9766828382930887,
    "rmse": 0.9766828382930887
}
[08/28/2025 07:24:09 INFO]: Val stats: {
    "score": -0.6770926280214761,
    "rmse": 0.6770926280214761
}
[08/28/2025 07:24:09 INFO]: Test stats: {
    "score": -0.8841987664661559,
    "rmse": 0.8841987664661559
}
[08/28/2025 07:24:10 INFO]: Training loss at epoch 27: 0.9035901129245758
[08/28/2025 07:24:11 INFO]: Training loss at epoch 35: 0.9016093313694
[08/28/2025 07:24:24 INFO]: Training loss at epoch 18: 1.0161307454109192
[08/28/2025 07:24:24 INFO]: Training loss at epoch 45: 0.9111312925815582
[08/28/2025 07:24:59 INFO]: Training stats: {
    "score": -0.9415998090652049,
    "rmse": 0.9415998090652049
}
[08/28/2025 07:24:59 INFO]: Val stats: {
    "score": -0.6941917261300242,
    "rmse": 0.6941917261300242
}
[08/28/2025 07:24:59 INFO]: Test stats: {
    "score": -0.8923803009596968,
    "rmse": 0.8923803009596968
}
[08/28/2025 07:25:02 INFO]: Running Final Evaluation...
[08/28/2025 07:25:17 INFO]: Training loss at epoch 2: 1.1503106355667114
[08/28/2025 07:25:22 INFO]: Training loss at epoch 28: 1.0553853809833527
[08/28/2025 07:25:22 INFO]: Training loss at epoch 36: 0.97124382853508
[08/28/2025 07:25:36 INFO]: Training loss at epoch 19: 0.8932249248027802
[08/28/2025 07:25:40 INFO]: Training loss at epoch 29: 0.9287439584732056
[08/28/2025 07:25:47 INFO]: Training loss at epoch 19: 0.8596502542495728
[08/28/2025 07:25:48 INFO]: Training loss at epoch 28: 0.9234344065189362
[08/28/2025 07:25:49 INFO]: Training loss at epoch 4: 1.308023452758789
[08/28/2025 07:26:01 INFO]: Training stats: {
    "score": -1.021944786870534,
    "rmse": 1.021944786870534
}
[08/28/2025 07:26:01 INFO]: Val stats: {
    "score": -0.7502282604727056,
    "rmse": 0.7502282604727056
}
[08/28/2025 07:26:01 INFO]: Test stats: {
    "score": -0.9322202564142138,
    "rmse": 0.9322202564142138
}
[08/28/2025 07:26:25 INFO]: Training loss at epoch 1: 0.8618200719356537
[08/28/2025 07:26:28 INFO]: Training loss at epoch 3: 0.8758437037467957
[08/28/2025 07:26:32 INFO]: Training loss at epoch 37: 1.0345014333724976
[08/28/2025 07:26:45 INFO]: Training accuracy: {
    "score": -0.9928294312960906,
    "rmse": 0.9928294312960906
}
[08/28/2025 07:26:45 INFO]: Val accuracy: {
    "score": -0.6852026562682214,
    "rmse": 0.6852026562682214
}
[08/28/2025 07:26:45 INFO]: Test accuracy: {
    "score": -0.8779991085316725,
    "rmse": 0.8779991085316725
}
[08/28/2025 07:26:45 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_99",
    "best_epoch": 14,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8779991085316725,
        "rmse": 0.8779991085316725
    },
    "train_stats": {
        "score": -0.9928294312960906,
        "rmse": 0.9928294312960906
    },
    "val_stats": {
        "score": -0.6852026562682214,
        "rmse": 0.6852026562682214
    }
}
[08/28/2025 07:26:45 INFO]: Procewss finished for trial blotchy-Amado_trial_99
[08/28/2025 07:26:46 INFO]: 
_________________________________________________

[08/28/2025 07:26:46 INFO]: train_net_for_optune.py main() running.
[08/28/2025 07:26:46 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 16
  d_ffn_factor: 2.555640970425018
  attention_dropout: 0.0026157020490510663
  ffn_dropout: 0.0026157020490510663
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.6889703856347544e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_120

[08/28/2025 07:26:46 INFO]: This ft_transformer has 1.972 million parameters.
[08/28/2025 07:26:46 INFO]: Training will start at epoch 0.
[08/28/2025 07:26:46 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 07:26:50 INFO]: Training loss at epoch 1: 0.9937257468700409
[08/28/2025 07:26:57 INFO]: Training loss at epoch 17: 1.1546231508255005
[08/28/2025 07:27:04 INFO]: Training loss at epoch 25: 0.9245948493480682
[08/28/2025 07:27:12 INFO]: Training loss at epoch 20: 1.032831609249115
[08/28/2025 07:27:27 INFO]: Training stats: {
    "score": -0.9199496676381493,
    "rmse": 0.9199496676381493
}
[08/28/2025 07:27:27 INFO]: Val stats: {
    "score": -0.6675502099241261,
    "rmse": 0.6675502099241261
}
[08/28/2025 07:27:27 INFO]: Test stats: {
    "score": -0.9051224108182166,
    "rmse": 0.9051224108182166
}
[08/28/2025 07:27:31 INFO]: New best epoch, val score: -0.6838339842223286
[08/28/2025 07:27:31 INFO]: Saving model to: blotchy-Amado_trial_117/model_best.pth
[08/28/2025 07:27:34 INFO]: Training stats: {
    "score": -0.965634627685819,
    "rmse": 0.965634627685819
}
[08/28/2025 07:27:34 INFO]: Val stats: {
    "score": -0.6629208598505169,
    "rmse": 0.6629208598505169
}
[08/28/2025 07:27:34 INFO]: Test stats: {
    "score": -0.883011092587708,
    "rmse": 0.883011092587708
}
[08/28/2025 07:27:36 INFO]: Training loss at epoch 4: 1.2696162462234497
[08/28/2025 07:27:41 INFO]: Training loss at epoch 38: 1.121167004108429
[08/28/2025 07:27:44 INFO]: Training loss at epoch 38: 0.8470529019832611
[08/28/2025 07:27:59 INFO]: Training loss at epoch 28: 0.9467180073261261
[08/28/2025 07:28:14 INFO]: New best epoch, val score: -0.6629208598505169
[08/28/2025 07:28:14 INFO]: Saving model to: blotchy-Amado_trial_113/model_best.pth
[08/28/2025 07:28:20 INFO]: Training loss at epoch 48: 0.6400348544120789
[08/28/2025 07:28:24 INFO]: Training loss at epoch 21: 1.2557566165924072
[08/28/2025 07:28:27 INFO]: Training loss at epoch 40: 0.8936099708080292
[08/28/2025 07:28:47 INFO]: Training loss at epoch 5: 0.9518246650695801
[08/28/2025 07:28:53 INFO]: Training loss at epoch 39: 0.9247013926506042
[08/28/2025 07:29:20 INFO]: Training stats: {
    "score": -0.9804706410903887,
    "rmse": 0.9804706410903887
}
[08/28/2025 07:29:20 INFO]: Val stats: {
    "score": -0.6732441605975366,
    "rmse": 0.6732441605975366
}
[08/28/2025 07:29:20 INFO]: Test stats: {
    "score": -0.8778883799648847,
    "rmse": 0.8778883799648847
}
[08/28/2025 07:29:30 INFO]: Running Final Evaluation...
[08/28/2025 07:29:37 INFO]: Training loss at epoch 22: 1.1227391958236694
[08/28/2025 07:29:47 INFO]: Training loss at epoch 29: 0.8641501665115356
[08/28/2025 07:29:52 INFO]: Training loss at epoch 0: 1.6990514993667603
[08/28/2025 07:29:55 INFO]: Training accuracy: {
    "score": -0.9996123675516798,
    "rmse": 0.9996123675516798
}
[08/28/2025 07:29:55 INFO]: Val accuracy: {
    "score": -0.6639184774839031,
    "rmse": 0.6639184774839031
}
[08/28/2025 07:29:55 INFO]: Test accuracy: {
    "score": -0.8737704070480918,
    "rmse": 0.8737704070480918
}
[08/28/2025 07:29:55 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_114",
    "best_epoch": 8,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8737704070480918,
        "rmse": 0.8737704070480918
    },
    "train_stats": {
        "score": -0.9996123675516798,
        "rmse": 0.9996123675516798
    },
    "val_stats": {
        "score": -0.6639184774839031,
        "rmse": 0.6639184774839031
    }
}
[08/28/2025 07:29:55 INFO]: Procewss finished for trial blotchy-Amado_trial_114
[08/28/2025 07:29:55 INFO]: 
_________________________________________________

[08/28/2025 07:29:55 INFO]: train_net_for_optune.py main() running.
[08/28/2025 07:29:55 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 16
  d_ffn_factor: 1.8450139617473587
  attention_dropout: 0.2726256079444639
  ffn_dropout: 0.2726256079444639
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.418666090705548e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_121

[08/28/2025 07:29:55 INFO]: This ft_transformer has 1.305 million parameters.
[08/28/2025 07:29:55 INFO]: Training will start at epoch 0.
[08/28/2025 07:29:55 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 07:30:01 INFO]: Training loss at epoch 6: 0.9777660071849823
[08/28/2025 07:30:18 INFO]: Training loss at epoch 5: 0.9584915339946747
[08/28/2025 07:30:20 INFO]: New best epoch, val score: -0.90546159107099
[08/28/2025 07:30:20 INFO]: Saving model to: blotchy-Amado_trial_120/model_best.pth
[08/28/2025 07:30:23 INFO]: Training loss at epoch 30: 0.9121219515800476
[08/28/2025 07:30:50 INFO]: Training loss at epoch 23: 1.1626462042331696
[08/28/2025 07:31:12 INFO]: Training loss at epoch 7: 0.9158799350261688
[08/28/2025 07:31:13 INFO]: Training loss at epoch 2: 1.2323022484779358
[08/28/2025 07:31:36 INFO]: Training loss at epoch 26: 0.849085658788681
[08/28/2025 07:31:46 INFO]: New best epoch, val score: -0.6714018205633957
[08/28/2025 07:31:46 INFO]: Saving model to: blotchy-Amado_trial_118/model_best.pth
[08/28/2025 07:31:57 INFO]: Training loss at epoch 29: 0.9659579396247864
[08/28/2025 07:32:04 INFO]: Training loss at epoch 24: 1.2736476063728333
[08/28/2025 07:32:10 INFO]: Training stats: {
    "score": -0.9367452967055617,
    "rmse": 0.9367452967055617
}
[08/28/2025 07:32:10 INFO]: Val stats: {
    "score": -0.6823513519732844,
    "rmse": 0.6823513519732844
}
[08/28/2025 07:32:10 INFO]: Test stats: {
    "score": -0.9048446319401542,
    "rmse": 0.9048446319401542
}
[08/28/2025 07:32:13 INFO]: Training loss at epoch 0: 1.0385168194770813
[08/28/2025 07:32:14 INFO]: Training loss at epoch 39: 0.9062356352806091
[08/28/2025 07:32:25 INFO]: Training loss at epoch 8: 0.8958638608455658
[08/28/2025 07:32:33 INFO]: New best epoch, val score: -0.6755701703880892
[08/28/2025 07:32:33 INFO]: Saving model to: blotchy-Amado_trial_121/model_best.pth
[08/28/2025 07:32:39 INFO]: Training loss at epoch 22: 1.022869884967804
[08/28/2025 07:32:54 INFO]: Training loss at epoch 41: 1.0618087351322174
[08/28/2025 07:32:57 INFO]: Training loss at epoch 30: 1.060307264328003
[08/28/2025 07:33:03 INFO]: Training loss at epoch 20: 1.0530685484409332
[08/28/2025 07:33:17 INFO]: Training loss at epoch 25: 0.8800401985645294
[08/28/2025 07:33:18 INFO]: Training stats: {
    "score": -0.9818879923664029,
    "rmse": 0.9818879923664029
}
[08/28/2025 07:33:18 INFO]: Val stats: {
    "score": -0.6764266660621432,
    "rmse": 0.6764266660621432
}
[08/28/2025 07:33:18 INFO]: Test stats: {
    "score": -0.8717519829890652,
    "rmse": 0.8717519829890652
}
[08/28/2025 07:33:26 INFO]: Training loss at epoch 49: 0.699884831905365
[08/28/2025 07:33:27 INFO]: Training loss at epoch 1: 1.4657368659973145
[08/28/2025 07:33:33 INFO]: Training loss at epoch 29: 1.0279760956764221
[08/28/2025 07:33:36 INFO]: Training loss at epoch 9: 1.2534664273262024
[08/28/2025 07:33:42 INFO]: New best epoch, val score: -0.6612037937151765
[08/28/2025 07:33:42 INFO]: Saving model to: blotchy-Amado_trial_113/model_best.pth
[08/28/2025 07:33:44 INFO]: Training stats: {
    "score": -0.8895819006284627,
    "rmse": 0.8895819006284627
}
[08/28/2025 07:33:44 INFO]: Val stats: {
    "score": -0.6764653344410246,
    "rmse": 0.6764653344410246
}
[08/28/2025 07:33:44 INFO]: Test stats: {
    "score": -0.9091098644087598,
    "rmse": 0.9091098644087598
}
[08/28/2025 07:33:53 INFO]: New best epoch, val score: -0.7326502638080369
[08/28/2025 07:33:53 INFO]: Saving model to: blotchy-Amado_trial_120/model_best.pth
[08/28/2025 07:34:01 INFO]: Training stats: {
    "score": -1.0024360847032454,
    "rmse": 1.0024360847032454
}
[08/28/2025 07:34:01 INFO]: Val stats: {
    "score": -0.7089157024433742,
    "rmse": 0.7089157024433742
}
[08/28/2025 07:34:01 INFO]: Test stats: {
    "score": -0.8927349083144988,
    "rmse": 0.8927349083144988
}
[08/28/2025 07:34:03 INFO]: Training loss at epoch 29: 0.995423823595047
[08/28/2025 07:34:28 INFO]: Training loss at epoch 26: 1.071588158607483
[08/28/2025 07:34:48 INFO]: Training loss at epoch 6: 1.2952359318733215
[08/28/2025 07:34:48 INFO]: Training loss at epoch 1: 0.8656930923461914
[08/28/2025 07:35:09 INFO]: New best epoch, val score: -0.667725954777481
[08/28/2025 07:35:09 INFO]: Saving model to: blotchy-Amado_trial_121/model_best.pth
[08/28/2025 07:35:10 INFO]: Training stats: {
    "score": -0.8264517824087675,
    "rmse": 0.8264517824087675
}
[08/28/2025 07:35:10 INFO]: Val stats: {
    "score": -0.7488043998844766,
    "rmse": 0.7488043998844766
}
[08/28/2025 07:35:10 INFO]: Test stats: {
    "score": -0.9578477030175774,
    "rmse": 0.9578477030175774
}
[08/28/2025 07:35:13 INFO]: Training loss at epoch 10: 0.8546101450920105
[08/28/2025 07:35:39 INFO]: Training loss at epoch 3: 1.406607449054718
[08/28/2025 07:35:41 INFO]: Training loss at epoch 27: 0.9256572127342224
[08/28/2025 07:35:46 INFO]: Running Final Evaluation...
[08/28/2025 07:35:51 INFO]: Training loss at epoch 31: 0.8994922637939453
[08/28/2025 07:35:57 INFO]: Training loss at epoch 2: 1.1240563988685608
[08/28/2025 07:36:12 INFO]: Training loss at epoch 27: 0.7979835867881775
[08/28/2025 07:36:20 INFO]: Training stats: {
    "score": -0.9475924469047604,
    "rmse": 0.9475924469047604
}
[08/28/2025 07:36:20 INFO]: Val stats: {
    "score": -0.7169709787969494,
    "rmse": 0.7169709787969494
}
[08/28/2025 07:36:20 INFO]: Test stats: {
    "score": -0.8937396937923133,
    "rmse": 0.8937396937923133
}
[08/28/2025 07:36:24 INFO]: Training loss at epoch 11: 0.8834174871444702
[08/28/2025 07:36:53 INFO]: Training stats: {
    "score": -0.9640853329404409,
    "rmse": 0.9640853329404409
}
[08/28/2025 07:36:53 INFO]: Val stats: {
    "score": -0.6900974211571674,
    "rmse": 0.6900974211571674
}
[08/28/2025 07:36:53 INFO]: Test stats: {
    "score": -0.8799833951592554,
    "rmse": 0.8799833951592554
}
[08/28/2025 07:36:53 INFO]: Training loss at epoch 28: 1.1341208815574646
[08/28/2025 07:36:59 INFO]: Training loss at epoch 2: 1.115216314792633
[08/28/2025 07:37:09 INFO]: Training loss at epoch 30: 0.9627831876277924
[08/28/2025 07:37:17 INFO]: Training loss at epoch 42: 1.0792662501335144
[08/28/2025 07:37:23 INFO]: Training loss at epoch 2: 1.0273500680923462
[08/28/2025 07:37:25 INFO]: New best epoch, val score: -0.6818629828769597
[08/28/2025 07:37:25 INFO]: Saving model to: blotchy-Amado_trial_120/model_best.pth
[08/28/2025 07:37:32 INFO]: Training accuracy: {
    "score": -0.961653461704372,
    "rmse": 0.961653461704372
}
[08/28/2025 07:37:32 INFO]: Val accuracy: {
    "score": -0.6896581589103737,
    "rmse": 0.6896581589103737
}
[08/28/2025 07:37:32 INFO]: Test accuracy: {
    "score": -0.8969720359168124,
    "rmse": 0.8969720359168124
}
[08/28/2025 07:37:32 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_97",
    "best_epoch": 18,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8969720359168124,
        "rmse": 0.8969720359168124
    },
    "train_stats": {
        "score": -0.961653461704372,
        "rmse": 0.961653461704372
    },
    "val_stats": {
        "score": -0.6896581589103737,
        "rmse": 0.6896581589103737
    }
}
[08/28/2025 07:37:32 INFO]: Procewss finished for trial blotchy-Amado_trial_97
[08/28/2025 07:37:32 INFO]: 
_________________________________________________

[08/28/2025 07:37:32 INFO]: train_net_for_optune.py main() running.
[08/28/2025 07:37:32 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 16
  d_ffn_factor: 1.632260050850274
  attention_dropout: 0.0001641564907161122
  ffn_dropout: 0.0001641564907161122
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.5597182803410598e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_122

[08/28/2025 07:37:32 INFO]: This ft_transformer has 1.218 million parameters.
[08/28/2025 07:37:32 INFO]: Training will start at epoch 0.
[08/28/2025 07:37:32 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 07:37:35 INFO]: Training loss at epoch 12: 0.9601169526576996
[08/28/2025 07:37:47 INFO]: New best epoch, val score: -0.6737080006403366
[08/28/2025 07:37:47 INFO]: Saving model to: blotchy-Amado_trial_106/model_best.pth
[08/28/2025 07:38:04 INFO]: Training loss at epoch 29: 1.0629739165306091
[08/28/2025 07:38:08 INFO]: Training loss at epoch 40: 0.8782623112201691
[08/28/2025 07:38:22 INFO]: Training loss at epoch 31: 0.8591248095035553
[08/28/2025 07:38:28 INFO]: Training loss at epoch 21: 1.1336911916732788
[08/28/2025 07:38:30 INFO]: Training stats: {
    "score": -1.0408797584096485,
    "rmse": 1.0408797584096485
}
[08/28/2025 07:38:30 INFO]: Val stats: {
    "score": -0.7937163162636466,
    "rmse": 0.7937163162636466
}
[08/28/2025 07:38:30 INFO]: Test stats: {
    "score": -0.9629741892468874,
    "rmse": 0.9629741892468874
}
[08/28/2025 07:38:46 INFO]: Training loss at epoch 13: 0.8995623886585236
[08/28/2025 07:38:53 INFO]: Training loss at epoch 30: 0.8951075971126556
[08/28/2025 07:39:08 INFO]: Training loss at epoch 18: 0.8918658196926117
[08/28/2025 07:39:13 INFO]: Training loss at epoch 7: 1.0134988725185394
[08/28/2025 07:39:42 INFO]: Training loss at epoch 30: 1.0819500088691711
[08/28/2025 07:39:45 INFO]: Training loss at epoch 0: 0.9467713832855225
[08/28/2025 07:39:57 INFO]: Training loss at epoch 14: 0.9181670546531677
[08/28/2025 07:39:57 INFO]: Training loss at epoch 3: 1.0304147601127625
[08/28/2025 07:40:00 INFO]: Training loss at epoch 4: 1.170291543006897
[08/28/2025 07:40:04 INFO]: New best epoch, val score: -0.6674487822288769
[08/28/2025 07:40:04 INFO]: Saving model to: blotchy-Amado_trial_122/model_best.pth
[08/28/2025 07:40:28 INFO]: Training loss at epoch 3: 0.9972509741783142
[08/28/2025 07:40:42 INFO]: Training loss at epoch 28: 1.2235289812088013
[08/28/2025 07:40:53 INFO]: Training loss at epoch 31: 1.0493062734603882
[08/28/2025 07:41:00 INFO]: Training loss at epoch 31: 1.0557026863098145
[08/28/2025 07:41:09 INFO]: Training loss at epoch 15: 1.0200050473213196
[08/28/2025 07:41:13 INFO]: Training loss at epoch 32: 0.8133862018585205
[08/28/2025 07:41:27 INFO]: Running Final Evaluation...
[08/28/2025 07:41:39 INFO]: Training loss at epoch 43: 1.138571321964264
[08/28/2025 07:42:07 INFO]: Training loss at epoch 32: 0.9623978137969971
[08/28/2025 07:42:20 INFO]: Training loss at epoch 1: 0.9756103754043579
[08/28/2025 07:42:22 INFO]: Training loss at epoch 16: 1.0422465801239014
[08/28/2025 07:42:35 INFO]: Training loss at epoch 41: 0.961909681558609
[08/28/2025 07:42:35 INFO]: Training loss at epoch 4: 1.043671190738678
[08/28/2025 07:42:51 INFO]: Training accuracy: {
    "score": -1.0012265570143615,
    "rmse": 1.0012265570143615
}
[08/28/2025 07:42:51 INFO]: Val accuracy: {
    "score": -0.660136891092942,
    "rmse": 0.660136891092942
}
[08/28/2025 07:42:51 INFO]: Test accuracy: {
    "score": -0.860427386825399,
    "rmse": 0.860427386825399
}
[08/28/2025 07:42:51 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_112",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.860427386825399,
        "rmse": 0.860427386825399
    },
    "train_stats": {
        "score": -1.0012265570143615,
        "rmse": 1.0012265570143615
    },
    "val_stats": {
        "score": -0.660136891092942,
        "rmse": 0.660136891092942
    }
}
[08/28/2025 07:42:51 INFO]: Procewss finished for trial blotchy-Amado_trial_112
[08/28/2025 07:42:51 INFO]: 
_________________________________________________

[08/28/2025 07:42:51 INFO]: train_net_for_optune.py main() running.
[08/28/2025 07:42:51 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 16
  d_ffn_factor: 2.372109291541189
  attention_dropout: 0.37681632822862954
  ffn_dropout: 0.37681632822862954
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.522354682282292e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_123

[08/28/2025 07:42:51 INFO]: This ft_transformer has 1.512 million parameters.
[08/28/2025 07:42:51 INFO]: Training will start at epoch 0.
[08/28/2025 07:42:51 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 07:43:20 INFO]: Training loss at epoch 33: 0.9751192331314087
[08/28/2025 07:43:34 INFO]: Training loss at epoch 17: 0.9209315478801727
[08/28/2025 07:43:44 INFO]: Training loss at epoch 8: 1.0942152738571167
[08/28/2025 07:43:49 INFO]: Training loss at epoch 32: 0.8873081505298615
[08/28/2025 07:43:54 INFO]: Training loss at epoch 22: 1.0429428219795227
[08/28/2025 07:44:02 INFO]: Training loss at epoch 4: 1.1478418111801147
[08/28/2025 07:44:26 INFO]: Training loss at epoch 5: 1.0502052903175354
[08/28/2025 07:44:30 INFO]: Training loss at epoch 30: 0.8586848676204681
[08/28/2025 07:44:31 INFO]: Training loss at epoch 34: 1.1382559537887573
[08/28/2025 07:44:43 INFO]: Training loss at epoch 23: 1.14991295337677
[08/28/2025 07:44:45 INFO]: Training loss at epoch 18: 0.9489878118038177
[08/28/2025 07:44:53 INFO]: Training loss at epoch 2: 0.8792997896671295
[08/28/2025 07:44:59 INFO]: New best epoch, val score: -0.6685055570993649
[08/28/2025 07:44:59 INFO]: Saving model to: blotchy-Amado_trial_118/model_best.pth
[08/28/2025 07:45:11 INFO]: Training loss at epoch 30: 1.1058003604412079
[08/28/2025 07:45:11 INFO]: Training loss at epoch 5: 1.1951453685760498
[08/28/2025 07:45:20 INFO]: Training loss at epoch 29: 1.1470142304897308
[08/28/2025 07:45:22 INFO]: Training loss at epoch 0: 0.8161205649375916
[08/28/2025 07:45:30 INFO]: Training loss at epoch 3: 1.0069639980793
[08/28/2025 07:45:42 INFO]: New best epoch, val score: -0.7837990970554295
[08/28/2025 07:45:42 INFO]: Saving model to: blotchy-Amado_trial_123/model_best.pth
[08/28/2025 07:45:44 INFO]: Training loss at epoch 31: 0.9282487630844116
[08/28/2025 07:45:45 INFO]: Training loss at epoch 35: 0.9685109257698059
[08/28/2025 07:45:59 INFO]: Training loss at epoch 19: 1.1145294904708862
[08/28/2025 07:46:06 INFO]: Training loss at epoch 44: 1.0697748064994812
[08/28/2025 07:46:23 INFO]: Training stats: {
    "score": -1.00020931950319,
    "rmse": 1.00020931950319
}
[08/28/2025 07:46:23 INFO]: Val stats: {
    "score": -0.7041939226492053,
    "rmse": 0.7041939226492053
}
[08/28/2025 07:46:23 INFO]: Test stats: {
    "score": -0.8888514073593003,
    "rmse": 0.8888514073593003
}
[08/28/2025 07:46:31 INFO]: Running Final Evaluation...
[08/28/2025 07:46:40 INFO]: Training loss at epoch 33: 1.0110687613487244
[08/28/2025 07:46:52 INFO]: Training stats: {
    "score": -0.9955642848567055,
    "rmse": 0.9955642848567055
}
[08/28/2025 07:46:52 INFO]: Val stats: {
    "score": -0.702530316712272,
    "rmse": 0.702530316712272
}
[08/28/2025 07:46:52 INFO]: Test stats: {
    "score": -0.8897185268638793,
    "rmse": 0.8897185268638793
}
[08/28/2025 07:46:56 INFO]: Training loss at epoch 36: 0.9027610123157501
[08/28/2025 07:47:00 INFO]: Training loss at epoch 42: 0.8315127491950989
[08/28/2025 07:47:27 INFO]: Training loss at epoch 3: 1.0067489445209503
[08/28/2025 07:47:33 INFO]: Running Final Evaluation...
[08/28/2025 07:47:36 INFO]: Training loss at epoch 20: 0.9677304923534393
[08/28/2025 07:47:37 INFO]: Training loss at epoch 5: 1.3101630806922913
[08/28/2025 07:47:47 INFO]: Training loss at epoch 6: 1.3605178594589233
[08/28/2025 07:48:04 INFO]: Training loss at epoch 1: 1.1412412524223328
[08/28/2025 07:48:08 INFO]: Training loss at epoch 37: 1.145213782787323
[08/28/2025 07:48:14 INFO]: Training loss at epoch 9: 0.9064929783344269
[08/28/2025 07:48:25 INFO]: New best epoch, val score: -0.7625067045077766
[08/28/2025 07:48:25 INFO]: Saving model to: blotchy-Amado_trial_123/model_best.pth
[08/28/2025 07:48:47 INFO]: Training loss at epoch 21: 1.0891755819320679
[08/28/2025 07:48:52 INFO]: Training loss at epoch 6: 0.9674144685268402
[08/28/2025 07:48:55 INFO]: Training accuracy: {
    "score": -1.0103121958246393,
    "rmse": 1.0103121958246393
}
[08/28/2025 07:48:55 INFO]: Val accuracy: {
    "score": -0.6481733996311175,
    "rmse": 0.6481733996311175
}
[08/28/2025 07:48:55 INFO]: Test accuracy: {
    "score": -0.8756640481762973,
    "rmse": 0.8756640481762973
}
[08/28/2025 07:48:55 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_104",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8756640481762973,
        "rmse": 0.8756640481762973
    },
    "train_stats": {
        "score": -1.0103121958246393,
        "rmse": 1.0103121958246393
    },
    "val_stats": {
        "score": -0.6481733996311175,
        "rmse": 0.6481733996311175
    }
}
[08/28/2025 07:48:55 INFO]: Procewss finished for trial blotchy-Amado_trial_104
[08/28/2025 07:48:55 INFO]: 
_________________________________________________

[08/28/2025 07:48:55 INFO]: train_net_for_optune.py main() running.
[08/28/2025 07:48:55 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 1.3842245467538274
  attention_dropout: 0.003832718266828461
  ffn_dropout: 0.003832718266828461
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.4810156572694058e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_124

[08/28/2025 07:48:55 INFO]: This ft_transformer has 0.853 million parameters.
[08/28/2025 07:48:55 INFO]: Training will start at epoch 0.
[08/28/2025 07:48:55 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 07:49:13 INFO]: Training accuracy: {
    "score": -0.9689077546935027,
    "rmse": 0.9689077546935027
}
[08/28/2025 07:49:13 INFO]: Val accuracy: {
    "score": -0.6608194074069031,
    "rmse": 0.6608194074069031
}
[08/28/2025 07:49:13 INFO]: Test accuracy: {
    "score": -0.88088817450877,
    "rmse": 0.88088817450877
}
[08/28/2025 07:49:13 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_107",
    "best_epoch": 11,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.88088817450877,
        "rmse": 0.88088817450877
    },
    "train_stats": {
        "score": -0.9689077546935027,
        "rmse": 0.9689077546935027
    },
    "val_stats": {
        "score": -0.6608194074069031,
        "rmse": 0.6608194074069031
    }
}
[08/28/2025 07:49:13 INFO]: Procewss finished for trial blotchy-Amado_trial_107
[08/28/2025 07:49:13 INFO]: 
_________________________________________________

[08/28/2025 07:49:13 INFO]: train_net_for_optune.py main() running.
[08/28/2025 07:49:13 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 2.369194776449019
  attention_dropout: 0.019168569093659972
  ffn_dropout: 0.019168569093659972
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.501132013053768e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_125

[08/28/2025 07:49:14 INFO]: This ft_transformer has 1.144 million parameters.
[08/28/2025 07:49:14 INFO]: Training will start at epoch 0.
[08/28/2025 07:49:14 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 07:49:18 INFO]: Training loss at epoch 33: 0.7899540960788727
[08/28/2025 07:49:22 INFO]: Training loss at epoch 38: 1.0226681232452393
[08/28/2025 07:49:24 INFO]: Training loss at epoch 23: 0.9575449824333191
[08/28/2025 07:49:25 INFO]: New best epoch, val score: -0.6639597531754614
[08/28/2025 07:49:25 INFO]: Saving model to: blotchy-Amado_trial_118/model_best.pth
[08/28/2025 07:49:31 INFO]: Running Final Evaluation...
[08/28/2025 07:49:46 INFO]: Training stats: {
    "score": -1.1636284838722486,
    "rmse": 1.1636284838722486
}
[08/28/2025 07:49:46 INFO]: Val stats: {
    "score": -0.8013871324783439,
    "rmse": 0.8013871324783439
}
[08/28/2025 07:49:46 INFO]: Test stats: {
    "score": -1.009289806437143,
    "rmse": 1.009289806437143
}
[08/28/2025 07:49:56 INFO]: Training accuracy: {
    "score": -1.0078054522216873,
    "rmse": 1.0078054522216873
}
[08/28/2025 07:49:56 INFO]: Val accuracy: {
    "score": -0.7081058480817166,
    "rmse": 0.7081058480817166
}
[08/28/2025 07:49:56 INFO]: Test accuracy: {
    "score": -0.9051072911608742,
    "rmse": 0.9051072911608742
}
[08/28/2025 07:49:56 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_115",
    "best_epoch": 7,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9051072911608742,
        "rmse": 0.9051072911608742
    },
    "train_stats": {
        "score": -1.0078054522216873,
        "rmse": 1.0078054522216873
    },
    "val_stats": {
        "score": -0.7081058480817166,
        "rmse": 0.7081058480817166
    }
}
[08/28/2025 07:49:56 INFO]: Procewss finished for trial blotchy-Amado_trial_115
[08/28/2025 07:49:56 INFO]: 
_________________________________________________

[08/28/2025 07:49:56 INFO]: train_net_for_optune.py main() running.
[08/28/2025 07:49:56 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 1.3674997420286932
  attention_dropout: 0.05164292641665229
  ffn_dropout: 0.05164292641665229
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.466792675209439e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_126

[08/28/2025 07:49:56 INFO]: This ft_transformer has 0.848 million parameters.
[08/28/2025 07:49:56 INFO]: Training will start at epoch 0.
[08/28/2025 07:49:56 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 07:49:58 INFO]: Training loss at epoch 22: 0.9778732657432556
[08/28/2025 07:49:59 INFO]: Training loss at epoch 4: 1.0247158408164978
[08/28/2025 07:50:21 INFO]: Training loss at epoch 7: 1.045022428035736
[08/28/2025 07:50:29 INFO]: Training loss at epoch 45: 0.8099449574947357
[08/28/2025 07:50:30 INFO]: Training loss at epoch 0: 1.3569719195365906
[08/28/2025 07:50:44 INFO]: New best epoch, val score: -0.9780615170001774
[08/28/2025 07:50:44 INFO]: Saving model to: blotchy-Amado_trial_124/model_best.pth
[08/28/2025 07:50:48 INFO]: Training loss at epoch 2: 1.383855164051056
[08/28/2025 07:50:55 INFO]: Training loss at epoch 0: 1.027588665485382
[08/28/2025 07:51:07 INFO]: Training loss at epoch 6: 1.154772162437439
[08/28/2025 07:51:09 INFO]: New best epoch, val score: -0.677311867972747
[08/28/2025 07:51:09 INFO]: Saving model to: blotchy-Amado_trial_125/model_best.pth
[08/28/2025 07:51:10 INFO]: Training loss at epoch 23: 0.944756031036377
[08/28/2025 07:51:25 INFO]: Training loss at epoch 30: 0.9697570204734802
[08/28/2025 07:51:28 INFO]: Training loss at epoch 19: 1.2029363811016083
[08/28/2025 07:51:31 INFO]: Training loss at epoch 0: 1.7890564799308777
[08/28/2025 07:51:45 INFO]: New best epoch, val score: -1.3777771937106549
[08/28/2025 07:51:45 INFO]: Saving model to: blotchy-Amado_trial_126/model_best.pth
[08/28/2025 07:52:10 INFO]: Training loss at epoch 34: 0.81648388504982
[08/28/2025 07:52:20 INFO]: Training loss at epoch 1: 1.1173742413520813
[08/28/2025 07:52:24 INFO]: Training loss at epoch 24: 0.9416466355323792
[08/28/2025 07:52:34 INFO]: Training loss at epoch 5: 0.8726462721824646
[08/28/2025 07:52:34 INFO]: New best epoch, val score: -0.8049790607363538
[08/28/2025 07:52:34 INFO]: Saving model to: blotchy-Amado_trial_124/model_best.pth
[08/28/2025 07:52:43 INFO]: Training loss at epoch 31: 0.963170737028122
[08/28/2025 07:52:56 INFO]: Training loss at epoch 1: 1.0076201558113098
[08/28/2025 07:53:01 INFO]: Training loss at epoch 8: 1.1527107954025269
[08/28/2025 07:53:17 INFO]: Training loss at epoch 7: 0.9829693138599396
[08/28/2025 07:53:21 INFO]: Training loss at epoch 1: 2.2660980224609375
[08/28/2025 07:53:30 INFO]: Training loss at epoch 31: 0.8425830602645874
[08/28/2025 07:53:35 INFO]: New best epoch, val score: -1.216449328334497
[08/28/2025 07:53:35 INFO]: Saving model to: blotchy-Amado_trial_126/model_best.pth
[08/28/2025 07:53:36 INFO]: Training loss at epoch 3: 1.2371890246868134
[08/28/2025 07:53:36 INFO]: Training loss at epoch 25: 1.0324150919914246
[08/28/2025 07:53:39 INFO]: Running Final Evaluation...
[08/28/2025 07:53:47 INFO]: New best epoch, val score: -0.6612582542302373
[08/28/2025 07:53:47 INFO]: Saving model to: blotchy-Amado_trial_118/model_best.pth
[08/28/2025 07:54:06 INFO]: Training loss at epoch 2: 1.2016162276268005
[08/28/2025 07:54:14 INFO]: Training loss at epoch 10: 1.2631614804267883
[08/28/2025 07:54:20 INFO]: New best epoch, val score: -0.6652468641417452
[08/28/2025 07:54:20 INFO]: Saving model to: blotchy-Amado_trial_124/model_best.pth
[08/28/2025 07:54:41 INFO]: Training loss at epoch 7: 1.3964601755142212
[08/28/2025 07:54:43 INFO]: Training loss at epoch 34: 0.8499614298343658
[08/28/2025 07:54:47 INFO]: Training loss at epoch 26: 1.0685269236564636
[08/28/2025 07:54:48 INFO]: Training loss at epoch 24: 0.8146010041236877
[08/28/2025 07:54:51 INFO]: Training loss at epoch 2: 0.9973734021186829
[08/28/2025 07:54:52 INFO]: Training loss at epoch 46: 0.8658449053764343
[08/28/2025 07:55:00 INFO]: Training loss at epoch 4: 1.1282100081443787
[08/28/2025 07:55:05 INFO]: Training loss at epoch 6: 0.8109180927276611
[08/28/2025 07:55:08 INFO]: Training loss at epoch 2: 1.3532313406467438
[08/28/2025 07:55:23 INFO]: New best epoch, val score: -0.9903857970146395
[08/28/2025 07:55:23 INFO]: Saving model to: blotchy-Amado_trial_126/model_best.pth
[08/28/2025 07:55:36 INFO]: Training loss at epoch 9: 0.992307722568512
[08/28/2025 07:55:42 INFO]: Training stats: {
    "score": -0.9700864044187689,
    "rmse": 0.9700864044187689
}
[08/28/2025 07:55:42 INFO]: Val stats: {
    "score": -0.6868195587331527,
    "rmse": 0.6868195587331527
}
[08/28/2025 07:55:42 INFO]: Test stats: {
    "score": -0.8813226194252243,
    "rmse": 0.8813226194252243
}
[08/28/2025 07:55:57 INFO]: Training loss at epoch 3: 1.1696643829345703
[08/28/2025 07:56:01 INFO]: Training loss at epoch 27: 1.0051595270633698
[08/28/2025 07:56:01 INFO]: Training loss at epoch 31: 1.2457282543182373
[08/28/2025 07:56:11 INFO]: New best epoch, val score: -0.6826856055232827
[08/28/2025 07:56:11 INFO]: Saving model to: blotchy-Amado_trial_117/model_best.pth
[08/28/2025 07:56:25 INFO]: Training loss at epoch 4: 0.9423255324363708
[08/28/2025 07:56:34 INFO]: Training stats: {
    "score": -1.0107463294843675,
    "rmse": 1.0107463294843675
}
[08/28/2025 07:56:34 INFO]: Val stats: {
    "score": -0.7328269123854779,
    "rmse": 0.7328269123854779
}
[08/28/2025 07:56:34 INFO]: Test stats: {
    "score": -0.9109374888579426,
    "rmse": 0.9109374888579426
}
[08/28/2025 07:56:50 INFO]: Training loss at epoch 3: 1.2411384582519531
[08/28/2025 07:56:55 INFO]: Training loss at epoch 24: 1.0671379268169403
[08/28/2025 07:56:57 INFO]: Training accuracy: {
    "score": -1.0425759532772312,
    "rmse": 1.0425759532772312
}
[08/28/2025 07:56:57 INFO]: Val accuracy: {
    "score": -0.6537080419180912,
    "rmse": 0.6537080419180912
}
[08/28/2025 07:56:57 INFO]: Test accuracy: {
    "score": -0.8942825555735915,
    "rmse": 0.8942825555735915
}
[08/28/2025 07:56:57 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_100",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8942825555735915,
        "rmse": 0.8942825555735915
    },
    "train_stats": {
        "score": -1.0425759532772312,
        "rmse": 1.0425759532772312
    },
    "val_stats": {
        "score": -0.6537080419180912,
        "rmse": 0.6537080419180912
    }
}
[08/28/2025 07:56:57 INFO]: Procewss finished for trial blotchy-Amado_trial_100
[08/28/2025 07:56:57 INFO]: 
_________________________________________________

[08/28/2025 07:56:57 INFO]: train_net_for_optune.py main() running.
[08/28/2025 07:56:57 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 1.4079019369486574
  attention_dropout: 0.004443135891799378
  ffn_dropout: 0.004443135891799378
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.559412375777086e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_127

[08/28/2025 07:56:57 INFO]: This ft_transformer has 13.133 million parameters.
[08/28/2025 07:56:57 INFO]: Training will start at epoch 0.
[08/28/2025 07:56:57 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 07:56:59 INFO]: Training loss at epoch 3: 1.3186331391334534
[08/28/2025 07:57:13 INFO]: New best epoch, val score: -0.7645201612038741
[08/28/2025 07:57:13 INFO]: Training loss at epoch 28: 1.0623472332954407
[08/28/2025 07:57:13 INFO]: Saving model to: blotchy-Amado_trial_126/model_best.pth
[08/28/2025 07:57:37 INFO]: Training loss at epoch 35: 0.9400952458381653
[08/28/2025 07:57:42 INFO]: Training loss at epoch 7: 0.7815494537353516
[08/28/2025 07:57:44 INFO]: Training loss at epoch 8: 1.0743147730827332
[08/28/2025 07:57:47 INFO]: Training loss at epoch 4: 1.1696004271507263
[08/28/2025 07:58:16 INFO]: New best epoch, val score: -0.6609026866434358
[08/28/2025 07:58:16 INFO]: Saving model to: blotchy-Amado_trial_118/model_best.pth
[08/28/2025 07:58:16 INFO]: Training loss at epoch 8: 1.1540913581848145
[08/28/2025 07:58:16 INFO]: Running Final Evaluation...
[08/28/2025 07:58:26 INFO]: Training loss at epoch 29: 0.8346957564353943
[08/28/2025 07:58:47 INFO]: Training loss at epoch 11: 0.957583874464035
[08/28/2025 07:58:48 INFO]: Training loss at epoch 4: 1.1034302115440369
[08/28/2025 07:58:48 INFO]: Training loss at epoch 4: 1.0060833096504211
[08/28/2025 07:58:51 INFO]: Training stats: {
    "score": -0.996200766741433,
    "rmse": 0.996200766741433
}
[08/28/2025 07:58:51 INFO]: Val stats: {
    "score": -0.675910805502729,
    "rmse": 0.675910805502729
}
[08/28/2025 07:58:51 INFO]: Test stats: {
    "score": -0.8722772975831609,
    "rmse": 0.8722772975831609
}
[08/28/2025 07:59:01 INFO]: New best epoch, val score: -0.6592422073112553
[08/28/2025 07:59:01 INFO]: Saving model to: blotchy-Amado_trial_126/model_best.pth
[08/28/2025 07:59:09 INFO]: Training loss at epoch 5: 0.8715564012527466
[08/28/2025 07:59:11 INFO]: Training loss at epoch 10: 1.2423730492591858
[08/28/2025 07:59:17 INFO]: Training loss at epoch 47: 0.909478634595871
[08/28/2025 07:59:33 INFO]: Training loss at epoch 5: 1.0821918845176697
[08/28/2025 08:00:02 INFO]: Training loss at epoch 30: 1.3317200243473053
[08/28/2025 08:00:10 INFO]: Training loss at epoch 35: 0.8117837309837341
[08/28/2025 08:00:15 INFO]: Training loss at epoch 8: 0.8897899687290192
[08/28/2025 08:00:16 INFO]: Training loss at epoch 25: 0.9461351633071899
[08/28/2025 08:00:21 INFO]: Training accuracy: {
    "score": -0.9825749211117469,
    "rmse": 0.9825749211117469
}
[08/28/2025 08:00:21 INFO]: Val accuracy: {
    "score": -0.6730040379926387,
    "rmse": 0.6730040379926387
}
[08/28/2025 08:00:21 INFO]: Test accuracy: {
    "score": -0.8782082934386449,
    "rmse": 0.8782082934386449
}
[08/28/2025 08:00:21 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_109",
    "best_epoch": 4,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8782082934386449,
        "rmse": 0.8782082934386449
    },
    "train_stats": {
        "score": -0.9825749211117469,
        "rmse": 0.9825749211117469
    },
    "val_stats": {
        "score": -0.6730040379926387,
        "rmse": 0.6730040379926387
    }
}
[08/28/2025 08:00:21 INFO]: Procewss finished for trial blotchy-Amado_trial_109
[08/28/2025 08:00:21 INFO]: 
_________________________________________________

[08/28/2025 08:00:21 INFO]: train_net_for_optune.py main() running.
[08/28/2025 08:00:21 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 1.9934658166297947
  attention_dropout: 0.08093913339421743
  ffn_dropout: 0.08093913339421743
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.669755664119837e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_128

[08/28/2025 08:00:22 INFO]: This ft_transformer has 15.902 million parameters.
[08/28/2025 08:00:22 INFO]: Training will start at epoch 0.
[08/28/2025 08:00:22 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 08:00:36 INFO]: Training loss at epoch 5: 0.8471485078334808
[08/28/2025 08:00:36 INFO]: Training loss at epoch 32: 0.9149571061134338
[08/28/2025 08:00:45 INFO]: Training loss at epoch 5: 0.8297479152679443
[08/28/2025 08:00:50 INFO]: New best epoch, val score: -0.6591959699353531
[08/28/2025 08:00:50 INFO]: Saving model to: blotchy-Amado_trial_126/model_best.pth
[08/28/2025 08:00:55 INFO]: New best epoch, val score: -0.6611301227463294
[08/28/2025 08:00:55 INFO]: Saving model to: blotchy-Amado_trial_113/model_best.pth
[08/28/2025 08:01:15 INFO]: Training loss at epoch 31: 1.152434229850769
[08/28/2025 08:01:23 INFO]: Training loss at epoch 6: 1.297359585762024
[08/28/2025 08:01:25 INFO]: Running Final Evaluation...
[08/28/2025 08:01:48 INFO]: Training loss at epoch 32: 1.047278881072998
[08/28/2025 08:01:49 INFO]: Training loss at epoch 9: 1.286590337753296
[08/28/2025 08:01:51 INFO]: Training accuracy: {
    "score": -1.0078176586779464,
    "rmse": 1.0078176586779464
}
[08/28/2025 08:01:51 INFO]: Val accuracy: {
    "score": -0.662466311421831,
    "rmse": 0.662466311421831
}
[08/28/2025 08:01:51 INFO]: Test accuracy: {
    "score": -0.8689534912039868,
    "rmse": 0.8689534912039868
}
[08/28/2025 08:01:51 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_119",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8689534912039868,
        "rmse": 0.8689534912039868
    },
    "train_stats": {
        "score": -1.0078176586779464,
        "rmse": 1.0078176586779464
    },
    "val_stats": {
        "score": -0.662466311421831,
        "rmse": 0.662466311421831
    }
}
[08/28/2025 08:01:51 INFO]: Procewss finished for trial blotchy-Amado_trial_119
[08/28/2025 08:01:51 INFO]: 
_________________________________________________

[08/28/2025 08:01:51 INFO]: train_net_for_optune.py main() running.
[08/28/2025 08:01:51 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 1.3698448406054387
  attention_dropout: 0.3278528106792603
  ffn_dropout: 0.3278528106792603
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.19174718845286e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_129

[08/28/2025 08:01:51 INFO]: This ft_transformer has 12.958 million parameters.
[08/28/2025 08:01:51 INFO]: Training will start at epoch 0.
[08/28/2025 08:01:51 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 08:01:53 INFO]: Training loss at epoch 11: 0.9013197124004364
[08/28/2025 08:01:57 INFO]: Training loss at epoch 6: 1.0417691469192505
[08/28/2025 08:02:07 INFO]: Training loss at epoch 9: 0.851957231760025
[08/28/2025 08:02:23 INFO]: Training loss at epoch 6: 0.9840581715106964
[08/28/2025 08:02:43 INFO]: Training loss at epoch 6: 0.9208513796329498
[08/28/2025 08:02:47 INFO]: New best epoch, val score: -0.6685807077778136
[08/28/2025 08:02:47 INFO]: Saving model to: blotchy-Amado_trial_98/model_best.pth
[08/28/2025 08:02:49 INFO]: Training loss at epoch 9: 1.09735769033432
[08/28/2025 08:02:57 INFO]: Training loss at epoch 0: 1.2823358178138733
[08/28/2025 08:03:06 INFO]: Training stats: {
    "score": -1.0887197665885011,
    "rmse": 1.0887197665885011
}
[08/28/2025 08:03:06 INFO]: Val stats: {
    "score": -0.911002297996733,
    "rmse": 0.911002297996733
}
[08/28/2025 08:03:06 INFO]: Test stats: {
    "score": -1.0209391999992798,
    "rmse": 1.0209391999992798
}
[08/28/2025 08:03:10 INFO]: Training loss at epoch 7: 1.1199560165405273
[08/28/2025 08:03:14 INFO]: Training loss at epoch 12: 1.0523709654808044
[08/28/2025 08:03:36 INFO]: Training stats: {
    "score": -0.9867058204412811,
    "rmse": 0.9867058204412811
}
[08/28/2025 08:03:36 INFO]: Val stats: {
    "score": -0.6622404709289859,
    "rmse": 0.6622404709289859
}
[08/28/2025 08:03:36 INFO]: Test stats: {
    "score": -0.883912456779045,
    "rmse": 0.883912456779045
}
[08/28/2025 08:03:39 INFO]: Training loss at epoch 48: 1.0496963262557983
[08/28/2025 08:03:42 INFO]: Training stats: {
    "score": -0.9939868394806078,
    "rmse": 0.9939868394806078
}
[08/28/2025 08:03:42 INFO]: Val stats: {
    "score": -0.6964485093242244,
    "rmse": 0.6964485093242244
}
[08/28/2025 08:03:42 INFO]: Test stats: {
    "score": -0.8767978649063473,
    "rmse": 0.8767978649063473
}
[08/28/2025 08:03:44 INFO]: New best epoch, val score: -0.9358146666058944
[08/28/2025 08:03:44 INFO]: Saving model to: blotchy-Amado_trial_127/model_best.pth
[08/28/2025 08:03:46 INFO]: New best epoch, val score: -0.6482649815048143
[08/28/2025 08:03:46 INFO]: Saving model to: blotchy-Amado_trial_116/model_best.pth
[08/28/2025 08:04:11 INFO]: Training loss at epoch 7: 1.267158567905426
[08/28/2025 08:04:29 INFO]: Training loss at epoch 12: 1.1490854620933533
[08/28/2025 08:04:35 INFO]: Training loss at epoch 5: 1.0096235871315002
[08/28/2025 08:04:39 INFO]: Training loss at epoch 7: 0.8457660675048828
[08/28/2025 08:04:40 INFO]: Training loss at epoch 7: 1.3085967004299164
[08/28/2025 08:04:58 INFO]: Training loss at epoch 8: 1.1251959800720215
[08/28/2025 08:05:08 INFO]: Training loss at epoch 33: 1.1533114910125732
[08/28/2025 08:05:37 INFO]: Training loss at epoch 36: 0.8825661242008209
[08/28/2025 08:05:42 INFO]: Training loss at epoch 26: 1.226752758026123
[08/28/2025 08:05:45 INFO]: New best epoch, val score: -0.6824557500546287
[08/28/2025 08:05:45 INFO]: Saving model to: blotchy-Amado_trial_117/model_best.pth
[08/28/2025 08:05:59 INFO]: Training loss at epoch 8: 1.1167921423912048
[08/28/2025 08:06:17 INFO]: Training loss at epoch 10: 0.8851726651191711
[08/28/2025 08:06:37 INFO]: Training loss at epoch 8: 0.8637036085128784
[08/28/2025 08:06:39 INFO]: Training loss at epoch 10: 1.1209548711776733
[08/28/2025 08:06:46 INFO]: Training loss at epoch 9: 1.223543107509613
[08/28/2025 08:07:05 INFO]: Training loss at epoch 13: 1.0791255235671997
[08/28/2025 08:07:15 INFO]: Training loss at epoch 0: 1.0050431191921234
[08/28/2025 08:07:22 INFO]: Training loss at epoch 8: 0.920207679271698
[08/28/2025 08:07:24 INFO]: Training stats: {
    "score": -1.0670570264019688,
    "rmse": 1.0670570264019688
}
[08/28/2025 08:07:24 INFO]: Val stats: {
    "score": -0.6782816399531129,
    "rmse": 0.6782816399531129
}
[08/28/2025 08:07:24 INFO]: Test stats: {
    "score": -0.9065116493429817,
    "rmse": 0.9065116493429817
}
[08/28/2025 08:07:43 INFO]: Training loss at epoch 13: 1.0320762395858765
[08/28/2025 08:07:45 INFO]: Training loss at epoch 0: 1.2294676899909973
[08/28/2025 08:07:48 INFO]: Training loss at epoch 9: 0.943117082118988
[08/28/2025 08:08:01 INFO]: Training loss at epoch 10: 0.8451562523841858
[08/28/2025 08:08:04 INFO]: Training loss at epoch 49: 0.8663069307804108
[08/28/2025 08:08:05 INFO]: Training loss at epoch 20: 0.8659480512142181
[08/28/2025 08:08:11 INFO]: New best epoch, val score: -0.67670120540818
[08/28/2025 08:08:11 INFO]: Saving model to: blotchy-Amado_trial_128/model_best.pth
[08/28/2025 08:08:18 INFO]: New best epoch, val score: -0.6449409170704297
[08/28/2025 08:08:18 INFO]: Saving model to: blotchy-Amado_trial_116/model_best.pth
[08/28/2025 08:08:31 INFO]: Training stats: {
    "score": -1.0189583997997487,
    "rmse": 1.0189583997997487
}
[08/28/2025 08:08:31 INFO]: Val stats: {
    "score": -0.6595817107513215,
    "rmse": 0.6595817107513215
}
[08/28/2025 08:08:31 INFO]: Test stats: {
    "score": -0.8774613230188204,
    "rmse": 0.8774613230188204
}
[08/28/2025 08:08:36 INFO]: Training loss at epoch 9: 0.9455690681934357
[08/28/2025 08:08:37 INFO]: New best epoch, val score: -0.767143260043662
[08/28/2025 08:08:37 INFO]: Saving model to: blotchy-Amado_trial_129/model_best.pth
[08/28/2025 08:08:53 INFO]: Training loss at epoch 11: 0.8864215314388275
[08/28/2025 08:09:02 INFO]: Training loss at epoch 25: 0.8653007745742798
[08/28/2025 08:09:12 INFO]: Training loss at epoch 10: 1.0996475219726562
[08/28/2025 08:09:18 INFO]: Training stats: {
    "score": -1.0011561985071633,
    "rmse": 1.0011561985071633
}
[08/28/2025 08:09:18 INFO]: Val stats: {
    "score": -0.6856890666067367,
    "rmse": 0.6856890666067367
}
[08/28/2025 08:09:18 INFO]: Test stats: {
    "score": -0.8858802859551985,
    "rmse": 0.8858802859551985
}
[08/28/2025 08:09:26 INFO]: New best epoch, val score: -0.6640941357914093
[08/28/2025 08:09:26 INFO]: Saving model to: blotchy-Amado_trial_124/model_best.pth
[08/28/2025 08:09:37 INFO]: Training stats: {
    "score": -1.0662093403437811,
    "rmse": 1.0662093403437811
}
[08/28/2025 08:09:37 INFO]: Val stats: {
    "score": -0.7378736904108343,
    "rmse": 0.7378736904108343
}
[08/28/2025 08:09:37 INFO]: Test stats: {
    "score": -0.9532240861182618,
    "rmse": 0.9532240861182618
}
[08/28/2025 08:09:41 INFO]: Training loss at epoch 14: 1.2555031776428223
[08/28/2025 08:09:42 INFO]: Training loss at epoch 34: 0.9585696458816528
[08/28/2025 08:09:48 INFO]: Training loss at epoch 1: 1.161877453327179
[08/28/2025 08:10:07 INFO]: Training loss at epoch 33: 0.945732980966568
[08/28/2025 08:10:08 INFO]: Training loss at epoch 9: 0.959811806678772
[08/28/2025 08:10:14 INFO]: Training loss at epoch 11: 1.128643274307251
[08/28/2025 08:10:22 INFO]: Training loss at epoch 10: 1.016975998878479
[08/28/2025 08:10:38 INFO]: New best epoch, val score: -0.8970634655700507
[08/28/2025 08:10:38 INFO]: Saving model to: blotchy-Amado_trial_127/model_best.pth
[08/28/2025 08:11:02 INFO]: Training loss at epoch 11: 1.1170340180397034
[08/28/2025 08:11:06 INFO]: New best epoch, val score: -0.6682147676112609
[08/28/2025 08:11:06 INFO]: Saving model to: blotchy-Amado_trial_98/model_best.pth
[08/28/2025 08:11:08 INFO]: Training loss at epoch 37: 0.8893420994281769
[08/28/2025 08:11:09 INFO]: Training stats: {
    "score": -1.0447466152796603,
    "rmse": 1.0447466152796603
}
[08/28/2025 08:11:09 INFO]: Val stats: {
    "score": -0.8145887348429759,
    "rmse": 0.8145887348429759
}
[08/28/2025 08:11:09 INFO]: Test stats: {
    "score": -0.9630849534904327,
    "rmse": 0.9630849534904327
}
[08/28/2025 08:11:10 INFO]: Training loss at epoch 27: 0.769546777009964
[08/28/2025 08:11:17 INFO]: New best epoch, val score: -0.6549318835748075
[08/28/2025 08:11:17 INFO]: Saving model to: blotchy-Amado_trial_124/model_best.pth
[08/28/2025 08:11:20 INFO]: Training loss at epoch 10: 1.1413934826850891
[08/28/2025 08:11:26 INFO]: Training loss at epoch 12: 0.8342334628105164
[08/28/2025 08:12:16 INFO]: Training loss at epoch 11: 1.0522842407226562
[08/28/2025 08:12:21 INFO]: Training loss at epoch 14: 1.0676521062850952
[08/28/2025 08:12:22 INFO]: Training loss at epoch 15: 1.0590399503707886
[08/28/2025 08:12:30 INFO]: New best epoch, val score: -0.6590748632514845
[08/28/2025 08:12:30 INFO]: Saving model to: blotchy-Amado_trial_126/model_best.pth
[08/28/2025 08:12:31 INFO]: Training loss at epoch 11: 1.1742021441459656
[08/28/2025 08:12:52 INFO]: Training loss at epoch 12: 1.1215977668762207
[08/28/2025 08:13:08 INFO]: New best epoch, val score: -0.6518030022069564
[08/28/2025 08:13:08 INFO]: Saving model to: blotchy-Amado_trial_124/model_best.pth
[08/28/2025 08:13:18 INFO]: Training loss at epoch 11: 0.9704691767692566
[08/28/2025 08:13:52 INFO]: Training loss at epoch 12: 1.149676501750946
[08/28/2025 08:13:55 INFO]: Training loss at epoch 10: 1.0487004518508911
[08/28/2025 08:14:03 INFO]: Training loss at epoch 13: 0.9705527424812317
[08/28/2025 08:14:08 INFO]: Training loss at epoch 12: 1.0437991619110107
[08/28/2025 08:14:09 INFO]: Training loss at epoch 50: 0.8464341759681702
[08/28/2025 08:14:14 INFO]: Training loss at epoch 6: 1.077060878276825
[08/28/2025 08:14:22 INFO]: New best epoch, val score: -0.6589858410266205
[08/28/2025 08:14:22 INFO]: Saving model to: blotchy-Amado_trial_126/model_best.pth
[08/28/2025 08:14:23 INFO]: Training loss at epoch 35: 1.0359864234924316
[08/28/2025 08:14:44 INFO]: Training loss at epoch 13: 0.9822210371494293
[08/28/2025 08:14:45 INFO]: Training loss at epoch 1: 0.9100437164306641
[08/28/2025 08:15:01 INFO]: Training loss at epoch 16: 0.9699012637138367
[08/28/2025 08:15:20 INFO]: Training loss at epoch 12: 0.9749878942966461
[08/28/2025 08:15:20 INFO]: Training loss at epoch 1: 1.2604479789733887
[08/28/2025 08:15:24 INFO]: New best epoch, val score: -0.6817837009042601
[08/28/2025 08:15:24 INFO]: Saving model to: blotchy-Amado_trial_117/model_best.pth
[08/28/2025 08:15:57 INFO]: Training loss at epoch 13: 1.0864277482032776
[08/28/2025 08:16:33 INFO]: Training loss at epoch 14: 1.1762520372867584
[08/28/2025 08:16:37 INFO]: Training loss at epoch 14: 1.152472972869873
[08/28/2025 08:16:41 INFO]: Training loss at epoch 11: 1.0644474029541016
[08/28/2025 08:16:43 INFO]: Training loss at epoch 28: 0.7385571599006653
[08/28/2025 08:16:44 INFO]: Training loss at epoch 38: 1.0014669299125671
[08/28/2025 08:16:50 INFO]: Training loss at epoch 2: 1.4152610301971436
[08/28/2025 08:16:57 INFO]: Training loss at epoch 15: 0.8169037401676178
[08/28/2025 08:17:03 INFO]: Training loss at epoch 12: 1.0509287118911743
[08/28/2025 08:17:18 INFO]: Training loss at epoch 13: 0.9367432594299316
[08/28/2025 08:17:26 INFO]: Training loss at epoch 13: 0.9860681295394897
[08/28/2025 08:17:36 INFO]: New best epoch, val score: -0.7557410137258108
[08/28/2025 08:17:36 INFO]: Saving model to: blotchy-Amado_trial_127/model_best.pth
[08/28/2025 08:17:37 INFO]: Training loss at epoch 17: 1.1429040431976318
[08/28/2025 08:17:46 INFO]: Training loss at epoch 14: 0.9198986291885376
[08/28/2025 08:17:57 INFO]: New best epoch, val score: -0.6673144310560971
[08/28/2025 08:17:57 INFO]: Saving model to: blotchy-Amado_trial_121/model_best.pth
[08/28/2025 08:18:20 INFO]: Training loss at epoch 15: 0.9810383319854736
[08/28/2025 08:18:31 INFO]: Training loss at epoch 34: 1.006193220615387
[08/28/2025 08:18:34 INFO]: Training loss at epoch 51: 1.0674335956573486
[08/28/2025 08:18:59 INFO]: Training loss at epoch 36: 0.7389923185110092
[08/28/2025 08:19:15 INFO]: Training loss at epoch 15: 1.0994487702846527
[08/28/2025 08:19:17 INFO]: Training loss at epoch 14: 0.8359930515289307
[08/28/2025 08:19:24 INFO]: Training loss at epoch 12: 1.2621992230415344
[08/28/2025 08:19:37 INFO]: Training loss at epoch 15: 0.8362389206886292
[08/28/2025 08:20:09 INFO]: Training loss at epoch 16: 0.8993300199508667
[08/28/2025 08:20:16 INFO]: Training loss at epoch 18: 1.165235310792923
[08/28/2025 08:20:35 INFO]: Training loss at epoch 21: 0.9610131680965424
[08/28/2025 08:20:36 INFO]: New best epoch, val score: -0.6667576053685822
[08/28/2025 08:20:36 INFO]: Saving model to: blotchy-Amado_trial_121/model_best.pth
[08/28/2025 08:21:00 INFO]: Training loss at epoch 14: 1.0292164087295532
[08/28/2025 08:21:13 INFO]: Training loss at epoch 15: 0.8858817517757416
[08/28/2025 08:21:22 INFO]: Training loss at epoch 26: 0.8214214146137238
[08/28/2025 08:21:23 INFO]: Training loss at epoch 16: 0.829040139913559
[08/28/2025 08:21:24 INFO]: Training loss at epoch 16: 1.08516126871109
[08/28/2025 08:21:27 INFO]: Training loss at epoch 13: 1.131282925605774
[08/28/2025 08:21:32 INFO]: Training loss at epoch 2: 1.1148077249526978
[08/28/2025 08:21:48 INFO]: Training loss at epoch 16: 0.8733713626861572
[08/28/2025 08:21:55 INFO]: Training loss at epoch 17: 1.0886209607124329
[08/28/2025 08:22:06 INFO]: Training loss at epoch 29: 0.9580705165863037
[08/28/2025 08:22:08 INFO]: Training loss at epoch 13: 1.0141876637935638
[08/28/2025 08:22:09 INFO]: Training loss at epoch 39: 0.903398871421814
[08/28/2025 08:22:22 INFO]: New best epoch, val score: -0.7226404516030969
[08/28/2025 08:22:22 INFO]: Saving model to: blotchy-Amado_trial_129/model_best.pth
[08/28/2025 08:22:30 INFO]: New best epoch, val score: -0.7513565678523144
[08/28/2025 08:22:30 INFO]: Saving model to: blotchy-Amado_trial_123/model_best.pth
[08/28/2025 08:22:51 INFO]: Training loss at epoch 19: 1.1643303036689758
[08/28/2025 08:22:58 INFO]: Training loss at epoch 52: 1.024133563041687
[08/28/2025 08:23:11 INFO]: Training loss at epoch 16: 0.8927374184131622
[08/28/2025 08:23:12 INFO]: Training loss at epoch 17: 0.8935922980308533
[08/28/2025 08:23:14 INFO]: Training loss at epoch 2: 1.098805546760559
[08/28/2025 08:23:33 INFO]: Training loss at epoch 37: 0.9474710822105408
[08/28/2025 08:23:39 INFO]: Training loss at epoch 3: 1.0140483677387238
[08/28/2025 08:23:43 INFO]: Training loss at epoch 18: 1.1326787173748016
[08/28/2025 08:23:46 INFO]: Training loss at epoch 7: 1.0218560695648193
[08/28/2025 08:23:48 INFO]: Training stats: {
    "score": -0.9997911742401501,
    "rmse": 0.9997911742401501
}
[08/28/2025 08:23:48 INFO]: Val stats: {
    "score": -0.6668996022930708,
    "rmse": 0.6668996022930708
}
[08/28/2025 08:23:48 INFO]: Test stats: {
    "score": -0.8739594904045942,
    "rmse": 0.8739594904045942
}
[08/28/2025 08:23:57 INFO]: Training stats: {
    "score": -0.9577604758339197,
    "rmse": 0.9577604758339197
}
[08/28/2025 08:23:57 INFO]: Val stats: {
    "score": -0.7279562330823665,
    "rmse": 0.7279562330823665
}
[08/28/2025 08:23:57 INFO]: Test stats: {
    "score": -0.9228552500444801,
    "rmse": 0.9228552500444801
}
[08/28/2025 08:24:02 INFO]: Training stats: {
    "score": -0.8865904545396326,
    "rmse": 0.8865904545396326
}
[08/28/2025 08:24:02 INFO]: Val stats: {
    "score": -0.6942128322883528,
    "rmse": 0.6942128322883528
}
[08/28/2025 08:24:02 INFO]: Test stats: {
    "score": -0.921216083800379,
    "rmse": 0.921216083800379
}
[08/28/2025 08:24:22 INFO]: Training loss at epoch 17: 0.9787139594554901
[08/28/2025 08:24:35 INFO]: Training loss at epoch 15: 1.0114243030548096
[08/28/2025 08:24:55 INFO]: Training loss at epoch 14: 1.0565836429595947
[08/28/2025 08:24:56 INFO]: New best epoch, val score: -0.6815422284504952
[08/28/2025 08:24:56 INFO]: Saving model to: blotchy-Amado_trial_117/model_best.pth
[08/28/2025 08:25:02 INFO]: Training loss at epoch 18: 0.8419026434421539
[08/28/2025 08:25:10 INFO]: Training loss at epoch 17: 0.9537824988365173
[08/28/2025 08:25:16 INFO]: New best epoch, val score: -0.7450567430924293
[08/28/2025 08:25:16 INFO]: Saving model to: blotchy-Amado_trial_123/model_best.pth
[08/28/2025 08:25:31 INFO]: Training loss at epoch 19: 0.9506791830062866
[08/28/2025 08:25:54 INFO]: Training loss at epoch 14: 1.0281780064105988
[08/28/2025 08:25:57 INFO]: Training loss at epoch 17: 1.0792637467384338
[08/28/2025 08:26:11 INFO]: Training stats: {
    "score": -1.0107522386906296,
    "rmse": 1.0107522386906296
}
[08/28/2025 08:26:11 INFO]: Val stats: {
    "score": -0.7067499573167344,
    "rmse": 0.7067499573167344
}
[08/28/2025 08:26:11 INFO]: Test stats: {
    "score": -0.8946922098313043,
    "rmse": 0.8946922098313043
}
[08/28/2025 08:26:25 INFO]: Training loss at epoch 20: 0.8096336126327515
[08/28/2025 08:26:48 INFO]: Training loss at epoch 35: 1.1021064221858978
[08/28/2025 08:26:48 INFO]: Training loss at epoch 19: 1.2417021691799164
[08/28/2025 08:26:54 INFO]: Training loss at epoch 18: 0.823428213596344
[08/28/2025 08:27:07 INFO]: Training loss at epoch 18: 0.9804517030715942
[08/28/2025 08:27:22 INFO]: Training loss at epoch 53: 1.0536634922027588
[08/28/2025 08:27:29 INFO]: Training stats: {
    "score": -0.9991325210623021,
    "rmse": 0.9991325210623021
}
[08/28/2025 08:27:29 INFO]: Val stats: {
    "score": -0.6731237751244844,
    "rmse": 0.6731237751244844
}
[08/28/2025 08:27:29 INFO]: Test stats: {
    "score": -0.8739343048745981,
    "rmse": 0.8739343048745981
}
[08/28/2025 08:27:36 INFO]: Training loss at epoch 15: 0.7768043726682663
[08/28/2025 08:27:48 INFO]: New best epoch, val score: -0.6679322277176754
[08/28/2025 08:27:48 INFO]: Saving model to: blotchy-Amado_trial_98/model_best.pth
[08/28/2025 08:27:58 INFO]: New best epoch, val score: -0.7397568138573791
[08/28/2025 08:27:58 INFO]: Saving model to: blotchy-Amado_trial_123/model_best.pth
[08/28/2025 08:27:59 INFO]: Training loss at epoch 20: 0.8671286702156067
[08/28/2025 08:28:06 INFO]: Training loss at epoch 38: 0.7099635004997253
[08/28/2025 08:28:09 INFO]: Training loss at epoch 16: 1.0102658867835999
[08/28/2025 08:28:21 INFO]: Training loss at epoch 3: 1.2905788719654083
[08/28/2025 08:28:35 INFO]: New best epoch, val score: -0.6803761549666486
[08/28/2025 08:28:35 INFO]: Saving model to: blotchy-Amado_trial_120/model_best.pth
[08/28/2025 08:29:02 INFO]: Training loss at epoch 21: 0.8978587687015533
[08/28/2025 08:29:04 INFO]: Training loss at epoch 19: 1.0296072959899902
[08/28/2025 08:29:11 INFO]: New best epoch, val score: -0.6735067208740936
[08/28/2025 08:29:11 INFO]: Saving model to: blotchy-Amado_trial_129/model_best.pth
[08/28/2025 08:29:19 INFO]: Training loss at epoch 20: 1.1015724539756775
[08/28/2025 08:29:24 INFO]: Training loss at epoch 30: 0.8613722324371338
[08/28/2025 08:29:29 INFO]: Training loss at epoch 19: 0.9426296055316925
[08/28/2025 08:29:30 INFO]: Training loss at epoch 40: 0.6377741396427155
[08/28/2025 08:29:47 INFO]: Training stats: {
    "score": -0.9970202393088595,
    "rmse": 0.9970202393088595
}
[08/28/2025 08:29:47 INFO]: Val stats: {
    "score": -0.6810274013721941,
    "rmse": 0.6810274013721941
}
[08/28/2025 08:29:47 INFO]: Test stats: {
    "score": -0.8831633946911355,
    "rmse": 0.8831633946911355
}
[08/28/2025 08:29:47 INFO]: Training loss at epoch 21: 0.9675298631191254
[08/28/2025 08:30:17 INFO]: Training loss at epoch 15: 0.749280646443367
[08/28/2025 08:30:20 INFO]: Training loss at epoch 16: 1.0794512629508972
[08/28/2025 08:30:23 INFO]: Training stats: {
    "score": -0.9875151840106099,
    "rmse": 0.9875151840106099
}
[08/28/2025 08:30:23 INFO]: Val stats: {
    "score": -0.6868175320121729,
    "rmse": 0.6868175320121729
}
[08/28/2025 08:30:23 INFO]: Test stats: {
    "score": -0.8716841512878313,
    "rmse": 0.8716841512878313
}
[08/28/2025 08:30:24 INFO]: Training loss at epoch 18: 0.9741376638412476
[08/28/2025 08:30:28 INFO]: Training loss at epoch 4: 1.1021526455879211
[08/28/2025 08:30:40 INFO]: New best epoch, val score: -0.7331452577013496
[08/28/2025 08:30:40 INFO]: Saving model to: blotchy-Amado_trial_123/model_best.pth
[08/28/2025 08:31:03 INFO]: Training loss at epoch 21: 1.0705386102199554
[08/28/2025 08:31:06 INFO]: Training loss at epoch 3: 0.8532102406024933
[08/28/2025 08:31:32 INFO]: Training loss at epoch 22: 0.8381717503070831
[08/28/2025 08:31:37 INFO]: Training loss at epoch 22: 1.061795175075531
[08/28/2025 08:31:40 INFO]: Training loss at epoch 17: 0.9080536961555481
[08/28/2025 08:31:42 INFO]: Training loss at epoch 20: 1.0120296776294708
[08/28/2025 08:31:44 INFO]: Training loss at epoch 54: 0.8328185081481934
[08/28/2025 08:32:02 INFO]: New best epoch, val score: -0.6634681429137226
[08/28/2025 08:32:02 INFO]: Saving model to: blotchy-Amado_trial_128/model_best.pth
[08/28/2025 08:32:06 INFO]: New best epoch, val score: -0.679739502438723
[08/28/2025 08:32:06 INFO]: Saving model to: blotchy-Amado_trial_120/model_best.pth
[08/28/2025 08:32:34 INFO]: Training loss at epoch 39: 1.0785270035266876
[08/28/2025 08:32:52 INFO]: Training loss at epoch 22: 0.9133791923522949
[08/28/2025 08:32:54 INFO]: Training loss at epoch 22: 0.9426811933517456
[08/28/2025 08:32:55 INFO]: Training loss at epoch 20: 0.990866631269455
[08/28/2025 08:33:05 INFO]: Training loss at epoch 17: 0.9971566796302795
[08/28/2025 08:33:11 INFO]: Training loss at epoch 8: 1.0680404305458069
[08/28/2025 08:33:18 INFO]: Training loss at epoch 23: 1.1338258981704712
[08/28/2025 08:33:24 INFO]: New best epoch, val score: -0.7292159732332989
[08/28/2025 08:33:24 INFO]: Saving model to: blotchy-Amado_trial_123/model_best.pth
[08/28/2025 08:33:27 INFO]: Training loss at epoch 27: 0.9989133477210999
[08/28/2025 08:33:37 INFO]: Training loss at epoch 21: 1.2131273746490479
[08/28/2025 08:34:07 INFO]: Training stats: {
    "score": -0.9904318510099327,
    "rmse": 0.9904318510099327
}
[08/28/2025 08:34:07 INFO]: Val stats: {
    "score": -0.6957836246910083,
    "rmse": 0.6957836246910083
}
[08/28/2025 08:34:07 INFO]: Test stats: {
    "score": -0.8873663067752963,
    "rmse": 0.8873663067752963
}
[08/28/2025 08:34:12 INFO]: Training loss at epoch 23: 0.9956944286823273
[08/28/2025 08:34:40 INFO]: Training loss at epoch 16: 1.013026237487793
[08/28/2025 08:34:42 INFO]: Training loss at epoch 23: 1.0597116351127625
[08/28/2025 08:34:43 INFO]: Training loss at epoch 31: 0.9679842889308929
[08/28/2025 08:34:50 INFO]: Training loss at epoch 41: 0.8309949934482574
[08/28/2025 08:34:51 INFO]: Training loss at epoch 19: 1.0926226377487183
[08/28/2025 08:35:03 INFO]: Training loss at epoch 36: 0.76832115650177
[08/28/2025 08:35:04 INFO]: Training loss at epoch 4: 1.1209609508514404
[08/28/2025 08:35:06 INFO]: Training loss at epoch 24: 1.057476818561554
[08/28/2025 08:35:13 INFO]: Training loss at epoch 18: 1.1535479426383972
[08/28/2025 08:35:28 INFO]: Training loss at epoch 21: 1.0146701335906982
[08/28/2025 08:35:36 INFO]: Training loss at epoch 22: 1.0894604921340942
[08/28/2025 08:35:40 INFO]: New best epoch, val score: -0.6795457782850335
[08/28/2025 08:35:40 INFO]: Saving model to: blotchy-Amado_trial_120/model_best.pth
[08/28/2025 08:35:49 INFO]: Training loss at epoch 18: 0.988195538520813
[08/28/2025 08:35:50 INFO]: New best epoch, val score: -0.6760396987447849
[08/28/2025 08:35:50 INFO]: Saving model to: blotchy-Amado_trial_125/model_best.pth
[08/28/2025 08:36:05 INFO]: Training loss at epoch 55: 1.0337964296340942
[08/28/2025 08:36:21 INFO]: Training stats: {
    "score": -0.9975119210313592,
    "rmse": 0.9975119210313592
}
[08/28/2025 08:36:21 INFO]: Val stats: {
    "score": -0.6456712048819641,
    "rmse": 0.6456712048819641
}
[08/28/2025 08:36:21 INFO]: Test stats: {
    "score": -0.868818120368167,
    "rmse": 0.868818120368167
}
[08/28/2025 08:36:31 INFO]: Training loss at epoch 24: 1.096284955739975
[08/28/2025 08:36:48 INFO]: Training loss at epoch 24: 1.0611782670021057
[08/28/2025 08:36:55 INFO]: Training loss at epoch 25: 1.0310903787612915
[08/28/2025 08:37:13 INFO]: Training loss at epoch 5: 1.0818719267845154
[08/28/2025 08:37:32 INFO]: Training loss at epoch 23: 0.934825986623764
[08/28/2025 08:37:47 INFO]: New best epoch, val score: -0.6758378509367677
[08/28/2025 08:37:47 INFO]: Saving model to: blotchy-Amado_trial_125/model_best.pth
[08/28/2025 08:38:02 INFO]: Training loss at epoch 22: 1.0115266740322113
[08/28/2025 08:38:20 INFO]: Training loss at epoch 25: 0.7876873910427094
[08/28/2025 08:38:32 INFO]: Training loss at epoch 19: 1.058853805065155
[08/28/2025 08:38:43 INFO]: Training loss at epoch 40: 0.8754365146160126
[08/28/2025 08:38:44 INFO]: Training loss at epoch 26: 0.9575857520103455
[08/28/2025 08:38:45 INFO]: Training loss at epoch 19: 0.8342475593090057
[08/28/2025 08:39:02 INFO]: Training loss at epoch 4: 1.0152546763420105
[08/28/2025 08:39:08 INFO]: Training loss at epoch 17: 1.2967379093170166
[08/28/2025 08:39:24 INFO]: Training loss at epoch 25: 0.979383260011673
[08/28/2025 08:39:30 INFO]: Training loss at epoch 24: 0.9047447443008423
[08/28/2025 08:39:31 INFO]: Training stats: {
    "score": -1.0047822988536237,
    "rmse": 1.0047822988536237
}
[08/28/2025 08:39:31 INFO]: Val stats: {
    "score": -0.7289060473859466,
    "rmse": 0.7289060473859466
}
[08/28/2025 08:39:31 INFO]: Test stats: {
    "score": -0.9003354083564157,
    "rmse": 0.9003354083564157
}
[08/28/2025 08:39:52 INFO]: New best epoch, val score: -0.7289060473859466
[08/28/2025 08:39:52 INFO]: Saving model to: blotchy-Amado_trial_123/model_best.pth
[08/28/2025 08:39:59 INFO]: New best epoch, val score: -0.6633683572651637
[08/28/2025 08:39:59 INFO]: Saving model to: blotchy-Amado_trial_128/model_best.pth
[08/28/2025 08:40:01 INFO]: Training stats: {
    "score": -1.006418176406731,
    "rmse": 1.006418176406731
}
[08/28/2025 08:40:01 INFO]: Val stats: {
    "score": -0.6796773434920874,
    "rmse": 0.6796773434920874
}
[08/28/2025 08:40:01 INFO]: Test stats: {
    "score": -0.8759433556714451,
    "rmse": 0.8759433556714451
}
[08/28/2025 08:40:08 INFO]: Training loss at epoch 26: 0.9792536199092865
[08/28/2025 08:40:11 INFO]: Training loss at epoch 32: 1.037876009941101
[08/28/2025 08:40:17 INFO]: Training loss at epoch 42: 1.0279639661312103
[08/28/2025 08:40:29 INFO]: Training loss at epoch 56: 0.9601163566112518
[08/28/2025 08:40:32 INFO]: Training loss at epoch 27: 0.8308461606502533
[08/28/2025 08:40:34 INFO]: Training loss at epoch 23: 0.8363482058048248
[08/28/2025 08:40:52 INFO]: Training loss at epoch 20: 1.1229500770568848
[08/28/2025 08:41:28 INFO]: Training loss at epoch 25: 0.9686806499958038
[08/28/2025 08:41:53 INFO]: Training loss at epoch 5: 1.0531646013259888
[08/28/2025 08:41:57 INFO]: Training loss at epoch 27: 0.841154932975769
[08/28/2025 08:42:00 INFO]: Training loss at epoch 26: 1.1513163447380066
[08/28/2025 08:42:18 INFO]: Training loss at epoch 20: 1.1515704989433289
[08/28/2025 08:42:22 INFO]: Training loss at epoch 28: 0.8869137167930603
[08/28/2025 08:42:38 INFO]: New best epoch, val score: -0.721543886872707
[08/28/2025 08:42:38 INFO]: Saving model to: blotchy-Amado_trial_123/model_best.pth
[08/28/2025 08:42:39 INFO]: Training loss at epoch 9: 1.0818004608154297
[08/28/2025 08:43:10 INFO]: Training loss at epoch 24: 0.8802652359008789
[08/28/2025 08:43:21 INFO]: Training loss at epoch 41: 0.8493739068508148
[08/28/2025 08:43:24 INFO]: Training loss at epoch 26: 1.2064762711524963
[08/28/2025 08:43:24 INFO]: Training loss at epoch 37: 0.8055100440979004
[08/28/2025 08:43:33 INFO]: Training loss at epoch 18: 1.1624861359596252
[08/28/2025 08:43:34 INFO]: Training loss at epoch 20: 1.2044330537319183
[08/28/2025 08:43:47 INFO]: Training loss at epoch 28: 0.8751097023487091
[08/28/2025 08:44:06 INFO]: Training loss at epoch 6: 1.3468610048294067
[08/28/2025 08:44:11 INFO]: Training loss at epoch 29: 1.1128089427947998
[08/28/2025 08:44:38 INFO]: Training loss at epoch 27: 0.9379431009292603
[08/28/2025 08:44:50 INFO]: Training stats: {
    "score": -1.0004981319595967,
    "rmse": 1.0004981319595967
}
[08/28/2025 08:44:50 INFO]: Val stats: {
    "score": -0.6707901488411508,
    "rmse": 0.6707901488411508
}
[08/28/2025 08:44:50 INFO]: Test stats: {
    "score": -0.8759395652297338,
    "rmse": 0.8759395652297338
}
[08/28/2025 08:44:54 INFO]: Training loss at epoch 57: 1.2108380198478699
[08/28/2025 08:45:02 INFO]: Training loss at epoch 21: 0.9396998286247253
[08/28/2025 08:45:13 INFO]: Training loss at epoch 23: 1.0413276553153992
[08/28/2025 08:45:21 INFO]: Training loss at epoch 27: 0.9128157496452332
[08/28/2025 08:45:21 INFO]: Training loss at epoch 21: 1.169090747833252
[08/28/2025 08:45:22 INFO]: New best epoch, val score: -0.7148214020032108
[08/28/2025 08:45:22 INFO]: Saving model to: blotchy-Amado_trial_123/model_best.pth
[08/28/2025 08:45:34 INFO]: Training loss at epoch 29: 1.1470777094364166
[08/28/2025 08:45:37 INFO]: Training loss at epoch 33: 0.8919731676578522
[08/28/2025 08:45:40 INFO]: Training loss at epoch 28: 0.9037283957004547
[08/28/2025 08:45:44 INFO]: Training loss at epoch 43: 0.8145860731601715
[08/28/2025 08:45:44 INFO]: Training loss at epoch 25: 0.7604836225509644
[08/28/2025 08:45:56 INFO]: Training stats: {
    "score": -0.9878004683389829,
    "rmse": 0.9878004683389829
}
[08/28/2025 08:45:56 INFO]: Val stats: {
    "score": -0.6855501129395747,
    "rmse": 0.6855501129395747
}
[08/28/2025 08:45:56 INFO]: Test stats: {
    "score": -0.8779207338268673,
    "rmse": 0.8779207338268673
}
[08/28/2025 08:46:14 INFO]: Training stats: {
    "score": -0.9982805740849329,
    "rmse": 0.9982805740849329
}
[08/28/2025 08:46:14 INFO]: Val stats: {
    "score": -0.6956790192874236,
    "rmse": 0.6956790192874236
}
[08/28/2025 08:46:14 INFO]: Test stats: {
    "score": -0.8839966647186344,
    "rmse": 0.8839966647186344
}
[08/28/2025 08:46:38 INFO]: Training loss at epoch 30: 0.913596510887146
[08/28/2025 08:47:01 INFO]: Training loss at epoch 5: 1.0058524012565613
[08/28/2025 08:47:06 INFO]: Training loss at epoch 21: 0.9714812636375427
[08/28/2025 08:47:14 INFO]: Training loss at epoch 28: 1.1942464709281921
[08/28/2025 08:47:20 INFO]: Training loss at epoch 28: 1.050541490316391
[08/28/2025 08:47:47 INFO]: Training loss at epoch 22: 1.0283316373825073
[08/28/2025 08:47:59 INFO]: Training loss at epoch 19: 0.9930817782878876
[08/28/2025 08:47:59 INFO]: Training loss at epoch 42: 1.2049532234668732
[08/28/2025 08:47:59 INFO]: New best epoch, val score: -0.6629208635788292
[08/28/2025 08:47:59 INFO]: Saving model to: blotchy-Amado_trial_128/model_best.pth
[08/28/2025 08:48:04 INFO]: Training loss at epoch 30: 1.1083448827266693
[08/28/2025 08:48:08 INFO]: New best epoch, val score: -0.7096854867875582
[08/28/2025 08:48:08 INFO]: Saving model to: blotchy-Amado_trial_123/model_best.pth
[08/28/2025 08:48:21 INFO]: Training loss at epoch 26: 0.8472601771354675
[08/28/2025 08:48:31 INFO]: Training loss at epoch 31: 0.9598675966262817
[08/28/2025 08:48:46 INFO]: Training loss at epoch 6: 0.8532454073429108
[08/28/2025 08:49:16 INFO]: Training loss at epoch 29: 1.0098938345909119
[08/28/2025 08:49:19 INFO]: Training loss at epoch 58: 1.0411766171455383
[08/28/2025 08:49:30 INFO]: Training stats: {
    "score": -0.9700032352530444,
    "rmse": 0.9700032352530444
}
[08/28/2025 08:49:30 INFO]: Val stats: {
    "score": -0.6719953476951089,
    "rmse": 0.6719953476951089
}
[08/28/2025 08:49:30 INFO]: Test stats: {
    "score": -0.8848262098244891,
    "rmse": 0.8848262098244891
}
[08/28/2025 08:49:53 INFO]: Training loss at epoch 29: 0.987751305103302
[08/28/2025 08:49:53 INFO]: Training loss at epoch 31: 1.081506997346878
[08/28/2025 08:49:54 INFO]: Training loss at epoch 22: 0.9863870739936829
[08/28/2025 08:50:01 INFO]: Training stats: {
    "score": -0.9931003723527771,
    "rmse": 0.9931003723527771
}
[08/28/2025 08:50:01 INFO]: Val stats: {
    "score": -0.6799500894263656,
    "rmse": 0.6799500894263656
}
[08/28/2025 08:50:01 INFO]: Test stats: {
    "score": -0.8831115548840716,
    "rmse": 0.8831115548840716
}
[08/28/2025 08:50:20 INFO]: Training loss at epoch 32: 1.2530935108661652
[08/28/2025 08:50:32 INFO]: Training loss at epoch 23: 0.9694995582103729
[08/28/2025 08:50:39 INFO]: Training loss at epoch 22: 0.9621917903423309
[08/28/2025 08:50:49 INFO]: Training stats: {
    "score": -0.9988117361530643,
    "rmse": 0.9988117361530643
}
[08/28/2025 08:50:49 INFO]: Val stats: {
    "score": -0.706125657281365,
    "rmse": 0.706125657281365
}
[08/28/2025 08:50:49 INFO]: Test stats: {
    "score": -0.8916366552736325,
    "rmse": 0.8916366552736325
}
[08/28/2025 08:50:52 INFO]: New best epoch, val score: -0.7032904660270561
[08/28/2025 08:50:52 INFO]: Saving model to: blotchy-Amado_trial_123/model_best.pth
[08/28/2025 08:50:54 INFO]: Training loss at epoch 27: 1.0978806912899017
[08/28/2025 08:50:56 INFO]: Training loss at epoch 7: 1.297167956829071
[08/28/2025 08:51:05 INFO]: Training loss at epoch 34: 1.1651282012462616
[08/28/2025 08:51:14 INFO]: Training loss at epoch 44: 0.7195852994918823
[08/28/2025 08:51:42 INFO]: Training loss at epoch 32: 0.8775505125522614
[08/28/2025 08:51:46 INFO]: Training loss at epoch 38: 1.0473785698413849
[08/28/2025 08:51:58 INFO]: Training loss at epoch 30: 0.9648067355155945
[08/28/2025 08:52:09 INFO]: Training loss at epoch 33: 1.1353350281715393
[08/28/2025 08:52:37 INFO]: Training loss at epoch 43: 0.9883831143379211
[08/28/2025 08:53:18 INFO]: Training loss at epoch 24: 0.7844900786876678
[08/28/2025 08:53:27 INFO]: Training loss at epoch 30: 1.1311399042606354
[08/28/2025 08:53:30 INFO]: Training loss at epoch 28: 0.7720416784286499
[08/28/2025 08:53:32 INFO]: Training loss at epoch 33: 0.8106298744678497
[08/28/2025 08:53:38 INFO]: New best epoch, val score: -0.6998349569239756
[08/28/2025 08:53:38 INFO]: Saving model to: blotchy-Amado_trial_123/model_best.pth
[08/28/2025 08:53:44 INFO]: Training loss at epoch 59: 1.1233219802379608
[08/28/2025 08:53:55 INFO]: Training loss at epoch 20: 1.0572950541973114
[08/28/2025 08:53:55 INFO]: Training loss at epoch 31: 1.2380704879760742
[08/28/2025 08:54:00 INFO]: Training loss at epoch 34: 1.131358951330185
[08/28/2025 08:54:12 INFO]: Training loss at epoch 23: 1.1211516857147217
[08/28/2025 08:54:24 INFO]: Training loss at epoch 23: 1.1686010956764221
[08/28/2025 08:55:00 INFO]: Training loss at epoch 6: 0.8891780376434326
[08/28/2025 08:55:15 INFO]: Training stats: {
    "score": -0.972982521119016,
    "rmse": 0.972982521119016
}
[08/28/2025 08:55:15 INFO]: Val stats: {
    "score": -0.6753422113107996,
    "rmse": 0.6753422113107996
}
[08/28/2025 08:55:15 INFO]: Test stats: {
    "score": -0.8884320129917667,
    "rmse": 0.8884320129917667
}
[08/28/2025 08:55:20 INFO]: Training loss at epoch 34: 0.8911032378673553
[08/28/2025 08:55:28 INFO]: Training loss at epoch 10: 1.1231163144111633
[08/28/2025 08:55:36 INFO]: Training loss at epoch 7: 1.0182145833969116
[08/28/2025 08:55:50 INFO]: Training loss at epoch 35: 1.1179699301719666
[08/28/2025 08:55:51 INFO]: Training loss at epoch 32: 0.8654696643352509
[08/28/2025 08:56:00 INFO]: New best epoch, val score: -0.6628459120934547
[08/28/2025 08:56:00 INFO]: Saving model to: blotchy-Amado_trial_128/model_best.pth
[08/28/2025 08:56:03 INFO]: Training loss at epoch 25: 1.0593300461769104
[08/28/2025 08:56:07 INFO]: Training loss at epoch 29: 0.7906233072280884
[08/28/2025 08:56:08 INFO]: Training loss at epoch 31: 1.039099782705307
[08/28/2025 08:56:34 INFO]: Training loss at epoch 35: 0.9332888722419739
[08/28/2025 08:56:43 INFO]: Training loss at epoch 45: 0.8173317015171051
[08/28/2025 08:57:01 INFO]: Training stats: {
    "score": -0.9826389617442243,
    "rmse": 0.9826389617442243
}
[08/28/2025 08:57:01 INFO]: Val stats: {
    "score": -0.6945253663670282,
    "rmse": 0.6945253663670282
}
[08/28/2025 08:57:01 INFO]: Test stats: {
    "score": -0.8748393863496863,
    "rmse": 0.8748393863496863
}
[08/28/2025 08:57:06 INFO]: Training loss at epoch 35: 0.8113290667533875
[08/28/2025 08:57:09 INFO]: Training loss at epoch 44: 0.9558934271335602
[08/28/2025 08:57:30 INFO]: Training loss at epoch 24: 0.804315835237503
[08/28/2025 08:57:36 INFO]: Training loss at epoch 36: 0.9679774045944214
[08/28/2025 08:57:40 INFO]: Training loss at epoch 24: 0.9472154974937439
[08/28/2025 08:57:45 INFO]: Training loss at epoch 8: 1.093656599521637
[08/28/2025 08:57:46 INFO]: Training loss at epoch 33: 0.8909037411212921
[08/28/2025 08:57:52 INFO]: Training loss at epoch 29: 1.0032684803009033
[08/28/2025 08:58:15 INFO]: Training loss at epoch 21: 1.2250314354896545
[08/28/2025 08:58:43 INFO]: Training loss at epoch 32: 1.0716239213943481
[08/28/2025 08:58:46 INFO]: Training loss at epoch 26: 0.9823973476886749
[08/28/2025 08:58:50 INFO]: Training loss at epoch 24: 1.139136791229248
[08/28/2025 08:58:55 INFO]: Training loss at epoch 36: 1.4052139818668365
[08/28/2025 08:59:25 INFO]: Training loss at epoch 37: 0.7980453372001648
[08/28/2025 08:59:36 INFO]: Training loss at epoch 30: 1.09615096449852
[08/28/2025 08:59:36 INFO]: Training loss at epoch 60: 1.0257034301757812
[08/28/2025 08:59:44 INFO]: Training loss at epoch 34: 0.9229503870010376
[08/28/2025 09:00:02 INFO]: Training loss at epoch 39: 0.9041832983493805
[08/28/2025 09:00:42 INFO]: Training loss at epoch 37: 1.2097200155258179
[08/28/2025 09:01:13 INFO]: Training loss at epoch 25: 0.8433587849140167
[08/28/2025 09:01:13 INFO]: Training loss at epoch 38: 1.019976943731308
[08/28/2025 09:01:21 INFO]: Training loss at epoch 33: 0.9290792346000671
[08/28/2025 09:01:30 INFO]: Training loss at epoch 27: 1.1105643808841705
[08/28/2025 09:01:39 INFO]: Training loss at epoch 35: 0.9024821221828461
[08/28/2025 09:01:39 INFO]: Training loss at epoch 45: 1.0514912009239197
[08/28/2025 09:01:55 INFO]: Training loss at epoch 36: 0.8525154888629913
[08/28/2025 09:01:58 INFO]: Training stats: {
    "score": -0.9594607661228731,
    "rmse": 0.9594607661228731
}
[08/28/2025 09:01:58 INFO]: Val stats: {
    "score": -0.8234802660857622,
    "rmse": 0.8234802660857622
}
[08/28/2025 09:01:58 INFO]: Test stats: {
    "score": -0.9692000409693644,
    "rmse": 0.9692000409693644
}
[08/28/2025 09:02:06 INFO]: Training loss at epoch 46: 0.6256745904684067
[08/28/2025 09:02:07 INFO]: Training loss at epoch 31: 1.1588562726974487
[08/28/2025 09:02:13 INFO]: Running Final Evaluation...
[08/28/2025 09:02:19 INFO]: Training loss at epoch 8: 0.892616480588913
[08/28/2025 09:02:27 INFO]: Running Final Evaluation...
[08/28/2025 09:02:28 INFO]: Training loss at epoch 38: 0.9521021544933319
[08/28/2025 09:02:37 INFO]: Training loss at epoch 22: 1.1021186113357544
[08/28/2025 09:02:48 INFO]: Training stats: {
    "score": -0.9571351204314655,
    "rmse": 0.9571351204314655
}
[08/28/2025 09:02:48 INFO]: Val stats: {
    "score": -0.6667744126046813,
    "rmse": 0.6667744126046813
}
[08/28/2025 09:02:48 INFO]: Test stats: {
    "score": -0.8783704227502569,
    "rmse": 0.8783704227502569
}
[08/28/2025 09:02:51 INFO]: Training loss at epoch 7: 0.9436058700084686
[08/28/2025 09:02:59 INFO]: Training loss at epoch 39: 0.8955712914466858
[08/28/2025 09:03:16 INFO]: Training loss at epoch 25: 1.046514868736267
[08/28/2025 09:03:23 INFO]: Training accuracy: {
    "score": -1.0043427153598246,
    "rmse": 1.0043427153598246
}
[08/28/2025 09:03:23 INFO]: Val accuracy: {
    "score": -0.6674487822288769,
    "rmse": 0.6674487822288769
}
[08/28/2025 09:03:23 INFO]: Test accuracy: {
    "score": -0.866068793943622,
    "rmse": 0.866068793943622
}
[08/28/2025 09:03:23 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_122",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.866068793943622,
        "rmse": 0.866068793943622
    },
    "train_stats": {
        "score": -1.0043427153598246,
        "rmse": 1.0043427153598246
    },
    "val_stats": {
        "score": -0.6674487822288769,
        "rmse": 0.6674487822288769
    }
}
[08/28/2025 09:03:23 INFO]: Procewss finished for trial blotchy-Amado_trial_122
[08/28/2025 09:03:23 INFO]: 
_________________________________________________

[08/28/2025 09:03:23 INFO]: train_net_for_optune.py main() running.
[08/28/2025 09:03:23 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 1.9904100015191966
  attention_dropout: 0.07250763176706942
  ffn_dropout: 0.07250763176706942
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.399019810843473e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_130

[08/28/2025 09:03:24 INFO]: This ft_transformer has 15.892 million parameters.
[08/28/2025 09:03:24 INFO]: Training will start at epoch 0.
[08/28/2025 09:03:24 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 09:03:35 INFO]: Training loss at epoch 36: 1.0731083750724792
[08/28/2025 09:03:39 INFO]: Training stats: {
    "score": -0.997784922910887,
    "rmse": 0.997784922910887
}
[08/28/2025 09:03:39 INFO]: Val stats: {
    "score": -0.6602690978639465,
    "rmse": 0.6602690978639465
}
[08/28/2025 09:03:39 INFO]: Test stats: {
    "score": -0.8731987971813195,
    "rmse": 0.8731987971813195
}
[08/28/2025 09:03:49 INFO]: New best epoch, val score: -0.6667744126046813
[08/28/2025 09:03:49 INFO]: Saving model to: blotchy-Amado_trial_98/model_best.pth
[08/28/2025 09:03:51 INFO]: Training accuracy: {
    "score": -1.0128094894461113,
    "rmse": 1.0128094894461113
}
[08/28/2025 09:03:51 INFO]: Val accuracy: {
    "score": -0.6582277989395023,
    "rmse": 0.6582277989395023
}
[08/28/2025 09:03:51 INFO]: Test accuracy: {
    "score": -0.8764155124407335,
    "rmse": 0.8764155124407335
}
[08/28/2025 09:03:51 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_111",
    "best_epoch": 14,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8764155124407335,
        "rmse": 0.8764155124407335
    },
    "train_stats": {
        "score": -1.0128094894461113,
        "rmse": 1.0128094894461113
    },
    "val_stats": {
        "score": -0.6582277989395023,
        "rmse": 0.6582277989395023
    }
}
[08/28/2025 09:03:51 INFO]: Procewss finished for trial blotchy-Amado_trial_111
[08/28/2025 09:03:51 INFO]: 
_________________________________________________

[08/28/2025 09:03:51 INFO]: train_net_for_optune.py main() running.
[08/28/2025 09:03:51 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 1.434805383625069
  attention_dropout: 0.1518783770616169
  ffn_dropout: 0.1518783770616169
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.27184379200552e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_131

[08/28/2025 09:03:51 INFO]: This ft_transformer has 0.229 million parameters.
[08/28/2025 09:03:51 INFO]: Training will start at epoch 0.
[08/28/2025 09:03:51 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 09:03:58 INFO]: Training loss at epoch 61: 1.0664246082305908
[08/28/2025 09:03:59 INFO]: Training loss at epoch 34: 1.0023012459278107
[08/28/2025 09:04:16 INFO]: Training loss at epoch 28: 0.9515442550182343
[08/28/2025 09:04:17 INFO]: Training loss at epoch 39: 0.9116425216197968
[08/28/2025 09:04:37 INFO]: Training loss at epoch 9: 1.2303847074508667
[08/28/2025 09:04:45 INFO]: Training loss at epoch 26: 1.0028805136680603
[08/28/2025 09:04:54 INFO]: Training loss at epoch 11: 0.890554815530777
[08/28/2025 09:04:58 INFO]: Training stats: {
    "score": -0.9960993821416676,
    "rmse": 0.9960993821416676
}
[08/28/2025 09:04:58 INFO]: Val stats: {
    "score": -0.6754883389150733,
    "rmse": 0.6754883389150733
}
[08/28/2025 09:04:58 INFO]: Test stats: {
    "score": -0.8744646869033613,
    "rmse": 0.8744646869033613
}
[08/28/2025 09:05:10 INFO]: Training loss at epoch 0: 1.1098353862762451
[08/28/2025 09:05:21 INFO]: New best epoch, val score: -0.6705121250449598
[08/28/2025 09:05:21 INFO]: Saving model to: blotchy-Amado_trial_131/model_best.pth
[08/28/2025 09:05:30 INFO]: Training loss at epoch 40: 0.995527058839798
[08/28/2025 09:05:34 INFO]: Training loss at epoch 37: 0.931533008813858
[08/28/2025 09:06:37 INFO]: Training loss at epoch 1: 0.8961783051490784
[08/28/2025 09:06:39 INFO]: Training loss at epoch 35: 0.9308447539806366
[08/28/2025 09:06:46 INFO]: Training loss at epoch 40: 1.0030671954154968
[08/28/2025 09:06:49 INFO]: New best epoch, val score: -0.6624354125458076
[08/28/2025 09:06:49 INFO]: Saving model to: blotchy-Amado_trial_131/model_best.pth
[08/28/2025 09:06:58 INFO]: Training stats: {
    "score": -1.047706580852204,
    "rmse": 1.047706580852204
}
[08/28/2025 09:06:58 INFO]: Val stats: {
    "score": -0.8373444955339737,
    "rmse": 0.8373444955339737
}
[08/28/2025 09:06:58 INFO]: Test stats: {
    "score": -0.9827395648516601,
    "rmse": 0.9827395648516601
}
[08/28/2025 09:07:00 INFO]: Training loss at epoch 29: 0.8388662338256836
[08/28/2025 09:07:03 INFO]: Training loss at epoch 23: 1.0932253003120422
[08/28/2025 09:07:21 INFO]: Training loss at epoch 41: 0.8266131579875946
[08/28/2025 09:07:26 INFO]: Training loss at epoch 37: 0.8948490917682648
[08/28/2025 09:07:32 INFO]: Training loss at epoch 38: 1.0189694166183472
[08/28/2025 09:07:37 INFO]: Training loss at epoch 47: 0.9936634302139282
[08/28/2025 09:07:49 INFO]: Training loss at epoch 26: 1.0296553671360016
[08/28/2025 09:07:58 INFO]: Training stats: {
    "score": -1.0048932373434214,
    "rmse": 1.0048932373434214
}
[08/28/2025 09:07:58 INFO]: Val stats: {
    "score": -0.7365003604863054,
    "rmse": 0.7365003604863054
}
[08/28/2025 09:07:58 INFO]: Test stats: {
    "score": -0.9056932906574985,
    "rmse": 0.9056932906574985
}
[08/28/2025 09:08:05 INFO]: Training loss at epoch 2: 1.0810731053352356
[08/28/2025 09:08:17 INFO]: Training loss at epoch 27: 0.7671578824520111
[08/28/2025 09:08:24 INFO]: Training loss at epoch 62: 0.9854938387870789
[08/28/2025 09:08:35 INFO]: Training loss at epoch 41: 1.0844364762306213
[08/28/2025 09:09:08 INFO]: Training loss at epoch 42: 1.1816238462924957
[08/28/2025 09:09:09 INFO]: Training loss at epoch 9: 0.9584152400493622
[08/28/2025 09:09:17 INFO]: Training loss at epoch 36: 1.1228846907615662
[08/28/2025 09:09:28 INFO]: Training loss at epoch 39: 1.0895406603813171
[08/28/2025 09:09:33 INFO]: Training loss at epoch 3: 0.912689208984375
[08/28/2025 09:09:46 INFO]: Training loss at epoch 25: 0.9674851298332214
[08/28/2025 09:10:12 INFO]: Training stats: {
    "score": -0.9890523580654366,
    "rmse": 0.9890523580654366
}
[08/28/2025 09:10:12 INFO]: Val stats: {
    "score": -0.7068141041428624,
    "rmse": 0.7068141041428624
}
[08/28/2025 09:10:12 INFO]: Test stats: {
    "score": -0.8923979948546948,
    "rmse": 0.8923979948546948
}
[08/28/2025 09:10:20 INFO]: Training loss at epoch 42: 0.950110912322998
[08/28/2025 09:10:23 INFO]: Training loss at epoch 0: 1.554771900177002
[08/28/2025 09:10:39 INFO]: Training loss at epoch 30: 0.910847544670105
[08/28/2025 09:10:50 INFO]: Training loss at epoch 8: 0.7586068212985992
[08/28/2025 09:10:55 INFO]: Training loss at epoch 43: 0.9348817467689514
[08/28/2025 09:11:02 INFO]: Training loss at epoch 4: 1.045615315437317
[08/28/2025 09:11:09 INFO]: Training loss at epoch 40: 0.9378863275051117
[08/28/2025 09:11:09 INFO]: Running Final Evaluation...
[08/28/2025 09:11:20 INFO]: New best epoch, val score: -0.839089323143753
[08/28/2025 09:11:20 INFO]: Saving model to: blotchy-Amado_trial_130/model_best.pth
[08/28/2025 09:11:25 INFO]: Training loss at epoch 24: 1.1798745393753052
[08/28/2025 09:11:27 INFO]: Training stats: {
    "score": -0.9980699364208532,
    "rmse": 0.9980699364208532
}
[08/28/2025 09:11:27 INFO]: Val stats: {
    "score": -0.7517567008341234,
    "rmse": 0.7517567008341234
}
[08/28/2025 09:11:27 INFO]: Test stats: {
    "score": -0.9278389069486338,
    "rmse": 0.9278389069486338
}
[08/28/2025 09:11:46 INFO]: Training loss at epoch 28: 0.7928058803081512
[08/28/2025 09:11:48 INFO]: Training accuracy: {
    "score": -1.0204275005172467,
    "rmse": 1.0204275005172467
}
[08/28/2025 09:11:48 INFO]: Val accuracy: {
    "score": -0.6518030022069564,
    "rmse": 0.6518030022069564
}
[08/28/2025 09:11:48 INFO]: Test accuracy: {
    "score": -0.8734579442356021,
    "rmse": 0.8734579442356021
}
[08/28/2025 09:11:48 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_124",
    "best_epoch": 12,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8734579442356021,
        "rmse": 0.8734579442356021
    },
    "train_stats": {
        "score": -1.0204275005172467,
        "rmse": 1.0204275005172467
    },
    "val_stats": {
        "score": -0.6518030022069564,
        "rmse": 0.6518030022069564
    }
}
[08/28/2025 09:11:48 INFO]: Procewss finished for trial blotchy-Amado_trial_124
[08/28/2025 09:11:48 INFO]: 
_________________________________________________

[08/28/2025 09:11:48 INFO]: train_net_for_optune.py main() running.
[08/28/2025 09:11:48 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 1.3792268764531912
  attention_dropout: 0.020405689119744136
  ffn_dropout: 0.020405689119744136
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.0377195865071607e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_132

[08/28/2025 09:11:49 INFO]: This ft_transformer has 13.004 million parameters.
[08/28/2025 09:11:49 INFO]: Training will start at epoch 0.
[08/28/2025 09:11:49 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 09:11:52 INFO]: Training loss at epoch 37: 0.9660347700119019
[08/28/2025 09:12:08 INFO]: New best epoch, val score: -0.6666749918829634
[08/28/2025 09:12:08 INFO]: Saving model to: blotchy-Amado_trial_98/model_best.pth
[08/28/2025 09:12:11 INFO]: Training loss at epoch 43: 0.9534547328948975
[08/28/2025 09:12:12 INFO]: Training loss at epoch 40: 0.78469517827034
[08/28/2025 09:12:16 INFO]: Training loss at epoch 27: 1.0061347484588623
[08/28/2025 09:12:25 INFO]: Running Final Evaluation...
[08/28/2025 09:12:31 INFO]: Training loss at epoch 5: 0.7948627769947052
[08/28/2025 09:12:45 INFO]: Training loss at epoch 63: 1.0108668208122253
[08/28/2025 09:12:50 INFO]: Training loss at epoch 38: 0.8537518680095673
[08/28/2025 09:13:00 INFO]: Training loss at epoch 48: 0.7817046344280243
[08/28/2025 09:13:05 INFO]: Training accuracy: {
    "score": -1.0131037631326485,
    "rmse": 1.0131037631326485
}
[08/28/2025 09:13:05 INFO]: Val accuracy: {
    "score": -0.6589858410266205,
    "rmse": 0.6589858410266205
}
[08/28/2025 09:13:05 INFO]: Test accuracy: {
    "score": -0.8743642684584906,
    "rmse": 0.8743642684584906
}
[08/28/2025 09:13:05 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_126",
    "best_epoch": 12,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8743642684584906,
        "rmse": 0.8743642684584906
    },
    "train_stats": {
        "score": -1.0131037631326485,
        "rmse": 1.0131037631326485
    },
    "val_stats": {
        "score": -0.6589858410266205,
        "rmse": 0.6589858410266205
    }
}
[08/28/2025 09:13:05 INFO]: Procewss finished for trial blotchy-Amado_trial_126
[08/28/2025 09:13:05 INFO]: 
_________________________________________________

[08/28/2025 09:13:05 INFO]: train_net_for_optune.py main() running.
[08/28/2025 09:13:05 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 1.3573581963926855
  attention_dropout: 0.022117374726504747
  ffn_dropout: 0.022117374726504747
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.291724672944098e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_133

[08/28/2025 09:13:05 INFO]: This ft_transformer has 0.223 million parameters.
[08/28/2025 09:13:05 INFO]: Training will start at epoch 0.
[08/28/2025 09:13:05 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 09:13:22 INFO]: Training loss at epoch 31: 1.059610903263092
[08/28/2025 09:13:39 INFO]: Running Final Evaluation...
[08/28/2025 09:13:45 INFO]: Training loss at epoch 10: 1.127391755580902
[08/28/2025 09:13:58 INFO]: Training loss at epoch 6: 1.0009248852729797
[08/28/2025 09:14:11 INFO]: Training loss at epoch 30: 0.9776266813278198
[08/28/2025 09:14:12 INFO]: Training loss at epoch 41: 0.8572299480438232
[08/28/2025 09:14:21 INFO]: Training loss at epoch 12: 0.8979225754737854
[08/28/2025 09:14:23 INFO]: Training loss at epoch 0: 1.1898947954177856
[08/28/2025 09:14:30 INFO]: Training loss at epoch 38: 0.8204569816589355
[08/28/2025 09:14:34 INFO]: New best epoch, val score: -0.7411442651031204
[08/28/2025 09:14:34 INFO]: Saving model to: blotchy-Amado_trial_133/model_best.pth
[08/28/2025 09:14:35 INFO]: New best epoch, val score: -0.6885758758345458
[08/28/2025 09:14:35 INFO]: Saving model to: blotchy-Amado_trial_127/model_best.pth
[08/28/2025 09:15:21 INFO]: Training loss at epoch 29: 0.9702163338661194
[08/28/2025 09:15:25 INFO]: Training loss at epoch 7: 1.0081086158752441
[08/28/2025 09:15:33 INFO]: Training accuracy: {
    "score": -0.9553275875522659,
    "rmse": 0.9553275875522659
}
[08/28/2025 09:15:33 INFO]: Val accuracy: {
    "score": -0.6647941479339562,
    "rmse": 0.6647941479339562
}
[08/28/2025 09:15:33 INFO]: Test accuracy: {
    "score": -0.9044935512152739,
    "rmse": 0.9044935512152739
}
[08/28/2025 09:15:33 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_110",
    "best_epoch": 17,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9044935512152739,
        "rmse": 0.9044935512152739
    },
    "train_stats": {
        "score": -0.9553275875522659,
        "rmse": 0.9553275875522659
    },
    "val_stats": {
        "score": -0.6647941479339562,
        "rmse": 0.6647941479339562
    }
}
[08/28/2025 09:15:33 INFO]: Procewss finished for trial blotchy-Amado_trial_110
[08/28/2025 09:15:33 INFO]: 
_________________________________________________

[08/28/2025 09:15:33 INFO]: train_net_for_optune.py main() running.
[08/28/2025 09:15:33 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 1.4589639610277108
  attention_dropout: 0.023357524354859046
  ffn_dropout: 0.023357524354859046
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.023948393805694e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_134

[08/28/2025 09:15:33 INFO]: This ft_transformer has 0.231 million parameters.
[08/28/2025 09:15:33 INFO]: Training will start at epoch 0.
[08/28/2025 09:15:33 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 09:15:49 INFO]: Training loss at epoch 1: 1.2132660150527954
[08/28/2025 09:15:50 INFO]: Training loss at epoch 25: 0.8585402965545654
[08/28/2025 09:16:02 INFO]: New best epoch, val score: -0.7035232498256814
[08/28/2025 09:16:02 INFO]: Saving model to: blotchy-Amado_trial_133/model_best.pth
[08/28/2025 09:16:07 INFO]: Training loss at epoch 32: 1.054103672504425
[08/28/2025 09:16:08 INFO]: Training loss at epoch 42: 0.9224753677845001
[08/28/2025 09:16:36 INFO]: Training stats: {
    "score": -0.9939364214760253,
    "rmse": 0.9939364214760253
}
[08/28/2025 09:16:36 INFO]: Val stats: {
    "score": -0.7122431172961327,
    "rmse": 0.7122431172961327
}
[08/28/2025 09:16:36 INFO]: Test stats: {
    "score": -0.8869790139435303,
    "rmse": 0.8869790139435303
}
[08/28/2025 09:16:46 INFO]: Training loss at epoch 28: 1.1451099514961243
[08/28/2025 09:16:48 INFO]: Training loss at epoch 0: 0.951271265745163
[08/28/2025 09:16:51 INFO]: Training loss at epoch 8: 0.8752509951591492
[08/28/2025 09:16:59 INFO]: New best epoch, val score: -0.6854909927107868
[08/28/2025 09:16:59 INFO]: Saving model to: blotchy-Amado_trial_134/model_best.pth
[08/28/2025 09:17:04 INFO]: Training loss at epoch 39: 1.0529752373695374
[08/28/2025 09:17:07 INFO]: Training loss at epoch 64: 1.3167755901813507
[08/28/2025 09:17:14 INFO]: Training loss at epoch 2: 1.0532315075397491
[08/28/2025 09:17:25 INFO]: New best epoch, val score: -0.6684692615360912
[08/28/2025 09:17:25 INFO]: Saving model to: blotchy-Amado_trial_133/model_best.pth
[08/28/2025 09:17:47 INFO]: Training loss at epoch 0: 0.8610848784446716
[08/28/2025 09:18:00 INFO]: Training stats: {
    "score": -0.9927750378448681,
    "rmse": 0.9927750378448681
}
[08/28/2025 09:18:00 INFO]: Val stats: {
    "score": -0.6794706294552145,
    "rmse": 0.6794706294552145
}
[08/28/2025 09:18:00 INFO]: Test stats: {
    "score": -0.874783817087096,
    "rmse": 0.874783817087096
}
[08/28/2025 09:18:03 INFO]: Training loss at epoch 43: 1.0348620116710663
[08/28/2025 09:18:14 INFO]: Training loss at epoch 39: 1.0639664232730865
[08/28/2025 09:18:14 INFO]: Training loss at epoch 10: 1.150858223438263
[08/28/2025 09:18:15 INFO]: Training loss at epoch 1: 1.160538375377655
[08/28/2025 09:18:17 INFO]: Training loss at epoch 9: 0.9921466112136841
[08/28/2025 09:18:17 INFO]: Training loss at epoch 1: 1.1828362941741943
[08/28/2025 09:18:27 INFO]: New best epoch, val score: -0.6774948095473007
[08/28/2025 09:18:27 INFO]: Saving model to: blotchy-Amado_trial_134/model_best.pth
[08/28/2025 09:18:37 INFO]: New best epoch, val score: -0.6850498213588619
[08/28/2025 09:18:37 INFO]: Saving model to: blotchy-Amado_trial_132/model_best.pth
[08/28/2025 09:18:42 INFO]: Training loss at epoch 3: 1.0408249497413635
[08/28/2025 09:18:47 INFO]: Training loss at epoch 9: 0.8957133591175079
[08/28/2025 09:18:49 INFO]: Training stats: {
    "score": -1.0022872467706416,
    "rmse": 1.0022872467706416
}
[08/28/2025 09:18:49 INFO]: Val stats: {
    "score": -0.7345574484126789,
    "rmse": 0.7345574484126789
}
[08/28/2025 09:18:49 INFO]: Test stats: {
    "score": -0.9053958520633619,
    "rmse": 0.9053958520633619
}
[08/28/2025 09:18:52 INFO]: Training loss at epoch 33: 0.9962590336799622
[08/28/2025 09:18:53 INFO]: New best epoch, val score: -0.6594006062954698
[08/28/2025 09:18:53 INFO]: Saving model to: blotchy-Amado_trial_133/model_best.pth
[08/28/2025 09:19:28 INFO]: Training loss at epoch 41: 1.1611559987068176
[08/28/2025 09:19:46 INFO]: Training loss at epoch 2: 1.003510743379593
[08/28/2025 09:19:58 INFO]: New best epoch, val score: -0.6694133626195735
[08/28/2025 09:19:58 INFO]: Saving model to: blotchy-Amado_trial_134/model_best.pth
[08/28/2025 09:20:02 INFO]: Training loss at epoch 44: 0.8538102209568024
[08/28/2025 09:20:07 INFO]: Training stats: {
    "score": -0.92467818560718,
    "rmse": 0.92467818560718
}
[08/28/2025 09:20:07 INFO]: Val stats: {
    "score": -0.6836169859126655,
    "rmse": 0.6836169859126655
}
[08/28/2025 09:20:07 INFO]: Test stats: {
    "score": -0.9034695891796201,
    "rmse": 0.9034695891796201
}
[08/28/2025 09:20:08 INFO]: Training loss at epoch 30: 0.9581714570522308
[08/28/2025 09:20:10 INFO]: Training loss at epoch 4: 0.9450689554214478
[08/28/2025 09:20:17 INFO]: Training loss at epoch 26: 0.9381482601165771
[08/28/2025 09:20:19 INFO]: Training loss at epoch 10: 1.2024165093898773
[08/28/2025 09:20:38 INFO]: Training loss at epoch 11: 1.0401471257209778
[08/28/2025 09:20:40 INFO]: Training loss at epoch 40: 0.9431791603565216
[08/28/2025 09:21:14 INFO]: Training loss at epoch 3: 1.080544888973236
[08/28/2025 09:21:15 INFO]: Training loss at epoch 29: 1.1623042821884155
[08/28/2025 09:21:25 INFO]: New best epoch, val score: -0.6656913501749495
[08/28/2025 09:21:25 INFO]: Saving model to: blotchy-Amado_trial_134/model_best.pth
[08/28/2025 09:21:30 INFO]: Training stats: {
    "score": -0.9617286934783648,
    "rmse": 0.9617286934783648
}
[08/28/2025 09:21:30 INFO]: Val stats: {
    "score": -0.6663919403901858,
    "rmse": 0.6663919403901858
}
[08/28/2025 09:21:30 INFO]: Test stats: {
    "score": -0.8844690510109928,
    "rmse": 0.8844690510109928
}
[08/28/2025 09:21:33 INFO]: Training loss at epoch 65: 1.1128570437431335
[08/28/2025 09:21:36 INFO]: Training loss at epoch 34: 1.2661553621292114
[08/28/2025 09:21:36 INFO]: Training loss at epoch 5: 0.8293201923370361
[08/28/2025 09:21:44 INFO]: Training loss at epoch 11: 1.1876904964447021
[08/28/2025 09:21:59 INFO]: Training loss at epoch 45: 1.071562111377716
[08/28/2025 09:22:05 INFO]: Training loss at epoch 26: 1.0497108399868011
[08/28/2025 09:22:43 INFO]: Training loss at epoch 4: 0.9108786284923553
[08/28/2025 09:22:49 INFO]: Training stats: {
    "score": -1.0191040994693505,
    "rmse": 1.0191040994693505
}
[08/28/2025 09:22:49 INFO]: Val stats: {
    "score": -0.6634377607453629,
    "rmse": 0.6634377607453629
}
[08/28/2025 09:22:49 INFO]: Test stats: {
    "score": -0.8889679989104727,
    "rmse": 0.8889679989104727
}
[08/28/2025 09:23:09 INFO]: Training loss at epoch 6: 1.0041348338127136
[08/28/2025 09:23:15 INFO]: Training loss at epoch 12: 1.1431981921195984
[08/28/2025 09:23:18 INFO]: Training loss at epoch 41: 1.063958317041397
[08/28/2025 09:23:41 INFO]: Training loss at epoch 31: 0.9418233335018158
[08/28/2025 09:23:46 INFO]: Training loss at epoch 13: 1.0094031393527985
[08/28/2025 09:23:56 INFO]: Training loss at epoch 46: 1.044117420911789
[08/28/2025 09:24:08 INFO]: Training loss at epoch 5: 0.9397858381271362
[08/28/2025 09:24:19 INFO]: Training loss at epoch 35: 1.06468665599823
[08/28/2025 09:24:33 INFO]: Training loss at epoch 7: 0.984535425901413
[08/28/2025 09:24:38 INFO]: Training loss at epoch 1: 1.0449844002723694
[08/28/2025 09:24:39 INFO]: Training loss at epoch 27: 1.0917365550994873
[08/28/2025 09:24:41 INFO]: Training loss at epoch 13: 0.8824332356452942
[08/28/2025 09:25:00 INFO]: Training loss at epoch 11: 0.8828449547290802
[08/28/2025 09:25:33 INFO]: Training loss at epoch 40: 1.1967055797576904
[08/28/2025 09:25:35 INFO]: Training loss at epoch 6: 1.0716697573661804
[08/28/2025 09:25:51 INFO]: Training loss at epoch 42: 0.990344226360321
[08/28/2025 09:25:53 INFO]: Training loss at epoch 47: 0.9122149646282196
[08/28/2025 09:25:57 INFO]: Training loss at epoch 66: 1.2388992309570312
[08/28/2025 09:26:00 INFO]: Training loss at epoch 8: 1.0296908617019653
[08/28/2025 09:26:07 INFO]: Training loss at epoch 14: 0.9611644446849823
[08/28/2025 09:26:13 INFO]: Training loss at epoch 2: 1.515198528766632
[08/28/2025 09:26:16 INFO]: Training loss at epoch 31: 0.7900121510028839
[08/28/2025 09:27:02 INFO]: Training loss at epoch 7: 1.0464322566986084
[08/28/2025 09:27:03 INFO]: Training loss at epoch 36: 0.9295323491096497
[08/28/2025 09:27:12 INFO]: Training loss at epoch 32: 0.9403860569000244
[08/28/2025 09:27:14 INFO]: Training loss at epoch 30: 1.002552330493927
[08/28/2025 09:27:23 INFO]: Training loss at epoch 12: 1.1269495487213135
[08/28/2025 09:27:25 INFO]: Training loss at epoch 9: 1.2231239080429077
[08/28/2025 09:27:32 INFO]: Training loss at epoch 15: 0.9785181283950806
[08/28/2025 09:27:41 INFO]: Running Final Evaluation...
[08/28/2025 09:27:46 INFO]: Training loss at epoch 42: 1.00472754240036
[08/28/2025 09:27:49 INFO]: Training loss at epoch 48: 1.0958701074123383
[08/28/2025 09:27:58 INFO]: Training stats: {
    "score": -1.0041351378186012,
    "rmse": 1.0041351378186012
}
[08/28/2025 09:27:58 INFO]: Val stats: {
    "score": -0.6942412213704179,
    "rmse": 0.6942412213704179
}
[08/28/2025 09:27:58 INFO]: Test stats: {
    "score": -0.8835288487346099,
    "rmse": 0.8835288487346099
}
[08/28/2025 09:28:25 INFO]: Training loss at epoch 43: 0.9531634747982025
[08/28/2025 09:28:30 INFO]: Training loss at epoch 8: 0.9262198805809021
[08/28/2025 09:28:58 INFO]: Training loss at epoch 16: 0.8793829083442688
[08/28/2025 09:29:02 INFO]: Training loss at epoch 28: 1.0755113363265991
[08/28/2025 09:29:25 INFO]: Training loss at epoch 10: 0.9987444579601288
[08/28/2025 09:29:25 INFO]: Training loss at epoch 10: 1.0468070209026337
[08/28/2025 09:29:46 INFO]: Training loss at epoch 37: 1.083209216594696
[08/28/2025 09:29:47 INFO]: Training loss at epoch 49: 1.1382637321949005
[08/28/2025 09:29:58 INFO]: Training loss at epoch 9: 1.0705775320529938
[08/28/2025 09:30:17 INFO]: Training loss at epoch 67: 0.7889678478240967
[08/28/2025 09:30:24 INFO]: Training loss at epoch 17: 1.0004846453666687
[08/28/2025 09:30:27 INFO]: Training stats: {
    "score": -0.990336507020928,
    "rmse": 0.990336507020928
}
[08/28/2025 09:30:27 INFO]: Val stats: {
    "score": -0.6747730391365219,
    "rmse": 0.6747730391365219
}
[08/28/2025 09:30:27 INFO]: Test stats: {
    "score": -0.8753043539945323,
    "rmse": 0.8753043539945323
}
[08/28/2025 09:30:29 INFO]: Training stats: {
    "score": -1.0025111686555972,
    "rmse": 1.0025111686555972
}
[08/28/2025 09:30:29 INFO]: Val stats: {
    "score": -0.6771838270887767,
    "rmse": 0.6771838270887767
}
[08/28/2025 09:30:29 INFO]: Test stats: {
    "score": -0.8632847896103791,
    "rmse": 0.8632847896103791
}
[08/28/2025 09:30:43 INFO]: New best epoch, val score: -0.6747730391365219
[08/28/2025 09:30:43 INFO]: Saving model to: blotchy-Amado_trial_125/model_best.pth
[08/28/2025 09:30:44 INFO]: Training loss at epoch 33: 1.1990629434585571
[08/28/2025 09:30:50 INFO]: Training loss at epoch 11: 1.125689148902893
[08/28/2025 09:31:00 INFO]: Training loss at epoch 41: 0.8915678262710571
[08/28/2025 09:31:01 INFO]: Training loss at epoch 44: 0.8597148060798645
[08/28/2025 09:31:22 INFO]: Training loss at epoch 2: 0.8710234761238098
[08/28/2025 09:31:42 INFO]: Training loss at epoch 31: 1.3396928906440735
[08/28/2025 09:31:46 INFO]: Training loss at epoch 12: 0.9158019125461578
[08/28/2025 09:31:50 INFO]: Training loss at epoch 18: 1.1107147932052612
[08/28/2025 09:31:55 INFO]: Training accuracy: {
    "score": -1.0109835885697114,
    "rmse": 1.0109835885697114
}
[08/28/2025 09:31:55 INFO]: Val accuracy: {
    "score": -0.68674903628246,
    "rmse": 0.68674903628246
}
[08/28/2025 09:31:55 INFO]: Test accuracy: {
    "score": -0.8819013859639162,
    "rmse": 0.8819013859639162
}
[08/28/2025 09:31:55 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_91",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8819013859639162,
        "rmse": 0.8819013859639162
    },
    "train_stats": {
        "score": -1.0109835885697114,
        "rmse": 1.0109835885697114
    },
    "val_stats": {
        "score": -0.68674903628246,
        "rmse": 0.68674903628246
    }
}
[08/28/2025 09:31:55 INFO]: Procewss finished for trial blotchy-Amado_trial_91
[08/28/2025 09:31:55 INFO]: 
_________________________________________________

[08/28/2025 09:31:55 INFO]: train_net_for_optune.py main() running.
[08/28/2025 09:31:55 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 1.9057628112248617
  attention_dropout: 0.07346361256323325
  ffn_dropout: 0.07346361256323325
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.7993425820204135e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_135

[08/28/2025 09:31:55 INFO]: This ft_transformer has 0.264 million parameters.
[08/28/2025 09:31:55 INFO]: Training will start at epoch 0.
[08/28/2025 09:31:55 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 09:31:56 INFO]: Training loss at epoch 10: 1.1214134991168976
[08/28/2025 09:32:11 INFO]: New best epoch, val score: -0.6765162476676134
[08/28/2025 09:32:11 INFO]: Saving model to: blotchy-Amado_trial_132/model_best.pth
[08/28/2025 09:32:16 INFO]: Training loss at epoch 12: 0.9303768873214722
[08/28/2025 09:32:25 INFO]: Training loss at epoch 50: 1.138686090707779
[08/28/2025 09:32:28 INFO]: Training loss at epoch 38: 1.0532029271125793
[08/28/2025 09:32:41 INFO]: New best epoch, val score: -0.6732789154659842
[08/28/2025 09:32:41 INFO]: Saving model to: blotchy-Amado_trial_125/model_best.pth
[08/28/2025 09:33:13 INFO]: Training loss at epoch 0: 1.0658385753631592
[08/28/2025 09:33:14 INFO]: Training loss at epoch 14: 1.0060229301452637
[08/28/2025 09:33:19 INFO]: Training loss at epoch 19: 1.1349282264709473
[08/28/2025 09:33:25 INFO]: New best epoch, val score: -0.8261944691422146
[08/28/2025 09:33:25 INFO]: Saving model to: blotchy-Amado_trial_135/model_best.pth
[08/28/2025 09:33:25 INFO]: Training loss at epoch 11: 0.9018261134624481
[08/28/2025 09:33:27 INFO]: Training loss at epoch 29: 1.1000860333442688
[08/28/2025 09:33:39 INFO]: Training loss at epoch 45: 1.2992613315582275
[08/28/2025 09:33:45 INFO]: Training loss at epoch 13: 0.8945205211639404
[08/28/2025 09:33:51 INFO]: Training stats: {
    "score": -0.9911691575584196,
    "rmse": 0.9911691575584196
}
[08/28/2025 09:33:51 INFO]: Val stats: {
    "score": -0.6828170322228319,
    "rmse": 0.6828170322228319
}
[08/28/2025 09:33:51 INFO]: Test stats: {
    "score": -0.8771722380192545,
    "rmse": 0.8771722380192545
}
[08/28/2025 09:34:02 INFO]: Training loss at epoch 3: 0.9947479367256165
[08/28/2025 09:34:11 INFO]: Training loss at epoch 13: 1.2396883964538574
[08/28/2025 09:34:16 INFO]: Training loss at epoch 34: 1.0687763690948486
[08/28/2025 09:34:22 INFO]: Training loss at epoch 51: 0.9265376627445221
[08/28/2025 09:34:24 INFO]: Training loss at epoch 27: 0.9853885769844055
[08/28/2025 09:34:38 INFO]: New best epoch, val score: -0.673182999774531
[08/28/2025 09:34:38 INFO]: Saving model to: blotchy-Amado_trial_125/model_best.pth
[08/28/2025 09:34:41 INFO]: Training loss at epoch 1: 1.1669209003448486
[08/28/2025 09:34:41 INFO]: Training loss at epoch 68: 0.9156596064567566
[08/28/2025 09:34:50 INFO]: Training loss at epoch 12: 1.0522524118423462
[08/28/2025 09:34:52 INFO]: New best epoch, val score: -0.7809076382046679
[08/28/2025 09:34:52 INFO]: Saving model to: blotchy-Amado_trial_135/model_best.pth
[08/28/2025 09:34:56 INFO]: Training stats: {
    "score": -0.9755688277054089,
    "rmse": 0.9755688277054089
}
[08/28/2025 09:34:56 INFO]: Val stats: {
    "score": -0.6687295660673894,
    "rmse": 0.6687295660673894
}
[08/28/2025 09:34:56 INFO]: Test stats: {
    "score": -0.8909490870023162,
    "rmse": 0.8909490870023162
}
[08/28/2025 09:34:59 INFO]: New best epoch, val score: -0.6929918471554477
[08/28/2025 09:34:59 INFO]: Saving model to: blotchy-Amado_trial_130/model_best.pth
[08/28/2025 09:35:12 INFO]: Training loss at epoch 39: 1.03512442111969
[08/28/2025 09:35:12 INFO]: Training loss at epoch 14: 0.9638310074806213
[08/28/2025 09:35:19 INFO]: Training loss at epoch 20: 1.1588662266731262
[08/28/2025 09:36:03 INFO]: Training loss at epoch 43: 0.9330433309078217
[08/28/2025 09:36:10 INFO]: Training loss at epoch 2: 1.2122709155082703
[08/28/2025 09:36:11 INFO]: Training stats: {
    "score": -0.9963933998679589,
    "rmse": 0.9963933998679589
}
[08/28/2025 09:36:11 INFO]: Val stats: {
    "score": -0.7147442744053555,
    "rmse": 0.7147442744053555
}
[08/28/2025 09:36:11 INFO]: Test stats: {
    "score": -0.8941885623798699,
    "rmse": 0.8941885623798699
}
[08/28/2025 09:36:13 INFO]: Training loss at epoch 32: 1.475402295589447
[08/28/2025 09:36:16 INFO]: Training loss at epoch 46: 0.7820800244808197
[08/28/2025 09:36:19 INFO]: Training loss at epoch 13: 1.0598047077655792
[08/28/2025 09:36:21 INFO]: Training loss at epoch 52: 1.3841067552566528
[08/28/2025 09:36:21 INFO]: New best epoch, val score: -0.7248005274145127
[08/28/2025 09:36:21 INFO]: Saving model to: blotchy-Amado_trial_135/model_best.pth
[08/28/2025 09:36:26 INFO]: Training loss at epoch 42: 0.7646768689155579
[08/28/2025 09:36:38 INFO]: Training loss at epoch 15: 0.9766873717308044
[08/28/2025 09:36:45 INFO]: Training loss at epoch 21: 1.0058185160160065
[08/28/2025 09:37:16 INFO]: Training loss at epoch 11: 0.7915551066398621
[08/28/2025 09:37:37 INFO]: Training loss at epoch 3: 0.997604489326477
[08/28/2025 09:37:44 INFO]: Training loss at epoch 14: 1.0126036405563354
[08/28/2025 09:37:48 INFO]: New best epoch, val score: -0.6772502012069911
[08/28/2025 09:37:48 INFO]: Saving model to: blotchy-Amado_trial_135/model_best.pth
[08/28/2025 09:37:49 INFO]: Training loss at epoch 35: 0.805001825094223
[08/28/2025 09:38:05 INFO]: Training loss at epoch 16: 1.1675655841827393
[08/28/2025 09:38:12 INFO]: Training loss at epoch 3: 1.1526037454605103
[08/28/2025 09:38:13 INFO]: Training loss at epoch 22: 0.9255336821079254
[08/28/2025 09:38:17 INFO]: Training loss at epoch 53: 1.1608296930789948
[08/28/2025 09:38:38 INFO]: Training loss at epoch 13: 1.1380128264427185
[08/28/2025 09:38:51 INFO]: Training loss at epoch 47: 0.9149137735366821
[08/28/2025 09:38:52 INFO]: Training loss at epoch 40: 1.1725868582725525
[08/28/2025 09:39:05 INFO]: Training loss at epoch 69: 1.1049240231513977
[08/28/2025 09:39:05 INFO]: Training loss at epoch 4: 1.0661876201629639
[08/28/2025 09:39:11 INFO]: Training loss at epoch 15: 0.8729240596294403
[08/28/2025 09:39:17 INFO]: New best epoch, val score: -0.6591921823261444
[08/28/2025 09:39:17 INFO]: Saving model to: blotchy-Amado_trial_135/model_best.pth
[08/28/2025 09:39:19 INFO]: Training loss at epoch 30: 1.1280553936958313
[08/28/2025 09:39:33 INFO]: Training loss at epoch 17: 0.9265615940093994
[08/28/2025 09:39:39 INFO]: Training loss at epoch 23: 1.1284866333007812
[08/28/2025 09:40:15 INFO]: Training loss at epoch 54: 0.914741188287735
[08/28/2025 09:40:36 INFO]: Training loss at epoch 5: 1.0535786151885986
[08/28/2025 09:40:36 INFO]: Training stats: {
    "score": -0.9566011749070235,
    "rmse": 0.9566011749070235
}
[08/28/2025 09:40:36 INFO]: Val stats: {
    "score": -0.6833490709780289,
    "rmse": 0.6833490709780289
}
[08/28/2025 09:40:36 INFO]: Test stats: {
    "score": -0.8918633216343594,
    "rmse": 0.8918633216343594
}
[08/28/2025 09:40:39 INFO]: Training loss at epoch 16: 0.9754526615142822
[08/28/2025 09:40:40 INFO]: Training loss at epoch 33: 0.9665308594703674
[08/28/2025 09:40:48 INFO]: New best epoch, val score: -0.6591595115468163
[08/28/2025 09:40:48 INFO]: Saving model to: blotchy-Amado_trial_135/model_best.pth
[08/28/2025 09:41:00 INFO]: Training loss at epoch 14: 1.6969603300094604
[08/28/2025 09:41:02 INFO]: Training loss at epoch 18: 0.8196209967136383
[08/28/2025 09:41:09 INFO]: Training loss at epoch 24: 1.0913840532302856
[08/28/2025 09:41:20 INFO]: Training loss at epoch 36: 1.1546082198619843
[08/28/2025 09:41:28 INFO]: Training loss at epoch 48: 0.8182446360588074
[08/28/2025 09:41:33 INFO]: Training loss at epoch 41: 1.1134134531021118
[08/28/2025 09:41:48 INFO]: Training loss at epoch 43: 0.8486608564853668
[08/28/2025 09:41:54 INFO]: Training loss at epoch 4: 1.1347008347511292
[08/28/2025 09:42:05 INFO]: Training loss at epoch 17: 1.0746068954467773
[08/28/2025 09:42:06 INFO]: Training loss at epoch 6: 0.8930749893188477
[08/28/2025 09:42:10 INFO]: Training loss at epoch 55: 0.9622131288051605
[08/28/2025 09:42:32 INFO]: Training loss at epoch 19: 0.928178459405899
[08/28/2025 09:42:38 INFO]: Training loss at epoch 25: 0.9066992700099945
[08/28/2025 09:42:42 INFO]: Training loss at epoch 15: 0.9122609198093414
[08/28/2025 09:43:05 INFO]: Training stats: {
    "score": -1.0010866938171448,
    "rmse": 1.0010866938171448
}
[08/28/2025 09:43:05 INFO]: Val stats: {
    "score": -0.6840793618209832,
    "rmse": 0.6840793618209832
}
[08/28/2025 09:43:05 INFO]: Test stats: {
    "score": -0.877555718612672,
    "rmse": 0.877555718612672
}
[08/28/2025 09:43:34 INFO]: Training loss at epoch 18: 1.1362817883491516
[08/28/2025 09:43:35 INFO]: Training loss at epoch 7: 0.8533425033092499
[08/28/2025 09:43:44 INFO]: Training loss at epoch 31: 1.0613771975040436
[08/28/2025 09:44:05 INFO]: Training loss at epoch 26: 0.931147426366806
[08/28/2025 09:44:06 INFO]: Training loss at epoch 49: 0.8605534732341766
[08/28/2025 09:44:08 INFO]: Training loss at epoch 56: 0.9092189073562622
[08/28/2025 09:44:19 INFO]: Training loss at epoch 42: 1.0417135655879974
[08/28/2025 09:44:19 INFO]: Training loss at epoch 44: 1.0542455315589905
[08/28/2025 09:44:32 INFO]: Training loss at epoch 20: 1.165607362985611
[08/28/2025 09:44:54 INFO]: Training loss at epoch 37: 0.983700692653656
[08/28/2025 09:45:00 INFO]: Training loss at epoch 70: 0.8289987742900848
[08/28/2025 09:45:03 INFO]: Training loss at epoch 19: 1.0581872463226318
[08/28/2025 09:45:03 INFO]: Training loss at epoch 4: 0.8998153507709503
[08/28/2025 09:45:05 INFO]: Training loss at epoch 8: 0.8158335387706757
[08/28/2025 09:45:05 INFO]: Training stats: {
    "score": -0.9919297971269386,
    "rmse": 0.9919297971269386
}
[08/28/2025 09:45:05 INFO]: Val stats: {
    "score": -0.6850201999545005,
    "rmse": 0.6850201999545005
}
[08/28/2025 09:45:05 INFO]: Test stats: {
    "score": -0.8767421769461076,
    "rmse": 0.8767421769461076
}
[08/28/2025 09:45:11 INFO]: Training loss at epoch 34: 0.9664212167263031
[08/28/2025 09:45:15 INFO]: Training loss at epoch 12: 1.055044412612915
[08/28/2025 09:45:25 INFO]: Running Final Evaluation...
[08/28/2025 09:45:29 INFO]: Training loss at epoch 14: 1.0721429884433746
[08/28/2025 09:45:33 INFO]: Training loss at epoch 27: 0.8499799072742462
[08/28/2025 09:45:35 INFO]: Training stats: {
    "score": -1.001296706333179,
    "rmse": 1.001296706333179
}
[08/28/2025 09:45:35 INFO]: Val stats: {
    "score": -0.6846286929506245,
    "rmse": 0.6846286929506245
}
[08/28/2025 09:45:35 INFO]: Test stats: {
    "score": -0.8662719247034735,
    "rmse": 0.8662719247034735
}
[08/28/2025 09:45:58 INFO]: Training loss at epoch 21: 1.149974375963211
[08/28/2025 09:46:06 INFO]: Training loss at epoch 57: 0.9521626830101013
[08/28/2025 09:46:20 INFO]: Training accuracy: {
    "score": -1.0001727890082728,
    "rmse": 1.0001727890082728
}
[08/28/2025 09:46:20 INFO]: Val accuracy: {
    "score": -0.6667576053685822,
    "rmse": 0.6667576053685822
}
[08/28/2025 09:46:20 INFO]: Test accuracy: {
    "score": -0.8740976178479589,
    "rmse": 0.8740976178479589
}
[08/28/2025 09:46:20 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_121",
    "best_epoch": 18,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8740976178479589,
        "rmse": 0.8740976178479589
    },
    "train_stats": {
        "score": -1.0001727890082728,
        "rmse": 1.0001727890082728
    },
    "val_stats": {
        "score": -0.6667576053685822,
        "rmse": 0.6667576053685822
    }
}
[08/28/2025 09:46:20 INFO]: Procewss finished for trial blotchy-Amado_trial_121
[08/28/2025 09:46:20 INFO]: 
_________________________________________________

[08/28/2025 09:46:20 INFO]: train_net_for_optune.py main() running.
[08/28/2025 09:46:20 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 1.2952914765154222
  attention_dropout: 0.020502588412965102
  ffn_dropout: 0.020502588412965102
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.7508728177999994e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_136

[08/28/2025 09:46:20 INFO]: This ft_transformer has 0.218 million parameters.
[08/28/2025 09:46:20 INFO]: Training will start at epoch 0.
[08/28/2025 09:46:20 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 09:46:32 INFO]: Training loss at epoch 9: 0.8649996221065521
[08/28/2025 09:46:44 INFO]: Training loss at epoch 28: 1.0620521903038025
[08/28/2025 09:47:00 INFO]: Training loss at epoch 28: 1.135539323091507
[08/28/2025 09:47:02 INFO]: Training loss at epoch 20: 0.9111803472042084
[08/28/2025 09:47:02 INFO]: Training loss at epoch 43: 1.243681162595749
[08/28/2025 09:47:07 INFO]: Training stats: {
    "score": -1.0117265453906992,
    "rmse": 1.0117265453906992
}
[08/28/2025 09:47:07 INFO]: Val stats: {
    "score": -0.6591677233438679,
    "rmse": 0.6591677233438679
}
[08/28/2025 09:47:07 INFO]: Test stats: {
    "score": -0.8785277013569424,
    "rmse": 0.8785277013569424
}
[08/28/2025 09:47:18 INFO]: Training loss at epoch 44: 0.9949930906295776
[08/28/2025 09:47:28 INFO]: Training loss at epoch 22: 1.1265098452568054
[08/28/2025 09:47:39 INFO]: Training loss at epoch 0: 0.9857280850410461
[08/28/2025 09:47:50 INFO]: New best epoch, val score: -0.6601826614794197
[08/28/2025 09:47:50 INFO]: Saving model to: blotchy-Amado_trial_136/model_best.pth
[08/28/2025 09:47:53 INFO]: Training loss at epoch 15: 1.2248109579086304
[08/28/2025 09:48:06 INFO]: Training loss at epoch 58: 0.9965932369232178
[08/28/2025 09:48:10 INFO]: Training loss at epoch 32: 1.0632545948028564
[08/28/2025 09:48:15 INFO]: New best epoch, val score: -0.6663253748844084
[08/28/2025 09:48:15 INFO]: Saving model to: blotchy-Amado_trial_101/model_best.pth
[08/28/2025 09:48:28 INFO]: Training loss at epoch 38: 0.9198872447013855
[08/28/2025 09:48:29 INFO]: Training loss at epoch 29: 0.8448098301887512
[08/28/2025 09:48:29 INFO]: Training loss at epoch 21: 1.1272562742233276
[08/28/2025 09:48:37 INFO]: Training loss at epoch 10: 1.043760597705841
[08/28/2025 09:48:40 INFO]: New best epoch, val score: -0.6825214479728133
[08/28/2025 09:48:40 INFO]: Saving model to: blotchy-Amado_trial_127/model_best.pth
[08/28/2025 09:48:48 INFO]: New best epoch, val score: -0.6591173131631693
[08/28/2025 09:48:48 INFO]: Saving model to: blotchy-Amado_trial_135/model_best.pth
[08/28/2025 09:48:54 INFO]: Training loss at epoch 23: 0.8853314816951752
[08/28/2025 09:49:00 INFO]: Training stats: {
    "score": -0.9919124787897534,
    "rmse": 0.9919124787897534
}
[08/28/2025 09:49:00 INFO]: Val stats: {
    "score": -0.6738809403282126,
    "rmse": 0.6738809403282126
}
[08/28/2025 09:49:00 INFO]: Test stats: {
    "score": -0.8741443575377897,
    "rmse": 0.8741443575377897
}
[08/28/2025 09:49:04 INFO]: Training loss at epoch 1: 1.084100365638733
[08/28/2025 09:49:22 INFO]: Training loss at epoch 71: 0.8255014419555664
[08/28/2025 09:49:40 INFO]: Training loss at epoch 35: 0.8569786846637726
[08/28/2025 09:49:47 INFO]: Training loss at epoch 44: 1.0615282654762268
[08/28/2025 09:49:53 INFO]: Training loss at epoch 5: 1.4876044392585754
[08/28/2025 09:49:57 INFO]: Training loss at epoch 22: 0.9875360131263733
[08/28/2025 09:50:03 INFO]: Training loss at epoch 59: 0.8195865154266357
[08/28/2025 09:50:07 INFO]: New best epoch, val score: -0.6976844726668932
[08/28/2025 09:50:07 INFO]: Saving model to: blotchy-Amado_trial_123/model_best.pth
[08/28/2025 09:50:07 INFO]: Training loss at epoch 11: 1.094562828540802
[08/28/2025 09:50:23 INFO]: Training loss at epoch 24: 1.012491226196289
[08/28/2025 09:50:29 INFO]: Training loss at epoch 30: 1.2173544466495514
[08/28/2025 09:50:33 INFO]: Training loss at epoch 2: 0.935888797044754
[08/28/2025 09:50:46 INFO]: Training stats: {
    "score": -0.9777907769447061,
    "rmse": 0.9777907769447061
}
[08/28/2025 09:50:46 INFO]: Val stats: {
    "score": -0.6922633362375084,
    "rmse": 0.6922633362375084
}
[08/28/2025 09:50:46 INFO]: Test stats: {
    "score": -0.8800063234850699,
    "rmse": 0.8800063234850699
}
[08/28/2025 09:51:24 INFO]: Training loss at epoch 23: 0.8999454379081726
[08/28/2025 09:51:36 INFO]: Training loss at epoch 12: 1.1636574268341064
[08/28/2025 09:51:53 INFO]: Training loss at epoch 25: 1.049975574016571
[08/28/2025 09:51:55 INFO]: Training loss at epoch 5: 0.8933768570423126
[08/28/2025 09:51:59 INFO]: Training loss at epoch 31: 1.0350079536437988
[08/28/2025 09:52:01 INFO]: Training loss at epoch 3: 1.0075211226940155
[08/28/2025 09:52:02 INFO]: Training loss at epoch 39: 0.9257822930812836
[08/28/2025 09:52:13 INFO]: Training loss at epoch 16: 1.056162178516388
[08/28/2025 09:52:20 INFO]: Training loss at epoch 15: 1.0175378620624542
[08/28/2025 09:52:31 INFO]: Training loss at epoch 45: 1.029721736907959
[08/28/2025 09:52:37 INFO]: Training loss at epoch 33: 0.8438511788845062
[08/28/2025 09:52:42 INFO]: Training loss at epoch 45: 0.9760733544826508
[08/28/2025 09:52:45 INFO]: Training loss at epoch 60: 0.8472037613391876
[08/28/2025 09:52:47 INFO]: Training loss at epoch 45: 0.8443496525287628
[08/28/2025 09:52:52 INFO]: Training loss at epoch 24: 1.2681453227996826
[08/28/2025 09:52:53 INFO]: New best epoch, val score: -0.6966105666467524
[08/28/2025 09:52:53 INFO]: Saving model to: blotchy-Amado_trial_123/model_best.pth
[08/28/2025 09:53:06 INFO]: Training loss at epoch 13: 0.892193078994751
[08/28/2025 09:53:11 INFO]: Training loss at epoch 13: 1.1648960411548615
[08/28/2025 09:53:17 INFO]: Training stats: {
    "score": -0.9896802493654563,
    "rmse": 0.9896802493654563
}
[08/28/2025 09:53:17 INFO]: Val stats: {
    "score": -0.6892050945383416,
    "rmse": 0.6892050945383416
}
[08/28/2025 09:53:17 INFO]: Test stats: {
    "score": -0.8770210510585305,
    "rmse": 0.8770210510585305
}
[08/28/2025 09:53:21 INFO]: Training loss at epoch 26: 1.151189535856247
[08/28/2025 09:53:27 INFO]: Training loss at epoch 32: 0.9622752964496613
[08/28/2025 09:53:28 INFO]: Training loss at epoch 4: 0.8883562088012695
[08/28/2025 09:53:39 INFO]: Running Final Evaluation...
[08/28/2025 09:53:49 INFO]: Training loss at epoch 72: 0.8733232319355011
[08/28/2025 09:54:11 INFO]: Training loss at epoch 36: 1.0757458806037903
[08/28/2025 09:54:12 INFO]: Training accuracy: {
    "score": -1.007927746029546,
    "rmse": 1.007927746029546
}
[08/28/2025 09:54:12 INFO]: Val accuracy: {
    "score": -0.6624354125458076,
    "rmse": 0.6624354125458076
}
[08/28/2025 09:54:12 INFO]: Test accuracy: {
    "score": -0.8791076101770287,
    "rmse": 0.8791076101770287
}
[08/28/2025 09:54:12 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_131",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8791076101770287,
        "rmse": 0.8791076101770287
    },
    "train_stats": {
        "score": -1.007927746029546,
        "rmse": 1.007927746029546
    },
    "val_stats": {
        "score": -0.6624354125458076,
        "rmse": 0.6624354125458076
    }
}
[08/28/2025 09:54:12 INFO]: Procewss finished for trial blotchy-Amado_trial_131
[08/28/2025 09:54:12 INFO]: 
_________________________________________________

[08/28/2025 09:54:12 INFO]: train_net_for_optune.py main() running.
[08/28/2025 09:54:12 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 1.274868743704885
  attention_dropout: 0.018259374107246013
  ffn_dropout: 0.018259374107246013
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.1922112592101e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_137

[08/28/2025 09:54:12 INFO]: This ft_transformer has 12.506 million parameters.
[08/28/2025 09:54:12 INFO]: Training will start at epoch 0.
[08/28/2025 09:54:12 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 09:54:19 INFO]: Training loss at epoch 25: 1.1437011361122131
[08/28/2025 09:54:36 INFO]: Training loss at epoch 14: 1.079532653093338
[08/28/2025 09:54:42 INFO]: Training loss at epoch 61: 0.8873592615127563
[08/28/2025 09:54:47 INFO]: Training loss at epoch 16: 1.128466546535492
[08/28/2025 09:54:49 INFO]: Training loss at epoch 27: 1.0580134391784668
[08/28/2025 09:54:55 INFO]: Training loss at epoch 5: 0.9552404284477234
[08/28/2025 09:55:15 INFO]: Training loss at epoch 46: 1.039198398590088
[08/28/2025 09:55:34 INFO]: New best epoch, val score: -0.6727794114099231
[08/28/2025 09:55:34 INFO]: Saving model to: blotchy-Amado_trial_127/model_best.pth
[08/28/2025 09:55:45 INFO]: Training loss at epoch 26: 0.9734113812446594
[08/28/2025 09:56:05 INFO]: Training loss at epoch 15: 0.7664196491241455
[08/28/2025 09:56:18 INFO]: Training loss at epoch 28: 1.192017525434494
[08/28/2025 09:56:24 INFO]: Training loss at epoch 6: 1.3711132109165192
[08/28/2025 09:56:36 INFO]: Training loss at epoch 62: 1.1188752055168152
[08/28/2025 09:56:49 INFO]: Training loss at epoch 40: 1.0316119194030762
[08/28/2025 09:56:56 INFO]: Training loss at epoch 34: 1.0195297598838806
[08/28/2025 09:57:11 INFO]: Training loss at epoch 27: 0.9282336235046387
[08/28/2025 09:57:37 INFO]: Training loss at epoch 16: 0.9955226480960846
[08/28/2025 09:57:45 INFO]: Training loss at epoch 6: 1.4675245881080627
[08/28/2025 09:57:49 INFO]: Training loss at epoch 29: 1.0073256492614746
[08/28/2025 09:57:55 INFO]: Training loss at epoch 7: 0.9845511317253113
[08/28/2025 09:57:56 INFO]: Training loss at epoch 47: 1.006331205368042
[08/28/2025 09:58:04 INFO]: Training loss at epoch 46: 0.9541928470134735
[08/28/2025 09:58:04 INFO]: Training loss at epoch 73: 1.169502854347229
[08/28/2025 09:58:20 INFO]: Training stats: {
    "score": -0.9998594414149651,
    "rmse": 0.9998594414149651
}
[08/28/2025 09:58:20 INFO]: Val stats: {
    "score": -0.672265782455795,
    "rmse": 0.672265782455795
}
[08/28/2025 09:58:20 INFO]: Test stats: {
    "score": -0.8713651299146015,
    "rmse": 0.8713651299146015
}
[08/28/2025 09:58:32 INFO]: Training loss at epoch 63: 1.080075740814209
[08/28/2025 09:58:32 INFO]: Training loss at epoch 37: 1.1935620307922363
[08/28/2025 09:58:36 INFO]: Running Final Evaluation...
[08/28/2025 09:58:37 INFO]: Training loss at epoch 28: 1.1912639141082764
[08/28/2025 09:58:38 INFO]: Training loss at epoch 6: 0.9232606291770935
[08/28/2025 09:59:02 INFO]: Training loss at epoch 29: 0.9894291162490845
[08/28/2025 09:59:03 INFO]: Training loss at epoch 16: 0.9928840696811676
[08/28/2025 09:59:05 INFO]: Training loss at epoch 17: 1.1843478977680206
[08/28/2025 09:59:21 INFO]: Training loss at epoch 8: 0.9762536585330963
[08/28/2025 09:59:48 INFO]: Training loss at epoch 30: 1.0262936055660248
[08/28/2025 09:59:59 INFO]: Training loss at epoch 0: 0.8604350686073303
[08/28/2025 10:00:05 INFO]: Training loss at epoch 29: 0.9437179565429688
[08/28/2025 10:00:10 INFO]: Training accuracy: {
    "score": -0.9853623764695658,
    "rmse": 0.9853623764695658
}
[08/28/2025 10:00:10 INFO]: Val accuracy: {
    "score": -0.6737080006403366,
    "rmse": 0.6737080006403366
}
[08/28/2025 10:00:10 INFO]: Test accuracy: {
    "score": -0.8872702759797567,
    "rmse": 0.8872702759797567
}
[08/28/2025 10:00:10 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_106",
    "best_epoch": 42,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8872702759797567,
        "rmse": 0.8872702759797567
    },
    "train_stats": {
        "score": -0.9853623764695658,
        "rmse": 0.9853623764695658
    },
    "val_stats": {
        "score": -0.6737080006403366,
        "rmse": 0.6737080006403366
    }
}
[08/28/2025 10:00:10 INFO]: Procewss finished for trial blotchy-Amado_trial_106
[08/28/2025 10:00:11 INFO]: 
_________________________________________________

[08/28/2025 10:00:11 INFO]: train_net_for_optune.py main() running.
[08/28/2025 10:00:11 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 1.322744450187629
  attention_dropout: 0.013118079170444469
  ffn_dropout: 0.013118079170444469
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.4190939598637598e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_138

[08/28/2025 10:00:11 INFO]: This ft_transformer has 0.834 million parameters.
[08/28/2025 10:00:11 INFO]: Training will start at epoch 0.
[08/28/2025 10:00:11 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 10:00:20 INFO]: Training loss at epoch 41: 0.8724377751350403
[08/28/2025 10:00:29 INFO]: Training loss at epoch 64: 1.0349836945533752
[08/28/2025 10:00:33 INFO]: Training loss at epoch 18: 1.104135811328888
[08/28/2025 10:00:36 INFO]: Training stats: {
    "score": -1.000963823008276,
    "rmse": 1.000963823008276
}
[08/28/2025 10:00:36 INFO]: Val stats: {
    "score": -0.6879844375234253,
    "rmse": 0.6879844375234253
}
[08/28/2025 10:00:36 INFO]: Test stats: {
    "score": -0.8679444716332574,
    "rmse": 0.8679444716332574
}
[08/28/2025 10:00:37 INFO]: Training loss at epoch 48: 1.041426956653595
[08/28/2025 10:00:45 INFO]: New best epoch, val score: -0.6733203176911534
[08/28/2025 10:00:45 INFO]: Saving model to: blotchy-Amado_trial_137/model_best.pth
[08/28/2025 10:00:47 INFO]: Training loss at epoch 9: 1.043441653251648
[08/28/2025 10:00:54 INFO]: Training loss at epoch 46: 1.030324101448059
[08/28/2025 10:01:03 INFO]: Training loss at epoch 14: 0.872688353061676
[08/28/2025 10:01:12 INFO]: Training loss at epoch 31: 0.8704170286655426
[08/28/2025 10:01:15 INFO]: Training loss at epoch 35: 1.057791531085968
[08/28/2025 10:01:19 INFO]: Training stats: {
    "score": -1.0173813582034594,
    "rmse": 1.0173813582034594
}
[08/28/2025 10:01:19 INFO]: Val stats: {
    "score": -0.7582362431387348,
    "rmse": 0.7582362431387348
}
[08/28/2025 10:01:19 INFO]: Test stats: {
    "score": -0.920994615130406,
    "rmse": 0.920994615130406
}
[08/28/2025 10:01:32 INFO]: Training loss at epoch 17: 1.1445752084255219
[08/28/2025 10:01:33 INFO]: Training loss at epoch 17: 0.9615616202354431
[08/28/2025 10:01:44 INFO]: Training loss at epoch 0: 1.9784345626831055
[08/28/2025 10:01:58 INFO]: New best epoch, val score: -1.0815688113964723
[08/28/2025 10:01:58 INFO]: Saving model to: blotchy-Amado_trial_138/model_best.pth
[08/28/2025 10:02:01 INFO]: Training loss at epoch 19: 0.9072408974170685
[08/28/2025 10:02:03 INFO]: Training loss at epoch 30: 1.148472398519516
[08/28/2025 10:02:25 INFO]: Training loss at epoch 65: 0.9459251761436462
[08/28/2025 10:02:33 INFO]: Training stats: {
    "score": -0.9971650400236423,
    "rmse": 0.9971650400236423
}
[08/28/2025 10:02:33 INFO]: Val stats: {
    "score": -0.6757101618130008,
    "rmse": 0.6757101618130008
}
[08/28/2025 10:02:33 INFO]: Test stats: {
    "score": -0.8815725708544574,
    "rmse": 0.8815725708544574
}
[08/28/2025 10:02:38 INFO]: Training loss at epoch 32: 0.8572995066642761
[08/28/2025 10:02:45 INFO]: Training loss at epoch 10: 1.0237691402435303
[08/28/2025 10:02:57 INFO]: Training loss at epoch 38: 0.9910238087177277
[08/28/2025 10:03:08 INFO]: Training stats: {
    "score": -0.9551574509499864,
    "rmse": 0.9551574509499864
}
[08/28/2025 10:03:08 INFO]: Val stats: {
    "score": -0.6688881791481032,
    "rmse": 0.6688881791481032
}
[08/28/2025 10:03:08 INFO]: Test stats: {
    "score": -0.8882856906951447,
    "rmse": 0.8882856906951447
}
[08/28/2025 10:03:19 INFO]: Training loss at epoch 49: 0.9269119799137115
[08/28/2025 10:03:26 INFO]: Training loss at epoch 47: 1.030138909816742
[08/28/2025 10:03:29 INFO]: Training loss at epoch 31: 1.1389228701591492
[08/28/2025 10:03:31 INFO]: Training loss at epoch 1: 1.8095069527626038
[08/28/2025 10:03:44 INFO]: New best epoch, val score: -0.987106518097418
[08/28/2025 10:03:44 INFO]: Saving model to: blotchy-Amado_trial_138/model_best.pth
[08/28/2025 10:03:50 INFO]: Training loss at epoch 42: 1.0712970495224
[08/28/2025 10:04:02 INFO]: Training loss at epoch 20: 1.2326456308364868
[08/28/2025 10:04:06 INFO]: Training loss at epoch 33: 0.9125970900058746
[08/28/2025 10:04:12 INFO]: Training loss at epoch 11: 1.0005126297473907
[08/28/2025 10:04:19 INFO]: Training stats: {
    "score": -0.992919514526882,
    "rmse": 0.992919514526882
}
[08/28/2025 10:04:19 INFO]: Val stats: {
    "score": -0.7068130484863365,
    "rmse": 0.7068130484863365
}
[08/28/2025 10:04:19 INFO]: Test stats: {
    "score": -0.89112903957277,
    "rmse": 0.89112903957277
}
[08/28/2025 10:04:23 INFO]: Training loss at epoch 66: 1.11924409866333
[08/28/2025 10:04:57 INFO]: Training loss at epoch 32: 0.9593908786773682
[08/28/2025 10:05:20 INFO]: Training loss at epoch 2: 1.931951642036438
[08/28/2025 10:05:23 INFO]: Training loss at epoch 7: 0.8453450798988342
[08/28/2025 10:05:30 INFO]: Training loss at epoch 21: 0.9613435864448547
[08/28/2025 10:05:33 INFO]: Training loss at epoch 34: 1.0144833624362946
[08/28/2025 10:05:34 INFO]: Training loss at epoch 7: 1.355199158191681
[08/28/2025 10:05:35 INFO]: New best epoch, val score: -0.8563406220144055
[08/28/2025 10:05:35 INFO]: Saving model to: blotchy-Amado_trial_138/model_best.pth
[08/28/2025 10:05:38 INFO]: Training loss at epoch 12: 0.8747254312038422
[08/28/2025 10:05:41 INFO]: Training loss at epoch 36: 0.7125656306743622
[08/28/2025 10:05:44 INFO]: Running Final Evaluation...
[08/28/2025 10:05:48 INFO]: Training loss at epoch 17: 1.0012917220592499
[08/28/2025 10:06:17 INFO]: Training accuracy: {
    "score": -1.0082473392295368,
    "rmse": 1.0082473392295368
}
[08/28/2025 10:06:17 INFO]: Val accuracy: {
    "score": -0.6594006062954698,
    "rmse": 0.6594006062954698
}
[08/28/2025 10:06:17 INFO]: Test accuracy: {
    "score": -0.8663723769305542,
    "rmse": 0.8663723769305542
}
[08/28/2025 10:06:17 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_133",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8663723769305542,
        "rmse": 0.8663723769305542
    },
    "train_stats": {
        "score": -1.0082473392295368,
        "rmse": 1.0082473392295368
    },
    "val_stats": {
        "score": -0.6594006062954698,
        "rmse": 0.6594006062954698
    }
}
[08/28/2025 10:06:17 INFO]: Procewss finished for trial blotchy-Amado_trial_133
[08/28/2025 10:06:17 INFO]: 
_________________________________________________

[08/28/2025 10:06:17 INFO]: train_net_for_optune.py main() running.
[08/28/2025 10:06:17 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 2.368149494943733
  attention_dropout: 0.3324958088876957
  ffn_dropout: 0.3324958088876957
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.3208153230295494e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_139

[08/28/2025 10:06:17 INFO]: This ft_transformer has 1.144 million parameters.
[08/28/2025 10:06:17 INFO]: Training will start at epoch 0.
[08/28/2025 10:06:17 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 10:06:22 INFO]: Training loss at epoch 67: 1.0377649068832397
[08/28/2025 10:06:26 INFO]: Training loss at epoch 33: 0.9629741311073303
[08/28/2025 10:06:37 INFO]: Training loss at epoch 1: 1.1724807620048523
[08/28/2025 10:06:59 INFO]: Training loss at epoch 22: 1.1595269441604614
[08/28/2025 10:07:06 INFO]: Training loss at epoch 50: 1.01984903216362
[08/28/2025 10:07:06 INFO]: Training loss at epoch 13: 1.153139591217041
[08/28/2025 10:07:11 INFO]: Training loss at epoch 3: 1.2486984729766846
[08/28/2025 10:07:24 INFO]: New best epoch, val score: -0.7216134270757983
[08/28/2025 10:07:24 INFO]: Saving model to: blotchy-Amado_trial_138/model_best.pth
[08/28/2025 10:07:26 INFO]: Training loss at epoch 43: 0.883661538362503
[08/28/2025 10:07:30 INFO]: Training loss at epoch 39: 1.0037092566490173
[08/28/2025 10:07:54 INFO]: Training loss at epoch 34: 1.0179656744003296
[08/28/2025 10:08:02 INFO]: Training loss at epoch 0: 1.0618793368339539
[08/28/2025 10:08:06 INFO]: Running Final Evaluation...
[08/28/2025 10:08:16 INFO]: New best epoch, val score: -0.6735492753678851
[08/28/2025 10:08:16 INFO]: Saving model to: blotchy-Amado_trial_139/model_best.pth
[08/28/2025 10:08:21 INFO]: Training loss at epoch 68: 0.9358347952365875
[08/28/2025 10:08:28 INFO]: Training loss at epoch 23: 0.9395491778850555
[08/28/2025 10:08:29 INFO]: Training loss at epoch 18: 1.0682346522808075
[08/28/2025 10:08:33 INFO]: Training loss at epoch 14: 0.9395576417446136
[08/28/2025 10:08:41 INFO]: Training accuracy: {
    "score": -1.0151027954688274,
    "rmse": 1.0151027954688274
}
[08/28/2025 10:08:41 INFO]: Val accuracy: {
    "score": -0.6656913501749495,
    "rmse": 0.6656913501749495
}
[08/28/2025 10:08:41 INFO]: Test accuracy: {
    "score": -0.8646710018265708,
    "rmse": 0.8646710018265708
}
[08/28/2025 10:08:41 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_134",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8646710018265708,
        "rmse": 0.8646710018265708
    },
    "train_stats": {
        "score": -1.0151027954688274,
        "rmse": 1.0151027954688274
    },
    "val_stats": {
        "score": -0.6656913501749495,
        "rmse": 0.6656913501749495
    }
}
[08/28/2025 10:08:41 INFO]: Procewss finished for trial blotchy-Amado_trial_134
[08/28/2025 10:08:42 INFO]: 
_________________________________________________

[08/28/2025 10:08:42 INFO]: train_net_for_optune.py main() running.
[08/28/2025 10:08:42 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 1.9916555667569202
  attention_dropout: 0.3374630216202133
  ffn_dropout: 0.3374630216202133
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.116957861996181e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_140

[08/28/2025 10:08:42 INFO]: This ft_transformer has 1.031 million parameters.
[08/28/2025 10:08:42 INFO]: Training will start at epoch 0.
[08/28/2025 10:08:42 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 10:08:57 INFO]: Training loss at epoch 48: 0.9158227145671844
[08/28/2025 10:08:59 INFO]: Training loss at epoch 4: 1.0462983846664429
[08/28/2025 10:09:02 INFO]: Training loss at epoch 15: 1.0351509749889374
[08/28/2025 10:09:03 INFO]: Training stats: {
    "score": -1.024647910564667,
    "rmse": 1.024647910564667
}
[08/28/2025 10:09:03 INFO]: Val stats: {
    "score": -0.6750229803795254,
    "rmse": 0.6750229803795254
}
[08/28/2025 10:09:03 INFO]: Test stats: {
    "score": -0.901918543801443,
    "rmse": 0.901918543801443
}
[08/28/2025 10:09:13 INFO]: Training loss at epoch 47: 0.8555794656276703
[08/28/2025 10:09:15 INFO]: New best epoch, val score: -0.6624801949906628
[08/28/2025 10:09:15 INFO]: Saving model to: blotchy-Amado_trial_138/model_best.pth
[08/28/2025 10:09:50 INFO]: Training loss at epoch 51: 0.9312708377838135
[08/28/2025 10:09:57 INFO]: Training loss at epoch 24: 0.8629141449928284
[08/28/2025 10:09:59 INFO]: Training loss at epoch 1: 1.0012345612049103
[08/28/2025 10:10:00 INFO]: Training loss at epoch 15: 0.9574155509471893
[08/28/2025 10:10:10 INFO]: Training loss at epoch 37: 1.06765815615654
[08/28/2025 10:10:20 INFO]: Training loss at epoch 69: 1.1590174734592438
[08/28/2025 10:10:21 INFO]: Training loss at epoch 0: 1.0583529472351074
[08/28/2025 10:10:35 INFO]: New best epoch, val score: -0.7736973755035512
[08/28/2025 10:10:35 INFO]: Saving model to: blotchy-Amado_trial_140/model_best.pth
[08/28/2025 10:10:48 INFO]: Training loss at epoch 5: 1.132713943719864
[08/28/2025 10:11:00 INFO]: Training loss at epoch 44: 0.7453601509332657
[08/28/2025 10:11:01 INFO]: Training stats: {
    "score": -0.9738798283112757,
    "rmse": 0.9738798283112757
}
[08/28/2025 10:11:01 INFO]: Val stats: {
    "score": -0.6811539498551592,
    "rmse": 0.6811539498551592
}
[08/28/2025 10:11:01 INFO]: Test stats: {
    "score": -0.8774522614475869,
    "rmse": 0.8774522614475869
}
[08/28/2025 10:11:02 INFO]: Training loss at epoch 18: 0.9498984813690186
[08/28/2025 10:11:24 INFO]: Training loss at epoch 25: 1.0274156332015991
[08/28/2025 10:11:25 INFO]: Training loss at epoch 16: 1.378783494234085
[08/28/2025 10:11:55 INFO]: Training loss at epoch 2: 1.133245199918747
[08/28/2025 10:12:11 INFO]: Training loss at epoch 8: 1.2325244545936584
[08/28/2025 10:12:12 INFO]: Training loss at epoch 1: 1.1874816417694092
[08/28/2025 10:12:28 INFO]: New best epoch, val score: -0.7386120986298144
[08/28/2025 10:12:28 INFO]: Saving model to: blotchy-Amado_trial_140/model_best.pth
[08/28/2025 10:12:34 INFO]: Training loss at epoch 52: 1.0122472047805786
[08/28/2025 10:12:37 INFO]: Training loss at epoch 6: 1.1173177361488342
[08/28/2025 10:12:37 INFO]: Training loss at epoch 18: 0.8425829708576202
[08/28/2025 10:12:53 INFO]: Training loss at epoch 17: 0.9432331919670105
[08/28/2025 10:12:54 INFO]: Training loss at epoch 26: 0.9977290332317352
[08/28/2025 10:12:58 INFO]: Training loss at epoch 70: 1.02168408036232
[08/28/2025 10:13:16 INFO]: Training loss at epoch 2: 1.2257606983184814
[08/28/2025 10:13:28 INFO]: Training loss at epoch 8: 1.0125433206558228
[08/28/2025 10:13:33 INFO]: Training loss at epoch 40: 1.3067434430122375
[08/28/2025 10:13:53 INFO]: Training loss at epoch 3: 0.8896344006061554
[08/28/2025 10:14:08 INFO]: Training loss at epoch 2: 1.1399181485176086
[08/28/2025 10:14:21 INFO]: Training loss at epoch 18: 0.8795047402381897
[08/28/2025 10:14:23 INFO]: Training loss at epoch 27: 0.8587098121643066
[08/28/2025 10:14:25 INFO]: Training loss at epoch 7: 1.0202259421348572
[08/28/2025 10:14:26 INFO]: Training loss at epoch 49: 0.9322800636291504
[08/28/2025 10:14:35 INFO]: Training loss at epoch 45: 1.131646066904068
[08/28/2025 10:14:36 INFO]: Training loss at epoch 38: 1.2746348977088928
[08/28/2025 10:14:59 INFO]: Training loss at epoch 71: 0.8557643890380859
[08/28/2025 10:15:20 INFO]: Training loss at epoch 53: 0.8763214945793152
[08/28/2025 10:15:26 INFO]: Training loss at epoch 19: 1.299400955438614
[08/28/2025 10:15:32 INFO]: Training loss at epoch 30: 0.9119745492935181
[08/28/2025 10:15:50 INFO]: Training loss at epoch 19: 0.8418294489383698
[08/28/2025 10:15:54 INFO]: Training loss at epoch 4: 1.0628234446048737
[08/28/2025 10:15:54 INFO]: Training loss at epoch 28: 0.9098736047744751
[08/28/2025 10:16:04 INFO]: Training loss at epoch 3: 1.1165034770965576
[08/28/2025 10:16:15 INFO]: Training loss at epoch 8: 0.9643443524837494
[08/28/2025 10:16:18 INFO]: New best epoch, val score: -0.7335012173986145
[08/28/2025 10:16:18 INFO]: Saving model to: blotchy-Amado_trial_140/model_best.pth
[08/28/2025 10:16:19 INFO]: Training stats: {
    "score": -0.9050392989457994,
    "rmse": 0.9050392989457994
}
[08/28/2025 10:16:19 INFO]: Val stats: {
    "score": -0.688959955113372,
    "rmse": 0.688959955113372
}
[08/28/2025 10:16:19 INFO]: Test stats: {
    "score": -0.9128358689953001,
    "rmse": 0.9128358689953001
}
[08/28/2025 10:16:22 INFO]: Training stats: {
    "score": -0.9957373889741636,
    "rmse": 0.9957373889741636
}
[08/28/2025 10:16:22 INFO]: Val stats: {
    "score": -0.6717504682969603,
    "rmse": 0.6717504682969603
}
[08/28/2025 10:16:22 INFO]: Test stats: {
    "score": -0.8737359319418355,
    "rmse": 0.8737359319418355
}
[08/28/2025 10:16:57 INFO]: Training loss at epoch 72: 1.0006495118141174
[08/28/2025 10:16:58 INFO]: Training loss at epoch 16: 0.9164439141750336
[08/28/2025 10:17:21 INFO]: Training loss at epoch 29: 1.0868312120437622
[08/28/2025 10:17:31 INFO]: Training loss at epoch 48: 1.009211540222168
[08/28/2025 10:17:47 INFO]: Training stats: {
    "score": -1.0053508715154167,
    "rmse": 1.0053508715154167
}
[08/28/2025 10:17:47 INFO]: Val stats: {
    "score": -0.7781328568194852,
    "rmse": 0.7781328568194852
}
[08/28/2025 10:17:47 INFO]: Test stats: {
    "score": -0.9406689762157425,
    "rmse": 0.9406689762157425
}
[08/28/2025 10:17:48 INFO]: Training loss at epoch 20: 1.028588205575943
[08/28/2025 10:17:49 INFO]: Training loss at epoch 5: 1.165206253528595
[08/28/2025 10:17:53 INFO]: Training stats: {
    "score": -0.9962084262935259,
    "rmse": 0.9962084262935259
}
[08/28/2025 10:17:53 INFO]: Val stats: {
    "score": -0.6797843776576674,
    "rmse": 0.6797843776576674
}
[08/28/2025 10:17:53 INFO]: Test stats: {
    "score": -0.8835661271284276,
    "rmse": 0.8835661271284276
}
[08/28/2025 10:17:56 INFO]: Training loss at epoch 4: 0.9400652945041656
[08/28/2025 10:18:01 INFO]: Training loss at epoch 9: 0.8705176711082458
[08/28/2025 10:18:02 INFO]: Training loss at epoch 54: 0.8272379040718079
[08/28/2025 10:18:04 INFO]: Training loss at epoch 41: 0.9953925609588623
[08/28/2025 10:18:07 INFO]: Training loss at epoch 46: 1.1942267715930939
[08/28/2025 10:18:09 INFO]: New best epoch, val score: -0.7093051933350154
[08/28/2025 10:18:09 INFO]: Saving model to: blotchy-Amado_trial_140/model_best.pth
[08/28/2025 10:18:39 INFO]: Training stats: {
    "score": -1.0018309784572879,
    "rmse": 1.0018309784572879
}
[08/28/2025 10:18:39 INFO]: Val stats: {
    "score": -0.6669963473587642,
    "rmse": 0.6669963473587642
}
[08/28/2025 10:18:39 INFO]: Test stats: {
    "score": -0.8718812418419261,
    "rmse": 0.8718812418419261
}
[08/28/2025 10:18:53 INFO]: Training loss at epoch 73: 0.9834660887718201
[08/28/2025 10:18:58 INFO]: Training loss at epoch 39: 1.0667842626571655
[08/28/2025 10:19:01 INFO]: Training loss at epoch 9: 1.0098598301410675
[08/28/2025 10:19:13 INFO]: Training loss at epoch 21: 0.8855681121349335
[08/28/2025 10:19:21 INFO]: Training loss at epoch 30: 1.0155977606773376
[08/28/2025 10:19:22 INFO]: Training loss at epoch 19: 0.8960121273994446
[08/28/2025 10:19:44 INFO]: Training loss at epoch 6: 0.8916947841644287
[08/28/2025 10:19:47 INFO]: Training loss at epoch 5: 0.876510500907898
[08/28/2025 10:19:54 INFO]: Training loss at epoch 3: 1.463174819946289
[08/28/2025 10:20:27 INFO]: Training loss at epoch 10: 1.2291979491710663
[08/28/2025 10:20:27 INFO]: Training stats: {
    "score": -0.9544293066192587,
    "rmse": 0.9544293066192587
}
[08/28/2025 10:20:27 INFO]: Val stats: {
    "score": -0.6788483206869872,
    "rmse": 0.6788483206869872
}
[08/28/2025 10:20:27 INFO]: Test stats: {
    "score": -0.8929587489520876,
    "rmse": 0.8929587489520876
}
[08/28/2025 10:20:31 INFO]: Training loss at epoch 19: 0.9887942671775818
[08/28/2025 10:20:38 INFO]: Training loss at epoch 22: 0.7594694495201111
[08/28/2025 10:20:44 INFO]: Training loss at epoch 55: 1.0631388425827026
[08/28/2025 10:20:49 INFO]: Training loss at epoch 31: 0.8631699979305267
[08/28/2025 10:20:51 INFO]: Training loss at epoch 74: 1.134072184562683
[08/28/2025 10:21:00 INFO]: Running Final Evaluation...
[08/28/2025 10:21:22 INFO]: Training stats: {
    "score": -0.9704503709143159,
    "rmse": 0.9704503709143159
}
[08/28/2025 10:21:22 INFO]: Val stats: {
    "score": -0.7034280467841874,
    "rmse": 0.7034280467841874
}
[08/28/2025 10:21:22 INFO]: Test stats: {
    "score": -0.9194731969690192,
    "rmse": 0.9194731969690192
}
[08/28/2025 10:21:23 INFO]: Training loss at epoch 9: 1.1526637077331543
[08/28/2025 10:21:39 INFO]: Training loss at epoch 47: 1.0365337133407593
[08/28/2025 10:21:42 INFO]: Training loss at epoch 6: 1.3710774183273315
[08/28/2025 10:21:43 INFO]: Training loss at epoch 7: 1.1847485303878784
[08/28/2025 10:21:43 INFO]: Training stats: {
    "score": -1.03496431866723,
    "rmse": 1.03496431866723
}
[08/28/2025 10:21:43 INFO]: Val stats: {
    "score": -0.8480573829003153,
    "rmse": 0.8480573829003153
}
[08/28/2025 10:21:43 INFO]: Test stats: {
    "score": -1.0018306133813546,
    "rmse": 1.0018306133813546
}
[08/28/2025 10:21:45 INFO]: Training loss at epoch 50: 0.9966963529586792
[08/28/2025 10:22:06 INFO]: Training loss at epoch 23: 0.9509534239768982
[08/28/2025 10:22:16 INFO]: Training loss at epoch 11: 0.8840214610099792
[08/28/2025 10:22:18 INFO]: Training loss at epoch 32: 1.1067107915878296
[08/28/2025 10:22:32 INFO]: Training loss at epoch 42: 0.8293014168739319
[08/28/2025 10:22:37 INFO]: Training accuracy: {
    "score": -0.9910913424342668,
    "rmse": 0.9910913424342668
}
[08/28/2025 10:22:37 INFO]: Val accuracy: {
    "score": -0.6609026866434358,
    "rmse": 0.6609026866434358
}
[08/28/2025 10:22:37 INFO]: Test accuracy: {
    "score": -0.8859544871432851,
    "rmse": 0.8859544871432851
}
[08/28/2025 10:22:37 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_118",
    "best_epoch": 8,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8859544871432851,
        "rmse": 0.8859544871432851
    },
    "train_stats": {
        "score": -0.9910913424342668,
        "rmse": 0.9910913424342668
    },
    "val_stats": {
        "score": -0.6609026866434358,
        "rmse": 0.6609026866434358
    }
}
[08/28/2025 10:22:37 INFO]: Procewss finished for trial blotchy-Amado_trial_118
[08/28/2025 10:22:38 INFO]: 
_________________________________________________

[08/28/2025 10:22:38 INFO]: train_net_for_optune.py main() running.
[08/28/2025 10:22:38 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 2.0142793578874802
  attention_dropout: 0.3253642764972958
  ffn_dropout: 0.3253642764972958
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.781402056298317e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_141

[08/28/2025 10:22:38 INFO]: This ft_transformer has 1.038 million parameters.
[08/28/2025 10:22:38 INFO]: Training will start at epoch 0.
[08/28/2025 10:22:38 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 10:22:47 INFO]: Training loss at epoch 75: 1.0552458763122559
[08/28/2025 10:23:28 INFO]: Training loss at epoch 56: 1.042232483625412
[08/28/2025 10:23:31 INFO]: Training loss at epoch 24: 1.002625733613968
[08/28/2025 10:23:34 INFO]: Training loss at epoch 7: 1.1774796843528748
[08/28/2025 10:23:38 INFO]: Training loss at epoch 8: 0.8186875879764557
[08/28/2025 10:23:45 INFO]: Training stats: {
    "score": -0.9765242033123476,
    "rmse": 0.9765242033123476
}
[08/28/2025 10:23:45 INFO]: Val stats: {
    "score": -0.6891561473717335,
    "rmse": 0.6891561473717335
}
[08/28/2025 10:23:45 INFO]: Test stats: {
    "score": -0.8826716152125216,
    "rmse": 0.8826716152125216
}
[08/28/2025 10:23:45 INFO]: Training loss at epoch 33: 1.216805875301361
[08/28/2025 10:24:00 INFO]: Training stats: {
    "score": -1.07961121910174,
    "rmse": 1.07961121910174
}
[08/28/2025 10:24:00 INFO]: Val stats: {
    "score": -0.7252063474037391,
    "rmse": 0.7252063474037391
}
[08/28/2025 10:24:00 INFO]: Test stats: {
    "score": -0.9366913325697231,
    "rmse": 0.9366913325697231
}
[08/28/2025 10:24:02 INFO]: Training loss at epoch 12: 0.8863719999790192
[08/28/2025 10:24:19 INFO]: Training loss at epoch 0: 1.349483072757721
[08/28/2025 10:24:34 INFO]: Training loss at epoch 20: 0.9361468255519867
[08/28/2025 10:24:35 INFO]: New best epoch, val score: -0.8027574229203472
[08/28/2025 10:24:35 INFO]: Saving model to: blotchy-Amado_trial_141/model_best.pth
[08/28/2025 10:24:45 INFO]: Training loss at epoch 76: 0.8890797793865204
[08/28/2025 10:24:48 INFO]: Training loss at epoch 17: 1.0868624448776245
[08/28/2025 10:24:57 INFO]: Training loss at epoch 25: 1.379231721162796
[08/28/2025 10:25:10 INFO]: Training loss at epoch 48: 0.8760777413845062
[08/28/2025 10:25:14 INFO]: Training loss at epoch 34: 0.8792120218276978
[08/28/2025 10:25:28 INFO]: Training loss at epoch 8: 0.9463117122650146
[08/28/2025 10:25:34 INFO]: Training loss at epoch 9: 1.4119795560836792
[08/28/2025 10:25:43 INFO]: Training loss at epoch 49: 1.022755891084671
[08/28/2025 10:25:52 INFO]: Training loss at epoch 13: 1.3603995740413666
[08/28/2025 10:26:13 INFO]: Training loss at epoch 57: 0.9874446392059326
[08/28/2025 10:26:16 INFO]: Training stats: {
    "score": -0.9998621590841202,
    "rmse": 0.9998621590841202
}
[08/28/2025 10:26:16 INFO]: Val stats: {
    "score": -0.6921977363841617,
    "rmse": 0.6921977363841617
}
[08/28/2025 10:26:16 INFO]: Test stats: {
    "score": -0.8841484348969982,
    "rmse": 0.8841484348969982
}
[08/28/2025 10:26:16 INFO]: Training loss at epoch 1: 1.0541549623012543
[08/28/2025 10:26:25 INFO]: Training loss at epoch 26: 0.9252054691314697
[08/28/2025 10:26:30 INFO]: Training loss at epoch 4: 0.8846961259841919
[08/28/2025 10:26:31 INFO]: New best epoch, val score: -0.683062306358239
[08/28/2025 10:26:31 INFO]: Saving model to: blotchy-Amado_trial_141/model_best.pth
[08/28/2025 10:26:41 INFO]: Training loss at epoch 35: 0.8848080337047577
[08/28/2025 10:26:43 INFO]: Training loss at epoch 77: 0.9233393967151642
[08/28/2025 10:27:02 INFO]: Training loss at epoch 43: 0.8873153626918793
[08/28/2025 10:27:09 INFO]: Training loss at epoch 51: 0.8385712802410126
[08/28/2025 10:27:22 INFO]: Training loss at epoch 9: 0.9421418309211731
[08/28/2025 10:27:41 INFO]: Training loss at epoch 14: 0.9364030063152313
[08/28/2025 10:27:43 INFO]: Training loss at epoch 31: 0.8632285594940186
[08/28/2025 10:27:54 INFO]: Training loss at epoch 27: 0.9011543095111847
[08/28/2025 10:28:04 INFO]: Training stats: {
    "score": -1.0050023401421915,
    "rmse": 1.0050023401421915
}
[08/28/2025 10:28:04 INFO]: Val stats: {
    "score": -0.7151096037792679,
    "rmse": 0.7151096037792679
}
[08/28/2025 10:28:04 INFO]: Test stats: {
    "score": -0.8938212643017707,
    "rmse": 0.8938212643017707
}
[08/28/2025 10:28:07 INFO]: Training loss at epoch 10: 0.8593933880329132
[08/28/2025 10:28:12 INFO]: Training loss at epoch 36: 1.07216477394104
[08/28/2025 10:28:13 INFO]: Training loss at epoch 2: 1.0081358551979065
[08/28/2025 10:28:13 INFO]: Training loss at epoch 10: 0.9180335700511932
[08/28/2025 10:28:27 INFO]: Training loss at epoch 20: 0.9119076132774353
[08/28/2025 10:28:27 INFO]: New best epoch, val score: -0.6568657769391383
[08/28/2025 10:28:27 INFO]: Saving model to: blotchy-Amado_trial_141/model_best.pth
[08/28/2025 10:28:32 INFO]: Training stats: {
    "score": -0.9425263761209137,
    "rmse": 0.9425263761209137
}
[08/28/2025 10:28:32 INFO]: Val stats: {
    "score": -0.6750543569514925,
    "rmse": 0.6750543569514925
}
[08/28/2025 10:28:32 INFO]: Test stats: {
    "score": -0.8866593511216273,
    "rmse": 0.8866593511216273
}
[08/28/2025 10:28:43 INFO]: Training loss at epoch 78: 1.066240906715393
[08/28/2025 10:28:47 INFO]: Training loss at epoch 49: 0.999602735042572
[08/28/2025 10:29:00 INFO]: Training loss at epoch 58: 1.038931131362915
[08/28/2025 10:29:22 INFO]: Training loss at epoch 28: 0.9652627110481262
[08/28/2025 10:29:30 INFO]: Training loss at epoch 15: 0.8564009964466095
[08/28/2025 10:29:40 INFO]: Training loss at epoch 37: 0.946928858757019
[08/28/2025 10:29:58 INFO]: Training loss at epoch 10: 1.2306731343269348
[08/28/2025 10:30:03 INFO]: Training stats: {
    "score": -0.9861768999273209,
    "rmse": 0.9861768999273209
}
[08/28/2025 10:30:03 INFO]: Val stats: {
    "score": -0.7085681317138798,
    "rmse": 0.7085681317138798
}
[08/28/2025 10:30:03 INFO]: Test stats: {
    "score": -0.8871238815058488,
    "rmse": 0.8871238815058488
}
[08/28/2025 10:30:08 INFO]: Training loss at epoch 3: 0.9501252174377441
[08/28/2025 10:30:10 INFO]: Training loss at epoch 11: 0.8642989695072174
[08/28/2025 10:30:29 INFO]: Running Final Evaluation...
[08/28/2025 10:30:40 INFO]: Training loss at epoch 79: 0.8555466532707214
[08/28/2025 10:30:50 INFO]: Training loss at epoch 29: 0.8366619348526001
[08/28/2025 10:31:06 INFO]: Training loss at epoch 38: 1.2410818338394165
[08/28/2025 10:31:19 INFO]: Training loss at epoch 16: 1.0515440106391907
[08/28/2025 10:31:20 INFO]: Training stats: {
    "score": -0.9646665200528984,
    "rmse": 0.9646665200528984
}
[08/28/2025 10:31:20 INFO]: Val stats: {
    "score": -0.6951467486952106,
    "rmse": 0.6951467486952106
}
[08/28/2025 10:31:20 INFO]: Test stats: {
    "score": -0.8795105675184577,
    "rmse": 0.8795105675184577
}
[08/28/2025 10:31:22 INFO]: Training stats: {
    "score": -0.9931390916996703,
    "rmse": 0.9931390916996703
}
[08/28/2025 10:31:22 INFO]: Val stats: {
    "score": -0.6851896677886946,
    "rmse": 0.6851896677886946
}
[08/28/2025 10:31:22 INFO]: Test stats: {
    "score": -0.8799163583745279,
    "rmse": 0.8799163583745279
}
[08/28/2025 10:31:23 INFO]: Training loss at epoch 21: 0.9475277066230774
[08/28/2025 10:31:29 INFO]: Training loss at epoch 44: 0.9895716607570648
[08/28/2025 10:31:40 INFO]: Training loss at epoch 59: 0.8797206282615662
[08/28/2025 10:31:45 INFO]: Training accuracy: {
    "score": -1.0079859694768385,
    "rmse": 1.0079859694768385
}
[08/28/2025 10:31:45 INFO]: Val accuracy: {
    "score": -0.6795457782850335,
    "rmse": 0.6795457782850335
}
[08/28/2025 10:31:45 INFO]: Test accuracy: {
    "score": -0.8764407582648888,
    "rmse": 0.8764407582648888
}
[08/28/2025 10:31:45 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_120",
    "best_epoch": 18,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8764407582648888,
        "rmse": 0.8764407582648888
    },
    "train_stats": {
        "score": -1.0079859694768385,
        "rmse": 1.0079859694768385
    },
    "val_stats": {
        "score": -0.6795457782850335,
        "rmse": 0.6795457782850335
    }
}
[08/28/2025 10:31:45 INFO]: Procewss finished for trial blotchy-Amado_trial_120
[08/28/2025 10:31:45 INFO]: 
_________________________________________________

[08/28/2025 10:31:45 INFO]: train_net_for_optune.py main() running.
[08/28/2025 10:31:45 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 8
  d_ffn_factor: 1.5112560929330399
  attention_dropout: 0.011623369763338436
  ffn_dropout: 0.011623369763338436
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.163768581925945e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_142

[08/28/2025 10:31:45 INFO]: This ft_transformer has 0.890 million parameters.
[08/28/2025 10:31:45 INFO]: Training will start at epoch 0.
[08/28/2025 10:31:45 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 10:31:48 INFO]: Training loss at epoch 11: 1.0582497715950012
[08/28/2025 10:31:49 INFO]: Training loss at epoch 10: 1.128997266292572
[08/28/2025 10:32:00 INFO]: Training loss at epoch 4: 1.2781232297420502
[08/28/2025 10:32:03 INFO]: Running Final Evaluation...
[08/28/2025 10:32:05 INFO]: Training loss at epoch 12: 0.9496090710163116
[08/28/2025 10:32:31 INFO]: Training loss at epoch 52: 0.8839707374572754
[08/28/2025 10:32:34 INFO]: Training loss at epoch 39: 0.9582735002040863
[08/28/2025 10:32:38 INFO]: Training loss at epoch 18: 0.8040867745876312
[08/28/2025 10:32:40 INFO]: Training stats: {
    "score": -0.9963930314959403,
    "rmse": 0.9963930314959403
}
[08/28/2025 10:32:40 INFO]: Val stats: {
    "score": -0.722518016518702,
    "rmse": 0.722518016518702
}
[08/28/2025 10:32:40 INFO]: Test stats: {
    "score": -0.9003581026134031,
    "rmse": 0.9003581026134031
}
[08/28/2025 10:32:46 INFO]: New best epoch, val score: -0.6742011894070502
[08/28/2025 10:32:46 INFO]: Saving model to: blotchy-Amado_trial_130/model_best.pth
[08/28/2025 10:32:50 INFO]: Training loss at epoch 0: 0.9280618131160736
[08/28/2025 10:32:51 INFO]: Training loss at epoch 30: 0.9515634179115295
[08/28/2025 10:32:59 INFO]: New best epoch, val score: -0.686133723622155
[08/28/2025 10:32:59 INFO]: Saving model to: blotchy-Amado_trial_142/model_best.pth
[08/28/2025 10:33:05 INFO]: Training loss at epoch 5: 1.506580412387848
[08/28/2025 10:33:08 INFO]: Training loss at epoch 17: 0.9542977809906006
[08/28/2025 10:33:08 INFO]: Training stats: {
    "score": -0.997539186881083,
    "rmse": 0.997539186881083
}
[08/28/2025 10:33:08 INFO]: Val stats: {
    "score": -0.6647375156187373,
    "rmse": 0.6647375156187373
}
[08/28/2025 10:33:08 INFO]: Test stats: {
    "score": -0.8772334244730658,
    "rmse": 0.8772334244730658
}
[08/28/2025 10:33:13 INFO]: Training loss at epoch 20: 1.0553749799728394
[08/28/2025 10:33:18 INFO]: Training loss at epoch 80: 0.864615797996521
[08/28/2025 10:33:44 INFO]: Training loss at epoch 12: 1.1143959164619446
[08/28/2025 10:33:51 INFO]: Training accuracy: {
    "score": -0.9996427999126913,
    "rmse": 0.9996427999126913
}
[08/28/2025 10:33:51 INFO]: Val accuracy: {
    "score": -0.6449409170704297,
    "rmse": 0.6449409170704297
}
[08/28/2025 10:33:51 INFO]: Test accuracy: {
    "score": -0.8678129638908865,
    "rmse": 0.8678129638908865
}
[08/28/2025 10:33:51 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_116",
    "best_epoch": 13,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8678129638908865,
        "rmse": 0.8678129638908865
    },
    "train_stats": {
        "score": -0.9996427999126913,
        "rmse": 0.9996427999126913
    },
    "val_stats": {
        "score": -0.6449409170704297,
        "rmse": 0.6449409170704297
    }
}
[08/28/2025 10:33:51 INFO]: Procewss finished for trial blotchy-Amado_trial_116
[08/28/2025 10:33:51 INFO]: 
_________________________________________________

[08/28/2025 10:33:51 INFO]: train_net_for_optune.py main() running.
[08/28/2025 10:33:52 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 1.5096384612249543
  attention_dropout: 0.06779001933726654
  ffn_dropout: 0.06779001933726654
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.702094490093416e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_143

[08/28/2025 10:33:52 INFO]: This ft_transformer has 13.613 million parameters.
[08/28/2025 10:33:52 INFO]: Training will start at epoch 0.
[08/28/2025 10:33:52 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 10:33:55 INFO]: Training loss at epoch 5: 0.9072853326797485
[08/28/2025 10:34:02 INFO]: Training loss at epoch 13: 0.923831045627594
[08/28/2025 10:34:02 INFO]: Training loss at epoch 1: 0.9046284854412079
[08/28/2025 10:34:11 INFO]: New best epoch, val score: -0.6694317238986206
[08/28/2025 10:34:11 INFO]: Saving model to: blotchy-Amado_trial_142/model_best.pth
[08/28/2025 10:34:20 INFO]: Training loss at epoch 31: 1.0771790444850922
[08/28/2025 10:34:32 INFO]: Running Final Evaluation...
[08/28/2025 10:34:36 INFO]: Training loss at epoch 40: 1.03295236825943
[08/28/2025 10:34:51 INFO]: Training loss at epoch 11: 1.0514625906944275
[08/28/2025 10:34:58 INFO]: Training loss at epoch 18: 0.8065342903137207
[08/28/2025 10:35:07 INFO]: Training accuracy: {
    "score": -1.014977531367724,
    "rmse": 1.014977531367724
}
[08/28/2025 10:35:07 INFO]: Val accuracy: {
    "score": -0.6601826614794197,
    "rmse": 0.6601826614794197
}
[08/28/2025 10:35:07 INFO]: Test accuracy: {
    "score": -0.8766082708453536,
    "rmse": 0.8766082708453536
}
[08/28/2025 10:35:07 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_136",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8766082708453536,
        "rmse": 0.8766082708453536
    },
    "train_stats": {
        "score": -1.014977531367724,
        "rmse": 1.014977531367724
    },
    "val_stats": {
        "score": -0.6601826614794197,
        "rmse": 0.6601826614794197
    }
}
[08/28/2025 10:35:07 INFO]: Procewss finished for trial blotchy-Amado_trial_136
[08/28/2025 10:35:08 INFO]: 
_________________________________________________

[08/28/2025 10:35:08 INFO]: train_net_for_optune.py main() running.
[08/28/2025 10:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 2.013539859114247
  attention_dropout: 0.06989369310015348
  ffn_dropout: 0.06989369310015348
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.374227086963518e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_144

[08/28/2025 10:35:08 INFO]: This ft_transformer has 15.994 million parameters.
[08/28/2025 10:35:08 INFO]: Training will start at epoch 0.
[08/28/2025 10:35:08 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 10:35:11 INFO]: Training loss at epoch 21: 0.833951473236084
[08/28/2025 10:35:14 INFO]: Training loss at epoch 2: 1.0210798382759094
[08/28/2025 10:35:16 INFO]: Training loss at epoch 81: 0.8527371287345886
[08/28/2025 10:35:25 INFO]: Training loss at epoch 60: 0.8131445944309235
[08/28/2025 10:35:36 INFO]: Training loss at epoch 13: 0.9695570468902588
[08/28/2025 10:35:49 INFO]: Training loss at epoch 6: 0.9230869114398956
[08/28/2025 10:35:59 INFO]: Training loss at epoch 14: 0.8587401807308197
[08/28/2025 10:36:05 INFO]: Training loss at epoch 41: 0.8643269240856171
[08/28/2025 10:36:17 INFO]: Running Final Evaluation...
[08/28/2025 10:36:26 INFO]: Training loss at epoch 3: 0.9137499034404755
[08/28/2025 10:36:43 INFO]: Training loss at epoch 50: 1.133434921503067
[08/28/2025 10:36:47 INFO]: Training loss at epoch 19: 0.8918852508068085
[08/28/2025 10:36:52 INFO]: Training accuracy: {
    "score": -1.0100693054958805,
    "rmse": 1.0100693054958805
}
[08/28/2025 10:36:52 INFO]: Val accuracy: {
    "score": -0.6591173131631693,
    "rmse": 0.6591173131631693
}
[08/28/2025 10:36:52 INFO]: Test accuracy: {
    "score": -0.8779135925890325,
    "rmse": 0.8779135925890325
}
[08/28/2025 10:36:52 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_135",
    "best_epoch": 10,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8779135925890325,
        "rmse": 0.8779135925890325
    },
    "train_stats": {
        "score": -1.0100693054958805,
        "rmse": 1.0100693054958805
    },
    "val_stats": {
        "score": -0.6591173131631693,
        "rmse": 0.6591173131631693
    }
}
[08/28/2025 10:36:52 INFO]: Procewss finished for trial blotchy-Amado_trial_135
[08/28/2025 10:36:52 INFO]: 
_________________________________________________

[08/28/2025 10:36:52 INFO]: train_net_for_optune.py main() running.
[08/28/2025 10:36:52 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 8
  d_ffn_factor: 2.0269636254701284
  attention_dropout: 0.1848794850331161
  ffn_dropout: 0.1848794850331161
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.460535881978881e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_145

[08/28/2025 10:36:53 INFO]: This ft_transformer has 16.059 million parameters.
[08/28/2025 10:36:53 INFO]: Training will start at epoch 0.
[08/28/2025 10:36:53 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 10:37:13 INFO]: Training loss at epoch 82: 1.0107846856117249
[08/28/2025 10:37:28 INFO]: Training stats: {
    "score": -0.9969677782079237,
    "rmse": 0.9969677782079237
}
[08/28/2025 10:37:28 INFO]: Val stats: {
    "score": -0.6930932171397051,
    "rmse": 0.6930932171397051
}
[08/28/2025 10:37:28 INFO]: Test stats: {
    "score": -0.8823744654795085,
    "rmse": 0.8823744654795085
}
[08/28/2025 10:37:28 INFO]: Running Final Evaluation...
[08/28/2025 10:37:30 INFO]: Training loss at epoch 14: 0.9740501046180725
[08/28/2025 10:37:37 INFO]: Training loss at epoch 4: 0.9798004627227783
[08/28/2025 10:37:42 INFO]: Training loss at epoch 7: 0.8843131065368652
[08/28/2025 10:37:54 INFO]: Training loss at epoch 15: 0.8974646031856537
[08/28/2025 10:37:56 INFO]: Training loss at epoch 53: 0.689860612154007
[08/28/2025 10:38:07 INFO]: Training loss at epoch 61: 1.158566027879715
[08/28/2025 10:38:11 INFO]: Training loss at epoch 22: 0.869124561548233
[08/28/2025 10:38:14 INFO]: Training accuracy: {
    "score": -0.992933954440645,
    "rmse": 0.992933954440645
}
[08/28/2025 10:38:14 INFO]: Val accuracy: {
    "score": -0.673182999774531,
    "rmse": 0.673182999774531
}
[08/28/2025 10:38:14 INFO]: Test accuracy: {
    "score": -0.8762287031115048,
    "rmse": 0.8762287031115048
}
[08/28/2025 10:38:14 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_125",
    "best_epoch": 51,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8762287031115048,
        "rmse": 0.8762287031115048
    },
    "train_stats": {
        "score": -0.992933954440645,
        "rmse": 0.992933954440645
    },
    "val_stats": {
        "score": -0.673182999774531,
        "rmse": 0.673182999774531
    }
}
[08/28/2025 10:38:14 INFO]: Procewss finished for trial blotchy-Amado_trial_125
[08/28/2025 10:38:14 INFO]: 
_________________________________________________

[08/28/2025 10:38:14 INFO]: train_net_for_optune.py main() running.
[08/28/2025 10:38:14 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 8
  d_ffn_factor: 2.0159812445263765
  attention_dropout: 0.3310188281112115
  ffn_dropout: 0.3310188281112115
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.1791685512160065e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_146

[08/28/2025 10:38:14 INFO]: This ft_transformer has 16.012 million parameters.
[08/28/2025 10:38:14 INFO]: Training will start at epoch 0.
[08/28/2025 10:38:14 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 10:38:46 INFO]: Training loss at epoch 5: 1.0516015589237213
[08/28/2025 10:39:14 INFO]: Training loss at epoch 20: 1.072151631116867
[08/28/2025 10:39:23 INFO]: Training loss at epoch 15: 1.0763533115386963
[08/28/2025 10:39:34 INFO]: Training loss at epoch 8: 1.1637466549873352
[08/28/2025 10:39:35 INFO]: Training loss at epoch 6: 1.7622889280319214
[08/28/2025 10:39:38 INFO]: Training loss at epoch 11: 0.9470173418521881
[08/28/2025 10:39:50 INFO]: Training loss at epoch 32: 0.962857186794281
[08/28/2025 10:39:51 INFO]: Training loss at epoch 16: 1.1380495429039001
[08/28/2025 10:39:59 INFO]: Training loss at epoch 6: 0.8450722992420197
[08/28/2025 10:40:00 INFO]: Training loss at epoch 0: 1.4720426201820374
[08/28/2025 10:40:29 INFO]: Training loss at epoch 19: 0.9660818874835968
[08/28/2025 10:40:49 INFO]: New best epoch, val score: -0.710586839344404
[08/28/2025 10:40:49 INFO]: Saving model to: blotchy-Amado_trial_143/model_best.pth
[08/28/2025 10:40:51 INFO]: Training loss at epoch 62: 0.9156847894191742
[08/28/2025 10:41:02 INFO]: Training loss at epoch 21: 1.17409285902977
[08/28/2025 10:41:10 INFO]: Training loss at epoch 7: 0.9419313669204712
[08/28/2025 10:41:16 INFO]: Training loss at epoch 16: 1.2005354166030884
[08/28/2025 10:41:28 INFO]: Training loss at epoch 9: 0.9955185949802399
[08/28/2025 10:41:33 INFO]: Training loss at epoch 12: 0.9189792573451996
[08/28/2025 10:41:47 INFO]: Training loss at epoch 17: 0.9229166507720947
[08/28/2025 10:41:54 INFO]: Training loss at epoch 22: 0.8931994438171387
[08/28/2025 10:42:04 INFO]: Training loss at epoch 0: 1.0701725482940674
[08/28/2025 10:42:10 INFO]: Training stats: {
    "score": -0.9899435425721973,
    "rmse": 0.9899435425721973
}
[08/28/2025 10:42:10 INFO]: Val stats: {
    "score": -0.6685711982915238,
    "rmse": 0.6685711982915238
}
[08/28/2025 10:42:10 INFO]: Test stats: {
    "score": -0.8735484335309468,
    "rmse": 0.8735484335309468
}
[08/28/2025 10:42:23 INFO]: Training loss at epoch 8: 0.853903204202652
[08/28/2025 10:42:36 INFO]: Training loss at epoch 21: 1.1191904842853546
[08/28/2025 10:42:52 INFO]: Training loss at epoch 22: 0.8396783471107483
[08/28/2025 10:43:00 INFO]: New best epoch, val score: -0.683037609043042
[08/28/2025 10:43:00 INFO]: Saving model to: blotchy-Amado_trial_144/model_best.pth
[08/28/2025 10:43:11 INFO]: Training stats: {
    "score": -0.9229993078479996,
    "rmse": 0.9229993078479996
}
[08/28/2025 10:43:11 INFO]: Val stats: {
    "score": -0.682658693859206,
    "rmse": 0.682658693859206
}
[08/28/2025 10:43:11 INFO]: Test stats: {
    "score": -0.9095042852569617,
    "rmse": 0.9095042852569617
}
[08/28/2025 10:43:11 INFO]: Training loss at epoch 17: 0.9390825629234314
[08/28/2025 10:43:19 INFO]: Training loss at epoch 0: 1.373992919921875
[08/28/2025 10:43:21 INFO]: Training loss at epoch 54: 0.7731569409370422
[08/28/2025 10:43:36 INFO]: Training loss at epoch 9: 1.1505607068538666
[08/28/2025 10:43:36 INFO]: Training loss at epoch 63: 1.1220392882823944
[08/28/2025 10:43:44 INFO]: Training loss at epoch 18: 1.2899870872497559
[08/28/2025 10:44:02 INFO]: Training stats: {
    "score": -0.9926928302314175,
    "rmse": 0.9926928302314175
}
[08/28/2025 10:44:02 INFO]: Val stats: {
    "score": -0.6906215873812265,
    "rmse": 0.6906215873812265
}
[08/28/2025 10:44:02 INFO]: Test stats: {
    "score": -0.8799916462296673,
    "rmse": 0.8799916462296673
}
[08/28/2025 10:44:04 INFO]: Training loss at epoch 10: 0.9798062741756439
[08/28/2025 10:44:12 INFO]: New best epoch, val score: -0.7632341180742522
[08/28/2025 10:44:12 INFO]: Saving model to: blotchy-Amado_trial_145/model_best.pth
[08/28/2025 10:44:20 INFO]: New best epoch, val score: -0.6550697220158165
[08/28/2025 10:44:20 INFO]: Saving model to: blotchy-Amado_trial_141/model_best.pth
[08/28/2025 10:44:41 INFO]: Training loss at epoch 0: 1.2819103002548218
[08/28/2025 10:44:41 INFO]: Training loss at epoch 23: 1.143583983182907
[08/28/2025 10:44:59 INFO]: Training loss at epoch 51: 0.9370379149913788
[08/28/2025 10:45:04 INFO]: Training loss at epoch 23: 1.055577039718628
[08/28/2025 10:45:07 INFO]: Training loss at epoch 18: 1.1450976729393005
[08/28/2025 10:45:16 INFO]: Training loss at epoch 10: 0.7666912376880646
[08/28/2025 10:45:36 INFO]: New best epoch, val score: -0.8575954473055492
[08/28/2025 10:45:36 INFO]: Saving model to: blotchy-Amado_trial_146/model_best.pth
[08/28/2025 10:45:45 INFO]: Training loss at epoch 19: 1.0269598960876465
[08/28/2025 10:45:55 INFO]: New best epoch, val score: -0.6689010644311504
[08/28/2025 10:45:55 INFO]: Saving model to: blotchy-Amado_trial_127/model_best.pth
[08/28/2025 10:46:02 INFO]: Training loss at epoch 11: 1.1389861106872559
[08/28/2025 10:46:15 INFO]: Training loss at epoch 7: 1.5718470811843872
[08/28/2025 10:46:17 INFO]: New best epoch, val score: -0.6501756726563553
[08/28/2025 10:46:17 INFO]: Saving model to: blotchy-Amado_trial_141/model_best.pth
[08/28/2025 10:46:24 INFO]: Training loss at epoch 64: 1.1723617911338806
[08/28/2025 10:46:26 INFO]: Training stats: {
    "score": -0.9962832877369547,
    "rmse": 0.9962832877369547
}
[08/28/2025 10:46:26 INFO]: Val stats: {
    "score": -0.6810511528678983,
    "rmse": 0.6810511528678983
}
[08/28/2025 10:46:26 INFO]: Test stats: {
    "score": -0.8794684045494635,
    "rmse": 0.8794684045494635
}
[08/28/2025 10:46:28 INFO]: Training loss at epoch 11: 1.1028116047382355
[08/28/2025 10:46:32 INFO]: Training loss at epoch 24: 0.8669396042823792
[08/28/2025 10:47:02 INFO]: Training loss at epoch 19: 0.9301434457302094
[08/28/2025 10:47:06 INFO]: Training loss at epoch 1: 1.3008034229278564
[08/28/2025 10:47:32 INFO]: Training loss at epoch 12: 1.2957451045513153
[08/28/2025 10:47:40 INFO]: Training loss at epoch 12: 1.1128993332386017
[08/28/2025 10:47:42 INFO]: Training stats: {
    "score": -1.0133488520868354,
    "rmse": 1.0133488520868354
}
[08/28/2025 10:47:42 INFO]: Val stats: {
    "score": -0.7493901032500323,
    "rmse": 0.7493901032500323
}
[08/28/2025 10:47:42 INFO]: Test stats: {
    "score": -0.917532168624939,
    "rmse": 0.917532168624939
}
[08/28/2025 10:47:55 INFO]: Training loss at epoch 12: 1.0538498759269714
[08/28/2025 10:48:21 INFO]: Training loss at epoch 25: 0.9277629852294922
[08/28/2025 10:48:24 INFO]: Training loss at epoch 13: 0.9141097664833069
[08/28/2025 10:48:24 INFO]: Training loss at epoch 20: 0.9537628591060638
[08/28/2025 10:48:42 INFO]: Training loss at epoch 23: 0.8387480080127716
[08/28/2025 10:48:50 INFO]: Training loss at epoch 55: 0.83017697930336
[08/28/2025 10:48:52 INFO]: Training loss at epoch 13: 1.0566769540309906
[08/28/2025 10:49:08 INFO]: Training loss at epoch 65: 1.0288728177547455
[08/28/2025 10:49:36 INFO]: Training loss at epoch 20: 0.876475602388382
[08/28/2025 10:49:50 INFO]: Training loss at epoch 13: 1.1330101490020752
[08/28/2025 10:50:01 INFO]: Training loss at epoch 1: 1.1577759981155396
[08/28/2025 10:50:02 INFO]: Training loss at epoch 14: 0.8754958808422089
[08/28/2025 10:50:08 INFO]: Training loss at epoch 26: 1.1373504996299744
[08/28/2025 10:50:17 INFO]: Training loss at epoch 21: 0.8283880650997162
[08/28/2025 10:50:31 INFO]: New best epoch, val score: -0.6714488546568789
[08/28/2025 10:50:31 INFO]: Saving model to: blotchy-Amado_trial_139/model_best.pth
[08/28/2025 10:50:40 INFO]: Training loss at epoch 1: 1.4703200459480286
[08/28/2025 10:51:07 INFO]: Training loss at epoch 20: 0.9155444204807281
[08/28/2025 10:51:14 INFO]: Training loss at epoch 15: 0.9262146055698395
[08/28/2025 10:51:27 INFO]: Training loss at epoch 21: 0.8885649144649506
[08/28/2025 10:51:43 INFO]: Training loss at epoch 14: 0.8552457690238953
[08/28/2025 10:51:50 INFO]: Training loss at epoch 66: 1.0956138670444489
[08/28/2025 10:51:54 INFO]: Training loss at epoch 24: 1.1158669292926788
[08/28/2025 10:51:56 INFO]: Training loss at epoch 27: 1.0995139479637146
[08/28/2025 10:51:58 INFO]: Training loss at epoch 1: 1.5224376916885376
[08/28/2025 10:52:07 INFO]: Training loss at epoch 22: 1.053893268108368
[08/28/2025 10:52:09 INFO]: Training loss at epoch 33: 0.9477637410163879
[08/28/2025 10:52:13 INFO]: Training loss at epoch 22: 1.298989623785019
[08/28/2025 10:52:27 INFO]: Training loss at epoch 16: 1.0813652276992798
[08/28/2025 10:52:48 INFO]: Training loss at epoch 8: 1.2469732165336609
[08/28/2025 10:53:15 INFO]: Training loss at epoch 52: 0.959798276424408
[08/28/2025 10:53:23 INFO]: Training loss at epoch 22: 0.7715472728013992
[08/28/2025 10:53:40 INFO]: Training loss at epoch 15: 0.9543475210666656
[08/28/2025 10:53:40 INFO]: Training loss at epoch 17: 1.0703801214694977
[08/28/2025 10:53:46 INFO]: Training loss at epoch 28: 0.9056355357170105
[08/28/2025 10:54:05 INFO]: Training loss at epoch 2: 1.5461978316307068
[08/28/2025 10:54:10 INFO]: Training loss at epoch 23: 1.2575580477714539
[08/28/2025 10:54:14 INFO]: Training loss at epoch 56: 0.8166279196739197
[08/28/2025 10:54:34 INFO]: Training loss at epoch 67: 0.8517638444900513
[08/28/2025 10:54:53 INFO]: Training loss at epoch 18: 1.025981605052948
[08/28/2025 10:54:55 INFO]: Running Final Evaluation...
[08/28/2025 10:54:59 INFO]: New best epoch, val score: -0.6697230183234582
[08/28/2025 10:54:59 INFO]: Saving model to: blotchy-Amado_trial_143/model_best.pth
[08/28/2025 10:55:02 INFO]: New best epoch, val score: -0.669276652154506
[08/28/2025 10:55:02 INFO]: Saving model to: blotchy-Amado_trial_142/model_best.pth
[08/28/2025 10:55:09 INFO]: Training loss at epoch 14: 0.9992051124572754
[08/28/2025 10:55:20 INFO]: Training loss at epoch 23: 1.1430109143257141
[08/28/2025 10:55:23 INFO]: Training loss at epoch 13: 0.9734798073768616
[08/28/2025 10:55:29 INFO]: Training loss at epoch 24: 0.8663723468780518
[08/28/2025 10:55:37 INFO]: Training loss at epoch 16: 1.0565588474273682
[08/28/2025 10:55:39 INFO]: Training loss at epoch 29: 1.031862199306488
[08/28/2025 10:56:06 INFO]: Training loss at epoch 19: 1.1448829770088196
[08/28/2025 10:56:08 INFO]: Training loss at epoch 24: 0.897722065448761
[08/28/2025 10:56:18 INFO]: Training stats: {
    "score": -0.9946221305332771,
    "rmse": 0.9946221305332771
}
[08/28/2025 10:56:18 INFO]: Val stats: {
    "score": -0.6819281450932283,
    "rmse": 0.6819281450932283
}
[08/28/2025 10:56:18 INFO]: Test stats: {
    "score": -0.8764375138361093,
    "rmse": 0.8764375138361093
}
[08/28/2025 10:56:30 INFO]: Training stats: {
    "score": -0.9883059485785742,
    "rmse": 0.9883059485785742
}
[08/28/2025 10:56:30 INFO]: Val stats: {
    "score": -0.6685798581025278,
    "rmse": 0.6685798581025278
}
[08/28/2025 10:56:30 INFO]: Test stats: {
    "score": -0.8709010086037416,
    "rmse": 0.8709010086037416
}
[08/28/2025 10:56:39 INFO]: New best epoch, val score: -0.6685798581025278
[08/28/2025 10:56:39 INFO]: Saving model to: blotchy-Amado_trial_142/model_best.pth
[08/28/2025 10:56:59 INFO]: Training accuracy: {
    "score": -0.9586879593942269,
    "rmse": 0.9586879593942269
}
[08/28/2025 10:56:59 INFO]: Val accuracy: {
    "score": -0.6611301227463294,
    "rmse": 0.6611301227463294
}
[08/28/2025 10:56:59 INFO]: Test accuracy: {
    "score": -0.88596983614985,
    "rmse": 0.88596983614985
}
[08/28/2025 10:56:59 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_113",
    "best_epoch": 25,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.88596983614985,
        "rmse": 0.88596983614985
    },
    "train_stats": {
        "score": -0.9586879593942269,
        "rmse": 0.9586879593942269
    },
    "val_stats": {
        "score": -0.6611301227463294,
        "rmse": 0.6611301227463294
    }
}
[08/28/2025 10:56:59 INFO]: Procewss finished for trial blotchy-Amado_trial_113
[08/28/2025 10:56:59 INFO]: 
_________________________________________________

[08/28/2025 10:56:59 INFO]: train_net_for_optune.py main() running.
[08/28/2025 10:56:59 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 8
  d_ffn_factor: 1.9878863259423492
  attention_dropout: 0.3332647748394102
  ffn_dropout: 0.3332647748394102
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.4452075648097053e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_147

[08/28/2025 10:57:00 INFO]: This ft_transformer has 15.874 million parameters.
[08/28/2025 10:57:00 INFO]: Training will start at epoch 0.
[08/28/2025 10:57:00 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 10:57:13 INFO]: Training loss at epoch 24: 0.9145839512348175
[08/28/2025 10:57:18 INFO]: Training loss at epoch 68: 0.9096504151821136
[08/28/2025 10:57:29 INFO]: Training loss at epoch 17: 1.118457317352295
[08/28/2025 10:57:42 INFO]: Training loss at epoch 20: 1.0774352848529816
[08/28/2025 10:57:52 INFO]: Training loss at epoch 2: 1.319323182106018
[08/28/2025 10:58:02 INFO]: Training loss at epoch 2: 1.0426953434944153
[08/28/2025 10:58:07 INFO]: Training loss at epoch 25: 0.8961128294467926
[08/28/2025 10:58:10 INFO]: Training loss at epoch 30: 0.9341852068901062
[08/28/2025 10:58:41 INFO]: Training loss at epoch 25: 1.0051003694534302
[08/28/2025 10:58:52 INFO]: Training loss at epoch 21: 1.0555730164051056
[08/28/2025 10:59:01 INFO]: Training loss at epoch 21: 0.9831609129905701
[08/28/2025 10:59:04 INFO]: Training loss at epoch 25: 1.0381101071834564
[08/28/2025 10:59:13 INFO]: Training loss at epoch 2: 1.49776691198349
[08/28/2025 10:59:20 INFO]: Training loss at epoch 9: 1.183882713317871
[08/28/2025 10:59:21 INFO]: Training loss at epoch 18: 1.0033243000507355
[08/28/2025 10:59:58 INFO]: Training loss at epoch 31: 0.9383500516414642
[08/28/2025 10:59:59 INFO]: Training loss at epoch 69: 0.9382829666137695
[08/28/2025 11:00:03 INFO]: Training loss at epoch 26: 0.8929845690727234
[08/28/2025 11:00:04 INFO]: Training loss at epoch 22: 0.8980403244495392
[08/28/2025 11:00:57 INFO]: Training loss at epoch 26: 0.884492427110672
[08/28/2025 11:00:57 INFO]: Training stats: {
    "score": -0.9958690522226027,
    "rmse": 0.9958690522226027
}
[08/28/2025 11:00:57 INFO]: Val stats: {
    "score": -0.7242622818028192,
    "rmse": 0.7242622818028192
}
[08/28/2025 11:00:57 INFO]: Test stats: {
    "score": -0.9038337004767227,
    "rmse": 0.9038337004767227
}
[08/28/2025 11:01:05 INFO]: Training loss at epoch 3: 1.1323808431625366
[08/28/2025 11:01:16 INFO]: Training loss at epoch 23: 0.8634206056594849
[08/28/2025 11:01:16 INFO]: Training loss at epoch 19: 0.9495656192302704
[08/28/2025 11:01:32 INFO]: Training loss at epoch 53: 1.0574828684329987
[08/28/2025 11:01:35 INFO]: Training loss at epoch 23: 1.0946616530418396
[08/28/2025 11:01:38 INFO]: Training stats: {
    "score": -1.0929239821442096,
    "rmse": 1.0929239821442096
}
[08/28/2025 11:01:38 INFO]: Val stats: {
    "score": -0.9342988058204597,
    "rmse": 0.9342988058204597
}
[08/28/2025 11:01:38 INFO]: Test stats: {
    "score": -1.0598266317929688,
    "rmse": 1.0598266317929688
}
[08/28/2025 11:01:46 INFO]: Training loss at epoch 32: 0.987824022769928
[08/28/2025 11:01:58 INFO]: Training loss at epoch 15: 0.8638748526573181
[08/28/2025 11:01:59 INFO]: Training stats: {
    "score": -0.9918310718533306,
    "rmse": 0.9918310718533306
}
[08/28/2025 11:01:59 INFO]: Val stats: {
    "score": -0.6949582557972519,
    "rmse": 0.6949582557972519
}
[08/28/2025 11:01:59 INFO]: Test stats: {
    "score": -0.8894566438782592,
    "rmse": 0.8894566438782592
}
[08/28/2025 11:02:01 INFO]: Training loss at epoch 27: 1.1782668828964233
[08/28/2025 11:02:15 INFO]: Training loss at epoch 25: 0.9419901371002197
[08/28/2025 11:02:30 INFO]: Training loss at epoch 24: 0.8579553067684174
[08/28/2025 11:02:52 INFO]: Training loss at epoch 27: 0.985065221786499
[08/28/2025 11:03:16 INFO]: Training loss at epoch 14: 1.1625666618347168
[08/28/2025 11:03:22 INFO]: Training loss at epoch 0: 0.8684160113334656
[08/28/2025 11:03:32 INFO]: Training loss at epoch 33: 0.9318861067295074
[08/28/2025 11:03:41 INFO]: Training loss at epoch 25: 1.005203902721405
[08/28/2025 11:03:43 INFO]: Training loss at epoch 70: 0.8609706163406372
[08/28/2025 11:03:53 INFO]: Training loss at epoch 20: 0.8384145498275757
[08/28/2025 11:03:57 INFO]: Training loss at epoch 28: 1.094710886478424
[08/28/2025 11:04:13 INFO]: New best epoch, val score: -0.7384536695941667
[08/28/2025 11:04:13 INFO]: Saving model to: blotchy-Amado_trial_147/model_best.pth
[08/28/2025 11:04:30 INFO]: Training loss at epoch 34: 1.2168851494789124
[08/28/2025 11:04:43 INFO]: Training loss at epoch 28: 1.0655168294906616
[08/28/2025 11:04:50 INFO]: Training loss at epoch 26: 0.9438208043575287
[08/28/2025 11:05:19 INFO]: Training loss at epoch 34: 0.9702603816986084
[08/28/2025 11:05:24 INFO]: Training loss at epoch 3: 1.5755510330200195
[08/28/2025 11:05:32 INFO]: Training loss at epoch 26: 0.8595394194126129
[08/28/2025 11:05:46 INFO]: Training loss at epoch 3: 1.0852279663085938
[08/28/2025 11:05:46 INFO]: Training loss at epoch 21: 1.0811219215393066
[08/28/2025 11:05:53 INFO]: Training loss at epoch 29: 1.3787153363227844
[08/28/2025 11:06:03 INFO]: Training loss at epoch 27: 0.9003514349460602
[08/28/2025 11:06:21 INFO]: New best epoch, val score: -0.6674556524012994
[08/28/2025 11:06:21 INFO]: Saving model to: blotchy-Amado_trial_127/model_best.pth
[08/28/2025 11:06:24 INFO]: Training loss at epoch 71: 1.1914142072200775
[08/28/2025 11:06:31 INFO]: Training loss at epoch 3: 0.9475888609886169
[08/28/2025 11:06:35 INFO]: Training stats: {
    "score": -0.9983439550910185,
    "rmse": 0.9983439550910185
}
[08/28/2025 11:06:35 INFO]: Val stats: {
    "score": -0.7028312251443251,
    "rmse": 0.7028312251443251
}
[08/28/2025 11:06:35 INFO]: Test stats: {
    "score": -0.8929289802869237,
    "rmse": 0.8929289802869237
}
[08/28/2025 11:06:38 INFO]: Training loss at epoch 29: 1.2830637097358704
[08/28/2025 11:06:53 INFO]: Training loss at epoch 22: 1.0327904224395752
[08/28/2025 11:07:05 INFO]: Training loss at epoch 35: 1.0245352387428284
[08/28/2025 11:07:13 INFO]: Training loss at epoch 28: 0.979133814573288
[08/28/2025 11:07:17 INFO]: Training stats: {
    "score": -1.0054527793332695,
    "rmse": 1.0054527793332695
}
[08/28/2025 11:07:17 INFO]: Val stats: {
    "score": -0.7333376417671851,
    "rmse": 0.7333376417671851
}
[08/28/2025 11:07:17 INFO]: Test stats: {
    "score": -0.9084740848107746,
    "rmse": 0.9084740848107746
}
[08/28/2025 11:07:19 INFO]: Running Final Evaluation...
[08/28/2025 11:07:20 INFO]: New best epoch, val score: -0.8319952148566376
[08/28/2025 11:07:20 INFO]: Saving model to: blotchy-Amado_trial_146/model_best.pth
[08/28/2025 11:07:39 INFO]: Training loss at epoch 22: 0.9147035479545593
[08/28/2025 11:08:03 INFO]: Training loss at epoch 4: 1.0950889587402344
[08/28/2025 11:08:04 INFO]: Training accuracy: {
    "score": -1.0096145002001526,
    "rmse": 1.0096145002001526
}
[08/28/2025 11:08:04 INFO]: Val accuracy: {
    "score": -0.6624801949906628,
    "rmse": 0.6624801949906628
}
[08/28/2025 11:08:04 INFO]: Test accuracy: {
    "score": -0.8735525842825989,
    "rmse": 0.8735525842825989
}
[08/28/2025 11:08:04 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_138",
    "best_epoch": 4,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8735525842825989,
        "rmse": 0.8735525842825989
    },
    "train_stats": {
        "score": -1.0096145002001526,
        "rmse": 1.0096145002001526
    },
    "val_stats": {
        "score": -0.6624801949906628,
        "rmse": 0.6624801949906628
    }
}
[08/28/2025 11:08:04 INFO]: Procewss finished for trial blotchy-Amado_trial_138
[08/28/2025 11:08:04 INFO]: 
_________________________________________________

[08/28/2025 11:08:04 INFO]: train_net_for_optune.py main() running.
[08/28/2025 11:08:04 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 8
  d_ffn_factor: 1.231057684324064
  attention_dropout: 0.3270194871203229
  ffn_dropout: 0.3270194871203229
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.108900221616132e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_148

[08/28/2025 11:08:04 INFO]: This ft_transformer has 12.303 million parameters.
[08/28/2025 11:08:04 INFO]: Training will start at epoch 0.
[08/28/2025 11:08:04 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 11:08:08 INFO]: Training loss at epoch 10: 0.8698051869869232
[08/28/2025 11:08:25 INFO]: Training loss at epoch 29: 0.9087201654911041
[08/28/2025 11:08:30 INFO]: Training loss at epoch 30: 0.9981812536716461
[08/28/2025 11:08:39 INFO]: Training loss at epoch 16: 0.9105238318443298
[08/28/2025 11:08:51 INFO]: Training stats: {
    "score": -0.980066951043185,
    "rmse": 0.980066951043185
}
[08/28/2025 11:08:51 INFO]: Val stats: {
    "score": -0.6725582057425362,
    "rmse": 0.6725582057425362
}
[08/28/2025 11:08:51 INFO]: Test stats: {
    "score": -0.875707586921407,
    "rmse": 0.875707586921407
}
[08/28/2025 11:08:55 INFO]: Training loss at epoch 26: 1.1826039254665375
[08/28/2025 11:09:05 INFO]: Training loss at epoch 72: 1.0205055475234985
[08/28/2025 11:09:12 INFO]: Training loss at epoch 30: 0.9478756189346313
[08/28/2025 11:09:32 INFO]: Training loss at epoch 23: 1.1539040803909302
[08/28/2025 11:09:42 INFO]: Training loss at epoch 54: 0.9070979356765747
[08/28/2025 11:10:04 INFO]: Training loss at epoch 30: 0.9880465567111969
[08/28/2025 11:10:13 INFO]: New best epoch, val score: -0.6669165360293154
[08/28/2025 11:10:13 INFO]: Saving model to: blotchy-Amado_trial_142/model_best.pth
[08/28/2025 11:10:28 INFO]: Training loss at epoch 31: 0.9605792760848999
[08/28/2025 11:10:36 INFO]: Training loss at epoch 1: 0.9744379818439484
[08/28/2025 11:11:03 INFO]: Training loss at epoch 24: 1.1021976470947266
[08/28/2025 11:11:07 INFO]: Training loss at epoch 31: 1.0103994011878967
[08/28/2025 11:11:09 INFO]: Training loss at epoch 15: 1.0562456846237183
[08/28/2025 11:11:20 INFO]: Training loss at epoch 31: 0.9782316982746124
[08/28/2025 11:11:30 INFO]: New best epoch, val score: -0.6646684666495314
[08/28/2025 11:11:30 INFO]: Saving model to: blotchy-Amado_trial_142/model_best.pth
[08/28/2025 11:11:30 INFO]: Training loss at epoch 24: 1.0185840129852295
[08/28/2025 11:11:52 INFO]: Training loss at epoch 73: 0.9990164041519165
[08/28/2025 11:12:23 INFO]: Training loss at epoch 27: 0.9508689045906067
[08/28/2025 11:12:30 INFO]: Training loss at epoch 32: 0.9685654938220978
[08/28/2025 11:12:32 INFO]: Training loss at epoch 32: 0.9804211556911469
[08/28/2025 11:12:42 INFO]: Training loss at epoch 4: 0.8591273427009583
[08/28/2025 11:12:59 INFO]: Training loss at epoch 32: 1.165255755186081
[08/28/2025 11:13:21 INFO]: Training loss at epoch 0: 1.1474543809890747
[08/28/2025 11:13:23 INFO]: Training loss at epoch 25: 1.0413129925727844
[08/28/2025 11:13:34 INFO]: New best epoch, val score: -0.6951366787224413
[08/28/2025 11:13:34 INFO]: Saving model to: blotchy-Amado_trial_145/model_best.pth
[08/28/2025 11:13:42 INFO]: Training loss at epoch 4: 1.0148970782756805
[08/28/2025 11:13:49 INFO]: Training loss at epoch 33: 0.9833171367645264
[08/28/2025 11:13:53 INFO]: Training loss at epoch 4: 1.1617347598075867
[08/28/2025 11:14:04 INFO]: New best epoch, val score: -0.9210517187669813
[08/28/2025 11:14:04 INFO]: Saving model to: blotchy-Amado_trial_148/model_best.pth
[08/28/2025 11:14:28 INFO]: Training loss at epoch 33: 0.9266865849494934
[08/28/2025 11:14:35 INFO]: Training loss at epoch 74: 0.9887184500694275
[08/28/2025 11:14:49 INFO]: Training loss at epoch 23: 0.7760500609874725
[08/28/2025 11:14:53 INFO]: Training loss at epoch 11: 0.9345619678497314
[08/28/2025 11:14:55 INFO]: Training loss at epoch 33: 1.286626785993576
[08/28/2025 11:15:01 INFO]: Training loss at epoch 34: 1.1542867720127106
[08/28/2025 11:15:07 INFO]: Training loss at epoch 5: 0.9735178351402283
[08/28/2025 11:15:18 INFO]: Training loss at epoch 26: 0.9822607040405273
[08/28/2025 11:15:36 INFO]: Training loss at epoch 17: 0.9245414137840271
[08/28/2025 11:15:50 INFO]: Training loss at epoch 27: 1.061331331729889
[08/28/2025 11:16:11 INFO]: Training loss at epoch 35: 0.8301600813865662
[08/28/2025 11:16:20 INFO]: New best epoch, val score: -0.6632077255887975
[08/28/2025 11:16:20 INFO]: Saving model to: blotchy-Amado_trial_142/model_best.pth
[08/28/2025 11:16:25 INFO]: Training loss at epoch 34: 0.8573595881462097
[08/28/2025 11:16:45 INFO]: Training loss at epoch 34: 1.1139075756072998
[08/28/2025 11:16:52 INFO]: Training loss at epoch 35: 0.7903536260128021
[08/28/2025 11:17:09 INFO]: Training loss at epoch 27: 0.9223428070545197
[08/28/2025 11:17:15 INFO]: Training loss at epoch 75: 0.9594154357910156
[08/28/2025 11:17:20 INFO]: Training loss at epoch 36: 0.8858691155910492
[08/28/2025 11:17:48 INFO]: Training loss at epoch 2: 1.3859874606132507
[08/28/2025 11:18:03 INFO]: Training loss at epoch 55: 0.8663511574268341
[08/28/2025 11:18:21 INFO]: Training loss at epoch 35: 1.199825942516327
[08/28/2025 11:18:33 INFO]: Training loss at epoch 37: 1.003704309463501
[08/28/2025 11:18:38 INFO]: Training loss at epoch 35: 1.1629210710525513
[08/28/2025 11:18:52 INFO]: Running Final Evaluation...
[08/28/2025 11:19:02 INFO]: Training loss at epoch 16: 0.8445147573947906
[08/28/2025 11:19:04 INFO]: Training loss at epoch 28: 0.9490468502044678
[08/28/2025 11:19:11 INFO]: Training loss at epoch 28: 1.1763017773628235
[08/28/2025 11:19:16 INFO]: Training loss at epoch 1: 1.5098639130592346
[08/28/2025 11:19:34 INFO]: Training accuracy: {
    "score": -1.003694313434866,
    "rmse": 1.003694313434866
}
[08/28/2025 11:19:34 INFO]: Val accuracy: {
    "score": -0.7093051933350154,
    "rmse": 0.7093051933350154
}
[08/28/2025 11:19:34 INFO]: Test accuracy: {
    "score": -0.8902664794902815,
    "rmse": 0.8902664794902815
}
[08/28/2025 11:19:34 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_140",
    "best_epoch": 4,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8902664794902815,
        "rmse": 0.8902664794902815
    },
    "train_stats": {
        "score": -1.003694313434866,
        "rmse": 1.003694313434866
    },
    "val_stats": {
        "score": -0.7093051933350154,
        "rmse": 0.7093051933350154
    }
}
[08/28/2025 11:19:34 INFO]: Procewss finished for trial blotchy-Amado_trial_140
[08/28/2025 11:19:34 INFO]: 
_________________________________________________

[08/28/2025 11:19:34 INFO]: train_net_for_optune.py main() running.
[08/28/2025 11:19:34 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 8
  d_ffn_factor: 1.4928314406979768
  attention_dropout: 0.058152029070541136
  ffn_dropout: 0.058152029070541136
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.457613943444867e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_149

[08/28/2025 11:19:34 INFO]: This ft_transformer has 13.539 million parameters.
[08/28/2025 11:19:34 INFO]: Training will start at epoch 0.
[08/28/2025 11:19:34 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 11:19:43 INFO]: Training loss at epoch 38: 1.159755676984787
[08/28/2025 11:19:59 INFO]: Training loss at epoch 76: 0.9458594918251038
[08/28/2025 11:20:03 INFO]: Training loss at epoch 5: 0.9045120775699615
[08/28/2025 11:20:16 INFO]: Training loss at epoch 36: 0.8021343648433685
[08/28/2025 11:20:19 INFO]: Running Final Evaluation...
[08/28/2025 11:20:28 INFO]: Training loss at epoch 25: 0.9702783524990082
[08/28/2025 11:20:54 INFO]: Training loss at epoch 39: 0.9893009066581726
[08/28/2025 11:20:55 INFO]: Training loss at epoch 29: 0.9543299973011017
[08/28/2025 11:21:04 INFO]: Training loss at epoch 5: 1.3401183485984802
[08/28/2025 11:21:18 INFO]: Training stats: {
    "score": -0.9755025671114596,
    "rmse": 0.9755025671114596
}
[08/28/2025 11:21:18 INFO]: Val stats: {
    "score": -0.6931425788583635,
    "rmse": 0.6931425788583635
}
[08/28/2025 11:21:18 INFO]: Test stats: {
    "score": -0.89127783778226,
    "rmse": 0.89127783778226
}
[08/28/2025 11:21:19 INFO]: Training accuracy: {
    "score": -0.9918060731376925,
    "rmse": 0.9918060731376925
}
[08/28/2025 11:21:19 INFO]: Val accuracy: {
    "score": -0.6966105666467524,
    "rmse": 0.6966105666467524
}
[08/28/2025 11:21:19 INFO]: Test accuracy: {
    "score": -0.8848833229547125,
    "rmse": 0.8848833229547125
}
[08/28/2025 11:21:19 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_123",
    "best_epoch": 45,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8848833229547125,
        "rmse": 0.8848833229547125
    },
    "train_stats": {
        "score": -0.9918060731376925,
        "rmse": 0.9918060731376925
    },
    "val_stats": {
        "score": -0.6966105666467524,
        "rmse": 0.6966105666467524
    }
}
[08/28/2025 11:21:19 INFO]: Procewss finished for trial blotchy-Amado_trial_123
[08/28/2025 11:21:19 INFO]: 
_________________________________________________

[08/28/2025 11:21:19 INFO]: train_net_for_optune.py main() running.
[08/28/2025 11:21:19 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 1.5028672892143633
  attention_dropout: 0.17840195647868323
  ffn_dropout: 0.17840195647868323
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.5108567805522566e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_150

[08/28/2025 11:21:19 INFO]: This ft_transformer has 13.585 million parameters.
[08/28/2025 11:21:19 INFO]: Training will start at epoch 0.
[08/28/2025 11:21:19 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 11:21:23 INFO]: Training loss at epoch 12: 1.1238186359405518
[08/28/2025 11:21:30 INFO]: Training loss at epoch 5: 1.0226716101169586
[08/28/2025 11:21:34 INFO]: Training stats: {
    "score": -0.9872934174986303,
    "rmse": 0.9872934174986303
}
[08/28/2025 11:21:34 INFO]: Val stats: {
    "score": -0.6851950965661414,
    "rmse": 0.6851950965661414
}
[08/28/2025 11:21:34 INFO]: Test stats: {
    "score": -0.8884934903169336,
    "rmse": 0.8884934903169336
}
[08/28/2025 11:22:02 INFO]: Training loss at epoch 6: 0.8588528633117676
[08/28/2025 11:22:10 INFO]: Training loss at epoch 37: 1.0173319578170776
[08/28/2025 11:22:17 INFO]: Training loss at epoch 18: 0.9777809083461761
[08/28/2025 11:22:30 INFO]: Training loss at epoch 40: 0.8220897912979126
[08/28/2025 11:22:32 INFO]: Training loss at epoch 28: 0.9555799961090088
[08/28/2025 11:22:37 INFO]: Training loss at epoch 24: 0.9358954727649689
[08/28/2025 11:23:29 INFO]: Training loss at epoch 30: 0.9026569724082947
[08/28/2025 11:23:40 INFO]: Training loss at epoch 41: 0.8639414608478546
[08/28/2025 11:24:06 INFO]: Training loss at epoch 38: 0.9113143682479858
[08/28/2025 11:24:52 INFO]: Training loss at epoch 42: 0.8699526488780975
[08/28/2025 11:24:59 INFO]: Training loss at epoch 3: 1.1207788586616516
[08/28/2025 11:25:07 INFO]: Training loss at epoch 2: 1.6366121172904968
[08/28/2025 11:25:09 INFO]: Training loss at epoch 0: 0.9503820836544037
[08/28/2025 11:25:20 INFO]: Training loss at epoch 31: 0.8926640450954437
[08/28/2025 11:25:51 INFO]: New best epoch, val score: -0.8489656008265294
[08/28/2025 11:25:51 INFO]: Saving model to: blotchy-Amado_trial_148/model_best.pth
[08/28/2025 11:25:53 INFO]: New best epoch, val score: -0.6591181343787774
[08/28/2025 11:25:53 INFO]: Saving model to: blotchy-Amado_trial_147/model_best.pth
[08/28/2025 11:25:57 INFO]: New best epoch, val score: -0.7271462599514088
[08/28/2025 11:25:57 INFO]: Saving model to: blotchy-Amado_trial_149/model_best.pth
[08/28/2025 11:25:58 INFO]: Training loss at epoch 29: 0.769797146320343
[08/28/2025 11:26:04 INFO]: Training loss at epoch 39: 1.1001448035240173
[08/28/2025 11:26:05 INFO]: Training loss at epoch 43: 0.7972705364227295
[08/28/2025 11:26:17 INFO]: Training loss at epoch 56: 1.068431168794632
[08/28/2025 11:26:44 INFO]: Training stats: {
    "score": -0.9975385805311014,
    "rmse": 0.9975385805311014
}
[08/28/2025 11:26:44 INFO]: Val stats: {
    "score": -0.7071033178626537,
    "rmse": 0.7071033178626537
}
[08/28/2025 11:26:44 INFO]: Test stats: {
    "score": -0.9015506057726367,
    "rmse": 0.9015506057726367
}
[08/28/2025 11:26:54 INFO]: Training loss at epoch 17: 1.1887144446372986
[08/28/2025 11:27:11 INFO]: Training loss at epoch 32: 1.1431864500045776
[08/28/2025 11:27:13 INFO]: Training loss at epoch 44: 1.0581002831459045
[08/28/2025 11:27:14 INFO]: Training loss at epoch 6: 1.054158866405487
[08/28/2025 11:27:27 INFO]: Training loss at epoch 0: 1.0751135349273682
[08/28/2025 11:27:59 INFO]: Training loss at epoch 13: 1.6074150800704956
[08/28/2025 11:28:14 INFO]: Training stats: {
    "score": -0.96629149951537,
    "rmse": 0.96629149951537
}
[08/28/2025 11:28:14 INFO]: Val stats: {
    "score": -0.7057209893974407,
    "rmse": 0.7057209893974407
}
[08/28/2025 11:28:14 INFO]: Test stats: {
    "score": -0.9014503975964916,
    "rmse": 0.9014503975964916
}
[08/28/2025 11:28:15 INFO]: Training loss at epoch 6: 1.4635884165763855
[08/28/2025 11:28:18 INFO]: New best epoch, val score: -0.6915669309797797
[08/28/2025 11:28:18 INFO]: Saving model to: blotchy-Amado_trial_150/model_best.pth
[08/28/2025 11:28:25 INFO]: Training loss at epoch 45: 0.981215238571167
[08/28/2025 11:28:39 INFO]: Training loss at epoch 40: 0.9686631858348846
[08/28/2025 11:28:58 INFO]: Training loss at epoch 7: 0.934712290763855
[08/28/2025 11:29:01 INFO]: Training loss at epoch 19: 0.82203009724617
[08/28/2025 11:29:02 INFO]: Training loss at epoch 36: 0.9127858579158783
[08/28/2025 11:29:05 INFO]: Training loss at epoch 33: 0.7879818677902222
[08/28/2025 11:29:15 INFO]: Training loss at epoch 29: 0.9032151103019714
[08/28/2025 11:29:23 INFO]: Training loss at epoch 6: 0.7950130105018616
[08/28/2025 11:29:37 INFO]: Training loss at epoch 46: 0.8718478381633759
[08/28/2025 11:29:52 INFO]: Training loss at epoch 26: 0.9923800230026245
[08/28/2025 11:30:29 INFO]: Training loss at epoch 25: 0.8881540298461914
[08/28/2025 11:30:38 INFO]: Training loss at epoch 41: 0.9693031311035156
[08/28/2025 11:30:50 INFO]: Training loss at epoch 47: 1.042756736278534
[08/28/2025 11:30:59 INFO]: New best epoch, val score: -0.6629823585677059
[08/28/2025 11:30:59 INFO]: Saving model to: blotchy-Amado_trial_142/model_best.pth
[08/28/2025 11:31:01 INFO]: Training loss at epoch 34: 0.9048244953155518
[08/28/2025 11:31:03 INFO]: Training loss at epoch 3: 1.0918489694595337
[08/28/2025 11:31:22 INFO]: Training stats: {
    "score": -0.9466747601065401,
    "rmse": 0.9466747601065401
}
[08/28/2025 11:31:22 INFO]: Val stats: {
    "score": -0.6784995650262857,
    "rmse": 0.6784995650262857
}
[08/28/2025 11:31:22 INFO]: Test stats: {
    "score": -0.899296912599999,
    "rmse": 0.899296912599999
}
[08/28/2025 11:31:31 INFO]: Training loss at epoch 1: 0.9612522423267365
[08/28/2025 11:31:36 INFO]: Training stats: {
    "score": -1.002699386147926,
    "rmse": 1.002699386147926
}
[08/28/2025 11:31:36 INFO]: Val stats: {
    "score": -0.8117234938644098,
    "rmse": 0.8117234938644098
}
[08/28/2025 11:31:36 INFO]: Test stats: {
    "score": -0.9726004059920711,
    "rmse": 0.9726004059920711
}
[08/28/2025 11:32:00 INFO]: Training loss at epoch 48: 0.8336115479469299
[08/28/2025 11:32:14 INFO]: Training loss at epoch 4: 1.228109359741211
[08/28/2025 11:32:35 INFO]: Training loss at epoch 42: 1.1029138565063477
[08/28/2025 11:32:53 INFO]: Training loss at epoch 35: 0.8960435688495636
[08/28/2025 11:33:12 INFO]: Training loss at epoch 49: 0.8856824338436127
[08/28/2025 11:33:36 INFO]: Training stats: {
    "score": -0.9638638552983109,
    "rmse": 0.9638638552983109
}
[08/28/2025 11:33:36 INFO]: Val stats: {
    "score": -0.6801340408940258,
    "rmse": 0.6801340408940258
}
[08/28/2025 11:33:36 INFO]: Test stats: {
    "score": -0.8835491736963412,
    "rmse": 0.8835491736963412
}
[08/28/2025 11:34:29 INFO]: Training loss at epoch 1: 1.0389574766159058
[08/28/2025 11:34:29 INFO]: Training loss at epoch 43: 0.8373873829841614
[08/28/2025 11:34:30 INFO]: Training loss at epoch 57: 0.9717667400836945
[08/28/2025 11:34:33 INFO]: Training loss at epoch 7: 1.0690008401870728
[08/28/2025 11:34:37 INFO]: Training loss at epoch 14: 1.2929742336273193
[08/28/2025 11:34:44 INFO]: Training loss at epoch 36: 1.0555726289749146
[08/28/2025 11:34:47 INFO]: Training loss at epoch 18: 0.7852551639080048
[08/28/2025 11:34:48 INFO]: Training loss at epoch 50: 0.7496942579746246
[08/28/2025 11:35:03 INFO]: Training loss at epoch 30: 1.2175137996673584
[08/28/2025 11:35:23 INFO]: New best epoch, val score: -0.6936680725065474
[08/28/2025 11:35:23 INFO]: Saving model to: blotchy-Amado_trial_145/model_best.pth
[08/28/2025 11:35:32 INFO]: Training loss at epoch 7: 1.0743337571620941
[08/28/2025 11:35:58 INFO]: Training loss at epoch 8: 1.182809978723526
[08/28/2025 11:35:58 INFO]: Training loss at epoch 51: 0.9747686386108398
[08/28/2025 11:36:22 INFO]: New best epoch, val score: -0.8159042470881882
[08/28/2025 11:36:22 INFO]: Saving model to: blotchy-Amado_trial_146/model_best.pth
[08/28/2025 11:36:26 INFO]: Training loss at epoch 44: 1.1501102149486542
[08/28/2025 11:36:35 INFO]: Training loss at epoch 37: 0.9801235795021057
[08/28/2025 11:36:54 INFO]: Training loss at epoch 4: 1.0667089223861694
[08/28/2025 11:37:09 INFO]: Training loss at epoch 52: 1.0505483448505402
[08/28/2025 11:37:16 INFO]: Training loss at epoch 7: 0.8030571937561035
[08/28/2025 11:37:51 INFO]: Training loss at epoch 2: 0.9745034277439117
[08/28/2025 11:38:07 INFO]: Training loss at epoch 20: 0.7523842751979828
[08/28/2025 11:38:22 INFO]: Training loss at epoch 53: 0.7894342541694641
[08/28/2025 11:38:23 INFO]: Training loss at epoch 26: 0.9856541156768799
[08/28/2025 11:38:23 INFO]: Training loss at epoch 30: 0.9364388287067413
[08/28/2025 11:38:24 INFO]: Training loss at epoch 45: 1.0072276294231415
[08/28/2025 11:38:29 INFO]: Training loss at epoch 38: 1.0831604599952698
[08/28/2025 11:39:19 INFO]: Training loss at epoch 27: 0.8646936118602753
[08/28/2025 11:39:28 INFO]: Training loss at epoch 5: 1.5615403652191162
[08/28/2025 11:39:32 INFO]: Training loss at epoch 54: 0.8941269516944885
[08/28/2025 11:40:20 INFO]: Training loss at epoch 46: 0.8887641429901123
[08/28/2025 11:40:20 INFO]: Training loss at epoch 39: 0.9717274308204651
[08/28/2025 11:40:43 INFO]: Training loss at epoch 55: 0.9836057424545288
[08/28/2025 11:41:00 INFO]: Training stats: {
    "score": -0.9894761190899615,
    "rmse": 0.9894761190899615
}
[08/28/2025 11:41:00 INFO]: Val stats: {
    "score": -0.6991905685063746,
    "rmse": 0.6991905685063746
}
[08/28/2025 11:41:00 INFO]: Test stats: {
    "score": -0.9028943858549128,
    "rmse": 0.9028943858549128
}
[08/28/2025 11:41:09 INFO]: Training loss at epoch 15: 0.8930555284023285
[08/28/2025 11:41:14 INFO]: Training loss at epoch 37: 0.8901675045490265
[08/28/2025 11:41:28 INFO]: Training loss at epoch 2: 0.8419205099344254
[08/28/2025 11:41:49 INFO]: Training loss at epoch 8: 1.0856202840805054
[08/28/2025 11:41:49 INFO]: Training loss at epoch 31: 0.9794168770313263
[08/28/2025 11:41:53 INFO]: Training loss at epoch 56: 0.8246268630027771
[08/28/2025 11:41:57 INFO]: New best epoch, val score: -0.6706567210319362
[08/28/2025 11:41:57 INFO]: Saving model to: blotchy-Amado_trial_137/model_best.pth
[08/28/2025 11:42:15 INFO]: Training loss at epoch 47: 0.9571892619132996
[08/28/2025 11:42:35 INFO]: Training loss at epoch 19: 1.0664320290088654
[08/28/2025 11:42:40 INFO]: New best epoch, val score: -0.6894671375439657
[08/28/2025 11:42:40 INFO]: Saving model to: blotchy-Amado_trial_145/model_best.pth
[08/28/2025 11:42:48 INFO]: Training loss at epoch 58: 0.8992204964160919
[08/28/2025 11:42:48 INFO]: Training loss at epoch 8: 1.1841144561767578
[08/28/2025 11:42:53 INFO]: Training loss at epoch 5: 1.3843358755111694
[08/28/2025 11:42:55 INFO]: Training loss at epoch 40: 0.7639466971158981
[08/28/2025 11:42:57 INFO]: Training loss at epoch 9: 1.0159385204315186
[08/28/2025 11:43:08 INFO]: Training loss at epoch 57: 0.9007924795150757
[08/28/2025 11:43:37 INFO]: New best epoch, val score: -0.7849799978287758
[08/28/2025 11:43:37 INFO]: Saving model to: blotchy-Amado_trial_146/model_best.pth
[08/28/2025 11:44:12 INFO]: Training loss at epoch 3: 1.1522198915481567
[08/28/2025 11:44:14 INFO]: Training loss at epoch 48: 1.4517594575881958
[08/28/2025 11:44:18 INFO]: Training loss at epoch 58: 0.8337396383285522
[08/28/2025 11:44:47 INFO]: Training loss at epoch 41: 0.9496811330318451
[08/28/2025 11:44:52 INFO]: Training loss at epoch 21: 0.9260473549365997
[08/28/2025 11:44:57 INFO]: New best epoch, val score: -0.6696426704761688
[08/28/2025 11:44:57 INFO]: Saving model to: blotchy-Amado_trial_149/model_best.pth
[08/28/2025 11:45:10 INFO]: Training loss at epoch 31: 1.0396915674209595
[08/28/2025 11:45:11 INFO]: Training loss at epoch 8: 1.0114420652389526
[08/28/2025 11:45:20 INFO]: Training stats: {
    "score": -0.9893556662594911,
    "rmse": 0.9893556662594911
}
[08/28/2025 11:45:20 INFO]: Val stats: {
    "score": -0.6687285221681853,
    "rmse": 0.6687285221681853
}
[08/28/2025 11:45:20 INFO]: Test stats: {
    "score": -0.8741379108155815,
    "rmse": 0.8741379108155815
}
[08/28/2025 11:45:22 INFO]: Training stats: {
    "score": -0.9974978544676041,
    "rmse": 0.9974978544676041
}
[08/28/2025 11:45:22 INFO]: Val stats: {
    "score": -0.7322160530800903,
    "rmse": 0.7322160530800903
}
[08/28/2025 11:45:22 INFO]: Test stats: {
    "score": -0.9160620458317084,
    "rmse": 0.9160620458317084
}
[08/28/2025 11:45:31 INFO]: Training loss at epoch 59: 0.8597981333732605
[08/28/2025 11:45:42 INFO]: New best epoch, val score: -0.6761791804924361
[08/28/2025 11:45:42 INFO]: Saving model to: blotchy-Amado_trial_132/model_best.pth
[08/28/2025 11:45:58 INFO]: Training stats: {
    "score": -0.9562960450412292,
    "rmse": 0.9562960450412292
}
[08/28/2025 11:45:58 INFO]: Val stats: {
    "score": -0.6679179319900591,
    "rmse": 0.6679179319900591
}
[08/28/2025 11:45:58 INFO]: Test stats: {
    "score": -0.8795717331507088,
    "rmse": 0.8795717331507088
}
[08/28/2025 11:46:12 INFO]: Training loss at epoch 49: 0.9051063358783722
[08/28/2025 11:46:16 INFO]: Training loss at epoch 27: 0.8356256783008575
[08/28/2025 11:46:17 INFO]: New best epoch, val score: -0.6687285221681853
[08/28/2025 11:46:17 INFO]: Saving model to: blotchy-Amado_trial_130/model_best.pth
[08/28/2025 11:46:39 INFO]: Training loss at epoch 6: 1.2606690526008606
[08/28/2025 11:46:40 INFO]: Training loss at epoch 42: 0.9512065351009369
[08/28/2025 11:46:52 INFO]: Training stats: {
    "score": -0.9884182656468707,
    "rmse": 0.9884182656468707
}
[08/28/2025 11:46:52 INFO]: Val stats: {
    "score": -0.6821407179315995,
    "rmse": 0.6821407179315995
}
[08/28/2025 11:46:52 INFO]: Test stats: {
    "score": -0.8933040061427313,
    "rmse": 0.8933040061427313
}
[08/28/2025 11:46:54 INFO]: Running Final Evaluation...
[08/28/2025 11:47:07 INFO]: Training loss at epoch 60: 1.0402394235134125
[08/28/2025 11:47:28 INFO]: New best epoch, val score: -0.6589309085009057
[08/28/2025 11:47:28 INFO]: Saving model to: blotchy-Amado_trial_147/model_best.pth
[08/28/2025 11:47:40 INFO]: Training accuracy: {
    "score": -0.9907211158827488,
    "rmse": 0.9907211158827488
}
[08/28/2025 11:47:40 INFO]: Val accuracy: {
    "score": -0.6501756726563553,
    "rmse": 0.6501756726563553
}
[08/28/2025 11:47:40 INFO]: Test accuracy: {
    "score": -0.8649168664686891,
    "rmse": 0.8649168664686891
}
[08/28/2025 11:47:40 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_141",
    "best_epoch": 11,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8649168664686891,
        "rmse": 0.8649168664686891
    },
    "train_stats": {
        "score": -0.9907211158827488,
        "rmse": 0.9907211158827488
    },
    "val_stats": {
        "score": -0.6501756726563553,
        "rmse": 0.6501756726563553
    }
}
[08/28/2025 11:47:40 INFO]: Procewss finished for trial blotchy-Amado_trial_141
[08/28/2025 11:47:40 INFO]: 
_________________________________________________

[08/28/2025 11:47:40 INFO]: train_net_for_optune.py main() running.
[08/28/2025 11:47:40 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 8
  d_ffn_factor: 1.5120064887990312
  attention_dropout: 0.31097780621593946
  ffn_dropout: 0.31097780621593946
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.9498563883971207e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_151

[08/28/2025 11:47:40 INFO]: This ft_transformer has 13.632 million parameters.
[08/28/2025 11:47:40 INFO]: Training will start at epoch 0.
[08/28/2025 11:47:40 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 11:47:50 INFO]: Training loss at epoch 16: 0.9227309823036194
[08/28/2025 11:48:20 INFO]: Training loss at epoch 61: 1.0338770151138306
[08/28/2025 11:48:31 INFO]: Training loss at epoch 3: 0.9908955097198486
[08/28/2025 11:48:41 INFO]: Training loss at epoch 32: 1.0434645414352417
[08/28/2025 11:48:49 INFO]: Training loss at epoch 28: 0.960383951663971
[08/28/2025 11:48:49 INFO]: Training loss at epoch 6: 1.4515914916992188
[08/28/2025 11:48:50 INFO]: Training loss at epoch 50: 1.1589861810207367
[08/28/2025 11:49:12 INFO]: Training loss at epoch 9: 1.1178332567214966
[08/28/2025 11:49:31 INFO]: Training loss at epoch 62: 0.8026531636714935
[08/28/2025 11:50:04 INFO]: Training loss at epoch 9: 1.0913602113723755
[08/28/2025 11:50:35 INFO]: Training loss at epoch 4: 1.0736365914344788
[08/28/2025 11:50:42 INFO]: Training loss at epoch 63: 0.8826940059661865
[08/28/2025 11:50:45 INFO]: Training loss at epoch 51: 1.0796216130256653
[08/28/2025 11:51:00 INFO]: New best epoch, val score: -0.6693364366092415
[08/28/2025 11:51:00 INFO]: Saving model to: blotchy-Amado_trial_139/model_best.pth
[08/28/2025 11:51:04 INFO]: Training loss at epoch 59: 0.8151426613330841
[08/28/2025 11:51:21 INFO]: New best epoch, val score: -0.6657419233029569
[08/28/2025 11:51:21 INFO]: Saving model to: blotchy-Amado_trial_149/model_best.pth
[08/28/2025 11:51:38 INFO]: Training stats: {
    "score": -1.0222725202443312,
    "rmse": 1.0222725202443312
}
[08/28/2025 11:51:38 INFO]: Val stats: {
    "score": -0.6861245185857648,
    "rmse": 0.6861245185857648
}
[08/28/2025 11:51:38 INFO]: Test stats: {
    "score": -0.8741024702434568,
    "rmse": 0.8741024702434568
}
[08/28/2025 11:51:42 INFO]: Training loss at epoch 22: 0.9571099281311035
[08/28/2025 11:51:53 INFO]: Training loss at epoch 64: 0.7988670468330383
[08/28/2025 11:51:58 INFO]: Training loss at epoch 32: 0.9167367815971375
[08/28/2025 11:52:19 INFO]: Training loss at epoch 10: 1.0281885862350464
[08/28/2025 11:52:28 INFO]: Training stats: {
    "score": -1.0067136285365395,
    "rmse": 1.0067136285365395
}
[08/28/2025 11:52:28 INFO]: Val stats: {
    "score": -0.7562010337016561,
    "rmse": 0.7562010337016561
}
[08/28/2025 11:52:28 INFO]: Test stats: {
    "score": -0.9343943712000173,
    "rmse": 0.9343943712000173
}
[08/28/2025 11:52:29 INFO]: New best epoch, val score: -0.6861245185857648
[08/28/2025 11:52:29 INFO]: Saving model to: blotchy-Amado_trial_145/model_best.pth
[08/28/2025 11:52:40 INFO]: Training loss at epoch 52: 1.0038333535194397
[08/28/2025 11:52:57 INFO]: New best epoch, val score: -0.66770320294193
[08/28/2025 11:52:57 INFO]: Saving model to: blotchy-Amado_trial_139/model_best.pth
[08/28/2025 11:53:04 INFO]: Training loss at epoch 65: 0.8374615013599396
[08/28/2025 11:53:07 INFO]: Training loss at epoch 9: 0.8895849585533142
[08/28/2025 11:53:16 INFO]: Training loss at epoch 20: 0.8663238883018494
[08/28/2025 11:53:21 INFO]: Training loss at epoch 0: 0.9474129974842072
[08/28/2025 11:53:22 INFO]: New best epoch, val score: -0.7562010337016561
[08/28/2025 11:53:22 INFO]: Saving model to: blotchy-Amado_trial_146/model_best.pth
[08/28/2025 11:53:29 INFO]: Training loss at epoch 38: 1.086178570985794
[08/28/2025 11:53:52 INFO]: Training loss at epoch 7: 1.139849066734314
[08/28/2025 11:53:53 INFO]: Training stats: {
    "score": -0.9486545187901001,
    "rmse": 0.9486545187901001
}
[08/28/2025 11:53:53 INFO]: Val stats: {
    "score": -0.6841000903230531,
    "rmse": 0.6841000903230531
}
[08/28/2025 11:53:53 INFO]: Test stats: {
    "score": -0.9042177287050029,
    "rmse": 0.9042177287050029
}
[08/28/2025 11:54:05 INFO]: New best epoch, val score: -0.8122462466879938
[08/28/2025 11:54:05 INFO]: Saving model to: blotchy-Amado_trial_151/model_best.pth
[08/28/2025 11:54:09 INFO]: Training loss at epoch 28: 0.6649865359067917
[08/28/2025 11:54:17 INFO]: Training loss at epoch 66: 0.9429721832275391
[08/28/2025 11:54:29 INFO]: Training loss at epoch 17: 0.8231267333030701
[08/28/2025 11:54:41 INFO]: Training loss at epoch 53: 1.027121663093567
[08/28/2025 11:54:43 INFO]: Training loss at epoch 7: 1.2307977080345154
[08/28/2025 11:54:44 INFO]: New best epoch, val score: -0.6579185127730452
[08/28/2025 11:54:44 INFO]: Saving model to: blotchy-Amado_trial_147/model_best.pth
[08/28/2025 11:54:55 INFO]: New best epoch, val score: -0.667089021116151
[08/28/2025 11:54:55 INFO]: Saving model to: blotchy-Amado_trial_139/model_best.pth
[08/28/2025 11:55:28 INFO]: Training loss at epoch 67: 1.0469878017902374
[08/28/2025 11:55:28 INFO]: Training loss at epoch 33: 1.019811898469925
[08/28/2025 11:55:31 INFO]: Training loss at epoch 4: 1.2036194503307343
[08/28/2025 11:55:49 INFO]: Training stats: {
    "score": -0.9566302949311605,
    "rmse": 0.9566302949311605
}
[08/28/2025 11:55:49 INFO]: Val stats: {
    "score": -0.7102392669352439,
    "rmse": 0.7102392669352439
}
[08/28/2025 11:55:49 INFO]: Test stats: {
    "score": -0.8879752227936043,
    "rmse": 0.8879752227936043
}
[08/28/2025 11:56:37 INFO]: Training loss at epoch 54: 1.0240435898303986
[08/28/2025 11:56:38 INFO]: Training loss at epoch 68: 0.9446810781955719
[08/28/2025 11:56:51 INFO]: New best epoch, val score: -0.666996500537824
[08/28/2025 11:56:51 INFO]: Saving model to: blotchy-Amado_trial_139/model_best.pth
[08/28/2025 11:56:56 INFO]: Training loss at epoch 5: 1.059393584728241
[08/28/2025 11:57:44 INFO]: New best epoch, val score: -0.6638051780329907
[08/28/2025 11:57:44 INFO]: Saving model to: blotchy-Amado_trial_149/model_best.pth
[08/28/2025 11:57:51 INFO]: Training loss at epoch 69: 0.7512011826038361
[08/28/2025 11:58:11 INFO]: Training loss at epoch 29: 1.3427456319332123
[08/28/2025 11:58:15 INFO]: Training stats: {
    "score": -0.9468646570732437,
    "rmse": 0.9468646570732437
}
[08/28/2025 11:58:15 INFO]: Val stats: {
    "score": -0.7039254864354507,
    "rmse": 0.7039254864354507
}
[08/28/2025 11:58:15 INFO]: Test stats: {
    "score": -0.9043579286445729,
    "rmse": 0.9043579286445729
}
[08/28/2025 11:58:29 INFO]: Training loss at epoch 23: 0.739819198846817
[08/28/2025 11:58:33 INFO]: Training loss at epoch 55: 0.9607075750827789
[08/28/2025 11:58:44 INFO]: Training loss at epoch 33: 1.1207092702388763
[08/28/2025 11:58:58 INFO]: Training loss at epoch 10: 1.0688123106956482
[08/28/2025 11:59:18 INFO]: Training loss at epoch 11: 0.997730016708374
[08/28/2025 11:59:25 INFO]: Training loss at epoch 70: 1.1033065915107727
[08/28/2025 11:59:45 INFO]: Training loss at epoch 1: 0.9529262781143188
[08/28/2025 11:59:47 INFO]: Training loss at epoch 10: 0.9401771724224091
[08/28/2025 12:00:08 INFO]: New best epoch, val score: -0.6657490814954473
[08/28/2025 12:00:08 INFO]: Saving model to: blotchy-Amado_trial_143/model_best.pth
[08/28/2025 12:00:30 INFO]: Training loss at epoch 56: 1.0894532799720764
[08/28/2025 12:00:38 INFO]: Training loss at epoch 71: 0.8807502388954163
[08/28/2025 12:00:38 INFO]: New best epoch, val score: -0.6757209282108269
[08/28/2025 12:00:38 INFO]: Saving model to: blotchy-Amado_trial_146/model_best.pth
[08/28/2025 12:00:39 INFO]: Training loss at epoch 8: 1.268452227115631
[08/28/2025 12:01:04 INFO]: Training loss at epoch 18: 1.120069146156311
[08/28/2025 12:01:05 INFO]: Training loss at epoch 8: 0.9178113639354706
[08/28/2025 12:01:08 INFO]: Training loss at epoch 21: 0.9876764416694641
[08/28/2025 12:01:22 INFO]: Training stats: {
    "score": -0.9708221276134601,
    "rmse": 0.9708221276134601
}
[08/28/2025 12:01:22 INFO]: Val stats: {
    "score": -0.6797309229642736,
    "rmse": 0.6797309229642736
}
[08/28/2025 12:01:22 INFO]: Test stats: {
    "score": -0.8757233208108214,
    "rmse": 0.8757233208108214
}
[08/28/2025 12:01:48 INFO]: Training loss at epoch 72: 0.8504502475261688
[08/28/2025 12:02:02 INFO]: Training loss at epoch 29: 1.0744019746780396
[08/28/2025 12:02:09 INFO]: Training loss at epoch 60: 1.0461552441120148
[08/28/2025 12:02:13 INFO]: Training loss at epoch 34: 0.9611164331436157
[08/28/2025 12:02:28 INFO]: Training loss at epoch 57: 0.8066858053207397
[08/28/2025 12:02:30 INFO]: Training loss at epoch 5: 1.1189158260822296
[08/28/2025 12:02:32 INFO]: New best epoch, val score: -0.6797309229642736
[08/28/2025 12:02:32 INFO]: Saving model to: blotchy-Amado_trial_117/model_best.pth
[08/28/2025 12:03:00 INFO]: Training loss at epoch 73: 0.780239999294281
[08/28/2025 12:03:03 INFO]: New best epoch, val score: -0.667392188642673
[08/28/2025 12:03:03 INFO]: Saving model to: blotchy-Amado_trial_127/model_best.pth
[08/28/2025 12:03:17 INFO]: Training loss at epoch 6: 1.304465264081955
[08/28/2025 12:03:43 INFO]: Training loss at epoch 10: 1.0665560364723206
[08/28/2025 12:04:05 INFO]: New best epoch, val score: -0.6623880187610973
[08/28/2025 12:04:05 INFO]: Saving model to: blotchy-Amado_trial_149/model_best.pth
[08/28/2025 12:04:13 INFO]: Training loss at epoch 74: 0.718350738286972
[08/28/2025 12:04:27 INFO]: Training loss at epoch 58: 0.9362443685531616
[08/28/2025 12:04:44 INFO]: Training stats: {
    "score": -0.8727902189374844,
    "rmse": 0.8727902189374844
}
[08/28/2025 12:04:44 INFO]: Val stats: {
    "score": -0.7146379021935612,
    "rmse": 0.7146379021935612
}
[08/28/2025 12:04:44 INFO]: Test stats: {
    "score": -0.944471003648483,
    "rmse": 0.944471003648483
}
[08/28/2025 12:05:17 INFO]: Training loss at epoch 24: 0.9750195741653442
[08/28/2025 12:05:23 INFO]: Training loss at epoch 75: 0.9300726354122162
[08/28/2025 12:05:31 INFO]: Training loss at epoch 34: 0.7622587531805038
[08/28/2025 12:05:41 INFO]: Training loss at epoch 39: 0.9519503712654114
[08/28/2025 12:06:08 INFO]: Training loss at epoch 2: 1.355734258890152
[08/28/2025 12:06:18 INFO]: Training loss at epoch 11: 0.9540217518806458
[08/28/2025 12:06:18 INFO]: Training loss at epoch 12: 1.005832314491272
[08/28/2025 12:06:19 INFO]: Running Final Evaluation...
[08/28/2025 12:06:23 INFO]: Training loss at epoch 59: 1.1584449410438538
[08/28/2025 12:06:33 INFO]: Training loss at epoch 76: 0.8507018089294434
[08/28/2025 12:06:33 INFO]: Training loss at epoch 9: 1.2941112518310547
[08/28/2025 12:06:52 INFO]: New best epoch, val score: -0.7611044794725714
[08/28/2025 12:06:52 INFO]: Saving model to: blotchy-Amado_trial_151/model_best.pth
[08/28/2025 12:07:00 INFO]: Training loss at epoch 11: 0.9303464889526367
[08/28/2025 12:07:03 INFO]: Training stats: {
    "score": -0.9856059237411554,
    "rmse": 0.9856059237411554
}
[08/28/2025 12:07:03 INFO]: Val stats: {
    "score": -0.6792131968590256,
    "rmse": 0.6792131968590256
}
[08/28/2025 12:07:03 INFO]: Test stats: {
    "score": -0.8926484671137788,
    "rmse": 0.8926484671137788
}
[08/28/2025 12:07:41 INFO]: Training loss at epoch 19: 1.013658583164215
[08/28/2025 12:07:43 INFO]: Training loss at epoch 77: 0.8870031237602234
[08/28/2025 12:08:18 INFO]: Training loss at epoch 9: 0.9313977360725403
[08/28/2025 12:08:33 INFO]: Training stats: {
    "score": -1.0972510956545025,
    "rmse": 1.0972510956545025
}
[08/28/2025 12:08:33 INFO]: Val stats: {
    "score": -0.92456392962432,
    "rmse": 0.92456392962432
}
[08/28/2025 12:08:33 INFO]: Test stats: {
    "score": -1.0554362905680348,
    "rmse": 1.0554362905680348
}
[08/28/2025 12:08:53 INFO]: Training accuracy: {
    "score": -1.014893789241734,
    "rmse": 1.014893789241734
}
[08/28/2025 12:08:53 INFO]: Val accuracy: {
    "score": -0.6735067208740936,
    "rmse": 0.6735067208740936
}
[08/28/2025 12:08:53 INFO]: Test accuracy: {
    "score": -0.8877274645135776,
    "rmse": 0.8877274645135776
}
[08/28/2025 12:08:53 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_129",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8877274645135776,
        "rmse": 0.8877274645135776
    },
    "train_stats": {
        "score": -1.014893789241734,
        "rmse": 1.014893789241734
    },
    "val_stats": {
        "score": -0.6735067208740936,
        "rmse": 0.6735067208740936
    }
}
[08/28/2025 12:08:53 INFO]: Procewss finished for trial blotchy-Amado_trial_129
[08/28/2025 12:08:53 INFO]: 
_________________________________________________

[08/28/2025 12:08:53 INFO]: train_net_for_optune.py main() running.
[08/28/2025 12:08:53 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 8
  d_ffn_factor: 1.5070448197379118
  attention_dropout: 0.061184550334435664
  ffn_dropout: 0.061184550334435664
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.519687790145545e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_152

[08/28/2025 12:08:53 INFO]: This ft_transformer has 13.604 million parameters.
[08/28/2025 12:08:53 INFO]: Training will start at epoch 0.
[08/28/2025 12:08:53 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 12:08:53 INFO]: Training loss at epoch 78: 0.7211893796920776
[08/28/2025 12:08:57 INFO]: Training loss at epoch 22: 1.0136989057064056
[08/28/2025 12:08:58 INFO]: Training loss at epoch 60: 1.0024231672286987
[08/28/2025 12:09:01 INFO]: Training loss at epoch 35: 0.9758581817150116
[08/28/2025 12:09:03 INFO]: Running Final Evaluation...
[08/28/2025 12:09:29 INFO]: Training loss at epoch 6: 0.9312407970428467
[08/28/2025 12:09:34 INFO]: Training accuracy: {
    "score": -0.9747726678279395,
    "rmse": 0.9747726678279395
}
[08/28/2025 12:09:34 INFO]: Val accuracy: {
    "score": -0.6629823585677059,
    "rmse": 0.6629823585677059
}
[08/28/2025 12:09:34 INFO]: Test accuracy: {
    "score": -0.876149335189282,
    "rmse": 0.876149335189282
}
[08/28/2025 12:09:34 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_142",
    "best_epoch": 47,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.876149335189282,
        "rmse": 0.876149335189282
    },
    "train_stats": {
        "score": -0.9747726678279395,
        "rmse": 0.9747726678279395
    },
    "val_stats": {
        "score": -0.6629823585677059,
        "rmse": 0.6629823585677059
    }
}
[08/28/2025 12:09:34 INFO]: Procewss finished for trial blotchy-Amado_trial_142
[08/28/2025 12:09:35 INFO]: 
_________________________________________________

[08/28/2025 12:09:35 INFO]: train_net_for_optune.py main() running.
[08/28/2025 12:09:35 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 2.0493812305580246
  attention_dropout: 0.17880823095862064
  ffn_dropout: 0.17880823095862064
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.7413638130901854e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_153

[08/28/2025 12:09:35 INFO]: This ft_transformer has 16.169 million parameters.
[08/28/2025 12:09:35 INFO]: Training will start at epoch 0.
[08/28/2025 12:09:35 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 12:09:39 INFO]: Training loss at epoch 7: 0.940243661403656
[08/28/2025 12:09:53 INFO]: Training stats: {
    "score": -0.9862072854221888,
    "rmse": 0.9862072854221888
}
[08/28/2025 12:09:53 INFO]: Val stats: {
    "score": -0.8204572429340925,
    "rmse": 0.8204572429340925
}
[08/28/2025 12:09:53 INFO]: Test stats: {
    "score": -0.9793890783281617,
    "rmse": 0.9793890783281617
}
[08/28/2025 12:10:00 INFO]: Training stats: {
    "score": -1.0014725582333883,
    "rmse": 1.0014725582333883
}
[08/28/2025 12:10:00 INFO]: Val stats: {
    "score": -0.7687781357777056,
    "rmse": 0.7687781357777056
}
[08/28/2025 12:10:00 INFO]: Test stats: {
    "score": -0.9279768941473703,
    "rmse": 0.9279768941473703
}
[08/28/2025 12:10:26 INFO]: Training loss at epoch 61: 0.8496837317943573
[08/28/2025 12:10:45 INFO]: Training stats: {
    "score": -0.996272625253151,
    "rmse": 0.996272625253151
}
[08/28/2025 12:10:45 INFO]: Val stats: {
    "score": -0.6641871610741998,
    "rmse": 0.6641871610741998
}
[08/28/2025 12:10:45 INFO]: Test stats: {
    "score": -0.8898109913841699,
    "rmse": 0.8898109913841699
}
[08/28/2025 12:10:53 INFO]: Training loss at epoch 30: 0.8480980098247528
[08/28/2025 12:10:55 INFO]: Training loss at epoch 61: 0.921914279460907
[08/28/2025 12:11:33 INFO]: Training loss at epoch 11: 0.8397663533687592
[08/28/2025 12:11:58 INFO]: New best epoch, val score: -0.6792409109468005
[08/28/2025 12:11:58 INFO]: Saving model to: blotchy-Amado_trial_117/model_best.pth
[08/28/2025 12:11:59 INFO]: Training loss at epoch 25: 0.9383367896080017
[08/28/2025 12:12:30 INFO]: Training loss at epoch 3: 1.0706045627593994
[08/28/2025 12:12:36 INFO]: Training loss at epoch 30: 1.1571056246757507
[08/28/2025 12:12:49 INFO]: Training loss at epoch 62: 1.028464913368225
[08/28/2025 12:13:15 INFO]: Training loss at epoch 13: 1.0083224773406982
[08/28/2025 12:13:36 INFO]: Training loss at epoch 12: 1.115186095237732
[08/28/2025 12:14:02 INFO]: New best epoch, val score: -0.6656213917834916
[08/28/2025 12:14:02 INFO]: Saving model to: blotchy-Amado_trial_143/model_best.pth
[08/28/2025 12:14:14 INFO]: Training loss at epoch 12: 1.1228204369544983
[08/28/2025 12:14:25 INFO]: Training loss at epoch 10: 1.0334390699863434
[08/28/2025 12:14:29 INFO]: Training loss at epoch 0: 1.0372492671012878
[08/28/2025 12:14:46 INFO]: Training loss at epoch 63: 0.9228253364562988
[08/28/2025 12:15:08 INFO]: New best epoch, val score: -0.6871716686155159
[08/28/2025 12:15:08 INFO]: Saving model to: blotchy-Amado_trial_148/model_best.pth
[08/28/2025 12:15:16 INFO]: New best epoch, val score: -0.720827643137366
[08/28/2025 12:15:16 INFO]: Saving model to: blotchy-Amado_trial_152/model_best.pth
[08/28/2025 12:15:48 INFO]: Training loss at epoch 36: 0.8203475177288055
[08/28/2025 12:16:00 INFO]: Training loss at epoch 8: 0.8562056720256805
[08/28/2025 12:16:28 INFO]: Training loss at epoch 7: 1.2421557009220123
[08/28/2025 12:16:33 INFO]: Training loss at epoch 0: 0.9606430232524872
[08/28/2025 12:16:34 INFO]: Training loss at epoch 20: 0.8123226761817932
[08/28/2025 12:16:40 INFO]: Training loss at epoch 64: 0.9286130666732788
[08/28/2025 12:16:46 INFO]: Training loss at epoch 23: 1.0887401700019836
[08/28/2025 12:17:27 INFO]: New best epoch, val score: -0.7291852185220619
[08/28/2025 12:17:27 INFO]: Saving model to: blotchy-Amado_trial_153/model_best.pth
[08/28/2025 12:17:53 INFO]: Training loss at epoch 10: 1.0788351893424988
[08/28/2025 12:18:35 INFO]: Training loss at epoch 65: 0.9636502265930176
[08/28/2025 12:18:38 INFO]: Training loss at epoch 62: 0.9707870781421661
[08/28/2025 12:18:44 INFO]: Training loss at epoch 26: 0.9928775429725647
[08/28/2025 12:18:51 INFO]: Training loss at epoch 4: 1.3482626676559448
[08/28/2025 12:19:26 INFO]: Training loss at epoch 12: 0.8427030444145203
[08/28/2025 12:19:37 INFO]: New best epoch, val score: -0.6754242461424769
[08/28/2025 12:19:37 INFO]: Saving model to: blotchy-Amado_trial_151/model_best.pth
[08/28/2025 12:20:10 INFO]: Training loss at epoch 14: 0.8455914855003357
[08/28/2025 12:20:16 INFO]: Training loss at epoch 31: 0.9879543483257294
[08/28/2025 12:20:20 INFO]: Training loss at epoch 11: 1.0991992354393005
[08/28/2025 12:20:32 INFO]: Training loss at epoch 31: 0.8515738248825073
[08/28/2025 12:20:32 INFO]: Training loss at epoch 66: 0.877669632434845
[08/28/2025 12:20:52 INFO]: Training loss at epoch 1: 1.0558240413665771
[08/28/2025 12:20:54 INFO]: Training loss at epoch 13: 1.414609968662262
[08/28/2025 12:21:28 INFO]: Training loss at epoch 13: 1.0958768129348755
[08/28/2025 12:21:37 INFO]: New best epoch, val score: -0.6950165455181692
[08/28/2025 12:21:37 INFO]: Saving model to: blotchy-Amado_trial_152/model_best.pth
[08/28/2025 12:22:05 INFO]: Training loss at epoch 40: 0.9010675251483917
[08/28/2025 12:22:17 INFO]: Training loss at epoch 9: 1.029647707939148
[08/28/2025 12:22:31 INFO]: Training loss at epoch 67: 0.9154098331928253
[08/28/2025 12:22:33 INFO]: Training loss at epoch 37: 0.8721318542957306
[08/28/2025 12:23:09 INFO]: Training loss at epoch 21: 1.0111551880836487
[08/28/2025 12:23:28 INFO]: Training loss at epoch 8: 0.9736298024654388
[08/28/2025 12:24:28 INFO]: Training stats: {
    "score": -0.9683506257660813,
    "rmse": 0.9683506257660813
}
[08/28/2025 12:24:28 INFO]: Val stats: {
    "score": -0.6753975948939672,
    "rmse": 0.6753975948939672
}
[08/28/2025 12:24:28 INFO]: Test stats: {
    "score": -0.8814008720572188,
    "rmse": 0.8814008720572188
}
[08/28/2025 12:24:32 INFO]: Training loss at epoch 1: 1.3777315020561218
[08/28/2025 12:24:34 INFO]: Training loss at epoch 68: 1.1287426352500916
[08/28/2025 12:24:39 INFO]: Training loss at epoch 24: 0.8398659229278564
[08/28/2025 12:25:08 INFO]: Training loss at epoch 11: 1.0974847674369812
[08/28/2025 12:25:20 INFO]: Training loss at epoch 5: 1.1497719287872314
[08/28/2025 12:25:33 INFO]: Training loss at epoch 27: 0.9372125864028931
[08/28/2025 12:26:18 INFO]: Training loss at epoch 12: 0.9787642359733582
[08/28/2025 12:26:30 INFO]: Training loss at epoch 69: 0.9845375716686249
[08/28/2025 12:26:54 INFO]: Training loss at epoch 63: 0.9162928760051727
[08/28/2025 12:27:11 INFO]: Training loss at epoch 15: 1.0422345101833344
[08/28/2025 12:27:13 INFO]: Training stats: {
    "score": -0.9982509279648203,
    "rmse": 0.9982509279648203
}
[08/28/2025 12:27:13 INFO]: Val stats: {
    "score": -0.7363159182331165,
    "rmse": 0.7363159182331165
}
[08/28/2025 12:27:13 INFO]: Test stats: {
    "score": -0.927452141514801,
    "rmse": 0.927452141514801
}
[08/28/2025 12:27:19 INFO]: Training loss at epoch 2: 0.9390954673290253
[08/28/2025 12:27:22 INFO]: Training loss at epoch 13: 1.100196897983551
[08/28/2025 12:28:16 INFO]: Training loss at epoch 14: 1.2207303643226624
[08/28/2025 12:28:26 INFO]: Training loss at epoch 32: 0.7620149552822113
[08/28/2025 12:28:42 INFO]: Training loss at epoch 14: 1.3733680844306946
[08/28/2025 12:29:08 INFO]: Training loss at epoch 70: 0.9631294310092926
[08/28/2025 12:29:21 INFO]: Training loss at epoch 38: 0.8354825675487518
[08/28/2025 12:29:43 INFO]: Training loss at epoch 22: 0.9794739782810211
[08/28/2025 12:29:47 INFO]: Training loss at epoch 32: 0.9955986440181732
[08/28/2025 12:30:28 INFO]: Training loss at epoch 9: 1.0485846400260925
[08/28/2025 12:30:49 INFO]: Training loss at epoch 10: 1.2729456424713135
[08/28/2025 12:31:04 INFO]: Training loss at epoch 71: 0.8474503457546234
[08/28/2025 12:31:43 INFO]: Training loss at epoch 6: 0.9158055484294891
[08/28/2025 12:32:12 INFO]: Training loss at epoch 13: 1.2498071789741516
[08/28/2025 12:32:17 INFO]: Training loss at epoch 28: 0.7205913066864014
[08/28/2025 12:32:21 INFO]: Training loss at epoch 12: 0.8350451588630676
[08/28/2025 12:32:28 INFO]: New best epoch, val score: -0.6719536358036551
[08/28/2025 12:32:28 INFO]: Saving model to: blotchy-Amado_trial_151/model_best.pth
[08/28/2025 12:32:30 INFO]: Training loss at epoch 25: 1.0419893562793732
[08/28/2025 12:32:30 INFO]: Training loss at epoch 2: 1.460733950138092
[08/28/2025 12:32:51 INFO]: Training stats: {
    "score": -0.9755274766078146,
    "rmse": 0.9755274766078146
}
[08/28/2025 12:32:51 INFO]: Val stats: {
    "score": -0.7146169323421953,
    "rmse": 0.7146169323421953
}
[08/28/2025 12:32:51 INFO]: Test stats: {
    "score": -0.9004375347076295,
    "rmse": 0.9004375347076295
}
[08/28/2025 12:33:03 INFO]: Training loss at epoch 72: 1.0292660892009735
[08/28/2025 12:33:38 INFO]: Training loss at epoch 3: 0.9164214730262756
[08/28/2025 12:34:07 INFO]: Training loss at epoch 16: 0.8358162939548492
[08/28/2025 12:34:16 INFO]: Training loss at epoch 41: 0.9853697419166565
[08/28/2025 12:34:58 INFO]: Training loss at epoch 73: 1.0357068181037903
[08/28/2025 12:35:07 INFO]: Training loss at epoch 64: 0.8057356178760529
[08/28/2025 12:35:11 INFO]: Training loss at epoch 14: 0.910075306892395
[08/28/2025 12:35:32 INFO]: Training loss at epoch 15: 1.143783688545227
[08/28/2025 12:35:56 INFO]: Training loss at epoch 15: 0.9885874688625336
[08/28/2025 12:36:07 INFO]: Training loss at epoch 39: 0.8363136053085327
[08/28/2025 12:36:13 INFO]: Training loss at epoch 23: 0.8849377036094666
[08/28/2025 12:36:16 INFO]: Training loss at epoch 33: 0.7729142606258392
[08/28/2025 12:36:51 INFO]: Training loss at epoch 74: 1.4293755292892456
[08/28/2025 12:37:04 INFO]: Training loss at epoch 11: 0.8198293745517731
[08/28/2025 12:38:00 INFO]: Training loss at epoch 14: 1.0131994485855103
[08/28/2025 12:38:03 INFO]: Training loss at epoch 7: 0.8830406367778778
[08/28/2025 12:38:25 INFO]: Training stats: {
    "score": -0.9552973209211295,
    "rmse": 0.9552973209211295
}
[08/28/2025 12:38:25 INFO]: Val stats: {
    "score": -0.7273405841918292,
    "rmse": 0.7273405841918292
}
[08/28/2025 12:38:25 INFO]: Test stats: {
    "score": -0.9232312891706266,
    "rmse": 0.9232312891706266
}
[08/28/2025 12:38:48 INFO]: Training loss at epoch 75: 0.9800289571285248
[08/28/2025 12:38:50 INFO]: New best epoch, val score: -0.667546958137722
[08/28/2025 12:38:50 INFO]: Saving model to: blotchy-Amado_trial_151/model_best.pth
[08/28/2025 12:38:59 INFO]: Training loss at epoch 29: 0.7841028869152069
[08/28/2025 12:39:06 INFO]: Training loss at epoch 33: 0.8589161932468414
[08/28/2025 12:39:32 INFO]: Training loss at epoch 13: 1.1229125261306763
[08/28/2025 12:39:51 INFO]: Training loss at epoch 10: 1.0945634841918945
[08/28/2025 12:40:03 INFO]: Training loss at epoch 4: 0.9476005136966705
[08/28/2025 12:40:17 INFO]: Training loss at epoch 26: 0.9630768597126007
[08/28/2025 12:40:24 INFO]: Training loss at epoch 3: 0.9878545999526978
[08/28/2025 12:40:39 INFO]: New best epoch, val score: -0.6913052435904858
[08/28/2025 12:40:39 INFO]: Saving model to: blotchy-Amado_trial_150/model_best.pth
[08/28/2025 12:40:45 INFO]: Training loss at epoch 76: 0.9250543415546417
[08/28/2025 12:41:05 INFO]: Training loss at epoch 17: 1.0085490345954895
[08/28/2025 12:41:17 INFO]: Training stats: {
    "score": -0.9210734928914618,
    "rmse": 0.9210734928914618
}
[08/28/2025 12:41:17 INFO]: Val stats: {
    "score": -0.6736379086888438,
    "rmse": 0.6736379086888438
}
[08/28/2025 12:41:17 INFO]: Test stats: {
    "score": -0.9048075035541596,
    "rmse": 0.9048075035541596
}
[08/28/2025 12:42:03 INFO]: New best epoch, val score: -0.6736379086888438
[08/28/2025 12:42:03 INFO]: Saving model to: blotchy-Amado_trial_132/model_best.pth
[08/28/2025 12:42:38 INFO]: Training loss at epoch 77: 1.0152494013309479
[08/28/2025 12:42:43 INFO]: Training loss at epoch 24: 0.9782899022102356
[08/28/2025 12:42:46 INFO]: Training loss at epoch 16: 0.860185444355011
[08/28/2025 12:42:57 INFO]: Training loss at epoch 15: 0.8573320508003235
[08/28/2025 12:43:05 INFO]: Training loss at epoch 16: 0.9475132822990417
[08/28/2025 12:43:16 INFO]: Training loss at epoch 65: 0.6988881975412369
[08/28/2025 12:43:24 INFO]: Training loss at epoch 12: 0.876045435667038
[08/28/2025 12:43:31 INFO]: New best epoch, val score: -0.6670834220864383
[08/28/2025 12:43:31 INFO]: Saving model to: blotchy-Amado_trial_137/model_best.pth
[08/28/2025 12:43:54 INFO]: Training loss at epoch 15: 1.487029790878296
[08/28/2025 12:44:05 INFO]: Training loss at epoch 34: 0.8357328772544861
[08/28/2025 12:44:25 INFO]: Training loss at epoch 8: 0.9818068444728851
[08/28/2025 12:44:34 INFO]: Training loss at epoch 78: 1.0304695963859558
[08/28/2025 12:45:07 INFO]: Training loss at epoch 40: 0.7447420060634613
[08/28/2025 12:45:08 INFO]: New best epoch, val score: -0.6632051258650846
[08/28/2025 12:45:08 INFO]: Saving model to: blotchy-Amado_trial_151/model_best.pth
[08/28/2025 12:46:17 INFO]: Training loss at epoch 5: 1.1345409154891968
[08/28/2025 12:46:21 INFO]: Training loss at epoch 42: 1.0538499057292938
[08/28/2025 12:46:27 INFO]: Training loss at epoch 79: 0.7965742349624634
[08/28/2025 12:46:37 INFO]: Training loss at epoch 14: 1.1007714867591858
[08/28/2025 12:46:43 INFO]: Training loss at epoch 11: 1.0639837384223938
[08/28/2025 12:47:10 INFO]: Training stats: {
    "score": -0.9918885102914959,
    "rmse": 0.9918885102914959
}
[08/28/2025 12:47:10 INFO]: Val stats: {
    "score": -0.7302428596902815,
    "rmse": 0.7302428596902815
}
[08/28/2025 12:47:10 INFO]: Test stats: {
    "score": -0.9231982593548371,
    "rmse": 0.9231982593548371
}
[08/28/2025 12:47:34 INFO]: New best epoch, val score: -0.687237271873934
[08/28/2025 12:47:34 INFO]: Saving model to: blotchy-Amado_trial_150/model_best.pth
[08/28/2025 12:47:56 INFO]: Training loss at epoch 18: 1.4107833504676819
[08/28/2025 12:47:57 INFO]: Training loss at epoch 30: 0.6711964905261993
[08/28/2025 12:47:59 INFO]: Training loss at epoch 27: 0.8389449417591095
[08/28/2025 12:48:11 INFO]: Training loss at epoch 4: 0.8573949635028839
[08/28/2025 12:48:21 INFO]: Training loss at epoch 34: 1.2152737081050873
[08/28/2025 12:49:02 INFO]: Training loss at epoch 80: 0.810186356306076
[08/28/2025 12:49:11 INFO]: Training loss at epoch 25: 0.8130143284797668
[08/28/2025 12:49:38 INFO]: Training loss at epoch 13: 0.813059538602829
[08/28/2025 12:49:39 INFO]: Training loss at epoch 16: 1.016021490097046
[08/28/2025 12:49:57 INFO]: Training loss at epoch 17: 1.1909922361373901
[08/28/2025 12:49:59 INFO]: New best epoch, val score: -0.6633665030869941
[08/28/2025 12:49:59 INFO]: Saving model to: blotchy-Amado_trial_137/model_best.pth
[08/28/2025 12:50:15 INFO]: Training loss at epoch 17: 1.0382658243179321
[08/28/2025 12:50:22 INFO]: New best epoch, val score: -0.6717392649665263
[08/28/2025 12:50:22 INFO]: Saving model to: blotchy-Amado_trial_148/model_best.pth
[08/28/2025 12:50:43 INFO]: Training loss at epoch 9: 0.8441807329654694
[08/28/2025 12:50:46 INFO]: Training loss at epoch 16: 0.8657931089401245
[08/28/2025 12:51:00 INFO]: Training loss at epoch 81: 0.9619258344173431
[08/28/2025 12:51:09 INFO]: New best epoch, val score: -0.6676686462740544
[08/28/2025 12:51:09 INFO]: Saving model to: blotchy-Amado_trial_146/model_best.pth
[08/28/2025 12:51:27 INFO]: Training loss at epoch 66: 0.8266954720020294
[08/28/2025 12:51:50 INFO]: Training loss at epoch 35: 0.9180307984352112
[08/28/2025 12:51:54 INFO]: Training loss at epoch 41: 0.9040181934833527
[08/28/2025 12:52:37 INFO]: Training loss at epoch 6: 1.2206459045410156
[08/28/2025 12:52:52 INFO]: Training stats: {
    "score": -1.0055392683795954,
    "rmse": 1.0055392683795954
}
[08/28/2025 12:52:52 INFO]: Val stats: {
    "score": -0.6595321515977146,
    "rmse": 0.6595321515977146
}
[08/28/2025 12:52:52 INFO]: Test stats: {
    "score": -0.8826702821009351,
    "rmse": 0.8826702821009351
}
[08/28/2025 12:52:53 INFO]: Training loss at epoch 82: 1.061353474855423
[08/28/2025 12:53:35 INFO]: New best epoch, val score: -0.6595321515977146
[08/28/2025 12:53:35 INFO]: Saving model to: blotchy-Amado_trial_151/model_best.pth
[08/28/2025 12:53:40 INFO]: Training loss at epoch 12: 1.26724773645401
[08/28/2025 12:53:45 INFO]: Training loss at epoch 15: 1.2161441445350647
[08/28/2025 12:54:38 INFO]: Training loss at epoch 31: 0.8752431571483612
[08/28/2025 12:54:47 INFO]: Training loss at epoch 83: 1.0022385716438293
[08/28/2025 12:54:49 INFO]: Training loss at epoch 19: 0.8538183569908142
[08/28/2025 12:55:31 INFO]: Training loss at epoch 17: 0.9307652711868286
[08/28/2025 12:55:43 INFO]: Training loss at epoch 26: 0.9736500680446625
[08/28/2025 12:55:47 INFO]: Training loss at epoch 28: 0.751298576593399
[08/28/2025 12:55:59 INFO]: Training loss at epoch 14: 1.0705686509609222
[08/28/2025 12:56:06 INFO]: Training loss at epoch 5: 0.8734793066978455
[08/28/2025 12:56:43 INFO]: New best epoch, val score: -0.6587192754636059
[08/28/2025 12:56:43 INFO]: Saving model to: blotchy-Amado_trial_149/model_best.pth
[08/28/2025 12:56:44 INFO]: Training loss at epoch 84: 0.9303456246852875
[08/28/2025 12:57:13 INFO]: Training loss at epoch 18: 0.7952570617198944
[08/28/2025 12:57:16 INFO]: Training stats: {
    "score": -0.968820704413226,
    "rmse": 0.968820704413226
}
[08/28/2025 12:57:16 INFO]: Val stats: {
    "score": -0.6901120777952324,
    "rmse": 0.6901120777952324
}
[08/28/2025 12:57:16 INFO]: Test stats: {
    "score": -0.8925457684293253,
    "rmse": 0.8925457684293253
}
[08/28/2025 12:57:27 INFO]: Training loss at epoch 18: 0.9249912798404694
[08/28/2025 12:57:41 INFO]: Training loss at epoch 35: 0.9368821382522583
[08/28/2025 12:58:30 INFO]: Training loss at epoch 43: 0.9132616221904755
[08/28/2025 12:58:36 INFO]: Training loss at epoch 17: 0.8755196034908295
[08/28/2025 12:58:40 INFO]: Training loss at epoch 42: 0.9702044129371643
[08/28/2025 12:58:42 INFO]: Training loss at epoch 85: 1.2334464192390442
[08/28/2025 12:58:56 INFO]: Running Final Evaluation...
[08/28/2025 12:58:58 INFO]: Training loss at epoch 7: 1.1295573115348816
[08/28/2025 12:59:14 INFO]: Training loss at epoch 10: 1.0808481574058533
[08/28/2025 12:59:36 INFO]: Training loss at epoch 36: 1.0069052577018738
[08/28/2025 12:59:38 INFO]: Training loss at epoch 67: 0.7685190439224243
[08/28/2025 12:59:43 INFO]: Training accuracy: {
    "score": -0.9887948091596221,
    "rmse": 0.9887948091596221
}
[08/28/2025 12:59:43 INFO]: Val accuracy: {
    "score": -0.666996500537824,
    "rmse": 0.666996500537824
}
[08/28/2025 12:59:43 INFO]: Test accuracy: {
    "score": -0.8876787619468651,
    "rmse": 0.8876787619468651
}
[08/28/2025 12:59:43 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_139",
    "best_epoch": 54,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8876787619468651,
        "rmse": 0.8876787619468651
    },
    "train_stats": {
        "score": -0.9887948091596221,
        "rmse": 0.9887948091596221
    },
    "val_stats": {
        "score": -0.666996500537824,
        "rmse": 0.666996500537824
    }
}
[08/28/2025 12:59:43 INFO]: Procewss finished for trial blotchy-Amado_trial_139
[08/28/2025 12:59:44 INFO]: 
_________________________________________________

[08/28/2025 12:59:44 INFO]: train_net_for_optune.py main() running.
[08/28/2025 12:59:44 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 2.067050574310333
  attention_dropout: 0.06345987105931025
  ffn_dropout: 0.06345987105931025
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.4934869130515273e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_154

[08/28/2025 12:59:44 INFO]: This ft_transformer has 16.252 million parameters.
[08/28/2025 12:59:44 INFO]: Training will start at epoch 0.
[08/28/2025 12:59:44 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 13:00:38 INFO]: Training loss at epoch 13: 1.1304412484169006
[08/28/2025 13:00:55 INFO]: Training loss at epoch 16: 1.1401135921478271
[08/28/2025 13:01:22 INFO]: Training loss at epoch 32: 0.8380284309387207
[08/28/2025 13:01:23 INFO]: Training loss at epoch 18: 1.0995984375476837
[08/28/2025 13:02:12 INFO]: Training loss at epoch 27: 0.9567258358001709
[08/28/2025 13:02:16 INFO]: Training loss at epoch 15: 0.8762712478637695
[08/28/2025 13:03:33 INFO]: Training loss at epoch 29: 0.7683943510055542
[08/28/2025 13:03:59 INFO]: Training loss at epoch 6: 0.8957809209823608
[08/28/2025 13:04:09 INFO]: Training loss at epoch 20: 1.1741218566894531
[08/28/2025 13:04:26 INFO]: Training loss at epoch 19: 1.0917853713035583
[08/28/2025 13:04:38 INFO]: Training loss at epoch 19: 0.9357093274593353
[08/28/2025 13:05:16 INFO]: Training loss at epoch 8: 1.040688395500183
[08/28/2025 13:05:22 INFO]: Training loss at epoch 43: 0.9048584401607513
[08/28/2025 13:05:33 INFO]: Training loss at epoch 11: 1.0645865201950073
[08/28/2025 13:06:03 INFO]: New best epoch, val score: -0.691434261559515
[08/28/2025 13:06:03 INFO]: Saving model to: blotchy-Amado_trial_152/model_best.pth
[08/28/2025 13:06:16 INFO]: Training stats: {
    "score": -0.9672740167054014,
    "rmse": 0.9672740167054014
}
[08/28/2025 13:06:16 INFO]: Val stats: {
    "score": -0.7140109121770271,
    "rmse": 0.7140109121770271
}
[08/28/2025 13:06:16 INFO]: Test stats: {
    "score": -0.901672551117033,
    "rmse": 0.901672551117033
}
[08/28/2025 13:06:27 INFO]: Training loss at epoch 18: 0.9297178685665131
[08/28/2025 13:06:42 INFO]: Training loss at epoch 0: 1.071641445159912
[08/28/2025 13:06:52 INFO]: Training stats: {
    "score": -0.9805463648683269,
    "rmse": 0.9805463648683269
}
[08/28/2025 13:06:52 INFO]: Val stats: {
    "score": -0.7109955689187444,
    "rmse": 0.7109955689187444
}
[08/28/2025 13:06:52 INFO]: Test stats: {
    "score": -0.8771894100016505,
    "rmse": 0.8771894100016505
}
[08/28/2025 13:07:02 INFO]: Training loss at epoch 36: 1.0681679248809814
[08/28/2025 13:07:02 INFO]: Training stats: {
    "score": -0.9913342908903628,
    "rmse": 0.9913342908903628
}
[08/28/2025 13:07:02 INFO]: Val stats: {
    "score": -0.7299439782749947,
    "rmse": 0.7299439782749947
}
[08/28/2025 13:07:02 INFO]: Test stats: {
    "score": -0.9199865231854257,
    "rmse": 0.9199865231854257
}
[08/28/2025 13:07:15 INFO]: Training loss at epoch 19: 1.4400080442428589
[08/28/2025 13:07:24 INFO]: Training loss at epoch 37: 0.7460960149765015
[08/28/2025 13:07:33 INFO]: Training loss at epoch 14: 1.0825608968734741
[08/28/2025 13:07:39 INFO]: New best epoch, val score: -0.7222755606321228
[08/28/2025 13:07:39 INFO]: Saving model to: blotchy-Amado_trial_154/model_best.pth
[08/28/2025 13:07:52 INFO]: Training loss at epoch 68: 0.9509096443653107
[08/28/2025 13:08:06 INFO]: Training loss at epoch 17: 0.9632130265235901
[08/28/2025 13:08:07 INFO]: Training loss at epoch 33: 0.9543474912643433
[08/28/2025 13:08:21 INFO]: Running Final Evaluation...
[08/28/2025 13:08:32 INFO]: Training loss at epoch 16: 0.9224317073822021
[08/28/2025 13:08:44 INFO]: Training loss at epoch 28: 1.1147211492061615
[08/28/2025 13:09:13 INFO]: Training stats: {
    "score": -1.01377960841102,
    "rmse": 1.01377960841102
}
[08/28/2025 13:09:13 INFO]: Val stats: {
    "score": -0.7890876295642145,
    "rmse": 0.7890876295642145
}
[08/28/2025 13:09:13 INFO]: Test stats: {
    "score": -0.9516600238888855,
    "rmse": 0.9516600238888855
}
[08/28/2025 13:10:31 INFO]: Training loss at epoch 44: 0.7915865182876587
[08/28/2025 13:11:06 INFO]: Training loss at epoch 21: 0.8753386735916138
[08/28/2025 13:11:32 INFO]: Training accuracy: {
    "score": -0.9686915045268478,
    "rmse": 0.9686915045268478
}
[08/28/2025 13:11:32 INFO]: Val accuracy: {
    "score": -0.6628459120934547,
    "rmse": 0.6628459120934547
}
[08/28/2025 13:11:32 INFO]: Test accuracy: {
    "score": -0.885165152890475,
    "rmse": 0.885165152890475
}
[08/28/2025 13:11:32 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_128",
    "best_epoch": 6,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.885165152890475,
        "rmse": 0.885165152890475
    },
    "train_stats": {
        "score": -0.9686915045268478,
        "rmse": 0.9686915045268478
    },
    "val_stats": {
        "score": -0.6628459120934547,
        "rmse": 0.6628459120934547
    }
}
[08/28/2025 13:11:32 INFO]: Procewss finished for trial blotchy-Amado_trial_128
[08/28/2025 13:11:32 INFO]: 
_________________________________________________

[08/28/2025 13:11:32 INFO]: train_net_for_optune.py main() running.
[08/28/2025 13:11:32 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 1.2382634362880505
  attention_dropout: 0.10292157731141444
  ffn_dropout: 0.10292157731141444
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.4783245103743168e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_155

[08/28/2025 13:11:33 INFO]: This ft_transformer has 12.330 million parameters.
[08/28/2025 13:11:33 INFO]: Training will start at epoch 0.
[08/28/2025 13:11:33 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 13:11:40 INFO]: Training loss at epoch 9: 0.851396918296814
[08/28/2025 13:11:51 INFO]: Training loss at epoch 7: 1.1011289954185486
[08/28/2025 13:11:51 INFO]: Training loss at epoch 12: 0.9541095197200775
[08/28/2025 13:12:06 INFO]: Training loss at epoch 44: 0.7427529990673065
[08/28/2025 13:13:47 INFO]: Training stats: {
    "score": -0.984858584497814,
    "rmse": 0.984858584497814
}
[08/28/2025 13:13:47 INFO]: Val stats: {
    "score": -0.6814760029154755,
    "rmse": 0.6814760029154755
}
[08/28/2025 13:13:47 INFO]: Test stats: {
    "score": -0.9181605311184878,
    "rmse": 0.9181605311184878
}
[08/28/2025 13:13:59 INFO]: Training loss at epoch 30: 0.9457969963550568
[08/28/2025 13:14:06 INFO]: Training loss at epoch 20: 0.9814320504665375
[08/28/2025 13:14:10 INFO]: Training loss at epoch 20: 1.0006832480430603
[08/28/2025 13:14:15 INFO]: Training loss at epoch 19: 0.9483009576797485
[08/28/2025 13:14:31 INFO]: Training loss at epoch 15: 1.1327935755252838
[08/28/2025 13:14:33 INFO]: Training loss at epoch 1: 0.9386675357818604
[08/28/2025 13:14:33 INFO]: New best epoch, val score: -0.6814760029154755
[08/28/2025 13:14:33 INFO]: Saving model to: blotchy-Amado_trial_152/model_best.pth
[08/28/2025 13:14:45 INFO]: Training loss at epoch 17: 0.9831071197986603
[08/28/2025 13:14:48 INFO]: Training loss at epoch 34: 1.002504676580429
[08/28/2025 13:15:06 INFO]: Training loss at epoch 20: 1.1062151789665222
[08/28/2025 13:15:12 INFO]: Training loss at epoch 29: 1.053754597902298
[08/28/2025 13:15:13 INFO]: Training loss at epoch 18: 1.0495626628398895
[08/28/2025 13:15:30 INFO]: New best epoch, val score: -0.6757041993440076
[08/28/2025 13:15:30 INFO]: Saving model to: blotchy-Amado_trial_154/model_best.pth
[08/28/2025 13:16:02 INFO]: Training loss at epoch 69: 0.8408902287483215
[08/28/2025 13:16:25 INFO]: Training loss at epoch 37: 0.9648732841014862
[08/28/2025 13:16:55 INFO]: Training stats: {
    "score": -0.9281221686609197,
    "rmse": 0.9281221686609197
}
[08/28/2025 13:16:55 INFO]: Val stats: {
    "score": -0.704429675431718,
    "rmse": 0.704429675431718
}
[08/28/2025 13:16:55 INFO]: Test stats: {
    "score": -0.893323450659657,
    "rmse": 0.893323450659657
}
[08/28/2025 13:17:13 INFO]: Training loss at epoch 0: 1.097739815711975
[08/28/2025 13:17:26 INFO]: Training stats: {
    "score": -0.9628490271692322,
    "rmse": 0.9628490271692322
}
[08/28/2025 13:17:26 INFO]: Val stats: {
    "score": -0.6968373417650623,
    "rmse": 0.6968373417650623
}
[08/28/2025 13:17:26 INFO]: Test stats: {
    "score": -0.89100038579944,
    "rmse": 0.89100038579944
}
[08/28/2025 13:17:55 INFO]: Training loss at epoch 22: 1.047627478837967
[08/28/2025 13:17:58 INFO]: New best epoch, val score: -0.8055827816990933
[08/28/2025 13:17:58 INFO]: Saving model to: blotchy-Amado_trial_155/model_best.pth
[08/28/2025 13:18:09 INFO]: Training loss at epoch 13: 0.9201768934726715
[08/28/2025 13:18:46 INFO]: Training stats: {
    "score": -0.9091613529728034,
    "rmse": 0.9091613529728034
}
[08/28/2025 13:18:46 INFO]: Val stats: {
    "score": -0.6922462077330761,
    "rmse": 0.6922462077330761
}
[08/28/2025 13:18:46 INFO]: Test stats: {
    "score": -0.9094514171598024,
    "rmse": 0.9094514171598024
}
[08/28/2025 13:18:48 INFO]: Training loss at epoch 45: 0.8788344264030457
[08/28/2025 13:19:40 INFO]: Training loss at epoch 8: 0.9405632615089417
[08/28/2025 13:20:08 INFO]: Training loss at epoch 10: 1.021266132593155
[08/28/2025 13:20:54 INFO]: Training loss at epoch 21: 0.9372990429401398
[08/28/2025 13:20:59 INFO]: Training loss at epoch 18: 0.8379742205142975
[08/28/2025 13:21:16 INFO]: Training loss at epoch 21: 1.0363908112049103
[08/28/2025 13:21:18 INFO]: Training loss at epoch 21: 1.37272110581398
[08/28/2025 13:21:28 INFO]: Training loss at epoch 16: 0.7871959209442139
[08/28/2025 13:21:28 INFO]: Training loss at epoch 35: 0.9108400344848633
[08/28/2025 13:21:42 INFO]: Training loss at epoch 31: 0.8538002967834473
[08/28/2025 13:21:45 INFO]: New best epoch, val score: -0.6557232978359311
[08/28/2025 13:21:45 INFO]: Saving model to: blotchy-Amado_trial_149/model_best.pth
[08/28/2025 13:22:18 INFO]: Training loss at epoch 19: 1.18435999751091
[08/28/2025 13:22:25 INFO]: Training loss at epoch 2: 1.0931943356990814
[08/28/2025 13:22:30 INFO]: Training loss at epoch 45: 1.187028706073761
[08/28/2025 13:23:36 INFO]: Training loss at epoch 1: 1.006914883852005
[08/28/2025 13:23:50 INFO]: Training loss at epoch 30: 0.8069817423820496
[08/28/2025 13:24:21 INFO]: New best epoch, val score: -0.7710469249734262
[08/28/2025 13:24:21 INFO]: Saving model to: blotchy-Amado_trial_155/model_best.pth
[08/28/2025 13:24:24 INFO]: Training loss at epoch 14: 1.0558043718338013
[08/28/2025 13:24:36 INFO]: Training loss at epoch 20: 0.9257249236106873
[08/28/2025 13:24:37 INFO]: Training stats: {
    "score": -0.987961540294148,
    "rmse": 0.987961540294148
}
[08/28/2025 13:24:37 INFO]: Val stats: {
    "score": -0.6962903240455985,
    "rmse": 0.6962903240455985
}
[08/28/2025 13:24:37 INFO]: Test stats: {
    "score": -0.9100439219783539,
    "rmse": 0.9100439219783539
}
[08/28/2025 13:24:42 INFO]: Training loss at epoch 23: 0.7373378872871399
[08/28/2025 13:25:26 INFO]: Training loss at epoch 46: 0.958814799785614
[08/28/2025 13:25:37 INFO]: Training loss at epoch 38: 0.9036833345890045
[08/28/2025 13:26:21 INFO]: Training loss at epoch 11: 1.0609389245510101
[08/28/2025 13:26:41 INFO]: Training loss at epoch 22: 0.8845746219158173
[08/28/2025 13:26:48 INFO]: Training loss at epoch 70: 0.8750919103622437
[08/28/2025 13:27:12 INFO]: Training loss at epoch 19: 0.9163826704025269
[08/28/2025 13:27:28 INFO]: Training loss at epoch 9: 0.8445104658603668
[08/28/2025 13:28:09 INFO]: Training loss at epoch 36: 0.8501288294792175
[08/28/2025 13:28:21 INFO]: Training loss at epoch 22: 1.2659681737422943
[08/28/2025 13:28:23 INFO]: Training loss at epoch 17: 0.9806111454963684
[08/28/2025 13:28:25 INFO]: Training loss at epoch 22: 1.1377230882644653
[08/28/2025 13:29:17 INFO]: Training stats: {
    "score": -0.9483959159757203,
    "rmse": 0.9483959159757203
}
[08/28/2025 13:29:17 INFO]: Val stats: {
    "score": -0.6584998047923222,
    "rmse": 0.6584998047923222
}
[08/28/2025 13:29:17 INFO]: Test stats: {
    "score": -0.8892015734063092,
    "rmse": 0.8892015734063092
}
[08/28/2025 13:29:23 INFO]: Training loss at epoch 32: 1.1665554642677307
[08/28/2025 13:29:59 INFO]: Training loss at epoch 2: 1.284318208694458
[08/28/2025 13:30:08 INFO]: Training stats: {
    "score": -0.9817537027245054,
    "rmse": 0.9817537027245054
}
[08/28/2025 13:30:08 INFO]: Val stats: {
    "score": -0.7444369072920911,
    "rmse": 0.7444369072920911
}
[08/28/2025 13:30:08 INFO]: Test stats: {
    "score": -0.9301551546581874,
    "rmse": 0.9301551546581874
}
[08/28/2025 13:30:10 INFO]: Training loss at epoch 3: 0.8005486726760864
[08/28/2025 13:30:14 INFO]: Training loss at epoch 31: 0.9188088178634644
[08/28/2025 13:30:38 INFO]: Training loss at epoch 15: 1.0660583972930908
[08/28/2025 13:30:43 INFO]: New best epoch, val score: -0.750440917151985
[08/28/2025 13:30:43 INFO]: Saving model to: blotchy-Amado_trial_155/model_best.pth
[08/28/2025 13:31:04 INFO]: New best epoch, val score: -0.6677012838609684
[08/28/2025 13:31:04 INFO]: Saving model to: blotchy-Amado_trial_154/model_best.pth
[08/28/2025 13:31:25 INFO]: New best epoch, val score: -0.6521185098767209
[08/28/2025 13:31:25 INFO]: Saving model to: blotchy-Amado_trial_151/model_best.pth
[08/28/2025 13:31:33 INFO]: Training loss at epoch 24: 0.7518540620803833
[08/28/2025 13:31:41 INFO]: Training loss at epoch 20: 1.2314963638782501
[08/28/2025 13:32:04 INFO]: Training loss at epoch 47: 0.8729256689548492
[08/28/2025 13:32:20 INFO]: Training loss at epoch 21: 0.8890049457550049
[08/28/2025 13:32:28 INFO]: Training loss at epoch 23: 1.104480504989624
[08/28/2025 13:32:38 INFO]: Training loss at epoch 12: 1.206122100353241
[08/28/2025 13:34:22 INFO]: Training loss at epoch 46: 1.0048327445983887
[08/28/2025 13:34:48 INFO]: Training loss at epoch 37: 0.7433993816375732
[08/28/2025 13:34:49 INFO]: Training loss at epoch 39: 0.9392751455307007
[08/28/2025 13:34:52 INFO]: Training loss at epoch 71: 0.7921674847602844
[08/28/2025 13:35:20 INFO]: Training loss at epoch 18: 0.8838335871696472
[08/28/2025 13:35:27 INFO]: Training loss at epoch 23: 0.9780347645282745
[08/28/2025 13:35:33 INFO]: Training loss at epoch 20: 0.8351597189903259
[08/28/2025 13:35:37 INFO]: Training loss at epoch 23: 0.9523389041423798
[08/28/2025 13:35:51 INFO]: Running Final Evaluation...
[08/28/2025 13:36:23 INFO]: Training loss at epoch 3: 0.8923714458942413
[08/28/2025 13:36:40 INFO]: Training loss at epoch 32: 0.8947155177593231
[08/28/2025 13:36:57 INFO]: Training loss at epoch 16: 0.9077650606632233
[08/28/2025 13:37:10 INFO]: New best epoch, val score: -0.7156233446658591
[08/28/2025 13:37:10 INFO]: Saving model to: blotchy-Amado_trial_155/model_best.pth
[08/28/2025 13:37:10 INFO]: Training loss at epoch 33: 0.8270240724086761
[08/28/2025 13:37:57 INFO]: Training loss at epoch 10: 0.7716926038265228
[08/28/2025 13:37:58 INFO]: Training stats: {
    "score": -0.9673294823288111,
    "rmse": 0.9673294823288111
}
[08/28/2025 13:37:58 INFO]: Val stats: {
    "score": -0.7203653552435135,
    "rmse": 0.7203653552435135
}
[08/28/2025 13:37:58 INFO]: Test stats: {
    "score": -0.9035672184852047,
    "rmse": 0.9035672184852047
}
[08/28/2025 13:37:59 INFO]: Training loss at epoch 4: 0.9494969546794891
[08/28/2025 13:38:12 INFO]: Training loss at epoch 24: 0.9383760392665863
[08/28/2025 13:38:23 INFO]: Training loss at epoch 25: 1.1203098595142365
[08/28/2025 13:38:44 INFO]: Training loss at epoch 48: 0.9123056530952454
[08/28/2025 13:38:45 INFO]: Training loss at epoch 21: 1.288576602935791
[08/28/2025 13:38:48 INFO]: Training accuracy: {
    "score": -0.9574513297018314,
    "rmse": 0.9574513297018314
}
[08/28/2025 13:38:48 INFO]: Val accuracy: {
    "score": -0.6666749918829634,
    "rmse": 0.6666749918829634
}
[08/28/2025 13:38:48 INFO]: Test accuracy: {
    "score": -0.8786154238236945,
    "rmse": 0.8786154238236945
}
[08/28/2025 13:38:48 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_98",
    "best_epoch": 40,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8786154238236945,
        "rmse": 0.8786154238236945
    },
    "train_stats": {
        "score": -0.9574513297018314,
        "rmse": 0.9574513297018314
    },
    "val_stats": {
        "score": -0.6666749918829634,
        "rmse": 0.6666749918829634
    }
}
[08/28/2025 13:38:48 INFO]: Procewss finished for trial blotchy-Amado_trial_98
[08/28/2025 13:38:49 INFO]: 
_________________________________________________

[08/28/2025 13:38:49 INFO]: train_net_for_optune.py main() running.
[08/28/2025 13:38:49 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 16
  d_ffn_factor: 2.0343324209142155
  attention_dropout: 0.05914812264120731
  ffn_dropout: 0.05914812264120731
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.4891903977148584e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_156

[08/28/2025 13:38:49 INFO]: This ft_transformer has 26.712 million parameters.
[08/28/2025 13:38:49 INFO]: Training will start at epoch 0.
[08/28/2025 13:38:49 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 13:38:52 INFO]: Training loss at epoch 13: 0.9909743964672089
[08/28/2025 13:38:56 INFO]: New best epoch, val score: -0.6880449588867309
[08/28/2025 13:38:56 INFO]: Saving model to: blotchy-Amado_trial_153/model_best.pth
[08/28/2025 13:40:08 INFO]: Training loss at epoch 22: 0.8802385032176971
[08/28/2025 13:41:26 INFO]: Training loss at epoch 38: 0.7308003902435303
[08/28/2025 13:41:45 INFO]: Training loss at epoch 21: 1.0044310092926025
[08/28/2025 13:42:11 INFO]: Training loss at epoch 19: 0.840961366891861
[08/28/2025 13:42:32 INFO]: Training loss at epoch 24: 0.8256503045558929
[08/28/2025 13:42:46 INFO]: Training loss at epoch 24: 1.0009453892707825
[08/28/2025 13:42:48 INFO]: Training loss at epoch 4: 1.1546351611614227
[08/28/2025 13:43:08 INFO]: Training loss at epoch 33: 0.9344010055065155
[08/28/2025 13:43:12 INFO]: Training loss at epoch 17: 0.9465625882148743
[08/28/2025 13:43:22 INFO]: New best epoch, val score: -0.6646325011232781
[08/28/2025 13:43:22 INFO]: Saving model to: blotchy-Amado_trial_146/model_best.pth
[08/28/2025 13:43:57 INFO]: Training loss at epoch 25: 0.9094443023204803
[08/28/2025 13:44:31 INFO]: Training stats: {
    "score": -0.9612025515885587,
    "rmse": 0.9612025515885587
}
[08/28/2025 13:44:31 INFO]: Val stats: {
    "score": -0.7074407547084952,
    "rmse": 0.7074407547084952
}
[08/28/2025 13:44:31 INFO]: Test stats: {
    "score": -0.8995954562294377,
    "rmse": 0.8995954562294377
}
[08/28/2025 13:44:55 INFO]: Training loss at epoch 34: 1.0847108960151672
[08/28/2025 13:45:11 INFO]: Training loss at epoch 26: 0.9750398099422455
[08/28/2025 13:45:13 INFO]: Training loss at epoch 14: 0.9038759171962738
[08/28/2025 13:45:32 INFO]: Training loss at epoch 49: 0.7633189558982849
[08/28/2025 13:45:49 INFO]: Training loss at epoch 5: 0.9856722056865692
[08/28/2025 13:45:49 INFO]: Training loss at epoch 22: 1.06644606590271
[08/28/2025 13:45:56 INFO]: Training loss at epoch 11: 0.9989687204360962
[08/28/2025 13:46:23 INFO]: Training loss at epoch 47: 0.7841424942016602
[08/28/2025 13:47:20 INFO]: Training loss at epoch 40: 0.8364971280097961
[08/28/2025 13:47:48 INFO]: Training stats: {
    "score": -0.9235539175186305,
    "rmse": 0.9235539175186305
}
[08/28/2025 13:47:48 INFO]: Val stats: {
    "score": -0.6784429365236376,
    "rmse": 0.6784429365236376
}
[08/28/2025 13:47:48 INFO]: Test stats: {
    "score": -0.9039744491749384,
    "rmse": 0.9039744491749384
}
[08/28/2025 13:47:57 INFO]: Training loss at epoch 23: 0.9014774262905121
[08/28/2025 13:47:59 INFO]: Training loss at epoch 22: 0.8453198969364166
[08/28/2025 13:48:05 INFO]: Training loss at epoch 39: 0.841646283864975
[08/28/2025 13:49:12 INFO]: Training loss at epoch 5: 1.0002435445785522
[08/28/2025 13:49:28 INFO]: Training loss at epoch 18: 0.9803022742271423
[08/28/2025 13:49:35 INFO]: Training loss at epoch 34: 1.0507481098175049
[08/28/2025 13:49:40 INFO]: Training loss at epoch 25: 0.9781546890735626
[08/28/2025 13:49:45 INFO]: Training loss at epoch 26: 1.213678926229477
[08/28/2025 13:49:54 INFO]: Training loss at epoch 25: 1.004533290863037
[08/28/2025 13:50:11 INFO]: New best epoch, val score: -0.6501290887671846
[08/28/2025 13:50:11 INFO]: Saving model to: blotchy-Amado_trial_151/model_best.pth
[08/28/2025 13:50:20 INFO]: Training stats: {
    "score": -0.8884301678245942,
    "rmse": 0.8884301678245942
}
[08/28/2025 13:50:20 INFO]: Val stats: {
    "score": -0.693868105909884,
    "rmse": 0.693868105909884
}
[08/28/2025 13:50:20 INFO]: Test stats: {
    "score": -0.9312314906436514,
    "rmse": 0.9312314906436514
}
[08/28/2025 13:51:04 INFO]: Training loss at epoch 0: 1.3991777896881104
[08/28/2025 13:51:28 INFO]: Training loss at epoch 20: 1.1530379951000214
[08/28/2025 13:51:31 INFO]: Training loss at epoch 15: 0.886235922574997
[08/28/2025 13:52:04 INFO]: Training loss at epoch 27: 0.6993819922208786
[08/28/2025 13:52:42 INFO]: Training loss at epoch 35: 0.8260307610034943
[08/28/2025 13:52:47 INFO]: New best epoch, val score: -0.6780762234764569
[08/28/2025 13:52:47 INFO]: Saving model to: blotchy-Amado_trial_156/model_best.pth
[08/28/2025 13:52:59 INFO]: Training loss at epoch 23: 1.174282968044281
[08/28/2025 13:53:42 INFO]: Training loss at epoch 6: 1.1385573148727417
[08/28/2025 13:53:52 INFO]: Training loss at epoch 12: 1.0124980807304382
[08/28/2025 13:54:15 INFO]: Training loss at epoch 23: 1.0149057507514954
[08/28/2025 13:54:32 INFO]: Training loss at epoch 50: 0.8099707663059235
[08/28/2025 13:54:49 INFO]: New best epoch, val score: -0.6867056744654779
[08/28/2025 13:54:49 INFO]: Saving model to: blotchy-Amado_trial_153/model_best.pth
[08/28/2025 13:55:38 INFO]: Training loss at epoch 27: 1.0418224036693573
[08/28/2025 13:55:41 INFO]: Training loss at epoch 6: 1.0566263794898987
[08/28/2025 13:55:44 INFO]: Training loss at epoch 24: 0.9713373780250549
[08/28/2025 13:55:50 INFO]: Training loss at epoch 19: 1.034660965204239
[08/28/2025 13:56:09 INFO]: Training loss at epoch 35: 0.9712164402008057
[08/28/2025 13:56:42 INFO]: Training loss at epoch 41: 0.9938644170761108
[08/28/2025 13:56:53 INFO]: Training loss at epoch 26: 0.9731768369674683
[08/28/2025 13:57:09 INFO]: Training loss at epoch 40: 0.7151256203651428
[08/28/2025 13:57:11 INFO]: Training loss at epoch 26: 0.9975065290927887
[08/28/2025 13:57:54 INFO]: Training loss at epoch 16: 0.8798947930335999
[08/28/2025 13:57:59 INFO]: Training stats: {
    "score": -0.9715522977794269,
    "rmse": 0.9715522977794269
}
[08/28/2025 13:57:59 INFO]: Val stats: {
    "score": -0.6524747280675997,
    "rmse": 0.6524747280675997
}
[08/28/2025 13:57:59 INFO]: Test stats: {
    "score": -0.8734458010558283,
    "rmse": 0.8734458010558283
}
[08/28/2025 13:58:26 INFO]: Training loss at epoch 48: 1.0108774900436401
[08/28/2025 13:58:27 INFO]: Training loss at epoch 21: 0.9080308079719543
[08/28/2025 13:58:40 INFO]: New best epoch, val score: -0.6772557544415817
[08/28/2025 13:58:40 INFO]: Saving model to: blotchy-Amado_trial_152/model_best.pth
[08/28/2025 13:59:06 INFO]: Training loss at epoch 28: 1.0097348093986511
[08/28/2025 14:00:09 INFO]: Training loss at epoch 24: 1.0176032781600952
[08/28/2025 14:00:34 INFO]: Training loss at epoch 24: 0.6512661427259445
[08/28/2025 14:00:37 INFO]: Training loss at epoch 36: 0.9540820717811584
[08/28/2025 14:01:22 INFO]: Training loss at epoch 51: 1.091920793056488
[08/28/2025 14:01:34 INFO]: Training loss at epoch 28: 1.1136526465415955
[08/28/2025 14:01:40 INFO]: Training loss at epoch 7: 0.9864358901977539
[08/28/2025 14:01:52 INFO]: Training loss at epoch 13: 0.9352457821369171
[08/28/2025 14:02:14 INFO]: Training loss at epoch 7: 0.9830274283885956
[08/28/2025 14:02:46 INFO]: Training loss at epoch 36: 0.9535074830055237
[08/28/2025 14:02:49 INFO]: New best epoch, val score: -0.6852947483150896
[08/28/2025 14:02:49 INFO]: Saving model to: blotchy-Amado_trial_153/model_best.pth
[08/28/2025 14:03:36 INFO]: Training loss at epoch 25: 0.7849124073982239
[08/28/2025 14:03:55 INFO]: Training loss at epoch 41: 0.8609039783477783
[08/28/2025 14:04:06 INFO]: Training loss at epoch 27: 0.9536113142967224
[08/28/2025 14:04:20 INFO]: Training loss at epoch 17: 0.9592210948467255
[08/28/2025 14:04:23 INFO]: Training loss at epoch 20: 0.8955255150794983
[08/28/2025 14:04:27 INFO]: Training loss at epoch 27: 0.9779237806797028
[08/28/2025 14:05:09 INFO]: Training loss at epoch 1: 1.0784795880317688
[08/28/2025 14:05:30 INFO]: Training loss at epoch 22: 1.0436090528964996
[08/28/2025 14:06:05 INFO]: Training loss at epoch 29: 0.9008334875106812
[08/28/2025 14:06:07 INFO]: Training loss at epoch 42: 1.0394928455352783
[08/28/2025 14:06:48 INFO]: Training loss at epoch 25: 0.9053468406200409
[08/28/2025 14:07:16 INFO]: Training loss at epoch 25: 1.1403868198394775
[08/28/2025 14:07:19 INFO]: Training loss at epoch 29: 0.8835276961326599
[08/28/2025 14:08:03 INFO]: Training loss at epoch 52: 0.7717637419700623
[08/28/2025 14:08:22 INFO]: Training loss at epoch 37: 0.8146109580993652
[08/28/2025 14:08:23 INFO]: Training stats: {
    "score": -0.9421278739695237,
    "rmse": 0.9421278739695237
}
[08/28/2025 14:08:23 INFO]: Val stats: {
    "score": -0.6868508324668006,
    "rmse": 0.6868508324668006
}
[08/28/2025 14:08:23 INFO]: Test stats: {
    "score": -0.8970847108885095,
    "rmse": 0.8970847108885095
}
[08/28/2025 14:08:38 INFO]: Training loss at epoch 8: 0.9830154478549957
[08/28/2025 14:09:08 INFO]: Training loss at epoch 37: 1.0857080221176147
[08/28/2025 14:09:16 INFO]: Training stats: {
    "score": -0.9819818176193893,
    "rmse": 0.9819818176193893
}
[08/28/2025 14:09:16 INFO]: Val stats: {
    "score": -0.6726248120085523,
    "rmse": 0.6726248120085523
}
[08/28/2025 14:09:16 INFO]: Test stats: {
    "score": -0.8821369674541403,
    "rmse": 0.8821369674541403
}
[08/28/2025 14:09:24 INFO]: Training loss at epoch 8: 0.8046337366104126
[08/28/2025 14:09:38 INFO]: Training loss at epoch 14: 1.0105480253696442
[08/28/2025 14:10:30 INFO]: Training loss at epoch 42: 0.7422696948051453
[08/28/2025 14:10:36 INFO]: Training loss at epoch 18: 0.9222274720668793
[08/28/2025 14:10:36 INFO]: Training loss at epoch 49: 1.0670927166938782
[08/28/2025 14:10:38 INFO]: Training loss at epoch 21: 1.1148114204406738
[08/28/2025 14:11:08 INFO]: Training loss at epoch 28: 0.9761702418327332
[08/28/2025 14:11:17 INFO]: Training loss at epoch 26: 0.9334670305252075
[08/28/2025 14:11:38 INFO]: Training loss at epoch 28: 0.9677483141422272
[08/28/2025 14:12:21 INFO]: Training loss at epoch 23: 0.8894001543521881
[08/28/2025 14:12:26 INFO]: New best epoch, val score: -0.6854910522837341
[08/28/2025 14:12:26 INFO]: Saving model to: blotchy-Amado_trial_145/model_best.pth
[08/28/2025 14:13:00 INFO]: Training loss at epoch 26: 1.084383249282837
[08/28/2025 14:13:43 INFO]: New best epoch, val score: -0.6557043059792318
[08/28/2025 14:13:43 INFO]: Saving model to: blotchy-Amado_trial_149/model_best.pth
[08/28/2025 14:14:17 INFO]: Training loss at epoch 26: 1.0718247294425964
[08/28/2025 14:14:40 INFO]: Training loss at epoch 53: 0.7729999125003815
[08/28/2025 14:14:46 INFO]: Training stats: {
    "score": -0.9606074915646408,
    "rmse": 0.9606074915646408
}
[08/28/2025 14:14:46 INFO]: Val stats: {
    "score": -0.793058487630521,
    "rmse": 0.793058487630521
}
[08/28/2025 14:14:46 INFO]: Test stats: {
    "score": -0.9682508871517125,
    "rmse": 0.9682508871517125
}
[08/28/2025 14:15:00 INFO]: Training loss at epoch 9: 1.0830222964286804
[08/28/2025 14:15:02 INFO]: Training loss at epoch 30: 0.7972652912139893
[08/28/2025 14:15:11 INFO]: Training loss at epoch 30: 0.9182982742786407
[08/28/2025 14:15:19 INFO]: Training loss at epoch 43: 1.0052716434001923
[08/28/2025 14:15:33 INFO]: Training loss at epoch 38: 0.9758890867233276
[08/28/2025 14:16:02 INFO]: Training loss at epoch 38: 0.9244222640991211
[08/28/2025 14:16:56 INFO]: Training loss at epoch 22: 1.2061455249786377
[08/28/2025 14:16:58 INFO]: Training loss at epoch 19: 0.8260467350482941
[08/28/2025 14:17:11 INFO]: Training stats: {
    "score": -0.9820460333836075,
    "rmse": 0.9820460333836075
}
[08/28/2025 14:17:11 INFO]: Val stats: {
    "score": -0.7019370539572153,
    "rmse": 0.7019370539572153
}
[08/28/2025 14:17:11 INFO]: Test stats: {
    "score": -0.8995664161700444,
    "rmse": 0.8995664161700444
}
[08/28/2025 14:17:13 INFO]: Training loss at epoch 9: 0.9092880487442017
[08/28/2025 14:17:16 INFO]: Training loss at epoch 43: 0.6546036750078201
[08/28/2025 14:17:32 INFO]: Training loss at epoch 15: 1.0806583762168884
[08/28/2025 14:17:57 INFO]: New best epoch, val score: -0.7019370539572153
[08/28/2025 14:17:57 INFO]: Saving model to: blotchy-Amado_trial_155/model_best.pth
[08/28/2025 14:18:15 INFO]: Training loss at epoch 29: 1.0251069068908691
[08/28/2025 14:18:52 INFO]: Training loss at epoch 29: 0.7877489626407623
[08/28/2025 14:18:56 INFO]: Training loss at epoch 2: 1.1160246133804321
[08/28/2025 14:19:02 INFO]: Training loss at epoch 27: 0.9588580429553986
[08/28/2025 14:19:12 INFO]: Training stats: {
    "score": -0.9493181988613528,
    "rmse": 0.9493181988613528
}
[08/28/2025 14:19:12 INFO]: Val stats: {
    "score": -0.6782160760685093,
    "rmse": 0.6782160760685093
}
[08/28/2025 14:19:12 INFO]: Test stats: {
    "score": -0.9178613630118487,
    "rmse": 0.9178613630118487
}
[08/28/2025 14:19:16 INFO]: Training loss at epoch 27: 0.7628395259380341
[08/28/2025 14:19:21 INFO]: Training loss at epoch 24: 1.1511772572994232
[08/28/2025 14:19:54 INFO]: Training stats: {
    "score": -0.9622308248951877,
    "rmse": 0.9622308248951877
}
[08/28/2025 14:19:54 INFO]: Val stats: {
    "score": -0.6816704885683792,
    "rmse": 0.6816704885683792
}
[08/28/2025 14:19:54 INFO]: Test stats: {
    "score": -0.8825537292051885,
    "rmse": 0.8825537292051885
}
[08/28/2025 14:20:39 INFO]: Training stats: {
    "score": -0.9829637675912597,
    "rmse": 0.9829637675912597
}
[08/28/2025 14:20:39 INFO]: Val stats: {
    "score": -0.6641726133940709,
    "rmse": 0.6641726133940709
}
[08/28/2025 14:20:39 INFO]: Test stats: {
    "score": -0.8897777731748757,
    "rmse": 0.8897777731748757
}
[08/28/2025 14:20:48 INFO]: Training loss at epoch 31: 1.2312355637550354
[08/28/2025 14:21:16 INFO]: Training stats: {
    "score": -0.9770194756685366,
    "rmse": 0.9770194756685366
}
[08/28/2025 14:21:16 INFO]: Val stats: {
    "score": -0.6792081144256444,
    "rmse": 0.6792081144256444
}
[08/28/2025 14:21:16 INFO]: Test stats: {
    "score": -0.8706167169958123,
    "rmse": 0.8706167169958123
}
[08/28/2025 14:21:22 INFO]: Training loss at epoch 54: 0.7039516270160675
[08/28/2025 14:21:25 INFO]: Training loss at epoch 27: 0.9453538060188293
[08/28/2025 14:21:30 INFO]: New best epoch, val score: -0.6641726133940709
[08/28/2025 14:21:30 INFO]: Saving model to: blotchy-Amado_trial_146/model_best.pth
[08/28/2025 14:22:02 INFO]: Training loss at epoch 39: 0.7473576068878174
[08/28/2025 14:22:04 INFO]: Training loss at epoch 31: 0.7775300145149231
[08/28/2025 14:22:08 INFO]: New best epoch, val score: -0.6792081144256444
[08/28/2025 14:22:08 INFO]: Saving model to: blotchy-Amado_trial_145/model_best.pth
[08/28/2025 14:23:16 INFO]: Training loss at epoch 23: 1.1367729306221008
[08/28/2025 14:23:39 INFO]: Training loss at epoch 10: 1.0214643478393555
[08/28/2025 14:23:48 INFO]: Training loss at epoch 39: 0.7495058178901672
[08/28/2025 14:23:59 INFO]: Training loss at epoch 44: 0.6444726437330246
[08/28/2025 14:24:17 INFO]: Training stats: {
    "score": -0.933652894264937,
    "rmse": 0.933652894264937
}
[08/28/2025 14:24:17 INFO]: Val stats: {
    "score": -0.6888804286265086,
    "rmse": 0.6888804286265086
}
[08/28/2025 14:24:17 INFO]: Test stats: {
    "score": -0.8930368854427844,
    "rmse": 0.8930368854427844
}
[08/28/2025 14:24:26 INFO]: New best epoch, val score: -0.6643362881771739
[08/28/2025 14:24:26 INFO]: Saving model to: blotchy-Amado_trial_155/model_best.pth
[08/28/2025 14:24:37 INFO]: Training loss at epoch 44: 0.9245402812957764
[08/28/2025 14:25:27 INFO]: Training loss at epoch 16: 1.139908492565155
[08/28/2025 14:25:32 INFO]: Training loss at epoch 28: 0.9888423085212708
[08/28/2025 14:25:33 INFO]: Training loss at epoch 20: 1.0498159527778625
[08/28/2025 14:26:22 INFO]: Training loss at epoch 25: 1.127505362033844
[08/28/2025 14:26:27 INFO]: Training stats: {
    "score": -0.9453917294625649,
    "rmse": 0.9453917294625649
}
[08/28/2025 14:26:27 INFO]: Val stats: {
    "score": -0.6991550152030674,
    "rmse": 0.6991550152030674
}
[08/28/2025 14:26:27 INFO]: Test stats: {
    "score": -0.8988615502057375,
    "rmse": 0.8988615502057375
}
[08/28/2025 14:26:38 INFO]: Training loss at epoch 32: 0.8337231278419495
[08/28/2025 14:26:48 INFO]: Training loss at epoch 28: 0.8803871870040894
[08/28/2025 14:26:50 INFO]: Training loss at epoch 50: 0.9355995059013367
[08/28/2025 14:27:13 INFO]: New best epoch, val score: -0.6843526743758285
[08/28/2025 14:27:14 INFO]: Saving model to: blotchy-Amado_trial_150/model_best.pth
[08/28/2025 14:27:43 INFO]: Training loss at epoch 10: 0.9715174734592438
[08/28/2025 14:27:47 INFO]: Training loss at epoch 30: 1.0328385531902313
[08/28/2025 14:28:10 INFO]: Training loss at epoch 55: 0.7107157707214355
[08/28/2025 14:28:32 INFO]: Training loss at epoch 30: 0.8882461786270142
[08/28/2025 14:28:33 INFO]: Training loss at epoch 28: 0.9711533784866333
[08/28/2025 14:28:58 INFO]: Training loss at epoch 32: 0.9345569312572479
[08/28/2025 14:29:25 INFO]: New best epoch, val score: -0.6773517632400781
[08/28/2025 14:29:25 INFO]: Saving model to: blotchy-Amado_trial_145/model_best.pth
[08/28/2025 14:29:43 INFO]: Training loss at epoch 24: 0.8816018402576447
[08/28/2025 14:30:09 INFO]: Training loss at epoch 11: 0.9899422526359558
[08/28/2025 14:30:27 INFO]: New best epoch, val score: -0.6465913990722236
[08/28/2025 14:30:27 INFO]: Saving model to: blotchy-Amado_trial_151/model_best.pth
[08/28/2025 14:30:40 INFO]: Training loss at epoch 45: 0.7369157671928406
[08/28/2025 14:30:49 INFO]: Training loss at epoch 40: 0.9608426988124847
[08/28/2025 14:31:48 INFO]: Training loss at epoch 29: 0.8889392614364624
[08/28/2025 14:31:53 INFO]: Training loss at epoch 21: 0.9332181215286255
[08/28/2025 14:32:27 INFO]: Training loss at epoch 33: 0.8081285357475281
[08/28/2025 14:32:52 INFO]: Training loss at epoch 3: 1.2397110164165497
[08/28/2025 14:33:23 INFO]: Training loss at epoch 17: 1.0600113272666931
[08/28/2025 14:33:30 INFO]: Training loss at epoch 26: 0.813888669013977
[08/28/2025 14:33:53 INFO]: Training stats: {
    "score": -0.9264783720699278,
    "rmse": 0.9264783720699278
}
[08/28/2025 14:33:53 INFO]: Val stats: {
    "score": -0.6999328363891757,
    "rmse": 0.6999328363891757
}
[08/28/2025 14:33:53 INFO]: Test stats: {
    "score": -0.9210705055268356,
    "rmse": 0.9210705055268356
}
[08/28/2025 14:33:58 INFO]: Training loss at epoch 45: 0.8781856596469879
[08/28/2025 14:34:18 INFO]: New best epoch, val score: -0.683299590745396
[08/28/2025 14:34:18 INFO]: Saving model to: blotchy-Amado_trial_150/model_best.pth
[08/28/2025 14:34:20 INFO]: Training loss at epoch 40: 0.9035325944423676
[08/28/2025 14:34:32 INFO]: Training loss at epoch 29: 0.9528051614761353
[08/28/2025 14:34:54 INFO]: Training loss at epoch 31: 0.9462029933929443
[08/28/2025 14:34:54 INFO]: Training loss at epoch 56: 0.8020956218242645
[08/28/2025 14:35:34 INFO]: Training loss at epoch 11: 0.810704380273819
[08/28/2025 14:35:38 INFO]: Training loss at epoch 29: 0.9051668047904968
[08/28/2025 14:35:46 INFO]: Training loss at epoch 31: 1.0413047671318054
[08/28/2025 14:35:47 INFO]: Training loss at epoch 33: 0.8248799741268158
[08/28/2025 14:35:58 INFO]: Training loss at epoch 25: 0.8668003380298615
[08/28/2025 14:36:33 INFO]: Training loss at epoch 12: 1.023038923740387
[08/28/2025 14:36:38 INFO]: New best epoch, val score: -0.6769470731766136
[08/28/2025 14:36:38 INFO]: Saving model to: blotchy-Amado_trial_145/model_best.pth
[08/28/2025 14:37:18 INFO]: Training stats: {
    "score": -0.898093266875541,
    "rmse": 0.898093266875541
}
[08/28/2025 14:37:18 INFO]: Val stats: {
    "score": -0.7245533027096566,
    "rmse": 0.7245533027096566
}
[08/28/2025 14:37:18 INFO]: Test stats: {
    "score": -0.9118659656436452,
    "rmse": 0.9118659656436452
}
[08/28/2025 14:37:23 INFO]: Training loss at epoch 41: 0.8012475669384003
[08/28/2025 14:37:27 INFO]: Training loss at epoch 46: 0.7404159307479858
[08/28/2025 14:38:05 INFO]: Training stats: {
    "score": -0.9766323029788816,
    "rmse": 0.9766323029788816
}
[08/28/2025 14:38:05 INFO]: Val stats: {
    "score": -0.6862024686552769,
    "rmse": 0.6862024686552769
}
[08/28/2025 14:38:05 INFO]: Test stats: {
    "score": -0.9024709118663814,
    "rmse": 0.9024709118663814
}
[08/28/2025 14:38:13 INFO]: Training loss at epoch 22: 0.8343404829502106
[08/28/2025 14:38:20 INFO]: Training loss at epoch 34: 1.148692935705185
[08/28/2025 14:38:55 INFO]: Training loss at epoch 51: 1.0198599994182587
[08/28/2025 14:40:17 INFO]: Training loss at epoch 30: 0.8238262832164764
[08/28/2025 14:40:26 INFO]: Training loss at epoch 27: 1.1423324048519135
[08/28/2025 14:41:13 INFO]: Training loss at epoch 18: 0.885945200920105
[08/28/2025 14:41:41 INFO]: Training loss at epoch 57: 0.6928883194923401
[08/28/2025 14:42:05 INFO]: Training loss at epoch 32: 1.212878406047821
[08/28/2025 14:42:07 INFO]: Training loss at epoch 41: 0.9268403351306915
[08/28/2025 14:42:22 INFO]: Training loss at epoch 26: 0.8228033781051636
[08/28/2025 14:42:43 INFO]: Training loss at epoch 34: 0.832815945148468
[08/28/2025 14:43:02 INFO]: Training loss at epoch 13: 1.0177287459373474
[08/28/2025 14:43:05 INFO]: Training loss at epoch 32: 1.0054409503936768
[08/28/2025 14:43:17 INFO]: Training loss at epoch 46: 1.039626955986023
[08/28/2025 14:43:25 INFO]: Training loss at epoch 12: 0.8554629385471344
[08/28/2025 14:43:44 INFO]: Training loss at epoch 42: 0.9216291904449463
[08/28/2025 14:44:01 INFO]: Training loss at epoch 35: 0.9729136824607849
[08/28/2025 14:44:03 INFO]: Training loss at epoch 47: 0.8080172538757324
[08/28/2025 14:44:24 INFO]: Training loss at epoch 23: 0.9379191994667053
[08/28/2025 14:44:54 INFO]: Training loss at epoch 30: 0.7752913236618042
[08/28/2025 14:45:05 INFO]: Training loss at epoch 30: 1.0549176335334778
[08/28/2025 14:46:28 INFO]: Training loss at epoch 31: 0.9096488952636719
[08/28/2025 14:46:39 INFO]: Training loss at epoch 4: 0.9501083493232727
[08/28/2025 14:47:14 INFO]: Training loss at epoch 28: 1.0622750520706177
[08/28/2025 14:48:22 INFO]: Training loss at epoch 58: 0.7982289493083954
[08/28/2025 14:48:39 INFO]: Training loss at epoch 27: 0.9318503141403198
[08/28/2025 14:49:03 INFO]: Training loss at epoch 19: 0.926019161939621
[08/28/2025 14:49:08 INFO]: Training loss at epoch 33: 0.8439848721027374
[08/28/2025 14:49:23 INFO]: Training loss at epoch 14: 1.018332600593567
[08/28/2025 14:49:30 INFO]: Training loss at epoch 35: 0.8602027297019958
[08/28/2025 14:49:48 INFO]: Training loss at epoch 36: 1.0302300453186035
[08/28/2025 14:50:01 INFO]: Training loss at epoch 42: 0.8927904665470123
[08/28/2025 14:50:09 INFO]: Training loss at epoch 33: 0.9985595941543579
[08/28/2025 14:50:10 INFO]: Training loss at epoch 43: 0.8982810378074646
[08/28/2025 14:50:42 INFO]: Training loss at epoch 24: 0.7715631425380707
[08/28/2025 14:50:48 INFO]: Training loss at epoch 48: 0.7742336392402649
[08/28/2025 14:50:56 INFO]: Training loss at epoch 52: 0.8988330364227295
[08/28/2025 14:51:12 INFO]: Training loss at epoch 13: 0.8442839980125427
[08/28/2025 14:51:42 INFO]: Training stats: {
    "score": -0.9523422866838297,
    "rmse": 0.9523422866838297
}
[08/28/2025 14:51:42 INFO]: Val stats: {
    "score": -0.6857332276707735,
    "rmse": 0.6857332276707735
}
[08/28/2025 14:51:42 INFO]: Test stats: {
    "score": -0.9074791380024368,
    "rmse": 0.9074791380024368
}
[08/28/2025 14:52:09 INFO]: Training loss at epoch 31: 0.8856779038906097
[08/28/2025 14:52:37 INFO]: Training loss at epoch 47: 0.8629963994026184
[08/28/2025 14:52:39 INFO]: Training loss at epoch 31: 0.6795309036970139
[08/28/2025 14:52:44 INFO]: Training loss at epoch 32: 0.9194456040859222
[08/28/2025 14:53:33 INFO]: Running Final Evaluation...
[08/28/2025 14:54:09 INFO]: Training loss at epoch 29: 0.8300757706165314
[08/28/2025 14:54:58 INFO]: Training loss at epoch 28: 0.8920880854129791
[08/28/2025 14:55:01 INFO]: Training loss at epoch 59: 0.6891046464443207
[08/28/2025 14:55:33 INFO]: Training loss at epoch 37: 0.817417711019516
[08/28/2025 14:55:44 INFO]: Training loss at epoch 15: 0.9974881708621979
[08/28/2025 14:56:10 INFO]: Training loss at epoch 34: 0.9429823458194733
[08/28/2025 14:56:17 INFO]: Training loss at epoch 36: 0.8723494708538055
[08/28/2025 14:56:25 INFO]: Training accuracy: {
    "score": -1.0053799677291992,
    "rmse": 1.0053799677291992
}
[08/28/2025 14:56:25 INFO]: Val accuracy: {
    "score": -0.683037609043042,
    "rmse": 0.683037609043042
}
[08/28/2025 14:56:25 INFO]: Test accuracy: {
    "score": -0.8643861080885727,
    "rmse": 0.8643861080885727
}
[08/28/2025 14:56:25 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_144",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8643861080885727,
        "rmse": 0.8643861080885727
    },
    "train_stats": {
        "score": -1.0053799677291992,
        "rmse": 1.0053799677291992
    },
    "val_stats": {
        "score": -0.683037609043042,
        "rmse": 0.683037609043042
    }
}
[08/28/2025 14:56:25 INFO]: Procewss finished for trial blotchy-Amado_trial_144
[08/28/2025 14:56:26 INFO]: 
_________________________________________________

[08/28/2025 14:56:26 INFO]: train_net_for_optune.py main() running.
[08/28/2025 14:56:26 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 2.0699510277817503
  attention_dropout: 0.05884002309779292
  ffn_dropout: 0.05884002309779292
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.0856469405725748e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_157

[08/28/2025 14:56:26 INFO]: This ft_transformer has 1.054 million parameters.
[08/28/2025 14:56:26 INFO]: Training will start at epoch 0.
[08/28/2025 14:56:26 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 14:56:27 INFO]: Training stats: {
    "score": -0.9465774933932198,
    "rmse": 0.9465774933932198
}
[08/28/2025 14:56:27 INFO]: Val stats: {
    "score": -0.7046433573402269,
    "rmse": 0.7046433573402269
}
[08/28/2025 14:56:27 INFO]: Test stats: {
    "score": -0.8998717505010747,
    "rmse": 0.8998717505010747
}
[08/28/2025 14:56:33 INFO]: Training loss at epoch 44: 0.7908350229263306
[08/28/2025 14:56:54 INFO]: Training loss at epoch 25: 0.9830513894557953
[08/28/2025 14:57:13 INFO]: Training loss at epoch 34: 0.9261986017227173
[08/28/2025 14:57:15 INFO]: Training stats: {
    "score": -0.9048281748822167,
    "rmse": 0.9048281748822167
}
[08/28/2025 14:57:15 INFO]: Val stats: {
    "score": -0.6894782361465923,
    "rmse": 0.6894782361465923
}
[08/28/2025 14:57:15 INFO]: Test stats: {
    "score": -0.9360558155989727,
    "rmse": 0.9360558155989727
}
[08/28/2025 14:57:27 INFO]: Training loss at epoch 49: 0.68336021900177
[08/28/2025 14:57:43 INFO]: Training loss at epoch 43: 0.7313830256462097
[08/28/2025 14:58:03 INFO]: Training loss at epoch 0: 1.4394596219062805
[08/28/2025 14:58:17 INFO]: New best epoch, val score: -0.8203104826842016
[08/28/2025 14:58:17 INFO]: Saving model to: blotchy-Amado_trial_157/model_best.pth
[08/28/2025 14:58:54 INFO]: Training loss at epoch 33: 0.8027702569961548
[08/28/2025 14:58:57 INFO]: Training loss at epoch 14: 1.0141612589359283
[08/28/2025 14:59:08 INFO]: Training loss at epoch 32: 0.8884260058403015
[08/28/2025 14:59:34 INFO]: Training loss at epoch 20: 0.9732395112514496
[08/28/2025 14:59:42 INFO]: Training stats: {
    "score": -0.8582523311970278,
    "rmse": 0.8582523311970278
}
[08/28/2025 14:59:42 INFO]: Val stats: {
    "score": -0.6866176415355056,
    "rmse": 0.6866176415355056
}
[08/28/2025 14:59:42 INFO]: Test stats: {
    "score": -0.9245522687930123,
    "rmse": 0.9245522687930123
}
[08/28/2025 14:59:55 INFO]: Training loss at epoch 1: 1.3902372121810913
[08/28/2025 15:00:11 INFO]: New best epoch, val score: -0.7583627840417567
[08/28/2025 15:00:11 INFO]: Saving model to: blotchy-Amado_trial_157/model_best.pth
[08/28/2025 15:00:25 INFO]: Training loss at epoch 5: 1.0195913910865784
[08/28/2025 15:00:31 INFO]: New best epoch, val score: -0.6741122865063495
[08/28/2025 15:00:31 INFO]: Saving model to: blotchy-Amado_trial_153/model_best.pth
[08/28/2025 15:01:16 INFO]: Training loss at epoch 29: 1.08982515335083
[08/28/2025 15:01:21 INFO]: Training loss at epoch 38: 0.8186003565788269
[08/28/2025 15:01:48 INFO]: Training loss at epoch 2: 1.4257152676582336
[08/28/2025 15:01:49 INFO]: Training loss at epoch 48: 0.897384524345398
[08/28/2025 15:02:02 INFO]: New best epoch, val score: -0.6914520314763271
[08/28/2025 15:02:02 INFO]: Saving model to: blotchy-Amado_trial_157/model_best.pth
[08/28/2025 15:02:06 INFO]: Training loss at epoch 16: 0.9295038878917694
[08/28/2025 15:02:51 INFO]: New best epoch, val score: -0.6621103621703021
[08/28/2025 15:02:51 INFO]: Saving model to: blotchy-Amado_trial_155/model_best.pth
[08/28/2025 15:02:53 INFO]: Training loss at epoch 53: 1.2433978915214539
[08/28/2025 15:02:59 INFO]: Training loss at epoch 45: 0.8927655220031738
[08/28/2025 15:03:05 INFO]: Training loss at epoch 37: 0.8218884468078613
[08/28/2025 15:03:08 INFO]: Training loss at epoch 26: 0.9411558508872986
[08/28/2025 15:03:14 INFO]: Training loss at epoch 35: 0.9323533177375793
[08/28/2025 15:03:21 INFO]: Training stats: {
    "score": -0.9713329135005501,
    "rmse": 0.9713329135005501
}
[08/28/2025 15:03:21 INFO]: Val stats: {
    "score": -0.7408222377645761,
    "rmse": 0.7408222377645761
}
[08/28/2025 15:03:21 INFO]: Test stats: {
    "score": -0.9255467517500429,
    "rmse": 0.9255467517500429
}
[08/28/2025 15:03:21 INFO]: Training loss at epoch 30: 0.8274003565311432
[08/28/2025 15:03:39 INFO]: Training loss at epoch 3: 1.0013445317745209
[08/28/2025 15:03:52 INFO]: New best epoch, val score: -0.6610130309334008
[08/28/2025 15:03:52 INFO]: Saving model to: blotchy-Amado_trial_157/model_best.pth
[08/28/2025 15:03:55 INFO]: Training loss at epoch 60: 0.8519697785377502
[08/28/2025 15:04:21 INFO]: Training loss at epoch 35: 1.070987582206726
[08/28/2025 15:05:07 INFO]: Training loss at epoch 34: 0.7937538325786591
[08/28/2025 15:05:23 INFO]: Training loss at epoch 44: 0.903090626001358
[08/28/2025 15:05:29 INFO]: Training loss at epoch 4: 1.0062701106071472
[08/28/2025 15:06:13 INFO]: Training loss at epoch 33: 0.973741352558136
[08/28/2025 15:06:21 INFO]: Training loss at epoch 50: 0.9338511824607849
[08/28/2025 15:06:47 INFO]: Training loss at epoch 15: 0.9341428875923157
[08/28/2025 15:07:09 INFO]: Training loss at epoch 39: 1.1300399899482727
[08/28/2025 15:07:22 INFO]: Training loss at epoch 5: 0.927951991558075
[08/28/2025 15:07:24 INFO]: Training loss at epoch 21: 0.7883455753326416
[08/28/2025 15:08:30 INFO]: Training loss at epoch 17: 0.8586407005786896
[08/28/2025 15:09:03 INFO]: Training stats: {
    "score": -0.9657289733081887,
    "rmse": 0.9657289733081887
}
[08/28/2025 15:09:03 INFO]: Val stats: {
    "score": -0.7171538967954221,
    "rmse": 0.7171538967954221
}
[08/28/2025 15:09:03 INFO]: Test stats: {
    "score": -0.9095196452069737,
    "rmse": 0.9095196452069737
}
[08/28/2025 15:09:11 INFO]: Training loss at epoch 6: 0.9802458584308624
[08/28/2025 15:09:24 INFO]: Training loss at epoch 27: 1.1380256116390228
[08/28/2025 15:09:26 INFO]: Training loss at epoch 46: 0.8711997270584106
[08/28/2025 15:09:37 INFO]: Training loss at epoch 30: 0.8666532039642334
[08/28/2025 15:09:58 INFO]: Training loss at epoch 38: 1.0239853262901306
[08/28/2025 15:10:11 INFO]: Training loss at epoch 31: 1.1296510100364685
[08/28/2025 15:10:22 INFO]: Training loss at epoch 36: 0.8783267438411713
[08/28/2025 15:10:40 INFO]: Training loss at epoch 61: 0.8874659836292267
[08/28/2025 15:11:02 INFO]: Training loss at epoch 7: 1.3149672746658325
[08/28/2025 15:11:05 INFO]: Training loss at epoch 49: 0.9183705449104309
[08/28/2025 15:11:27 INFO]: Training loss at epoch 35: 0.8390453457832336
[08/28/2025 15:11:29 INFO]: Training loss at epoch 36: 1.0239362716674805
[08/28/2025 15:12:54 INFO]: Training loss at epoch 8: 1.0469200611114502
[08/28/2025 15:13:07 INFO]: Training loss at epoch 51: 0.8100210726261139
[08/28/2025 15:13:16 INFO]: Training loss at epoch 34: 1.3380313515663147
[08/28/2025 15:13:16 INFO]: Training loss at epoch 45: 0.8308081030845642
[08/28/2025 15:14:14 INFO]: Training stats: {
    "score": -0.9524796371028676,
    "rmse": 0.9524796371028676
}
[08/28/2025 15:14:14 INFO]: Val stats: {
    "score": -0.7145609697155793,
    "rmse": 0.7145609697155793
}
[08/28/2025 15:14:14 INFO]: Test stats: {
    "score": -0.9031611044979013,
    "rmse": 0.9031611044979013
}
[08/28/2025 15:14:16 INFO]: Training loss at epoch 6: 1.1912888288497925
[08/28/2025 15:14:33 INFO]: Training loss at epoch 16: 0.7088973373174667
[08/28/2025 15:14:47 INFO]: Training loss at epoch 9: 1.1950932145118713
[08/28/2025 15:14:50 INFO]: Training loss at epoch 40: 0.9515086114406586
[08/28/2025 15:14:53 INFO]: Training loss at epoch 18: 0.8357745110988617
[08/28/2025 15:15:04 INFO]: Training loss at epoch 54: 0.822777271270752
[08/28/2025 15:15:13 INFO]: Training loss at epoch 22: 0.7703883647918701
[08/28/2025 15:15:27 INFO]: Training stats: {
    "score": -1.0072598054486372,
    "rmse": 1.0072598054486372
}
[08/28/2025 15:15:27 INFO]: Val stats: {
    "score": -0.7158515136374446,
    "rmse": 0.7158515136374446
}
[08/28/2025 15:15:27 INFO]: Test stats: {
    "score": -0.8949246191232159,
    "rmse": 0.8949246191232159
}
[08/28/2025 15:15:40 INFO]: Training loss at epoch 28: 0.9772507548332214
[08/28/2025 15:15:54 INFO]: Training loss at epoch 47: 0.9158402383327484
[08/28/2025 15:15:55 INFO]: Training loss at epoch 31: 1.0442264676094055
[08/28/2025 15:16:52 INFO]: Training loss at epoch 39: 0.8203813433647156
[08/28/2025 15:17:04 INFO]: Training loss at epoch 32: 0.9693399667739868
[08/28/2025 15:17:19 INFO]: Training loss at epoch 10: 1.1415847539901733
[08/28/2025 15:17:20 INFO]: Training loss at epoch 62: 0.7894157767295837
[08/28/2025 15:17:29 INFO]: Training loss at epoch 37: 0.9023634195327759
[08/28/2025 15:17:42 INFO]: Training loss at epoch 36: 0.952260285615921
[08/28/2025 15:18:39 INFO]: Training loss at epoch 37: 1.0027092695236206
[08/28/2025 15:19:10 INFO]: Training loss at epoch 11: 1.2831936478614807
[08/28/2025 15:19:11 INFO]: Training stats: {
    "score": -0.9409164852668386,
    "rmse": 0.9409164852668386
}
[08/28/2025 15:19:11 INFO]: Val stats: {
    "score": -0.8077144812932888,
    "rmse": 0.8077144812932888
}
[08/28/2025 15:19:11 INFO]: Test stats: {
    "score": -0.9866469569404172,
    "rmse": 0.9866469569404172
}
[08/28/2025 15:19:44 INFO]: Training loss at epoch 52: 0.8397407829761505
[08/28/2025 15:20:19 INFO]: Training loss at epoch 35: 0.8492956757545471
[08/28/2025 15:20:35 INFO]: Training loss at epoch 41: 1.0383434891700745
[08/28/2025 15:20:57 INFO]: Training loss at epoch 46: 0.8363340198993683
[08/28/2025 15:21:00 INFO]: Training loss at epoch 12: 1.1790056824684143
[08/28/2025 15:21:13 INFO]: Training loss at epoch 19: 0.9965718388557434
[08/28/2025 15:21:57 INFO]: Training loss at epoch 29: 1.029574990272522
[08/28/2025 15:22:15 INFO]: Training loss at epoch 32: 1.033334106206894
[08/28/2025 15:22:19 INFO]: Training loss at epoch 48: 0.8027234077453613
[08/28/2025 15:22:20 INFO]: Training loss at epoch 17: 0.6682563722133636
[08/28/2025 15:22:51 INFO]: Training loss at epoch 13: 1.1750292778015137
[08/28/2025 15:23:05 INFO]: Training loss at epoch 23: 0.9103074967861176
[08/28/2025 15:23:24 INFO]: Training stats: {
    "score": -0.9688710851635511,
    "rmse": 0.9688710851635511
}
[08/28/2025 15:23:24 INFO]: Val stats: {
    "score": -0.7059123590483648,
    "rmse": 0.7059123590483648
}
[08/28/2025 15:23:24 INFO]: Test stats: {
    "score": -0.9045188517439069,
    "rmse": 0.9045188517439069
}
[08/28/2025 15:23:33 INFO]: Training loss at epoch 50: 0.7917508780956268
[08/28/2025 15:23:51 INFO]: Training loss at epoch 33: 0.8738802373409271
[08/28/2025 15:23:58 INFO]: Training loss at epoch 37: 0.932174026966095
[08/28/2025 15:24:00 INFO]: Training stats: {
    "score": -0.928376410502678,
    "rmse": 0.928376410502678
}
[08/28/2025 15:24:00 INFO]: Val stats: {
    "score": -0.6854164552740056,
    "rmse": 0.6854164552740056
}
[08/28/2025 15:24:00 INFO]: Test stats: {
    "score": -0.9298834221148792,
    "rmse": 0.9298834221148792
}
[08/28/2025 15:24:05 INFO]: Training loss at epoch 63: 0.897959440946579
[08/28/2025 15:24:31 INFO]: Training loss at epoch 38: 1.081513524055481
[08/28/2025 15:24:40 INFO]: Training loss at epoch 14: 0.9170100390911102
[08/28/2025 15:25:40 INFO]: Training loss at epoch 38: 0.9777390360832214
[08/28/2025 15:25:54 INFO]: Training loss at epoch 40: 1.1631191968917847
[08/28/2025 15:26:16 INFO]: Training loss at epoch 53: 0.6808882057666779
[08/28/2025 15:26:17 INFO]: Training loss at epoch 42: 1.0881488025188446
[08/28/2025 15:26:28 INFO]: Training loss at epoch 15: 1.135088711977005
[08/28/2025 15:27:04 INFO]: Training loss at epoch 55: 0.9241595566272736
[08/28/2025 15:27:15 INFO]: Training loss at epoch 36: 0.9496936798095703
[08/28/2025 15:28:10 INFO]: Training loss at epoch 7: 1.0652678310871124
[08/28/2025 15:28:21 INFO]: Training loss at epoch 16: 1.1254090666770935
[08/28/2025 15:28:27 INFO]: Training loss at epoch 33: 0.8928458988666534
[08/28/2025 15:28:34 INFO]: Training loss at epoch 47: 0.8883469998836517
[08/28/2025 15:28:42 INFO]: Training loss at epoch 49: 0.7512320280075073
[08/28/2025 15:29:46 INFO]: Training loss at epoch 20: 0.85841304063797
[08/28/2025 15:30:05 INFO]: Training loss at epoch 18: 0.8783299922943115
[08/28/2025 15:30:11 INFO]: Training loss at epoch 17: 0.8835159540176392
[08/28/2025 15:30:12 INFO]: Training loss at epoch 38: 0.8607530295848846
[08/28/2025 15:30:15 INFO]: Training loss at epoch 30: 0.8090329766273499
[08/28/2025 15:30:40 INFO]: Training loss at epoch 34: 1.058894395828247
[08/28/2025 15:30:44 INFO]: Training loss at epoch 64: 0.6568770706653595
[08/28/2025 15:30:49 INFO]: Training loss at epoch 24: 1.0105923414230347
[08/28/2025 15:30:53 INFO]: Training stats: {
    "score": -0.9076290176595804,
    "rmse": 0.9076290176595804
}
[08/28/2025 15:30:53 INFO]: Val stats: {
    "score": -0.6916913542557639,
    "rmse": 0.6916913542557639
}
[08/28/2025 15:30:53 INFO]: Test stats: {
    "score": -0.9219973349917061,
    "rmse": 0.9219973349917061
}
[08/28/2025 15:31:32 INFO]: Training loss at epoch 39: 0.9772678017616272
[08/28/2025 15:31:58 INFO]: Training loss at epoch 18: 1.0128272771835327
[08/28/2025 15:32:01 INFO]: Training loss at epoch 43: 0.7368191927671432
[08/28/2025 15:32:45 INFO]: Training loss at epoch 41: 0.8504330515861511
[08/28/2025 15:32:46 INFO]: Training loss at epoch 51: 1.0336323976516724
[08/28/2025 15:32:49 INFO]: Training loss at epoch 39: 1.1456679999828339
[08/28/2025 15:32:54 INFO]: Training loss at epoch 54: 0.7393313050270081
[08/28/2025 15:33:52 INFO]: Training loss at epoch 19: 0.910205215215683
[08/28/2025 15:33:57 INFO]: Training stats: {
    "score": -0.9764503558705967,
    "rmse": 0.9764503558705967
}
[08/28/2025 15:33:57 INFO]: Val stats: {
    "score": -0.6727667922772471,
    "rmse": 0.6727667922772471
}
[08/28/2025 15:33:57 INFO]: Test stats: {
    "score": -0.8959555929834462,
    "rmse": 0.8959555929834462
}
[08/28/2025 15:34:20 INFO]: Training loss at epoch 37: 0.9804269075393677
[08/28/2025 15:34:32 INFO]: Training stats: {
    "score": -0.9999830120580737,
    "rmse": 0.9999830120580737
}
[08/28/2025 15:34:32 INFO]: Val stats: {
    "score": -0.6849297704369044,
    "rmse": 0.6849297704369044
}
[08/28/2025 15:34:32 INFO]: Test stats: {
    "score": -0.8770820317331828,
    "rmse": 0.8770820317331828
}
[08/28/2025 15:34:40 INFO]: Training loss at epoch 34: 1.163215845823288
[08/28/2025 15:35:12 INFO]: Training stats: {
    "score": -0.9831490888522262,
    "rmse": 0.9831490888522262
}
[08/28/2025 15:35:12 INFO]: Val stats: {
    "score": -0.7866911926197268,
    "rmse": 0.7866911926197268
}
[08/28/2025 15:35:12 INFO]: Test stats: {
    "score": -0.9393342299156113,
    "rmse": 0.9393342299156113
}
[08/28/2025 15:36:07 INFO]: Training loss at epoch 21: 0.9517878293991089
[08/28/2025 15:36:14 INFO]: Training loss at epoch 48: 0.7137868404388428
[08/28/2025 15:36:23 INFO]: Training loss at epoch 20: 0.909568578004837
[08/28/2025 15:36:26 INFO]: Training loss at epoch 39: 0.8999693989753723
[08/28/2025 15:36:33 INFO]: Training loss at epoch 31: 0.7997417747974396
[08/28/2025 15:37:20 INFO]: Training loss at epoch 50: 0.7307957410812378
[08/28/2025 15:37:31 INFO]: Training loss at epoch 35: 0.8259606957435608
[08/28/2025 15:37:34 INFO]: Training loss at epoch 65: 0.8210906088352203
[08/28/2025 15:37:48 INFO]: Training loss at epoch 44: 1.1355095207691193
[08/28/2025 15:37:52 INFO]: Training loss at epoch 19: 0.8801876902580261
[08/28/2025 15:38:16 INFO]: Training loss at epoch 21: 0.9970809817314148
[08/28/2025 15:38:23 INFO]: Running Final Evaluation...
[08/28/2025 15:38:34 INFO]: Training stats: {
    "score": -0.8995458397131071,
    "rmse": 0.8995458397131071
}
[08/28/2025 15:38:34 INFO]: Val stats: {
    "score": -0.7141057270297152,
    "rmse": 0.7141057270297152
}
[08/28/2025 15:38:34 INFO]: Test stats: {
    "score": -0.9323213247360728,
    "rmse": 0.9323213247360728
}
[08/28/2025 15:38:49 INFO]: Training loss at epoch 25: 0.8957956433296204
[08/28/2025 15:39:10 INFO]: Training loss at epoch 56: 0.882012128829956
[08/28/2025 15:39:31 INFO]: Training loss at epoch 42: 0.9566813111305237
[08/28/2025 15:39:41 INFO]: Training loss at epoch 55: 0.7638792991638184
[08/28/2025 15:40:04 INFO]: Training loss at epoch 22: 1.2470801174640656
[08/28/2025 15:40:28 INFO]: Training stats: {
    "score": -0.9338741437396021,
    "rmse": 0.9338741437396021
}
[08/28/2025 15:40:28 INFO]: Val stats: {
    "score": -0.6811721144780393,
    "rmse": 0.6811721144780393
}
[08/28/2025 15:40:28 INFO]: Test stats: {
    "score": -0.8873083901385674,
    "rmse": 0.8873083901385674
}
[08/28/2025 15:40:55 INFO]: Training loss at epoch 35: 1.285714715719223
[08/28/2025 15:40:59 INFO]: Training loss at epoch 40: 1.0958942472934723
[08/28/2025 15:41:03 INFO]: Training accuracy: {
    "score": -0.9626326355284146,
    "rmse": 0.9626326355284146
}
[08/28/2025 15:41:03 INFO]: Val accuracy: {
    "score": -0.667392188642673,
    "rmse": 0.667392188642673
}
[08/28/2025 15:41:03 INFO]: Test accuracy: {
    "score": -0.8854911200695017,
    "rmse": 0.8854911200695017
}
[08/28/2025 15:41:03 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_127",
    "best_epoch": 34,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8854911200695017,
        "rmse": 0.8854911200695017
    },
    "train_stats": {
        "score": -0.9626326355284146,
        "rmse": 0.9626326355284146
    },
    "val_stats": {
        "score": -0.667392188642673,
        "rmse": 0.667392188642673
    }
}
[08/28/2025 15:41:03 INFO]: Procewss finished for trial blotchy-Amado_trial_127
[08/28/2025 15:41:03 INFO]: 
_________________________________________________

[08/28/2025 15:41:03 INFO]: train_net_for_optune.py main() running.
[08/28/2025 15:41:03 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 1.0608776134109699
  attention_dropout: 0.05760137096592322
  ffn_dropout: 0.05760137096592322
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.838649779187281e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_158

[08/28/2025 15:41:03 INFO]: This ft_transformer has 1.056 million parameters.
[08/28/2025 15:41:03 INFO]: Training will start at epoch 0.
[08/28/2025 15:41:03 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 15:41:21 INFO]: Training loss at epoch 38: 1.1431534886360168
[08/28/2025 15:41:54 INFO]: Training loss at epoch 23: 0.9254510998725891
[08/28/2025 15:41:59 INFO]: Training loss at epoch 52: 1.017061173915863
[08/28/2025 15:42:05 INFO]: Training loss at epoch 8: 0.8472549617290497
[08/28/2025 15:42:10 INFO]: Running Final Evaluation...
[08/28/2025 15:42:16 INFO]: Training loss at epoch 40: 0.9324630796909332
[08/28/2025 15:42:26 INFO]: Training loss at epoch 22: 1.0008147358894348
[08/28/2025 15:42:42 INFO]: Training loss at epoch 0: 1.3426830768585205
[08/28/2025 15:42:44 INFO]: Training loss at epoch 32: 0.7455279529094696
[08/28/2025 15:42:56 INFO]: New best epoch, val score: -0.9465155873869014
[08/28/2025 15:42:56 INFO]: Saving model to: blotchy-Amado_trial_158/model_best.pth
[08/28/2025 15:43:30 INFO]: Training loss at epoch 45: 0.7815727293491364
[08/28/2025 15:43:39 INFO]: Training loss at epoch 51: 0.8347288370132446
[08/28/2025 15:43:42 INFO]: Training loss at epoch 24: 0.928371250629425
[08/28/2025 15:43:49 INFO]: Training loss at epoch 49: 1.2224308252334595
[08/28/2025 15:44:17 INFO]: Training loss at epoch 36: 1.0478491187095642
[08/28/2025 15:44:34 INFO]: Training loss at epoch 1: 1.1869510412216187
[08/28/2025 15:44:40 INFO]: Training loss at epoch 40: 0.8135204315185547
[08/28/2025 15:44:48 INFO]: New best epoch, val score: -0.7184348577721457
[08/28/2025 15:44:48 INFO]: Saving model to: blotchy-Amado_trial_158/model_best.pth
[08/28/2025 15:44:54 INFO]: Training accuracy: {
    "score": -1.005387032689121,
    "rmse": 1.005387032689121
}
[08/28/2025 15:44:54 INFO]: Val accuracy: {
    "score": -0.6579185127730452,
    "rmse": 0.6579185127730452
}
[08/28/2025 15:44:54 INFO]: Test accuracy: {
    "score": -0.8897097299282238,
    "rmse": 0.8897097299282238
}
[08/28/2025 15:44:54 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_147",
    "best_epoch": 7,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8897097299282238,
        "rmse": 0.8897097299282238
    },
    "train_stats": {
        "score": -1.005387032689121,
        "rmse": 1.005387032689121
    },
    "val_stats": {
        "score": -0.6579185127730452,
        "rmse": 0.6579185127730452
    }
}
[08/28/2025 15:44:54 INFO]: Procewss finished for trial blotchy-Amado_trial_147
[08/28/2025 15:44:55 INFO]: 
_________________________________________________

[08/28/2025 15:44:55 INFO]: train_net_for_optune.py main() running.
[08/28/2025 15:44:55 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 16
  d_ffn_factor: 2.0531777812467515
  attention_dropout: 0.052678857255462015
  ffn_dropout: 0.052678857255462015
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.2606219191356048e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_159

[08/28/2025 15:44:55 INFO]: This ft_transformer has 1.383 million parameters.
[08/28/2025 15:44:55 INFO]: Training will start at epoch 0.
[08/28/2025 15:44:55 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 15:45:35 INFO]: Training loss at epoch 25: 1.0666415691375732
[08/28/2025 15:46:17 INFO]: Training loss at epoch 56: 0.7411604225635529
[08/28/2025 15:46:18 INFO]: Training loss at epoch 43: 0.8350731432437897
[08/28/2025 15:46:27 INFO]: Training loss at epoch 2: 0.9750450253486633
[08/28/2025 15:46:29 INFO]: Training stats: {
    "score": -0.932114610428753,
    "rmse": 0.932114610428753
}
[08/28/2025 15:46:29 INFO]: Val stats: {
    "score": -0.696254426055055,
    "rmse": 0.696254426055055
}
[08/28/2025 15:46:29 INFO]: Test stats: {
    "score": -0.9126264292736501,
    "rmse": 0.9126264292736501
}
[08/28/2025 15:46:32 INFO]: Training loss at epoch 26: 0.8007634282112122
[08/28/2025 15:46:42 INFO]: New best epoch, val score: -0.6652479043842627
[08/28/2025 15:46:42 INFO]: Saving model to: blotchy-Amado_trial_158/model_best.pth
[08/28/2025 15:47:10 INFO]: Training loss at epoch 36: 1.087611585855484
[08/28/2025 15:47:26 INFO]: Training loss at epoch 26: 0.9001273810863495
[08/28/2025 15:47:43 INFO]: Training loss at epoch 0: 1.0735862255096436
[08/28/2025 15:48:05 INFO]: Training loss at epoch 41: 0.9581233263015747
[08/28/2025 15:48:07 INFO]: New best epoch, val score: -0.735912141684849
[08/28/2025 15:48:07 INFO]: Saving model to: blotchy-Amado_trial_159/model_best.pth
[08/28/2025 15:48:13 INFO]: Training loss at epoch 20: 0.8381495177745819
[08/28/2025 15:48:17 INFO]: Training loss at epoch 3: 1.1697487831115723
[08/28/2025 15:48:31 INFO]: New best epoch, val score: -0.6621067867584016
[08/28/2025 15:48:31 INFO]: Saving model to: blotchy-Amado_trial_158/model_best.pth
[08/28/2025 15:48:48 INFO]: Training loss at epoch 23: 0.9226274192333221
[08/28/2025 15:48:59 INFO]: Training loss at epoch 33: 0.8351497650146484
[08/28/2025 15:49:17 INFO]: Training loss at epoch 27: 1.0438808798789978
[08/28/2025 15:49:19 INFO]: Training loss at epoch 46: 0.9868902564048767
[08/28/2025 15:49:25 INFO]: Training loss at epoch 41: 0.983384758234024
[08/28/2025 15:50:08 INFO]: Training loss at epoch 52: 0.7829129099845886
[08/28/2025 15:50:19 INFO]: Training loss at epoch 4: 1.2030561566352844
[08/28/2025 15:50:55 INFO]: Training loss at epoch 41: 0.7128357291221619
[08/28/2025 15:50:57 INFO]: Training loss at epoch 1: 1.1487849950790405
[08/28/2025 15:51:05 INFO]: Training loss at epoch 57: 0.9881089925765991
[08/28/2025 15:51:10 INFO]: Training loss at epoch 28: 0.8849969208240509
[08/28/2025 15:51:11 INFO]: Training loss at epoch 37: 0.8649136424064636
[08/28/2025 15:51:16 INFO]: Training loss at epoch 53: 0.8455541729927063
[08/28/2025 15:51:21 INFO]: New best epoch, val score: -0.6546907106983554
[08/28/2025 15:51:21 INFO]: Saving model to: blotchy-Amado_trial_159/model_best.pth
[08/28/2025 15:52:12 INFO]: Training loss at epoch 5: 1.0412883162498474
[08/28/2025 15:52:59 INFO]: Training loss at epoch 29: 1.2846611738204956
[08/28/2025 15:53:04 INFO]: Training loss at epoch 57: 0.7925260961055756
[08/28/2025 15:53:05 INFO]: Training loss at epoch 44: 0.946279764175415
[08/28/2025 15:53:34 INFO]: Training loss at epoch 37: 0.8652345538139343
[08/28/2025 15:53:38 INFO]: Training stats: {
    "score": -0.9992123651244927,
    "rmse": 0.9992123651244927
}
[08/28/2025 15:53:38 INFO]: Val stats: {
    "score": -0.6733722312992065,
    "rmse": 0.6733722312992065
}
[08/28/2025 15:53:38 INFO]: Test stats: {
    "score": -0.8724495223618597,
    "rmse": 0.8724495223618597
}
[08/28/2025 15:53:52 INFO]: Running Final Evaluation...
[08/28/2025 15:54:02 INFO]: Training loss at epoch 6: 1.2178862690925598
[08/28/2025 15:54:04 INFO]: Training loss at epoch 2: 1.1943363547325134
[08/28/2025 15:54:18 INFO]: Training loss at epoch 50: 0.8211799263954163
[08/28/2025 15:54:29 INFO]: Training loss at epoch 27: 0.9874317944049835
[08/28/2025 15:55:01 INFO]: Training loss at epoch 47: 1.1671724915504456
[08/28/2025 15:55:09 INFO]: Training loss at epoch 24: 0.9051909148693085
[08/28/2025 15:55:12 INFO]: Training loss at epoch 42: 1.0424429178237915
[08/28/2025 15:55:13 INFO]: Training loss at epoch 34: 0.7512380480766296
[08/28/2025 15:55:16 INFO]: Running Final Evaluation...
[08/28/2025 15:55:28 INFO]: Training loss at epoch 30: 0.9871638715267181
[08/28/2025 15:55:41 INFO]: Running Final Evaluation...
[08/28/2025 15:55:54 INFO]: Training loss at epoch 7: 0.9211339950561523
[08/28/2025 15:55:54 INFO]: Training loss at epoch 9: 1.0016826689243317
[08/28/2025 15:55:58 INFO]: Training loss at epoch 21: 0.8173063099384308
[08/28/2025 15:56:25 INFO]: Training loss at epoch 42: 0.8707331717014313
[08/28/2025 15:56:28 INFO]: Training loss at epoch 53: 0.5784241706132889
[08/28/2025 15:56:32 INFO]: Training accuracy: {
    "score": -0.990584936635204,
    "rmse": 0.990584936635204
}
[08/28/2025 15:56:32 INFO]: Val accuracy: {
    "score": -0.6656213917834916,
    "rmse": 0.6656213917834916
}
[08/28/2025 15:56:32 INFO]: Test accuracy: {
    "score": -0.8826814787644685,
    "rmse": 0.8826814787644685
}
[08/28/2025 15:56:32 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_143",
    "best_epoch": 13,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8826814787644685,
        "rmse": 0.8826814787644685
    },
    "train_stats": {
        "score": -0.990584936635204,
        "rmse": 0.990584936635204
    },
    "val_stats": {
        "score": -0.6656213917834916,
        "rmse": 0.6656213917834916
    }
}
[08/28/2025 15:56:32 INFO]: Procewss finished for trial blotchy-Amado_trial_143
[08/28/2025 15:56:32 INFO]: 
_________________________________________________

[08/28/2025 15:56:32 INFO]: train_net_for_optune.py main() running.
[08/28/2025 15:56:32 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 16
  d_ffn_factor: 1.2229998859606606
  attention_dropout: 0.31546718803201484
  ffn_dropout: 0.31546718803201484
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.251904950276215e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_160

[08/28/2025 15:56:32 INFO]: This ft_transformer has 1.110 million parameters.
[08/28/2025 15:56:32 INFO]: Training will start at epoch 0.
[08/28/2025 15:56:32 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 15:57:02 INFO]: Training loss at epoch 42: 0.7809787690639496
[08/28/2025 15:57:15 INFO]: Training loss at epoch 3: 1.1545446515083313
[08/28/2025 15:57:18 INFO]: Training loss at epoch 31: 0.9428210556507111
[08/28/2025 15:57:39 INFO]: New best epoch, val score: -0.6539475320969751
[08/28/2025 15:57:39 INFO]: Saving model to: blotchy-Amado_trial_159/model_best.pth
[08/28/2025 15:57:45 INFO]: Training loss at epoch 8: 1.0428785681724548
[08/28/2025 15:57:59 INFO]: Training loss at epoch 38: 0.8511249721050262
[08/28/2025 15:58:05 INFO]: Training accuracy: {
    "score": -1.0162086632973537,
    "rmse": 1.0162086632973537
}
[08/28/2025 15:58:05 INFO]: Val accuracy: {
    "score": -0.6717392649665263,
    "rmse": 0.6717392649665263
}
[08/28/2025 15:58:05 INFO]: Test accuracy: {
    "score": -0.8909522536678492,
    "rmse": 0.8909522536678492
}
[08/28/2025 15:58:05 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_148",
    "best_epoch": 16,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8909522536678492,
        "rmse": 0.8909522536678492
    },
    "train_stats": {
        "score": -1.0162086632973537,
        "rmse": 1.0162086632973537
    },
    "val_stats": {
        "score": -0.6717392649665263,
        "rmse": 0.6717392649665263
    }
}
[08/28/2025 15:58:05 INFO]: Procewss finished for trial blotchy-Amado_trial_148
[08/28/2025 15:58:05 INFO]: 
_________________________________________________

[08/28/2025 15:58:05 INFO]: train_net_for_optune.py main() running.
[08/28/2025 15:58:05 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 16
  d_ffn_factor: 1.079046065668963
  attention_dropout: 0.05514610559914573
  ffn_dropout: 0.05514610559914573
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.1109984676439061e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_161

[08/28/2025 15:58:05 INFO]: This ft_transformer has 1.242 million parameters.
[08/28/2025 15:58:05 INFO]: Training will start at epoch 0.
[08/28/2025 15:58:05 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 15:58:20 INFO]: Training accuracy: {
    "score": -0.9893556662106291,
    "rmse": 0.9893556662106291
}
[08/28/2025 15:58:20 INFO]: Val accuracy: {
    "score": -0.6687285221681853,
    "rmse": 0.6687285221681853
}
[08/28/2025 15:58:20 INFO]: Test accuracy: {
    "score": -0.8741379108155815,
    "rmse": 0.8741379108155815
}
[08/28/2025 15:58:20 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_130",
    "best_epoch": 19,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8741379108155815,
        "rmse": 0.8741379108155815
    },
    "train_stats": {
        "score": -0.9893556662106291,
        "rmse": 0.9893556662106291
    },
    "val_stats": {
        "score": -0.6687285221681853,
        "rmse": 0.6687285221681853
    }
}
[08/28/2025 15:58:20 INFO]: Procewss finished for trial blotchy-Amado_trial_130
[08/28/2025 15:58:21 INFO]: 
_________________________________________________

[08/28/2025 15:58:21 INFO]: train_net_for_optune.py main() running.
[08/28/2025 15:58:21 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 2.284995986358979
  attention_dropout: 0.3654732795412392
  ffn_dropout: 0.3654732795412392
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.0946057253648967e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_162

[08/28/2025 15:58:21 INFO]: This ft_transformer has 1.460 million parameters.
[08/28/2025 15:58:21 INFO]: Training will start at epoch 0.
[08/28/2025 15:58:21 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 15:59:11 INFO]: Training loss at epoch 32: 0.8745102286338806
[08/28/2025 15:59:15 INFO]: Training loss at epoch 0: 1.285293698310852
[08/28/2025 15:59:38 INFO]: New best epoch, val score: -0.685011691023722
[08/28/2025 15:59:38 INFO]: Saving model to: blotchy-Amado_trial_160/model_best.pth
[08/28/2025 15:59:40 INFO]: Training loss at epoch 9: 1.0065842866897583
[08/28/2025 15:59:41 INFO]: Training loss at epoch 58: 0.6464257538318634
[08/28/2025 15:59:45 INFO]: Training loss at epoch 0: 1.0524869561195374
[08/28/2025 15:59:50 INFO]: Training loss at epoch 38: 1.272175371646881
[08/28/2025 15:59:56 INFO]: New best epoch, val score: -0.6573690079798331
[08/28/2025 15:59:56 INFO]: Saving model to: blotchy-Amado_trial_162/model_best.pth
[08/28/2025 16:00:20 INFO]: Training stats: {
    "score": -0.9969571500959162,
    "rmse": 0.9969571500959162
}
[08/28/2025 16:00:20 INFO]: Val stats: {
    "score": -0.6891685653966392,
    "rmse": 0.6891685653966392
}
[08/28/2025 16:00:20 INFO]: Test stats: {
    "score": -0.8757490241469268,
    "rmse": 0.8757490241469268
}
[08/28/2025 16:00:26 INFO]: Training loss at epoch 4: 0.9212963879108429
[08/28/2025 16:00:27 INFO]: Training loss at epoch 54: 0.8486259281635284
[08/28/2025 16:00:35 INFO]: Training stats: {
    "score": -0.9775246649027632,
    "rmse": 0.9775246649027632
}
[08/28/2025 16:00:35 INFO]: Val stats: {
    "score": -0.7511119882659947,
    "rmse": 0.7511119882659947
}
[08/28/2025 16:00:35 INFO]: Test stats: {
    "score": -0.9120450147145602,
    "rmse": 0.9120450147145602
}
[08/28/2025 16:00:50 INFO]: Training loss at epoch 0: 1.096970021724701
[08/28/2025 16:01:01 INFO]: Training loss at epoch 33: 1.0606545507907867
[08/28/2025 16:01:14 INFO]: New best epoch, val score: -0.8765610786905927
[08/28/2025 16:01:14 INFO]: Saving model to: blotchy-Amado_trial_161/model_best.pth
[08/28/2025 16:01:18 INFO]: Training loss at epoch 1: 1.0164401531219482
[08/28/2025 16:01:26 INFO]: Training loss at epoch 35: 1.0057968199253082
[08/28/2025 16:01:29 INFO]: Training loss at epoch 25: 0.9451809823513031
[08/28/2025 16:02:12 INFO]: Training loss at epoch 10: 0.9460780620574951
[08/28/2025 16:02:16 INFO]: Training loss at epoch 1: 1.0307635962963104
[08/28/2025 16:02:17 INFO]: Training loss at epoch 28: 0.7583796679973602
[08/28/2025 16:02:19 INFO]: Training loss at epoch 43: 1.0800509452819824
[08/28/2025 16:02:41 INFO]: New best epoch, val score: -0.6616643438931394
[08/28/2025 16:02:41 INFO]: Saving model to: blotchy-Amado_trial_160/model_best.pth
[08/28/2025 16:02:54 INFO]: Training loss at epoch 34: 1.0388537645339966
[08/28/2025 16:02:56 INFO]: Training loss at epoch 2: 1.047692060470581
[08/28/2025 16:02:57 INFO]: Training loss at epoch 58: 0.9127946197986603
[08/28/2025 16:02:57 INFO]: Training loss at epoch 54: 0.7407016754150391
[08/28/2025 16:03:08 INFO]: Running Final Evaluation...
[08/28/2025 16:03:16 INFO]: Training loss at epoch 43: 0.8164248168468475
[08/28/2025 16:03:37 INFO]: Training loss at epoch 43: 0.9969893395900726
[08/28/2025 16:03:38 INFO]: Training loss at epoch 5: 0.974134773015976
[08/28/2025 16:03:50 INFO]: Training loss at epoch 22: 1.0770654082298279
[08/28/2025 16:03:57 INFO]: Training accuracy: {
    "score": -1.0150897383820507,
    "rmse": 1.0150897383820507
}
[08/28/2025 16:03:57 INFO]: Val accuracy: {
    "score": -0.6610130309334008,
    "rmse": 0.6610130309334008
}
[08/28/2025 16:03:57 INFO]: Test accuracy: {
    "score": -0.871623262722055,
    "rmse": 0.871623262722055
}
[08/28/2025 16:03:57 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_157",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.871623262722055,
        "rmse": 0.871623262722055
    },
    "train_stats": {
        "score": -1.0150897383820507,
        "rmse": 1.0150897383820507
    },
    "val_stats": {
        "score": -0.6610130309334008,
        "rmse": 0.6610130309334008
    }
}
[08/28/2025 16:03:57 INFO]: Procewss finished for trial blotchy-Amado_trial_157
[08/28/2025 16:03:58 INFO]: 
_________________________________________________

[08/28/2025 16:03:58 INFO]: train_net_for_optune.py main() running.
[08/28/2025 16:03:58 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 1.810065423189116
  attention_dropout: 0.3855763594922055
  ffn_dropout: 0.3855763594922055
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.0125449812281296e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_163

[08/28/2025 16:03:58 INFO]: This ft_transformer has 1.303 million parameters.
[08/28/2025 16:03:58 INFO]: Training will start at epoch 0.
[08/28/2025 16:03:58 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 16:03:59 INFO]: Training loss at epoch 1: 1.1513088941574097
[08/28/2025 16:04:07 INFO]: Training loss at epoch 11: 0.8717529475688934
[08/28/2025 16:04:23 INFO]: New best epoch, val score: -0.8062373398132595
[08/28/2025 16:04:23 INFO]: Saving model to: blotchy-Amado_trial_161/model_best.pth
[08/28/2025 16:04:29 INFO]: Training loss at epoch 3: 0.8729277849197388
[08/28/2025 16:04:54 INFO]: Training loss at epoch 39: 1.0341650247573853
[08/28/2025 16:05:18 INFO]: Training loss at epoch 0: 1.242289423942566
[08/28/2025 16:05:22 INFO]: Training loss at epoch 2: 0.9519991874694824
[08/28/2025 16:05:29 INFO]: New best epoch, val score: -0.7081964352518185
[08/28/2025 16:05:29 INFO]: Saving model to: blotchy-Amado_trial_163/model_best.pth
[08/28/2025 16:06:01 INFO]: Training loss at epoch 12: 1.375723123550415
[08/28/2025 16:06:05 INFO]: Training loss at epoch 4: 1.039797842502594
[08/28/2025 16:06:09 INFO]: Training loss at epoch 39: 1.0418522357940674
[08/28/2025 16:06:22 INFO]: Training loss at epoch 59: 0.6163153052330017
[08/28/2025 16:06:47 INFO]: Training loss at epoch 1: 0.9496479034423828
[08/28/2025 16:06:50 INFO]: Training loss at epoch 6: 0.9850680530071259
[08/28/2025 16:06:58 INFO]: New best epoch, val score: -0.6609310066969608
[08/28/2025 16:06:58 INFO]: Saving model to: blotchy-Amado_trial_163/model_best.pth
[08/28/2025 16:07:08 INFO]: Training loss at epoch 2: 0.9819002151489258
[08/28/2025 16:07:18 INFO]: Training stats: {
    "score": -0.9320627024682747,
    "rmse": 0.9320627024682747
}
[08/28/2025 16:07:18 INFO]: Val stats: {
    "score": -0.7020906477101598,
    "rmse": 0.7020906477101598
}
[08/28/2025 16:07:18 INFO]: Test stats: {
    "score": -0.9005138839558846,
    "rmse": 0.9005138839558846
}
[08/28/2025 16:07:31 INFO]: New best epoch, val score: -0.7261296951881624
[08/28/2025 16:07:31 INFO]: Saving model to: blotchy-Amado_trial_161/model_best.pth
[08/28/2025 16:07:38 INFO]: Training loss at epoch 5: 0.9901905953884125
[08/28/2025 16:07:48 INFO]: Training loss at epoch 36: 0.67929707467556
[08/28/2025 16:07:54 INFO]: Training loss at epoch 13: 1.1124471426010132
[08/28/2025 16:07:58 INFO]: Training loss at epoch 26: 1.114599585533142
[08/28/2025 16:08:13 INFO]: Training loss at epoch 2: 1.2949855327606201
[08/28/2025 16:08:16 INFO]: Training stats: {
    "score": -0.9270879873427036,
    "rmse": 0.9270879873427036
}
[08/28/2025 16:08:16 INFO]: Val stats: {
    "score": -0.6698636754058496,
    "rmse": 0.6698636754058496
}
[08/28/2025 16:08:16 INFO]: Test stats: {
    "score": -0.8905845250055168,
    "rmse": 0.8905845250055168
}
[08/28/2025 16:08:21 INFO]: Training loss at epoch 3: 0.9787459075450897
[08/28/2025 16:08:38 INFO]: Training stats: {
    "score": -0.8319060491779255,
    "rmse": 0.8319060491779255
}
[08/28/2025 16:08:38 INFO]: Val stats: {
    "score": -0.7042291209013772,
    "rmse": 0.7042291209013772
}
[08/28/2025 16:08:38 INFO]: Test stats: {
    "score": -0.9483475350264058,
    "rmse": 0.9483475350264058
}
[08/28/2025 16:09:09 INFO]: Training loss at epoch 6: 0.9492504596710205
[08/28/2025 16:09:24 INFO]: Training loss at epoch 55: 0.6887027025222778
[08/28/2025 16:09:28 INFO]: Training loss at epoch 44: 0.9748565256595612
[08/28/2025 16:09:29 INFO]: Training loss at epoch 44: 0.8806244134902954
[08/28/2025 16:09:41 INFO]: Training loss at epoch 3: 1.0776080191135406
[08/28/2025 16:09:44 INFO]: Training loss at epoch 55: 0.869928389787674
[08/28/2025 16:09:45 INFO]: Training loss at epoch 14: 0.8793343901634216
[08/28/2025 16:09:57 INFO]: Training loss at epoch 7: 1.1010994911193848
[08/28/2025 16:10:11 INFO]: Training loss at epoch 29: 0.8270117044448853
[08/28/2025 16:10:12 INFO]: Training loss at epoch 3: 1.0901933908462524
[08/28/2025 16:10:35 INFO]: New best epoch, val score: -0.6681779061643291
[08/28/2025 16:10:35 INFO]: Saving model to: blotchy-Amado_trial_161/model_best.pth
[08/28/2025 16:10:44 INFO]: Training loss at epoch 7: 0.8974753320217133
[08/28/2025 16:10:48 INFO]: Training loss at epoch 44: 0.9716140925884247
[08/28/2025 16:11:12 INFO]: Training loss at epoch 4: 1.2744852304458618
[08/28/2025 16:11:24 INFO]: Training loss at epoch 4: 1.2813694477081299
[08/28/2025 16:11:40 INFO]: Training loss at epoch 15: 0.8573687076568604
[08/28/2025 16:11:40 INFO]: New best epoch, val score: -0.6761456362553087
[08/28/2025 16:11:40 INFO]: Saving model to: blotchy-Amado_trial_145/model_best.pth
[08/28/2025 16:11:43 INFO]: Training loss at epoch 23: 0.7498627603054047
[08/28/2025 16:12:21 INFO]: Training loss at epoch 8: 1.0864399671554565
[08/28/2025 16:12:39 INFO]: Training loss at epoch 5: 1.0777587294578552
[08/28/2025 16:12:51 INFO]: Training stats: {
    "score": -0.9408544303401966,
    "rmse": 0.9408544303401966
}
[08/28/2025 16:12:51 INFO]: Val stats: {
    "score": -0.757385250274017,
    "rmse": 0.757385250274017
}
[08/28/2025 16:12:51 INFO]: Test stats: {
    "score": -0.9612425558396399,
    "rmse": 0.9612425558396399
}
[08/28/2025 16:13:12 INFO]: Training loss at epoch 8: 1.1480236649513245
[08/28/2025 16:13:23 INFO]: Training loss at epoch 4: 1.048846185207367
[08/28/2025 16:13:35 INFO]: Training loss at epoch 16: 0.8836427330970764
[08/28/2025 16:13:46 INFO]: New best epoch, val score: -0.6559576869068722
[08/28/2025 16:13:46 INFO]: Saving model to: blotchy-Amado_trial_161/model_best.pth
[08/28/2025 16:13:56 INFO]: Training loss at epoch 9: 0.9975028932094574
[08/28/2025 16:14:02 INFO]: Training loss at epoch 37: 0.938604861497879
[08/28/2025 16:14:05 INFO]: Training loss at epoch 6: 1.051291674375534
[08/28/2025 16:14:08 INFO]: Training loss at epoch 40: 1.1067568957805634
[08/28/2025 16:14:23 INFO]: Training loss at epoch 5: 0.9958944916725159
[08/28/2025 16:14:23 INFO]: Training loss at epoch 27: 0.7985441386699677
[08/28/2025 16:14:27 INFO]: Training loss at epoch 10: 0.7729900479316711
[08/28/2025 16:14:31 INFO]: Training stats: {
    "score": -1.011686076610152,
    "rmse": 1.011686076610152
}
[08/28/2025 16:14:31 INFO]: Val stats: {
    "score": -0.7354718043017283,
    "rmse": 0.7354718043017283
}
[08/28/2025 16:14:31 INFO]: Test stats: {
    "score": -0.9101058653341512,
    "rmse": 0.9101058653341512
}
[08/28/2025 16:14:34 INFO]: Training loss at epoch 40: 0.885459303855896
[08/28/2025 16:14:59 INFO]: Training loss at epoch 59: 0.9595683813095093
[08/28/2025 16:15:19 INFO]: Training loss at epoch 60: 0.6865633428096771
[08/28/2025 16:15:28 INFO]: Training loss at epoch 17: 0.9013461470603943
[08/28/2025 16:15:35 INFO]: Training loss at epoch 7: 0.9461388885974884
[08/28/2025 16:15:49 INFO]: Training loss at epoch 45: 0.7716699540615082
[08/28/2025 16:15:56 INFO]: Training loss at epoch 56: 0.7778026759624481
[08/28/2025 16:16:06 INFO]: New best epoch, val score: -0.6773500697746917
[08/28/2025 16:16:06 INFO]: Saving model to: blotchy-Amado_trial_156/model_best.pth
[08/28/2025 16:16:07 INFO]: Training loss at epoch 10: 0.8698395192623138
[08/28/2025 16:16:09 INFO]: Running Final Evaluation...
[08/28/2025 16:16:24 INFO]: Training loss at epoch 9: 0.8622199594974518
[08/28/2025 16:16:34 INFO]: Training loss at epoch 5: 1.0274578034877777
[08/28/2025 16:16:41 INFO]: Training loss at epoch 45: 0.9695282578468323
[08/28/2025 16:16:44 INFO]: Running Final Evaluation...
[08/28/2025 16:17:03 INFO]: Training loss at epoch 8: 1.2012068629264832
[08/28/2025 16:17:22 INFO]: Training loss at epoch 18: 1.0133312940597534
[08/28/2025 16:17:23 INFO]: Training loss at epoch 6: 1.041304498910904
[08/28/2025 16:17:32 INFO]: Training stats: {
    "score": -1.0027990499293276,
    "rmse": 1.0027990499293276
}
[08/28/2025 16:17:32 INFO]: Val stats: {
    "score": -0.6756076181700104,
    "rmse": 0.6756076181700104
}
[08/28/2025 16:17:32 INFO]: Test stats: {
    "score": -0.8801089732674167,
    "rmse": 0.8801089732674167
}
[08/28/2025 16:17:42 INFO]: Training loss at epoch 11: 0.9181805849075317
[08/28/2025 16:17:57 INFO]: Training loss at epoch 45: 0.938825249671936
[08/28/2025 16:18:30 INFO]: Training loss at epoch 9: 1.0100978016853333
[08/28/2025 16:18:49 INFO]: New best epoch, val score: -0.6754690282119401
[08/28/2025 16:18:49 INFO]: Saving model to: blotchy-Amado_trial_145/model_best.pth
[08/28/2025 16:18:57 INFO]: Training accuracy: {
    "score": -0.9210734930955444,
    "rmse": 0.9210734930955444
}
[08/28/2025 16:18:57 INFO]: Val accuracy: {
    "score": -0.6736379086888438,
    "rmse": 0.6736379086888438
}
[08/28/2025 16:18:57 INFO]: Test accuracy: {
    "score": -0.9048075035541596,
    "rmse": 0.9048075035541596
}
[08/28/2025 16:18:57 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_132",
    "best_epoch": 29,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9048075035541596,
        "rmse": 0.9048075035541596
    },
    "train_stats": {
        "score": -0.9210734930955444,
        "rmse": 0.9210734930955444
    },
    "val_stats": {
        "score": -0.6736379086888438,
        "rmse": 0.6736379086888438
    }
}
[08/28/2025 16:18:57 INFO]: Procewss finished for trial blotchy-Amado_trial_132
[08/28/2025 16:18:58 INFO]: 
_________________________________________________

[08/28/2025 16:18:58 INFO]: train_net_for_optune.py main() running.
[08/28/2025 16:18:58 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 1.6803750132028923
  attention_dropout: 0.36549553898925347
  ffn_dropout: 0.36549553898925347
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.977295640504328e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_164

[08/28/2025 16:18:58 INFO]: This ft_transformer has 1.262 million parameters.
[08/28/2025 16:18:58 INFO]: Training will start at epoch 0.
[08/28/2025 16:18:58 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 16:19:01 INFO]: Training loss at epoch 56: 1.164029210805893
[08/28/2025 16:19:02 INFO]: Training stats: {
    "score": -1.03054441209298,
    "rmse": 1.03054441209298
}
[08/28/2025 16:19:02 INFO]: Val stats: {
    "score": -0.7805449095476654,
    "rmse": 0.7805449095476654
}
[08/28/2025 16:19:02 INFO]: Test stats: {
    "score": -0.9370061223233296,
    "rmse": 0.9370061223233296
}
[08/28/2025 16:19:08 INFO]: Training stats: {
    "score": -0.9086848035403288,
    "rmse": 0.9086848035403288
}
[08/28/2025 16:19:08 INFO]: Val stats: {
    "score": -0.7034607308034551,
    "rmse": 0.7034607308034551
}
[08/28/2025 16:19:08 INFO]: Test stats: {
    "score": -0.9329326158036816,
    "rmse": 0.9329326158036816
}
[08/28/2025 16:19:15 INFO]: Training loss at epoch 19: 0.9269343912601471
[08/28/2025 16:19:18 INFO]: Training loss at epoch 12: 0.9238670170307159
[08/28/2025 16:19:28 INFO]: Training accuracy: {
    "score": -0.9730752385673123,
    "rmse": 0.9730752385673123
}
[08/28/2025 16:19:28 INFO]: Val accuracy: {
    "score": -0.6633665030869941,
    "rmse": 0.6633665030869941
}
[08/28/2025 16:19:28 INFO]: Test accuracy: {
    "score": -0.8758891094528992,
    "rmse": 0.8758891094528992
}
[08/28/2025 16:19:28 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_137",
    "best_epoch": 25,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8758891094528992,
        "rmse": 0.8758891094528992
    },
    "train_stats": {
        "score": -0.9730752385673123,
        "rmse": 0.9730752385673123
    },
    "val_stats": {
        "score": -0.6633665030869941,
        "rmse": 0.6633665030869941
    }
}
[08/28/2025 16:19:28 INFO]: Procewss finished for trial blotchy-Amado_trial_137
[08/28/2025 16:19:28 INFO]: 
_________________________________________________

[08/28/2025 16:19:28 INFO]: train_net_for_optune.py main() running.
[08/28/2025 16:19:28 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 1.7023382774386355
  attention_dropout: 0.38532269059531
  ffn_dropout: 0.38532269059531
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.022795158894011e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_165

[08/28/2025 16:19:29 INFO]: This ft_transformer has 1.267 million parameters.
[08/28/2025 16:19:29 INFO]: Training will start at epoch 0.
[08/28/2025 16:19:29 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 16:19:33 INFO]: Training loss at epoch 24: 0.8442587852478027
[08/28/2025 16:19:40 INFO]: Training loss at epoch 6: 0.9043603539466858
[08/28/2025 16:19:56 INFO]: Training stats: {
    "score": -0.9951464786659762,
    "rmse": 0.9951464786659762
}
[08/28/2025 16:19:56 INFO]: Val stats: {
    "score": -0.6869053594240202,
    "rmse": 0.6869053594240202
}
[08/28/2025 16:19:56 INFO]: Test stats: {
    "score": -0.8750673320864735,
    "rmse": 0.8750673320864735
}
[08/28/2025 16:20:18 INFO]: Training loss at epoch 0: 1.374706506729126
[08/28/2025 16:20:20 INFO]: Training loss at epoch 38: 0.7823293507099152
[08/28/2025 16:20:24 INFO]: Training loss at epoch 7: 1.0625959038734436
[08/28/2025 16:20:29 INFO]: New best epoch, val score: -0.6659947389376263
[08/28/2025 16:20:29 INFO]: Saving model to: blotchy-Amado_trial_164/model_best.pth
[08/28/2025 16:20:33 INFO]: Training loss at epoch 10: 1.4691309332847595
[08/28/2025 16:20:38 INFO]: Running Final Evaluation...
[08/28/2025 16:20:41 INFO]: Training loss at epoch 30: 0.857469230890274
[08/28/2025 16:20:42 INFO]: Training loss at epoch 10: 0.9206587076187134
[08/28/2025 16:20:47 INFO]: Training loss at epoch 0: 1.0895318388938904
[08/28/2025 16:20:50 INFO]: Training loss at epoch 28: 0.9287566244602203
[08/28/2025 16:20:51 INFO]: Training loss at epoch 13: 1.0578923225402832
[08/28/2025 16:20:53 INFO]: Training loss at epoch 41: 0.9491229057312012
[08/28/2025 16:20:57 INFO]: New best epoch, val score: -0.6813457824484056
[08/28/2025 16:20:57 INFO]: Saving model to: blotchy-Amado_trial_165/model_best.pth
[08/28/2025 16:21:01 INFO]: Training loss at epoch 41: 0.9909980297088623
[08/28/2025 16:21:48 INFO]: Training loss at epoch 20: 0.9396860301494598
[08/28/2025 16:21:49 INFO]: Training loss at epoch 1: 0.9041196703910828
[08/28/2025 16:22:01 INFO]: New best epoch, val score: -0.6650127668509205
[08/28/2025 16:22:01 INFO]: Saving model to: blotchy-Amado_trial_164/model_best.pth
[08/28/2025 16:22:02 INFO]: Training loss at epoch 11: 0.9167770147323608
[08/28/2025 16:22:03 INFO]: Training loss at epoch 46: 0.7663594782352448
[08/28/2025 16:22:16 INFO]: Training loss at epoch 1: 1.0900384783744812
[08/28/2025 16:22:25 INFO]: Training loss at epoch 14: 0.985135555267334
[08/28/2025 16:22:27 INFO]: New best epoch, val score: -0.6691700655191829
[08/28/2025 16:22:27 INFO]: Saving model to: blotchy-Amado_trial_165/model_best.pth
[08/28/2025 16:22:45 INFO]: Training loss at epoch 7: 1.0257414877414703
[08/28/2025 16:23:19 INFO]: Training loss at epoch 2: 0.9165953993797302
[08/28/2025 16:23:28 INFO]: Training loss at epoch 8: 1.1149378418922424
[08/28/2025 16:23:32 INFO]: Training loss at epoch 12: 1.178308129310608
[08/28/2025 16:23:43 INFO]: Training loss at epoch 21: 0.8979929387569427
[08/28/2025 16:23:45 INFO]: Training loss at epoch 2: 1.030792236328125
[08/28/2025 16:23:48 INFO]: Training loss at epoch 46: 0.8695931732654572
[08/28/2025 16:23:52 INFO]: Training loss at epoch 11: 0.9066483676433563
[08/28/2025 16:23:59 INFO]: Training loss at epoch 15: 1.0844005346298218
[08/28/2025 16:24:47 INFO]: Training loss at epoch 3: 0.7911991477012634
[08/28/2025 16:25:00 INFO]: Training loss at epoch 13: 0.984056144952774
[08/28/2025 16:25:12 INFO]: Training loss at epoch 3: 1.0733811259269714
[08/28/2025 16:25:14 INFO]: Training loss at epoch 46: 1.1126722395420074
[08/28/2025 16:25:29 INFO]: Training accuracy: {
    "score": -0.9576438353111711,
    "rmse": 0.9576438353111711
}
[08/28/2025 16:25:29 INFO]: Val accuracy: {
    "score": -0.6663253748844084,
    "rmse": 0.6663253748844084
}
[08/28/2025 16:25:29 INFO]: Test accuracy: {
    "score": -0.8866273628760819,
    "rmse": 0.8866273628760819
}
[08/28/2025 16:25:29 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_101",
    "best_epoch": 28,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8866273628760819,
        "rmse": 0.8866273628760819
    },
    "train_stats": {
        "score": -0.9576438353111711,
        "rmse": 0.9576438353111711
    },
    "val_stats": {
        "score": -0.6663253748844084,
        "rmse": 0.6663253748844084
    }
}
[08/28/2025 16:25:29 INFO]: Procewss finished for trial blotchy-Amado_trial_101
[08/28/2025 16:25:29 INFO]: 
_________________________________________________

[08/28/2025 16:25:29 INFO]: train_net_for_optune.py main() running.
[08/28/2025 16:25:29 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 2.2876982730366753
  attention_dropout: 0.34693328782521393
  ffn_dropout: 0.34693328782521393
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.0151616180246086e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_166

[08/28/2025 16:25:29 INFO]: This ft_transformer has 1.460 million parameters.
[08/28/2025 16:25:29 INFO]: Training will start at epoch 0.
[08/28/2025 16:25:29 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 16:25:32 INFO]: Training loss at epoch 16: 1.0597844123840332
[08/28/2025 16:25:36 INFO]: Training loss at epoch 22: 1.026593565940857
[08/28/2025 16:25:50 INFO]: Training loss at epoch 8: 1.0942949056625366
[08/28/2025 16:26:03 INFO]: New best epoch, val score: -0.675278670865503
[08/28/2025 16:26:03 INFO]: Saving model to: blotchy-Amado_trial_145/model_best.pth
[08/28/2025 16:26:14 INFO]: Training loss at epoch 4: 0.9607518911361694
[08/28/2025 16:26:27 INFO]: Training loss at epoch 14: 1.199241042137146
[08/28/2025 16:26:28 INFO]: Training loss at epoch 9: 0.9179633557796478
[08/28/2025 16:26:40 INFO]: Training loss at epoch 4: 1.0974818468093872
[08/28/2025 16:26:40 INFO]: Training loss at epoch 39: 0.8032921254634857
[08/28/2025 16:26:56 INFO]: Training loss at epoch 0: 1.1440004110336304
[08/28/2025 16:27:03 INFO]: Training loss at epoch 12: 1.1930067241191864
[08/28/2025 16:27:07 INFO]: Training loss at epoch 17: 0.8556472957134247
[08/28/2025 16:27:09 INFO]: New best epoch, val score: -0.8359284247925317
[08/28/2025 16:27:09 INFO]: Saving model to: blotchy-Amado_trial_166/model_best.pth
[08/28/2025 16:27:14 INFO]: Training loss at epoch 42: 0.9655534029006958
[08/28/2025 16:27:18 INFO]: Training loss at epoch 29: 1.0008942186832428
[08/28/2025 16:27:26 INFO]: Training loss at epoch 25: 0.9030507504940033
[08/28/2025 16:27:31 INFO]: Training loss at epoch 23: 1.0279878079891205
[08/28/2025 16:27:36 INFO]: Training stats: {
    "score": -1.010174670513587,
    "rmse": 1.010174670513587
}
[08/28/2025 16:27:36 INFO]: Val stats: {
    "score": -0.7341778616320226,
    "rmse": 0.7341778616320226
}
[08/28/2025 16:27:36 INFO]: Test stats: {
    "score": -0.9102744325967892,
    "rmse": 0.9102744325967892
}
[08/28/2025 16:27:43 INFO]: Training loss at epoch 5: 0.9901846647262573
[08/28/2025 16:27:57 INFO]: Training loss at epoch 15: 1.0409385561943054
[08/28/2025 16:27:59 INFO]: Training loss at epoch 42: 0.9287760853767395
[08/28/2025 16:28:07 INFO]: Training loss at epoch 5: 1.0661974549293518
[08/28/2025 16:28:19 INFO]: Training loss at epoch 57: 0.825046956539154
[08/28/2025 16:28:20 INFO]: Training loss at epoch 47: 0.6066193580627441
[08/28/2025 16:28:24 INFO]: Training loss at epoch 11: 1.0370811223983765
[08/28/2025 16:28:34 INFO]: Training loss at epoch 31: 0.8949297964572906
[08/28/2025 16:28:35 INFO]: Training loss at epoch 1: 0.9650507271289825
[08/28/2025 16:28:42 INFO]: Training loss at epoch 18: 1.2508363723754883
[08/28/2025 16:28:48 INFO]: New best epoch, val score: -0.7331392310244812
[08/28/2025 16:28:48 INFO]: Saving model to: blotchy-Amado_trial_166/model_best.pth
[08/28/2025 16:28:52 INFO]: Training stats: {
    "score": -0.8969285947856168,
    "rmse": 0.8969285947856168
}
[08/28/2025 16:28:52 INFO]: Val stats: {
    "score": -0.692103969444948,
    "rmse": 0.692103969444948
}
[08/28/2025 16:28:52 INFO]: Test stats: {
    "score": -0.9497411218499636,
    "rmse": 0.9497411218499636
}
[08/28/2025 16:29:00 INFO]: Training loss at epoch 9: 1.0203955173492432
[08/28/2025 16:29:13 INFO]: Training loss at epoch 6: 1.0894794464111328
[08/28/2025 16:29:27 INFO]: Training loss at epoch 24: 0.9417636096477509
[08/28/2025 16:29:28 INFO]: Training loss at epoch 16: 0.8445116579532623
[08/28/2025 16:29:32 INFO]: Training stats: {
    "score": -0.9501199456582896,
    "rmse": 0.9501199456582896
}
[08/28/2025 16:29:32 INFO]: Val stats: {
    "score": -0.6699142859204725,
    "rmse": 0.6699142859204725
}
[08/28/2025 16:29:32 INFO]: Test stats: {
    "score": -0.8859778217969678,
    "rmse": 0.8859778217969678
}
[08/28/2025 16:29:37 INFO]: Training loss at epoch 6: 1.132985532283783
[08/28/2025 16:30:06 INFO]: Training stats: {
    "score": -1.0275840043333513,
    "rmse": 1.0275840043333513
}
[08/28/2025 16:30:06 INFO]: Val stats: {
    "score": -0.6558591253823123,
    "rmse": 0.6558591253823123
}
[08/28/2025 16:30:06 INFO]: Test stats: {
    "score": -0.8684654926713191,
    "rmse": 0.8684654926713191
}
[08/28/2025 16:30:14 INFO]: Training loss at epoch 2: 0.9403574764728546
[08/28/2025 16:30:15 INFO]: Training loss at epoch 19: 1.314506471157074
[08/28/2025 16:30:16 INFO]: Training loss at epoch 13: 0.9296669960021973
[08/28/2025 16:30:26 INFO]: New best epoch, val score: -0.6612348894213499
[08/28/2025 16:30:26 INFO]: Saving model to: blotchy-Amado_trial_166/model_best.pth
[08/28/2025 16:30:30 INFO]: New best epoch, val score: -0.6558591253823123
[08/28/2025 16:30:30 INFO]: Saving model to: blotchy-Amado_trial_161/model_best.pth
[08/28/2025 16:30:38 INFO]: Training loss at epoch 10: 1.0235012769699097
[08/28/2025 16:30:39 INFO]: Training loss at epoch 7: 1.0574407577514648
[08/28/2025 16:30:47 INFO]: Training stats: {
    "score": -0.9975128247717951,
    "rmse": 0.9975128247717951
}
[08/28/2025 16:30:47 INFO]: Val stats: {
    "score": -0.6707513854363572,
    "rmse": 0.6707513854363572
}
[08/28/2025 16:30:47 INFO]: Test stats: {
    "score": -0.8693466883106222,
    "rmse": 0.8693466883106222
}
[08/28/2025 16:30:56 INFO]: Training loss at epoch 17: 0.8440356254577637
[08/28/2025 16:30:59 INFO]: Training loss at epoch 47: 0.8346693515777588
[08/28/2025 16:31:04 INFO]: Training loss at epoch 7: 0.9522711336612701
[08/28/2025 16:31:20 INFO]: Training loss at epoch 25: 0.898171454668045
[08/28/2025 16:31:53 INFO]: Training loss at epoch 3: 1.0382212400436401
[08/28/2025 16:32:07 INFO]: Training loss at epoch 8: 1.2650173902511597
[08/28/2025 16:32:23 INFO]: Training loss at epoch 20: 0.9105334877967834
[08/28/2025 16:32:26 INFO]: Training loss at epoch 18: 0.9949354529380798
[08/28/2025 16:32:29 INFO]: Training loss at epoch 47: 0.9692232608795166
[08/28/2025 16:32:32 INFO]: Training loss at epoch 8: 1.0185626745224
[08/28/2025 16:33:14 INFO]: Training loss at epoch 10: 0.9751181602478027
[08/28/2025 16:33:15 INFO]: Training loss at epoch 26: 0.8791798651218414
[08/28/2025 16:33:29 INFO]: Training loss at epoch 14: 0.8797432482242584
[08/28/2025 16:33:34 INFO]: Training loss at epoch 4: 1.0273606777191162
[08/28/2025 16:33:36 INFO]: Training loss at epoch 43: 0.9239906668663025
[08/28/2025 16:33:37 INFO]: Training loss at epoch 9: 1.05234956741333
[08/28/2025 16:33:38 INFO]: New best epoch, val score: -0.6549997134571784
[08/28/2025 16:33:38 INFO]: Saving model to: blotchy-Amado_trial_161/model_best.pth
[08/28/2025 16:33:42 INFO]: Training loss at epoch 11: 1.1534238457679749
[08/28/2025 16:33:54 INFO]: Training loss at epoch 19: 0.9978231489658356
[08/28/2025 16:33:57 INFO]: Training loss at epoch 21: 1.1464351415634155
[08/28/2025 16:34:00 INFO]: Training loss at epoch 9: 1.0500051379203796
[08/28/2025 16:34:06 INFO]: Training stats: {
    "score": -1.0138587270874937,
    "rmse": 1.0138587270874937
}
[08/28/2025 16:34:06 INFO]: Val stats: {
    "score": -0.7417509316785752,
    "rmse": 0.7417509316785752
}
[08/28/2025 16:34:06 INFO]: Test stats: {
    "score": -0.9037126928241441,
    "rmse": 0.9037126928241441
}
[08/28/2025 16:34:24 INFO]: Training stats: {
    "score": -1.0013823600542946,
    "rmse": 1.0013823600542946
}
[08/28/2025 16:34:24 INFO]: Val stats: {
    "score": -0.6625075860615036,
    "rmse": 0.6625075860615036
}
[08/28/2025 16:34:24 INFO]: Test stats: {
    "score": -0.8677235587100373,
    "rmse": 0.8677235587100373
}
[08/28/2025 16:34:29 INFO]: Training stats: {
    "score": -1.0725622693578556,
    "rmse": 1.0725622693578556
}
[08/28/2025 16:34:29 INFO]: Val stats: {
    "score": -0.7052405162591123,
    "rmse": 0.7052405162591123
}
[08/28/2025 16:34:29 INFO]: Test stats: {
    "score": -0.9055504723386821,
    "rmse": 0.9055504723386821
}
[08/28/2025 16:34:38 INFO]: Training loss at epoch 48: 1.0516899228096008
[08/28/2025 16:34:59 INFO]: Training loss at epoch 43: 1.0059182941913605
[08/28/2025 16:35:08 INFO]: Training loss at epoch 27: 0.936794638633728
[08/28/2025 16:35:10 INFO]: Training loss at epoch 40: 0.7855705320835114
[08/28/2025 16:35:12 INFO]: Training loss at epoch 5: 1.0360804498195648
[08/28/2025 16:35:20 INFO]: Training loss at epoch 26: 0.8742246925830841
[08/28/2025 16:35:31 INFO]: Training loss at epoch 22: 0.9567114412784576
[08/28/2025 16:35:33 INFO]: Training loss at epoch 10: 1.056551992893219
[08/28/2025 16:35:52 INFO]: Training loss at epoch 20: 0.9001583158969879
[08/28/2025 16:35:57 INFO]: Training loss at epoch 10: 1.0576894581317902
[08/28/2025 16:35:58 INFO]: Training loss at epoch 30: 1.0415787994861603
[08/28/2025 16:36:19 INFO]: Training loss at epoch 11: 0.8739247620105743
[08/28/2025 16:36:28 INFO]: Training loss at epoch 32: 0.9557338356971741
[08/28/2025 16:36:40 INFO]: Training loss at epoch 15: 0.8443032801151276
[08/28/2025 16:36:44 INFO]: Training loss at epoch 12: 1.1387881636619568
[08/28/2025 16:36:51 INFO]: Training loss at epoch 6: 1.0404979586601257
[08/28/2025 16:37:03 INFO]: Training loss at epoch 11: 0.9856422543525696
[08/28/2025 16:37:03 INFO]: Training loss at epoch 28: 1.1574781835079193
[08/28/2025 16:37:06 INFO]: Training loss at epoch 23: 1.1614649295806885
[08/28/2025 16:37:23 INFO]: Training loss at epoch 21: 1.1471052765846252
[08/28/2025 16:37:26 INFO]: Training loss at epoch 11: 1.2193872928619385
[08/28/2025 16:37:41 INFO]: Training loss at epoch 58: 0.8957454264163971
[08/28/2025 16:38:12 INFO]: Training loss at epoch 48: 0.9183251857757568
[08/28/2025 16:38:30 INFO]: Training loss at epoch 7: 1.182811975479126
[08/28/2025 16:38:31 INFO]: Training loss at epoch 12: 0.8915195465087891
[08/28/2025 16:38:40 INFO]: Training loss at epoch 24: 1.0166361033916473
[08/28/2025 16:38:51 INFO]: Training loss at epoch 22: 1.1189952492713928
[08/28/2025 16:38:57 INFO]: Training loss at epoch 29: 0.8657883107662201
[08/28/2025 16:38:58 INFO]: Training loss at epoch 12: 1.2213818430900574
[08/28/2025 16:39:09 INFO]: New best epoch, val score: -0.668864918495295
[08/28/2025 16:39:09 INFO]: Saving model to: blotchy-Amado_trial_165/model_best.pth
[08/28/2025 16:39:25 INFO]: Training loss at epoch 12: 1.0018710494041443
[08/28/2025 16:39:37 INFO]: Training stats: {
    "score": -0.9941504468800367,
    "rmse": 0.9941504468800367
}
[08/28/2025 16:39:37 INFO]: Val stats: {
    "score": -0.6908375750554387,
    "rmse": 0.6908375750554387
}
[08/28/2025 16:39:37 INFO]: Test stats: {
    "score": -0.8774096115967852,
    "rmse": 0.8774096115967852
}
[08/28/2025 16:39:41 INFO]: Training loss at epoch 48: 1.065404862165451
[08/28/2025 16:39:48 INFO]: Training loss at epoch 13: 1.0448387265205383
[08/28/2025 16:39:52 INFO]: Training loss at epoch 16: 1.0974333584308624
[08/28/2025 16:39:56 INFO]: Training loss at epoch 13: 1.160257339477539
[08/28/2025 16:39:57 INFO]: Training loss at epoch 44: 0.8931438326835632
[08/28/2025 16:40:08 INFO]: Training loss at epoch 8: 0.9098857641220093
[08/28/2025 16:40:11 INFO]: Training loss at epoch 25: 0.9495851397514343
[08/28/2025 16:40:17 INFO]: Training loss at epoch 23: 1.3031153082847595
[08/28/2025 16:40:25 INFO]: Training loss at epoch 13: 1.069869577884674
[08/28/2025 16:40:37 INFO]: New best epoch, val score: -0.6688119866876341
[08/28/2025 16:40:37 INFO]: Saving model to: blotchy-Amado_trial_165/model_best.pth
[08/28/2025 16:40:57 INFO]: Training loss at epoch 49: 0.7209033370018005
[08/28/2025 16:41:26 INFO]: Training loss at epoch 14: 0.920697033405304
[08/28/2025 16:41:31 INFO]: Training loss at epoch 41: 0.8215565979480743
[08/28/2025 16:41:33 INFO]: Training loss at epoch 30: 1.0410666465759277
[08/28/2025 16:41:47 INFO]: Training loss at epoch 26: 0.9022472202777863
[08/28/2025 16:41:48 INFO]: Training loss at epoch 24: 1.3214668929576874
[08/28/2025 16:41:49 INFO]: Training loss at epoch 9: 1.128865897655487
[08/28/2025 16:41:56 INFO]: Training loss at epoch 14: 1.2289116382598877
[08/28/2025 16:41:57 INFO]: Training loss at epoch 44: 0.9398596584796906
[08/28/2025 16:42:22 INFO]: Training stats: {
    "score": -1.0260272669014046,
    "rmse": 1.0260272669014046
}
[08/28/2025 16:42:22 INFO]: Val stats: {
    "score": -0.6599353004183526,
    "rmse": 0.6599353004183526
}
[08/28/2025 16:42:22 INFO]: Test stats: {
    "score": -0.8775321627683492,
    "rmse": 0.8775321627683492
}
[08/28/2025 16:42:23 INFO]: Training loss at epoch 12: 1.1235274076461792
[08/28/2025 16:42:26 INFO]: Training loss at epoch 31: 0.8897028863430023
[08/28/2025 16:42:31 INFO]: Training loss at epoch 13: 1.0356996059417725
[08/28/2025 16:42:34 INFO]: New best epoch, val score: -0.6599353004183526
[08/28/2025 16:42:34 INFO]: Saving model to: blotchy-Amado_trial_166/model_best.pth
[08/28/2025 16:42:52 INFO]: Training loss at epoch 14: 1.0715150833129883
[08/28/2025 16:42:52 INFO]: Training loss at epoch 15: 1.0500143766403198
[08/28/2025 16:43:05 INFO]: Training stats: {
    "score": -0.8651654267896114,
    "rmse": 0.8651654267896114
}
[08/28/2025 16:43:05 INFO]: Val stats: {
    "score": -0.7033879511182417,
    "rmse": 0.7033879511182417
}
[08/28/2025 16:43:05 INFO]: Test stats: {
    "score": -0.9460715830725596,
    "rmse": 0.9460715830725596
}
[08/28/2025 16:43:05 INFO]: Training loss at epoch 17: 1.2400970160961151
[08/28/2025 16:43:14 INFO]: Training loss at epoch 27: 0.7450456321239471
[08/28/2025 16:43:17 INFO]: Training loss at epoch 25: 1.1035059690475464
[08/28/2025 16:43:17 INFO]: New best epoch, val score: -0.6613638706087412
[08/28/2025 16:43:17 INFO]: Saving model to: blotchy-Amado_trial_160/model_best.pth
[08/28/2025 16:43:21 INFO]: Training loss at epoch 27: 1.2616859674453735
[08/28/2025 16:43:24 INFO]: Training loss at epoch 15: 0.8945616781711578
[08/28/2025 16:43:26 INFO]: Training loss at epoch 31: 0.8814946115016937
[08/28/2025 16:44:00 INFO]: Training loss at epoch 10: 1.162049412727356
[08/28/2025 16:44:12 INFO]: New best epoch, val score: -0.6570697282058154
[08/28/2025 16:44:12 INFO]: Saving model to: blotchy-Amado_trial_166/model_best.pth
[08/28/2025 16:44:19 INFO]: Training loss at epoch 16: 1.0613952279090881
[08/28/2025 16:44:20 INFO]: Training loss at epoch 33: 0.7576058506965637
[08/28/2025 16:44:45 INFO]: Training loss at epoch 26: 0.9141534864902496
[08/28/2025 16:44:52 INFO]: Training loss at epoch 16: 1.1369269490242004
[08/28/2025 16:44:54 INFO]: Training loss at epoch 28: 0.9346181750297546
[08/28/2025 16:45:18 INFO]: Training loss at epoch 49: 1.0934501588344574
[08/28/2025 16:45:19 INFO]: Training loss at epoch 32: 0.8831906318664551
[08/28/2025 16:45:37 INFO]: Training loss at epoch 14: 0.9735701978206635
[08/28/2025 16:45:39 INFO]: Training loss at epoch 11: 1.1564451456069946
[08/28/2025 16:45:47 INFO]: Training loss at epoch 17: 0.9407036304473877
[08/28/2025 16:45:54 INFO]: Training loss at epoch 15: 1.0094457566738129
[08/28/2025 16:46:12 INFO]: Training loss at epoch 27: 1.152269423007965
[08/28/2025 16:46:15 INFO]: Training loss at epoch 18: 1.0758797824382782
[08/28/2025 16:46:16 INFO]: Training loss at epoch 45: 1.0012739598751068
[08/28/2025 16:46:17 INFO]: New best epoch, val score: -0.6612526019796646
[08/28/2025 16:46:17 INFO]: Saving model to: blotchy-Amado_trial_160/model_best.pth
[08/28/2025 16:46:20 INFO]: Training loss at epoch 17: 0.9384610652923584
[08/28/2025 16:46:26 INFO]: Training loss at epoch 29: 1.0939894318580627
[08/28/2025 16:46:53 INFO]: Training loss at epoch 49: 1.02824667096138
[08/28/2025 16:46:57 INFO]: Training loss at epoch 59: 1.052906334400177
[08/28/2025 16:47:00 INFO]: Training stats: {
    "score": -0.9987182356411335,
    "rmse": 0.9987182356411335
}
[08/28/2025 16:47:00 INFO]: Val stats: {
    "score": -0.6945045767759301,
    "rmse": 0.6945045767759301
}
[08/28/2025 16:47:00 INFO]: Test stats: {
    "score": -0.8828662149497535,
    "rmse": 0.8828662149497535
}
[08/28/2025 16:47:15 INFO]: Training loss at epoch 33: 0.9276910126209259
[08/28/2025 16:47:17 INFO]: Training loss at epoch 18: 1.0483886003494263
[08/28/2025 16:47:19 INFO]: Training loss at epoch 12: 0.9658337533473969
[08/28/2025 16:47:43 INFO]: Training stats: {
    "score": -0.9836955923189327,
    "rmse": 0.9836955923189327
}
[08/28/2025 16:47:43 INFO]: Val stats: {
    "score": -0.6940115313991058,
    "rmse": 0.6940115313991058
}
[08/28/2025 16:47:43 INFO]: Test stats: {
    "score": -0.9191377347420663,
    "rmse": 0.9191377347420663
}
[08/28/2025 16:47:44 INFO]: Training loss at epoch 28: 0.9466972351074219
[08/28/2025 16:47:49 INFO]: Training loss at epoch 42: 0.7578111290931702
[08/28/2025 16:47:52 INFO]: Training loss at epoch 18: 1.1784362196922302
[08/28/2025 16:48:37 INFO]: Training loss at epoch 30: 0.7569371610879898
[08/28/2025 16:48:46 INFO]: Training loss at epoch 15: 1.1226798295974731
[08/28/2025 16:48:48 INFO]: Training loss at epoch 19: 0.7944110780954361
[08/28/2025 16:48:57 INFO]: Training loss at epoch 45: 0.8890939652919769
[08/28/2025 16:48:57 INFO]: Training loss at epoch 32: 0.7715840041637421
[08/28/2025 16:49:00 INFO]: Training loss at epoch 16: 1.1996234059333801
[08/28/2025 16:49:00 INFO]: Training loss at epoch 13: 1.0913029313087463
[08/28/2025 16:49:12 INFO]: Training loss at epoch 34: 1.1525073051452637
[08/28/2025 16:49:15 INFO]: Training loss at epoch 29: 0.9341861307621002
[08/28/2025 16:49:17 INFO]: Training stats: {
    "score": -0.9996688965651405,
    "rmse": 0.9996688965651405
}
[08/28/2025 16:49:17 INFO]: Val stats: {
    "score": -0.6853493499779365,
    "rmse": 0.6853493499779365
}
[08/28/2025 16:49:17 INFO]: Test stats: {
    "score": -0.871526123868227,
    "rmse": 0.871526123868227
}
[08/28/2025 16:49:21 INFO]: Training stats: {
    "score": -0.9465848522323301,
    "rmse": 0.9465848522323301
}
[08/28/2025 16:49:21 INFO]: Val stats: {
    "score": -0.7182600618908143,
    "rmse": 0.7182600618908143
}
[08/28/2025 16:49:21 INFO]: Test stats: {
    "score": -0.9033953740339642,
    "rmse": 0.9033953740339642
}
[08/28/2025 16:49:22 INFO]: Training loss at epoch 19: 1.1803056597709656
[08/28/2025 16:49:25 INFO]: Training loss at epoch 50: 0.7932102084159851
[08/28/2025 16:49:26 INFO]: Running Final Evaluation...
[08/28/2025 16:49:32 INFO]: Training loss at epoch 19: 0.9709957838058472
[08/28/2025 16:49:45 INFO]: Training stats: {
    "score": -0.9977776530499426,
    "rmse": 0.9977776530499426
}
[08/28/2025 16:49:45 INFO]: Val stats: {
    "score": -0.6699367739762702,
    "rmse": 0.6699367739762702
}
[08/28/2025 16:49:45 INFO]: Test stats: {
    "score": -0.8692681708017149,
    "rmse": 0.8692681708017149
}
[08/28/2025 16:49:53 INFO]: Training stats: {
    "score": -0.9994624103719991,
    "rmse": 0.9994624103719991
}
[08/28/2025 16:49:53 INFO]: Val stats: {
    "score": -0.6813404205597642,
    "rmse": 0.6813404205597642
}
[08/28/2025 16:49:53 INFO]: Test stats: {
    "score": -0.8627036694007302,
    "rmse": 0.8627036694007302
}
[08/28/2025 16:50:12 INFO]: Training loss at epoch 31: 1.1959789097309113
[08/28/2025 16:50:13 INFO]: Training stats: {
    "score": -0.9629824870773291,
    "rmse": 0.9629824870773291
}
[08/28/2025 16:50:13 INFO]: Val stats: {
    "score": -0.7733439443647576,
    "rmse": 0.7733439443647576
}
[08/28/2025 16:50:13 INFO]: Test stats: {
    "score": -0.9518730258897499,
    "rmse": 0.9518730258897499
}
[08/28/2025 16:50:14 INFO]: Training accuracy: {
    "score": -1.0197138716299563,
    "rmse": 1.0197138716299563
}
[08/28/2025 16:50:14 INFO]: Val accuracy: {
    "score": -0.6621067867584016,
    "rmse": 0.6621067867584016
}
[08/28/2025 16:50:14 INFO]: Test accuracy: {
    "score": -0.8749135126999701,
    "rmse": 0.8749135126999701
}
[08/28/2025 16:50:14 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_158",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8749135126999701,
        "rmse": 0.8749135126999701
    },
    "train_stats": {
        "score": -1.0197138716299563,
        "rmse": 1.0197138716299563
    },
    "val_stats": {
        "score": -0.6621067867584016,
        "rmse": 0.6621067867584016
    }
}
[08/28/2025 16:50:14 INFO]: Procewss finished for trial blotchy-Amado_trial_158
[08/28/2025 16:50:15 INFO]: 
_________________________________________________

[08/28/2025 16:50:15 INFO]: train_net_for_optune.py main() running.
[08/28/2025 16:50:15 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 1.6864026164722095
  attention_dropout: 0.3696369756066371
  ffn_dropout: 0.3696369756066371
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.0445060176894163e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_167

[08/28/2025 16:50:15 INFO]: This ft_transformer has 1.539 million parameters.
[08/28/2025 16:50:15 INFO]: Training will start at epoch 0.
[08/28/2025 16:50:15 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 16:50:23 INFO]: Running Final Evaluation...
[08/28/2025 16:50:38 INFO]: Training loss at epoch 14: 1.0106413662433624
[08/28/2025 16:50:43 INFO]: Training stats: {
    "score": -1.002863169322607,
    "rmse": 1.002863169322607
}
[08/28/2025 16:50:43 INFO]: Val stats: {
    "score": -0.6647911567202788,
    "rmse": 0.6647911567202788
}
[08/28/2025 16:50:43 INFO]: Test stats: {
    "score": -0.8755044433009945,
    "rmse": 0.8755044433009945
}
[08/28/2025 16:50:45 INFO]: Training loss at epoch 20: 0.9722642600536346
[08/28/2025 16:51:01 INFO]: Training accuracy: {
    "score": -1.0076188829724897,
    "rmse": 1.0076188829724897
}
[08/28/2025 16:51:01 INFO]: Val accuracy: {
    "score": -0.6573690079798331,
    "rmse": 0.6573690079798331
}
[08/28/2025 16:51:01 INFO]: Test accuracy: {
    "score": -0.8633288457989513,
    "rmse": 0.8633288457989513
}
[08/28/2025 16:51:01 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_162",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8633288457989513,
        "rmse": 0.8633288457989513
    },
    "train_stats": {
        "score": -1.0076188829724897,
        "rmse": 1.0076188829724897
    },
    "val_stats": {
        "score": -0.6573690079798331,
        "rmse": 0.6573690079798331
    }
}
[08/28/2025 16:51:01 INFO]: Procewss finished for trial blotchy-Amado_trial_162
[08/28/2025 16:51:01 INFO]: 
_________________________________________________

[08/28/2025 16:51:01 INFO]: train_net_for_optune.py main() running.
[08/28/2025 16:51:01 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 1.9415517136635616
  attention_dropout: 0.3174478728586401
  ffn_dropout: 0.3174478728586401
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.034864096942567e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_168

[08/28/2025 16:51:01 INFO]: This ft_transformer has 1.017 million parameters.
[08/28/2025 16:51:01 INFO]: Training will start at epoch 0.
[08/28/2025 16:51:01 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 16:51:08 INFO]: Training loss at epoch 28: 0.8461036086082458
[08/28/2025 16:51:14 INFO]: Training loss at epoch 30: 1.007219523191452
[08/28/2025 16:51:21 INFO]: Training loss at epoch 20: 0.8687281608581543
[08/28/2025 16:51:39 INFO]: Training loss at epoch 0: 1.1629731059074402
[08/28/2025 16:51:50 INFO]: New best epoch, val score: -0.8610797139877198
[08/28/2025 16:51:50 INFO]: Saving model to: blotchy-Amado_trial_167/model_best.pth
[08/28/2025 16:51:51 INFO]: Training loss at epoch 0: 0.9179665148258209
[08/28/2025 16:51:52 INFO]: Training loss at epoch 16: 1.041521966457367
[08/28/2025 16:51:58 INFO]: New best epoch, val score: -0.7423068133675268
[08/28/2025 16:51:58 INFO]: Saving model to: blotchy-Amado_trial_168/model_best.pth
[08/28/2025 16:52:02 INFO]: Training loss at epoch 17: 0.9962483942508698
[08/28/2025 16:52:15 INFO]: Training loss at epoch 21: 1.1607081890106201
[08/28/2025 16:52:19 INFO]: Training loss at epoch 34: 1.0039618015289307
[08/28/2025 16:52:19 INFO]: Training loss at epoch 15: 1.0957468152046204
[08/28/2025 16:52:43 INFO]: Training loss at epoch 46: 0.9889195263385773
[08/28/2025 16:52:46 INFO]: Training loss at epoch 31: 0.9973253607749939
[08/28/2025 16:52:48 INFO]: Training loss at epoch 1: 1.0484536290168762
[08/28/2025 16:52:49 INFO]: Training loss at epoch 21: 0.9711946249008179
[08/28/2025 16:52:55 INFO]: New best epoch, val score: -0.6874379928541797
[08/28/2025 16:52:55 INFO]: Saving model to: blotchy-Amado_trial_168/model_best.pth
[08/28/2025 16:53:15 INFO]: Training loss at epoch 1: 0.9800174236297607
[08/28/2025 16:53:26 INFO]: New best epoch, val score: -0.7786991091542853
[08/28/2025 16:53:26 INFO]: Saving model to: blotchy-Amado_trial_167/model_best.pth
[08/28/2025 16:53:42 INFO]: Training loss at epoch 22: 0.9256214499473572
[08/28/2025 16:53:44 INFO]: Training loss at epoch 2: 1.0535656213760376
[08/28/2025 16:53:51 INFO]: New best epoch, val score: -0.6763249598508889
[08/28/2025 16:53:51 INFO]: Saving model to: blotchy-Amado_trial_168/model_best.pth
[08/28/2025 16:53:56 INFO]: Training loss at epoch 20: 1.01573646068573
[08/28/2025 16:53:57 INFO]: Training loss at epoch 16: 0.9021086096763611
[08/28/2025 16:54:13 INFO]: Training loss at epoch 43: 0.8846052587032318
[08/28/2025 16:54:15 INFO]: Training loss at epoch 32: 0.9972316324710846
[08/28/2025 16:54:17 INFO]: Training loss at epoch 22: 1.0421019196510315
[08/28/2025 16:54:25 INFO]: Running Final Evaluation...
[08/28/2025 16:54:39 INFO]: Training loss at epoch 3: 0.8916036784648895
[08/28/2025 16:54:51 INFO]: Training loss at epoch 2: 1.007652461528778
[08/28/2025 16:54:56 INFO]: Training loss at epoch 50: 0.8596819043159485
[08/28/2025 16:54:59 INFO]: Training loss at epoch 17: 1.2603895664215088
[08/28/2025 16:55:03 INFO]: Training accuracy: {
    "score": -1.0229095546415639,
    "rmse": 1.0229095546415639
}
[08/28/2025 16:55:03 INFO]: Val accuracy: {
    "score": -0.6609310066969608,
    "rmse": 0.6609310066969608
}
[08/28/2025 16:55:03 INFO]: Test accuracy: {
    "score": -0.8766166162327768,
    "rmse": 0.8766166162327768
}
[08/28/2025 16:55:03 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_163",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8766166162327768,
        "rmse": 0.8766166162327768
    },
    "train_stats": {
        "score": -1.0229095546415639,
        "rmse": 1.0229095546415639
    },
    "val_stats": {
        "score": -0.6609310066969608,
        "rmse": 0.6609310066969608
    }
}
[08/28/2025 16:55:03 INFO]: Procewss finished for trial blotchy-Amado_trial_163
[08/28/2025 16:55:03 INFO]: 
_________________________________________________

[08/28/2025 16:55:03 INFO]: train_net_for_optune.py main() running.
[08/28/2025 16:55:03 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 1.682570945568199
  attention_dropout: 0.11091959858657421
  ffn_dropout: 0.11091959858657421
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.631757171893514e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_169

[08/28/2025 16:55:03 INFO]: This ft_transformer has 0.941 million parameters.
[08/28/2025 16:55:03 INFO]: Training will start at epoch 0.
[08/28/2025 16:55:03 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 16:55:04 INFO]: New best epoch, val score: -0.7183156131828363
[08/28/2025 16:55:04 INFO]: Saving model to: blotchy-Amado_trial_167/model_best.pth
[08/28/2025 16:55:04 INFO]: Training loss at epoch 18: 0.8500948250293732
[08/28/2025 16:55:10 INFO]: Training loss at epoch 23: 0.7588375806808472
[08/28/2025 16:55:26 INFO]: Training loss at epoch 33: 0.8580967783927917
[08/28/2025 16:55:35 INFO]: Training loss at epoch 17: 1.2696036398410797
[08/28/2025 16:55:36 INFO]: Training loss at epoch 4: 1.2263582944869995
[08/28/2025 16:55:44 INFO]: Training loss at epoch 23: 1.3731826543807983
[08/28/2025 16:55:45 INFO]: Training loss at epoch 51: 0.720205694437027
[08/28/2025 16:55:50 INFO]: Training loss at epoch 0: 1.1561937928199768
[08/28/2025 16:55:56 INFO]: New best epoch, val score: -0.6851994847452172
[08/28/2025 16:55:56 INFO]: Saving model to: blotchy-Amado_trial_169/model_best.pth
[08/28/2025 16:55:57 INFO]: Training loss at epoch 46: 1.0343213975429535
[08/28/2025 16:56:21 INFO]: Training loss at epoch 13: 1.0027044415473938
[08/28/2025 16:56:27 INFO]: Training loss at epoch 3: 1.141254484653473
[08/28/2025 16:56:32 INFO]: Training loss at epoch 5: 0.8621082603931427
[08/28/2025 16:56:35 INFO]: Training loss at epoch 50: 1.0304920971393585
[08/28/2025 16:56:37 INFO]: Training loss at epoch 24: 1.1847968697547913
[08/28/2025 16:56:40 INFO]: New best epoch, val score: -0.7089046676487167
[08/28/2025 16:56:40 INFO]: Saving model to: blotchy-Amado_trial_167/model_best.pth
[08/28/2025 16:56:45 INFO]: Training loss at epoch 1: 1.0953741073608398
[08/28/2025 16:56:51 INFO]: New best epoch, val score: -0.6820980097690174
[08/28/2025 16:56:51 INFO]: Saving model to: blotchy-Amado_trial_169/model_best.pth
[08/28/2025 16:57:06 INFO]: Training loss at epoch 21: 0.9380944669246674
[08/28/2025 16:57:11 INFO]: Training loss at epoch 24: 0.9468901455402374
[08/28/2025 16:57:13 INFO]: Training loss at epoch 18: 1.0471398830413818
[08/28/2025 16:57:27 INFO]: Training loss at epoch 6: 0.9118544459342957
[08/28/2025 16:57:38 INFO]: Training loss at epoch 2: 0.9334602057933807
[08/28/2025 16:58:02 INFO]: Training loss at epoch 4: 1.0043321549892426
[08/28/2025 16:58:03 INFO]: Training loss at epoch 25: 1.226658582687378
[08/28/2025 16:58:03 INFO]: Training loss at epoch 18: 1.0910566449165344
[08/28/2025 16:58:04 INFO]: Training loss at epoch 19: 0.93490070104599
[08/28/2025 16:58:22 INFO]: Training loss at epoch 7: 1.192211925983429
[08/28/2025 16:58:30 INFO]: Training loss at epoch 3: 1.2192654013633728
[08/28/2025 16:58:37 INFO]: Training loss at epoch 25: 0.9884382784366608
[08/28/2025 16:58:51 INFO]: Training loss at epoch 19: 0.7849489599466324
[08/28/2025 16:59:02 INFO]: Training loss at epoch 47: 0.9662398099899292
[08/28/2025 16:59:04 INFO]: Training loss at epoch 29: 0.8222623467445374
[08/28/2025 16:59:12 INFO]: Training stats: {
    "score": -1.0111728687520642,
    "rmse": 1.0111728687520642
}
[08/28/2025 16:59:12 INFO]: Val stats: {
    "score": -0.6612066012955956,
    "rmse": 0.6612066012955956
}
[08/28/2025 16:59:12 INFO]: Test stats: {
    "score": -0.872325445156864,
    "rmse": 0.872325445156864
}
[08/28/2025 16:59:20 INFO]: Training loss at epoch 8: 0.9954541027545929
[08/28/2025 16:59:26 INFO]: Training loss at epoch 4: 0.9159465134143829
[08/28/2025 16:59:27 INFO]: Training stats: {
    "score": -1.007979149459856,
    "rmse": 1.007979149459856
}
[08/28/2025 16:59:27 INFO]: Val stats: {
    "score": -0.7220456469522305,
    "rmse": 0.7220456469522305
}
[08/28/2025 16:59:27 INFO]: Test stats: {
    "score": -0.8952960367249655,
    "rmse": 0.8952960367249655
}
[08/28/2025 16:59:33 INFO]: Training loss at epoch 60: 0.9079442620277405
[08/28/2025 16:59:33 INFO]: Training loss at epoch 26: 1.3233113288879395
[08/28/2025 16:59:38 INFO]: New best epoch, val score: -0.6612066012955956
[08/28/2025 16:59:38 INFO]: Saving model to: blotchy-Amado_trial_160/model_best.pth
[08/28/2025 16:59:40 INFO]: Training loss at epoch 5: 1.070763349533081
[08/28/2025 17:00:05 INFO]: Training loss at epoch 26: 0.9027560353279114
[08/28/2025 17:00:10 INFO]: Training loss at epoch 35: 0.8745525777339935
[08/28/2025 17:00:15 INFO]: Training loss at epoch 9: 1.4516839385032654
[08/28/2025 17:00:18 INFO]: Training loss at epoch 22: 1.047348141670227
[08/28/2025 17:00:19 INFO]: Training loss at epoch 5: 1.5299417972564697
[08/28/2025 17:00:31 INFO]: Training loss at epoch 44: 0.9917443990707397
[08/28/2025 17:00:34 INFO]: Training stats: {
    "score": -1.0005466376917738,
    "rmse": 1.0005466376917738
}
[08/28/2025 17:00:34 INFO]: Val stats: {
    "score": -0.68876625836613,
    "rmse": 0.68876625836613
}
[08/28/2025 17:00:34 INFO]: Test stats: {
    "score": -0.8731880722385155,
    "rmse": 0.8731880722385155
}
[08/28/2025 17:01:01 INFO]: Training loss at epoch 27: 1.0951555967330933
[08/28/2025 17:01:05 INFO]: Training loss at epoch 20: 1.3204866647720337
[08/28/2025 17:01:10 INFO]: Training loss at epoch 19: 0.9530828595161438
[08/28/2025 17:01:15 INFO]: Training loss at epoch 6: 1.0977586507797241
[08/28/2025 17:01:17 INFO]: Training loss at epoch 6: 1.2863067090511322
[08/28/2025 17:01:33 INFO]: Training loss at epoch 10: 0.868148922920227
[08/28/2025 17:01:35 INFO]: Training loss at epoch 27: 1.1272521018981934
[08/28/2025 17:01:48 INFO]: Training stats: {
    "score": -0.8955473270867762,
    "rmse": 0.8955473270867762
}
[08/28/2025 17:01:48 INFO]: Val stats: {
    "score": -0.7037821228358627,
    "rmse": 0.7037821228358627
}
[08/28/2025 17:01:48 INFO]: Test stats: {
    "score": -0.9141203982169592,
    "rmse": 0.9141203982169592
}
[08/28/2025 17:01:55 INFO]: Training loss at epoch 34: 0.9220486879348755
[08/28/2025 17:02:06 INFO]: Training loss at epoch 52: 0.7154505550861359
[08/28/2025 17:02:10 INFO]: Training loss at epoch 51: 1.0828858017921448
[08/28/2025 17:02:10 INFO]: Training loss at epoch 7: 1.3948156833648682
[08/28/2025 17:02:18 INFO]: Training loss at epoch 20: 1.115786075592041
[08/28/2025 17:02:21 INFO]: Training stats: {
    "score": -1.0057868037927684,
    "rmse": 1.0057868037927684
}
[08/28/2025 17:02:21 INFO]: Val stats: {
    "score": -0.6875090653512768,
    "rmse": 0.6875090653512768
}
[08/28/2025 17:02:21 INFO]: Test stats: {
    "score": -0.8729986836545313,
    "rmse": 0.8729986836545313
}
[08/28/2025 17:02:29 INFO]: Training loss at epoch 11: 1.2071510553359985
[08/28/2025 17:02:30 INFO]: Training loss at epoch 28: 0.836613804101944
[08/28/2025 17:02:45 INFO]: Training loss at epoch 21: 0.9434786736965179
[08/28/2025 17:02:54 INFO]: Training loss at epoch 7: 0.9188737869262695
[08/28/2025 17:03:00 INFO]: Training loss at epoch 47: 0.7402968108654022
[08/28/2025 17:03:04 INFO]: Training loss at epoch 8: 1.5015888810157776
[08/28/2025 17:03:04 INFO]: Training loss at epoch 28: 0.9558368623256683
[08/28/2025 17:03:27 INFO]: Training loss at epoch 12: 1.0649617910385132
[08/28/2025 17:03:35 INFO]: Training loss at epoch 23: 1.0340585708618164
[08/28/2025 17:03:53 INFO]: Training loss at epoch 51: 1.0100925266742706
[08/28/2025 17:04:00 INFO]: Training loss at epoch 9: 1.428775668144226
[08/28/2025 17:04:00 INFO]: Training loss at epoch 29: 1.0786330103874207
[08/28/2025 17:04:17 INFO]: Training stats: {
    "score": -1.0630758889416447,
    "rmse": 1.0630758889416447
}
[08/28/2025 17:04:17 INFO]: Val stats: {
    "score": -0.8549084913047993,
    "rmse": 0.8549084913047993
}
[08/28/2025 17:04:17 INFO]: Test stats: {
    "score": -0.9898720962394937,
    "rmse": 0.9898720962394937
}
[08/28/2025 17:04:23 INFO]: Training loss at epoch 13: 0.969720721244812
[08/28/2025 17:04:26 INFO]: Training loss at epoch 22: 1.19972825050354
[08/28/2025 17:04:30 INFO]: Training stats: {
    "score": -1.0061618143786275,
    "rmse": 1.0061618143786275
}
[08/28/2025 17:04:30 INFO]: Val stats: {
    "score": -0.7241611553438283,
    "rmse": 0.7241611553438283
}
[08/28/2025 17:04:30 INFO]: Test stats: {
    "score": -0.8928359310560284,
    "rmse": 0.8928359310560284
}
[08/28/2025 17:04:31 INFO]: Training loss at epoch 8: 1.1940737962722778
[08/28/2025 17:04:33 INFO]: Training loss at epoch 29: 1.2662951350212097
[08/28/2025 17:05:04 INFO]: Training stats: {
    "score": -1.0035668065168646,
    "rmse": 1.0035668065168646
}
[08/28/2025 17:05:04 INFO]: Val stats: {
    "score": -0.6698944893683924,
    "rmse": 0.6698944893683924
}
[08/28/2025 17:05:04 INFO]: Test stats: {
    "score": -0.8613814020280766,
    "rmse": 0.8613814020280766
}
[08/28/2025 17:05:12 INFO]: Training loss at epoch 10: 1.2360403537750244
[08/28/2025 17:05:19 INFO]: Training loss at epoch 14: 1.0310987532138824
[08/28/2025 17:05:21 INFO]: Training loss at epoch 21: 0.8434436619281769
[08/28/2025 17:05:29 INFO]: Training loss at epoch 20: 0.9126181602478027
[08/28/2025 17:05:29 INFO]: Training loss at epoch 48: 0.8000583648681641
[08/28/2025 17:05:57 INFO]: Training loss at epoch 30: 1.0835543274879456
[08/28/2025 17:06:04 INFO]: Training loss at epoch 11: 0.8065229952335358
[08/28/2025 17:06:04 INFO]: Training loss at epoch 23: 0.9383961856365204
[08/28/2025 17:06:07 INFO]: Training loss at epoch 9: 1.2555513381958008
[08/28/2025 17:06:13 INFO]: Training loss at epoch 15: 0.8708993196487427
[08/28/2025 17:06:30 INFO]: Training loss at epoch 30: 1.2290610671043396
[08/28/2025 17:06:38 INFO]: Training stats: {
    "score": -1.0088594323880058,
    "rmse": 1.0088594323880058
}
[08/28/2025 17:06:38 INFO]: Val stats: {
    "score": -0.7211361622138701,
    "rmse": 0.7211361622138701
}
[08/28/2025 17:06:38 INFO]: Test stats: {
    "score": -0.9019313468908614,
    "rmse": 0.9019313468908614
}
[08/28/2025 17:06:41 INFO]: New best epoch, val score: -0.6687597385711745
[08/28/2025 17:06:41 INFO]: Saving model to: blotchy-Amado_trial_165/model_best.pth
[08/28/2025 17:06:45 INFO]: Training loss at epoch 24: 0.852048933506012
[08/28/2025 17:06:56 INFO]: Training loss at epoch 45: 0.7775673866271973
[08/28/2025 17:06:58 INFO]: Training loss at epoch 12: 1.098377764225006
[08/28/2025 17:07:10 INFO]: Training loss at epoch 16: 1.0742256045341492
[08/28/2025 17:07:25 INFO]: Training loss at epoch 31: 0.8738303184509277
[08/28/2025 17:07:42 INFO]: Training loss at epoch 24: 0.8886018991470337
[08/28/2025 17:07:50 INFO]: Training loss at epoch 13: 1.1405853629112244
[08/28/2025 17:07:57 INFO]: Training loss at epoch 31: 0.975833386182785
[08/28/2025 17:08:05 INFO]: Training loss at epoch 17: 1.0422768592834473
[08/28/2025 17:08:08 INFO]: Training loss at epoch 36: 0.8251858651638031
[08/28/2025 17:08:09 INFO]: New best epoch, val score: -0.6677985513758767
[08/28/2025 17:08:09 INFO]: Saving model to: blotchy-Amado_trial_165/model_best.pth
[08/28/2025 17:08:16 INFO]: Training loss at epoch 10: 1.0139715671539307
[08/28/2025 17:08:22 INFO]: Training loss at epoch 22: 1.0424591898918152
[08/28/2025 17:08:24 INFO]: Training loss at epoch 53: 0.7350607514381409
[08/28/2025 17:08:25 INFO]: Training loss at epoch 35: 0.9841302037239075
[08/28/2025 17:08:33 INFO]: Training loss at epoch 21: 0.9419232904911041
[08/28/2025 17:08:45 INFO]: Training loss at epoch 14: 1.1382714807987213
[08/28/2025 17:08:51 INFO]: New best epoch, val score: -0.6726027025076582
[08/28/2025 17:08:51 INFO]: Saving model to: blotchy-Amado_trial_169/model_best.pth
[08/28/2025 17:08:53 INFO]: Training loss at epoch 32: 1.0130868554115295
[08/28/2025 17:08:54 INFO]: Training loss at epoch 61: 0.7996439039707184
[08/28/2025 17:09:01 INFO]: Training loss at epoch 18: 1.145774483680725
[08/28/2025 17:09:03 INFO]: Running Final Evaluation...
[08/28/2025 17:09:08 INFO]: New best epoch, val score: -0.6762936684292901
[08/28/2025 17:09:08 INFO]: Saving model to: blotchy-Amado_trial_168/model_best.pth
[08/28/2025 17:09:21 INFO]: Training loss at epoch 52: 0.866039514541626
[08/28/2025 17:09:22 INFO]: Training loss at epoch 25: 0.9697665572166443
[08/28/2025 17:09:27 INFO]: Training loss at epoch 32: 1.1654492020606995
[08/28/2025 17:09:39 INFO]: Training loss at epoch 15: 1.052053540945053
[08/28/2025 17:09:39 INFO]: Training accuracy: {
    "score": -1.0104918992654965,
    "rmse": 1.0104918992654965
}
[08/28/2025 17:09:39 INFO]: Val accuracy: {
    "score": -0.6650127668509205,
    "rmse": 0.6650127668509205
}
[08/28/2025 17:09:39 INFO]: Test accuracy: {
    "score": -0.8685716008091581,
    "rmse": 0.8685716008091581
}
[08/28/2025 17:09:39 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_164",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8685716008091581,
        "rmse": 0.8685716008091581
    },
    "train_stats": {
        "score": -1.0104918992654965,
        "rmse": 1.0104918992654965
    },
    "val_stats": {
        "score": -0.6650127668509205,
        "rmse": 0.6650127668509205
    }
}
[08/28/2025 17:09:39 INFO]: Procewss finished for trial blotchy-Amado_trial_164
[08/28/2025 17:09:40 INFO]: 
_________________________________________________

[08/28/2025 17:09:40 INFO]: train_net_for_optune.py main() running.
[08/28/2025 17:09:40 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 2.4378150588029586
  attention_dropout: 0.1067560035482535
  ffn_dropout: 0.1067560035482535
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.651182118468611e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_170

[08/28/2025 17:09:40 INFO]: This ft_transformer has 1.165 million parameters.
[08/28/2025 17:09:40 INFO]: Training will start at epoch 0.
[08/28/2025 17:09:40 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 17:09:46 INFO]: New best epoch, val score: -0.6646109214217065
[08/28/2025 17:09:46 INFO]: Saving model to: blotchy-Amado_trial_169/model_best.pth
[08/28/2025 17:09:47 INFO]: Training loss at epoch 30: 0.7127891480922699
[08/28/2025 17:09:52 INFO]: Training loss at epoch 11: 0.9041853547096252
[08/28/2025 17:09:59 INFO]: Training loss at epoch 25: 0.8816279172897339
[08/28/2025 17:09:59 INFO]: Training loss at epoch 48: 1.06642085313797
[08/28/2025 17:09:59 INFO]: Training loss at epoch 19: 0.9954490661621094
[08/28/2025 17:10:03 INFO]: Running Final Evaluation...
[08/28/2025 17:10:19 INFO]: Training stats: {
    "score": -1.0022630034859543,
    "rmse": 1.0022630034859543
}
[08/28/2025 17:10:19 INFO]: Val stats: {
    "score": -0.6751095669434871,
    "rmse": 0.6751095669434871
}
[08/28/2025 17:10:19 INFO]: Test stats: {
    "score": -0.8697715269559898,
    "rmse": 0.8697715269559898
}
[08/28/2025 17:10:25 INFO]: New best epoch, val score: -0.6751095669434871
[08/28/2025 17:10:25 INFO]: Saving model to: blotchy-Amado_trial_168/model_best.pth
[08/28/2025 17:10:26 INFO]: Training loss at epoch 14: 0.8429453372955322
[08/28/2025 17:10:34 INFO]: Training loss at epoch 16: 0.8629074096679688
[08/28/2025 17:10:40 INFO]: New best epoch, val score: -0.6627126547647427
[08/28/2025 17:10:40 INFO]: Saving model to: blotchy-Amado_trial_169/model_best.pth
[08/28/2025 17:10:57 INFO]: Training loss at epoch 33: 0.9537400305271149
[08/28/2025 17:11:02 INFO]: Training loss at epoch 26: 1.0414289236068726
[08/28/2025 17:11:07 INFO]: Training loss at epoch 52: 0.822568416595459
[08/28/2025 17:11:16 INFO]: Training loss at epoch 20: 1.1852243542671204
[08/28/2025 17:11:22 INFO]: New best epoch, val score: -0.6743968775901986
[08/28/2025 17:11:22 INFO]: Saving model to: blotchy-Amado_trial_168/model_best.pth
[08/28/2025 17:11:23 INFO]: Training loss at epoch 0: 1.3599741458892822
[08/28/2025 17:11:26 INFO]: Training loss at epoch 23: 1.0791290402412415
[08/28/2025 17:11:29 INFO]: Training loss at epoch 17: 1.2183606922626495
[08/28/2025 17:11:30 INFO]: Training loss at epoch 12: 1.149981141090393
[08/28/2025 17:11:37 INFO]: New best epoch, val score: -0.7710482535417533
[08/28/2025 17:11:37 INFO]: Saving model to: blotchy-Amado_trial_170/model_best.pth
[08/28/2025 17:11:41 INFO]: Training loss at epoch 22: 0.9708715081214905
[08/28/2025 17:11:50 INFO]: Training loss at epoch 49: 0.8111443817615509
[08/28/2025 17:12:05 INFO]: New best epoch, val score: -0.6693443185217691
[08/28/2025 17:12:05 INFO]: Saving model to: blotchy-Amado_trial_156/model_best.pth
[08/28/2025 17:12:11 INFO]: Training loss at epoch 21: 1.167621910572052
[08/28/2025 17:12:19 INFO]: New best epoch, val score: -0.6742307978780904
[08/28/2025 17:12:19 INFO]: Saving model to: blotchy-Amado_trial_168/model_best.pth
[08/28/2025 17:12:24 INFO]: Training loss at epoch 18: 1.0789865255355835
[08/28/2025 17:12:25 INFO]: Training loss at epoch 34: 0.9732546806335449
[08/28/2025 17:12:42 INFO]: Training loss at epoch 27: 0.9268564879894257
[08/28/2025 17:13:06 INFO]: Training loss at epoch 13: 1.0676254034042358
[08/28/2025 17:13:08 INFO]: Training loss at epoch 22: 1.0326231718063354
[08/28/2025 17:13:11 INFO]: Training loss at epoch 26: 1.0262067317962646
[08/28/2025 17:13:15 INFO]: Training loss at epoch 46: 0.7543152272701263
[08/28/2025 17:13:16 INFO]: Training loss at epoch 19: 0.9532499015331268
[08/28/2025 17:13:19 INFO]: Training loss at epoch 1: 1.2339646220207214
[08/28/2025 17:13:33 INFO]: New best epoch, val score: -0.6783885131183609
[08/28/2025 17:13:33 INFO]: Saving model to: blotchy-Amado_trial_170/model_best.pth
[08/28/2025 17:13:34 INFO]: Training stats: {
    "score": -1.0168567073310921,
    "rmse": 1.0168567073310921
}
[08/28/2025 17:13:34 INFO]: Val stats: {
    "score": -0.6637049200846129,
    "rmse": 0.6637049200846129
}
[08/28/2025 17:13:34 INFO]: Test stats: {
    "score": -0.8810722757342467,
    "rmse": 0.8810722757342467
}
[08/28/2025 17:13:47 INFO]: Training accuracy: {
    "score": -0.9705377724569448,
    "rmse": 0.9705377724569448
}
[08/28/2025 17:13:47 INFO]: Val accuracy: {
    "score": -0.6792409109468005,
    "rmse": 0.6792409109468005
}
[08/28/2025 17:13:47 INFO]: Test accuracy: {
    "score": -0.8753508120996932,
    "rmse": 0.8753508120996932
}
[08/28/2025 17:13:48 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_117",
    "best_epoch": 30,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8753508120996932,
        "rmse": 0.8753508120996932
    },
    "train_stats": {
        "score": -0.9705377724569448,
        "rmse": 0.9705377724569448
    },
    "val_stats": {
        "score": -0.6792409109468005,
        "rmse": 0.6792409109468005
    }
}
[08/28/2025 17:13:48 INFO]: Procewss finished for trial blotchy-Amado_trial_117
[08/28/2025 17:13:48 INFO]: 
_________________________________________________

[08/28/2025 17:13:48 INFO]: train_net_for_optune.py main() running.
[08/28/2025 17:13:48 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 1.9187783438409762
  attention_dropout: 0.4067319628368248
  ffn_dropout: 0.4067319628368248
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.565914534645131e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_171

[08/28/2025 17:13:48 INFO]: This ft_transformer has 11.668 million parameters.
[08/28/2025 17:13:48 INFO]: Training will start at epoch 0.
[08/28/2025 17:13:48 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 17:13:50 INFO]: Training loss at epoch 35: 1.0530645251274109
[08/28/2025 17:14:00 INFO]: Training stats: {
    "score": -0.9089554713278384,
    "rmse": 0.9089554713278384
}
[08/28/2025 17:14:00 INFO]: Val stats: {
    "score": -0.6629544641379937,
    "rmse": 0.6629544641379937
}
[08/28/2025 17:14:00 INFO]: Test stats: {
    "score": -0.9064667490269608,
    "rmse": 0.9064667490269608
}
[08/28/2025 17:14:04 INFO]: Training loss at epoch 23: 1.0130450129508972
[08/28/2025 17:14:20 INFO]: Training loss at epoch 28: 1.1646885573863983
[08/28/2025 17:14:28 INFO]: Training loss at epoch 24: 1.124691128730774
[08/28/2025 17:14:28 INFO]: Training loss at epoch 20: 1.0354086756706238
[08/28/2025 17:14:42 INFO]: Training loss at epoch 14: 1.1219177842140198
[08/28/2025 17:14:44 INFO]: Training loss at epoch 54: 0.7354108989238739
[08/28/2025 17:14:45 INFO]: Training loss at epoch 23: 1.0719386339187622
[08/28/2025 17:14:54 INFO]: Training loss at epoch 36: 1.0639964044094086
[08/28/2025 17:15:00 INFO]: Training loss at epoch 24: 0.9172912538051605
[08/28/2025 17:15:17 INFO]: Training loss at epoch 2: 1.0421562194824219
[08/28/2025 17:15:20 INFO]: Training loss at epoch 36: 1.1057385802268982
[08/28/2025 17:15:22 INFO]: Training loss at epoch 21: 0.9966966807842255
[08/28/2025 17:15:29 INFO]: New best epoch, val score: -0.6622135324907747
[08/28/2025 17:15:29 INFO]: Saving model to: blotchy-Amado_trial_169/model_best.pth
[08/28/2025 17:15:56 INFO]: Training loss at epoch 25: 1.0506533086299896
[08/28/2025 17:15:58 INFO]: Training loss at epoch 29: 0.8715876042842865
[08/28/2025 17:16:03 INFO]: Training loss at epoch 37: 1.1166231334209442
[08/28/2025 17:16:16 INFO]: Training loss at epoch 22: 1.0357490181922913
[08/28/2025 17:16:19 INFO]: Training loss at epoch 15: 1.0408214926719666
[08/28/2025 17:16:25 INFO]: Training loss at epoch 27: 0.8907485008239746
[08/28/2025 17:16:33 INFO]: Training loss at epoch 53: 1.2163564264774323
[08/28/2025 17:16:34 INFO]: Training stats: {
    "score": -0.9990612706096416,
    "rmse": 0.9990612706096416
}
[08/28/2025 17:16:34 INFO]: Val stats: {
    "score": -0.6733881817847602,
    "rmse": 0.6733881817847602
}
[08/28/2025 17:16:34 INFO]: Test stats: {
    "score": -0.8682616086144355,
    "rmse": 0.8682616086144355
}
[08/28/2025 17:16:48 INFO]: Training loss at epoch 37: 1.0469059944152832
[08/28/2025 17:16:54 INFO]: Training loss at epoch 26: 1.0464592576026917
[08/28/2025 17:17:01 INFO]: Training loss at epoch 49: 0.8903837203979492
[08/28/2025 17:17:11 INFO]: Training loss at epoch 23: 1.1175369620323181
[08/28/2025 17:17:14 INFO]: Training loss at epoch 3: 0.9414081275463104
[08/28/2025 17:17:33 INFO]: Training loss at epoch 25: 0.8541487753391266
[08/28/2025 17:17:50 INFO]: Training loss at epoch 31: 0.7435973584651947
[08/28/2025 17:17:52 INFO]: Training loss at epoch 27: 1.0047302842140198
[08/28/2025 17:17:55 INFO]: Training loss at epoch 24: 1.0759421288967133
[08/28/2025 17:17:58 INFO]: Training loss at epoch 16: 0.9695217311382294
[08/28/2025 17:18:06 INFO]: Training loss at epoch 24: 1.114368587732315
[08/28/2025 17:18:15 INFO]: Training loss at epoch 30: 1.1053909659385681
[08/28/2025 17:18:17 INFO]: Training loss at epoch 38: 1.1911526918411255
[08/28/2025 17:18:22 INFO]: Training loss at epoch 53: 1.0632870197296143
[08/28/2025 17:18:46 INFO]: Training loss at epoch 28: 1.1659095883369446
[08/28/2025 17:18:57 INFO]: Training loss at epoch 0: 1.2158215641975403
[08/28/2025 17:18:58 INFO]: Training loss at epoch 25: 0.839127391576767
[08/28/2025 17:19:10 INFO]: Training loss at epoch 4: 1.1218474507331848
[08/28/2025 17:19:24 INFO]: Training stats: {
    "score": -0.9222339907824427,
    "rmse": 0.9222339907824427
}
[08/28/2025 17:19:24 INFO]: Val stats: {
    "score": -0.6928909097389402,
    "rmse": 0.6928909097389402
}
[08/28/2025 17:19:24 INFO]: Test stats: {
    "score": -0.9023574525977308,
    "rmse": 0.9023574525977308
}
[08/28/2025 17:19:34 INFO]: Training loss at epoch 17: 0.9981275796890259
[08/28/2025 17:19:37 INFO]: Training loss at epoch 47: 0.8705905973911285
[08/28/2025 17:19:38 INFO]: New best epoch, val score: -0.6806183891499898
[08/28/2025 17:19:38 INFO]: Saving model to: blotchy-Amado_trial_171/model_best.pth
[08/28/2025 17:19:39 INFO]: Training loss at epoch 28: 1.01852947473526
[08/28/2025 17:19:43 INFO]: Training loss at epoch 29: 0.9260920286178589
[08/28/2025 17:19:44 INFO]: Training loss at epoch 39: 0.9802350401878357
[08/28/2025 17:19:53 INFO]: Training loss at epoch 26: 1.128341555595398
[08/28/2025 17:19:53 INFO]: Training loss at epoch 31: 1.2585783004760742
[08/28/2025 17:20:01 INFO]: Training stats: {
    "score": -0.9951210802306196,
    "rmse": 0.9951210802306196
}
[08/28/2025 17:20:01 INFO]: Val stats: {
    "score": -0.6796936990376856,
    "rmse": 0.6796936990376856
}
[08/28/2025 17:20:01 INFO]: Test stats: {
    "score": -0.8723814419699568,
    "rmse": 0.8723814419699568
}
[08/28/2025 17:20:14 INFO]: Training stats: {
    "score": -1.0086941665083065,
    "rmse": 1.0086941665083065
}
[08/28/2025 17:20:14 INFO]: Val stats: {
    "score": -0.6686228172297806,
    "rmse": 0.6686228172297806
}
[08/28/2025 17:20:14 INFO]: Test stats: {
    "score": -0.8640968989156445,
    "rmse": 0.8640968989156445
}
[08/28/2025 17:20:20 INFO]: Running Final Evaluation...
[08/28/2025 17:20:23 INFO]: Training loss at epoch 50: 0.8235450387001038
[08/28/2025 17:20:34 INFO]: Training loss at epoch 26: 0.866552084684372
[08/28/2025 17:20:46 INFO]: Training loss at epoch 27: 1.007411003112793
[08/28/2025 17:20:56 INFO]: Training loss at epoch 30: 1.088890790939331
[08/28/2025 17:20:58 INFO]: Training loss at epoch 25: 0.9969930350780487
[08/28/2025 17:21:04 INFO]: Training loss at epoch 55: 0.8446975946426392
[08/28/2025 17:21:06 INFO]: Training loss at epoch 5: 1.0751880705356598
[08/28/2025 17:21:07 INFO]: Training loss at epoch 18: 1.2548764944076538
[08/28/2025 17:21:22 INFO]: Training loss at epoch 37: 0.9736491143703461
[08/28/2025 17:21:29 INFO]: Training loss at epoch 32: 0.7439897358417511
[08/28/2025 17:21:39 INFO]: Training loss at epoch 40: 1.0756986141204834
[08/28/2025 17:21:40 INFO]: Training loss at epoch 28: 0.8957959413528442
[08/28/2025 17:21:50 INFO]: Training loss at epoch 31: 1.1955364644527435
[08/28/2025 17:22:33 INFO]: Training loss at epoch 29: 1.2512537240982056
[08/28/2025 17:22:42 INFO]: Training loss at epoch 19: 0.9852105975151062
[08/28/2025 17:22:46 INFO]: Training loss at epoch 32: 1.107003629207611
[08/28/2025 17:22:49 INFO]: Training loss at epoch 29: 1.136748731136322
[08/28/2025 17:22:51 INFO]: Training accuracy: {
    "score": -0.9610088024348035,
    "rmse": 0.9610088024348035
}
[08/28/2025 17:22:51 INFO]: Val accuracy: {
    "score": -0.6772557544415817,
    "rmse": 0.6772557544415817
}
[08/28/2025 17:22:51 INFO]: Test accuracy: {
    "score": -0.9158215497777081,
    "rmse": 0.9158215497777081
}
[08/28/2025 17:22:51 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_152",
    "best_epoch": 16,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9158215497777081,
        "rmse": 0.9158215497777081
    },
    "train_stats": {
        "score": -0.9610088024348035,
        "rmse": 0.9610088024348035
    },
    "val_stats": {
        "score": -0.6772557544415817,
        "rmse": 0.6772557544415817
    }
}
[08/28/2025 17:22:51 INFO]: Procewss finished for trial blotchy-Amado_trial_152
[08/28/2025 17:22:52 INFO]: 
_________________________________________________

[08/28/2025 17:22:52 INFO]: train_net_for_optune.py main() running.
[08/28/2025 17:22:52 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 1.7166836184315684
  attention_dropout: 0.4031794931747336
  ffn_dropout: 0.4031794931747336
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.6607589952737484e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_172

[08/28/2025 17:22:52 INFO]: This ft_transformer has 10.958 million parameters.
[08/28/2025 17:22:52 INFO]: Training will start at epoch 0.
[08/28/2025 17:22:52 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 17:22:53 INFO]: Training stats: {
    "score": -0.9931210938738654,
    "rmse": 0.9931210938738654
}
[08/28/2025 17:22:53 INFO]: Val stats: {
    "score": -0.6930074708459389,
    "rmse": 0.6930074708459389
}
[08/28/2025 17:22:53 INFO]: Test stats: {
    "score": -0.8817898739981088,
    "rmse": 0.8817898739981088
}
[08/28/2025 17:23:01 INFO]: Training loss at epoch 6: 1.4930895566940308
[08/28/2025 17:23:05 INFO]: Training loss at epoch 41: 1.1821205615997314
[08/28/2025 17:23:08 INFO]: Training loss at epoch 33: 1.0698797702789307
[08/28/2025 17:23:16 INFO]: Training stats: {
    "score": -1.0276892350822282,
    "rmse": 1.0276892350822282
}
[08/28/2025 17:23:16 INFO]: Val stats: {
    "score": -0.7692522293286717,
    "rmse": 0.7692522293286717
}
[08/28/2025 17:23:16 INFO]: Test stats: {
    "score": -0.9363271713576146,
    "rmse": 0.9363271713576146
}
[08/28/2025 17:23:38 INFO]: Training loss at epoch 27: 0.8610790967941284
[08/28/2025 17:23:39 INFO]: Training loss at epoch 54: 0.869656890630722
[08/28/2025 17:23:42 INFO]: Training loss at epoch 33: 1.2267727255821228
[08/28/2025 17:23:49 INFO]: Training loss at epoch 30: 0.8182094097137451
[08/28/2025 17:23:55 INFO]: Training loss at epoch 38: 0.8349350094795227
[08/28/2025 17:23:59 INFO]: Training stats: {
    "score": -1.0004734108940023,
    "rmse": 1.0004734108940023
}
[08/28/2025 17:23:59 INFO]: Val stats: {
    "score": -0.6953200812096095,
    "rmse": 0.6953200812096095
}
[08/28/2025 17:23:59 INFO]: Test stats: {
    "score": -0.8902033356133461,
    "rmse": 0.8902033356133461
}
[08/28/2025 17:24:01 INFO]: Training loss at epoch 26: 0.991140604019165
[08/28/2025 17:24:21 INFO]: Training loss at epoch 15: 1.0484773516654968
[08/28/2025 17:24:31 INFO]: Training loss at epoch 42: 1.0088278353214264
[08/28/2025 17:24:37 INFO]: Training loss at epoch 34: 0.9788941740989685
[08/28/2025 17:24:37 INFO]: Training loss at epoch 1: 1.0894628167152405
[08/28/2025 17:24:42 INFO]: Training loss at epoch 31: 1.0205938816070557
[08/28/2025 17:24:45 INFO]: Training loss at epoch 34: 0.9736470580101013
[08/28/2025 17:24:52 INFO]: Training loss at epoch 20: 0.9382056295871735
[08/28/2025 17:24:56 INFO]: Training loss at epoch 7: 1.3458241820335388
[08/28/2025 17:25:24 INFO]: Training loss at epoch 54: 1.0496317148208618
[08/28/2025 17:25:32 INFO]: Training loss at epoch 35: 1.070388376712799
[08/28/2025 17:25:37 INFO]: Training loss at epoch 32: 0.7228759527206421
[08/28/2025 17:25:37 INFO]: Training loss at epoch 32: 1.0803983211517334
[08/28/2025 17:25:57 INFO]: Training loss at epoch 43: 1.299284815788269
[08/28/2025 17:26:15 INFO]: Training loss at epoch 50: 0.9696305692195892
[08/28/2025 17:26:24 INFO]: Training loss at epoch 35: 0.9790321290493011
[08/28/2025 17:26:28 INFO]: Training loss at epoch 36: 1.140362799167633
[08/28/2025 17:26:30 INFO]: Training loss at epoch 21: 0.9836077690124512
[08/28/2025 17:26:35 INFO]: Training loss at epoch 33: 0.9839901328086853
[08/28/2025 17:26:38 INFO]: Training loss at epoch 51: 1.116275042295456
[08/28/2025 17:26:40 INFO]: Training loss at epoch 28: 1.1518301367759705
[08/28/2025 17:26:53 INFO]: Training loss at epoch 8: 1.2250309586524963
[08/28/2025 17:27:06 INFO]: Training loss at epoch 27: 0.9997372031211853
[08/28/2025 17:27:11 INFO]: Training loss at epoch 30: 0.8532904088497162
[08/28/2025 17:27:16 INFO]: Training loss at epoch 56: 0.84941765666008
[08/28/2025 17:27:25 INFO]: Training loss at epoch 44: 0.9460607469081879
[08/28/2025 17:27:25 INFO]: Training loss at epoch 37: 1.1191805005073547
[08/28/2025 17:27:31 INFO]: Training loss at epoch 34: 0.9968198835849762
[08/28/2025 17:27:43 INFO]: Training loss at epoch 0: 1.1950145363807678
[08/28/2025 17:27:47 INFO]: Training loss at epoch 38: 0.7647634744644165
[08/28/2025 17:28:06 INFO]: Training loss at epoch 36: 1.072398066520691
[08/28/2025 17:28:11 INFO]: Training loss at epoch 22: 1.1009438633918762
[08/28/2025 17:28:23 INFO]: New best epoch, val score: -0.7611336236549696
[08/28/2025 17:28:23 INFO]: Saving model to: blotchy-Amado_trial_172/model_best.pth
[08/28/2025 17:28:25 INFO]: Training loss at epoch 38: 0.944532722234726
[08/28/2025 17:28:29 INFO]: Training loss at epoch 35: 1.1748996078968048
[08/28/2025 17:28:52 INFO]: Training loss at epoch 9: 1.0012205839157104
[08/28/2025 17:28:55 INFO]: Training loss at epoch 45: 0.7905210554599762
[08/28/2025 17:29:20 INFO]: Training loss at epoch 39: 0.8210610747337341
[08/28/2025 17:29:22 INFO]: Training loss at epoch 36: 0.9885998070240021
[08/28/2025 17:29:33 INFO]: Training stats: {
    "score": -1.0821784565839232,
    "rmse": 1.0821784565839232
}
[08/28/2025 17:29:33 INFO]: Val stats: {
    "score": -0.8941682785821353,
    "rmse": 0.8941682785821353
}
[08/28/2025 17:29:33 INFO]: Test stats: {
    "score": -1.0062857525797029,
    "rmse": 1.0062857525797029
}
[08/28/2025 17:29:38 INFO]: Training stats: {
    "score": -0.9915568399541644,
    "rmse": 0.9915568399541644
}
[08/28/2025 17:29:38 INFO]: Val stats: {
    "score": -0.6802440787490789,
    "rmse": 0.6802440787490789
}
[08/28/2025 17:29:38 INFO]: Test stats: {
    "score": -0.8736115299452462,
    "rmse": 0.8736115299452462
}
[08/28/2025 17:29:44 INFO]: Training loss at epoch 29: 1.0444111227989197
[08/28/2025 17:29:45 INFO]: Training loss at epoch 37: 0.8622986972332001
[08/28/2025 17:29:46 INFO]: Training loss at epoch 23: 1.1622378826141357
[08/28/2025 17:30:13 INFO]: Training loss at epoch 28: 0.85091432929039
[08/28/2025 17:30:17 INFO]: Training loss at epoch 37: 0.8570947647094727
[08/28/2025 17:30:23 INFO]: Training loss at epoch 46: 0.8401732444763184
[08/28/2025 17:30:26 INFO]: Training loss at epoch 2: 1.123313546180725
[08/28/2025 17:30:29 INFO]: Training loss at epoch 31: 1.1026592254638672
[08/28/2025 17:30:35 INFO]: Training loss at epoch 40: 0.988565593957901
[08/28/2025 17:30:49 INFO]: Training loss at epoch 55: 0.9732229709625244
[08/28/2025 17:30:53 INFO]: Training stats: {
    "score": -0.9982783080180705,
    "rmse": 0.9982783080180705
}
[08/28/2025 17:30:53 INFO]: Val stats: {
    "score": -0.6751203828371368,
    "rmse": 0.6751203828371368
}
[08/28/2025 17:30:53 INFO]: Test stats: {
    "score": -0.87383113245912,
    "rmse": 0.87383113245912
}
[08/28/2025 17:31:05 INFO]: New best epoch, val score: -0.6765935378439722
[08/28/2025 17:31:05 INFO]: Saving model to: blotchy-Amado_trial_171/model_best.pth
[08/28/2025 17:31:10 INFO]: Training loss at epoch 38: 0.9961842000484467
[08/28/2025 17:31:21 INFO]: Training loss at epoch 24: 1.2048377990722656
[08/28/2025 17:31:24 INFO]: Training loss at epoch 38: 1.0144919753074646
[08/28/2025 17:31:29 INFO]: Training loss at epoch 10: 1.206273078918457
[08/28/2025 17:31:30 INFO]: Training loss at epoch 41: 0.922179639339447
[08/28/2025 17:31:47 INFO]: Training loss at epoch 39: 0.7389882504940033
[08/28/2025 17:31:47 INFO]: Training loss at epoch 47: 1.1683388948440552
[08/28/2025 17:32:02 INFO]: Training loss at epoch 39: 0.934156596660614
[08/28/2025 17:32:20 INFO]: Training stats: {
    "score": -0.9909112271664829,
    "rmse": 0.9909112271664829
}
[08/28/2025 17:32:20 INFO]: Val stats: {
    "score": -0.6890293884781036,
    "rmse": 0.6890293884781036
}
[08/28/2025 17:32:20 INFO]: Test stats: {
    "score": -0.8801899847280474,
    "rmse": 0.8801899847280474
}
[08/28/2025 17:32:25 INFO]: Training loss at epoch 42: 0.961706280708313
[08/28/2025 17:32:35 INFO]: Training loss at epoch 55: 0.9027076065540314
[08/28/2025 17:32:57 INFO]: Training loss at epoch 25: 0.9147961735725403
[08/28/2025 17:32:59 INFO]: Training loss at epoch 52: 1.1174694895744324
[08/28/2025 17:33:02 INFO]: Training loss at epoch 39: 1.0892407894134521
[08/28/2025 17:33:10 INFO]: Training loss at epoch 1: 1.1262593865394592
[08/28/2025 17:33:14 INFO]: Training loss at epoch 51: 0.9819261133670807
[08/28/2025 17:33:14 INFO]: Training loss at epoch 48: 0.8729450404644012
[08/28/2025 17:33:15 INFO]: Training loss at epoch 40: 0.9419845342636108
[08/28/2025 17:33:15 INFO]: Training loss at epoch 29: 1.116531878709793
[08/28/2025 17:33:22 INFO]: Training loss at epoch 43: 0.8188315033912659
[08/28/2025 17:33:24 INFO]: Training loss at epoch 11: 0.9713107347488403
[08/28/2025 17:33:30 INFO]: Training loss at epoch 33: 0.9923729598522186
[08/28/2025 17:33:30 INFO]: Training loss at epoch 57: 0.9348485469818115
[08/28/2025 17:33:34 INFO]: Training stats: {
    "score": -0.9985664253776896,
    "rmse": 0.9985664253776896
}
[08/28/2025 17:33:34 INFO]: Val stats: {
    "score": -0.6851376186881498,
    "rmse": 0.6851376186881498
}
[08/28/2025 17:33:34 INFO]: Test stats: {
    "score": -0.8732834230266878,
    "rmse": 0.8732834230266878
}
[08/28/2025 17:33:40 INFO]: Training loss at epoch 32: 0.921056717634201
[08/28/2025 17:33:46 INFO]: New best epoch, val score: -0.7038346872242607
[08/28/2025 17:33:46 INFO]: Saving model to: blotchy-Amado_trial_172/model_best.pth
[08/28/2025 17:33:58 INFO]: Training loss at epoch 30: 1.0935903787612915
[08/28/2025 17:34:08 INFO]: Training loss at epoch 41: 0.9851783812046051
[08/28/2025 17:34:10 INFO]: Training loss at epoch 39: 0.8424136936664581
[08/28/2025 17:34:13 INFO]: Running Final Evaluation...
[08/28/2025 17:34:16 INFO]: Training loss at epoch 44: 1.0749595761299133
[08/28/2025 17:34:21 INFO]: Training stats: {
    "score": -1.0039970429507428,
    "rmse": 1.0039970429507428
}
[08/28/2025 17:34:21 INFO]: Val stats: {
    "score": -0.6725798927506403,
    "rmse": 0.6725798927506403
}
[08/28/2025 17:34:21 INFO]: Test stats: {
    "score": -0.8664849055597931,
    "rmse": 0.8664849055597931
}
[08/28/2025 17:34:25 INFO]: Training stats: {
    "score": -0.8713538651245456,
    "rmse": 0.8713538651245456
}
[08/28/2025 17:34:25 INFO]: Val stats: {
    "score": -0.7081932544153474,
    "rmse": 0.7081932544153474
}
[08/28/2025 17:34:25 INFO]: Test stats: {
    "score": -0.9421638169496024,
    "rmse": 0.9421638169496024
}
[08/28/2025 17:34:31 INFO]: Training loss at epoch 26: 0.9870949387550354
[08/28/2025 17:34:39 INFO]: Training loss at epoch 49: 0.9704505801200867
[08/28/2025 17:35:00 INFO]: Training loss at epoch 42: 0.9939290285110474
[08/28/2025 17:35:09 INFO]: Training stats: {
    "score": -0.99976762631405,
    "rmse": 0.99976762631405
}
[08/28/2025 17:35:09 INFO]: Val stats: {
    "score": -0.6747892374338413,
    "rmse": 0.6747892374338413
}
[08/28/2025 17:35:09 INFO]: Test stats: {
    "score": -0.8635846374973989,
    "rmse": 0.8635846374973989
}
[08/28/2025 17:35:11 INFO]: Training loss at epoch 40: 1.0559000372886658
[08/28/2025 17:35:11 INFO]: Training loss at epoch 45: 0.8755935430526733
[08/28/2025 17:35:17 INFO]: Training loss at epoch 12: 0.7800330221652985
[08/28/2025 17:35:55 INFO]: Training loss at epoch 43: 0.837096244096756
[08/28/2025 17:36:06 INFO]: Training loss at epoch 3: 1.089612364768982
[08/28/2025 17:36:07 INFO]: Training loss at epoch 27: 1.3192870318889618
[08/28/2025 17:36:08 INFO]: Training loss at epoch 46: 1.1274340152740479
[08/28/2025 17:36:21 INFO]: Training stats: {
    "score": -0.9341212420227674,
    "rmse": 0.9341212420227674
}
[08/28/2025 17:36:21 INFO]: Val stats: {
    "score": -0.7142155824446077,
    "rmse": 0.7142155824446077
}
[08/28/2025 17:36:21 INFO]: Test stats: {
    "score": -0.9124087306148965,
    "rmse": 0.9124087306148965
}
[08/28/2025 17:36:35 INFO]: Training loss at epoch 50: 0.9379103779792786
[08/28/2025 17:36:36 INFO]: Training accuracy: {
    "score": -0.9392683600992803,
    "rmse": 0.9392683600992803
}
[08/28/2025 17:36:36 INFO]: Val accuracy: {
    "score": -0.6557043059792318,
    "rmse": 0.6557043059792318
}
[08/28/2025 17:36:36 INFO]: Test accuracy: {
    "score": -0.9058807224755,
    "rmse": 0.9058807224755
}
[08/28/2025 17:36:36 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_149",
    "best_epoch": 26,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9058807224755,
        "rmse": 0.9058807224755
    },
    "train_stats": {
        "score": -0.9392683600992803,
        "rmse": 0.9392683600992803
    },
    "val_stats": {
        "score": -0.6557043059792318,
        "rmse": 0.6557043059792318
    }
}
[08/28/2025 17:36:36 INFO]: Procewss finished for trial blotchy-Amado_trial_149
[08/28/2025 17:36:36 INFO]: 
_________________________________________________

[08/28/2025 17:36:36 INFO]: train_net_for_optune.py main() running.
[08/28/2025 17:36:36 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 2.051759375779412
  attention_dropout: 0.17427438120241961
  ffn_dropout: 0.17427438120241961
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.6590130173145294e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_173

[08/28/2025 17:36:36 INFO]: This ft_transformer has 12.139 million parameters.
[08/28/2025 17:36:36 INFO]: Training will start at epoch 0.
[08/28/2025 17:36:36 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 17:36:46 INFO]: New best epoch, val score: -0.6688703519836847
[08/28/2025 17:36:46 INFO]: Saving model to: blotchy-Amado_trial_171/model_best.pth
[08/28/2025 17:36:50 INFO]: Training loss at epoch 44: 0.8307627439498901
[08/28/2025 17:36:51 INFO]: Training loss at epoch 33: 1.0473324656486511
[08/28/2025 17:36:52 INFO]: Training loss at epoch 41: 0.868512749671936
[08/28/2025 17:37:02 INFO]: Training loss at epoch 31: 0.9085403978824615
[08/28/2025 17:37:04 INFO]: Running Final Evaluation...
[08/28/2025 17:37:05 INFO]: Training loss at epoch 47: 0.9839012324810028
[08/28/2025 17:37:16 INFO]: Training loss at epoch 13: 1.1607151329517365
[08/28/2025 17:37:26 INFO]: Training loss at epoch 30: 1.1345458030700684
[08/28/2025 17:37:41 INFO]: Training accuracy: {
    "score": -1.0114603427990838,
    "rmse": 1.0114603427990838
}
[08/28/2025 17:37:41 INFO]: Val accuracy: {
    "score": -0.6570697282058154,
    "rmse": 0.6570697282058154
}
[08/28/2025 17:37:41 INFO]: Test accuracy: {
    "score": -0.8689480309292822,
    "rmse": 0.8689480309292822
}
[08/28/2025 17:37:41 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_166",
    "best_epoch": 10,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8689480309292822,
        "rmse": 0.8689480309292822
    },
    "train_stats": {
        "score": -1.0114603427990838,
        "rmse": 1.0114603427990838
    },
    "val_stats": {
        "score": -0.6570697282058154,
        "rmse": 0.6570697282058154
    }
}
[08/28/2025 17:37:41 INFO]: Procewss finished for trial blotchy-Amado_trial_166
[08/28/2025 17:37:41 INFO]: 
_________________________________________________

[08/28/2025 17:37:41 INFO]: train_net_for_optune.py main() running.
[08/28/2025 17:37:41 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 1.9286773618989574
  attention_dropout: 0.41051903412073504
  ffn_dropout: 0.41051903412073504
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.7622423867705495e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_174

[08/28/2025 17:37:41 INFO]: This ft_transformer has 11.705 million parameters.
[08/28/2025 17:37:41 INFO]: Training will start at epoch 0.
[08/28/2025 17:37:41 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 17:37:43 INFO]: Training loss at epoch 28: 1.1251388788223267
[08/28/2025 17:37:43 INFO]: Training loss at epoch 45: 0.8795263469219208
[08/28/2025 17:37:52 INFO]: Training loss at epoch 56: 0.9301974475383759
[08/28/2025 17:37:59 INFO]: Training loss at epoch 48: 1.3323447406291962
[08/28/2025 17:38:03 INFO]: Training loss at epoch 51: 1.0624192357063293
[08/28/2025 17:38:15 INFO]: Training loss at epoch 16: 0.7933743000030518
[08/28/2025 17:38:35 INFO]: Training loss at epoch 2: 1.0175144672393799
[08/28/2025 17:38:38 INFO]: Training loss at epoch 46: 0.9647247791290283
[08/28/2025 17:38:57 INFO]: Training loss at epoch 49: 0.9479588866233826
[08/28/2025 17:39:14 INFO]: Training loss at epoch 14: 1.0543664991855621
[08/28/2025 17:39:16 INFO]: New best epoch, val score: -0.6874423299027559
[08/28/2025 17:39:16 INFO]: Saving model to: blotchy-Amado_trial_172/model_best.pth
[08/28/2025 17:39:18 INFO]: Training stats: {
    "score": -0.989012307936056,
    "rmse": 0.989012307936056
}
[08/28/2025 17:39:18 INFO]: Val stats: {
    "score": -0.6859177359643135,
    "rmse": 0.6859177359643135
}
[08/28/2025 17:39:18 INFO]: Test stats: {
    "score": -0.8767358051443628,
    "rmse": 0.8767358051443628
}
[08/28/2025 17:39:21 INFO]: Training loss at epoch 53: 0.8124560117721558
[08/28/2025 17:39:22 INFO]: Training loss at epoch 29: 1.0290324687957764
[08/28/2025 17:39:34 INFO]: Training loss at epoch 52: 1.0123590528964996
[08/28/2025 17:39:35 INFO]: Training loss at epoch 47: 1.0782545804977417
[08/28/2025 17:39:49 INFO]: Training loss at epoch 56: 1.1292717456817627
[08/28/2025 17:39:55 INFO]: Training stats: {
    "score": -1.0318845370765162,
    "rmse": 1.0318845370765162
}
[08/28/2025 17:39:55 INFO]: Val stats: {
    "score": -0.7792654835834476,
    "rmse": 0.7792654835834476
}
[08/28/2025 17:39:55 INFO]: Test stats: {
    "score": -0.9432955463209952,
    "rmse": 0.9432955463209952
}
[08/28/2025 17:40:03 INFO]: Training loss at epoch 34: 1.4094485938549042
[08/28/2025 17:40:08 INFO]: Training loss at epoch 32: 0.9833794236183167
[08/28/2025 17:40:14 INFO]: Training loss at epoch 52: 0.7151107043027878
[08/28/2025 17:40:15 INFO]: Training loss at epoch 50: 0.9855101406574249
[08/28/2025 17:40:29 INFO]: Running Final Evaluation...
[08/28/2025 17:40:29 INFO]: Training loss at epoch 48: 0.9603902101516724
[08/28/2025 17:40:34 INFO]: Training loss at epoch 31: 1.0281323790550232
[08/28/2025 17:41:01 INFO]: Training loss at epoch 53: 1.154037594795227
[08/28/2025 17:41:10 INFO]: Training loss at epoch 51: 0.9470559656620026
[08/28/2025 17:41:11 INFO]: Training loss at epoch 15: 0.9073383510112762
[08/28/2025 17:41:22 INFO]: Training loss at epoch 49: 0.9730861783027649
[08/28/2025 17:41:23 INFO]: Training loss at epoch 34: 0.7786488234996796
[08/28/2025 17:41:30 INFO]: Training loss at epoch 30: 1.1504268050193787
[08/28/2025 17:41:40 INFO]: Training stats: {
    "score": -0.9921128909906789,
    "rmse": 0.9921128909906789
}
[08/28/2025 17:41:40 INFO]: Val stats: {
    "score": -0.705476773747612,
    "rmse": 0.705476773747612
}
[08/28/2025 17:41:40 INFO]: Test stats: {
    "score": -0.8889794311124032,
    "rmse": 0.8889794311124032
}
[08/28/2025 17:41:41 INFO]: Training accuracy: {
    "score": -1.0199825086221699,
    "rmse": 1.0199825086221699
}
[08/28/2025 17:41:41 INFO]: Val accuracy: {
    "score": -0.6539475320969751,
    "rmse": 0.6539475320969751
}
[08/28/2025 17:41:41 INFO]: Test accuracy: {
    "score": -0.8777476746473687,
    "rmse": 0.8777476746473687
}
[08/28/2025 17:41:41 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_159",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8777476746473687,
        "rmse": 0.8777476746473687
    },
    "train_stats": {
        "score": -1.0199825086221699,
        "rmse": 1.0199825086221699
    },
    "val_stats": {
        "score": -0.6539475320969751,
        "rmse": 0.6539475320969751
    }
}
[08/28/2025 17:41:41 INFO]: Procewss finished for trial blotchy-Amado_trial_159
[08/28/2025 17:41:41 INFO]: 
_________________________________________________

[08/28/2025 17:41:41 INFO]: train_net_for_optune.py main() running.
[08/28/2025 17:41:41 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 16
  d_ffn_factor: 1.9383463257193732
  attention_dropout: 0.4347091979970425
  ffn_dropout: 0.4347091979970425
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.2308072651357302e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_175

[08/28/2025 17:41:41 INFO]: This ft_transformer has 1.667 million parameters.
[08/28/2025 17:41:41 INFO]: Training will start at epoch 0.
[08/28/2025 17:41:41 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 17:41:55 INFO]: Training loss at epoch 4: 1.026145100593567
[08/28/2025 17:41:56 INFO]: Training loss at epoch 0: 1.4711175560951233
[08/28/2025 17:42:06 INFO]: Training loss at epoch 52: 1.022757649421692
[08/28/2025 17:42:13 INFO]: Running Final Evaluation...
[08/28/2025 17:42:20 INFO]: Running Final Evaluation...
[08/28/2025 17:42:21 INFO]: Training loss at epoch 40: 1.0057679414749146
[08/28/2025 17:42:28 INFO]: Training loss at epoch 54: 0.970949798822403
[08/28/2025 17:42:34 INFO]: New best epoch, val score: -0.6600676004353306
[08/28/2025 17:42:34 INFO]: Saving model to: blotchy-Amado_trial_171/model_best.pth
[08/28/2025 17:42:35 INFO]: Training accuracy: {
    "score": -1.00216118604931,
    "rmse": 1.00216118604931
}
[08/28/2025 17:42:35 INFO]: Val accuracy: {
    "score": -0.6742307978780904,
    "rmse": 0.6742307978780904
}
[08/28/2025 17:42:35 INFO]: Test accuracy: {
    "score": -0.8699497113598791,
    "rmse": 0.8699497113598791
}
[08/28/2025 17:42:36 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_168",
    "best_epoch": 21,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8699497113598791,
        "rmse": 0.8699497113598791
    },
    "train_stats": {
        "score": -1.00216118604931,
        "rmse": 1.00216118604931
    },
    "val_stats": {
        "score": -0.6742307978780904,
        "rmse": 0.6742307978780904
    }
}
[08/28/2025 17:42:36 INFO]: Procewss finished for trial blotchy-Amado_trial_168
[08/28/2025 17:42:36 INFO]: 
_________________________________________________

[08/28/2025 17:42:36 INFO]: train_net_for_optune.py main() running.
[08/28/2025 17:42:36 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 16
  d_ffn_factor: 2.159504777318885
  attention_dropout: 0.402715599315092
  ffn_dropout: 0.402715599315092
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.2639281967134796e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_176

[08/28/2025 17:42:36 INFO]: This ft_transformer has 1.775 million parameters.
[08/28/2025 17:42:36 INFO]: Training will start at epoch 0.
[08/28/2025 17:42:36 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 17:42:36 INFO]: Training loss at epoch 50: 0.977914959192276
[08/28/2025 17:42:37 INFO]: New best epoch, val score: -0.6906032780761514
[08/28/2025 17:42:37 INFO]: Saving model to: blotchy-Amado_trial_173/model_best.pth
[08/28/2025 17:42:53 INFO]: Training loss at epoch 40: 0.9562985301017761
[08/28/2025 17:42:55 INFO]: Training loss at epoch 0: 0.9423121213912964
[08/28/2025 17:43:08 INFO]: Training loss at epoch 16: 1.0185438990592957
[08/28/2025 17:43:08 INFO]: Training loss at epoch 31: 0.97965869307518
[08/28/2025 17:43:13 INFO]: Training loss at epoch 33: 1.2981122434139252
[08/28/2025 17:43:22 INFO]: New best epoch, val score: -0.6750101424152394
[08/28/2025 17:43:22 INFO]: Saving model to: blotchy-Amado_trial_170/model_best.pth
[08/28/2025 17:43:29 INFO]: Training loss at epoch 51: 1.0670220851898193
[08/28/2025 17:43:35 INFO]: New best epoch, val score: -0.6959991482160895
[08/28/2025 17:43:35 INFO]: Saving model to: blotchy-Amado_trial_174/model_best.pth
[08/28/2025 17:43:38 INFO]: Training loss at epoch 32: 1.396482229232788
[08/28/2025 17:43:57 INFO]: Training loss at epoch 55: 1.1030743718147278
[08/28/2025 17:44:06 INFO]: Training loss at epoch 3: 0.9995494484901428
[08/28/2025 17:44:25 INFO]: Training loss at epoch 52: 0.8831861913204193
[08/28/2025 17:44:31 INFO]: Running Final Evaluation...
[08/28/2025 17:44:37 INFO]: Training loss at epoch 0: 0.8531555831432343
[08/28/2025 17:44:43 INFO]: Training loss at epoch 32: 1.0994898676872253
[08/28/2025 17:44:52 INFO]: Training accuracy: {
    "score": -1.009187250030674,
    "rmse": 1.009187250030674
}
[08/28/2025 17:44:52 INFO]: Val accuracy: {
    "score": -0.6622135324907747,
    "rmse": 0.6622135324907747
}
[08/28/2025 17:44:52 INFO]: Test accuracy: {
    "score": -0.8765356651859316,
    "rmse": 0.8765356651859316
}
[08/28/2025 17:44:52 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_169",
    "best_epoch": 21,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8765356651859316,
        "rmse": 0.8765356651859316
    },
    "train_stats": {
        "score": -1.009187250030674,
        "rmse": 1.009187250030674
    },
    "val_stats": {
        "score": -0.6622135324907747,
        "rmse": 0.6622135324907747
    }
}
[08/28/2025 17:44:52 INFO]: Procewss finished for trial blotchy-Amado_trial_169
[08/28/2025 17:44:52 INFO]: 
_________________________________________________

[08/28/2025 17:44:52 INFO]: train_net_for_optune.py main() running.
[08/28/2025 17:44:52 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 16
  d_ffn_factor: 2.0635024998343727
  attention_dropout: 0.1044470271658928
  ffn_dropout: 0.1044470271658928
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.2373133920287607e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_177

[08/28/2025 17:44:52 INFO]: This ft_transformer has 1.728 million parameters.
[08/28/2025 17:44:52 INFO]: Training will start at epoch 0.
[08/28/2025 17:44:52 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 17:45:02 INFO]: New best epoch, val score: -0.6799557400283417
[08/28/2025 17:45:02 INFO]: Saving model to: blotchy-Amado_trial_175/model_best.pth
[08/28/2025 17:45:03 INFO]: Training loss at epoch 17: 1.147249549627304
[08/28/2025 17:45:07 INFO]: Training loss at epoch 57: 1.0004225373268127
[08/28/2025 17:45:17 INFO]: New best epoch, val score: -0.6742093396295961
[08/28/2025 17:45:17 INFO]: Saving model to: blotchy-Amado_trial_170/model_best.pth
[08/28/2025 17:45:20 INFO]: Training accuracy: {
    "score": -0.9876997844789969,
    "rmse": 0.9876997844789969
}
[08/28/2025 17:45:20 INFO]: Val accuracy: {
    "score": -0.6677012838609684,
    "rmse": 0.6677012838609684
}
[08/28/2025 17:45:20 INFO]: Test accuracy: {
    "score": -0.8837176465041665,
    "rmse": 0.8837176465041665
}
[08/28/2025 17:45:20 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_154",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8837176465041665,
        "rmse": 0.8837176465041665
    },
    "train_stats": {
        "score": -0.9876997844789969,
        "rmse": 0.9876997844789969
    },
    "val_stats": {
        "score": -0.6677012838609684,
        "rmse": 0.6677012838609684
    }
}
[08/28/2025 17:45:20 INFO]: Procewss finished for trial blotchy-Amado_trial_154
[08/28/2025 17:45:20 INFO]: 
_________________________________________________

[08/28/2025 17:45:20 INFO]: train_net_for_optune.py main() running.
[08/28/2025 17:45:20 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 16
  d_ffn_factor: 2.435609153002815
  attention_dropout: 0.4038736894913167
  ffn_dropout: 0.4038736894913167
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0006287026820588696
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_178

[08/28/2025 17:45:20 INFO]: This ft_transformer has 1.910 million parameters.
[08/28/2025 17:45:20 INFO]: Training will start at epoch 0.
[08/28/2025 17:45:20 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 17:45:23 INFO]: Training loss at epoch 56: 0.9874544739723206
[08/28/2025 17:45:36 INFO]: Training loss at epoch 0: 1.0895425081253052
[08/28/2025 17:45:38 INFO]: Training loss at epoch 54: 0.8664347529411316
[08/28/2025 17:46:02 INFO]: New best epoch, val score: -0.8921856536697256
[08/28/2025 17:46:02 INFO]: Saving model to: blotchy-Amado_trial_176/model_best.pth
[08/28/2025 17:46:13 INFO]: Training loss at epoch 34: 1.0937597751617432
[08/28/2025 17:46:16 INFO]: Training loss at epoch 33: 0.9948803782463074
[08/28/2025 17:46:43 INFO]: Training loss at epoch 33: 1.1187188029289246
[08/28/2025 17:46:48 INFO]: Training loss at epoch 57: 1.0247495770454407
[08/28/2025 17:46:55 INFO]: Training loss at epoch 18: 0.8997809588909149
[08/28/2025 17:46:56 INFO]: Training loss at epoch 57: 1.108373910188675
[08/28/2025 17:47:04 INFO]: Training loss at epoch 53: 0.8870697617530823
[08/28/2025 17:47:37 INFO]: Training loss at epoch 5: 1.0285135805606842
[08/28/2025 17:47:46 INFO]: Training loss at epoch 0: 1.0746965408325195
[08/28/2025 17:47:49 INFO]: Training loss at epoch 1: 1.0918474197387695
[08/28/2025 17:47:51 INFO]: Training loss at epoch 34: 1.3195935785770416
[08/28/2025 17:47:55 INFO]: Training loss at epoch 1: 0.875392496585846
[08/28/2025 17:48:02 INFO]: Running Final Evaluation...
[08/28/2025 17:48:11 INFO]: New best epoch, val score: -0.6820317262876866
[08/28/2025 17:48:11 INFO]: Saving model to: blotchy-Amado_trial_177/model_best.pth
[08/28/2025 17:48:16 INFO]: Training loss at epoch 58: 1.0278805494308472
[08/28/2025 17:48:20 INFO]: Training loss at epoch 0: 0.9210545122623444
[08/28/2025 17:48:38 INFO]: Training accuracy: {
    "score": -1.00565167648267,
    "rmse": 1.00565167648267
}
[08/28/2025 17:48:38 INFO]: Val accuracy: {
    "score": -0.7089046676487167,
    "rmse": 0.7089046676487167
}
[08/28/2025 17:48:38 INFO]: Test accuracy: {
    "score": -0.8936154963401529,
    "rmse": 0.8936154963401529
}
[08/28/2025 17:48:38 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_167",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8936154963401529,
        "rmse": 0.8936154963401529
    },
    "train_stats": {
        "score": -1.00565167648267,
        "rmse": 1.00565167648267
    },
    "val_stats": {
        "score": -0.7089046676487167,
        "rmse": 0.7089046676487167
    }
}
[08/28/2025 17:48:38 INFO]: Procewss finished for trial blotchy-Amado_trial_167
[08/28/2025 17:48:38 INFO]: 
_________________________________________________

[08/28/2025 17:48:38 INFO]: train_net_for_optune.py main() running.
[08/28/2025 17:48:38 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 16
  d_ffn_factor: 2.0478135268473965
  attention_dropout: 0.10249120794000935
  ffn_dropout: 0.10249120794000935
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.619081019246731e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_179

[08/28/2025 17:48:39 INFO]: This ft_transformer has 1.383 million parameters.
[08/28/2025 17:48:39 INFO]: Training will start at epoch 0.
[08/28/2025 17:48:39 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 17:48:40 INFO]: Training loss at epoch 1: 1.2187591195106506
[08/28/2025 17:48:46 INFO]: New best epoch, val score: -0.6752567932441498
[08/28/2025 17:48:46 INFO]: Saving model to: blotchy-Amado_trial_178/model_best.pth
[08/28/2025 17:48:51 INFO]: Training loss at epoch 19: 1.0994396805763245
[08/28/2025 17:49:00 INFO]: Training loss at epoch 1: 1.2582704424858093
[08/28/2025 17:49:13 INFO]: Training loss at epoch 35: 0.8728763461112976
[08/28/2025 17:49:14 INFO]: Training loss at epoch 41: 0.9918235540390015
[08/28/2025 17:49:27 INFO]: New best epoch, val score: -0.7719902993099022
[08/28/2025 17:49:27 INFO]: Saving model to: blotchy-Amado_trial_176/model_best.pth
[08/28/2025 17:49:30 INFO]: Training loss at epoch 4: 1.1909135580062866
[08/28/2025 17:49:34 INFO]: Training stats: {
    "score": -1.0109887256347914,
    "rmse": 1.0109887256347914
}
[08/28/2025 17:49:34 INFO]: Val stats: {
    "score": -0.6746467784378587,
    "rmse": 0.6746467784378587
}
[08/28/2025 17:49:34 INFO]: Test stats: {
    "score": -0.8660005852239301,
    "rmse": 0.8660005852239301
}
[08/28/2025 17:49:44 INFO]: Training loss at epoch 59: 1.100346028804779
[08/28/2025 17:49:49 INFO]: Training loss at epoch 34: 1.1218455135822296
[08/28/2025 17:50:07 INFO]: New best epoch, val score: -0.6636221596162832
[08/28/2025 17:50:07 INFO]: Saving model to: blotchy-Amado_trial_172/model_best.pth
[08/28/2025 17:50:08 INFO]: Training loss at epoch 41: 0.7598981857299805
[08/28/2025 17:50:14 INFO]: Training stats: {
    "score": -0.9997721169053476,
    "rmse": 0.9997721169053476
}
[08/28/2025 17:50:14 INFO]: Val stats: {
    "score": -0.6734439960236459,
    "rmse": 0.6734439960236459
}
[08/28/2025 17:50:14 INFO]: Test stats: {
    "score": -0.8635352046973214,
    "rmse": 0.8635352046973214
}
[08/28/2025 17:51:08 INFO]: Training loss at epoch 1: 0.9630439281463623
[08/28/2025 17:51:16 INFO]: Training loss at epoch 2: 0.8258565068244934
[08/28/2025 17:51:28 INFO]: Training loss at epoch 0: 1.163993000984192
[08/28/2025 17:51:31 INFO]: Training loss at epoch 20: 1.2858699560165405
[08/28/2025 17:51:44 INFO]: Training loss at epoch 60: 1.0666868090629578
[08/28/2025 17:51:51 INFO]: Training loss at epoch 1: 1.55724036693573
[08/28/2025 17:51:51 INFO]: New best epoch, val score: -0.8943114315295889
[08/28/2025 17:51:51 INFO]: Saving model to: blotchy-Amado_trial_179/model_best.pth
[08/28/2025 17:51:56 INFO]: Training loss at epoch 55: 1.0643368065357208
[08/28/2025 17:52:08 INFO]: Training loss at epoch 17: 1.004357635974884
[08/28/2025 17:52:17 INFO]: Training loss at epoch 58: 1.0389405488967896
[08/28/2025 17:52:19 INFO]: Training loss at epoch 36: 0.9993226826190948
[08/28/2025 17:52:28 INFO]: Training loss at epoch 2: 1.2102007865905762
[08/28/2025 17:52:42 INFO]: Running Final Evaluation...
[08/28/2025 17:52:53 INFO]: New best epoch, val score: -0.6893234300203301
[08/28/2025 17:52:53 INFO]: Saving model to: blotchy-Amado_trial_176/model_best.pth
[08/28/2025 17:52:57 INFO]: Training loss at epoch 35: 1.0090296864509583
[08/28/2025 17:53:12 INFO]: Training loss at epoch 61: 1.2999703884124756
[08/28/2025 17:53:25 INFO]: Training loss at epoch 6: 1.239621341228485
[08/28/2025 17:53:27 INFO]: Training loss at epoch 21: 0.9680583477020264
[08/28/2025 17:53:50 INFO]: Training loss at epoch 2: 1.4362767934799194
[08/28/2025 17:54:03 INFO]: Training loss at epoch 54: 0.69794961810112
[08/28/2025 17:54:11 INFO]: Training loss at epoch 58: 0.9423298835754395
[08/28/2025 17:54:32 INFO]: New best epoch, val score: -0.6771520776100012
[08/28/2025 17:54:32 INFO]: Saving model to: blotchy-Amado_trial_173/model_best.pth
[08/28/2025 17:54:33 INFO]: Training loss at epoch 2: 0.9582603871822357
[08/28/2025 17:54:36 INFO]: Training loss at epoch 2: 1.2993008494377136
[08/28/2025 17:54:38 INFO]: Training loss at epoch 3: 0.8934142291545868
[08/28/2025 17:54:38 INFO]: Training loss at epoch 1: 1.2608586549758911
[08/28/2025 17:54:41 INFO]: Training loss at epoch 62: 1.0239540338516235
[08/28/2025 17:54:51 INFO]: Running Final Evaluation...
[08/28/2025 17:55:01 INFO]: Training loss at epoch 5: 1.0689305067062378
[08/28/2025 17:55:03 INFO]: New best epoch, val score: -0.7250334761965075
[08/28/2025 17:55:03 INFO]: Saving model to: blotchy-Amado_trial_179/model_best.pth
[08/28/2025 17:55:09 INFO]: Training accuracy: {
    "score": -0.9736159063525545,
    "rmse": 0.9736159063525545
}
[08/28/2025 17:55:09 INFO]: Val accuracy: {
    "score": -0.6465913990722236,
    "rmse": 0.6465913990722236
}
[08/28/2025 17:55:09 INFO]: Test accuracy: {
    "score": -0.8776525964952337,
    "rmse": 0.8776525964952337
}
[08/28/2025 17:55:09 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_151",
    "best_epoch": 24,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8776525964952337,
        "rmse": 0.8776525964952337
    },
    "train_stats": {
        "score": -0.9736159063525545,
        "rmse": 0.9736159063525545
    },
    "val_stats": {
        "score": -0.6465913990722236,
        "rmse": 0.6465913990722236
    }
}
[08/28/2025 17:55:09 INFO]: Procewss finished for trial blotchy-Amado_trial_151
[08/28/2025 17:55:09 INFO]: 
_________________________________________________

[08/28/2025 17:55:09 INFO]: train_net_for_optune.py main() running.
[08/28/2025 17:55:09 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 16
  d_ffn_factor: 2.1034996835316258
  attention_dropout: 0.4026073917579112
  ffn_dropout: 0.4026073917579112
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream4_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream4_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.246818366844902e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: blotchy-Amado_trial_180

[08/28/2025 17:55:09 INFO]: This ft_transformer has 1.401 million parameters.
[08/28/2025 17:55:09 INFO]: Training will start at epoch 0.
[08/28/2025 17:55:09 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 17:55:14 INFO]: New best epoch, val score: -0.6939535629483472
[08/28/2025 17:55:14 INFO]: Saving model to: blotchy-Amado_trial_174/model_best.pth
[08/28/2025 17:55:21 INFO]: Training loss at epoch 37: 0.8578447997570038
[08/28/2025 17:55:22 INFO]: Training loss at epoch 2: 1.0081579387187958
[08/28/2025 17:55:22 INFO]: Training loss at epoch 22: 1.0001070201396942
[08/28/2025 17:55:28 INFO]: Training accuracy: {
    "score": -1.0089991233367808,
    "rmse": 1.0089991233367808
}
[08/28/2025 17:55:28 INFO]: Val accuracy: {
    "score": -0.6677985513758767,
    "rmse": 0.6677985513758767
}
[08/28/2025 17:55:28 INFO]: Test accuracy: {
    "score": -0.8634149259798757,
    "rmse": 0.8634149259798757
}
[08/28/2025 17:55:29 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_165",
    "best_epoch": 31,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8634149259798757,
        "rmse": 0.8634149259798757
    },
    "train_stats": {
        "score": -1.0089991233367808,
        "rmse": 1.0089991233367808
    },
    "val_stats": {
        "score": -0.6677985513758767,
        "rmse": 0.6677985513758767
    }
}
[08/28/2025 17:55:29 INFO]: Procewss finished for trial blotchy-Amado_trial_165
[08/28/2025 17:55:43 INFO]: Training loss at epoch 42: 0.9196944832801819
[08/28/2025 17:55:50 INFO]: Training loss at epoch 3: 0.973778486251831
[08/28/2025 17:56:03 INFO]: Training loss at epoch 36: 1.0220230221748352
[08/28/2025 17:56:15 INFO]: New best epoch, val score: -0.683950996329659
[08/28/2025 17:56:15 INFO]: Saving model to: blotchy-Amado_trial_176/model_best.pth
[08/28/2025 17:57:23 INFO]: Training loss at epoch 23: 1.0191137194633484
[08/28/2025 17:57:55 INFO]: Training loss at epoch 4: 0.9537771642208099
[08/28/2025 17:57:58 INFO]: Training loss at epoch 2: 1.063495934009552
[08/28/2025 17:58:00 INFO]: Training loss at epoch 42: 0.8130719363689423
[08/28/2025 17:58:07 INFO]: Training loss at epoch 3: 0.9845996201038361
[08/28/2025 17:58:09 INFO]: Training loss at epoch 0: 0.9698654115200043
[08/28/2025 17:58:22 INFO]: New best epoch, val score: -0.6656991001696854
[08/28/2025 17:58:22 INFO]: Saving model to: blotchy-Amado_trial_179/model_best.pth
[08/28/2025 17:58:30 INFO]: Training loss at epoch 38: 1.0709537863731384
[08/28/2025 17:58:33 INFO]: New best epoch, val score: -0.7009084071337365
[08/28/2025 17:58:33 INFO]: Saving model to: blotchy-Amado_trial_180/model_best.pth
[08/28/2025 17:58:46 INFO]: Training loss at epoch 3: 1.3888724446296692
[08/28/2025 17:59:03 INFO]: Training loss at epoch 7: 1.0004683136940002
[08/28/2025 17:59:06 INFO]: Training loss at epoch 37: 1.1391613185405731
[08/28/2025 17:59:11 INFO]: Training loss at epoch 4: 1.0171793699264526
[08/28/2025 17:59:16 INFO]: Training loss at epoch 24: 1.1266662776470184
[08/28/2025 17:59:20 INFO]: Training loss at epoch 59: 0.889552891254425
[08/28/2025 17:59:41 INFO]: Training loss at epoch 3: 1.1877995431423187
[08/28/2025 18:00:20 INFO]: Training loss at epoch 6: 0.9347789883613586
[08/28/2025 18:00:22 INFO]: Training loss at epoch 3: 0.9594858288764954
[08/28/2025 18:00:57 INFO]: Training loss at epoch 55: 1.004597544670105
[08/28/2025 18:01:10 INFO]: Training loss at epoch 5: 1.0006491839885712
[08/28/2025 18:01:11 INFO]: Training loss at epoch 59: 0.8705442547798157
[08/28/2025 18:01:12 INFO]: Training loss at epoch 3: 1.0209719240665436
[08/28/2025 18:01:16 INFO]: Training loss at epoch 25: 1.0919624865055084
[08/28/2025 18:01:25 INFO]: Training loss at epoch 1: 0.9627380967140198
[08/28/2025 18:01:31 INFO]: Training loss at epoch 4: 0.8040045499801636
[08/28/2025 18:01:35 INFO]: Training loss at epoch 39: 0.8802834451198578
[08/28/2025 18:01:41 INFO]: Training stats: {
    "score": -0.9738378196916256,
    "rmse": 0.9738378196916256
}
[08/28/2025 18:01:41 INFO]: Val stats: {
    "score": -0.7246184946249224,
    "rmse": 0.7246184946249224
}
[08/28/2025 18:01:41 INFO]: Test stats: {
    "score": -0.9594523633722234,
    "rmse": 0.9594523633722234
}
[08/28/2025 18:01:49 INFO]: New best epoch, val score: -0.6727256350742713
[08/28/2025 18:01:49 INFO]: Saving model to: blotchy-Amado_trial_180/model_best.pth
[08/28/2025 18:02:00 INFO]: Training loss at epoch 43: 0.968009352684021
[08/28/2025 18:02:08 INFO]: Training loss at epoch 38: 0.8548029959201813
[08/28/2025 18:02:08 INFO]: Training loss at epoch 4: 1.214530885219574
[08/28/2025 18:02:29 INFO]: Training loss at epoch 5: 1.0400501489639282
[08/28/2025 18:02:34 INFO]: New best epoch, val score: -0.6648593962509899
[08/28/2025 18:02:34 INFO]: Saving model to: blotchy-Amado_trial_178/model_best.pth
[08/28/2025 18:02:40 INFO]: Training stats: {
    "score": -0.9978440057525146,
    "rmse": 0.9978440057525146
}
[08/28/2025 18:02:40 INFO]: Val stats: {
    "score": -0.6767391436451917,
    "rmse": 0.6767391436451917
}
[08/28/2025 18:02:40 INFO]: Test stats: {
    "score": -0.8740580307598272,
    "rmse": 0.8740580307598272
}
[08/28/2025 18:03:09 INFO]: Training loss at epoch 26: 0.9670964777469635
[08/28/2025 18:03:31 INFO]: Training stats: {
    "score": -0.9413646792125236,
    "rmse": 0.9413646792125236
}
[08/28/2025 18:03:31 INFO]: Val stats: {
    "score": -0.674585996929018,
    "rmse": 0.674585996929018
}
[08/28/2025 18:03:31 INFO]: Test stats: {
    "score": -0.8954627762078071,
    "rmse": 0.8954627762078071
}
[08/28/2025 18:04:20 INFO]: New best epoch, val score: -0.674585996929018
[08/28/2025 18:04:20 INFO]: Saving model to: blotchy-Amado_trial_145/model_best.pth
[08/28/2025 18:04:22 INFO]: Training loss at epoch 4: 1.0320415496826172
[08/28/2025 18:04:24 INFO]: Training loss at epoch 6: 1.0791766047477722
[08/28/2025 18:04:38 INFO]: Training loss at epoch 2: 0.934533417224884
[08/28/2025 18:04:38 INFO]: Training loss at epoch 8: 1.3052178621292114
[08/28/2025 18:04:52 INFO]: Training loss at epoch 5: 0.9991191923618317
[08/28/2025 18:05:02 INFO]: New best epoch, val score: -0.6538633886635677
[08/28/2025 18:05:02 INFO]: Saving model to: blotchy-Amado_trial_180/model_best.pth
[08/28/2025 18:05:05 INFO]: Training loss at epoch 27: 0.917854517698288
[08/28/2025 18:05:10 INFO]: Training loss at epoch 39: 1.3232132196426392
[08/28/2025 18:05:28 INFO]: Training loss at epoch 4: 1.286086082458496
[08/28/2025 18:05:31 INFO]: Training loss at epoch 5: 1.0669831037521362
[08/28/2025 18:05:38 INFO]: Training loss at epoch 7: 1.0187125205993652
[08/28/2025 18:05:41 INFO]: Training loss at epoch 43: 0.7515028119087219
[08/28/2025 18:05:41 INFO]: Training loss at epoch 40: 1.0728738903999329
[08/28/2025 18:05:47 INFO]: Training loss at epoch 6: 0.8928424417972565
[08/28/2025 18:05:48 INFO]: Training loss at epoch 18: 0.8217467963695526
[08/28/2025 18:06:03 INFO]: Training loss at epoch 4: 1.1114997863769531
[08/28/2025 18:06:09 INFO]: New best epoch, val score: -0.674104276947742
[08/28/2025 18:06:09 INFO]: Saving model to: blotchy-Amado_trial_173/model_best.pth
[08/28/2025 18:06:15 INFO]: New best epoch, val score: -0.6633441957730927
[08/28/2025 18:06:15 INFO]: Saving model to: blotchy-Amado_trial_172/model_best.pth
[08/28/2025 18:06:17 INFO]: Training stats: {
    "score": -1.0046784883062299,
    "rmse": 1.0046784883062299
}
[08/28/2025 18:06:17 INFO]: Val stats: {
    "score": -0.6634907770922429,
    "rmse": 0.6634907770922429
}
[08/28/2025 18:06:17 INFO]: Test stats: {
    "score": -0.8632275682368362,
    "rmse": 0.8632275682368362
}
[08/28/2025 18:06:59 INFO]: Training loss at epoch 28: 1.0285180807113647
[08/28/2025 18:07:30 INFO]: Training loss at epoch 5: 1.1841915845870972
[08/28/2025 18:07:38 INFO]: Training loss at epoch 7: 0.9744657874107361
[08/28/2025 18:07:45 INFO]: Training loss at epoch 56: 0.7957155108451843
[08/28/2025 18:07:48 INFO]: Training loss at epoch 3: 1.2032445669174194
[08/28/2025 18:08:10 INFO]: Training loss at epoch 6: 1.001018911600113
[08/28/2025 18:08:12 INFO]: New best epoch, val score: -0.6534481276921902
[08/28/2025 18:08:12 INFO]: Saving model to: blotchy-Amado_trial_180/model_best.pth
[08/28/2025 18:08:14 INFO]: Training loss at epoch 44: 0.9443057179450989
[08/28/2025 18:08:38 INFO]: Training loss at epoch 60: 0.8728853762149811
[08/28/2025 18:08:40 INFO]: Training loss at epoch 41: 0.9355989098548889
[08/28/2025 18:08:52 INFO]: Training loss at epoch 29: 0.8995223641395569
[08/28/2025 18:08:53 INFO]: Training loss at epoch 6: 1.0333142280578613
[08/28/2025 18:09:04 INFO]: Training loss at epoch 7: 1.0091822147369385
[08/28/2025 18:09:18 INFO]: Training loss at epoch 40: 0.977703332901001
[08/28/2025 18:09:19 INFO]: New best epoch, val score: -0.6647799091448398
[08/28/2025 18:09:19 INFO]: Saving model to: blotchy-Amado_trial_178/model_best.pth
[08/28/2025 18:09:27 INFO]: Running Final Evaluation...
[08/28/2025 18:09:32 INFO]: Training stats: {
    "score": -0.9914624031940142,
    "rmse": 0.9914624031940142
}
[08/28/2025 18:09:32 INFO]: Val stats: {
    "score": -0.7006747734581681,
    "rmse": 0.7006747734581681
}
[08/28/2025 18:09:32 INFO]: Test stats: {
    "score": -0.870748452363159,
    "rmse": 0.870748452363159
}
[08/28/2025 18:10:11 INFO]: Training loss at epoch 9: 0.9029061794281006
[08/28/2025 18:10:29 INFO]: Training loss at epoch 60: 1.0430147051811218
[08/28/2025 18:10:37 INFO]: Training loss at epoch 6: 1.2948122024536133
[08/28/2025 18:10:51 INFO]: Training loss at epoch 8: 0.8574117422103882
[08/28/2025 18:10:56 INFO]: Training loss at epoch 8: 1.1170284748077393
[08/28/2025 18:10:57 INFO]: Training loss at epoch 4: 1.0503004789352417
[08/28/2025 18:11:15 INFO]: Training loss at epoch 5: 1.0223410427570343
[08/28/2025 18:11:25 INFO]: Training loss at epoch 30: 0.8732607364654541
[08/28/2025 18:11:26 INFO]: Training loss at epoch 7: 1.0673134326934814
[08/28/2025 18:11:33 INFO]: New best epoch, val score: -0.662465275833522
[08/28/2025 18:11:33 INFO]: Saving model to: blotchy-Amado_trial_172/model_best.pth
[08/28/2025 18:11:38 INFO]: Training loss at epoch 42: 0.7907819151878357
[08/28/2025 18:11:42 INFO]: Training loss at epoch 5: 0.8548100590705872
[08/28/2025 18:12:02 INFO]: Training stats: {
    "score": -0.9840044267457096,
    "rmse": 0.9840044267457096
}
[08/28/2025 18:12:02 INFO]: Val stats: {
    "score": -0.6676426130849583,
    "rmse": 0.6676426130849583
}
[08/28/2025 18:12:02 INFO]: Test stats: {
    "score": -0.8844160869675014,
    "rmse": 0.8844160869675014
}
[08/28/2025 18:12:10 INFO]: Training accuracy: {
    "score": -0.9829637674249899,
    "rmse": 0.9829637674249899
}
[08/28/2025 18:12:10 INFO]: Val accuracy: {
    "score": -0.6641726133940709,
    "rmse": 0.6641726133940709
}
[08/28/2025 18:12:10 INFO]: Test accuracy: {
    "score": -0.8897777731748757,
    "rmse": 0.8897777731748757
}
[08/28/2025 18:12:10 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_146",
    "best_epoch": 29,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8897777731748757,
        "rmse": 0.8897777731748757
    },
    "train_stats": {
        "score": -0.9829637674249899,
        "rmse": 0.9829637674249899
    },
    "val_stats": {
        "score": -0.6641726133940709,
        "rmse": 0.6641726133940709
    }
}
[08/28/2025 18:12:10 INFO]: Procewss finished for trial blotchy-Amado_trial_146
[08/28/2025 18:12:15 INFO]: Training loss at epoch 7: 1.036067008972168
[08/28/2025 18:12:18 INFO]: Training loss at epoch 41: 1.0968655943870544
[08/28/2025 18:12:21 INFO]: Training loss at epoch 8: 1.0930109024047852
[08/28/2025 18:12:40 INFO]: New best epoch, val score: -0.6643306422807835
[08/28/2025 18:12:40 INFO]: Saving model to: blotchy-Amado_trial_178/model_best.pth
[08/28/2025 18:12:42 INFO]: Running Final Evaluation...
[08/28/2025 18:12:46 INFO]: New best epoch, val score: -0.6831179004032979
[08/28/2025 18:12:46 INFO]: Saving model to: blotchy-Amado_trial_176/model_best.pth
[08/28/2025 18:13:18 INFO]: Training loss at epoch 44: 0.6829793155193329
[08/28/2025 18:13:18 INFO]: Training loss at epoch 31: 0.8992976248264313
[08/28/2025 18:13:44 INFO]: Training loss at epoch 7: 1.1350321173667908
[08/28/2025 18:13:56 INFO]: Training accuracy: {
    "score": -1.0229708754035782,
    "rmse": 1.0229708754035782
}
[08/28/2025 18:13:56 INFO]: Val accuracy: {
    "score": -0.6549997134571784,
    "rmse": 0.6549997134571784
}
[08/28/2025 18:13:56 INFO]: Test accuracy: {
    "score": -0.8659212389206068,
    "rmse": 0.8659212389206068
}
[08/28/2025 18:13:56 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_161",
    "best_epoch": 10,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8659212389206068,
        "rmse": 0.8659212389206068
    },
    "train_stats": {
        "score": -1.0229708754035782,
        "rmse": 1.0229708754035782
    },
    "val_stats": {
        "score": -0.6549997134571784,
        "rmse": 0.6549997134571784
    }
}
[08/28/2025 18:13:56 INFO]: Procewss finished for trial blotchy-Amado_trial_161
[08/28/2025 18:14:04 INFO]: Training loss at epoch 9: 1.177545815706253
[08/28/2025 18:14:06 INFO]: Training loss at epoch 5: 1.0267273485660553
[08/28/2025 18:14:28 INFO]: Training loss at epoch 45: 0.8317047655582428
[08/28/2025 18:14:31 INFO]: Training loss at epoch 57: 0.880054384469986
[08/28/2025 18:14:36 INFO]: Training loss at epoch 43: 0.9469257891178131
[08/28/2025 18:14:44 INFO]: Training loss at epoch 8: 0.9009429514408112
[08/28/2025 18:15:11 INFO]: Training loss at epoch 32: 0.9438196420669556
[08/28/2025 18:15:13 INFO]: Training stats: {
    "score": -1.0215054544808664,
    "rmse": 1.0215054544808664
}
[08/28/2025 18:15:13 INFO]: Val stats: {
    "score": -0.7334697210976324,
    "rmse": 0.7334697210976324
}
[08/28/2025 18:15:13 INFO]: Test stats: {
    "score": -0.9036366855929286,
    "rmse": 0.9036366855929286
}
[08/28/2025 18:15:19 INFO]: Running Final Evaluation...
[08/28/2025 18:15:34 INFO]: Training loss at epoch 8: 1.1627660989761353
[08/28/2025 18:15:36 INFO]: Training loss at epoch 9: 0.9955266118049622
[08/28/2025 18:15:59 INFO]: New best epoch, val score: -0.6637038740746776
[08/28/2025 18:15:59 INFO]: Saving model to: blotchy-Amado_trial_178/model_best.pth
[08/28/2025 18:16:12 INFO]: Training loss at epoch 9: 0.8873471021652222
[08/28/2025 18:16:46 INFO]: Training stats: {
    "score": -1.0242812622273791,
    "rmse": 1.0242812622273791
}
[08/28/2025 18:16:46 INFO]: Val stats: {
    "score": -0.6821207418832019,
    "rmse": 0.6821207418832019
}
[08/28/2025 18:16:46 INFO]: Test stats: {
    "score": -0.9033292576751056,
    "rmse": 0.9033292576751056
}
[08/28/2025 18:16:49 INFO]: Training loss at epoch 8: 1.1101245284080505
[08/28/2025 18:16:58 INFO]: Training loss at epoch 6: 0.9146554470062256
[08/28/2025 18:17:03 INFO]: Training loss at epoch 33: 0.9572146534919739
[08/28/2025 18:17:10 INFO]: New best epoch, val score: -0.6821207418832019
[08/28/2025 18:17:10 INFO]: Saving model to: blotchy-Amado_trial_176/model_best.pth
[08/28/2025 18:17:13 INFO]: Training loss at epoch 6: 0.9990775287151337
[08/28/2025 18:17:19 INFO]: Training loss at epoch 6: 0.871408224105835
[08/28/2025 18:17:24 INFO]: Training loss at epoch 61: 0.8564623594284058
[08/28/2025 18:17:32 INFO]: Training loss at epoch 10: 1.1491525173187256
[08/28/2025 18:17:33 INFO]: Training loss at epoch 44: 0.9348510503768921
[08/28/2025 18:17:57 INFO]: Training stats: {
    "score": -1.0093841357138624,
    "rmse": 1.0093841357138624
}
[08/28/2025 18:17:57 INFO]: Val stats: {
    "score": -0.6614274980422783,
    "rmse": 0.6614274980422783
}
[08/28/2025 18:17:57 INFO]: Test stats: {
    "score": -0.8799524082036212,
    "rmse": 0.8799524082036212
}
[08/28/2025 18:17:57 INFO]: Training accuracy: {
    "score": -0.9608273091536765,
    "rmse": 0.9608273091536765
}
[08/28/2025 18:17:57 INFO]: Val accuracy: {
    "score": -0.683299590745396,
    "rmse": 0.683299590745396
}
[08/28/2025 18:17:57 INFO]: Test accuracy: {
    "score": -0.8925152331146766,
    "rmse": 0.8925152331146766
}
[08/28/2025 18:17:57 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_150",
    "best_epoch": 26,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8925152331146766,
        "rmse": 0.8925152331146766
    },
    "train_stats": {
        "score": -0.9608273091536765,
        "rmse": 0.9608273091536765
    },
    "val_stats": {
        "score": -0.683299590745396,
        "rmse": 0.683299590745396
    }
}
[08/28/2025 18:17:57 INFO]: Procewss finished for trial blotchy-Amado_trial_150
[08/28/2025 18:17:58 INFO]: Training loss at epoch 9: 1.0342103838920593
[08/28/2025 18:18:23 INFO]: Training loss at epoch 10: 1.3526681065559387
[08/28/2025 18:18:33 INFO]: New best epoch, val score: -0.6614274980422783
[08/28/2025 18:18:33 INFO]: Saving model to: blotchy-Amado_trial_172/model_best.pth
[08/28/2025 18:18:52 INFO]: Training loss at epoch 9: 0.8508138060569763
[08/28/2025 18:18:54 INFO]: Training loss at epoch 34: 0.8948996365070343
[08/28/2025 18:19:08 INFO]: Training stats: {
    "score": -0.9945577693530193,
    "rmse": 0.9945577693530193
}
[08/28/2025 18:19:08 INFO]: Val stats: {
    "score": -0.6999105673331641,
    "rmse": 0.6999105673331641
}
[08/28/2025 18:19:08 INFO]: Test stats: {
    "score": -0.8656884536408368,
    "rmse": 0.8656884536408368
}
[08/28/2025 18:19:15 INFO]: Training loss at epoch 19: 0.8868605494499207
[08/28/2025 18:19:54 INFO]: Training loss at epoch 9: 0.9451557993888855
[08/28/2025 18:19:59 INFO]: Training loss at epoch 10: 1.22157883644104
[08/28/2025 18:20:03 INFO]: Training stats: {
    "score": -1.0204637284018037,
    "rmse": 1.0204637284018037
}
[08/28/2025 18:20:03 INFO]: Val stats: {
    "score": -0.6630397949255852,
    "rmse": 0.6630397949255852
}
[08/28/2025 18:20:03 INFO]: Test stats: {
    "score": -0.8783775527755483,
    "rmse": 0.8783775527755483
}
[08/28/2025 18:20:18 INFO]: Training loss at epoch 7: 1.1745152473449707
[08/28/2025 18:20:24 INFO]: New best epoch, val score: -0.6795136724494814
[08/28/2025 18:20:24 INFO]: Saving model to: blotchy-Amado_trial_176/model_best.pth
[08/28/2025 18:20:28 INFO]: Training loss at epoch 45: 0.9802400469779968
[08/28/2025 18:20:28 INFO]: New best epoch, val score: -0.6630397949255852
[08/28/2025 18:20:28 INFO]: Saving model to: blotchy-Amado_trial_178/model_best.pth
[08/28/2025 18:20:37 INFO]: Training loss at epoch 46: 0.7930512428283691
[08/28/2025 18:20:46 INFO]: Training loss at epoch 35: 0.9015942513942719
[08/28/2025 18:20:53 INFO]: Training loss at epoch 45: 0.7957142889499664
[08/28/2025 18:21:01 INFO]: Training stats: {
    "score": -1.048928116590337,
    "rmse": 1.048928116590337
}
[08/28/2025 18:21:01 INFO]: Val stats: {
    "score": -0.6783285915804061,
    "rmse": 0.6783285915804061
}
[08/28/2025 18:21:01 INFO]: Test stats: {
    "score": -0.8950453121540761,
    "rmse": 0.8950453121540761
}
[08/28/2025 18:21:32 INFO]: Training loss at epoch 11: 1.2155968248844147
[08/28/2025 18:22:21 INFO]: Training loss at epoch 10: 0.901617020368576
[08/28/2025 18:22:37 INFO]: Training loss at epoch 36: 0.9624475538730621
[08/28/2025 18:22:40 INFO]: Training loss at epoch 7: 1.2033353447914124
[08/28/2025 18:22:54 INFO]: Training loss at epoch 7: 1.2840967774391174
[08/28/2025 18:23:02 INFO]: Training loss at epoch 11: 0.8827643394470215
[08/28/2025 18:23:11 INFO]: Training loss at epoch 10: 0.9122958183288574
[08/28/2025 18:23:13 INFO]: Training loss at epoch 11: 1.0648791193962097
[08/28/2025 18:23:22 INFO]: Training loss at epoch 10: 0.9649429321289062
[08/28/2025 18:23:23 INFO]: Training loss at epoch 46: 0.8896453380584717
[08/28/2025 18:23:24 INFO]: Training loss at epoch 8: 1.1984341144561768
[08/28/2025 18:23:47 INFO]: Training stats: {
    "score": -0.940989948827427,
    "rmse": 0.940989948827427
}
[08/28/2025 18:23:47 INFO]: Val stats: {
    "score": -0.7033711298204859,
    "rmse": 0.7033711298204859
}
[08/28/2025 18:23:47 INFO]: Test stats: {
    "score": -0.8962035785386914,
    "rmse": 0.8962035785386914
}
[08/28/2025 18:24:05 INFO]: Training loss at epoch 10: 1.2977589964866638
[08/28/2025 18:24:17 INFO]: Training loss at epoch 62: 1.0367906987667084
[08/28/2025 18:24:28 INFO]: Training loss at epoch 37: 0.984189510345459
[08/28/2025 18:24:29 INFO]: New best epoch, val score: -0.6633321024300946
[08/28/2025 18:24:29 INFO]: Saving model to: blotchy-Amado_trial_179/model_best.pth
[08/28/2025 18:24:43 INFO]: Training loss at epoch 12: 1.1208029389381409
[08/28/2025 18:25:35 INFO]: Training loss at epoch 11: 0.9125223755836487
[08/28/2025 18:26:18 INFO]: Training loss at epoch 47: 0.8909404575824738
[08/28/2025 18:26:19 INFO]: Training loss at epoch 38: 0.8871736228466034
[08/28/2025 18:26:26 INFO]: Training loss at epoch 12: 0.9128258228302002
[08/28/2025 18:26:29 INFO]: Training loss at epoch 9: 1.1345455646514893
[08/28/2025 18:26:40 INFO]: Training loss at epoch 11: 0.9635122716426849
[08/28/2025 18:26:47 INFO]: Training loss at epoch 47: 0.7932699024677277
[08/28/2025 18:27:09 INFO]: Training loss at epoch 11: 0.8629544377326965
[08/28/2025 18:27:31 INFO]: Running Final Evaluation...
[08/28/2025 18:27:33 INFO]: New best epoch, val score: -0.6625898016167231
[08/28/2025 18:27:33 INFO]: Saving model to: blotchy-Amado_trial_179/model_best.pth
[08/28/2025 18:27:36 INFO]: Training stats: {
    "score": -1.0006549711656687,
    "rmse": 1.0006549711656687
}
[08/28/2025 18:27:36 INFO]: Val stats: {
    "score": -0.6613152869978096,
    "rmse": 0.6613152869978096
}
[08/28/2025 18:27:36 INFO]: Test stats: {
    "score": -0.8650407686120157,
    "rmse": 0.8650407686120157
}
[08/28/2025 18:27:52 INFO]: Training loss at epoch 13: 0.9929047524929047
[08/28/2025 18:28:10 INFO]: Training loss at epoch 39: 1.4813430309295654
[08/28/2025 18:28:20 INFO]: Training loss at epoch 8: 0.9425289928913116
[08/28/2025 18:28:24 INFO]: Training loss at epoch 11: 0.8877526521682739
[08/28/2025 18:28:28 INFO]: Training loss at epoch 46: 0.7168022394180298
[08/28/2025 18:28:29 INFO]: Training loss at epoch 8: 0.8536774516105652
[08/28/2025 18:28:33 INFO]: Training loss at epoch 12: 1.1746399104595184
[08/28/2025 18:28:48 INFO]: Training loss at epoch 12: 0.8915658593177795
[08/28/2025 18:28:50 INFO]: Training stats: {
    "score": -0.9911843582309356,
    "rmse": 0.9911843582309356
}
[08/28/2025 18:28:50 INFO]: Val stats: {
    "score": -0.7148210567188391,
    "rmse": 0.7148210567188391
}
[08/28/2025 18:28:50 INFO]: Test stats: {
    "score": -0.8784663517325261,
    "rmse": 0.8784663517325261
}
[08/28/2025 18:29:12 INFO]: Training loss at epoch 48: 0.9367541670799255
[08/28/2025 18:29:39 INFO]: Training loss at epoch 13: 1.130556583404541
[08/28/2025 18:29:58 INFO]: Training loss at epoch 12: 1.1548041701316833
[08/28/2025 18:30:12 INFO]: Training accuracy: {
    "score": -0.975258336565694,
    "rmse": 0.975258336565694
}
[08/28/2025 18:30:12 INFO]: Val accuracy: {
    "score": -0.6621103621703021,
    "rmse": 0.6621103621703021
}
[08/28/2025 18:30:12 INFO]: Test accuracy: {
    "score": -0.8819132456811795,
    "rmse": 0.8819132456811795
}
[08/28/2025 18:30:12 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_155",
    "best_epoch": 16,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8819132456811795,
        "rmse": 0.8819132456811795
    },
    "train_stats": {
        "score": -0.975258336565694,
        "rmse": 0.975258336565694
    },
    "val_stats": {
        "score": -0.6621103621703021,
        "rmse": 0.6621103621703021
    }
}
[08/28/2025 18:30:12 INFO]: Procewss finished for trial blotchy-Amado_trial_155
[08/28/2025 18:30:13 INFO]: Training loss at epoch 12: 1.3217605650424957
[08/28/2025 18:30:41 INFO]: Training loss at epoch 10: 0.9710609912872314
[08/28/2025 18:30:42 INFO]: Training loss at epoch 40: 0.8145079612731934
[08/28/2025 18:31:01 INFO]: Training loss at epoch 14: 1.0509306192398071
[08/28/2025 18:31:10 INFO]: Training loss at epoch 63: 0.7826801240444183
[08/28/2025 18:32:01 INFO]: Training loss at epoch 13: 0.9629134833812714
[08/28/2025 18:32:07 INFO]: Training loss at epoch 49: 1.012245774269104
[08/28/2025 18:32:32 INFO]: Training loss at epoch 41: 1.0360163748264313
[08/28/2025 18:32:52 INFO]: Training loss at epoch 14: 0.8674784898757935
[08/28/2025 18:33:11 INFO]: Training stats: {
    "score": -0.9997033412633989,
    "rmse": 0.9997033412633989
}
[08/28/2025 18:33:11 INFO]: Val stats: {
    "score": -0.6676325391331143,
    "rmse": 0.6676325391331143
}
[08/28/2025 18:33:11 INFO]: Test stats: {
    "score": -0.8705276568912382,
    "rmse": 0.8705276568912382
}
[08/28/2025 18:33:15 INFO]: Training loss at epoch 13: 1.0222451388835907
[08/28/2025 18:33:15 INFO]: Training loss at epoch 13: 1.0984878242015839
[08/28/2025 18:33:37 INFO]: Training loss at epoch 12: 1.2407113015651703
[08/28/2025 18:33:44 INFO]: Training loss at epoch 11: 0.9611503779888153
[08/28/2025 18:34:01 INFO]: Training loss at epoch 9: 1.0957586765289307
[08/28/2025 18:34:02 INFO]: Training loss at epoch 13: 1.1328431963920593
[08/28/2025 18:34:03 INFO]: Training loss at epoch 9: 1.2194116711616516
[08/28/2025 18:34:11 INFO]: Training loss at epoch 15: 0.8726325631141663
[08/28/2025 18:34:23 INFO]: Training loss at epoch 42: 1.0360313057899475
[08/28/2025 18:35:14 INFO]: Training loss at epoch 14: 1.4493200182914734
[08/28/2025 18:35:54 INFO]: Training stats: {
    "score": -0.9787910880444295,
    "rmse": 0.9787910880444295
}
[08/28/2025 18:35:54 INFO]: Val stats: {
    "score": -0.6894887007976835,
    "rmse": 0.6894887007976835
}
[08/28/2025 18:35:54 INFO]: Test stats: {
    "score": -0.8907528985737809,
    "rmse": 0.8907528985737809
}
[08/28/2025 18:35:55 INFO]: Training stats: {
    "score": -1.0667715572423717,
    "rmse": 1.0667715572423717
}
[08/28/2025 18:35:55 INFO]: Val stats: {
    "score": -0.7221720714622927,
    "rmse": 0.7221720714622927
}
[08/28/2025 18:35:55 INFO]: Test stats: {
    "score": -0.9402251720931649,
    "rmse": 0.9402251720931649
}
[08/28/2025 18:36:03 INFO]: Training loss at epoch 47: 1.0323522984981537
[08/28/2025 18:36:04 INFO]: Training loss at epoch 15: 1.1703904271125793
[08/28/2025 18:36:06 INFO]: Training loss at epoch 50: 1.1289777159690857
[08/28/2025 18:36:14 INFO]: Training loss at epoch 43: 0.7952414751052856
[08/28/2025 18:36:19 INFO]: Training loss at epoch 14: 0.8994935750961304
[08/28/2025 18:36:28 INFO]: Running Final Evaluation...
[08/28/2025 18:36:31 INFO]: Training loss at epoch 14: 1.069005012512207
[08/28/2025 18:36:49 INFO]: Training loss at epoch 12: 1.1184845566749573
[08/28/2025 18:37:06 INFO]: Training loss at epoch 20: 0.9379175901412964
[08/28/2025 18:37:20 INFO]: Training loss at epoch 16: 1.102523148059845
[08/28/2025 18:37:35 INFO]: Training accuracy: {
    "score": -1.011172868847526,
    "rmse": 1.011172868847526
}
[08/28/2025 18:37:35 INFO]: Val accuracy: {
    "score": -0.6612066012955956,
    "rmse": 0.6612066012955956
}
[08/28/2025 18:37:35 INFO]: Test accuracy: {
    "score": -0.872325445156864,
    "rmse": 0.872325445156864
}
[08/28/2025 18:37:36 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_160",
    "best_epoch": 19,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.872325445156864,
        "rmse": 0.872325445156864
    },
    "train_stats": {
        "score": -1.011172868847526,
        "rmse": 1.011172868847526
    },
    "val_stats": {
        "score": -0.6612066012955956,
        "rmse": 0.6612066012955956
    }
}
[08/28/2025 18:37:36 INFO]: Procewss finished for trial blotchy-Amado_trial_160
[08/28/2025 18:38:02 INFO]: Training loss at epoch 64: 0.7890530526638031
[08/28/2025 18:38:05 INFO]: Training loss at epoch 44: 1.0581316351890564
[08/28/2025 18:38:27 INFO]: Training loss at epoch 15: 1.0237711668014526
[08/28/2025 18:38:49 INFO]: Training loss at epoch 13: 1.365328848361969
[08/28/2025 18:39:16 INFO]: Training loss at epoch 16: 1.2199553847312927
[08/28/2025 18:39:21 INFO]: Training loss at epoch 15: 1.0241220593452454
[08/28/2025 18:39:30 INFO]: Training loss at epoch 14: 1.053863763809204
[08/28/2025 18:39:47 INFO]: Training loss at epoch 15: 1.0167025923728943
[08/28/2025 18:39:52 INFO]: Training loss at epoch 13: 1.066981017589569
[08/28/2025 18:39:55 INFO]: Training loss at epoch 45: 0.9752897024154663
[08/28/2025 18:40:28 INFO]: Training loss at epoch 17: 0.9545061588287354
[08/28/2025 18:41:27 INFO]: Training loss at epoch 10: 0.9831297993659973
[08/28/2025 18:41:33 INFO]: Training loss at epoch 10: 1.2522009313106537
[08/28/2025 18:41:39 INFO]: Training loss at epoch 16: 1.0942333340644836
[08/28/2025 18:41:45 INFO]: Training loss at epoch 46: 1.2563459873199463
[08/28/2025 18:42:06 INFO]: New best epoch, val score: -0.6909526697772197
[08/28/2025 18:42:06 INFO]: Saving model to: blotchy-Amado_trial_174/model_best.pth
[08/28/2025 18:42:26 INFO]: Training loss at epoch 16: 1.0031706392765045
[08/28/2025 18:42:28 INFO]: Training loss at epoch 17: 1.05270117521286
[08/28/2025 18:42:54 INFO]: Training loss at epoch 14: 0.9209122657775879
[08/28/2025 18:43:04 INFO]: Training loss at epoch 16: 1.0062001943588257
[08/28/2025 18:43:34 INFO]: Training loss at epoch 48: 0.7190796136856079
[08/28/2025 18:43:35 INFO]: Training loss at epoch 47: 0.944348156452179
[08/28/2025 18:43:37 INFO]: Training loss at epoch 18: 1.0590218305587769
[08/28/2025 18:44:01 INFO]: Training loss at epoch 14: 1.1747862696647644
[08/28/2025 18:44:50 INFO]: Training loss at epoch 17: 0.7524316161870956
[08/28/2025 18:44:53 INFO]: Training loss at epoch 65: 0.9613404870033264
[08/28/2025 18:45:01 INFO]: Training loss at epoch 15: 1.0226031839847565
[08/28/2025 18:45:25 INFO]: Training loss at epoch 48: 0.9102046191692352
[08/28/2025 18:45:28 INFO]: Training loss at epoch 17: 0.9511369466781616
[08/28/2025 18:45:39 INFO]: Running Final Evaluation...
[08/28/2025 18:45:40 INFO]: Training loss at epoch 18: 1.1566646099090576
[08/28/2025 18:45:57 INFO]: Training loss at epoch 15: 1.170847237110138
[08/28/2025 18:46:20 INFO]: Training loss at epoch 17: 1.0303568243980408
[08/28/2025 18:46:24 INFO]: Training accuracy: {
    "score": -1.0075548808080774,
    "rmse": 1.0075548808080774
}
[08/28/2025 18:46:24 INFO]: Val accuracy: {
    "score": -0.6742093396295961,
    "rmse": 0.6742093396295961
}
[08/28/2025 18:46:24 INFO]: Test accuracy: {
    "score": -0.8637445975878443,
    "rmse": 0.8637445975878443
}
[08/28/2025 18:46:24 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_170",
    "best_epoch": 17,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8637445975878443,
        "rmse": 0.8637445975878443
    },
    "train_stats": {
        "score": -1.0075548808080774,
        "rmse": 1.0075548808080774
    },
    "val_stats": {
        "score": -0.6742093396295961,
        "rmse": 0.6742093396295961
    }
}
[08/28/2025 18:46:24 INFO]: Procewss finished for trial blotchy-Amado_trial_170
[08/28/2025 18:46:45 INFO]: Training loss at epoch 19: 0.8774226903915405
[08/28/2025 18:46:59 INFO]: Training loss at epoch 11: 0.7973544597625732
[08/28/2025 18:47:15 INFO]: Training loss at epoch 11: 0.8090439438819885
[08/28/2025 18:47:39 INFO]: New best epoch, val score: -0.6791943460675564
[08/28/2025 18:47:39 INFO]: Saving model to: blotchy-Amado_trial_174/model_best.pth
[08/28/2025 18:47:53 INFO]: Training stats: {
    "score": -1.018466812472715,
    "rmse": 1.018466812472715
}
[08/28/2025 18:47:53 INFO]: Val stats: {
    "score": -0.7328257665683724,
    "rmse": 0.7328257665683724
}
[08/28/2025 18:47:53 INFO]: Test stats: {
    "score": -0.9017659642660243,
    "rmse": 0.9017659642660243
}
[08/28/2025 18:48:01 INFO]: Training loss at epoch 18: 0.8693713843822479
[08/28/2025 18:48:31 INFO]: Training loss at epoch 18: 0.823129415512085
[08/28/2025 18:48:50 INFO]: Training loss at epoch 19: 0.991222470998764
[08/28/2025 18:48:58 INFO]: Training loss at epoch 16: 1.2559911012649536
[08/28/2025 18:49:10 INFO]: Training loss at epoch 15: 1.1493344902992249
[08/28/2025 18:49:34 INFO]: Training loss at epoch 18: 0.9637230634689331
[08/28/2025 18:49:58 INFO]: Training stats: {
    "score": -0.9982177972222891,
    "rmse": 0.9982177972222891
}
[08/28/2025 18:49:58 INFO]: Val stats: {
    "score": -0.7139435450611914,
    "rmse": 0.7139435450611914
}
[08/28/2025 18:49:58 INFO]: Test stats: {
    "score": -0.9023509118795258,
    "rmse": 0.9023509118795258
}
[08/28/2025 18:50:20 INFO]: Training loss at epoch 21: 0.924824446439743
[08/28/2025 18:50:24 INFO]: Training loss at epoch 16: 0.9564394354820251
[08/28/2025 18:50:58 INFO]: Training loss at epoch 20: 0.9346451461315155
[08/28/2025 18:51:01 INFO]: New best epoch, val score: -0.6535051931940553
[08/28/2025 18:51:01 INFO]: Saving model to: blotchy-Amado_trial_171/model_best.pth
[08/28/2025 18:51:02 INFO]: Training loss at epoch 49: 0.7862286567687988
[08/28/2025 18:51:11 INFO]: Training loss at epoch 19: 0.8039712905883789
[08/28/2025 18:51:31 INFO]: Training loss at epoch 19: 0.7768260687589645
[08/28/2025 18:51:39 INFO]: Training loss at epoch 66: 0.7993820905685425
[08/28/2025 18:51:55 INFO]: New best epoch, val score: -0.6679736175574991
[08/28/2025 18:51:55 INFO]: Saving model to: blotchy-Amado_trial_156/model_best.pth
[08/28/2025 18:51:59 INFO]: Training loss at epoch 17: 0.9823006987571716
[08/28/2025 18:52:19 INFO]: Training stats: {
    "score": -0.9942855315628099,
    "rmse": 0.9942855315628099
}
[08/28/2025 18:52:19 INFO]: Val stats: {
    "score": -0.7228242068973252,
    "rmse": 0.7228242068973252
}
[08/28/2025 18:52:19 INFO]: Test stats: {
    "score": -0.8808878981598791,
    "rmse": 0.8808878981598791
}
[08/28/2025 18:52:29 INFO]: Training loss at epoch 12: 1.262920320034027
[08/28/2025 18:52:36 INFO]: Training stats: {
    "score": -1.0044487058563731,
    "rmse": 1.0044487058563731
}
[08/28/2025 18:52:36 INFO]: Val stats: {
    "score": -0.7127520143999408,
    "rmse": 0.7127520143999408
}
[08/28/2025 18:52:36 INFO]: Test stats: {
    "score": -0.8906511868935404,
    "rmse": 0.8906511868935404
}
[08/28/2025 18:52:47 INFO]: Training loss at epoch 19: 0.9172834753990173
[08/28/2025 18:52:55 INFO]: Training loss at epoch 12: 0.8460105061531067
[08/28/2025 18:53:07 INFO]: Training loss at epoch 20: 1.0443728566169739
[08/28/2025 18:53:08 INFO]: New best epoch, val score: -0.6789986251084683
[08/28/2025 18:53:08 INFO]: Saving model to: blotchy-Amado_trial_174/model_best.pth
[08/28/2025 18:53:34 INFO]: Training stats: {
    "score": -0.8447135753120868,
    "rmse": 0.8447135753120868
}
[08/28/2025 18:53:34 INFO]: Val stats: {
    "score": -0.8357755067592578,
    "rmse": 0.8357755067592578
}
[08/28/2025 18:53:34 INFO]: Test stats: {
    "score": -1.05060246102415,
    "rmse": 1.05060246102415
}
[08/28/2025 18:53:57 INFO]: Training stats: {
    "score": -0.9971859913401384,
    "rmse": 0.9971859913401384
}
[08/28/2025 18:53:57 INFO]: Val stats: {
    "score": -0.6790446890272213,
    "rmse": 0.6790446890272213
}
[08/28/2025 18:53:57 INFO]: Test stats: {
    "score": -0.8756595028566451,
    "rmse": 0.8756595028566451
}
[08/28/2025 18:54:05 INFO]: Training loss at epoch 21: 1.0216754078865051
[08/28/2025 18:54:17 INFO]: Training loss at epoch 16: 1.1157272160053253
[08/28/2025 18:55:00 INFO]: Training loss at epoch 18: 0.9972813725471497
[08/28/2025 18:55:29 INFO]: Training loss at epoch 20: 1.0534272491931915
[08/28/2025 18:55:37 INFO]: Training loss at epoch 20: 1.0470029711723328
[08/28/2025 18:55:49 INFO]: Training loss at epoch 17: 0.899265855550766
[08/28/2025 18:56:17 INFO]: Training loss at epoch 21: 1.3469144403934479
[08/28/2025 18:57:10 INFO]: Training loss at epoch 20: 0.9853395223617554
[08/28/2025 18:57:11 INFO]: Training loss at epoch 22: 1.0129660665988922
[08/28/2025 18:58:00 INFO]: Training loss at epoch 13: 1.26250159740448
[08/28/2025 18:58:02 INFO]: Training loss at epoch 19: 1.2512012124061584
[08/28/2025 18:58:23 INFO]: Training loss at epoch 67: 0.9099277853965759
[08/28/2025 18:58:34 INFO]: Training loss at epoch 13: 1.1767113208770752
[08/28/2025 18:58:38 INFO]: Training loss at epoch 21: 0.9266629815101624
[08/28/2025 18:58:38 INFO]: New best epoch, val score: -0.6782367979912831
[08/28/2025 18:58:38 INFO]: Saving model to: blotchy-Amado_trial_174/model_best.pth
[08/28/2025 18:58:41 INFO]: Training loss at epoch 21: 1.0737852454185486
[08/28/2025 18:59:09 INFO]: Training stats: {
    "score": -1.0010128277372663,
    "rmse": 1.0010128277372663
}
[08/28/2025 18:59:09 INFO]: Val stats: {
    "score": -0.659120146148964,
    "rmse": 0.659120146148964
}
[08/28/2025 18:59:09 INFO]: Test stats: {
    "score": -0.8650829687039615,
    "rmse": 0.8650829687039615
}
[08/28/2025 18:59:23 INFO]: Training loss at epoch 17: 1.0415311455726624
[08/28/2025 18:59:26 INFO]: Training loss at epoch 22: 0.9085689187049866
[08/28/2025 19:00:16 INFO]: Training loss at epoch 23: 1.0188733637332916
[08/28/2025 19:00:23 INFO]: Training loss at epoch 21: 0.9229004979133606
[08/28/2025 19:01:02 INFO]: Training loss at epoch 50: 0.7078721821308136
[08/28/2025 19:01:10 INFO]: Training loss at epoch 18: 1.2982587814331055
[08/28/2025 19:01:39 INFO]: Training loss at epoch 22: 1.221659779548645
[08/28/2025 19:01:51 INFO]: Training loss at epoch 22: 0.9783888757228851
[08/28/2025 19:02:10 INFO]: Training loss at epoch 20: 0.8203921020030975
[08/28/2025 19:02:34 INFO]: Training loss at epoch 23: 1.0805130004882812
[08/28/2025 19:03:21 INFO]: Training loss at epoch 24: 1.1419203281402588
[08/28/2025 19:03:31 INFO]: Training loss at epoch 14: 1.1990347802639008
[08/28/2025 19:03:36 INFO]: Training loss at epoch 22: 1.0109083354473114
[08/28/2025 19:03:37 INFO]: Training loss at epoch 22: 1.073356717824936
[08/28/2025 19:04:16 INFO]: Training loss at epoch 14: 0.882197380065918
[08/28/2025 19:04:30 INFO]: Training loss at epoch 18: 0.9484144449234009
[08/28/2025 19:04:39 INFO]: Training loss at epoch 23: 1.0787818729877472
[08/28/2025 19:05:00 INFO]: Training loss at epoch 23: 0.9392017126083374
[08/28/2025 19:05:07 INFO]: Training loss at epoch 68: 0.9450218677520752
[08/28/2025 19:05:10 INFO]: Training loss at epoch 21: 1.3214555978775024
[08/28/2025 19:05:45 INFO]: Training loss at epoch 24: 1.0369099378585815
[08/28/2025 19:06:29 INFO]: Training loss at epoch 25: 0.9485170245170593
[08/28/2025 19:06:33 INFO]: Training loss at epoch 19: 1.0303301513195038
[08/28/2025 19:06:52 INFO]: Training loss at epoch 23: 0.9425580203533173
[08/28/2025 19:07:39 INFO]: Training loss at epoch 24: 1.1040423512458801
[08/28/2025 19:08:09 INFO]: Training loss at epoch 24: 0.8817947506904602
[08/28/2025 19:08:11 INFO]: Training loss at epoch 22: 1.0670091807842255
[08/28/2025 19:08:20 INFO]: Training stats: {
    "score": -0.9884909592971067,
    "rmse": 0.9884909592971067
}
[08/28/2025 19:08:20 INFO]: Val stats: {
    "score": -0.6507282639253817,
    "rmse": 0.6507282639253817
}
[08/28/2025 19:08:20 INFO]: Test stats: {
    "score": -0.8942988067591421,
    "rmse": 0.8942988067591421
}
[08/28/2025 19:08:27 INFO]: Training loss at epoch 51: 0.8219292759895325
[08/28/2025 19:08:53 INFO]: Training loss at epoch 25: 1.0277772545814514
[08/28/2025 19:08:57 INFO]: New best epoch, val score: -0.6507282639253817
[08/28/2025 19:08:57 INFO]: Saving model to: blotchy-Amado_trial_171/model_best.pth
[08/28/2025 19:08:59 INFO]: Training loss at epoch 15: 0.9484379887580872
[08/28/2025 19:09:21 INFO]: Running Final Evaluation...
[08/28/2025 19:09:35 INFO]: Training loss at epoch 26: 1.0542797446250916
[08/28/2025 19:09:37 INFO]: Training loss at epoch 19: 1.0433987975120544
[08/28/2025 19:09:59 INFO]: Training loss at epoch 15: 1.137020856142044
[08/28/2025 19:10:06 INFO]: Training loss at epoch 24: 0.9562033712863922
[08/28/2025 19:10:39 INFO]: Training loss at epoch 25: 1.0554304122924805
[08/28/2025 19:11:13 INFO]: Training loss at epoch 23: 0.8743842840194702
[08/28/2025 19:11:18 INFO]: Training stats: {
    "score": -0.9870296542027708,
    "rmse": 0.9870296542027708
}
[08/28/2025 19:11:18 INFO]: Val stats: {
    "score": -0.6639570606246085,
    "rmse": 0.6639570606246085
}
[08/28/2025 19:11:18 INFO]: Test stats: {
    "score": -0.8786256509340096,
    "rmse": 0.8786256509340096
}
[08/28/2025 19:11:20 INFO]: Training loss at epoch 25: 1.3098882734775543
[08/28/2025 19:11:51 INFO]: Training loss at epoch 69: 0.9656261205673218
[08/28/2025 19:12:03 INFO]: Training loss at epoch 26: 1.009010910987854
[08/28/2025 19:12:12 INFO]: Training accuracy: {
    "score": -0.9541127910683517,
    "rmse": 0.9541127910683517
}
[08/28/2025 19:12:12 INFO]: Val accuracy: {
    "score": -0.6741122865063495,
    "rmse": 0.6741122865063495
}
[08/28/2025 19:12:12 INFO]: Test accuracy: {
    "score": -0.903994445757725,
    "rmse": 0.903994445757725
}
[08/28/2025 19:12:12 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_153",
    "best_epoch": 20,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.903994445757725,
        "rmse": 0.903994445757725
    },
    "train_stats": {
        "score": -0.9541127910683517,
        "rmse": 0.9541127910683517
    },
    "val_stats": {
        "score": -0.6741122865063495,
        "rmse": 0.6741122865063495
    }
}
[08/28/2025 19:12:12 INFO]: Procewss finished for trial blotchy-Amado_trial_153
[08/28/2025 19:12:40 INFO]: Training loss at epoch 27: 0.926167219877243
[08/28/2025 19:13:19 INFO]: Training loss at epoch 25: 0.8767403662204742
[08/28/2025 19:13:39 INFO]: Training loss at epoch 26: 1.0380292534828186
[08/28/2025 19:13:42 INFO]: Training loss at epoch 20: 1.1170854568481445
[08/28/2025 19:14:07 INFO]: Training stats: {
    "score": -0.9193068054414993,
    "rmse": 0.9193068054414993
}
[08/28/2025 19:14:07 INFO]: Val stats: {
    "score": -0.7195459782511187,
    "rmse": 0.7195459782511187
}
[08/28/2025 19:14:07 INFO]: Test stats: {
    "score": -0.9268644006169744,
    "rmse": 0.9268644006169744
}
[08/28/2025 19:14:12 INFO]: Training loss at epoch 24: 0.8525879681110382
[08/28/2025 19:14:24 INFO]: Training loss at epoch 16: 0.9555397927761078
[08/28/2025 19:14:27 INFO]: Training loss at epoch 26: 0.9379346966743469
[08/28/2025 19:15:12 INFO]: Training loss at epoch 27: 1.0321000814437866
[08/28/2025 19:15:33 INFO]: Training loss at epoch 16: 1.0199854969978333
[08/28/2025 19:15:46 INFO]: Training loss at epoch 28: 1.1534098386764526
[08/28/2025 19:16:23 INFO]: Training loss at epoch 20: 0.8915573060512543
[08/28/2025 19:16:32 INFO]: Training loss at epoch 26: 1.0375453233718872
[08/28/2025 19:16:36 INFO]: Training loss at epoch 27: 1.04899001121521
[08/28/2025 19:16:36 INFO]: Training loss at epoch 23: 1.0992509126663208
[08/28/2025 19:17:09 INFO]: Training loss at epoch 25: 0.8570615649223328
[08/28/2025 19:17:33 INFO]: Training loss at epoch 27: 1.1966752707958221
[08/28/2025 19:18:21 INFO]: Training loss at epoch 28: 0.9216731488704681
[08/28/2025 19:18:51 INFO]: Training loss at epoch 29: 1.1034228801727295
[08/28/2025 19:19:03 INFO]: Training loss at epoch 21: 0.9256694614887238
[08/28/2025 19:19:32 INFO]: Training loss at epoch 28: 0.8120837509632111
[08/28/2025 19:19:45 INFO]: Training loss at epoch 27: 0.8661352097988129
[08/28/2025 19:19:45 INFO]: Training loss at epoch 17: 0.9819754958152771
[08/28/2025 19:19:58 INFO]: Training stats: {
    "score": -1.0105409241236654,
    "rmse": 1.0105409241236654
}
[08/28/2025 19:19:58 INFO]: Val stats: {
    "score": -0.7158663136181771,
    "rmse": 0.7158663136181771
}
[08/28/2025 19:19:58 INFO]: Test stats: {
    "score": -0.8918961904859922,
    "rmse": 0.8918961904859922
}
[08/28/2025 19:20:07 INFO]: Training loss at epoch 26: 0.9241003692150116
[08/28/2025 19:20:40 INFO]: Training loss at epoch 28: 0.9179943799972534
[08/28/2025 19:20:50 INFO]: Training loss at epoch 70: 1.1981450319290161
[08/28/2025 19:21:03 INFO]: Training loss at epoch 17: 1.051175057888031
[08/28/2025 19:21:28 INFO]: Training loss at epoch 21: 0.9549259543418884
[08/28/2025 19:21:30 INFO]: Training loss at epoch 29: 1.1637211441993713
[08/28/2025 19:22:03 INFO]: New best epoch, val score: -0.6609909430066229
[08/28/2025 19:22:03 INFO]: Saving model to: blotchy-Amado_trial_172/model_best.pth
[08/28/2025 19:22:30 INFO]: Training loss at epoch 29: 1.158406376838684
[08/28/2025 19:22:38 INFO]: Training stats: {
    "score": -0.9984889460619755,
    "rmse": 0.9984889460619755
}
[08/28/2025 19:22:38 INFO]: Val stats: {
    "score": -0.6797166881069244,
    "rmse": 0.6797166881069244
}
[08/28/2025 19:22:38 INFO]: Test stats: {
    "score": -0.8877061148532598,
    "rmse": 0.8877061148532598
}
[08/28/2025 19:22:58 INFO]: Training loss at epoch 28: 1.2507039308547974
[08/28/2025 19:23:03 INFO]: Training loss at epoch 30: 0.7865870296955109
[08/28/2025 19:23:05 INFO]: Training loss at epoch 27: 1.0500009953975677
[08/28/2025 19:23:35 INFO]: Training stats: {
    "score": -1.0017215406641586,
    "rmse": 1.0017215406641586
}
[08/28/2025 19:23:35 INFO]: Val stats: {
    "score": -0.6675867914737805,
    "rmse": 0.6675867914737805
}
[08/28/2025 19:23:35 INFO]: Test stats: {
    "score": -0.868104359061987,
    "rmse": 0.868104359061987
}
[08/28/2025 19:23:46 INFO]: Training loss at epoch 29: 0.8501806855201721
[08/28/2025 19:24:24 INFO]: Training loss at epoch 22: 0.9783365428447723
[08/28/2025 19:24:53 INFO]: Training stats: {
    "score": -0.9859824178242615,
    "rmse": 0.9859824178242615
}
[08/28/2025 19:24:53 INFO]: Val stats: {
    "score": -0.698246872551946,
    "rmse": 0.698246872551946
}
[08/28/2025 19:24:53 INFO]: Test stats: {
    "score": -0.8686150838092248,
    "rmse": 0.8686150838092248
}
[08/28/2025 19:25:06 INFO]: Training loss at epoch 18: 1.0045901536941528
[08/28/2025 19:25:46 INFO]: Training loss at epoch 30: 1.0935006737709045
[08/28/2025 19:26:02 INFO]: Training loss at epoch 28: 1.0056579113006592
[08/28/2025 19:26:08 INFO]: Training loss at epoch 31: 1.0309082865715027
[08/28/2025 19:26:10 INFO]: New best epoch, val score: -0.6792164502619579
[08/28/2025 19:26:10 INFO]: Saving model to: blotchy-Amado_trial_176/model_best.pth
[08/28/2025 19:26:12 INFO]: Training loss at epoch 29: 1.1027553081512451
[08/28/2025 19:26:31 INFO]: Training loss at epoch 30: 1.0407652258872986
[08/28/2025 19:26:32 INFO]: Training loss at epoch 18: 0.9719237685203552
[08/28/2025 19:26:32 INFO]: Running Final Evaluation...
[08/28/2025 19:26:34 INFO]: Training loss at epoch 22: 0.9106142520904541
[08/28/2025 19:27:20 INFO]: Training stats: {
    "score": -0.9956612624753935,
    "rmse": 0.9956612624753935
}
[08/28/2025 19:27:20 INFO]: Val stats: {
    "score": -0.6776793594683158,
    "rmse": 0.6776793594683158
}
[08/28/2025 19:27:20 INFO]: Test stats: {
    "score": -0.8748329460083185,
    "rmse": 0.8748329460083185
}
[08/28/2025 19:27:32 INFO]: Training loss at epoch 71: 0.8675172626972198
[08/28/2025 19:27:45 INFO]: Training accuracy: {
    "score": -1.0083861415370625,
    "rmse": 1.0083861415370625
}
[08/28/2025 19:27:45 INFO]: Val accuracy: {
    "score": -0.6799557400283417,
    "rmse": 0.6799557400283417
}
[08/28/2025 19:27:45 INFO]: Test accuracy: {
    "score": -0.871547671881336,
    "rmse": 0.871547671881336
}
[08/28/2025 19:27:45 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_175",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.871547671881336,
        "rmse": 0.871547671881336
    },
    "train_stats": {
        "score": -1.0083861415370625,
        "rmse": 1.0083861415370625
    },
    "val_stats": {
        "score": -0.6799557400283417,
        "rmse": 0.6799557400283417
    }
}
[08/28/2025 19:27:45 INFO]: Procewss finished for trial blotchy-Amado_trial_175
[08/28/2025 19:27:59 INFO]: Training loss at epoch 30: 1.107795089483261
[08/28/2025 19:28:53 INFO]: Training loss at epoch 31: 0.9872480034828186
[08/28/2025 19:29:01 INFO]: Training loss at epoch 29: 1.0932314991950989
[08/28/2025 19:29:16 INFO]: New best epoch, val score: -0.6790568365804825
[08/28/2025 19:29:16 INFO]: Saving model to: blotchy-Amado_trial_176/model_best.pth
[08/28/2025 19:29:25 INFO]: Training loss at epoch 24: 0.9316025078296661
[08/28/2025 19:29:28 INFO]: Training loss at epoch 31: 0.9872376620769501
[08/28/2025 19:29:41 INFO]: Training loss at epoch 23: 0.9604671597480774
[08/28/2025 19:30:05 INFO]: Training stats: {
    "score": -0.9987634435449916,
    "rmse": 0.9987634435449916
}
[08/28/2025 19:30:05 INFO]: Val stats: {
    "score": -0.6662672929037764,
    "rmse": 0.6662672929037764
}
[08/28/2025 19:30:05 INFO]: Test stats: {
    "score": -0.8673218660740367,
    "rmse": 0.8673218660740367
}
[08/28/2025 19:30:26 INFO]: Training loss at epoch 19: 1.4186137020587921
[08/28/2025 19:30:29 INFO]: Training loss at epoch 30: 0.9122789800167084
[08/28/2025 19:31:05 INFO]: Training loss at epoch 31: 1.072145402431488
[08/28/2025 19:31:29 INFO]: Running Final Evaluation...
[08/28/2025 19:31:33 INFO]: Training loss at epoch 23: 0.9853114187717438
[08/28/2025 19:31:58 INFO]: Training loss at epoch 32: 1.056020200252533
[08/28/2025 19:32:00 INFO]: Training loss at epoch 19: 0.8658710718154907
[08/28/2025 19:32:13 INFO]: Training stats: {
    "score": -1.0344924511683748,
    "rmse": 1.0344924511683748
}
[08/28/2025 19:32:13 INFO]: Val stats: {
    "score": -0.694729007818858,
    "rmse": 0.694729007818858
}
[08/28/2025 19:32:13 INFO]: Test stats: {
    "score": -0.9203079254912919,
    "rmse": 0.9203079254912919
}
[08/28/2025 19:32:24 INFO]: Training loss at epoch 32: 0.8966588079929352
[08/28/2025 19:32:41 INFO]: Training accuracy: {
    "score": -1.0081108971617943,
    "rmse": 1.0081108971617943
}
[08/28/2025 19:32:41 INFO]: Val accuracy: {
    "score": -0.6820317262876866,
    "rmse": 0.6820317262876866
}
[08/28/2025 19:32:41 INFO]: Test accuracy: {
    "score": -0.8575181659774588,
    "rmse": 0.8575181659774588
}
[08/28/2025 19:32:41 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_177",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8575181659774588,
        "rmse": 0.8575181659774588
    },
    "train_stats": {
        "score": -1.0081108971617943,
        "rmse": 1.0081108971617943
    },
    "val_stats": {
        "score": -0.6820317262876866,
        "rmse": 0.6820317262876866
    }
}
[08/28/2025 19:32:41 INFO]: Procewss finished for trial blotchy-Amado_trial_177
[08/28/2025 19:33:04 INFO]: Training loss at epoch 30: 0.9361879229545593
[08/28/2025 19:33:38 INFO]: Training loss at epoch 31: 1.1657266318798065
[08/28/2025 19:33:48 INFO]: Training stats: {
    "score": -0.9671273003769036,
    "rmse": 0.9671273003769036
}
[08/28/2025 19:33:48 INFO]: Val stats: {
    "score": -0.7006720074268669,
    "rmse": 0.7006720074268669
}
[08/28/2025 19:33:48 INFO]: Test stats: {
    "score": -0.9022660064033103,
    "rmse": 0.9022660064033103
}
[08/28/2025 19:34:05 INFO]: Training loss at epoch 72: 0.8858915269374847
[08/28/2025 19:34:54 INFO]: Training loss at epoch 24: 1.1627727150917053
[08/28/2025 19:35:03 INFO]: Training loss at epoch 33: 0.8486202359199524
[08/28/2025 19:35:18 INFO]: Training loss at epoch 33: 0.773601621389389
[08/28/2025 19:35:58 INFO]: Training loss at epoch 31: 1.1478743255138397
[08/28/2025 19:36:31 INFO]: Training loss at epoch 24: 1.0129833817481995
[08/28/2025 19:36:46 INFO]: Training loss at epoch 32: 0.9359925389289856
[08/28/2025 19:37:27 INFO]: Training loss at epoch 20: 0.9262531399726868
[08/28/2025 19:38:07 INFO]: Training loss at epoch 34: 1.0823876857757568
[08/28/2025 19:38:11 INFO]: Training loss at epoch 34: 1.1924152076244354
[08/28/2025 19:38:54 INFO]: Training loss at epoch 32: 0.901298463344574
[08/28/2025 19:39:11 INFO]: Training loss at epoch 20: 0.9347770512104034
[08/28/2025 19:39:54 INFO]: Training loss at epoch 33: 0.8617831468582153
[08/28/2025 19:40:06 INFO]: Training loss at epoch 25: 1.3410587906837463
[08/28/2025 19:40:37 INFO]: Training loss at epoch 73: 0.8748018443584442
[08/28/2025 19:41:05 INFO]: Training loss at epoch 35: 1.302230417728424
[08/28/2025 19:41:11 INFO]: Training loss at epoch 35: 1.0161890983581543
[08/28/2025 19:41:30 INFO]: Training loss at epoch 25: 0.8448793590068817
[08/28/2025 19:41:49 INFO]: Training loss at epoch 33: 1.251405417919159
[08/28/2025 19:42:03 INFO]: Training loss at epoch 25: 0.8660163283348083
[08/28/2025 19:42:41 INFO]: Training loss at epoch 21: 0.8350740075111389
[08/28/2025 19:43:03 INFO]: Training loss at epoch 34: 1.1460897326469421
[08/28/2025 19:43:19 INFO]: New best epoch, val score: -0.6722127992557833
[08/28/2025 19:43:19 INFO]: Saving model to: blotchy-Amado_trial_174/model_best.pth
[08/28/2025 19:43:58 INFO]: Training loss at epoch 36: 1.2424104809761047
[08/28/2025 19:44:15 INFO]: Training loss at epoch 36: 0.9745884239673615
[08/28/2025 19:44:34 INFO]: Training loss at epoch 21: 0.9008610248565674
[08/28/2025 19:44:44 INFO]: Training loss at epoch 34: 1.1661124229431152
[08/28/2025 19:45:06 INFO]: Running Final Evaluation...
[08/28/2025 19:45:19 INFO]: Training loss at epoch 26: 1.0909602046012878
[08/28/2025 19:46:12 INFO]: Training loss at epoch 35: 0.9487577676773071
[08/28/2025 19:46:14 INFO]: Training accuracy: {
    "score": -1.0061840925668493,
    "rmse": 1.0061840925668493
}
[08/28/2025 19:46:14 INFO]: Val accuracy: {
    "score": -0.6534481276921902,
    "rmse": 0.6534481276921902
}
[08/28/2025 19:46:14 INFO]: Test accuracy: {
    "score": -0.8635927058105549,
    "rmse": 0.8635927058105549
}
[08/28/2025 19:46:14 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_180",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8635927058105549,
        "rmse": 0.8635927058105549
    },
    "train_stats": {
        "score": -1.0061840925668493,
        "rmse": 1.0061840925668493
    },
    "val_stats": {
        "score": -0.6534481276921902,
        "rmse": 0.6534481276921902
    }
}
[08/28/2025 19:46:14 INFO]: Procewss finished for trial blotchy-Amado_trial_180
[08/28/2025 19:46:27 INFO]: Training loss at epoch 26: 0.9404043853282928
[08/28/2025 19:46:51 INFO]: Training loss at epoch 37: 1.114745557308197
[08/28/2025 19:47:10 INFO]: Training loss at epoch 74: 0.8603419959545135
[08/28/2025 19:47:19 INFO]: Training loss at epoch 37: 0.9654902517795563
[08/28/2025 19:47:52 INFO]: Training loss at epoch 22: 1.1917203068733215
[08/28/2025 19:48:28 INFO]: New best epoch, val score: -0.6718502387985795
[08/28/2025 19:48:28 INFO]: Saving model to: blotchy-Amado_trial_174/model_best.pth
[08/28/2025 19:49:20 INFO]: Training loss at epoch 36: 1.08229398727417
[08/28/2025 19:49:40 INFO]: Training loss at epoch 38: 0.9628876149654388
[08/28/2025 19:49:48 INFO]: Training loss at epoch 22: 0.9510325193405151
[08/28/2025 19:50:22 INFO]: Training loss at epoch 38: 1.2596960961818695
[08/28/2025 19:50:30 INFO]: Training loss at epoch 27: 1.0656211078166962
[08/28/2025 19:51:23 INFO]: Training loss at epoch 27: 0.8910036683082581
[08/28/2025 19:52:28 INFO]: Training loss at epoch 39: 1.0585481524467468
[08/28/2025 19:52:28 INFO]: Training loss at epoch 37: 0.8614959418773651
[08/28/2025 19:52:56 INFO]: Training loss at epoch 23: 0.9942480027675629
[08/28/2025 19:53:26 INFO]: Training loss at epoch 39: 0.9997698366641998
[08/28/2025 19:53:29 INFO]: Training stats: {
    "score": -1.000645376086455,
    "rmse": 1.000645376086455
}
[08/28/2025 19:53:29 INFO]: Val stats: {
    "score": -0.6688043213345053,
    "rmse": 0.6688043213345053
}
[08/28/2025 19:53:29 INFO]: Test stats: {
    "score": -0.8680866362164483,
    "rmse": 0.8680866362164483
}
[08/28/2025 19:53:31 INFO]: New best epoch, val score: -0.6715222739688937
[08/28/2025 19:53:31 INFO]: Saving model to: blotchy-Amado_trial_174/model_best.pth
[08/28/2025 19:53:43 INFO]: Training loss at epoch 75: 1.0104768872261047
[08/28/2025 19:54:21 INFO]: Training loss at epoch 26: 0.9137714803218842
[08/28/2025 19:54:32 INFO]: Training stats: {
    "score": -0.9965615210303571,
    "rmse": 0.9965615210303571
}
[08/28/2025 19:54:32 INFO]: Val stats: {
    "score": -0.6812442069168496,
    "rmse": 0.6812442069168496
}
[08/28/2025 19:54:32 INFO]: Test stats: {
    "score": -0.886480111783519,
    "rmse": 0.886480111783519
}
[08/28/2025 19:55:01 INFO]: Training loss at epoch 23: 0.8094240128993988
[08/28/2025 19:55:35 INFO]: Training loss at epoch 38: 1.2514750957489014
[08/28/2025 19:55:39 INFO]: Training loss at epoch 28: 1.0347392559051514
[08/28/2025 19:56:17 INFO]: Training loss at epoch 28: 0.8812035620212555
[08/28/2025 19:56:21 INFO]: Training loss at epoch 40: 1.046802043914795
[08/28/2025 19:57:33 INFO]: Training loss at epoch 40: 1.166100800037384
[08/28/2025 19:58:05 INFO]: Training loss at epoch 24: 1.090293824672699
[08/28/2025 19:58:42 INFO]: Training loss at epoch 39: 0.8947978019714355
[08/28/2025 19:59:11 INFO]: Training loss at epoch 41: 1.0784435272216797
[08/28/2025 19:59:49 INFO]: Training stats: {
    "score": -0.9952277031275185,
    "rmse": 0.9952277031275185
}
[08/28/2025 19:59:49 INFO]: Val stats: {
    "score": -0.6972374147600422,
    "rmse": 0.6972374147600422
}
[08/28/2025 19:59:49 INFO]: Test stats: {
    "score": -0.8862049457320239,
    "rmse": 0.8862049457320239
}
[08/28/2025 20:00:10 INFO]: Training loss at epoch 76: 0.8388403058052063
[08/28/2025 20:00:25 INFO]: Training loss at epoch 24: 0.9696617722511292
[08/28/2025 20:00:37 INFO]: Training loss at epoch 41: 1.0046824514865875
[08/28/2025 20:00:49 INFO]: Training loss at epoch 29: 0.8433338105678558
[08/28/2025 20:01:11 INFO]: Training loss at epoch 29: 0.8938951790332794
[08/28/2025 20:02:02 INFO]: Training loss at epoch 42: 1.0456315875053406
[08/28/2025 20:02:24 INFO]: Running Final Evaluation...
[08/28/2025 20:02:30 INFO]: Training stats: {
    "score": -0.966214624756415,
    "rmse": 0.966214624756415
}
[08/28/2025 20:02:30 INFO]: Val stats: {
    "score": -0.64999812404002,
    "rmse": 0.64999812404002
}
[08/28/2025 20:02:30 INFO]: Test stats: {
    "score": -0.8879404054955393,
    "rmse": 0.8879404054955393
}
[08/28/2025 20:02:47 INFO]: Training stats: {
    "score": -0.9817169068718343,
    "rmse": 0.9817169068718343
}
[08/28/2025 20:02:47 INFO]: Val stats: {
    "score": -0.7095161098663434,
    "rmse": 0.7095161098663434
}
[08/28/2025 20:02:47 INFO]: Test stats: {
    "score": -0.9018029624669374,
    "rmse": 0.9018029624669374
}
[08/28/2025 20:02:54 INFO]: Training loss at epoch 40: 0.9956583976745605
[08/28/2025 20:03:06 INFO]: New best epoch, val score: -0.64999812404002
[08/28/2025 20:03:06 INFO]: Saving model to: blotchy-Amado_trial_171/model_best.pth
[08/28/2025 20:03:14 INFO]: Training loss at epoch 25: 0.8779024481773376
[08/28/2025 20:03:18 INFO]: Running Final Evaluation...
[08/28/2025 20:03:29 INFO]: Training accuracy: {
    "score": -1.0071243439366513,
    "rmse": 1.0071243439366513
}
[08/28/2025 20:03:29 INFO]: Val accuracy: {
    "score": -0.6625898016167231,
    "rmse": 0.6625898016167231
}
[08/28/2025 20:03:29 INFO]: Test accuracy: {
    "score": -0.86844531082405,
    "rmse": 0.86844531082405
}
[08/28/2025 20:03:29 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_179",
    "best_epoch": 11,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.86844531082405,
        "rmse": 0.86844531082405
    },
    "train_stats": {
        "score": -1.0071243439366513,
        "rmse": 1.0071243439366513
    },
    "val_stats": {
        "score": -0.6625898016167231,
        "rmse": 0.6625898016167231
    }
}
[08/28/2025 20:03:29 INFO]: Procewss finished for trial blotchy-Amado_trial_179
[08/28/2025 20:03:37 INFO]: Training loss at epoch 42: 0.8800906538963318
[08/28/2025 20:04:32 INFO]: Training accuracy: {
    "score": -1.02046372841725,
    "rmse": 1.02046372841725
}
[08/28/2025 20:04:32 INFO]: Val accuracy: {
    "score": -0.6630397949255852,
    "rmse": 0.6630397949255852
}
[08/28/2025 20:04:32 INFO]: Test accuracy: {
    "score": -0.8783775527755483,
    "rmse": 0.8783775527755483
}
[08/28/2025 20:04:32 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_178",
    "best_epoch": 9,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8783775527755483,
        "rmse": 0.8783775527755483
    },
    "train_stats": {
        "score": -1.02046372841725,
        "rmse": 1.02046372841725
    },
    "val_stats": {
        "score": -0.6630397949255852,
        "rmse": 0.6630397949255852
    }
}
[08/28/2025 20:04:32 INFO]: Procewss finished for trial blotchy-Amado_trial_178
[08/28/2025 20:05:43 INFO]: Training loss at epoch 25: 0.9652738869190216
[08/28/2025 20:06:32 INFO]: Training loss at epoch 77: 0.8116421699523926
[08/28/2025 20:06:35 INFO]: Training loss at epoch 43: 1.0621434450149536
[08/28/2025 20:06:39 INFO]: Training loss at epoch 27: 0.9960400462150574
[08/28/2025 20:07:32 INFO]: Training loss at epoch 30: 1.1765868663787842
[08/28/2025 20:07:34 INFO]: Training loss at epoch 30: 1.026776522397995
[08/28/2025 20:08:07 INFO]: New best epoch, val score: -0.6469750137103896
[08/28/2025 20:08:07 INFO]: Saving model to: blotchy-Amado_trial_171/model_best.pth
[08/28/2025 20:08:15 INFO]: Training loss at epoch 26: 1.0562665462493896
[08/28/2025 20:09:34 INFO]: Training loss at epoch 44: 0.9854611158370972
[08/28/2025 20:10:50 INFO]: Training loss at epoch 26: 0.9172624051570892
[08/28/2025 20:12:25 INFO]: Training loss at epoch 31: 0.9216470420360565
[08/28/2025 20:12:33 INFO]: Training loss at epoch 45: 1.0444045066833496
[08/28/2025 20:12:34 INFO]: Training loss at epoch 31: 0.8660390973091125
[08/28/2025 20:12:45 INFO]: Training loss at epoch 78: 0.8842062652111053
[08/28/2025 20:13:18 INFO]: Training loss at epoch 27: 1.155636489391327
[08/28/2025 20:15:32 INFO]: Training loss at epoch 46: 0.990129142999649
[08/28/2025 20:15:58 INFO]: Training loss at epoch 27: 0.8754362463951111
[08/28/2025 20:17:15 INFO]: Training loss at epoch 32: 0.9177854657173157
[08/28/2025 20:17:36 INFO]: Training loss at epoch 32: 1.1922033429145813
[08/28/2025 20:18:22 INFO]: Training loss at epoch 28: 1.1268027424812317
[08/28/2025 20:18:33 INFO]: Training loss at epoch 47: 1.030076265335083
[08/28/2025 20:18:38 INFO]: Training loss at epoch 28: 0.9983671009540558
[08/28/2025 20:18:58 INFO]: Training loss at epoch 79: 0.9192672371864319
[08/28/2025 20:21:04 INFO]: Training stats: {
    "score": -0.9019702084198539,
    "rmse": 0.9019702084198539
}
[08/28/2025 20:21:04 INFO]: Val stats: {
    "score": -0.7302023943678397,
    "rmse": 0.7302023943678397
}
[08/28/2025 20:21:04 INFO]: Test stats: {
    "score": -0.9452509670398218,
    "rmse": 0.9452509670398218
}
[08/28/2025 20:21:05 INFO]: Training loss at epoch 28: 0.9793092608451843
[08/28/2025 20:21:32 INFO]: Training loss at epoch 48: 0.9959258139133453
[08/28/2025 20:22:05 INFO]: Training loss at epoch 33: 0.9101355671882629
[08/28/2025 20:22:38 INFO]: Training loss at epoch 33: 0.8221419453620911
[08/28/2025 20:23:25 INFO]: Training loss at epoch 29: 0.941648930311203
[08/28/2025 20:24:32 INFO]: Training loss at epoch 49: 0.8824599981307983
[08/28/2025 20:25:07 INFO]: Training stats: {
    "score": -0.9860006570926648,
    "rmse": 0.9860006570926648
}
[08/28/2025 20:25:07 INFO]: Val stats: {
    "score": -0.6686810931660349,
    "rmse": 0.6686810931660349
}
[08/28/2025 20:25:07 INFO]: Test stats: {
    "score": -0.8960338882360611,
    "rmse": 0.8960338882360611
}
[08/28/2025 20:25:36 INFO]: Training stats: {
    "score": -0.9964523885870703,
    "rmse": 0.9964523885870703
}
[08/28/2025 20:25:36 INFO]: Val stats: {
    "score": -0.6798528683382459,
    "rmse": 0.6798528683382459
}
[08/28/2025 20:25:36 INFO]: Test stats: {
    "score": -0.8856974426946044,
    "rmse": 0.8856974426946044
}
[08/28/2025 20:25:43 INFO]: New best epoch, val score: -0.6686810931660349
[08/28/2025 20:25:43 INFO]: Saving model to: blotchy-Amado_trial_174/model_best.pth
[08/28/2025 20:26:20 INFO]: Training loss at epoch 29: 0.7399163842201233
[08/28/2025 20:26:47 INFO]: Training loss at epoch 34: 0.9208948016166687
[08/28/2025 20:27:18 INFO]: Training loss at epoch 80: 0.9026087522506714
[08/28/2025 20:27:20 INFO]: New best epoch, val score: -0.65956503661053
[08/28/2025 20:27:20 INFO]: Saving model to: blotchy-Amado_trial_172/model_best.pth
[08/28/2025 20:27:41 INFO]: Training loss at epoch 34: 0.9147413372993469
[08/28/2025 20:28:04 INFO]: Training stats: {
    "score": -0.9578286088902269,
    "rmse": 0.9578286088902269
}
[08/28/2025 20:28:04 INFO]: Val stats: {
    "score": -0.7403240635781736,
    "rmse": 0.7403240635781736
}
[08/28/2025 20:28:04 INFO]: Test stats: {
    "score": -0.93899941326415,
    "rmse": 0.93899941326415
}
[08/28/2025 20:28:42 INFO]: Training loss at epoch 50: 1.0228820443153381
[08/28/2025 20:30:11 INFO]: Training loss at epoch 30: 1.12711900472641
[08/28/2025 20:30:40 INFO]: Training loss at epoch 29: 0.8597734868526459
[08/28/2025 20:31:32 INFO]: Training loss at epoch 35: 1.0558737516403198
[08/28/2025 20:31:42 INFO]: Training loss at epoch 51: 1.080004632472992
[08/28/2025 20:32:05 INFO]: New best epoch, val score: -0.6595205741448301
[08/28/2025 20:32:05 INFO]: Saving model to: blotchy-Amado_trial_172/model_best.pth
[08/28/2025 20:32:41 INFO]: Training loss at epoch 35: 1.1841237843036652
[08/28/2025 20:33:17 INFO]: Training loss at epoch 30: 0.9106982350349426
[08/28/2025 20:33:35 INFO]: Training loss at epoch 81: 1.0089982151985168
[08/28/2025 20:34:42 INFO]: Training loss at epoch 52: 0.954342782497406
[08/28/2025 20:34:47 INFO]: Training stats: {
    "score": -0.9028548799430304,
    "rmse": 0.9028548799430304
}
[08/28/2025 20:34:47 INFO]: Val stats: {
    "score": -0.6837262402863374,
    "rmse": 0.6837262402863374
}
[08/28/2025 20:34:47 INFO]: Test stats: {
    "score": -0.8897978980980055,
    "rmse": 0.8897978980980055
}
[08/28/2025 20:35:05 INFO]: New best epoch, val score: -0.6783209827740253
[08/28/2025 20:35:05 INFO]: Saving model to: blotchy-Amado_trial_176/model_best.pth
[08/28/2025 20:35:16 INFO]: Training loss at epoch 31: 1.2446538805961609
[08/28/2025 20:36:15 INFO]: Training loss at epoch 36: 1.1224702596664429
[08/28/2025 20:37:42 INFO]: Training loss at epoch 53: 1.0388017296791077
[08/28/2025 20:37:45 INFO]: Training loss at epoch 36: 1.0280163884162903
[08/28/2025 20:38:05 INFO]: New best epoch, val score: -0.6778720267394286
[08/28/2025 20:38:05 INFO]: Saving model to: blotchy-Amado_trial_176/model_best.pth
[08/28/2025 20:38:31 INFO]: Training loss at epoch 31: 1.0046668946743011
[08/28/2025 20:39:48 INFO]: Training loss at epoch 82: 0.8430825769901276
[08/28/2025 20:40:21 INFO]: Training loss at epoch 32: 0.8847319781780243
[08/28/2025 20:40:43 INFO]: Training loss at epoch 54: 1.426118165254593
[08/28/2025 20:40:57 INFO]: Training loss at epoch 37: 0.9004892706871033
[08/28/2025 20:40:57 INFO]: New best epoch, val score: -0.667312578117944
[08/28/2025 20:40:57 INFO]: Saving model to: blotchy-Amado_trial_174/model_best.pth
[08/28/2025 20:41:06 INFO]: New best epoch, val score: -0.6775544341840495
[08/28/2025 20:41:06 INFO]: Saving model to: blotchy-Amado_trial_176/model_best.pth
[08/28/2025 20:42:48 INFO]: Training loss at epoch 37: 0.9084510803222656
[08/28/2025 20:43:43 INFO]: Training loss at epoch 55: 1.089317262172699
[08/28/2025 20:43:46 INFO]: Training loss at epoch 32: 0.9101740419864655
[08/28/2025 20:45:27 INFO]: Training loss at epoch 33: 0.850781112909317
[08/28/2025 20:45:39 INFO]: Training loss at epoch 38: 0.997087836265564
[08/28/2025 20:46:01 INFO]: Training loss at epoch 83: 0.8043606281280518
[08/28/2025 20:46:44 INFO]: Training loss at epoch 56: 1.008097618818283
[08/28/2025 20:46:47 INFO]: Training loss at epoch 30: 0.6907325685024261
[08/28/2025 20:47:48 INFO]: Training loss at epoch 38: 0.911226898431778
[08/28/2025 20:48:55 INFO]: Training loss at epoch 33: 0.9180964529514313
[08/28/2025 20:49:44 INFO]: Training loss at epoch 57: 1.149025946855545
[08/28/2025 20:50:24 INFO]: Training loss at epoch 34: 0.8132289946079254
[08/28/2025 20:50:24 INFO]: Training loss at epoch 39: 0.9512686431407928
[08/28/2025 20:52:00 INFO]: Training stats: {
    "score": -0.9732792978151228,
    "rmse": 0.9732792978151228
}
[08/28/2025 20:52:00 INFO]: Val stats: {
    "score": -0.709990815400449,
    "rmse": 0.709990815400449
}
[08/28/2025 20:52:00 INFO]: Test stats: {
    "score": -0.9071667181486925,
    "rmse": 0.9071667181486925
}
[08/28/2025 20:52:17 INFO]: Training loss at epoch 84: 0.9403020739555359
[08/28/2025 20:52:43 INFO]: Training loss at epoch 58: 1.023350715637207
[08/28/2025 20:52:43 INFO]: Training loss at epoch 39: 0.9836245477199554
[08/28/2025 20:54:02 INFO]: Training loss at epoch 34: 0.7748262286186218
[08/28/2025 20:54:21 INFO]: Training stats: {
    "score": -0.9581954396831996,
    "rmse": 0.9581954396831996
}
[08/28/2025 20:54:21 INFO]: Val stats: {
    "score": -0.6623771946200518,
    "rmse": 0.6623771946200518
}
[08/28/2025 20:54:21 INFO]: Test stats: {
    "score": -0.8942502112404476,
    "rmse": 0.8942502112404476
}
[08/28/2025 20:55:21 INFO]: Training loss at epoch 35: 0.8144960701465607
[08/28/2025 20:55:41 INFO]: Training loss at epoch 59: 0.9292623698711395
[08/28/2025 20:56:44 INFO]: Training loss at epoch 40: 1.1547368466854095
[08/28/2025 20:56:45 INFO]: Training stats: {
    "score": -0.995866814804743,
    "rmse": 0.995866814804743
}
[08/28/2025 20:56:45 INFO]: Val stats: {
    "score": -0.6793923695021695,
    "rmse": 0.6793923695021695
}
[08/28/2025 20:56:45 INFO]: Test stats: {
    "score": -0.8851041297304135,
    "rmse": 0.8851041297304135
}
[08/28/2025 20:58:33 INFO]: Training loss at epoch 85: 0.9494264125823975
[08/28/2025 20:58:53 INFO]: Training loss at epoch 31: 0.7850042581558228
[08/28/2025 20:59:09 INFO]: Training loss at epoch 35: 0.7742996513843536
[08/28/2025 20:59:17 INFO]: Training loss at epoch 40: 1.2121241092681885
[08/28/2025 20:59:42 INFO]: Training loss at epoch 60: 1.1004758179187775
[08/28/2025 20:59:45 INFO]: Running Final Evaluation...
[08/28/2025 21:00:19 INFO]: Training loss at epoch 36: 0.9119739234447479
[08/28/2025 21:01:28 INFO]: Training loss at epoch 41: 0.9868136942386627
[08/28/2025 21:02:01 INFO]: Training accuracy: {
    "score": -0.9851318984656819,
    "rmse": 0.9851318984656819
}
[08/28/2025 21:02:01 INFO]: Val accuracy: {
    "score": -0.674104276947742,
    "rmse": 0.674104276947742
}
[08/28/2025 21:02:01 INFO]: Test accuracy: {
    "score": -0.8865380128344738,
    "rmse": 0.8865380128344738
}
[08/28/2025 21:02:01 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_173",
    "best_epoch": 4,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8865380128344738,
        "rmse": 0.8865380128344738
    },
    "train_stats": {
        "score": -0.9851318984656819,
        "rmse": 0.9851318984656819
    },
    "val_stats": {
        "score": -0.674104276947742,
        "rmse": 0.674104276947742
    }
}
[08/28/2025 21:02:01 INFO]: Procewss finished for trial blotchy-Amado_trial_173
[08/28/2025 21:02:46 INFO]: Training loss at epoch 61: 0.9085464179515839
[08/28/2025 21:04:16 INFO]: Training loss at epoch 41: 0.9507438838481903
[08/28/2025 21:04:46 INFO]: Training loss at epoch 86: 0.9165655672550201
[08/28/2025 21:05:19 INFO]: Training loss at epoch 37: 1.1333417296409607
[08/28/2025 21:05:46 INFO]: Training loss at epoch 62: 0.9089314341545105
[08/28/2025 21:06:10 INFO]: Training loss at epoch 42: 0.8616531193256378
[08/28/2025 21:08:43 INFO]: Training loss at epoch 63: 1.0509160459041595
[08/28/2025 21:09:15 INFO]: Training loss at epoch 42: 0.9312495291233063
[08/28/2025 21:10:20 INFO]: Training loss at epoch 38: 1.090680718421936
[08/28/2025 21:10:50 INFO]: Training loss at epoch 43: 1.1895524859428406
[08/28/2025 21:10:51 INFO]: Training loss at epoch 32: 0.9186712503433228
[08/28/2025 21:10:58 INFO]: Training loss at epoch 87: 0.8803052306175232
[08/28/2025 21:11:40 INFO]: Training loss at epoch 64: 0.8239545226097107
[08/28/2025 21:14:12 INFO]: Training loss at epoch 43: 1.038952112197876
[08/28/2025 21:14:37 INFO]: Training loss at epoch 65: 1.0644051432609558
[08/28/2025 21:15:18 INFO]: Training loss at epoch 39: 1.2070735692977905
[08/28/2025 21:15:31 INFO]: Training loss at epoch 44: 1.0840103030204773
[08/28/2025 21:16:59 INFO]: Training stats: {
    "score": -1.0292578251126607,
    "rmse": 1.0292578251126607
}
[08/28/2025 21:16:59 INFO]: Val stats: {
    "score": -0.6992670582374844,
    "rmse": 0.6992670582374844
}
[08/28/2025 21:16:59 INFO]: Test stats: {
    "score": -0.9325888858020797,
    "rmse": 0.9325888858020797
}
[08/28/2025 21:17:09 INFO]: Training loss at epoch 88: 0.9043318033218384
[08/28/2025 21:17:35 INFO]: Training loss at epoch 66: 1.096163809299469
[08/28/2025 21:19:12 INFO]: Training loss at epoch 44: 1.0940324068069458
[08/28/2025 21:20:11 INFO]: Training loss at epoch 45: 1.0652281641960144
[08/28/2025 21:20:32 INFO]: Training loss at epoch 67: 1.0624225735664368
[08/28/2025 21:21:59 INFO]: Training loss at epoch 40: 1.0184795558452606
[08/28/2025 21:22:47 INFO]: Training loss at epoch 33: 0.8949714601039886
[08/28/2025 21:23:21 INFO]: Training loss at epoch 89: 0.8078912198543549
[08/28/2025 21:23:29 INFO]: Training loss at epoch 68: 0.9926499724388123
[08/28/2025 21:24:09 INFO]: Training loss at epoch 45: 1.2079195976257324
[08/28/2025 21:24:52 INFO]: Training loss at epoch 46: 0.9020824432373047
[08/28/2025 21:25:27 INFO]: Training stats: {
    "score": -0.9100865071079881,
    "rmse": 0.9100865071079881
}
[08/28/2025 21:25:27 INFO]: Val stats: {
    "score": -0.7950749848984978,
    "rmse": 0.7950749848984978
}
[08/28/2025 21:25:27 INFO]: Test stats: {
    "score": -0.9916056972868041,
    "rmse": 0.9916056972868041
}
[08/28/2025 21:26:26 INFO]: Training loss at epoch 69: 0.9772914350032806
[08/28/2025 21:26:58 INFO]: Training loss at epoch 41: 1.0536675453186035
[08/28/2025 21:27:30 INFO]: Training stats: {
    "score": -0.992042173702289,
    "rmse": 0.992042173702289
}
[08/28/2025 21:27:30 INFO]: Val stats: {
    "score": -0.6860238246612382,
    "rmse": 0.6860238246612382
}
[08/28/2025 21:27:30 INFO]: Test stats: {
    "score": -0.8866855938092376,
    "rmse": 0.8866855938092376
}
[08/28/2025 21:29:06 INFO]: Training loss at epoch 46: 0.8662418723106384
[08/28/2025 21:29:33 INFO]: Training loss at epoch 47: 0.9857116639614105
[08/28/2025 21:30:26 INFO]: Training loss at epoch 70: 0.8900262117385864
[08/28/2025 21:31:38 INFO]: Training loss at epoch 90: 0.8533287942409515
[08/28/2025 21:31:57 INFO]: Training loss at epoch 42: 0.995846152305603
[08/28/2025 21:32:22 INFO]: Running Final Evaluation...
[08/28/2025 21:33:24 INFO]: Training loss at epoch 71: 1.0048908591270447
[08/28/2025 21:34:04 INFO]: Training loss at epoch 47: 0.7556861788034439
[08/28/2025 21:34:13 INFO]: Training loss at epoch 48: 1.0643060803413391
[08/28/2025 21:34:42 INFO]: Training loss at epoch 34: 0.7579589486122131
[08/28/2025 21:34:53 INFO]: Training accuracy: {
    "score": -0.9413646792590098,
    "rmse": 0.9413646792590098
}
[08/28/2025 21:34:53 INFO]: Val accuracy: {
    "score": -0.674585996929018,
    "rmse": 0.674585996929018
}
[08/28/2025 21:34:53 INFO]: Test accuracy: {
    "score": -0.8954627762078071,
    "rmse": 0.8954627762078071
}
[08/28/2025 21:34:53 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_145",
    "best_epoch": 59,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8954627762078071,
        "rmse": 0.8954627762078071
    },
    "train_stats": {
        "score": -0.9413646792590098,
        "rmse": 0.9413646792590098
    },
    "val_stats": {
        "score": -0.674585996929018,
        "rmse": 0.674585996929018
    }
}
[08/28/2025 21:34:53 INFO]: Procewss finished for trial blotchy-Amado_trial_145
[08/28/2025 21:36:21 INFO]: Training loss at epoch 72: 0.9218933880329132
[08/28/2025 21:36:58 INFO]: Training loss at epoch 43: 0.879103809595108
[08/28/2025 21:39:01 INFO]: Training loss at epoch 49: 0.7532977759838104
[08/28/2025 21:39:03 INFO]: Training loss at epoch 48: 1.0520091652870178
[08/28/2025 21:39:18 INFO]: Training loss at epoch 73: 0.9981372356414795
[08/28/2025 21:40:35 INFO]: Training stats: {
    "score": -0.9718696046019325,
    "rmse": 0.9718696046019325
}
[08/28/2025 21:40:35 INFO]: Val stats: {
    "score": -0.6600341359539752,
    "rmse": 0.6600341359539752
}
[08/28/2025 21:40:35 INFO]: Test stats: {
    "score": -0.8958677733921838,
    "rmse": 0.8958677733921838
}
[08/28/2025 21:41:57 INFO]: Training loss at epoch 44: 1.075475662946701
[08/28/2025 21:42:15 INFO]: Training loss at epoch 74: 1.049250304698944
[08/28/2025 21:42:32 INFO]: New best epoch, val score: -0.6661568854934936
[08/28/2025 21:42:32 INFO]: Saving model to: blotchy-Amado_trial_174/model_best.pth
[08/28/2025 21:44:02 INFO]: Training loss at epoch 49: 0.9668565392494202
[08/28/2025 21:45:16 INFO]: Training loss at epoch 75: 0.8122385144233704
[08/28/2025 21:45:22 INFO]: Training loss at epoch 50: 0.9728974401950836
[08/28/2025 21:45:44 INFO]: Training stats: {
    "score": -0.9829381733267879,
    "rmse": 0.9829381733267879
}
[08/28/2025 21:45:44 INFO]: Val stats: {
    "score": -0.6612520506528651,
    "rmse": 0.6612520506528651
}
[08/28/2025 21:45:44 INFO]: Test stats: {
    "score": -0.9078052037792639,
    "rmse": 0.9078052037792639
}
[08/28/2025 21:46:53 INFO]: Training loss at epoch 35: 0.7780988216400146
[08/28/2025 21:47:01 INFO]: Training loss at epoch 45: 1.0717517733573914
[08/28/2025 21:47:39 INFO]: New best epoch, val score: -0.6643142443457373
[08/28/2025 21:47:39 INFO]: Saving model to: blotchy-Amado_trial_174/model_best.pth
[08/28/2025 21:48:18 INFO]: Training loss at epoch 76: 1.1749058961868286
[08/28/2025 21:50:06 INFO]: Training loss at epoch 51: 1.0107742846012115
[08/28/2025 21:50:46 INFO]: Training loss at epoch 50: 0.9446885585784912
[08/28/2025 21:51:15 INFO]: Training loss at epoch 77: 0.9829129576683044
[08/28/2025 21:52:03 INFO]: Training loss at epoch 46: 0.8467711806297302
[08/28/2025 21:54:12 INFO]: Training loss at epoch 78: 1.0881589353084564
[08/28/2025 21:54:53 INFO]: Training loss at epoch 52: 0.8890618681907654
[08/28/2025 21:55:43 INFO]: Training loss at epoch 51: 0.8985728025436401
[08/28/2025 21:57:02 INFO]: Training loss at epoch 47: 0.8959680795669556
[08/28/2025 21:57:09 INFO]: Training loss at epoch 79: 0.9261773228645325
[08/28/2025 21:58:12 INFO]: Training stats: {
    "score": -0.9904718173465494,
    "rmse": 0.9904718173465494
}
[08/28/2025 21:58:12 INFO]: Val stats: {
    "score": -0.6931502068848772,
    "rmse": 0.6931502068848772
}
[08/28/2025 21:58:12 INFO]: Test stats: {
    "score": -0.8899820496753416,
    "rmse": 0.8899820496753416
}
[08/28/2025 21:59:05 INFO]: Training loss at epoch 36: 0.9144537150859833
[08/28/2025 21:59:39 INFO]: Training loss at epoch 53: 0.8214237093925476
[08/28/2025 22:00:44 INFO]: Training loss at epoch 52: 0.9058881103992462
[08/28/2025 22:01:12 INFO]: Training loss at epoch 80: 0.987620621919632
[08/28/2025 22:02:00 INFO]: Training loss at epoch 48: 1.1789869368076324
[08/28/2025 22:04:13 INFO]: Training loss at epoch 81: 1.1135132610797882
[08/28/2025 22:04:22 INFO]: Training loss at epoch 54: 1.00603586435318
[08/28/2025 22:05:50 INFO]: Training loss at epoch 53: 0.9871265590190887
[08/28/2025 22:06:58 INFO]: Training loss at epoch 49: 1.088418424129486
[08/28/2025 22:07:15 INFO]: Training loss at epoch 82: 0.8839728236198425
[08/28/2025 22:08:38 INFO]: Training stats: {
    "score": -0.9886035718474672,
    "rmse": 0.9886035718474672
}
[08/28/2025 22:08:38 INFO]: Val stats: {
    "score": -0.6757383354730934,
    "rmse": 0.6757383354730934
}
[08/28/2025 22:08:38 INFO]: Test stats: {
    "score": -0.911158191056079,
    "rmse": 0.911158191056079
}
[08/28/2025 22:09:02 INFO]: Training loss at epoch 55: 1.3154402077198029
[08/28/2025 22:10:16 INFO]: Training loss at epoch 83: 1.0276289284229279
[08/28/2025 22:10:59 INFO]: Training loss at epoch 54: 1.1972749829292297
[08/28/2025 22:11:02 INFO]: Training loss at epoch 37: 0.7659398317337036
[08/28/2025 22:13:16 INFO]: Training loss at epoch 84: 0.8035113513469696
[08/28/2025 22:13:35 INFO]: Training loss at epoch 50: 1.1223482489585876
[08/28/2025 22:13:48 INFO]: Training loss at epoch 56: 1.1963737308979034
[08/28/2025 22:15:59 INFO]: Training loss at epoch 55: 0.9576175212860107
[08/28/2025 22:16:14 INFO]: Training loss at epoch 85: 0.9987629652023315
[08/28/2025 22:16:36 INFO]: Running Final Evaluation...
[08/28/2025 22:17:43 INFO]: Training accuracy: {
    "score": -0.998672381149561,
    "rmse": 0.998672381149561
}
[08/28/2025 22:17:43 INFO]: Val accuracy: {
    "score": -0.6775544341840495,
    "rmse": 0.6775544341840495
}
[08/28/2025 22:17:43 INFO]: Test accuracy: {
    "score": -0.8855635480990486,
    "rmse": 0.8855635480990486
}
[08/28/2025 22:17:43 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_176",
    "best_epoch": 54,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8855635480990486,
        "rmse": 0.8855635480990486
    },
    "train_stats": {
        "score": -0.998672381149561,
        "rmse": 0.998672381149561
    },
    "val_stats": {
        "score": -0.6775544341840495,
        "rmse": 0.6775544341840495
    }
}
[08/28/2025 22:17:43 INFO]: Procewss finished for trial blotchy-Amado_trial_176
[08/28/2025 22:18:31 INFO]: Training loss at epoch 57: 0.9037509858608246
[08/28/2025 22:18:40 INFO]: Training loss at epoch 51: 1.0583361387252808
[08/28/2025 22:20:58 INFO]: Training loss at epoch 56: 0.9000019729137421
[08/28/2025 22:23:12 INFO]: Training loss at epoch 38: 0.7731981873512268
[08/28/2025 22:23:16 INFO]: Training loss at epoch 58: 0.8562449514865875
[08/28/2025 22:23:39 INFO]: Training loss at epoch 52: 0.8632155656814575
[08/28/2025 22:25:56 INFO]: Training loss at epoch 57: 1.0368502736091614
[08/28/2025 22:28:00 INFO]: Training loss at epoch 59: 0.805423766374588
[08/28/2025 22:28:40 INFO]: Training loss at epoch 53: 0.9873822331428528
[08/28/2025 22:29:36 INFO]: Training stats: {
    "score": -0.9465683264438093,
    "rmse": 0.9465683264438093
}
[08/28/2025 22:29:36 INFO]: Val stats: {
    "score": -0.6748745826270153,
    "rmse": 0.6748745826270153
}
[08/28/2025 22:29:36 INFO]: Test stats: {
    "score": -0.9021024955474984,
    "rmse": 0.9021024955474984
}
[08/28/2025 22:30:55 INFO]: Training loss at epoch 58: 0.8287409543991089
[08/28/2025 22:33:41 INFO]: Training loss at epoch 54: 0.8776625096797943
[08/28/2025 22:34:20 INFO]: Training loss at epoch 60: 0.9438167214393616
[08/28/2025 22:35:20 INFO]: Training loss at epoch 39: 0.7927321195602417
[08/28/2025 22:35:53 INFO]: Training loss at epoch 59: 0.9653556048870087
[08/28/2025 22:37:30 INFO]: Training stats: {
    "score": -0.9416198815263463,
    "rmse": 0.9416198815263463
}
[08/28/2025 22:37:30 INFO]: Val stats: {
    "score": -0.6732874450053992,
    "rmse": 0.6732874450053992
}
[08/28/2025 22:37:30 INFO]: Test stats: {
    "score": -0.9085111910134526,
    "rmse": 0.9085111910134526
}
[08/28/2025 22:38:41 INFO]: Training loss at epoch 55: 0.9500474035739899
[08/28/2025 22:39:01 INFO]: Training loss at epoch 61: 0.9505364298820496
[08/28/2025 22:39:30 INFO]: Training stats: {
    "score": -0.8420060841465767,
    "rmse": 0.8420060841465767
}
[08/28/2025 22:39:30 INFO]: Val stats: {
    "score": -0.7222161843249981,
    "rmse": 0.7222161843249981
}
[08/28/2025 22:39:30 INFO]: Test stats: {
    "score": -0.9284532419347598,
    "rmse": 0.9284532419347598
}
[08/28/2025 22:42:27 INFO]: Training loss at epoch 60: 1.0634679198265076
[08/28/2025 22:43:42 INFO]: Training loss at epoch 62: 0.9311952590942383
[08/28/2025 22:43:43 INFO]: Training loss at epoch 56: 0.8716496527194977
[08/28/2025 22:47:24 INFO]: Training loss at epoch 61: 0.907433271408081
[08/28/2025 22:47:59 INFO]: Running Final Evaluation...
[08/28/2025 22:48:26 INFO]: Training loss at epoch 63: 0.8218016922473907
[08/28/2025 22:48:43 INFO]: Training loss at epoch 57: 0.8550105690956116
[08/28/2025 22:50:09 INFO]: Training accuracy: {
    "score": -0.9695356871190604,
    "rmse": 0.9695356871190604
}
[08/28/2025 22:50:09 INFO]: Val accuracy: {
    "score": -0.6469750137103896,
    "rmse": 0.6469750137103896
}
[08/28/2025 22:50:09 INFO]: Test accuracy: {
    "score": -0.8898237600116695,
    "rmse": 0.8898237600116695
}
[08/28/2025 22:50:09 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_171",
    "best_epoch": 30,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8898237600116695,
        "rmse": 0.8898237600116695
    },
    "train_stats": {
        "score": -0.9695356871190604,
        "rmse": 0.9695356871190604
    },
    "val_stats": {
        "score": -0.6469750137103896,
        "rmse": 0.6469750137103896
    }
}
[08/28/2025 22:50:09 INFO]: Procewss finished for trial blotchy-Amado_trial_171
[08/28/2025 22:51:31 INFO]: Training loss at epoch 40: 0.7512122392654419
[08/28/2025 22:53:26 INFO]: Training loss at epoch 64: 0.9737990498542786
[08/28/2025 22:53:38 INFO]: Training loss at epoch 58: 1.0295715630054474
[08/28/2025 22:58:34 INFO]: Training loss at epoch 59: 0.7445589452981949
[08/28/2025 22:58:37 INFO]: Training loss at epoch 65: 0.7289422899484634
[08/28/2025 23:00:12 INFO]: Training stats: {
    "score": -0.941453894091228,
    "rmse": 0.941453894091228
}
[08/28/2025 23:00:12 INFO]: Val stats: {
    "score": -0.6854201785012051,
    "rmse": 0.6854201785012051
}
[08/28/2025 23:00:12 INFO]: Test stats: {
    "score": -0.9094996849566391,
    "rmse": 0.9094996849566391
}
[08/28/2025 23:03:27 INFO]: Training loss at epoch 41: 0.6051834523677826
[08/28/2025 23:03:44 INFO]: Training loss at epoch 66: 1.0227050185203552
[08/28/2025 23:04:20 INFO]: Running Final Evaluation...
[08/28/2025 23:05:09 INFO]: Training loss at epoch 60: 1.135679006576538
[08/28/2025 23:06:24 INFO]: Training accuracy: {
    "score": -0.9794960191960626,
    "rmse": 0.9794960191960626
}
[08/28/2025 23:06:24 INFO]: Val accuracy: {
    "score": -0.6595205741448301,
    "rmse": 0.6595205741448301
}
[08/28/2025 23:06:24 INFO]: Test accuracy: {
    "score": -0.8829481582498663,
    "rmse": 0.8829481582498663
}
[08/28/2025 23:06:24 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_172",
    "best_epoch": 35,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8829481582498663,
        "rmse": 0.8829481582498663
    },
    "train_stats": {
        "score": -0.9794960191960626,
        "rmse": 0.9794960191960626
    },
    "val_stats": {
        "score": -0.6595205741448301,
        "rmse": 0.6595205741448301
    }
}
[08/28/2025 23:06:24 INFO]: Procewss finished for trial blotchy-Amado_trial_172
[08/28/2025 23:10:26 INFO]: Training loss at epoch 61: 0.8039867579936981
[08/28/2025 23:15:40 INFO]: Training loss at epoch 62: 0.9940985441207886
[08/28/2025 23:15:49 INFO]: Training loss at epoch 42: 0.7858711183071136
[08/28/2025 23:21:02 INFO]: Training loss at epoch 63: 0.9822254180908203
[08/28/2025 23:26:16 INFO]: Training loss at epoch 64: 1.1664958000183105
[08/28/2025 23:28:24 INFO]: Training loss at epoch 43: 0.5688599050045013
[08/28/2025 23:31:32 INFO]: Training loss at epoch 65: 0.8355723023414612
[08/28/2025 23:36:56 INFO]: Training loss at epoch 66: 1.0327215194702148
[08/28/2025 23:41:01 INFO]: Training loss at epoch 44: 0.7339083552360535
[08/28/2025 23:42:09 INFO]: Training loss at epoch 67: 1.2258903682231903
[08/28/2025 23:47:34 INFO]: Training loss at epoch 68: 0.9443638026714325
[08/28/2025 23:52:53 INFO]: Training loss at epoch 69: 0.7378984987735748
[08/28/2025 23:53:27 INFO]: Training loss at epoch 45: 0.7005762755870819
[08/28/2025 23:54:40 INFO]: Training stats: {
    "score": -0.9345287769199958,
    "rmse": 0.9345287769199958
}
[08/28/2025 23:54:40 INFO]: Val stats: {
    "score": -0.693209387708018,
    "rmse": 0.693209387708018
}
[08/28/2025 23:54:40 INFO]: Test stats: {
    "score": -0.932695984835977,
    "rmse": 0.932695984835977
}
[08/29/2025 00:00:05 INFO]: Training loss at epoch 70: 1.1611919105052948
[08/29/2025 00:05:23 INFO]: Training loss at epoch 71: 0.7515254616737366
[08/29/2025 00:06:02 INFO]: Training loss at epoch 46: 0.8713337182998657
[08/29/2025 00:10:42 INFO]: Training loss at epoch 72: 0.9460961818695068
[08/29/2025 00:16:07 INFO]: Training loss at epoch 73: 0.8288077414035797
[08/29/2025 00:18:33 INFO]: Training loss at epoch 47: 0.6446118056774139
[08/29/2025 00:21:23 INFO]: Training loss at epoch 74: 0.776042252779007
[08/29/2025 00:26:47 INFO]: Training loss at epoch 75: 0.8182387948036194
[08/29/2025 00:31:07 INFO]: Training loss at epoch 48: 0.649854451417923
[08/29/2025 00:32:10 INFO]: Training loss at epoch 76: 0.9580697417259216
[08/29/2025 00:32:49 INFO]: Running Final Evaluation...
[08/29/2025 00:34:52 INFO]: Training accuracy: {
    "score": -0.9700219315850599,
    "rmse": 0.9700219315850599
}
[08/29/2025 00:34:52 INFO]: Val accuracy: {
    "score": -0.6643142443457373,
    "rmse": 0.6643142443457373
}
[08/29/2025 00:34:52 INFO]: Test accuracy: {
    "score": -0.8984164490104811,
    "rmse": 0.8984164490104811
}
[08/29/2025 00:34:52 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_174",
    "best_epoch": 45,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8984164490104811,
        "rmse": 0.8984164490104811
    },
    "train_stats": {
        "score": -0.9700219315850599,
        "rmse": 0.9700219315850599
    },
    "val_stats": {
        "score": -0.6643142443457373,
        "rmse": 0.6643142443457373
    }
}
[08/29/2025 00:34:52 INFO]: Procewss finished for trial blotchy-Amado_trial_174
[08/29/2025 00:43:01 INFO]: Training loss at epoch 49: 0.6627726256847382
[08/29/2025 00:47:07 INFO]: Training stats: {
    "score": -0.780725259677723,
    "rmse": 0.780725259677723
}
[08/29/2025 00:47:07 INFO]: Val stats: {
    "score": -0.7888679702950967,
    "rmse": 0.7888679702950967
}
[08/29/2025 00:47:07 INFO]: Test stats: {
    "score": -1.0099462007380144,
    "rmse": 1.0099462007380144
}
[08/29/2025 00:59:04 INFO]: Training loss at epoch 50: 0.6816088855266571
[08/29/2025 01:11:01 INFO]: Training loss at epoch 51: 0.5766029953956604
[08/29/2025 01:22:57 INFO]: Training loss at epoch 52: 0.6413576900959015
[08/29/2025 01:24:23 INFO]: Running Final Evaluation...
[08/29/2025 01:28:56 INFO]: Training accuracy: {
    "score": -0.9412275829080049,
    "rmse": 0.9412275829080049
}
[08/29/2025 01:28:56 INFO]: Val accuracy: {
    "score": -0.6679736175574991,
    "rmse": 0.6679736175574991
}
[08/29/2025 01:28:56 INFO]: Test accuracy: {
    "score": -0.877485032191847,
    "rmse": 0.877485032191847
}
[08/29/2025 01:28:56 INFO]: {
    "dataset": "ic_upstream4_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "blotchy-Amado_trial_156",
    "best_epoch": 21,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.877485032191847,
        "rmse": 0.877485032191847
    },
    "train_stats": {
        "score": -0.9412275829080049,
        "rmse": 0.9412275829080049
    },
    "val_stats": {
        "score": -0.6679736175574991,
        "rmse": 0.6679736175574991
    }
}
[08/29/2025 01:28:56 INFO]: Procewss finished for trial blotchy-Amado_trial_156
