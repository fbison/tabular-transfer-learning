Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: hydra-core==1.1.1 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (1.1.1)
Requirement already satisfied: icecream~=2.1.1 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (2.1.5)
Requirement already satisfied: matplotlib~=3.5.0 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (3.5.3)
Requirement already satisfied: numpy~=1.21.4 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (1.21.6)
Requirement already satisfied: omegaconf~=2.1.1 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (2.1.2)
Requirement already satisfied: pandas~=2.0.3 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (2.0.3)
Requirement already satisfied: Pillow==8.2.0 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (8.2.0)
Requirement already satisfied: scipy==1.7.0 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (1.7.0)
Requirement already satisfied: seaborn~=0.11.2 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (0.11.2)
Requirement already satisfied: svglib==1.1.0 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (1.1.0)
Requirement already satisfied: tensorboard==2.2.2 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (2.2.2)
Requirement already satisfied: tensorboard-plugin-wit==1.8.0 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 12)) (1.8.0)
Requirement already satisfied: torch~=2.4.1 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 13)) (2.4.1)
Requirement already satisfied: torchvision==0.19.1 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 14)) (0.19.1)
Requirement already satisfied: tqdm~=4.62.3 in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 15)) (4.62.3)
Requirement already satisfied: openml in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 16)) (0.15.1)
Requirement already satisfied: optuna in /temporario2/14570653/.local/lib/python3.9/site-packages (from -r requirements.txt (line 17)) (4.4.0)
Loading dataset: ic_upstream2 for task: regression at stage: pretrain
Target columns: 63
[08/12/2025 16:31:35 INFO]: Building Dataset
[08/12/2025 16:31:35 INFO]: pre normalizer.fit

[08/12/2025 16:31:35 INFO]: pos normalizer.fit

Normalizer saved to ../../../data/ic_upstream2/normalizer.pkl
dataser shape: 282
x shape: torch.Size([282, 100]), torch.Size([282, 0]), y shape: torch.Size([282, 1])
[08/12/2025 16:31:37 INFO]: Task: regression, Dataset: ic_upstream2, n_numerical: 100, n_categorical: 0, n_classes: 1, n_train_samples: 282, n_val_samples: 66, n_test_samples: 87
Train set features: torch.Size([282, 100]), 
Running trial 1/180
Running trial 1/180
Running trial 1/180
Running trial 1/180
Running trial 1/180
Running trial 1/180
Running trial 1/180
Running trial 1/180
Running trial 1/180
Running trial 1/180
[08/12/2025 16:31:50 INFO]: 
_________________________________________________

[08/12/2025 16:31:50 INFO]: 
_________________________________________________

[08/12/2025 16:31:50 INFO]: 
_________________________________________________

[08/12/2025 16:31:50 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:31:50 INFO]: 
_________________________________________________

[08/12/2025 16:31:50 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:31:50 INFO]: 
_________________________________________________

[08/12/2025 16:31:50 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:31:50 INFO]: 
_________________________________________________

[08/12/2025 16:31:51 INFO]: 
_________________________________________________

[08/12/2025 16:31:51 INFO]: 
_________________________________________________

[08/12/2025 16:31:51 INFO]: 
_________________________________________________

[08/12/2025 16:31:51 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:31:51 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:31:51 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 0.9245243684477294
  attention_dropout: 0.0862053007683779
  ffn_dropout: 0.0862053007683779
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.3789717413129504e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unforced-Rulon

[08/12/2025 16:31:51 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 1.3319819948952922
  attention_dropout: 0.010013845654746112
  ffn_dropout: 0.010013845654746112
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0002567752539752897
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: -1
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unforced-Rulon

[08/12/2025 16:31:51 INFO]: 
_________________________________________________

[08/12/2025 16:31:51 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:31:51 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:31:51 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 1.4385528778624406
  attention_dropout: 0.36743552563799553
  ffn_dropout: 0.36743552563799553
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00015248333260109957
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unforced-Rulon

[08/12/2025 16:31:51 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:31:51 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:31:51 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 2.1982622771641522
  attention_dropout: 0.3159488687169244
  ffn_dropout: 0.3159488687169244
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00012893339173688175
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unforced-Rulon

[08/12/2025 16:31:51 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 2.1268114214699194
  attention_dropout: 0.04534269052100304
  ffn_dropout: 0.04534269052100304
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0008432705961638474
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unforced-Rulon

[08/12/2025 16:31:51 INFO]: train_net_for_optune.py main() running.
[08/12/2025 16:31:51 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 2.550511424520133
  attention_dropout: 0.382955390962517
  ffn_dropout: 0.382955390962517
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00019323685933336145
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unforced-Rulon

[08/12/2025 16:31:51 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 12
  d_ffn_factor: 0.7515143465354147
  attention_dropout: 0.027179623953505427
  ffn_dropout: 0.027179623953505427
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.000279333299572177
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unforced-Rulon

[08/12/2025 16:31:51 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 4
  d_ffn_factor: 1.4603021723163128
  attention_dropout: 0.38836098754651377
  ffn_dropout: 0.38836098754651377
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.2863641441087446e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unforced-Rulon

[08/12/2025 16:31:51 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 1.4720777899620607
  attention_dropout: 0.1468524134062773
  ffn_dropout: 0.1468524134062773
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0002636210080291049
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unforced-Rulon

[08/12/2025 16:31:51 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 0.697669239482776
  attention_dropout: 0.04764511346879208
  ffn_dropout: 0.04764511346879208
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream2
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream2/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0003613241327333436
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unforced-Rulon

[08/12/2025 16:31:51 INFO]: This ft_transformer has 0.963 million parameters.
[08/12/2025 16:31:51 INFO]: Training will start at epoch 0.
[08/12/2025 16:31:52 INFO]: This ft_transformer has 0.127 million parameters.
[08/12/2025 16:31:52 INFO]: Training will start at epoch 0.
[08/12/2025 16:31:52 INFO]: This ft_transformer has 4.149 million parameters.
[08/12/2025 16:31:52 INFO]: Training will start at epoch 0.
[08/12/2025 16:31:52 INFO]: This ft_transformer has 3.807 million parameters.
[08/12/2025 16:31:52 INFO]: Training will start at epoch 0.
[08/12/2025 16:31:52 INFO]: This ft_transformer has 4.966 million parameters.
[08/12/2025 16:31:52 INFO]: Training will start at epoch 0.
[08/12/2025 16:31:52 INFO]: This ft_transformer has 4.508 million parameters.
[08/12/2025 16:31:52 INFO]: Training will start at epoch 0.
[08/12/2025 16:31:53 INFO]: This ft_transformer has 14.113 million parameters.
[08/12/2025 16:31:53 INFO]: Training will start at epoch 0.
[08/12/2025 16:31:53 INFO]: This ft_transformer has 0.365 million parameters.
[08/12/2025 16:31:53 INFO]: Training will start at epoch 0.
[08/12/2025 16:31:53 INFO]: This ft_transformer has 5.487 million parameters.
[08/12/2025 16:31:53 INFO]: Training will start at epoch 0.
[08/12/2025 16:31:53 INFO]: This ft_transformer has 5.579 million parameters.
[08/12/2025 16:31:53 INFO]: Training will start at epoch 0.
[08/12/2025 16:32:07 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:32:07 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:32:07 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:32:07 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:32:07 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:32:07 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:32:07 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:32:07 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:32:07 INFO]: ==> Starting training for 200 epochs...
[08/12/2025 16:32:07 INFO]: ==> Starting training for 200 epochs...
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:32:18 INFO]: Training loss at epoch 0: 1.180643916130066
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:32:20 INFO]: Training loss at epoch 0: 1.0728800296783447
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:32:20 INFO]: New best epoch, val score: -0.9389163099350574
[08/12/2025 16:32:20 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:32:21 INFO]: New best epoch, val score: -0.9374619594700461
[08/12/2025 16:32:21 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:32:30 INFO]: Training loss at epoch 1: 1.2016328573226929
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:32:31 INFO]: New best epoch, val score: -0.9374567907681488
[08/12/2025 16:32:32 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:32:32 INFO]: Training loss at epoch 1: 0.8891406953334808
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:32:34 INFO]: New best epoch, val score: -0.9350101885276062
[08/12/2025 16:32:34 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:32:39 INFO]: Training loss at epoch 0: 1.1393577456474304
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:32:41 INFO]: Training loss at epoch 2: 1.035634458065033
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:32:43 INFO]: New best epoch, val score: -1.1378318945374177
[08/12/2025 16:32:43 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:32:45 INFO]: Training loss at epoch 2: 1.2146368026733398
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:32:46 INFO]: New best epoch, val score: -0.9337627030132329
[08/12/2025 16:32:46 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:32:52 INFO]: Training loss at epoch 3: 0.9682084023952484
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:32:57 INFO]: Training loss at epoch 3: 0.8216455578804016
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:33:01 INFO]: Training loss at epoch 0: 1.2268367409706116
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:33:03 INFO]: Training loss at epoch 4: 0.9321616291999817
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:33:06 INFO]: Training loss at epoch 0: 0.9859527349472046
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:33:08 INFO]: New best epoch, val score: -1.085672294155995
[08/12/2025 16:33:08 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:33:10 INFO]: Training loss at epoch 4: 0.9318452179431915
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:33:13 INFO]: New best epoch, val score: -0.955971517914976
[08/12/2025 16:33:13 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:33:14 INFO]: Training loss at epoch 1: 1.06974196434021
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:33:15 INFO]: Training loss at epoch 5: 0.9781631827354431
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:33:18 INFO]: New best epoch, val score: -0.9332575031436386
[08/12/2025 16:33:18 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:33:22 INFO]: Training loss at epoch 5: 1.0906398296356201
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:33:26 INFO]: Training loss at epoch 6: 1.0530492663383484
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:33:30 INFO]: Training loss at epoch 0: 0.867180347442627
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:33:32 INFO]: Training loss at epoch 0: 0.9022911190986633
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:33:34 INFO]: Training loss at epoch 6: 1.1776219010353088
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:33:37 INFO]: Training loss at epoch 7: 1.0720319151878357
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:33:41 INFO]: Training loss at epoch 0: 2.8198312520980835
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:33:41 INFO]: New best epoch, val score: -0.9759390098369645
[08/12/2025 16:33:41 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:33:43 INFO]: New best epoch, val score: -0.966242756524977
[08/12/2025 16:33:43 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:33:47 INFO]: Training loss at epoch 7: 0.9256742000579834
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:33:47 INFO]: Training loss at epoch 8: 0.8138473927974701
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:33:48 INFO]: Training loss at epoch 2: 1.2741121649742126
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:33:53 INFO]: Training loss at epoch 0: 1.2346105575561523
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:33:54 INFO]: New best epoch, val score: -1.7990767596979589
[08/12/2025 16:33:54 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:33:59 INFO]: Training loss at epoch 9: 0.9086759090423584
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:33:59 INFO]: Training loss at epoch 8: 1.0883243680000305
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:34:00 INFO]: Training loss at epoch 1: 1.6278466582298279
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:34:03 INFO]: Training stats: {
    "score": -0.9945640826252798,
    "rmse": 0.9945640826252798
}
[08/12/2025 16:34:03 INFO]: Val stats: {
    "score": -0.9621424954112713,
    "rmse": 0.9621424954112713
}
[08/12/2025 16:34:03 INFO]: Test stats: {
    "score": -0.923494770607595,
    "rmse": 0.923494770607595
}
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:34:07 INFO]: New best epoch, val score: -0.9852573376778101
[08/12/2025 16:34:07 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:34:11 INFO]: Training loss at epoch 1: 1.0602419972419739
[08/12/2025 16:34:11 INFO]: Training loss at epoch 9: 1.213792860507965
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:34:14 INFO]: Training loss at epoch 10: 0.8966576457023621
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:34:16 INFO]: Training stats: {
    "score": -0.9986493910921717,
    "rmse": 0.9986493910921717
}
[08/12/2025 16:34:16 INFO]: Val stats: {
    "score": -0.936939209411263,
    "rmse": 0.936939209411263
}
[08/12/2025 16:34:16 INFO]: Test stats: {
    "score": -0.9159704332083799,
    "rmse": 0.9159704332083799
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:34:22 INFO]: Training loss at epoch 3: 1.1889048218727112
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:34:25 INFO]: Training loss at epoch 11: 0.8920745253562927
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:34:26 INFO]: New best epoch, val score: -0.9284399372181775
[08/12/2025 16:34:26 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:34:28 INFO]: Training loss at epoch 10: 1.132135808467865
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:34:36 INFO]: Training loss at epoch 12: 0.9053911864757538
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:34:40 INFO]: Training loss at epoch 11: 1.0319172739982605
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:34:47 INFO]: Training loss at epoch 13: 0.8728611767292023
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:34:52 INFO]: Training loss at epoch 12: 0.9291626811027527
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:34:56 INFO]: Training loss at epoch 4: 0.9503957331180573
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:34:58 INFO]: Training loss at epoch 14: 1.0784147679805756
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:35:00 INFO]: Training loss at epoch 2: 1.8511688709259033
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:35:04 INFO]: Training loss at epoch 1: 3.8685359358787537
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:35:05 INFO]: Training loss at epoch 13: 1.1923288702964783
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:35:07 INFO]: New best epoch, val score: -0.9604466970994309
[08/12/2025 16:35:07 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:35:08 INFO]: Training loss at epoch 1: 0.9433024227619171
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:35:09 INFO]: Training loss at epoch 15: 1.06547811627388
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:35:16 INFO]: Training loss at epoch 2: 1.1352795362472534
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:35:17 INFO]: Training loss at epoch 14: 1.1828306019306183
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:35:20 INFO]: Training loss at epoch 16: 0.9354137778282166
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:35:26 INFO]: Training loss at epoch 0: 1.234076738357544
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
[08/12/2025 16:35:27 INFO]: Training loss at epoch 1: 3.5241193771362305
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:35:30 INFO]: Training loss at epoch 15: 0.8995251953601837
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:35:30 INFO]: Training loss at epoch 5: 0.9698834419250488
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:35:31 INFO]: Training loss at epoch 17: 1.0648885369300842
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:35:39 INFO]: New best epoch, val score: -0.942972899721497
[08/12/2025 16:35:39 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:35:42 INFO]: Training loss at epoch 18: 0.8989636898040771
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:35:42 INFO]: Training loss at epoch 16: 0.8503285348415375
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:35:52 INFO]: Training loss at epoch 1: 1.936318814754486
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:35:53 INFO]: Training loss at epoch 19: 1.12895929813385
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:35:53 INFO]: New best epoch, val score: -0.9703455328225115
[08/12/2025 16:35:53 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:35:55 INFO]: Training loss at epoch 17: 1.1197651028633118
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:35:58 INFO]: Training stats: {
    "score": -0.990011828641507,
    "rmse": 0.990011828641507
}
[08/12/2025 16:35:58 INFO]: Val stats: {
    "score": -0.9517284036415297,
    "rmse": 0.9517284036415297
}
[08/12/2025 16:35:58 INFO]: Test stats: {
    "score": -0.9167179444103061,
    "rmse": 0.9167179444103061
}
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:36:01 INFO]: Training loss at epoch 3: 1.0796645283699036
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:36:04 INFO]: Training loss at epoch 6: 0.9431798458099365
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:36:08 INFO]: Training loss at epoch 18: 0.923751950263977
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:36:09 INFO]: Training loss at epoch 20: 0.9473313391208649
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:36:20 INFO]: Training loss at epoch 19: 1.1745165288448334
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:36:20 INFO]: Training loss at epoch 21: 1.0361360311508179
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:36:22 INFO]: Training loss at epoch 3: 1.179242193698883
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:36:24 INFO]: Training stats: {
    "score": -0.995042404463138,
    "rmse": 0.995042404463138
}
[08/12/2025 16:36:24 INFO]: Val stats: {
    "score": -0.9344772756501675,
    "rmse": 0.9344772756501675
}
[08/12/2025 16:36:24 INFO]: Test stats: {
    "score": -0.9132609817355993,
    "rmse": 0.9132609817355993
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:36:32 INFO]: Training loss at epoch 22: 0.9509122669696808
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:36:36 INFO]: Training loss at epoch 20: 1.3124882280826569
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:36:38 INFO]: Training loss at epoch 2: 1.6104243397712708
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:36:38 INFO]: Training loss at epoch 7: 0.9874787032604218
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:36:43 INFO]: Training loss at epoch 23: 0.8532167971134186
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:36:43 INFO]: Training loss at epoch 2: 2.18938809633255
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:36:49 INFO]: Training loss at epoch 21: 0.8956553041934967
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:36:50 INFO]: New best epoch, val score: -0.9325812069091695
[08/12/2025 16:36:50 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:36:54 INFO]: Training loss at epoch 24: 1.0015000700950623
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:36:55 INFO]: New best epoch, val score: -0.9000251277185987
[08/12/2025 16:36:55 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:37:00 INFO]: Training loss at epoch 4: 1.8404417037963867
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:37:01 INFO]: Training loss at epoch 22: 1.3028314709663391
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:37:03 INFO]: New best epoch, val score: -0.9315651335280069
[08/12/2025 16:37:03 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:37:05 INFO]: Training loss at epoch 25: 0.8258151113986969
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:37:07 INFO]: New best epoch, val score: -0.9369833428101185
[08/12/2025 16:37:07 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:37:12 INFO]: Training loss at epoch 8: 0.9396492838859558
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:37:13 INFO]: Training loss at epoch 2: 1.2471871972084045
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:37:14 INFO]: Training loss at epoch 23: 0.901999443769455
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:37:16 INFO]: New best epoch, val score: -0.9313747410388024
[08/12/2025 16:37:16 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:37:16 INFO]: Training loss at epoch 26: 0.8739686608314514
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:37:27 INFO]: Training loss at epoch 24: 1.0478023290634155
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:37:27 INFO]: Training loss at epoch 4: 1.0782317221164703
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:37:27 INFO]: Training loss at epoch 27: 0.961528480052948
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:37:28 INFO]: New best epoch, val score: -0.9311481830363442
[08/12/2025 16:37:28 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:37:38 INFO]: Training loss at epoch 28: 0.8554625511169434
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:37:39 INFO]: Training loss at epoch 25: 1.246613085269928
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:37:41 INFO]: New best epoch, val score: -0.9310061008961271
[08/12/2025 16:37:41 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:37:46 INFO]: Training loss at epoch 9: 1.1420117616653442
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:37:49 INFO]: Training loss at epoch 29: 1.1382985413074493
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:37:51 INFO]: Training loss at epoch 2: 1.3020607829093933
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:37:52 INFO]: Training loss at epoch 26: 0.7502856552600861
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:37:53 INFO]: New best epoch, val score: -0.9308094478704818
[08/12/2025 16:37:53 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:37:53 INFO]: Training stats: {
    "score": -0.9855257154642523,
    "rmse": 0.9855257154642523
}
[08/12/2025 16:37:53 INFO]: Val stats: {
    "score": -0.9459958854203175,
    "rmse": 0.9459958854203175
}
[08/12/2025 16:37:53 INFO]: Test stats: {
    "score": -0.9122284946263394,
    "rmse": 0.9122284946263394
}
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:37:58 INFO]: Training stats: {
    "score": -0.9972700825512482,
    "rmse": 0.9972700825512482
}
[08/12/2025 16:37:58 INFO]: Val stats: {
    "score": -0.9809678580607425,
    "rmse": 0.9809678580607425
}
[08/12/2025 16:37:58 INFO]: Test stats: {
    "score": -0.9396070423258822,
    "rmse": 0.9396070423258822
}
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:38:00 INFO]: Training loss at epoch 5: 0.938549131155014
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:38:04 INFO]: Training loss at epoch 27: 1.0889694094657898
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:38:04 INFO]: Training loss at epoch 30: 1.00167316198349
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:38:05 INFO]: New best epoch, val score: -0.9499502518891141
[08/12/2025 16:38:05 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:38:11 INFO]: Training loss at epoch 3: 1.2610650658607483
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:38:15 INFO]: Training loss at epoch 31: 0.8871491551399231
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:38:17 INFO]: Training loss at epoch 28: 1.1539035439491272
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:38:18 INFO]: Training loss at epoch 3: 1.455947995185852
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:38:22 INFO]: New best epoch, val score: -0.9285390129464018
[08/12/2025 16:38:22 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:38:26 INFO]: Training loss at epoch 32: 1.2105601131916046
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:38:28 INFO]: Running Final Evaluation...
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:38:29 INFO]: Training loss at epoch 29: 0.9908465445041656
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[W 2025-08-12 16:38:29,663] Trial 4 failed with parameters: {'embedding_head_pair': (64, 8), 'n_layers': 4, 'd_ffn_factor': 0.9245243684477294, 'attention_dropout': 0.0862053007683779, 'activation': 'reglu', 'lr': 3.3789717413129504e-05} because of the following error: RuntimeError('Error(s) in loading state_dict for FTTransformer:\n\tUnexpected key(s) in state_dict: "layers.4.attention.W_q.weight", "layers.4.attention.W_q.bias", "layers.4.attention.W_k.weight", "layers.4.attention.W_k.bias", "layers.4.attention.W_v.weight", "layers.4.attention.W_v.bias", "layers.4.attention.W_out.weight", "layers.4.attention.W_out.bias", "layers.4.linear0.weight", "layers.4.linear0.bias", "layers.4.linear1.weight", "layers.4.linear1.bias", "layers.4.norm1.weight", "layers.4.norm1.bias", "layers.4.norm0.weight", "layers.4.norm0.bias", "layers.5.attention.W_q.weight", "layers.5.attention.W_q.bias", "layers.5.attention.W_k.weight", "layers.5.attention.W_k.bias", "layers.5.attention.W_v.weight", "layers.5.attention.W_v.bias", "layers.5.attention.W_out.weight", "layers.5.attention.W_out.bias", "layers.5.linear0.weight", "layers.5.linear0.bias", "layers.5.linear1.weight", "layers.5.linear1.bias", "layers.5.norm1.weight", "layers.5.norm1.bias", "layers.5.norm0.weight", "layers.5.norm0.bias", "layers.6.attention.W_q.weight", "layers.6.attention.W_q.bias", "layers.6.attention.W_k.weight", "layers.6.attention.W_k.bias", "layers.6.attention.W_v.weight", "layers.6.attention.W_v.bias", "layers.6.attention.W_out.weight", "layers.6.attention.W_out.bias", "layers.6.linear0.weight", "layers.6.linear0.bias", "layers.6.linear1.weight", "layers.6.linear1.bias", "layers.6.norm1.weight", "layers.6.norm1.bias", "layers.6.norm0.weight", "layers.6.norm0.bias", "layers.7.attention.W_q.weight", "layers.7.attention.W_q.bias", "layers.7.attention.W_k.weight", "layers.7.attention.W_k.bias", "layers.7.attention.W_v.weight", "layers.7.attention.W_v.bias", "layers.7.attention.W_out.weight", "layers.7.attention.W_out.bias", "layers.7.linear0.weight", "layers.7.linear0.bias", "layers.7.linear1.weight", "layers.7.linear1.bias", "layers.7.norm1.weight", "layers.7.norm1.bias", "layers.7.norm0.weight", "layers.7.norm0.bias". \n\tsize mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 320]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 320]) from checkpoint, the shape in current model is torch.Size([100, 64]).\n\tsize mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([223, 320]) from checkpoint, the shape in current model is torch.Size([118, 64]).\n\tsize mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([223]) from checkpoint, the shape in current model is torch.Size([118]).\n\tsize mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([320, 223]) from checkpoint, the shape in current model is torch.Size([64, 59]).\n\tsize mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([223, 320]) from checkpoint, the shape in current model is torch.Size([118, 64]).\n\tsize mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([223]) from checkpoint, the shape in current model is torch.Size([118]).\n\tsize mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([320, 223]) from checkpoint, the shape in current model is torch.Size([64, 59]).\n\tsize mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.2.attention.W_q.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.2.attention.W_q.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.2.attention.W_k.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.2.attention.W_k.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.2.attention.W_v.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.2.attention.W_v.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.2.attention.W_out.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.2.attention.W_out.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.2.linear0.weight: copying a param with shape torch.Size([223, 320]) from checkpoint, the shape in current model is torch.Size([118, 64]).\n\tsize mismatch for layers.2.linear0.bias: copying a param with shape torch.Size([223]) from checkpoint, the shape in current model is torch.Size([118]).\n\tsize mismatch for layers.2.linear1.weight: copying a param with shape torch.Size([320, 223]) from checkpoint, the shape in current model is torch.Size([64, 59]).\n\tsize mismatch for layers.2.linear1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.2.norm1.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.2.norm1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.2.norm0.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.2.norm0.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.3.attention.W_q.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.3.attention.W_q.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.3.attention.W_k.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.3.attention.W_k.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.3.attention.W_v.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.3.attention.W_v.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.3.attention.W_out.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.3.attention.W_out.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.3.linear0.weight: copying a param with shape torch.Size([223, 320]) from checkpoint, the shape in current model is torch.Size([118, 64]).\n\tsize mismatch for layers.3.linear0.bias: copying a param with shape torch.Size([223]) from checkpoint, the shape in current model is torch.Size([118]).\n\tsize mismatch for layers.3.linear1.weight: copying a param with shape torch.Size([320, 223]) from checkpoint, the shape in current model is torch.Size([64, 59]).\n\tsize mismatch for layers.3.linear1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.3.norm1.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.3.norm1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.3.norm0.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.3.norm0.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for last_normalization.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for last_normalization.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for head.weight: copying a param with shape torch.Size([1, 320]) from checkpoint, the shape in current model is torch.Size([1, 64]).').
Traceback (most recent call last):
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 145, in <lambda>
    func = lambda trial: objective(trial, cfg, trial_stats, trial_counter, n_optuna_trials,
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 104, in objective
    stats = train_net_for_optuna.main(config, loaders, unique_categories, n_numerical, n_classes)
  File "/temporario2/14570653/tabular-transfer-learning/train_net_for_optuna.py", line 127, in main
    net.load_state_dict(torch.load(checkpoint_path)["net"])
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2215, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for FTTransformer:
	Unexpected key(s) in state_dict: "layers.4.attention.W_q.weight", "layers.4.attention.W_q.bias", "layers.4.attention.W_k.weight", "layers.4.attention.W_k.bias", "layers.4.attention.W_v.weight", "layers.4.attention.W_v.bias", "layers.4.attention.W_out.weight", "layers.4.attention.W_out.bias", "layers.4.linear0.weight", "layers.4.linear0.bias", "layers.4.linear1.weight", "layers.4.linear1.bias", "layers.4.norm1.weight", "layers.4.norm1.bias", "layers.4.norm0.weight", "layers.4.norm0.bias", "layers.5.attention.W_q.weight", "layers.5.attention.W_q.bias", "layers.5.attention.W_k.weight", "layers.5.attention.W_k.bias", "layers.5.attention.W_v.weight", "layers.5.attention.W_v.bias", "layers.5.attention.W_out.weight", "layers.5.attention.W_out.bias", "layers.5.linear0.weight", "layers.5.linear0.bias", "layers.5.linear1.weight", "layers.5.linear1.bias", "layers.5.norm1.weight", "layers.5.norm1.bias", "layers.5.norm0.weight", "layers.5.norm0.bias", "layers.6.attention.W_q.weight", "layers.6.attention.W_q.bias", "layers.6.attention.W_k.weight", "layers.6.attention.W_k.bias", "layers.6.attention.W_v.weight", "layers.6.attention.W_v.bias", "layers.6.attention.W_out.weight", "layers.6.attention.W_out.bias", "layers.6.linear0.weight", "layers.6.linear0.bias", "layers.6.linear1.weight", "layers.6.linear1.bias", "layers.6.norm1.weight", "layers.6.norm1.bias", "layers.6.norm0.weight", "layers.6.norm0.bias", "layers.7.attention.W_q.weight", "layers.7.attention.W_q.bias", "layers.7.attention.W_k.weight", "layers.7.attention.W_k.bias", "layers.7.attention.W_v.weight", "layers.7.attention.W_v.bias", "layers.7.attention.W_out.weight", "layers.7.attention.W_out.bias", "layers.7.linear0.weight", "layers.7.linear0.bias", "layers.7.linear1.weight", "layers.7.linear1.bias", "layers.7.norm1.weight", "layers.7.norm1.bias", "layers.7.norm0.weight", "layers.7.norm0.bias". 
	size mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 320]) from checkpoint, the shape in current model is torch.Size([101, 64]).
	size mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 320]) from checkpoint, the shape in current model is torch.Size([100, 64]).
	size mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([223, 320]) from checkpoint, the shape in current model is torch.Size([118, 64]).
	size mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([223]) from checkpoint, the shape in current model is torch.Size([118]).
	size mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([320, 223]) from checkpoint, the shape in current model is torch.Size([64, 59]).
	size mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([223, 320]) from checkpoint, the shape in current model is torch.Size([118, 64]).
	size mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([223]) from checkpoint, the shape in current model is torch.Size([118]).
	size mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([320, 223]) from checkpoint, the shape in current model is torch.Size([64, 59]).
	size mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.2.attention.W_q.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.2.attention.W_q.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.2.attention.W_k.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.2.attention.W_k.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.2.attention.W_v.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.2.attention.W_v.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.2.attention.W_out.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.2.attention.W_out.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.2.linear0.weight: copying a param with shape torch.Size([223, 320]) from checkpoint, the shape in current model is torch.Size([118, 64]).
	size mismatch for layers.2.linear0.bias: copying a param with shape torch.Size([223]) from checkpoint, the shape in current model is torch.Size([118]).
	size mismatch for layers.2.linear1.weight: copying a param with shape torch.Size([320, 223]) from checkpoint, the shape in current model is torch.Size([64, 59]).
	size mismatch for layers.2.linear1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.2.norm1.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.2.norm1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.2.norm0.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.2.norm0.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.3.attention.W_q.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.3.attention.W_q.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.3.attention.W_k.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.3.attention.W_k.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.3.attention.W_v.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.3.attention.W_v.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.3.attention.W_out.weight: copying a param with shape torch.Size([320, 320]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.3.attention.W_out.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.3.linear0.weight: copying a param with shape torch.Size([223, 320]) from checkpoint, the shape in current model is torch.Size([118, 64]).
	size mismatch for layers.3.linear0.bias: copying a param with shape torch.Size([223]) from checkpoint, the shape in current model is torch.Size([118]).
	size mismatch for layers.3.linear1.weight: copying a param with shape torch.Size([320, 223]) from checkpoint, the shape in current model is torch.Size([64, 59]).
	size mismatch for layers.3.linear1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.3.norm1.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.3.norm1.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.3.norm0.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.3.norm0.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for last_normalization.weight: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for last_normalization.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for head.weight: copying a param with shape torch.Size([1, 320]) from checkpoint, the shape in current model is torch.Size([1, 64]).
[W 2025-08-12 16:38:29,717] Trial 4 failed with value None.
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:38:31 INFO]: Training loss at epoch 5: 1.4002474546432495
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:38:32 INFO]: Training loss at epoch 10: 0.8995456993579865
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:38:33 INFO]: Training stats: {
    "score": -0.9934635568029917,
    "rmse": 0.9934635568029917
}
[08/12/2025 16:38:33 INFO]: Val stats: {
    "score": -0.9320165982477427,
    "rmse": 0.9320165982477427
}
[08/12/2025 16:38:33 INFO]: Test stats: {
    "score": -0.9113934421162109,
    "rmse": 0.9113934421162109
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:38:45 INFO]: Training loss at epoch 30: 1.125281810760498
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:38:57 INFO]: Training loss at epoch 3: 1.1819691061973572
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:38:57 INFO]: Training loss at epoch 31: 1.0843186676502228
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:39:00 INFO]: Training loss at epoch 6: 0.928886741399765
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:39:05 INFO]: Training loss at epoch 11: 0.9323844909667969
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:39:09 INFO]: Training loss at epoch 32: 1.2716382443904877
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:39:09 INFO]: New best epoch, val score: -0.9340638709084087
[08/12/2025 16:39:09 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:39:12 INFO]: Training loss at epoch 1: 0.8930759131908417
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:39:21 INFO]: Training loss at epoch 33: 1.140950858592987
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:39:33 INFO]: Training loss at epoch 34: 0.9055372178554535
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:39:36 INFO]: Training loss at epoch 6: 1.0308598577976227
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:39:38 INFO]: Training loss at epoch 12: 1.0129494071006775
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:39:41 INFO]: Training loss at epoch 4: 1.150870144367218
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:39:42 INFO]: New best epoch, val score: -0.9244754731385392
[08/12/2025 16:39:42 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:39:44 INFO]: Training loss at epoch 35: 0.9542210102081299
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:39:46 INFO]: New best epoch, val score: -0.9300562265659198
[08/12/2025 16:39:46 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:39:47 INFO]: Training loss at epoch 3: 1.1693155765533447
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:39:53 INFO]: Training loss at epoch 4: 1.3630435466766357
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:39:57 INFO]: Training loss at epoch 36: 1.0315334498882294
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:39:58 INFO]: New best epoch, val score: -0.9290288081429163
[08/12/2025 16:39:58 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:39:59 INFO]: Training loss at epoch 7: 1.0681812167167664
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:40:00 INFO]: New best epoch, val score: -0.9255445208463362
[08/12/2025 16:40:00 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:40:09 INFO]: Training loss at epoch 37: 1.0497718751430511
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:40:11 INFO]: New best epoch, val score: -0.9286301633502166
[08/12/2025 16:40:11 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:40:11 INFO]: Training loss at epoch 13: 0.9854899346828461
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:40:15 INFO]: New best epoch, val score: -0.9164464963355458
[08/12/2025 16:40:15 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:40:21 INFO]: Training loss at epoch 38: 1.2470993399620056
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:40:23 INFO]: New best epoch, val score: -0.9281648168462354
[08/12/2025 16:40:23 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:40:33 INFO]: Training loss at epoch 39: 1.040501892566681
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:40:38 INFO]: Training stats: {
    "score": -0.9926432578032678,
    "rmse": 0.9926432578032678
}
[08/12/2025 16:40:38 INFO]: Val stats: {
    "score": -0.9280651448162522,
    "rmse": 0.9280651448162522
}
[08/12/2025 16:40:38 INFO]: Test stats: {
    "score": -0.9091706991150332,
    "rmse": 0.9091706991150332
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:40:39 INFO]: New best epoch, val score: -0.9280651448162522
[08/12/2025 16:40:39 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:40:41 INFO]: Training loss at epoch 4: 1.0531439781188965
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:40:41 INFO]: Training loss at epoch 7: 1.2130756378173828
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:40:44 INFO]: Training loss at epoch 14: 1.3897766768932343
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:40:48 INFO]: New best epoch, val score: -0.9133392650218543
[08/12/2025 16:40:48 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:40:50 INFO]: Training loss at epoch 40: 1.1093437671661377
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:40:58 INFO]: Training loss at epoch 8: 0.9825313985347748
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:41:02 INFO]: Training loss at epoch 41: 1.195924460887909
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:41:13 INFO]: Training loss at epoch 5: 1.137108862400055
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:41:14 INFO]: Training loss at epoch 42: 0.9521902203559875
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:41:18 INFO]: Training loss at epoch 15: 0.9851058423519135
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:41:26 INFO]: Training loss at epoch 5: 1.1372613310813904
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:41:27 INFO]: Training loss at epoch 43: 1.0600959062576294
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:41:39 INFO]: Training loss at epoch 44: 1.140305519104004
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:41:44 INFO]: Training loss at epoch 4: 1.0553428530693054
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:41:47 INFO]: Training loss at epoch 8: 1.2627277970314026
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:41:51 INFO]: Training loss at epoch 45: 1.0896867513656616
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:41:51 INFO]: Training loss at epoch 16: 0.9284175038337708
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:41:56 INFO]: Training loss at epoch 9: 0.9454784095287323
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:42:03 INFO]: Training loss at epoch 46: 0.9588701128959656
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:42:15 INFO]: Training stats: {
    "score": -0.9943288145895242,
    "rmse": 0.9943288145895242
}
[08/12/2025 16:42:15 INFO]: Val stats: {
    "score": -0.9486141817852948,
    "rmse": 0.9486141817852948
}
[08/12/2025 16:42:15 INFO]: Test stats: {
    "score": -0.9204811366630199,
    "rmse": 0.9204811366630199
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:42:15 INFO]: Training loss at epoch 47: 0.8357490301132202
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:42:22 INFO]: Training loss at epoch 5: 1.0445640087127686
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:42:25 INFO]: Training loss at epoch 17: 1.0184892416000366
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:42:28 INFO]: Training loss at epoch 48: 1.0623002648353577
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:42:40 INFO]: Training loss at epoch 49: 1.1657278537750244
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:42:44 INFO]: Training stats: {
    "score": -0.9846149952098516,
    "rmse": 0.9846149952098516
}
[08/12/2025 16:42:44 INFO]: Val stats: {
    "score": -0.9329733881390274,
    "rmse": 0.9329733881390274
}
[08/12/2025 16:42:44 INFO]: Test stats: {
    "score": -0.9092672172788785,
    "rmse": 0.9092672172788785
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:42:45 INFO]: Training loss at epoch 6: 1.0046386420726776
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:42:51 INFO]: Training loss at epoch 2: 3.932436943054199
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
[08/12/2025 16:42:52 INFO]: Training loss at epoch 9: 0.9976358413696289
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:42:56 INFO]: Training loss at epoch 50: 0.9156706929206848
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:42:58 INFO]: Training loss at epoch 6: 0.8097520470619202
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:42:58 INFO]: Training loss at epoch 18: 0.8867502212524414
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:43:09 INFO]: Training loss at epoch 51: 1.1222988963127136
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:43:13 INFO]: Training loss at epoch 10: 0.8648311197757721
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:43:14 INFO]: Training stats: {
    "score": -1.093307512174377,
    "rmse": 1.093307512174377
}
[08/12/2025 16:43:14 INFO]: Val stats: {
    "score": -1.1598537365408348,
    "rmse": 1.1598537365408348
}
[08/12/2025 16:43:14 INFO]: Test stats: {
    "score": -1.0801907208082144,
    "rmse": 1.0801907208082144
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:43:21 INFO]: Training loss at epoch 52: 0.9980883300304413
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:43:32 INFO]: Training loss at epoch 19: 1.0519617795944214
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:43:33 INFO]: Training loss at epoch 53: 0.9795740842819214
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:43:41 INFO]: Training loss at epoch 5: 1.1038730144500732
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:43:44 INFO]: Training stats: {
    "score": -0.9564486902652638,
    "rmse": 0.9564486902652638
}
[08/12/2025 16:43:44 INFO]: Val stats: {
    "score": -0.9343117489746491,
    "rmse": 0.9343117489746491
}
[08/12/2025 16:43:44 INFO]: Test stats: {
    "score": -0.8985916330772935,
    "rmse": 0.8985916330772935
}
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:43:45 INFO]: Training loss at epoch 54: 1.103468894958496
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:43:58 INFO]: Training loss at epoch 55: 1.2477872669696808
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:44:03 INFO]: Training loss at epoch 6: 1.0833989977836609
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:44:10 INFO]: Training loss at epoch 56: 1.033439815044403
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:44:11 INFO]: Training loss at epoch 11: 1.1381380259990692
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:44:17 INFO]: Training loss at epoch 20: 0.8939667642116547
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:44:17 INFO]: Training loss at epoch 7: 1.065951406955719
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:44:19 INFO]: Training loss at epoch 10: 1.0629351735115051
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:44:23 INFO]: Training loss at epoch 57: 1.0482216477394104
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:44:26 INFO]: New best epoch, val score: -0.8991635232722774
[08/12/2025 16:44:26 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:44:30 INFO]: Training loss at epoch 7: 1.245571494102478
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:44:35 INFO]: Training loss at epoch 58: 1.068309247493744
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:44:47 INFO]: Training loss at epoch 59: 1.0925562977790833
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:44:51 INFO]: Training loss at epoch 21: 0.8608110845088959
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:44:51 INFO]: Training stats: {
    "score": -0.9801580473028444,
    "rmse": 0.9801580473028444
}
[08/12/2025 16:44:51 INFO]: Val stats: {
    "score": -0.9422945537232078,
    "rmse": 0.9422945537232078
}
[08/12/2025 16:44:51 INFO]: Test stats: {
    "score": -0.9135078859145519,
    "rmse": 0.9135078859145519
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:45:03 INFO]: Training loss at epoch 60: 1.0672058463096619
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:45:09 INFO]: Training loss at epoch 12: 1.1452054381370544
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:45:16 INFO]: Training loss at epoch 61: 1.0057713687419891
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:45:24 INFO]: Training loss at epoch 11: 1.4722163081169128
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:45:24 INFO]: Training loss at epoch 22: 0.9270166754722595
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:45:28 INFO]: Training loss at epoch 62: 1.1680352687835693
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:45:28 INFO]: New best epoch, val score: -0.9122594650663608
[08/12/2025 16:45:28 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:45:30 INFO]: New best epoch, val score: -0.9271864492679208
[08/12/2025 16:45:30 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:45:39 INFO]: Training loss at epoch 6: 1.2602369785308838
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:45:40 INFO]: Training loss at epoch 63: 1.0106698870658875
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:45:42 INFO]: New best epoch, val score: -0.9243373351666836
[08/12/2025 16:45:42 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:45:44 INFO]: Training loss at epoch 7: 1.0106124877929688
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:45:50 INFO]: Training loss at epoch 8: 1.0995301604270935
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:45:53 INFO]: Training loss at epoch 64: 0.8091136515140533
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:45:54 INFO]: New best epoch, val score: -0.9228878910690725
[08/12/2025 16:45:54 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:45:58 INFO]: Training loss at epoch 23: 0.9619277119636536
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:46:01 INFO]: New best epoch, val score: -0.9278893328316079
[08/12/2025 16:46:01 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:46:02 INFO]: New best epoch, val score: -0.9023870234808583
[08/12/2025 16:46:02 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:46:03 INFO]: Training loss at epoch 8: 1.3525118231773376
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:46:05 INFO]: Training loss at epoch 65: 0.8920566439628601
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:46:06 INFO]: Training loss at epoch 13: 0.9812560975551605
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:46:07 INFO]: New best epoch, val score: -0.9220525230492591
[08/12/2025 16:46:07 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:46:18 INFO]: Training loss at epoch 66: 1.2768924832344055
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:46:28 INFO]: Training loss at epoch 3: 1.114102691411972
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
[08/12/2025 16:46:29 INFO]: Training loss at epoch 12: 1.0569450557231903
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:46:30 INFO]: Training loss at epoch 67: 0.9761311113834381
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:46:31 INFO]: Training loss at epoch 24: 0.8664363622665405
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:46:36 INFO]: New best epoch, val score: -0.8927493535976188
[08/12/2025 16:46:36 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:46:42 INFO]: Training loss at epoch 68: 1.0975146293640137
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
[08/12/2025 16:46:55 INFO]: Training loss at epoch 69: 0.983758956193924
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:46:59 INFO]: Training stats: {
    "score": -0.9792565868531504,
    "rmse": 0.9792565868531504
}
[08/12/2025 16:46:59 INFO]: Val stats: {
    "score": -0.925720714626423,
    "rmse": 0.925720714626423
}
[08/12/2025 16:46:59 INFO]: Test stats: {
    "score": -0.9037407850478307,
    "rmse": 0.9037407850478307
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:47:04 INFO]: Training loss at epoch 14: 1.2190014123916626
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:47:05 INFO]: Training loss at epoch 25: 0.7006467878818512
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:47:09 INFO]: New best epoch, val score: -0.8788194359990873
[08/12/2025 16:47:09 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:47:11 INFO]: Training loss at epoch 70: 0.8903057873249054
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:47:22 INFO]: Training loss at epoch 9: 1.0514704585075378
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:47:26 INFO]: Training loss at epoch 8: 0.8682262599468231
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:47:29 INFO]: Training loss at epoch 71: 0.8936900496482849
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:47:34 INFO]: Training loss at epoch 13: 1.316918432712555
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:47:35 INFO]: Training loss at epoch 9: 1.0292773246765137
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:47:36 INFO]: Training loss at epoch 7: 1.170601487159729
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:47:38 INFO]: Training loss at epoch 26: 0.917193591594696
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:47:41 INFO]: Training loss at epoch 72: 1.3765519559383392
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:47:42 INFO]: New best epoch, val score: -0.8622531244808129
[08/12/2025 16:47:42 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:47:53 INFO]: Training loss at epoch 73: 1.0826263725757599
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:47:54 INFO]: Training stats: {
    "score": -1.0192984472353543,
    "rmse": 1.0192984472353543
}
[08/12/2025 16:47:54 INFO]: Val stats: {
    "score": -0.9263695183880934,
    "rmse": 0.9263695183880934
}
[08/12/2025 16:47:54 INFO]: Test stats: {
    "score": -0.9201675963623084,
    "rmse": 0.9201675963623084
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:48:05 INFO]: New best epoch, val score: -0.9263695183880934
[08/12/2025 16:48:05 INFO]: Saving model to: model_best.pth
[08/12/2025 16:48:05 INFO]: Training loss at epoch 74: 0.9682389199733734
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:48:08 INFO]: Training stats: {
    "score": -1.0131162987109887,
    "rmse": 1.0131162987109887
}
[08/12/2025 16:48:08 INFO]: Val stats: {
    "score": -0.9186845388980637,
    "rmse": 0.9186845388980637
}
[08/12/2025 16:48:08 INFO]: Test stats: {
    "score": -0.9146445831638745,
    "rmse": 0.9146445831638745
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:48:10 INFO]: Training loss at epoch 27: 0.8256314992904663
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:48:11 INFO]: Training loss at epoch 15: 0.9751309454441071
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:48:14 INFO]: New best epoch, val score: -0.8575773314044149
[08/12/2025 16:48:14 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:48:17 INFO]: Training loss at epoch 75: 0.957520455121994
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:48:29 INFO]: Training loss at epoch 76: 0.90916907787323
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:48:40 INFO]: Training loss at epoch 14: 1.110082060098648
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:48:42 INFO]: Training loss at epoch 77: 1.0996208488941193
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:48:44 INFO]: Training loss at epoch 28: 0.6954655647277832
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:48:48 INFO]: New best epoch, val score: -0.8974171721863307
[08/12/2025 16:48:48 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:48:54 INFO]: Training loss at epoch 78: 1.2174562811851501
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:49:06 INFO]: Training loss at epoch 79: 0.8686480522155762
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:49:09 INFO]: Training loss at epoch 9: 0.9561669826507568
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:49:09 INFO]: Training loss at epoch 16: 0.9197206795215607
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:49:10 INFO]: Training stats: {
    "score": -0.9750983483213238,
    "rmse": 0.9750983483213238
}
[08/12/2025 16:49:10 INFO]: Val stats: {
    "score": -0.9277299696476423,
    "rmse": 0.9277299696476423
}
[08/12/2025 16:49:10 INFO]: Test stats: {
    "score": -0.9033609420829984,
    "rmse": 0.9033609420829984
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:49:18 INFO]: Training loss at epoch 29: 0.6949540078639984
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:49:23 INFO]: Training loss at epoch 80: 1.0329121351242065
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:49:26 INFO]: Training loss at epoch 10: 0.8422890603542328
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:49:30 INFO]: Training stats: {
    "score": -0.8411109721488392,
    "rmse": 0.8411109721488392
}
[08/12/2025 16:49:30 INFO]: Val stats: {
    "score": -0.8809106503615064,
    "rmse": 0.8809106503615064
}
[08/12/2025 16:49:30 INFO]: Test stats: {
    "score": -0.8278299430431506,
    "rmse": 0.8278299430431506
}
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:49:32 INFO]: Training loss at epoch 8: 1.0393165946006775
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:49:35 INFO]: Training loss at epoch 81: 0.755823478102684
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:49:40 INFO]: Training loss at epoch 10: 0.9620326459407806
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:49:43 INFO]: Training stats: {
    "score": -1.0073495797608265,
    "rmse": 1.0073495797608265
}
[08/12/2025 16:49:43 INFO]: Val stats: {
    "score": -0.9373317420866016,
    "rmse": 0.9373317420866016
}
[08/12/2025 16:49:43 INFO]: Test stats: {
    "score": -0.9193782467905307,
    "rmse": 0.9193782467905307
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:49:46 INFO]: Training loss at epoch 15: 0.9471956789493561
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:49:47 INFO]: Training loss at epoch 82: 1.087627351284027
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:49:59 INFO]: Training loss at epoch 83: 1.0351443886756897
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:50:01 INFO]: New best epoch, val score: -0.920310791101511
[08/12/2025 16:50:01 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:50:04 INFO]: Training loss at epoch 30: 0.688641756772995
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:50:06 INFO]: Training loss at epoch 17: 1.0912935733795166
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:50:08 INFO]: Training loss at epoch 4: 1.558628112077713
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
[08/12/2025 16:50:08 INFO]: New best epoch, val score: -0.8558448557898499
[08/12/2025 16:50:08 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:50:12 INFO]: Training loss at epoch 84: 1.0092738270759583
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:50:13 INFO]: New best epoch, val score: -0.9188196102547879
[08/12/2025 16:50:13 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:50:24 INFO]: Training loss at epoch 85: 1.238711178302765
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:50:26 INFO]: New best epoch, val score: -0.9177394705190917
[08/12/2025 16:50:26 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:50:33 INFO]: New best epoch, val score: -0.9532286861130483
[08/12/2025 16:50:33 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:50:37 INFO]: Training loss at epoch 86: 1.1332832872867584
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:50:37 INFO]: Training loss at epoch 31: 0.7542005479335785
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:50:38 INFO]: New best epoch, val score: -0.9169473834205509
[08/12/2025 16:50:38 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:50:42 INFO]: New best epoch, val score: -0.8431271748046607
[08/12/2025 16:50:42 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:50:49 INFO]: Training loss at epoch 87: 0.9874133467674255
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:50:51 INFO]: Training loss at epoch 16: 0.7918066680431366
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:50:58 INFO]: Training loss at epoch 11: 0.850633978843689
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:51:01 INFO]: Training loss at epoch 88: 0.9938423931598663
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:51:05 INFO]: Training loss at epoch 18: 1.157974898815155
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:51:11 INFO]: Training loss at epoch 32: 0.6630011200904846
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:51:12 INFO]: Training loss at epoch 11: 1.0837724804878235
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:51:13 INFO]: Training loss at epoch 89: 0.8896427154541016
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:51:17 INFO]: Training stats: {
    "score": -0.9766116193599407,
    "rmse": 0.9766116193599407
}
[08/12/2025 16:51:17 INFO]: Val stats: {
    "score": -0.9175668318057915,
    "rmse": 0.9175668318057915
}
[08/12/2025 16:51:17 INFO]: Test stats: {
    "score": -0.8980498492347544,
    "rmse": 0.8980498492347544
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:51:25 INFO]: Training loss at epoch 10: 0.9402479827404022
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:51:30 INFO]: Training loss at epoch 90: 0.972061812877655
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:51:30 INFO]: Training loss at epoch 9: 1.1790346503257751
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:51:42 INFO]: Training loss at epoch 91: 1.1136330962181091
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:51:43 INFO]: New best epoch, val score: -0.916028534305613
[08/12/2025 16:51:43 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:51:44 INFO]: Training loss at epoch 33: 0.6219049394130707
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:51:54 INFO]: Training loss at epoch 92: 0.9424558281898499
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:51:55 INFO]: Training loss at epoch 17: 1.0754131078720093
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:51:56 INFO]: New best epoch, val score: -0.9151326545044529
[08/12/2025 16:51:56 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:52:03 INFO]: Training loss at epoch 19: 0.9421870112419128
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:52:07 INFO]: Training loss at epoch 93: 1.105561077594757
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:52:08 INFO]: New best epoch, val score: -0.9145709728075974
[08/12/2025 16:52:08 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:52:10 INFO]: Training stats: {
    "score": -1.057864418178076,
    "rmse": 1.057864418178076
}
[08/12/2025 16:52:10 INFO]: Val stats: {
    "score": -1.0949999939359787,
    "rmse": 1.0949999939359787
}
[08/12/2025 16:52:10 INFO]: Test stats: {
    "score": -1.0308821384270952,
    "rmse": 1.0308821384270952
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:52:18 INFO]: Training loss at epoch 34: 0.61386439204216
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:52:19 INFO]: Training loss at epoch 94: 0.9982525110244751
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:52:20 INFO]: New best epoch, val score: -0.91372467919548
[08/12/2025 16:52:20 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:52:22 INFO]: Training stats: {
    "score": -0.9940630911886078,
    "rmse": 0.9940630911886078
}
[08/12/2025 16:52:22 INFO]: Val stats: {
    "score": -0.9726683628680428,
    "rmse": 0.9726683628680428
}
[08/12/2025 16:52:22 INFO]: Test stats: {
    "score": -0.9331730950556583,
    "rmse": 0.9331730950556583
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:52:31 INFO]: Training loss at epoch 12: 0.9683403968811035
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:52:31 INFO]: Training loss at epoch 95: 1.2485060691833496
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:52:33 INFO]: New best epoch, val score: -0.912928489522666
[08/12/2025 16:52:33 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:52:43 INFO]: Training loss at epoch 96: 1.0583581328392029
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:52:45 INFO]: Training loss at epoch 12: 1.3655996322631836
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:52:51 INFO]: Training loss at epoch 35: 0.7071802616119385
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:52:55 INFO]: New best epoch, val score: -0.833869560827865
[08/12/2025 16:52:55 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:52:55 INFO]: Training loss at epoch 97: 1.0465803742408752
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:53:01 INFO]: Training loss at epoch 18: 0.9045027196407318
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:53:07 INFO]: Training loss at epoch 98: 0.893193244934082
[08/12/2025 16:53:07 INFO]: Training loss at epoch 11: 0.8699958920478821
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:53:19 INFO]: Training loss at epoch 99: 0.9443331360816956
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:53:21 INFO]: Training loss at epoch 20: 0.8132887184619904
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:53:23 INFO]: Training stats: {
    "score": -0.9739631450218449,
    "rmse": 0.9739631450218449
}
[08/12/2025 16:53:23 INFO]: Val stats: {
    "score": -0.9155338133124404,
    "rmse": 0.9155338133124404
}
[08/12/2025 16:53:23 INFO]: Test stats: {
    "score": -0.8957506264397032,
    "rmse": 0.8957506264397032
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:53:24 INFO]: Training loss at epoch 36: 0.7016191780567169
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:53:35 INFO]: Training loss at epoch 100: 0.8738091886043549
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:53:47 INFO]: Training loss at epoch 101: 0.9940430521965027
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:53:48 INFO]: Training loss at epoch 5: 0.9390185177326202
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:53:56 INFO]: Training loss at epoch 37: 0.6484018564224243
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:53:59 INFO]: Training loss at epoch 102: 1.2359228730201721
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:54:01 INFO]: Training loss at epoch 13: 1.0681617856025696
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:54:05 INFO]: Training loss at epoch 10: 0.943920910358429
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:54:06 INFO]: Training loss at epoch 19: 0.8831499814987183
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:54:11 INFO]: Training loss at epoch 103: 0.8265333473682404
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:54:19 INFO]: Training loss at epoch 13: 1.2121354341506958
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:54:20 INFO]: Training loss at epoch 21: 0.9111969172954559
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:54:23 INFO]: Training loss at epoch 104: 1.0474455058574677
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:54:28 INFO]: Training stats: {
    "score": -0.9998691766512098,
    "rmse": 0.9998691766512098
}
[08/12/2025 16:54:28 INFO]: Val stats: {
    "score": -1.0324059390109102,
    "rmse": 1.0324059390109102
}
[08/12/2025 16:54:28 INFO]: Test stats: {
    "score": -0.9670352611884048,
    "rmse": 0.9670352611884048
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:54:29 INFO]: Training loss at epoch 38: 0.7332330644130707
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:54:35 INFO]: Training loss at epoch 105: 1.1892215609550476
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:54:47 INFO]: Training loss at epoch 106: 1.0583257675170898
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:54:51 INFO]: Training loss at epoch 12: 1.1149746775627136
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:54:58 INFO]: Training loss at epoch 107: 1.100968599319458
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:55:02 INFO]: Training loss at epoch 39: 0.5338685661554337
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:55:11 INFO]: Training loss at epoch 108: 0.8921286463737488
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:55:14 INFO]: Training stats: {
    "score": -0.7399541110251076,
    "rmse": 0.7399541110251076
}
[08/12/2025 16:55:14 INFO]: Val stats: {
    "score": -0.8438442326522443,
    "rmse": 0.8438442326522443
}
[08/12/2025 16:55:14 INFO]: Test stats: {
    "score": -0.7927975307673113,
    "rmse": 0.7927975307673113
}
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:55:19 INFO]: Training loss at epoch 22: 0.8509029746055603
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:55:23 INFO]: Training loss at epoch 109: 1.059924066066742
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:55:27 INFO]: Training stats: {
    "score": -0.9672457047931202,
    "rmse": 0.9672457047931202
}
[08/12/2025 16:55:27 INFO]: Val stats: {
    "score": -0.9283203513239451,
    "rmse": 0.9283203513239451
}
[08/12/2025 16:55:27 INFO]: Test stats: {
    "score": -0.9003497070643928,
    "rmse": 0.9003497070643928
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:55:31 INFO]: Training loss at epoch 14: 1.0439595580101013
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:55:33 INFO]: Training loss at epoch 20: 0.8026340305805206
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:55:39 INFO]: Training loss at epoch 110: 0.7685321271419525
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:55:47 INFO]: Training loss at epoch 40: 0.4752623587846756
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:55:52 INFO]: Training loss at epoch 111: 0.9354687929153442
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:55:52 INFO]: Training loss at epoch 14: 1.2237879931926727
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:56:01 INFO]: Training loss at epoch 11: 1.0185253620147705
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:56:04 INFO]: Training loss at epoch 112: 0.9570854902267456
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:56:15 INFO]: New best epoch, val score: -0.9194080522098054
[08/12/2025 16:56:15 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:56:18 INFO]: Training loss at epoch 113: 0.9265521764755249
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:56:18 INFO]: Training loss at epoch 23: 0.9656399786472321
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:56:22 INFO]: Training loss at epoch 41: 0.5151175111532211
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:56:27 INFO]: New best epoch, val score: -0.820176835980236
[08/12/2025 16:56:27 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:56:30 INFO]: Training loss at epoch 114: 1.03390571475029
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:56:34 INFO]: Training loss at epoch 13: 0.9322788417339325
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:56:41 INFO]: Training loss at epoch 21: 0.9236022233963013
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:56:42 INFO]: Training loss at epoch 115: 0.9165672361850739
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:56:54 INFO]: Training loss at epoch 116: 0.9154826700687408
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:56:55 INFO]: Training loss at epoch 42: 0.7972680032253265
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:56:59 INFO]: New best epoch, val score: -0.8046679953767538
[08/12/2025 16:56:59 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:57:05 INFO]: Training loss at epoch 15: 0.9951521754264832
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:57:06 INFO]: Training loss at epoch 117: 1.0765660107135773
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:57:17 INFO]: Training loss at epoch 24: 1.1075882315635681
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:57:18 INFO]: Training loss at epoch 118: 0.9905572831630707
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:57:26 INFO]: Training loss at epoch 15: 0.8513040542602539
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:57:29 INFO]: Training loss at epoch 6: 1.0547987818717957
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
[08/12/2025 16:57:29 INFO]: Training loss at epoch 43: 0.4461366534233093
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:57:30 INFO]: Training loss at epoch 119: 0.9707281291484833
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:57:35 INFO]: Training stats: {
    "score": -0.9683081768635835,
    "rmse": 0.9683081768635835
}
[08/12/2025 16:57:35 INFO]: Val stats: {
    "score": -0.9123848915034483,
    "rmse": 0.9123848915034483
}
[08/12/2025 16:57:35 INFO]: Test stats: {
    "score": -0.8914976064617139,
    "rmse": 0.8914976064617139
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:57:36 INFO]: New best epoch, val score: -0.9123848915034483
[08/12/2025 16:57:36 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:57:46 INFO]: Training loss at epoch 22: 1.0220940113067627
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:57:47 INFO]: Training loss at epoch 120: 1.0256307423114777
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:57:48 INFO]: New best epoch, val score: -0.9115605367153823
[08/12/2025 16:57:48 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:57:54 INFO]: New best epoch, val score: -0.8841904860983827
[08/12/2025 16:57:54 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:58:00 INFO]: Training loss at epoch 121: 0.8783314228057861
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:58:01 INFO]: Training loss at epoch 12: 1.0176436603069305
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:58:02 INFO]: New best epoch, val score: -0.9105698732664566
[08/12/2025 16:58:02 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:58:03 INFO]: Training loss at epoch 44: 0.5379122197628021
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:58:12 INFO]: Training loss at epoch 122: 1.117891013622284
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:58:14 INFO]: New best epoch, val score: -0.9094013221009513
[08/12/2025 16:58:14 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:58:15 INFO]: Training loss at epoch 25: 1.0152419805526733
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:58:16 INFO]: Training loss at epoch 14: 0.9803411364555359
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:58:22 INFO]: New best epoch, val score: -0.9298836810830683
[08/12/2025 16:58:22 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:58:25 INFO]: Training loss at epoch 123: 1.282935082912445
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:58:26 INFO]: New best epoch, val score: -0.9093314921772178
[08/12/2025 16:58:26 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:58:37 INFO]: Training loss at epoch 45: 0.4935562014579773
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:58:37 INFO]: Training loss at epoch 124: 0.8149652779102325
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:58:38 INFO]: Training loss at epoch 16: 0.9702627956867218
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:58:41 INFO]: New best epoch, val score: -0.8037163472647055
[08/12/2025 16:58:41 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:58:50 INFO]: Training loss at epoch 125: 0.9820778965950012
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:58:52 INFO]: Training loss at epoch 23: 0.8861020803451538
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:58:59 INFO]: Training loss at epoch 16: 0.947164386510849
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:59:00 INFO]: New best epoch, val score: -0.8793651641479541
[08/12/2025 16:59:00 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:59:02 INFO]: Training loss at epoch 126: 0.9289076328277588
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:59:03 INFO]: New best epoch, val score: -0.9091033670949134
[08/12/2025 16:59:03 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 16:59:11 INFO]: Training loss at epoch 46: 0.47013211250305176
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 16:59:13 INFO]: Training loss at epoch 26: 1.1609222292900085
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:59:14 INFO]: Training loss at epoch 127: 1.3030818104743958
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:59:15 INFO]: New best epoch, val score: -0.7777741207160754
[08/12/2025 16:59:15 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:59:27 INFO]: Training loss at epoch 128: 0.8112847208976746
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:59:39 INFO]: Training loss at epoch 129: 0.7729818224906921
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:59:43 INFO]: Training stats: {
    "score": -0.9665776975162762,
    "rmse": 0.9665776975162762
}
[08/12/2025 16:59:43 INFO]: Val stats: {
    "score": -0.909288580675542,
    "rmse": 0.909288580675542
}
[08/12/2025 16:59:43 INFO]: Test stats: {
    "score": -0.8891686267848099,
    "rmse": 0.8891686267848099
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:59:44 INFO]: Training loss at epoch 47: 0.621689110994339
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:59:49 INFO]: New best epoch, val score: -0.7750421651188756
[08/12/2025 16:59:49 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:59:55 INFO]: Training loss at epoch 130: 0.9693911373615265
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:59:57 INFO]: Training loss at epoch 24: 0.9795035719871521
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 16:59:58 INFO]: Training loss at epoch 15: 0.8902910649776459
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 16:59:59 INFO]: Training loss at epoch 13: 1.0910166501998901
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:00:05 INFO]: New best epoch, val score: -0.8757755252033284
[08/12/2025 17:00:05 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:00:08 INFO]: Training loss at epoch 131: 0.8353833556175232
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:00:09 INFO]: New best epoch, val score: -0.9089728348642074
[08/12/2025 17:00:09 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:00:11 INFO]: Training loss at epoch 17: 0.8620554506778717
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:00:11 INFO]: Training loss at epoch 27: 1.1832148432731628
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:00:18 INFO]: Training loss at epoch 48: 0.4754108786582947
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:00:20 INFO]: Training loss at epoch 132: 1.1967926621437073
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:00:22 INFO]: New best epoch, val score: -0.9085408604887005
[08/12/2025 17:00:22 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:00:22 INFO]: New best epoch, val score: -0.9200278585690124
[08/12/2025 17:00:22 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:00:23 INFO]: New best epoch, val score: -0.7730361593587718
[08/12/2025 17:00:23 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:00:31 INFO]: Training loss at epoch 17: 1.2457640767097473
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:00:33 INFO]: Training loss at epoch 133: 1.1100568175315857
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:00:45 INFO]: Training loss at epoch 134: 0.9577983319759369
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:00:52 INFO]: Training loss at epoch 49: 0.4446578770875931
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:00:57 INFO]: Training loss at epoch 135: 1.0542628169059753
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:01:02 INFO]: Training loss at epoch 25: 0.7814320921897888
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:01:05 INFO]: Training stats: {
    "score": -0.6847731907052661,
    "rmse": 0.6847731907052661
}
[08/12/2025 17:01:05 INFO]: Val stats: {
    "score": -0.7818406089939196,
    "rmse": 0.7818406089939196
}
[08/12/2025 17:01:05 INFO]: Test stats: {
    "score": -0.7379705474131352,
    "rmse": 0.7379705474131352
}
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:01:07 INFO]: Training loss at epoch 7: 1.050809919834137
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:01:09 INFO]: Training loss at epoch 28: 0.8938226699829102
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:01:09 INFO]: Training loss at epoch 136: 0.8228667676448822
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:01:10 INFO]: New best epoch, val score: -0.8753263479612217
[08/12/2025 17:01:10 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:01:22 INFO]: Training loss at epoch 137: 1.021759033203125
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:01:34 INFO]: Training loss at epoch 138: 0.9891617000102997
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:01:38 INFO]: Training loss at epoch 50: 0.5623746812343597
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:01:40 INFO]: Training loss at epoch 16: 0.9262892305850983
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:01:43 INFO]: Training loss at epoch 18: 0.8882140517234802
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:01:46 INFO]: Training loss at epoch 139: 1.016688495874405
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:01:50 INFO]: Training stats: {
    "score": -0.9593012717414301,
    "rmse": 0.9593012717414301
}
[08/12/2025 17:01:50 INFO]: Val stats: {
    "score": -0.9184920560397267,
    "rmse": 0.9184920560397267
}
[08/12/2025 17:01:50 INFO]: Test stats: {
    "score": -0.891672623592596,
    "rmse": 0.891672623592596
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:01:55 INFO]: New best epoch, val score: -0.9175246621517443
[08/12/2025 17:01:55 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:01:57 INFO]: Training loss at epoch 14: 1.0520814657211304
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:02:03 INFO]: Training loss at epoch 140: 0.9065592288970947
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:02:03 INFO]: Training loss at epoch 18: 1.072180062532425
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:02:07 INFO]: Training loss at epoch 29: 0.9999122619628906
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:02:07 INFO]: Training loss at epoch 26: 0.833488792181015
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:02:11 INFO]: Training loss at epoch 51: 0.4342617690563202
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:02:15 INFO]: Training loss at epoch 141: 0.9989381730556488
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:02:27 INFO]: Training loss at epoch 142: 0.9809094071388245
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:02:27 INFO]: Training stats: {
    "score": -0.9956925495081458,
    "rmse": 0.9956925495081458
}
[08/12/2025 17:02:27 INFO]: Val stats: {
    "score": -0.9920659339697495,
    "rmse": 0.9920659339697495
}
[08/12/2025 17:02:27 INFO]: Test stats: {
    "score": -0.9451709736137365,
    "rmse": 0.9451709736137365
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:02:39 INFO]: Training loss at epoch 143: 1.0073089599609375
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:02:44 INFO]: Training loss at epoch 52: 0.4270833134651184
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:02:51 INFO]: Training loss at epoch 144: 0.9685003161430359
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:03:03 INFO]: Training loss at epoch 145: 0.8934416174888611
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:03:13 INFO]: Training loss at epoch 27: 0.801435261964798
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:03:15 INFO]: Training loss at epoch 19: 1.1777922213077545
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:03:16 INFO]: Training loss at epoch 146: 0.7907668948173523
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:03:18 INFO]: Training loss at epoch 53: 0.593584418296814
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:03:22 INFO]: Training loss at epoch 17: 1.5114474296569824
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:03:25 INFO]: Training loss at epoch 30: 1.110018104314804
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:03:28 INFO]: Training loss at epoch 147: 0.7737280428409576
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:03:35 INFO]: Training loss at epoch 19: 0.9469799995422363
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:03:40 INFO]: Training loss at epoch 148: 1.1761753261089325
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:03:47 INFO]: Training stats: {
    "score": -0.9940052843160582,
    "rmse": 0.9940052843160582
}
[08/12/2025 17:03:47 INFO]: Val stats: {
    "score": -0.9163383272847229,
    "rmse": 0.9163383272847229
}
[08/12/2025 17:03:47 INFO]: Test stats: {
    "score": -0.9026894930188791,
    "rmse": 0.9026894930188791
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:03:51 INFO]: Training loss at epoch 54: 0.5624149739742279
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:03:52 INFO]: Training loss at epoch 149: 0.9222738146781921
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:03:53 INFO]: Training loss at epoch 15: 0.9069004952907562
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:03:56 INFO]: Training stats: {
    "score": -0.9575049575300262,
    "rmse": 0.9575049575300262
}
[08/12/2025 17:03:56 INFO]: Val stats: {
    "score": -0.9107013321431116,
    "rmse": 0.9107013321431116
}
[08/12/2025 17:03:56 INFO]: Test stats: {
    "score": -0.8864951996152375,
    "rmse": 0.8864951996152375
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:03:58 INFO]: New best epoch, val score: -0.9163383272847229
[08/12/2025 17:03:58 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:04:07 INFO]: Training stats: {
    "score": -0.9507067897615417,
    "rmse": 0.9507067897615417
}
[08/12/2025 17:04:07 INFO]: Val stats: {
    "score": -0.9128045081853796,
    "rmse": 0.9128045081853796
}
[08/12/2025 17:04:07 INFO]: Test stats: {
    "score": -0.8802534979365255,
    "rmse": 0.8802534979365255
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:04:08 INFO]: New best epoch, val score: -0.9187877936532559
[08/12/2025 17:04:08 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:04:09 INFO]: Training loss at epoch 150: 1.5118193924427032
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:04:10 INFO]: New best epoch, val score: -0.9076506037322934
[08/12/2025 17:04:10 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:04:18 INFO]: Training loss at epoch 28: 0.828391432762146
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:04:21 INFO]: Training loss at epoch 151: 0.8911952376365662
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:04:23 INFO]: Training loss at epoch 31: 1.0345506072044373
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:04:23 INFO]: New best epoch, val score: -0.9062548857242455
[08/12/2025 17:04:23 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:04:25 INFO]: Training loss at epoch 55: 0.48084160685539246
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:04:34 INFO]: Training loss at epoch 152: 1.111530363559723
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:04:35 INFO]: New best epoch, val score: -0.9049594850770202
[08/12/2025 17:04:35 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:04:45 INFO]: Training loss at epoch 8: 1.0748620927333832
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:04:46 INFO]: Training loss at epoch 153: 1.0094422101974487
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:04:48 INFO]: New best epoch, val score: -0.904277170811252
[08/12/2025 17:04:48 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:04:58 INFO]: Training loss at epoch 154: 0.7302080988883972
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:04:59 INFO]: Training loss at epoch 56: 0.45473769307136536
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:05:00 INFO]: New best epoch, val score: -0.9041187719995556
[08/12/2025 17:05:00 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:05:03 INFO]: Training loss at epoch 18: 0.8936535716056824
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:05:11 INFO]: Training loss at epoch 155: 0.9061903953552246
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:05:20 INFO]: Training loss at epoch 20: 0.915910542011261
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:05:20 INFO]: Training loss at epoch 32: 1.0982917845249176
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:05:22 INFO]: Training loss at epoch 29: 0.8372896909713745
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:05:23 INFO]: Training loss at epoch 156: 1.0965314209461212
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:05:32 INFO]: Training loss at epoch 57: 0.5066189020872116
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:05:35 INFO]: Training loss at epoch 157: 0.9745941162109375
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:05:39 INFO]: Training loss at epoch 20: 0.8896295726299286
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:05:44 INFO]: Training stats: {
    "score": -0.9081414365506438,
    "rmse": 0.9081414365506438
}
[08/12/2025 17:05:44 INFO]: Val stats: {
    "score": -0.9186298541642591,
    "rmse": 0.9186298541642591
}
[08/12/2025 17:05:44 INFO]: Test stats: {
    "score": -0.8628133676408527,
    "rmse": 0.8628133676408527
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:05:48 INFO]: Training loss at epoch 158: 1.1737829744815826
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:05:51 INFO]: Training loss at epoch 16: 1.162835419178009
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:06:00 INFO]: Training loss at epoch 159: 0.8983429968357086
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:06:04 INFO]: Training stats: {
    "score": -0.9544608973935081,
    "rmse": 0.9544608973935081
}
[08/12/2025 17:06:04 INFO]: Val stats: {
    "score": -0.9123113924084293,
    "rmse": 0.9123113924084293
}
[08/12/2025 17:06:04 INFO]: Test stats: {
    "score": -0.8861362771378357,
    "rmse": 0.8861362771378357
}
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:06:06 INFO]: Training loss at epoch 58: 0.4094516932964325
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:06:16 INFO]: Training loss at epoch 160: 1.0269000232219696
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:06:18 INFO]: Training loss at epoch 33: 1.0405648350715637
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:06:28 INFO]: Training loss at epoch 161: 1.0301336348056793
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:06:39 INFO]: Training loss at epoch 59: 0.3444559574127197
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:06:40 INFO]: Training loss at epoch 162: 0.8277082443237305
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:06:45 INFO]: Training loss at epoch 19: 0.9536979496479034
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:06:50 INFO]: Training loss at epoch 30: 1.0156286656856537
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:06:51 INFO]: Training stats: {
    "score": -0.6469258925397545,
    "rmse": 0.6469258925397545
}
[08/12/2025 17:06:51 INFO]: Val stats: {
    "score": -0.7657903138904865,
    "rmse": 0.7657903138904865
}
[08/12/2025 17:06:51 INFO]: Test stats: {
    "score": -0.7252641552090117,
    "rmse": 0.7252641552090117
}
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:06:52 INFO]: Training loss at epoch 21: 0.9831287264823914
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:06:52 INFO]: Training loss at epoch 163: 1.065209686756134
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:06:55 INFO]: New best epoch, val score: -0.7657903138904865
[08/12/2025 17:06:55 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:07:04 INFO]: Training loss at epoch 164: 0.887712150812149
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:07:12 INFO]: Training loss at epoch 21: 0.8999722599983215
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:07:16 INFO]: Training loss at epoch 165: 0.9629475772380829
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:07:17 INFO]: Training loss at epoch 34: 0.8741992712020874
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:07:20 INFO]: Training stats: {
    "score": -0.9996020861249827,
    "rmse": 0.9996020861249827
}
[08/12/2025 17:07:20 INFO]: Val stats: {
    "score": -0.9489999652502931,
    "rmse": 0.9489999652502931
}
[08/12/2025 17:07:20 INFO]: Test stats: {
    "score": -0.9220879604055638,
    "rmse": 0.9220879604055638
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:07:23 INFO]: Training loss at epoch 60: 0.4913318604230881
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:07:28 INFO]: Training loss at epoch 166: 0.8516537249088287
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:07:40 INFO]: Training loss at epoch 167: 0.949739933013916
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:07:46 INFO]: Training loss at epoch 17: 0.9914127588272095
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:07:51 INFO]: Training loss at epoch 168: 1.020008534193039
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:07:55 INFO]: Training loss at epoch 31: 0.9313662052154541
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:07:56 INFO]: Training loss at epoch 61: 0.5973671823740005
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:08:03 INFO]: New best epoch, val score: -0.8456816260554789
[08/12/2025 17:08:03 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:08:04 INFO]: Training loss at epoch 169: 1.1532880067825317
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:08:08 INFO]: Training stats: {
    "score": -0.9515761769470751,
    "rmse": 0.9515761769470751
}
[08/12/2025 17:08:08 INFO]: Val stats: {
    "score": -0.9146929933805229,
    "rmse": 0.9146929933805229
}
[08/12/2025 17:08:08 INFO]: Test stats: {
    "score": -0.8861437379419278,
    "rmse": 0.8861437379419278
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:08:17 INFO]: Training loss at epoch 35: 0.8647000193595886
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:08:20 INFO]: Training loss at epoch 170: 0.8878131210803986
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:08:23 INFO]: Training loss at epoch 22: 1.0388704240322113
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:08:24 INFO]: New best epoch, val score: -0.9192519094247938
[08/12/2025 17:08:24 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:08:26 INFO]: Training loss at epoch 9: 1.1907318234443665
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:08:30 INFO]: Training loss at epoch 62: 0.4883183240890503
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:08:33 INFO]: Training loss at epoch 171: 0.8066956102848053
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:08:45 INFO]: Training loss at epoch 172: 1.1151277720928192
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:08:47 INFO]: Training loss at epoch 22: 1.1100887656211853
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:08:57 INFO]: Training loss at epoch 173: 0.8471893668174744
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:09:01 INFO]: Training loss at epoch 32: 0.6606185138225555
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:09:02 INFO]: Training loss at epoch 63: 0.4526832550764084
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:09:04 INFO]: Training loss at epoch 20: 1.015129029750824
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:09:09 INFO]: New best epoch, val score: -0.8354838066501491
[08/12/2025 17:09:09 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:09:09 INFO]: Training loss at epoch 174: 0.9469271600246429
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:09:16 INFO]: Training loss at epoch 36: 0.7776781320571899
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:09:21 INFO]: Training loss at epoch 175: 0.8136989176273346
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:09:23 INFO]: New best epoch, val score: -0.9040880579819842
[08/12/2025 17:09:23 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:09:23 INFO]: New best epoch, val score: -0.9143264920211428
[08/12/2025 17:09:23 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:09:34 INFO]: Training loss at epoch 176: 0.9944868087768555
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:09:35 INFO]: New best epoch, val score: -0.9023059973293621
[08/12/2025 17:09:35 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:09:36 INFO]: Training loss at epoch 64: 0.5051223039627075
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:09:41 INFO]: Training stats: {
    "score": -0.9972964724953715,
    "rmse": 0.9972964724953715
}
[08/12/2025 17:09:41 INFO]: Val stats: {
    "score": -0.9618232477625573,
    "rmse": 0.9618232477625573
}
[08/12/2025 17:09:41 INFO]: Test stats: {
    "score": -0.9277792553281969,
    "rmse": 0.9277792553281969
}
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
[08/12/2025 17:09:43 INFO]: Training loss at epoch 18: 1.2787167131900787
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:09:46 INFO]: Training loss at epoch 177: 1.0632811188697815
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
[08/12/2025 17:09:54 INFO]: Training loss at epoch 23: 0.8927435874938965
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:09:58 INFO]: Training loss at epoch 178: 1.0850822925567627
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
[08/12/2025 17:10:07 INFO]: Training loss at epoch 33: 0.9087227582931519
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:10:10 INFO]: Training loss at epoch 65: 0.48835553228855133
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:10:10 INFO]: Training loss at epoch 179: 1.168138474225998
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:10:14 INFO]: New best epoch, val score: -0.763050664797048
[08/12/2025 17:10:14 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:10:14 INFO]: Training loss at epoch 37: 1.1495469212532043
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:10:15 INFO]: Training stats: {
    "score": -0.9495943349279051,
    "rmse": 0.9495943349279051
}
[08/12/2025 17:10:15 INFO]: Val stats: {
    "score": -0.9058023936185983,
    "rmse": 0.9058023936185983
}
[08/12/2025 17:10:15 INFO]: Test stats: {
    "score": -0.8803979098658441,
    "rmse": 0.8803979098658441
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:10:15 INFO]: New best epoch, val score: -0.8277959664446787
[08/12/2025 17:10:15 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:10:20 INFO]: Training loss at epoch 23: 0.8521524369716644
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:10:27 INFO]: Training loss at epoch 180: 1.1692880690097809
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:10:39 INFO]: Training loss at epoch 181: 0.9742039442062378
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:10:43 INFO]: Training loss at epoch 66: 0.36874350905418396
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:10:46 INFO]: Training loss at epoch 21: 1.010791540145874
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:10:48 INFO]: New best epoch, val score: -0.7541903676471619
[08/12/2025 17:10:48 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:10:52 INFO]: Training loss at epoch 182: 0.9413305521011353
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:11:04 INFO]: Training loss at epoch 183: 0.8905770182609558
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:11:12 INFO]: Training loss at epoch 38: 1.03801628947258
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:11:12 INFO]: Training loss at epoch 34: 0.7199178338050842
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:11:16 INFO]: Training loss at epoch 184: 1.1333158910274506
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:11:17 INFO]: Training loss at epoch 67: 0.34844984859228134
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:11:20 INFO]: New best epoch, val score: -0.8242499076831313
[08/12/2025 17:11:20 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:11:26 INFO]: Training loss at epoch 24: 1.03560870885849
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:11:28 INFO]: Training loss at epoch 185: 0.8592358827590942
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:11:40 INFO]: Training loss at epoch 186: 0.9837392270565033
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:11:43 INFO]: Training loss at epoch 19: 0.9406872391700745
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:11:49 INFO]: Training loss at epoch 68: 0.4099404513835907
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:11:52 INFO]: Training loss at epoch 187: 0.8984362483024597
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:11:56 INFO]: Training loss at epoch 24: 0.9181232452392578
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:12:04 INFO]: Training loss at epoch 188: 1.0427910089492798
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:12:05 INFO]: New best epoch, val score: -0.9021704614802489
[08/12/2025 17:12:05 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:12:11 INFO]: Training loss at epoch 39: 1.068077951669693
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:12:16 INFO]: Training loss at epoch 35: 0.8966074585914612
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:12:16 INFO]: Training loss at epoch 189: 0.9294382929801941
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:12:20 INFO]: Training stats: {
    "score": -0.9467268811840183,
    "rmse": 0.9467268811840183
}
[08/12/2025 17:12:20 INFO]: Val stats: {
    "score": -0.9003898071287125,
    "rmse": 0.9003898071287125
}
[08/12/2025 17:12:20 INFO]: Test stats: {
    "score": -0.8762528066035009,
    "rmse": 0.8762528066035009
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:12:22 INFO]: New best epoch, val score: -0.9003898071287125
[08/12/2025 17:12:22 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:12:22 INFO]: Training loss at epoch 69: 0.47650302946567535
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:12:24 INFO]: Training stats: {
    "score": -1.0172524703247416,
    "rmse": 1.0172524703247416
}
[08/12/2025 17:12:24 INFO]: Val stats: {
    "score": -1.0375253309282526,
    "rmse": 1.0375253309282526
}
[08/12/2025 17:12:24 INFO]: Test stats: {
    "score": -0.9776010181235607,
    "rmse": 0.9776010181235607
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:12:28 INFO]: Training loss at epoch 22: 1.0449978113174438
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:12:31 INFO]: Training stats: {
    "score": -0.9808148295012004,
    "rmse": 0.9808148295012004
}
[08/12/2025 17:12:31 INFO]: Val stats: {
    "score": -0.9779742232979151,
    "rmse": 0.9779742232979151
}
[08/12/2025 17:12:31 INFO]: Test stats: {
    "score": -0.9308019281544423,
    "rmse": 0.9308019281544423
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:12:32 INFO]: Training loss at epoch 190: 0.9186452925205231
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:12:34 INFO]: New best epoch, val score: -0.8991223927452217
[08/12/2025 17:12:34 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:12:35 INFO]: Training stats: {
    "score": -0.605005944474352,
    "rmse": 0.605005944474352
}
[08/12/2025 17:12:35 INFO]: Val stats: {
    "score": -0.7748925775633095,
    "rmse": 0.7748925775633095
}
[08/12/2025 17:12:35 INFO]: Test stats: {
    "score": -0.7150592887237709,
    "rmse": 0.7150592887237709
}
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:12:45 INFO]: Training loss at epoch 191: 0.9659782648086548
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:12:46 INFO]: New best epoch, val score: -0.8987822640551157
[08/12/2025 17:12:46 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:12:57 INFO]: Training loss at epoch 192: 1.0300095677375793
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:12:57 INFO]: Training loss at epoch 25: 1.1230533123016357
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:13:08 INFO]: Training loss at epoch 70: 0.34463268518447876
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:13:09 INFO]: Training loss at epoch 193: 1.106087565422058
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:13:12 INFO]: New best epoch, val score: -0.7446832407519567
[08/12/2025 17:13:12 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:13:19 INFO]: Training loss at epoch 10: 1.0998519659042358
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
[08/12/2025 17:13:21 INFO]: Training loss at epoch 36: 0.8060047328472137
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:13:22 INFO]: Training loss at epoch 194: 0.8537041246891022
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:13:28 INFO]: Training loss at epoch 25: 0.7646557092666626
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:13:29 INFO]: Training loss at epoch 40: 0.9522968232631683
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:13:34 INFO]: Training loss at epoch 195: 0.8067260384559631
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:13:40 INFO]: New best epoch, val score: -0.8753225592551065
[08/12/2025 17:13:40 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:13:42 INFO]: Training loss at epoch 71: 0.4265381246805191
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:13:46 INFO]: New best epoch, val score: -0.7368792709733076
[08/12/2025 17:13:46 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:13:46 INFO]: Training loss at epoch 196: 0.9892706871032715
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:13:59 INFO]: Training loss at epoch 197: 1.091407597064972
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:14:09 INFO]: Training loss at epoch 23: 0.8490661680698395
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:14:11 INFO]: Training loss at epoch 198: 0.9648399949073792
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:14:15 INFO]: Training loss at epoch 72: 0.3527272120118141
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:14:23 INFO]: Training loss at epoch 199: 1.1549262702465057
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:14:23 INFO]: Training loss at epoch 20: 1.101156234741211
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:14:25 INFO]: Training loss at epoch 37: 0.8125677704811096
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:14:26 INFO]: Training loss at epoch 41: 1.0182937383651733
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:14:27 INFO]: Training stats: {
    "score": -0.9413908616149559,
    "rmse": 0.9413908616149559
}
[08/12/2025 17:14:27 INFO]: Val stats: {
    "score": -0.9064120504444031,
    "rmse": 0.9064120504444031
}
[08/12/2025 17:14:27 INFO]: Test stats: {
    "score": -0.8775788356793502,
    "rmse": 0.8775788356793502
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:14:29 INFO]: Running Final Evaluation...
[W 2025-08-12 17:14:29,118] Trial 6 failed with parameters: {'embedding_head_pair': (256, 4), 'n_layers': 2, 'd_ffn_factor': 1.4603021723163128, 'attention_dropout': 0.38836098754651377, 'activation': 'gelu', 'lr': 1.2863641441087446e-05} because of the following error: RuntimeError('Error(s) in loading state_dict for FTTransformer:\n\tUnexpected key(s) in state_dict: "layers.2.attention.W_q.weight", "layers.2.attention.W_q.bias", "layers.2.attention.W_k.weight", "layers.2.attention.W_k.bias", "layers.2.attention.W_v.weight", "layers.2.attention.W_v.bias", "layers.2.attention.W_out.weight", "layers.2.attention.W_out.bias", "layers.2.linear0.weight", "layers.2.linear0.bias", "layers.2.linear1.weight", "layers.2.linear1.bias", "layers.2.norm1.weight", "layers.2.norm1.bias", "layers.2.norm0.weight", "layers.2.norm0.bias", "layers.3.attention.W_q.weight", "layers.3.attention.W_q.bias", "layers.3.attention.W_k.weight", "layers.3.attention.W_k.bias", "layers.3.attention.W_v.weight", "layers.3.attention.W_v.bias", "layers.3.attention.W_out.weight", "layers.3.attention.W_out.bias", "layers.3.linear0.weight", "layers.3.linear0.bias", "layers.3.linear1.weight", "layers.3.linear1.bias", "layers.3.norm1.weight", "layers.3.norm1.bias", "layers.3.norm0.weight", "layers.3.norm0.bias", "layers.4.attention.W_q.weight", "layers.4.attention.W_q.bias", "layers.4.attention.W_k.weight", "layers.4.attention.W_k.bias", "layers.4.attention.W_v.weight", "layers.4.attention.W_v.bias", "layers.4.attention.W_out.weight", "layers.4.attention.W_out.bias", "layers.4.linear0.weight", "layers.4.linear0.bias", "layers.4.linear1.weight", "layers.4.linear1.bias", "layers.4.norm1.weight", "layers.4.norm1.bias", "layers.4.norm0.weight", "layers.4.norm0.bias", "layers.5.attention.W_q.weight", "layers.5.attention.W_q.bias", "layers.5.attention.W_k.weight", "layers.5.attention.W_k.bias", "layers.5.attention.W_v.weight", "layers.5.attention.W_v.bias", "layers.5.attention.W_out.weight", "layers.5.attention.W_out.bias", "layers.5.linear0.weight", "layers.5.linear0.bias", "layers.5.linear1.weight", "layers.5.linear1.bias", "layers.5.norm1.weight", "layers.5.norm1.bias", "layers.5.norm0.weight", "layers.5.norm0.bias", "layers.6.attention.W_q.weight", "layers.6.attention.W_q.bias", "layers.6.attention.W_k.weight", "layers.6.attention.W_k.bias", "layers.6.attention.W_v.weight", "layers.6.attention.W_v.bias", "layers.6.attention.W_out.weight", "layers.6.attention.W_out.bias", "layers.6.linear0.weight", "layers.6.linear0.bias", "layers.6.linear1.weight", "layers.6.linear1.bias", "layers.6.norm1.weight", "layers.6.norm1.bias", "layers.6.norm0.weight", "layers.6.norm0.bias", "layers.7.attention.W_q.weight", "layers.7.attention.W_q.bias", "layers.7.attention.W_k.weight", "layers.7.attention.W_k.bias", "layers.7.attention.W_v.weight", "layers.7.attention.W_v.bias", "layers.7.attention.W_out.weight", "layers.7.attention.W_out.bias", "layers.7.linear0.weight", "layers.7.linear0.bias", "layers.7.linear1.weight", "layers.7.linear1.bias", "layers.7.norm1.weight", "layers.7.norm1.bias", "layers.7.norm0.weight", "layers.7.norm0.bias", "layers.8.attention.W_q.weight", "layers.8.attention.W_q.bias", "layers.8.attention.W_k.weight", "layers.8.attention.W_k.bias", "layers.8.attention.W_v.weight", "layers.8.attention.W_v.bias", "layers.8.attention.W_out.weight", "layers.8.attention.W_out.bias", "layers.8.linear0.weight", "layers.8.linear0.bias", "layers.8.linear1.weight", "layers.8.linear1.bias", "layers.8.norm1.weight", "layers.8.norm1.bias", "layers.8.norm0.weight", "layers.8.norm0.bias", "layers.9.attention.W_q.weight", "layers.9.attention.W_q.bias", "layers.9.attention.W_k.weight", "layers.9.attention.W_k.bias", "layers.9.attention.W_v.weight", "layers.9.attention.W_v.bias", "layers.9.attention.W_out.weight", "layers.9.attention.W_out.bias", "layers.9.linear0.weight", "layers.9.linear0.bias", "layers.9.linear1.weight", "layers.9.linear1.bias", "layers.9.norm1.weight", "layers.9.norm1.bias", "layers.9.norm0.weight", "layers.9.norm0.bias". \n\tsize mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 64]) from checkpoint, the shape in current model is torch.Size([101, 256]).\n\tsize mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 64]) from checkpoint, the shape in current model is torch.Size([100, 256]).\n\tsize mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([188, 64]) from checkpoint, the shape in current model is torch.Size([373, 256]).\n\tsize mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([188]) from checkpoint, the shape in current model is torch.Size([373]).\n\tsize mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([64, 94]) from checkpoint, the shape in current model is torch.Size([256, 373]).\n\tsize mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([188, 64]) from checkpoint, the shape in current model is torch.Size([373, 256]).\n\tsize mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([188]) from checkpoint, the shape in current model is torch.Size([373]).\n\tsize mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([64, 94]) from checkpoint, the shape in current model is torch.Size([256, 373]).\n\tsize mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for last_normalization.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for last_normalization.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for head.weight: copying a param with shape torch.Size([1, 64]) from checkpoint, the shape in current model is torch.Size([1, 256]).').
Traceback (most recent call last):
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 145, in <lambda>
    func = lambda trial: objective(trial, cfg, trial_stats, trial_counter, n_optuna_trials,
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 104, in objective
    stats = train_net_for_optuna.main(config, loaders, unique_categories, n_numerical, n_classes)
  File "/temporario2/14570653/tabular-transfer-learning/train_net_for_optuna.py", line 127, in main
    net.load_state_dict(torch.load(checkpoint_path)["net"])
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2215, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for FTTransformer:
	Unexpected key(s) in state_dict: "layers.2.attention.W_q.weight", "layers.2.attention.W_q.bias", "layers.2.attention.W_k.weight", "layers.2.attention.W_k.bias", "layers.2.attention.W_v.weight", "layers.2.attention.W_v.bias", "layers.2.attention.W_out.weight", "layers.2.attention.W_out.bias", "layers.2.linear0.weight", "layers.2.linear0.bias", "layers.2.linear1.weight", "layers.2.linear1.bias", "layers.2.norm1.weight", "layers.2.norm1.bias", "layers.2.norm0.weight", "layers.2.norm0.bias", "layers.3.attention.W_q.weight", "layers.3.attention.W_q.bias", "layers.3.attention.W_k.weight", "layers.3.attention.W_k.bias", "layers.3.attention.W_v.weight", "layers.3.attention.W_v.bias", "layers.3.attention.W_out.weight", "layers.3.attention.W_out.bias", "layers.3.linear0.weight", "layers.3.linear0.bias", "layers.3.linear1.weight", "layers.3.linear1.bias", "layers.3.norm1.weight", "layers.3.norm1.bias", "layers.3.norm0.weight", "layers.3.norm0.bias", "layers.4.attention.W_q.weight", "layers.4.attention.W_q.bias", "layers.4.attention.W_k.weight", "layers.4.attention.W_k.bias", "layers.4.attention.W_v.weight", "layers.4.attention.W_v.bias", "layers.4.attention.W_out.weight", "layers.4.attention.W_out.bias", "layers.4.linear0.weight", "layers.4.linear0.bias", "layers.4.linear1.weight", "layers.4.linear1.bias", "layers.4.norm1.weight", "layers.4.norm1.bias", "layers.4.norm0.weight", "layers.4.norm0.bias", "layers.5.attention.W_q.weight", "layers.5.attention.W_q.bias", "layers.5.attention.W_k.weight", "layers.5.attention.W_k.bias", "layers.5.attention.W_v.weight", "layers.5.attention.W_v.bias", "layers.5.attention.W_out.weight", "layers.5.attention.W_out.bias", "layers.5.linear0.weight", "layers.5.linear0.bias", "layers.5.linear1.weight", "layers.5.linear1.bias", "layers.5.norm1.weight", "layers.5.norm1.bias", "layers.5.norm0.weight", "layers.5.norm0.bias", "layers.6.attention.W_q.weight", "layers.6.attention.W_q.bias", "layers.6.attention.W_k.weight", "layers.6.attention.W_k.bias", "layers.6.attention.W_v.weight", "layers.6.attention.W_v.bias", "layers.6.attention.W_out.weight", "layers.6.attention.W_out.bias", "layers.6.linear0.weight", "layers.6.linear0.bias", "layers.6.linear1.weight", "layers.6.linear1.bias", "layers.6.norm1.weight", "layers.6.norm1.bias", "layers.6.norm0.weight", "layers.6.norm0.bias", "layers.7.attention.W_q.weight", "layers.7.attention.W_q.bias", "layers.7.attention.W_k.weight", "layers.7.attention.W_k.bias", "layers.7.attention.W_v.weight", "layers.7.attention.W_v.bias", "layers.7.attention.W_out.weight", "layers.7.attention.W_out.bias", "layers.7.linear0.weight", "layers.7.linear0.bias", "layers.7.linear1.weight", "layers.7.linear1.bias", "layers.7.norm1.weight", "layers.7.norm1.bias", "layers.7.norm0.weight", "layers.7.norm0.bias", "layers.8.attention.W_q.weight", "layers.8.attention.W_q.bias", "layers.8.attention.W_k.weight", "layers.8.attention.W_k.bias", "layers.8.attention.W_v.weight", "layers.8.attention.W_v.bias", "layers.8.attention.W_out.weight", "layers.8.attention.W_out.bias", "layers.8.linear0.weight", "layers.8.linear0.bias", "layers.8.linear1.weight", "layers.8.linear1.bias", "layers.8.norm1.weight", "layers.8.norm1.bias", "layers.8.norm0.weight", "layers.8.norm0.bias", "layers.9.attention.W_q.weight", "layers.9.attention.W_q.bias", "layers.9.attention.W_k.weight", "layers.9.attention.W_k.bias", "layers.9.attention.W_v.weight", "layers.9.attention.W_v.bias", "layers.9.attention.W_out.weight", "layers.9.attention.W_out.bias", "layers.9.linear0.weight", "layers.9.linear0.bias", "layers.9.linear1.weight", "layers.9.linear1.bias", "layers.9.norm1.weight", "layers.9.norm1.bias", "layers.9.norm0.weight", "layers.9.norm0.bias". 
	size mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 64]) from checkpoint, the shape in current model is torch.Size([101, 256]).
	size mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 64]) from checkpoint, the shape in current model is torch.Size([100, 256]).
	size mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([188, 64]) from checkpoint, the shape in current model is torch.Size([373, 256]).
	size mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([188]) from checkpoint, the shape in current model is torch.Size([373]).
	size mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([64, 94]) from checkpoint, the shape in current model is torch.Size([256, 373]).
	size mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([188, 64]) from checkpoint, the shape in current model is torch.Size([373, 256]).
	size mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([188]) from checkpoint, the shape in current model is torch.Size([373]).
	size mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([64, 94]) from checkpoint, the shape in current model is torch.Size([256, 373]).
	size mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for last_normalization.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for last_normalization.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for head.weight: copying a param with shape torch.Size([1, 64]) from checkpoint, the shape in current model is torch.Size([1, 256]).
[W 2025-08-12 17:14:29,126] Trial 6 failed with value None.
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:14:30 INFO]: Training loss at epoch 26: 0.9406753182411194
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:14:48 INFO]: Training loss at epoch 73: 0.3832361698150635
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:15:01 INFO]: Training loss at epoch 26: 0.6592245846986771
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:15:12 INFO]: New best epoch, val score: -0.8582696980442867
[08/12/2025 17:15:12 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:15:21 INFO]: Training loss at epoch 74: 0.48832380771636963
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:15:24 INFO]: Training loss at epoch 42: 1.1681774854660034
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:15:28 INFO]: Training loss at epoch 38: 0.8814606666564941
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:15:50 INFO]: Training loss at epoch 24: 0.9599516689777374
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:15:53 INFO]: Training loss at epoch 75: 0.3869404196739197
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:15:59 INFO]: Training loss at epoch 27: 1.138625293970108
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:16:10 INFO]: New best epoch, val score: -0.9103792327429059
[08/12/2025 17:16:10 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:16:18 INFO]: Training loss at epoch 21: 0.8600470423698425
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:16:22 INFO]: Training loss at epoch 43: 0.9446321725845337
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:16:26 INFO]: Training loss at epoch 76: 0.4152723699808121
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:16:31 INFO]: Training loss at epoch 39: 0.6476134955883026
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:16:33 INFO]: Training loss at epoch 27: 0.8282263278961182
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
[08/12/2025 17:16:44 INFO]: New best epoch, val score: -0.8483152800856721
[08/12/2025 17:16:44 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:16:52 INFO]: Training stats: {
    "score": -0.7741845827473814,
    "rmse": 0.7741845827473814
}
[08/12/2025 17:16:52 INFO]: Val stats: {
    "score": -0.8017249814962967,
    "rmse": 0.8017249814962967
}
[08/12/2025 17:16:52 INFO]: Test stats: {
    "score": -0.7433067473406939,
    "rmse": 0.7433067473406939
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:16:56 INFO]: Training loss at epoch 11: 1.139937847852707
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:16:59 INFO]: Training loss at epoch 77: 0.3913290202617645
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:17:00 INFO]: New best epoch, val score: -0.8017249814962967
[08/12/2025 17:17:00 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:17:19 INFO]: Training loss at epoch 44: 1.0035902261734009
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:17:29 INFO]: Training loss at epoch 28: 1.1012420952320099
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:17:31 INFO]: Training loss at epoch 78: 0.3216943144798279
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:17:31 INFO]: Training loss at epoch 25: 0.9749481379985809
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:17:55 INFO]: Training loss at epoch 40: 0.6502111554145813
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:18:03 INFO]: New best epoch, val score: -0.7988432998773303
[08/12/2025 17:18:03 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:18:04 INFO]: Training loss at epoch 79: 0.510389432311058
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:18:05 INFO]: Training loss at epoch 28: 0.8692892789840698
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:18:12 INFO]: Training loss at epoch 22: 1.0068728923797607
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:18:16 INFO]: Training stats: {
    "score": -0.5763418328516098,
    "rmse": 0.5763418328516098
}
[08/12/2025 17:18:16 INFO]: Val stats: {
    "score": -0.7488617475350317,
    "rmse": 0.7488617475350317
}
[08/12/2025 17:18:16 INFO]: Test stats: {
    "score": -0.725666114198778,
    "rmse": 0.725666114198778
}
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:18:17 INFO]: Training loss at epoch 45: 0.9865376353263855
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:18:48 INFO]: Training loss at epoch 80: 0.45372165739536285
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:18:58 INFO]: Training loss at epoch 41: 0.7066412568092346
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:18:59 INFO]: Training loss at epoch 29: 0.8460812866687775
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:19:13 INFO]: Training loss at epoch 26: 0.987451434135437
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:19:15 INFO]: Training loss at epoch 46: 0.8996949791908264
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:19:21 INFO]: Training loss at epoch 81: 0.32449477910995483
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:19:30 INFO]: Training stats: {
    "score": -0.9439951412980324,
    "rmse": 0.9439951412980324
}
[08/12/2025 17:19:30 INFO]: Val stats: {
    "score": -0.9369960933139967,
    "rmse": 0.9369960933139967
}
[08/12/2025 17:19:30 INFO]: Test stats: {
    "score": -0.8901929819944062,
    "rmse": 0.8901929819944062
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:19:37 INFO]: Training loss at epoch 29: 0.8768058717250824
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:19:53 INFO]: Training loss at epoch 82: 0.33102676272392273
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:20:00 INFO]: Training loss at epoch 42: 0.6813079416751862
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:20:05 INFO]: Training loss at epoch 23: 0.9655698537826538
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:20:09 INFO]: Training stats: {
    "score": -0.869029056998224,
    "rmse": 0.869029056998224
}
[08/12/2025 17:20:09 INFO]: Val stats: {
    "score": -0.9094983282498283,
    "rmse": 0.9094983282498283
}
[08/12/2025 17:20:09 INFO]: Test stats: {
    "score": -0.8371594717546458,
    "rmse": 0.8371594717546458
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:20:13 INFO]: Training loss at epoch 47: 0.9538779556751251
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:20:25 INFO]: Training loss at epoch 83: 0.39157313108444214
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:20:33 INFO]: Training loss at epoch 12: 0.9364936053752899
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:20:54 INFO]: Training loss at epoch 27: 1.1491314768791199
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:20:58 INFO]: Training loss at epoch 84: 0.33644986152648926
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:20:59 INFO]: Training loss at epoch 30: 0.9153978228569031
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:21:03 INFO]: Training loss at epoch 43: 0.7256170213222504
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:21:10 INFO]: Training loss at epoch 48: 0.7738532423973083
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:21:17 INFO]: New best epoch, val score: -0.9134776733712788
[08/12/2025 17:21:17 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:21:30 INFO]: Training loss at epoch 85: 0.4453623592853546
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:21:35 INFO]: New best epoch, val score: -0.7271892327879569
[08/12/2025 17:21:35 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:21:41 INFO]: Training loss at epoch 30: 0.6894469261169434
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:21:59 INFO]: Training loss at epoch 24: 0.8842776119709015
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:22:03 INFO]: Training loss at epoch 86: 0.39815692603588104
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:22:06 INFO]: Training loss at epoch 44: 0.5977122187614441
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:22:07 INFO]: New best epoch, val score: -0.726939028964675
[08/12/2025 17:22:07 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:22:08 INFO]: Training loss at epoch 49: 0.9821422100067139
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:22:13 INFO]: New best epoch, val score: -0.9130865173986055
[08/12/2025 17:22:13 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:22:28 INFO]: Training stats: {
    "score": -0.9397288927372135,
    "rmse": 0.9397288927372135
}
[08/12/2025 17:22:28 INFO]: Val stats: {
    "score": -0.9031383870364187,
    "rmse": 0.9031383870364187
}
[08/12/2025 17:22:28 INFO]: Test stats: {
    "score": -0.8700588890730412,
    "rmse": 0.8700588890730412
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:22:29 INFO]: Training loss at epoch 31: 0.7850298881530762
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:22:35 INFO]: New best epoch, val score: -0.9031383870364187
[08/12/2025 17:22:35 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:22:35 INFO]: Training loss at epoch 28: 1.020838975906372
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:22:36 INFO]: Training loss at epoch 87: 0.3690079301595688
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:23:09 INFO]: Training loss at epoch 88: 0.44037581980228424
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:23:09 INFO]: Training loss at epoch 45: 0.6619356274604797
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:23:12 INFO]: Training loss at epoch 31: 0.7589243948459625
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:23:26 INFO]: Training loss at epoch 50: 0.8279229402542114
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:23:33 INFO]: New best epoch, val score: -0.8956236473639428
[08/12/2025 17:23:33 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:23:41 INFO]: Training loss at epoch 89: 0.3887229710817337
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:23:53 INFO]: Training stats: {
    "score": -0.5528951468785324,
    "rmse": 0.5528951468785324
}
[08/12/2025 17:23:53 INFO]: Val stats: {
    "score": -0.7371953748434991,
    "rmse": 0.7371953748434991
}
[08/12/2025 17:23:53 INFO]: Test stats: {
    "score": -0.7090572983360959,
    "rmse": 0.7090572983360959
}
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:23:53 INFO]: Training loss at epoch 25: 1.0452644526958466
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:23:59 INFO]: Training loss at epoch 32: 0.8499522507190704
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:24:07 INFO]: New best epoch, val score: -0.9074051369082863
[08/12/2025 17:24:07 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:24:10 INFO]: New best epoch, val score: -0.8871042976449683
[08/12/2025 17:24:10 INFO]: Saving model to: model_best.pth
[08/12/2025 17:24:10 INFO]: Training loss at epoch 13: 1.0894863605499268
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:24:12 INFO]: Training loss at epoch 46: 0.564397931098938
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:24:16 INFO]: Training loss at epoch 29: 1.0705682933330536
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:24:24 INFO]: Training loss at epoch 51: 0.8846322000026703
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:24:25 INFO]: Training loss at epoch 90: 0.35841551423072815
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:24:44 INFO]: Training loss at epoch 32: 0.5820369571447372
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:24:50 INFO]: Training stats: {
    "score": -0.9984811609382879,
    "rmse": 0.9984811609382879
}
[08/12/2025 17:24:50 INFO]: Val stats: {
    "score": -0.9564034802819306,
    "rmse": 0.9564034802819306
}
[08/12/2025 17:24:50 INFO]: Test stats: {
    "score": -0.9257261841852114,
    "rmse": 0.9257261841852114
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:24:55 INFO]: New best epoch, val score: -0.8131634922700111
[08/12/2025 17:24:55 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:24:58 INFO]: Training loss at epoch 91: 0.3401193469762802
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:25:15 INFO]: Training loss at epoch 47: 0.6522461771965027
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:25:22 INFO]: Training loss at epoch 52: 0.6833579242229462
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:25:29 INFO]: Training loss at epoch 33: 0.657143235206604
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:25:30 INFO]: Training loss at epoch 92: 0.4128778278827667
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:25:40 INFO]: New best epoch, val score: -0.8560361060188757
[08/12/2025 17:25:40 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:25:48 INFO]: Training loss at epoch 26: 0.9241752326488495
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:26:01 INFO]: New best epoch, val score: -0.9055969869970542
[08/12/2025 17:26:01 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:26:03 INFO]: Training loss at epoch 93: 0.312227264046669
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:26:16 INFO]: Training loss at epoch 33: 0.5949211120605469
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:26:17 INFO]: Training loss at epoch 48: 0.6181764900684357
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:26:19 INFO]: Training loss at epoch 53: 0.8007968366146088
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:26:27 INFO]: New best epoch, val score: -0.7796058462784763
[08/12/2025 17:26:27 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:26:32 INFO]: Training loss at epoch 30: 1.0734076499938965
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:26:36 INFO]: Training loss at epoch 94: 0.3515625
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:26:59 INFO]: Training loss at epoch 34: 0.8246934413909912
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:27:08 INFO]: Training loss at epoch 95: 0.37644046545028687
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:27:10 INFO]: New best epoch, val score: -0.8522651500111266
[08/12/2025 17:27:10 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:27:17 INFO]: Training loss at epoch 54: 0.7848130464553833
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:27:20 INFO]: Training loss at epoch 49: 0.7584018111228943
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:27:24 INFO]: New best epoch, val score: -0.8779762623360386
[08/12/2025 17:27:24 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:27:41 INFO]: Training loss at epoch 96: 0.3067232668399811
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:27:41 INFO]: Training stats: {
    "score": -0.8322027110264745,
    "rmse": 0.8322027110264745
}
[08/12/2025 17:27:41 INFO]: Val stats: {
    "score": -0.9615272556844169,
    "rmse": 0.9615272556844169
}
[08/12/2025 17:27:41 INFO]: Test stats: {
    "score": -0.8634455540835312,
    "rmse": 0.8634455540835312
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:27:42 INFO]: Training loss at epoch 27: 0.8295215368270874
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:27:45 INFO]: New best epoch, val score: -0.7131956075395011
[08/12/2025 17:27:45 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:27:47 INFO]: Training loss at epoch 14: 1.0771762132644653
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
[08/12/2025 17:27:48 INFO]: Training loss at epoch 34: 0.49915793538093567
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
[08/12/2025 17:27:59 INFO]: New best epoch, val score: -0.7772689529161089
[08/12/2025 17:27:59 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:28:13 INFO]: Training loss at epoch 31: 1.0551073849201202
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
[08/12/2025 17:28:14 INFO]: Training loss at epoch 97: 0.28196482360363007
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:28:15 INFO]: Training loss at epoch 55: 0.8038530051708221
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:28:19 INFO]: New best epoch, val score: -0.7048310803661171
[08/12/2025 17:28:19 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:28:22 INFO]: New best epoch, val score: -0.8610555263905318
[08/12/2025 17:28:22 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:28:29 INFO]: Training loss at epoch 35: 0.8297638297080994
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:28:40 INFO]: New best epoch, val score: -0.8358538117972559
[08/12/2025 17:28:40 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:28:45 INFO]: Training loss at epoch 50: 0.4430806636810303
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:28:47 INFO]: Training loss at epoch 98: 0.3608264625072479
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:29:13 INFO]: Training loss at epoch 56: 0.6852786839008331
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:29:19 INFO]: New best epoch, val score: -0.8522481175628975
[08/12/2025 17:29:19 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:29:20 INFO]: Training loss at epoch 35: 0.616062730550766
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:29:20 INFO]: Training loss at epoch 99: 0.3767978250980377
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:29:32 INFO]: Training stats: {
    "score": -0.5642378684381322,
    "rmse": 0.5642378684381322
}
[08/12/2025 17:29:32 INFO]: Val stats: {
    "score": -0.7850579156010297,
    "rmse": 0.7850579156010297
}
[08/12/2025 17:29:32 INFO]: Test stats: {
    "score": -0.7431155281385671,
    "rmse": 0.7431155281385671
}
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:29:38 INFO]: Training loss at epoch 28: 0.9524410367012024
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:29:48 INFO]: Training loss at epoch 51: 0.5665469765663147
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:29:54 INFO]: Training loss at epoch 32: 0.8731618225574493
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:29:59 INFO]: Training loss at epoch 36: 0.7775317430496216
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:30:04 INFO]: Training loss at epoch 100: 0.31316234171390533
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:30:10 INFO]: Training loss at epoch 57: 1.121105283498764
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:30:37 INFO]: Training loss at epoch 101: 0.3366320878267288
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:30:50 INFO]: Training loss at epoch 52: 0.3628096729516983
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:30:52 INFO]: Training loss at epoch 36: 0.5985834896564484
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:31:08 INFO]: Training loss at epoch 58: 0.8300206363201141
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:31:10 INFO]: Training loss at epoch 102: 0.31339961290359497
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:31:24 INFO]: Training loss at epoch 15: 0.8958715200424194
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:31:29 INFO]: Training loss at epoch 37: 0.7901105582714081
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:31:32 INFO]: Training loss at epoch 29: 0.9545150101184845
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
[08/12/2025 17:31:35 INFO]: Training loss at epoch 33: 0.84429731965065
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:31:42 INFO]: Training loss at epoch 103: 0.4691728353500366
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:31:50 INFO]: New best epoch, val score: -0.9363187959939393
[08/12/2025 17:31:50 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
[08/12/2025 17:31:54 INFO]: Training loss at epoch 53: 0.4530039578676224
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:32:06 INFO]: Training loss at epoch 59: 0.8250277936458588
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:32:11 INFO]: Training stats: {
    "score": -0.9504829208423284,
    "rmse": 0.9504829208423284
}
[08/12/2025 17:32:11 INFO]: Val stats: {
    "score": -0.9208783030279625,
    "rmse": 0.9208783030279625
}
[08/12/2025 17:32:11 INFO]: Test stats: {
    "score": -0.87968355936369,
    "rmse": 0.87968355936369
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:32:15 INFO]: Training loss at epoch 104: 0.36635836958885193
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:32:23 INFO]: Training loss at epoch 37: 0.7336222231388092
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:32:25 INFO]: Training stats: {
    "score": -0.9095515781514855,
    "rmse": 0.9095515781514855
}
[08/12/2025 17:32:25 INFO]: Val stats: {
    "score": -0.9718691291760435,
    "rmse": 0.9718691291760435
}
[08/12/2025 17:32:25 INFO]: Test stats: {
    "score": -0.8924730578102408,
    "rmse": 0.8924730578102408
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:32:48 INFO]: Training loss at epoch 105: 0.3855656087398529
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:32:56 INFO]: Training loss at epoch 54: 0.48870936036109924
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:32:59 INFO]: Training loss at epoch 38: 0.7773723900318146
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:33:17 INFO]: Training loss at epoch 34: 0.9995353519916534
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:33:20 INFO]: Training loss at epoch 106: 0.36929042637348175
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:33:23 INFO]: Training loss at epoch 60: 0.8857432901859283
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:33:29 INFO]: Running Final Evaluation...
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[W 2025-08-12 17:33:30,017] Trial 5 failed with parameters: {'embedding_head_pair': (256, 4), 'n_layers': 10, 'd_ffn_factor': 2.1268114214699194, 'attention_dropout': 0.04534269052100304, 'activation': 'gelu', 'lr': 0.0008432705961638474} because of the following error: RuntimeError('Error(s) in loading state_dict for FTTransformer:\n\tMissing key(s) in state_dict: "layers.8.attention.W_q.weight", "layers.8.attention.W_q.bias", "layers.8.attention.W_k.weight", "layers.8.attention.W_k.bias", "layers.8.attention.W_v.weight", "layers.8.attention.W_v.bias", "layers.8.attention.W_out.weight", "layers.8.attention.W_out.bias", "layers.8.linear0.weight", "layers.8.linear0.bias", "layers.8.linear1.weight", "layers.8.linear1.bias", "layers.8.norm1.weight", "layers.8.norm1.bias", "layers.8.norm0.weight", "layers.8.norm0.bias", "layers.9.attention.W_q.weight", "layers.9.attention.W_q.bias", "layers.9.attention.W_k.weight", "layers.9.attention.W_k.bias", "layers.9.attention.W_v.weight", "layers.9.attention.W_v.bias", "layers.9.attention.W_out.weight", "layers.9.attention.W_out.bias", "layers.9.linear0.weight", "layers.9.linear0.bias", "layers.9.linear1.weight", "layers.9.linear1.bias", "layers.9.norm1.weight", "layers.9.norm1.bias", "layers.9.norm0.weight", "layers.9.norm0.bias". \n\tsize mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 512]) from checkpoint, the shape in current model is torch.Size([101, 256]).\n\tsize mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 512]) from checkpoint, the shape in current model is torch.Size([100, 256]).\n\tsize mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([544, 256]).\n\tsize mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([544]).\n\tsize mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([256, 544]).\n\tsize mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([544, 256]).\n\tsize mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([544]).\n\tsize mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([256, 544]).\n\tsize mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.2.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.2.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.2.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.2.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.2.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.2.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.2.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.2.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.2.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([544, 256]).\n\tsize mismatch for layers.2.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([544]).\n\tsize mismatch for layers.2.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([256, 544]).\n\tsize mismatch for layers.2.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.2.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.2.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.2.norm0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.2.norm0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.3.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.3.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.3.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.3.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.3.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.3.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.3.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.3.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.3.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([544, 256]).\n\tsize mismatch for layers.3.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([544]).\n\tsize mismatch for layers.3.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([256, 544]).\n\tsize mismatch for layers.3.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.3.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.3.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.3.norm0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.3.norm0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.4.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.4.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.4.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.4.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.4.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.4.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.4.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.4.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.4.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([544, 256]).\n\tsize mismatch for layers.4.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([544]).\n\tsize mismatch for layers.4.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([256, 544]).\n\tsize mismatch for layers.4.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.4.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.4.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.4.norm0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.4.norm0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.5.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.5.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.5.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.5.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.5.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.5.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.5.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.5.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.5.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([544, 256]).\n\tsize mismatch for layers.5.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([544]).\n\tsize mismatch for layers.5.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([256, 544]).\n\tsize mismatch for layers.5.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.5.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.5.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.5.norm0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.5.norm0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.6.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.6.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.6.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.6.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.6.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.6.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.6.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.6.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.6.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([544, 256]).\n\tsize mismatch for layers.6.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([544]).\n\tsize mismatch for layers.6.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([256, 544]).\n\tsize mismatch for layers.6.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.6.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.6.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.6.norm0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.6.norm0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.7.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.7.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.7.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.7.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.7.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.7.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.7.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for layers.7.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.7.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([544, 256]).\n\tsize mismatch for layers.7.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([544]).\n\tsize mismatch for layers.7.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([256, 544]).\n\tsize mismatch for layers.7.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.7.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.7.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.7.norm0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layers.7.norm0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for last_normalization.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for last_normalization.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for head.weight: copying a param with shape torch.Size([1, 512]) from checkpoint, the shape in current model is torch.Size([1, 256]).').
Traceback (most recent call last):
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 145, in <lambda>
    func = lambda trial: objective(trial, cfg, trial_stats, trial_counter, n_optuna_trials,
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 104, in objective
    stats = train_net_for_optuna.main(config, loaders, unique_categories, n_numerical, n_classes)
  File "/temporario2/14570653/tabular-transfer-learning/train_net_for_optuna.py", line 127, in main
    net.load_state_dict(torch.load(checkpoint_path)["net"])
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2215, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for FTTransformer:
	Missing key(s) in state_dict: "layers.8.attention.W_q.weight", "layers.8.attention.W_q.bias", "layers.8.attention.W_k.weight", "layers.8.attention.W_k.bias", "layers.8.attention.W_v.weight", "layers.8.attention.W_v.bias", "layers.8.attention.W_out.weight", "layers.8.attention.W_out.bias", "layers.8.linear0.weight", "layers.8.linear0.bias", "layers.8.linear1.weight", "layers.8.linear1.bias", "layers.8.norm1.weight", "layers.8.norm1.bias", "layers.8.norm0.weight", "layers.8.norm0.bias", "layers.9.attention.W_q.weight", "layers.9.attention.W_q.bias", "layers.9.attention.W_k.weight", "layers.9.attention.W_k.bias", "layers.9.attention.W_v.weight", "layers.9.attention.W_v.bias", "layers.9.attention.W_out.weight", "layers.9.attention.W_out.bias", "layers.9.linear0.weight", "layers.9.linear0.bias", "layers.9.linear1.weight", "layers.9.linear1.bias", "layers.9.norm1.weight", "layers.9.norm1.bias", "layers.9.norm0.weight", "layers.9.norm0.bias". 
	size mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 512]) from checkpoint, the shape in current model is torch.Size([101, 256]).
	size mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 512]) from checkpoint, the shape in current model is torch.Size([100, 256]).
	size mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([544, 256]).
	size mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([544]).
	size mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([256, 544]).
	size mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([544, 256]).
	size mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([544]).
	size mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([256, 544]).
	size mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.2.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.2.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.2.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.2.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.2.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.2.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.2.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.2.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.2.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([544, 256]).
	size mismatch for layers.2.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([544]).
	size mismatch for layers.2.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([256, 544]).
	size mismatch for layers.2.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.2.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.2.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.2.norm0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.2.norm0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.3.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.3.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.3.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.3.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.3.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.3.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.3.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.3.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.3.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([544, 256]).
	size mismatch for layers.3.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([544]).
	size mismatch for layers.3.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([256, 544]).
	size mismatch for layers.3.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.3.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.3.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.3.norm0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.3.norm0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.4.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.4.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.4.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.4.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.4.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.4.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.4.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.4.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.4.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([544, 256]).
	size mismatch for layers.4.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([544]).
	size mismatch for layers.4.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([256, 544]).
	size mismatch for layers.4.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.4.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.4.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.4.norm0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.4.norm0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.5.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.5.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.5.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.5.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.5.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.5.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.5.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.5.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.5.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([544, 256]).
	size mismatch for layers.5.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([544]).
	size mismatch for layers.5.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([256, 544]).
	size mismatch for layers.5.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.5.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.5.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.5.norm0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.5.norm0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.6.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.6.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.6.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.6.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.6.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.6.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.6.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.6.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.6.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([544, 256]).
	size mismatch for layers.6.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([544]).
	size mismatch for layers.6.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([256, 544]).
	size mismatch for layers.6.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.6.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.6.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.6.norm0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.6.norm0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.7.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.7.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.7.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.7.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.7.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.7.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.7.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).
	size mismatch for layers.7.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.7.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([544, 256]).
	size mismatch for layers.7.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([544]).
	size mismatch for layers.7.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([256, 544]).
	size mismatch for layers.7.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.7.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.7.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.7.norm0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.7.norm0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for last_normalization.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for last_normalization.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for head.weight: copying a param with shape torch.Size([1, 512]) from checkpoint, the shape in current model is torch.Size([1, 256]).
[W 2025-08-12 17:33:30,022] Trial 5 failed with value None.
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:33:52 INFO]: Training loss at epoch 107: 0.30002640932798386
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:33:54 INFO]: Training loss at epoch 38: 0.5015528798103333
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:33:57 INFO]: New best epoch, val score: -0.7036500201656277
[08/12/2025 17:33:57 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:33:59 INFO]: Training loss at epoch 55: 0.5183453261852264
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:34:05 INFO]: Training loss at epoch 30: 0.7434835433959961
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:34:05 INFO]: New best epoch, val score: -0.7755449533985481
[08/12/2025 17:34:05 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:34:19 INFO]: Training loss at epoch 61: 0.7174088358879089
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:34:25 INFO]: Training loss at epoch 108: 0.3762107640504837
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:34:26 INFO]: New best epoch, val score: -0.810535444159028
[08/12/2025 17:34:26 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:34:31 INFO]: Training loss at epoch 39: 0.5956647992134094
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:35:01 INFO]: Training loss at epoch 109: 0.3669183701276779
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:35:02 INFO]: Training loss at epoch 16: 0.9216305911540985
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
[08/12/2025 17:35:02 INFO]: Training stats: {
    "score": -0.7770271207413867,
    "rmse": 0.7770271207413867
}
[08/12/2025 17:35:02 INFO]: Val stats: {
    "score": -0.8104305462312125,
    "rmse": 0.8104305462312125
}
[08/12/2025 17:35:02 INFO]: Test stats: {
    "score": -0.7452052330092339,
    "rmse": 0.7452052330092339
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:35:04 INFO]: Training loss at epoch 56: 0.5319249927997589
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:35:12 INFO]: Training stats: {
    "score": -0.5231985341444899,
    "rmse": 0.5231985341444899
}
[08/12/2025 17:35:12 INFO]: Val stats: {
    "score": -0.7337126960998436,
    "rmse": 0.7337126960998436
}
[08/12/2025 17:35:12 INFO]: Test stats: {
    "score": -0.7203213995794547,
    "rmse": 0.7203213995794547
}
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
[08/12/2025 17:35:13 INFO]: New best epoch, val score: -0.8104305462312125
[08/12/2025 17:35:13 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:35:18 INFO]: Training loss at epoch 62: 0.7672905325889587
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:35:26 INFO]: Training loss at epoch 39: 0.4596959054470062
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:35:27 INFO]: New best epoch, val score: -0.9330578007668792
[08/12/2025 17:35:27 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:35:45 INFO]: Training loss at epoch 110: 0.3348512649536133
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:35:58 INFO]: Training stats: {
    "score": -0.7210062493811721,
    "rmse": 0.7210062493811721
}
[08/12/2025 17:35:58 INFO]: Val stats: {
    "score": -0.7773996358607967,
    "rmse": 0.7773996358607967
}
[08/12/2025 17:35:58 INFO]: Test stats: {
    "score": -0.7068562376108354,
    "rmse": 0.7068562376108354
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:36:01 INFO]: Training loss at epoch 31: 0.8509088158607483
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:36:07 INFO]: Training loss at epoch 57: 0.452329620718956
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:36:15 INFO]: Training loss at epoch 63: 0.7820284962654114
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:36:18 INFO]: Training loss at epoch 111: 0.278510719537735
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:36:32 INFO]: Training loss at epoch 40: 0.5563534498214722
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:36:43 INFO]: New best epoch, val score: -0.8084809221623971
[08/12/2025 17:36:43 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:36:51 INFO]: Training loss at epoch 112: 0.2625231519341469
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:37:10 INFO]: Training loss at epoch 58: 0.5521814823150635
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:37:12 INFO]: Training loss at epoch 64: 0.6762179434299469
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:37:23 INFO]: Training loss at epoch 113: 0.3183172643184662
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:37:28 INFO]: Training loss at epoch 40: 0.5872666239738464
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:37:55 INFO]: Training loss at epoch 32: 0.898051381111145
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:37:55 INFO]: Training loss at epoch 114: 0.4660850018262863
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:38:02 INFO]: Training loss at epoch 41: 0.6188561618328094
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:38:09 INFO]: Training loss at epoch 65: 0.6958940029144287
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:38:12 INFO]: Training loss at epoch 59: 0.4398297965526581
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:38:15 INFO]: New best epoch, val score: -0.8043752994549797
[08/12/2025 17:38:15 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:38:28 INFO]: Training loss at epoch 115: 0.37128861248493195
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:38:33 INFO]: Training stats: {
    "score": -0.7590204656906394,
    "rmse": 0.7590204656906394
}
[08/12/2025 17:38:33 INFO]: Val stats: {
    "score": -0.8074531347815911,
    "rmse": 0.8074531347815911
}
[08/12/2025 17:38:33 INFO]: Test stats: {
    "score": -0.822949905730358,
    "rmse": 0.822949905730358
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:38:36 INFO]: Training loss at epoch 17: 0.7825019955635071
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:38:58 INFO]: Training loss at epoch 41: 0.4939758628606796
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:39:00 INFO]: Training loss at epoch 116: 0.2893376648426056
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:39:06 INFO]: Training loss at epoch 66: 0.7485135197639465
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:39:31 INFO]: Training loss at epoch 42: 0.4725028723478317
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:39:32 INFO]: Training loss at epoch 117: 0.2905874699354172
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:39:36 INFO]: Training loss at epoch 60: 0.5143378376960754
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
[08/12/2025 17:39:49 INFO]: Training loss at epoch 33: 0.9387593269348145
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:40:02 INFO]: New best epoch, val score: -0.9026994138917396
[08/12/2025 17:40:02 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:40:03 INFO]: Training loss at epoch 67: 0.7986417412757874
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:40:05 INFO]: Training loss at epoch 118: 0.31022901833057404
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:40:28 INFO]: Training loss at epoch 42: 0.40480904281139374
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:40:37 INFO]: Training loss at epoch 119: 0.27893422544002533
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:40:38 INFO]: Training loss at epoch 61: 0.6414041221141815
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:40:49 INFO]: Training stats: {
    "score": -0.5221133068174934,
    "rmse": 0.5221133068174934
}
[08/12/2025 17:40:49 INFO]: Val stats: {
    "score": -0.7492643849374402,
    "rmse": 0.7492643849374402
}
[08/12/2025 17:40:49 INFO]: Test stats: {
    "score": -0.7201404275761666,
    "rmse": 0.7201404275761666
}
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:40:59 INFO]: Training loss at epoch 68: 0.9002975523471832
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:41:00 INFO]: Training loss at epoch 43: 0.5756820738315582
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:41:21 INFO]: Training loss at epoch 120: 0.25624779611825943
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:41:41 INFO]: Training loss at epoch 62: 0.5204465389251709
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:41:43 INFO]: Training loss at epoch 34: 0.9178087115287781
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:41:54 INFO]: Training loss at epoch 121: 0.3196689039468765
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:41:55 INFO]: Training loss at epoch 69: 0.5634610950946808
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:41:56 INFO]: New best epoch, val score: -0.8789704414777768
[08/12/2025 17:41:56 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:41:57 INFO]: Training loss at epoch 43: 0.46764394640922546
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:41:58 INFO]: New best epoch, val score: -0.6933729307720452
[08/12/2025 17:41:58 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:42:09 INFO]: Training loss at epoch 18: 0.9117232263088226
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:42:14 INFO]: Training stats: {
    "score": -0.7976926840360284,
    "rmse": 0.7976926840360284
}
[08/12/2025 17:42:14 INFO]: Val stats: {
    "score": -0.8445066756037333,
    "rmse": 0.8445066756037333
}
[08/12/2025 17:42:14 INFO]: Test stats: {
    "score": -0.7818232546731338,
    "rmse": 0.7818232546731338
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:42:27 INFO]: Training loss at epoch 122: 0.3381928503513336
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:42:30 INFO]: Training loss at epoch 44: 0.552192360162735
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:42:31 INFO]: New best epoch, val score: -0.6926497897376079
[08/12/2025 17:42:31 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:42:43 INFO]: Training loss at epoch 63: 0.3902047872543335
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:42:59 INFO]: Training loss at epoch 123: 0.2604702487587929
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:43:11 INFO]: Training loss at epoch 70: 0.6266821026802063
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:43:27 INFO]: Training loss at epoch 44: 0.5268566012382507
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:43:32 INFO]: Training loss at epoch 124: 0.3349110335111618
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:43:36 INFO]: Training loss at epoch 35: 1.0500713884830475
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:43:46 INFO]: Training loss at epoch 64: 0.5437861382961273
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:43:50 INFO]: New best epoch, val score: -0.8761104710549493
[08/12/2025 17:43:50 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:43:54 INFO]: New best epoch, val score: -0.7931293010806507
[08/12/2025 17:43:54 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:44:00 INFO]: Training loss at epoch 45: 0.5552651882171631
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:44:05 INFO]: Training loss at epoch 125: 0.3944963961839676
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:44:08 INFO]: Training loss at epoch 71: 0.7970969974994659
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:44:11 INFO]: New best epoch, val score: -0.7909898282374905
[08/12/2025 17:44:11 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:44:38 INFO]: Training loss at epoch 126: 0.30583274364471436
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:44:50 INFO]: Training loss at epoch 65: 0.49724170565605164
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:44:58 INFO]: Training loss at epoch 45: 0.4286428987979889
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:45:05 INFO]: Training loss at epoch 72: 0.5433565080165863
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:45:09 INFO]: New best epoch, val score: -0.7519128832062069
[08/12/2025 17:45:09 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:45:10 INFO]: Training loss at epoch 127: 0.2667919918894768
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:45:30 INFO]: Training loss at epoch 46: 0.4728124439716339
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:45:32 INFO]: Training loss at epoch 36: 0.76032754778862
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:45:42 INFO]: Training loss at epoch 19: 0.9147601127624512
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
[08/12/2025 17:45:43 INFO]: Training loss at epoch 128: 0.3130065053701401
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:45:53 INFO]: Training loss at epoch 66: 0.49178558588027954
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:46:02 INFO]: Training loss at epoch 73: 0.678975909948349
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:46:16 INFO]: Training loss at epoch 129: 0.20322509109973907
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:46:27 INFO]: Training stats: {
    "score": -0.48616183285133246,
    "rmse": 0.48616183285133246
}
[08/12/2025 17:46:27 INFO]: Val stats: {
    "score": -0.690700603334065,
    "rmse": 0.690700603334065
}
[08/12/2025 17:46:27 INFO]: Test stats: {
    "score": -0.6878443451079979,
    "rmse": 0.6878443451079979
}
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:46:28 INFO]: Training loss at epoch 46: 0.423391655087471
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:46:32 INFO]: New best epoch, val score: -0.690700603334065
[08/12/2025 17:46:32 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:46:38 INFO]: New best epoch, val score: -0.7370763329850777
[08/12/2025 17:46:38 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:46:54 INFO]: Training stats: {
    "score": -0.9812945537605875,
    "rmse": 0.9812945537605875
}
[08/12/2025 17:46:54 INFO]: Val stats: {
    "score": -0.9450253793509638,
    "rmse": 0.9450253793509638
}
[08/12/2025 17:46:54 INFO]: Test stats: {
    "score": -0.9102676194581932,
    "rmse": 0.9102676194581932
}
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:46:55 INFO]: Training loss at epoch 67: 0.5266787111759186
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:46:58 INFO]: Training loss at epoch 74: 0.5384500920772552
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:46:59 INFO]: Training loss at epoch 47: 0.4245958924293518
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:47:00 INFO]: Training loss at epoch 130: 0.27870385348796844
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
[08/12/2025 17:47:25 INFO]: Training loss at epoch 37: 0.7566220164299011
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:47:32 INFO]: Training loss at epoch 131: 0.3621253967285156
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:47:54 INFO]: Training loss at epoch 75: 0.654342532157898
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:47:57 INFO]: Training loss at epoch 47: 0.4836399108171463
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:47:58 INFO]: Training loss at epoch 68: 0.391349196434021
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:48:05 INFO]: Training loss at epoch 132: 0.2987288385629654
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:48:05 INFO]: New best epoch, val score: -0.7828647718155584
[08/12/2025 17:48:05 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:48:09 INFO]: New best epoch, val score: -0.6805086780591353
[08/12/2025 17:48:09 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:48:28 INFO]: Training loss at epoch 48: 0.5342232882976532
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:48:37 INFO]: Training loss at epoch 133: 0.2011362724006176
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:48:39 INFO]: New best epoch, val score: -0.7712071764607856
[08/12/2025 17:48:39 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:48:52 INFO]: Training loss at epoch 76: 0.5828090012073517
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:49:01 INFO]: Training loss at epoch 69: 0.461959645152092
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:49:10 INFO]: Training loss at epoch 134: 0.30304157733917236
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:49:14 INFO]: New best epoch, val score: -0.6763214039488247
[08/12/2025 17:49:14 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:49:19 INFO]: Training loss at epoch 38: 0.6442406475543976
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:49:22 INFO]: Training stats: {
    "score": -0.6800859273111779,
    "rmse": 0.6800859273111779
}
[08/12/2025 17:49:22 INFO]: Val stats: {
    "score": -0.792842355083134,
    "rmse": 0.792842355083134
}
[08/12/2025 17:49:22 INFO]: Test stats: {
    "score": -0.7390692956065864,
    "rmse": 0.7390692956065864
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:49:27 INFO]: Training loss at epoch 48: 0.5563225448131561
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:49:43 INFO]: Training loss at epoch 135: 0.24712885171175003
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:49:48 INFO]: Training loss at epoch 77: 0.5948072373867035
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:49:58 INFO]: Training loss at epoch 49: 0.5618626773357391
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
[08/12/2025 17:50:15 INFO]: Training loss at epoch 136: 0.27799779176712036
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:50:24 INFO]: Training loss at epoch 70: 0.4296039193868637
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:50:28 INFO]: Training loss at epoch 20: 1.0060104131698608
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:50:29 INFO]: Training stats: {
    "score": -0.7085868036763098,
    "rmse": 0.7085868036763098
}
[08/12/2025 17:50:29 INFO]: Val stats: {
    "score": -0.7453334263267487,
    "rmse": 0.7453334263267487
}
[08/12/2025 17:50:29 INFO]: Test stats: {
    "score": -0.6929143014441252,
    "rmse": 0.6929143014441252
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
[08/12/2025 17:50:40 INFO]: New best epoch, val score: -0.7453334263267487
[08/12/2025 17:50:40 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:50:45 INFO]: Training loss at epoch 78: 0.7090823352336884
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:50:48 INFO]: Training loss at epoch 137: 0.2757023796439171
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
[08/12/2025 17:50:57 INFO]: Training loss at epoch 49: 0.4253484755754471
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:51:12 INFO]: Training loss at epoch 39: 1.0315421521663666
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:51:20 INFO]: Training loss at epoch 138: 0.3493814021348953
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:51:27 INFO]: Training loss at epoch 71: 0.42850178480148315
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:51:28 INFO]: Training stats: {
    "score": -0.6586221916191956,
    "rmse": 0.6586221916191956
}
[08/12/2025 17:51:28 INFO]: Val stats: {
    "score": -0.7750270799125959,
    "rmse": 0.7750270799125959
}
[08/12/2025 17:51:28 INFO]: Test stats: {
    "score": -0.6838526027727545,
    "rmse": 0.6838526027727545
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
[08/12/2025 17:51:41 INFO]: Training loss at epoch 79: 0.5610487759113312
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:51:51 INFO]: Training stats: {
    "score": -0.8412768302717925,
    "rmse": 0.8412768302717925
}
[08/12/2025 17:51:51 INFO]: Val stats: {
    "score": -0.8544580712751362,
    "rmse": 0.8544580712751362
}
[08/12/2025 17:51:51 INFO]: Test stats: {
    "score": -0.7890485789518148,
    "rmse": 0.7890485789518148
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:51:53 INFO]: Training loss at epoch 139: 0.2875269204378128
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:51:59 INFO]: Training loss at epoch 50: 0.4917736202478409
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:52:00 INFO]: Training stats: {
    "score": -0.7549017340072107,
    "rmse": 0.7549017340072107
}
[08/12/2025 17:52:00 INFO]: Val stats: {
    "score": -0.803323922968517,
    "rmse": 0.803323922968517
}
[08/12/2025 17:52:00 INFO]: Test stats: {
    "score": -0.733213399225455,
    "rmse": 0.733213399225455
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:52:04 INFO]: Training stats: {
    "score": -0.4681749912746787,
    "rmse": 0.4681749912746787
}
[08/12/2025 17:52:04 INFO]: Val stats: {
    "score": -0.6797914868679085,
    "rmse": 0.6797914868679085
}
[08/12/2025 17:52:04 INFO]: Test stats: {
    "score": -0.6779767988958804,
    "rmse": 0.6779767988958804
}
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:52:04 INFO]: New best epoch, val score: -0.8544580712751362
[08/12/2025 17:52:04 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:52:07 INFO]: New best epoch, val score: -0.803323922968517
[08/12/2025 17:52:07 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:52:29 INFO]: Training loss at epoch 72: 0.41613711416721344
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:52:37 INFO]: Training loss at epoch 140: 0.3359347879886627
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:52:57 INFO]: Training loss at epoch 80: 0.5231172144412994
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:52:58 INFO]: Training loss at epoch 50: 0.49683381617069244
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:53:04 INFO]: New best epoch, val score: -0.8028755380429198
[08/12/2025 17:53:04 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:53:09 INFO]: Training loss at epoch 141: 0.3180634528398514
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:53:28 INFO]: Training loss at epoch 51: 0.5077251642942429
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:53:32 INFO]: Training loss at epoch 73: 0.41400574147701263
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:53:42 INFO]: Training loss at epoch 142: 0.3265220373868942
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:53:45 INFO]: Training loss at epoch 40: 0.9303675293922424
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:53:54 INFO]: Training loss at epoch 81: 0.5320676267147064
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:54:01 INFO]: Training loss at epoch 21: 0.9024704694747925
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:54:14 INFO]: Training loss at epoch 143: 0.3294917643070221
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:54:18 INFO]: New best epoch, val score: -0.6697118458802671
[08/12/2025 17:54:18 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:54:27 INFO]: New best epoch, val score: -0.9318891189312362
[08/12/2025 17:54:27 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
[08/12/2025 17:54:28 INFO]: Training loss at epoch 51: 0.4208638221025467
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:54:35 INFO]: Training loss at epoch 74: 0.3526615798473358
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:54:39 INFO]: New best epoch, val score: -0.7339571517337308
[08/12/2025 17:54:39 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:54:47 INFO]: Training loss at epoch 144: 0.27260228991508484
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:54:51 INFO]: Training loss at epoch 82: 0.5768686980009079
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:54:58 INFO]: Training loss at epoch 52: 0.6214032173156738
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:55:19 INFO]: Training loss at epoch 145: 0.2480083927512169
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:55:37 INFO]: Training loss at epoch 75: 0.4853750169277191
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:55:39 INFO]: Training loss at epoch 41: 1.0072475373744965
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:55:47 INFO]: Training loss at epoch 83: 0.40525607764720917
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:55:52 INFO]: Training loss at epoch 146: 0.30111952126026154
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:55:59 INFO]: Training loss at epoch 52: 0.36629724502563477
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:56:10 INFO]: New best epoch, val score: -0.7179699051500043
[08/12/2025 17:56:10 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:56:24 INFO]: Training loss at epoch 147: 0.27137590944767
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:56:27 INFO]: Training loss at epoch 53: 0.3450378403067589
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:56:38 INFO]: New best epoch, val score: -0.7278929397821544
[08/12/2025 17:56:38 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:56:40 INFO]: Training loss at epoch 76: 0.38750849664211273
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:56:44 INFO]: Training loss at epoch 84: 0.5516172647476196
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:56:57 INFO]: Training loss at epoch 148: 0.2742583900690079
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:57:29 INFO]: Training loss at epoch 149: 0.19977334141731262
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:57:30 INFO]: Training loss at epoch 53: 0.38299836218357086
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:57:33 INFO]: Training loss at epoch 42: 0.6627032160758972
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:57:36 INFO]: Training loss at epoch 22: 0.9971091151237488
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:57:41 INFO]: Training stats: {
    "score": -0.46256715013965727,
    "rmse": 0.46256715013965727
}
[08/12/2025 17:57:41 INFO]: Val stats: {
    "score": -0.7007183125887689,
    "rmse": 0.7007183125887689
}
[08/12/2025 17:57:41 INFO]: Test stats: {
    "score": -0.7019190862508259,
    "rmse": 0.7019190862508259
}
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:57:44 INFO]: Training loss at epoch 77: 0.4762174040079117
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 17:57:47 INFO]: Training loss at epoch 85: 0.5103636980056763
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:57:57 INFO]: Training loss at epoch 54: 0.5588670969009399
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:58:02 INFO]: New best epoch, val score: -0.9231077223366914
[08/12/2025 17:58:02 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:58:09 INFO]: New best epoch, val score: -0.7273933661708287
[08/12/2025 17:58:09 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:58:15 INFO]: Training loss at epoch 150: 0.2890387997031212
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:58:45 INFO]: Training loss at epoch 86: 0.5231987833976746
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:58:47 INFO]: Training loss at epoch 151: 0.2403343766927719
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:58:48 INFO]: Training loss at epoch 78: 0.4217274487018585
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:59:02 INFO]: Training loss at epoch 54: 0.4352593421936035
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:59:19 INFO]: Training loss at epoch 152: 0.2849823832511902
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:59:26 INFO]: Training loss at epoch 43: 0.7301766872406006
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:59:27 INFO]: Training loss at epoch 55: 0.533077597618103
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 17:59:40 INFO]: New best epoch, val score: -0.8290610324588147
[08/12/2025 17:59:40 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 17:59:43 INFO]: Training loss at epoch 87: 0.37904995679855347
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:59:51 INFO]: Training loss at epoch 153: 0.3336050510406494
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 17:59:52 INFO]: Training loss at epoch 79: 0.46669530868530273
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:00:13 INFO]: Training stats: {
    "score": -0.7372457479831185,
    "rmse": 0.7372457479831185
}
[08/12/2025 18:00:13 INFO]: Val stats: {
    "score": -0.8945770896283322,
    "rmse": 0.8945770896283322
}
[08/12/2025 18:00:13 INFO]: Test stats: {
    "score": -0.8344949147366565,
    "rmse": 0.8344949147366565
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:00:24 INFO]: Training loss at epoch 154: 0.22652661800384521
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:00:32 INFO]: Training loss at epoch 55: 0.5017910152673721
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:00:39 INFO]: Training loss at epoch 88: 0.545900285243988
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:00:56 INFO]: Training loss at epoch 155: 0.31072065234184265
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:00:56 INFO]: Training loss at epoch 56: 0.4926629364490509
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:01:11 INFO]: Training loss at epoch 23: 0.9910274744033813
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:01:16 INFO]: Training loss at epoch 80: 0.35993462800979614
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:01:20 INFO]: Training loss at epoch 44: 0.6072132587432861
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:01:29 INFO]: Training loss at epoch 156: 0.3028203248977661
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:01:34 INFO]: New best epoch, val score: -0.805178930634918
[08/12/2025 18:01:34 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:01:36 INFO]: Training loss at epoch 89: 0.4009810984134674
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:01:36 INFO]: New best epoch, val score: -0.9203287030205182
[08/12/2025 18:01:36 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:01:55 INFO]: Training stats: {
    "score": -0.7244777372692162,
    "rmse": 0.7244777372692162
}
[08/12/2025 18:01:55 INFO]: Val stats: {
    "score": -0.8272718935296849,
    "rmse": 0.8272718935296849
}
[08/12/2025 18:01:55 INFO]: Test stats: {
    "score": -0.7323735385962707,
    "rmse": 0.7323735385962707
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:02:02 INFO]: Training loss at epoch 157: 0.2647877186536789
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:02:02 INFO]: Training loss at epoch 56: 0.27443309128284454
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:02:13 INFO]: New best epoch, val score: -0.7131177932894112
[08/12/2025 18:02:13 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:02:19 INFO]: Training loss at epoch 81: 0.3884696364402771
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
[08/12/2025 18:02:26 INFO]: Training loss at epoch 57: 0.5531542897224426
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:02:34 INFO]: Training loss at epoch 158: 0.274711549282074
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:02:38 INFO]: New best epoch, val score: -0.6680382958539373
[08/12/2025 18:02:38 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:02:52 INFO]: Training loss at epoch 90: 0.426469087600708
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:03:06 INFO]: Training loss at epoch 159: 0.26084938645362854
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:03:14 INFO]: Training loss at epoch 45: 0.5402203798294067
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:03:18 INFO]: Training stats: {
    "score": -0.47097246482968047,
    "rmse": 0.47097246482968047
}
[08/12/2025 18:03:18 INFO]: Val stats: {
    "score": -0.6689683206750056,
    "rmse": 0.6689683206750056
}
[08/12/2025 18:03:18 INFO]: Test stats: {
    "score": -0.6874688960710479,
    "rmse": 0.6874688960710479
}
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:03:22 INFO]: Training loss at epoch 82: 0.4217926114797592
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:03:32 INFO]: Training loss at epoch 57: 0.41509342193603516
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:03:42 INFO]: New best epoch, val score: -0.7063436735472588
[08/12/2025 18:03:42 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:03:48 INFO]: Training loss at epoch 91: 0.523872971534729
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:03:50 INFO]: Training loss at epoch 160: 0.22338557243347168
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:03:55 INFO]: Training loss at epoch 58: 0.48938728868961334
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:04:06 INFO]: New best epoch, val score: -0.7203663433129477
[08/12/2025 18:04:06 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:04:23 INFO]: Training loss at epoch 161: 0.2703695744276047
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:04:25 INFO]: Training loss at epoch 83: 0.3563317209482193
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:04:45 INFO]: Training loss at epoch 92: 0.4654330164194107
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:04:45 INFO]: Training loss at epoch 24: 0.83839151263237
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
[08/12/2025 18:04:56 INFO]: Training loss at epoch 162: 0.31694161891937256
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:05:01 INFO]: Training loss at epoch 58: 0.3777868449687958
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:05:08 INFO]: Training loss at epoch 46: 0.4572497010231018
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:05:25 INFO]: Training loss at epoch 59: 0.46174994111061096
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:05:27 INFO]: Training loss at epoch 84: 0.4400140345096588
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:05:28 INFO]: Training loss at epoch 163: 0.239780992269516
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:05:41 INFO]: Training loss at epoch 93: 0.49686838686466217
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
[08/12/2025 18:05:56 INFO]: Training stats: {
    "score": -0.6838648929381045,
    "rmse": 0.6838648929381045
}
[08/12/2025 18:05:56 INFO]: Val stats: {
    "score": -0.7188678326224608,
    "rmse": 0.7188678326224608
}
[08/12/2025 18:05:56 INFO]: Test stats: {
    "score": -0.6816964713782534,
    "rmse": 0.6816964713782534
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:06:01 INFO]: Training loss at epoch 164: 0.29945365339517593
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:06:05 INFO]: New best epoch, val score: -0.664662596938547
[08/12/2025 18:06:05 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:06:07 INFO]: New best epoch, val score: -0.7188678326224608
[08/12/2025 18:06:07 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:06:31 INFO]: Training loss at epoch 85: 0.33873286843299866
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:06:31 INFO]: Training loss at epoch 59: 0.42809776961803436
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:06:34 INFO]: Training loss at epoch 165: 0.22473958879709244
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:06:38 INFO]: Training loss at epoch 94: 0.4184761345386505
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:06:45 INFO]: New best epoch, val score: -0.7779258700811269
[08/12/2025 18:06:45 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:07:03 INFO]: Training loss at epoch 47: 0.7112447619438171
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:07:04 INFO]: Training stats: {
    "score": -0.6339252364494928,
    "rmse": 0.6339252364494928
}
[08/12/2025 18:07:04 INFO]: Val stats: {
    "score": -0.7891642266301775,
    "rmse": 0.7891642266301775
}
[08/12/2025 18:07:04 INFO]: Test stats: {
    "score": -0.6943842868004738,
    "rmse": 0.6943842868004738
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:07:07 INFO]: Training loss at epoch 166: 0.2183115854859352
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:07:27 INFO]: Training loss at epoch 60: 0.371841236948967
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:07:34 INFO]: Training loss at epoch 86: 0.38378138840198517
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:07:36 INFO]: Training loss at epoch 95: 0.4077887386083603
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:07:39 INFO]: Training loss at epoch 167: 0.2441227212548256
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:07:42 INFO]: New best epoch, val score: -0.7760471966183486
[08/12/2025 18:07:42 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:08:12 INFO]: Training loss at epoch 168: 0.21925338357686996
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:08:19 INFO]: Training loss at epoch 25: 0.9885717332363129
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
[08/12/2025 18:08:32 INFO]: Training loss at epoch 96: 0.42955872416496277
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:08:34 INFO]: Training loss at epoch 60: 0.4149849861860275
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:08:37 INFO]: Training loss at epoch 87: 0.40915098786354065
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:08:44 INFO]: Training loss at epoch 169: 0.1942124217748642
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:08:44 INFO]: New best epoch, val score: -0.7014049961920664
[08/12/2025 18:08:44 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:08:56 INFO]: Training loss at epoch 48: 0.6484946012496948
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:08:56 INFO]: Training stats: {
    "score": -0.4687720086620901,
    "rmse": 0.4687720086620901
}
[08/12/2025 18:08:56 INFO]: Val stats: {
    "score": -0.6884913744526395,
    "rmse": 0.6884913744526395
}
[08/12/2025 18:08:56 INFO]: Test stats: {
    "score": -0.707906867331831,
    "rmse": 0.707906867331831
}
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:08:56 INFO]: Training loss at epoch 61: 0.43565361201763153
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:09:29 INFO]: Training loss at epoch 170: 0.285947248339653
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:09:29 INFO]: Training loss at epoch 97: 0.36271868646144867
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:09:39 INFO]: Training loss at epoch 88: 0.4656153470277786
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:10:01 INFO]: Training loss at epoch 171: 0.25072481483221054
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:10:04 INFO]: Training loss at epoch 61: 0.34306491911411285
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:10:15 INFO]: New best epoch, val score: -0.6931440322356915
[08/12/2025 18:10:15 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:10:26 INFO]: Training loss at epoch 62: 0.5449346601963043
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:10:26 INFO]: Training loss at epoch 98: 0.4242965131998062
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:10:34 INFO]: Training loss at epoch 172: 0.24926788359880447
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:10:42 INFO]: Training loss at epoch 89: 0.4279125928878784
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:10:50 INFO]: Training loss at epoch 49: 0.5767411887645721
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:11:03 INFO]: Training stats: {
    "score": -0.6513115936961276,
    "rmse": 0.6513115936961276
}
[08/12/2025 18:11:03 INFO]: Val stats: {
    "score": -0.793163247715949,
    "rmse": 0.793163247715949
}
[08/12/2025 18:11:03 INFO]: Test stats: {
    "score": -0.7560905236371798,
    "rmse": 0.7560905236371798
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:11:06 INFO]: Training loss at epoch 173: 0.20684444159269333
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:11:22 INFO]: Training loss at epoch 99: 0.3299436494708061
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:11:28 INFO]: Training stats: {
    "score": -0.8246948152006885,
    "rmse": 0.8246948152006885
}
[08/12/2025 18:11:28 INFO]: Val stats: {
    "score": -0.9435896636233756,
    "rmse": 0.9435896636233756
}
[08/12/2025 18:11:28 INFO]: Test stats: {
    "score": -0.8368788906588562,
    "rmse": 0.8368788906588562
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:11:34 INFO]: Training loss at epoch 62: 0.3475010097026825
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:11:38 INFO]: Training loss at epoch 174: 0.2220577970147133
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:11:41 INFO]: Training stats: {
    "score": -0.6592133387093005,
    "rmse": 0.6592133387093005
}
[08/12/2025 18:11:41 INFO]: Val stats: {
    "score": -0.7797765615402998,
    "rmse": 0.7797765615402998
}
[08/12/2025 18:11:41 INFO]: Test stats: {
    "score": -0.6814823096754081,
    "rmse": 0.6814823096754081
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:11:42 INFO]: New best epoch, val score: -0.6600842648699469
[08/12/2025 18:11:42 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:11:53 INFO]: Training loss at epoch 26: 0.9898492991924286
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
[08/12/2025 18:11:55 INFO]: Training loss at epoch 63: 0.3558248281478882
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
[08/12/2025 18:12:05 INFO]: Training loss at epoch 90: 0.3193725496530533
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:12:06 INFO]: New best epoch, val score: -0.7156196006818601
[08/12/2025 18:12:06 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:12:11 INFO]: Training loss at epoch 175: 0.22403236478567123
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:12:13 INFO]: New best epoch, val score: -0.7734968465790847
[08/12/2025 18:12:13 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:12:38 INFO]: Training loss at epoch 100: 0.549412190914154
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:12:44 INFO]: Training loss at epoch 176: 0.23434241116046906
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:13:04 INFO]: Training loss at epoch 63: 0.32835324108600616
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:13:08 INFO]: Training loss at epoch 91: 0.33090998977422714
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:13:16 INFO]: New best epoch, val score: -0.7487499136785593
[08/12/2025 18:13:16 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:13:16 INFO]: Training loss at epoch 177: 0.18388623744249344
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:13:22 INFO]: Training loss at epoch 50: 0.7040209770202637
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:13:25 INFO]: Training loss at epoch 64: 0.5547627508640289
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:13:35 INFO]: Training loss at epoch 101: 0.5017790049314499
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:13:35 INFO]: New best epoch, val score: -0.7120447835838045
[08/12/2025 18:13:35 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:13:49 INFO]: Training loss at epoch 178: 0.26400458812713623
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:14:11 INFO]: Training loss at epoch 92: 0.36399032175540924
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:14:21 INFO]: Training loss at epoch 179: 0.3087303340435028
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:14:32 INFO]: Training loss at epoch 102: 0.37784741818904877
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:14:33 INFO]: Training stats: {
    "score": -0.43225311138495676,
    "rmse": 0.43225311138495676
}
[08/12/2025 18:14:33 INFO]: Val stats: {
    "score": -0.6877929106295146,
    "rmse": 0.6877929106295146
}
[08/12/2025 18:14:33 INFO]: Test stats: {
    "score": -0.7193343176062554,
    "rmse": 0.7193343176062554
}
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:14:34 INFO]: Training loss at epoch 64: 0.38994425535202026
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:14:54 INFO]: Training loss at epoch 65: 0.5089365243911743
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:15:05 INFO]: Training loss at epoch 180: 0.2725009173154831
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
[08/12/2025 18:15:14 INFO]: Training loss at epoch 93: 0.30090805143117905
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:15:16 INFO]: Training loss at epoch 51: 0.5145223140716553
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:15:26 INFO]: Training loss at epoch 27: 0.8669566214084625
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
[08/12/2025 18:15:28 INFO]: Training loss at epoch 103: 0.40117305517196655
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
[08/12/2025 18:15:38 INFO]: Training loss at epoch 181: 0.21389605849981308
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:16:04 INFO]: Training loss at epoch 65: 0.2940377965569496
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:16:12 INFO]: Training loss at epoch 182: 0.26610932499170303
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:16:16 INFO]: Training loss at epoch 94: 0.3067014440894127
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:16:23 INFO]: Training loss at epoch 66: 0.45444734394550323
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:16:23 INFO]: New best epoch, val score: -0.7377317327219521
[08/12/2025 18:16:23 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:16:26 INFO]: Training loss at epoch 104: 0.5360726118087769
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:16:32 INFO]: New best epoch, val score: -0.7741424238794382
[08/12/2025 18:16:32 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:16:46 INFO]: Training loss at epoch 183: 0.20005730539560318
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:17:08 INFO]: Training loss at epoch 52: 0.634813517332077
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:17:19 INFO]: Training loss at epoch 95: 0.41729016602039337
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:17:20 INFO]: Training loss at epoch 184: 0.20462623983621597
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:17:24 INFO]: New best epoch, val score: -0.6563093265111515
[08/12/2025 18:17:24 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:17:24 INFO]: Training loss at epoch 105: 0.4195142984390259
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:17:26 INFO]: New best epoch, val score: -0.7361710084513713
[08/12/2025 18:17:26 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:17:36 INFO]: Training loss at epoch 66: 0.36179301142692566
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:17:52 INFO]: Training loss at epoch 67: 0.4390339255332947
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:17:53 INFO]: Training loss at epoch 185: 0.2691745460033417
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:18:21 INFO]: Training loss at epoch 96: 0.2786189168691635
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:18:22 INFO]: Training loss at epoch 106: 0.4417410045862198
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:18:25 INFO]: Training loss at epoch 186: 0.20635618269443512
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:18:58 INFO]: Training loss at epoch 187: 0.29122649133205414
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:19:01 INFO]: Training loss at epoch 53: 0.5323713421821594
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:19:03 INFO]: Training loss at epoch 28: 0.9623197317123413
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:19:08 INFO]: Training loss at epoch 67: 0.31908711791038513
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:19:19 INFO]: New best epoch, val score: -0.6911285972565038
[08/12/2025 18:19:19 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:19:20 INFO]: Training loss at epoch 107: 0.5960701107978821
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:19:20 INFO]: Training loss at epoch 68: 0.3003644421696663
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:19:23 INFO]: Training loss at epoch 97: 0.360270231962204
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
[08/12/2025 18:19:30 INFO]: Training loss at epoch 188: 0.22355867177248
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:19:31 INFO]: New best epoch, val score: -0.7065553532223345
[08/12/2025 18:19:31 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:20:02 INFO]: Training loss at epoch 189: 0.27376826107501984
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:20:14 INFO]: Training stats: {
    "score": -0.43953412476991804,
    "rmse": 0.43953412476991804
}
[08/12/2025 18:20:14 INFO]: Val stats: {
    "score": -0.6852112734855671,
    "rmse": 0.6852112734855671
}
[08/12/2025 18:20:14 INFO]: Test stats: {
    "score": -0.7126878033747983,
    "rmse": 0.7126878033747983
}
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:20:16 INFO]: Training loss at epoch 108: 0.4532318562269211
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:20:23 INFO]: New best epoch, val score: -0.7668587570228127
[08/12/2025 18:20:23 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:20:26 INFO]: Training loss at epoch 98: 0.5655952394008636
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:20:42 INFO]: Training loss at epoch 68: 0.24792903661727905
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:20:47 INFO]: Training loss at epoch 190: 0.195139080286026
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:20:49 INFO]: Training loss at epoch 69: 0.43372562527656555
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:20:53 INFO]: New best epoch, val score: -0.6671002962690751
[08/12/2025 18:20:53 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:20:54 INFO]: Training loss at epoch 54: 0.4661221355199814
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:21:14 INFO]: Training loss at epoch 109: 0.4057270586490631
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:21:19 INFO]: Training loss at epoch 191: 0.2583960071206093
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:21:20 INFO]: Training stats: {
    "score": -0.6779734831100447,
    "rmse": 0.6779734831100447
}
[08/12/2025 18:21:20 INFO]: Val stats: {
    "score": -0.7089550311851444,
    "rmse": 0.7089550311851444
}
[08/12/2025 18:21:20 INFO]: Test stats: {
    "score": -0.6789896796508431,
    "rmse": 0.6789896796508431
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:21:27 INFO]: Training loss at epoch 99: 0.24102108925580978
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:21:34 INFO]: Training stats: {
    "score": -0.6982046370007721,
    "rmse": 0.6982046370007721
}
[08/12/2025 18:21:34 INFO]: Val stats: {
    "score": -0.7729714088079261,
    "rmse": 0.7729714088079261
}
[08/12/2025 18:21:34 INFO]: Test stats: {
    "score": -0.7199592559311635,
    "rmse": 0.7199592559311635
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:21:48 INFO]: Training stats: {
    "score": -0.6206162293646615,
    "rmse": 0.6206162293646615
}
[08/12/2025 18:21:48 INFO]: Val stats: {
    "score": -0.7605630621986391,
    "rmse": 0.7605630621986391
}
[08/12/2025 18:21:48 INFO]: Test stats: {
    "score": -0.7344453961548332,
    "rmse": 0.7344453961548332
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:21:52 INFO]: Training loss at epoch 192: 0.25562314689159393
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:22:16 INFO]: Training loss at epoch 69: 0.42315053939819336
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:22:25 INFO]: Training loss at epoch 193: 0.19671960175037384
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:22:31 INFO]: Training loss at epoch 110: 0.5077821016311646
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:22:39 INFO]: Training loss at epoch 29: 0.6953215599060059
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:22:46 INFO]: Training loss at epoch 55: 0.5387063920497894
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:22:48 INFO]: Training stats: {
    "score": -0.560288772330697,
    "rmse": 0.560288772330697
}
[08/12/2025 18:22:48 INFO]: Val stats: {
    "score": -0.6718821518025118,
    "rmse": 0.6718821518025118
}
[08/12/2025 18:22:48 INFO]: Test stats: {
    "score": -0.6592192197039405,
    "rmse": 0.6592192197039405
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:22:48 INFO]: Training loss at epoch 70: 0.4637931138277054
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:22:50 INFO]: Training loss at epoch 100: 0.26613418757915497
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:22:57 INFO]: Training loss at epoch 194: 0.2573898807168007
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:22:59 INFO]: New best epoch, val score: -0.7932835036991058
[08/12/2025 18:22:59 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:23:29 INFO]: Training loss at epoch 111: 0.35557566583156586
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:23:30 INFO]: Training loss at epoch 195: 0.28448329865932465
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:23:34 INFO]: New best epoch, val score: -0.6509677459354295
[08/12/2025 18:23:34 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:23:52 INFO]: Training loss at epoch 101: 0.4534773826599121
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:23:53 INFO]: Training stats: {
    "score": -0.9535013378104845,
    "rmse": 0.9535013378104845
}
[08/12/2025 18:23:53 INFO]: Val stats: {
    "score": -0.9137182469346435,
    "rmse": 0.9137182469346435
}
[08/12/2025 18:23:53 INFO]: Test stats: {
    "score": -0.8797588953554609,
    "rmse": 0.8797588953554609
}
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:24:03 INFO]: Training loss at epoch 196: 0.2437559738755226
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:24:07 INFO]: New best epoch, val score: -0.6476827362062588
[08/12/2025 18:24:07 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:24:16 INFO]: Training loss at epoch 71: 0.5181768834590912
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:24:19 INFO]: New best epoch, val score: -0.9137182469346435
[08/12/2025 18:24:19 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
[08/12/2025 18:24:20 INFO]: Training loss at epoch 70: 0.301888182759285
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:24:27 INFO]: Training loss at epoch 112: 0.4027773588895798
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:24:36 INFO]: Training loss at epoch 197: 0.2800348401069641
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:24:38 INFO]: Training loss at epoch 56: 0.645832359790802
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:24:53 INFO]: Training loss at epoch 102: 0.382093608379364
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
[08/12/2025 18:25:09 INFO]: Training loss at epoch 198: 0.29543131589889526
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:25:25 INFO]: Training loss at epoch 113: 0.3852149695158005
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:25:41 INFO]: Training loss at epoch 199: 0.23913652449846268
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:25:44 INFO]: Training loss at epoch 72: 0.38212642073631287
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:25:52 INFO]: Training loss at epoch 71: 0.3558236062526703
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 64])
Tokenizer  -1: 64
[08/12/2025 18:25:53 INFO]: Training stats: {
    "score": -0.42346095591515687,
    "rmse": 0.42346095591515687
}
[08/12/2025 18:25:53 INFO]: Val stats: {
    "score": -0.6991206253970647,
    "rmse": 0.6991206253970647
}
[08/12/2025 18:25:53 INFO]: Test stats: {
    "score": -0.6911931718743242,
    "rmse": 0.6911931718743242
}
Tokenizer output shape: torch.Size([66, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([87, 101, 64])
Tokenizer  -1: 64
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:25:55 INFO]: Training loss at epoch 103: 0.33683159947395325
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:25:57 INFO]: Running Final Evaluation...
[W 2025-08-12 18:25:58,543] Trial 3 failed with parameters: {'embedding_head_pair': (64, 8), 'n_layers': 10, 'd_ffn_factor': 1.4720777899620607, 'attention_dropout': 0.1468524134062773, 'activation': 'reglu', 'lr': 0.0002636210080291049} because of the following error: RuntimeError('Error(s) in loading state_dict for FTTransformer:\n\tMissing key(s) in state_dict: "layers.8.attention.W_q.weight", "layers.8.attention.W_q.bias", "layers.8.attention.W_k.weight", "layers.8.attention.W_k.bias", "layers.8.attention.W_v.weight", "layers.8.attention.W_v.bias", "layers.8.attention.W_out.weight", "layers.8.attention.W_out.bias", "layers.8.linear0.weight", "layers.8.linear0.bias", "layers.8.linear1.weight", "layers.8.linear1.bias", "layers.8.norm1.weight", "layers.8.norm1.bias", "layers.8.norm0.weight", "layers.8.norm0.bias", "layers.9.attention.W_q.weight", "layers.9.attention.W_q.bias", "layers.9.attention.W_k.weight", "layers.9.attention.W_k.bias", "layers.9.attention.W_v.weight", "layers.9.attention.W_v.bias", "layers.9.attention.W_out.weight", "layers.9.attention.W_out.bias", "layers.9.linear0.weight", "layers.9.linear0.bias", "layers.9.linear1.weight", "layers.9.linear1.bias", "layers.9.norm1.weight", "layers.9.norm1.bias", "layers.9.norm0.weight", "layers.9.norm0.bias". \n\tsize mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 512]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 512]) from checkpoint, the shape in current model is torch.Size([100, 64]).\n\tsize mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([188, 64]).\n\tsize mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([188]).\n\tsize mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([64, 94]).\n\tsize mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([188, 64]).\n\tsize mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([188]).\n\tsize mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([64, 94]).\n\tsize mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.2.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.2.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.2.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.2.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.2.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.2.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.2.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.2.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.2.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([188, 64]).\n\tsize mismatch for layers.2.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([188]).\n\tsize mismatch for layers.2.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([64, 94]).\n\tsize mismatch for layers.2.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.2.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.2.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.2.norm0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.2.norm0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.3.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.3.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.3.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.3.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.3.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.3.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.3.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.3.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.3.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([188, 64]).\n\tsize mismatch for layers.3.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([188]).\n\tsize mismatch for layers.3.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([64, 94]).\n\tsize mismatch for layers.3.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.3.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.3.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.3.norm0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.3.norm0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.4.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.4.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.4.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.4.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.4.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.4.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.4.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.4.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.4.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([188, 64]).\n\tsize mismatch for layers.4.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([188]).\n\tsize mismatch for layers.4.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([64, 94]).\n\tsize mismatch for layers.4.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.4.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.4.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.4.norm0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.4.norm0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.5.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.5.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.5.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.5.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.5.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.5.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.5.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.5.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.5.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([188, 64]).\n\tsize mismatch for layers.5.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([188]).\n\tsize mismatch for layers.5.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([64, 94]).\n\tsize mismatch for layers.5.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.5.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.5.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.5.norm0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.5.norm0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.6.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.6.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.6.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.6.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.6.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.6.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.6.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.6.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.6.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([188, 64]).\n\tsize mismatch for layers.6.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([188]).\n\tsize mismatch for layers.6.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([64, 94]).\n\tsize mismatch for layers.6.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.6.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.6.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.6.norm0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.6.norm0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.7.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.7.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.7.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.7.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.7.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.7.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.7.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for layers.7.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.7.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([188, 64]).\n\tsize mismatch for layers.7.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([188]).\n\tsize mismatch for layers.7.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([64, 94]).\n\tsize mismatch for layers.7.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.7.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.7.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.7.norm0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layers.7.norm0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for last_normalization.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for last_normalization.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for head.weight: copying a param with shape torch.Size([1, 512]) from checkpoint, the shape in current model is torch.Size([1, 64]).').
Traceback (most recent call last):
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 145, in <lambda>
    func = lambda trial: objective(trial, cfg, trial_stats, trial_counter, n_optuna_trials,
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 104, in objective
    stats = train_net_for_optuna.main(config, loaders, unique_categories, n_numerical, n_classes)
  File "/temporario2/14570653/tabular-transfer-learning/train_net_for_optuna.py", line 127, in main
    net.load_state_dict(torch.load(checkpoint_path)["net"])
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2215, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for FTTransformer:
	Missing key(s) in state_dict: "layers.8.attention.W_q.weight", "layers.8.attention.W_q.bias", "layers.8.attention.W_k.weight", "layers.8.attention.W_k.bias", "layers.8.attention.W_v.weight", "layers.8.attention.W_v.bias", "layers.8.attention.W_out.weight", "layers.8.attention.W_out.bias", "layers.8.linear0.weight", "layers.8.linear0.bias", "layers.8.linear1.weight", "layers.8.linear1.bias", "layers.8.norm1.weight", "layers.8.norm1.bias", "layers.8.norm0.weight", "layers.8.norm0.bias", "layers.9.attention.W_q.weight", "layers.9.attention.W_q.bias", "layers.9.attention.W_k.weight", "layers.9.attention.W_k.bias", "layers.9.attention.W_v.weight", "layers.9.attention.W_v.bias", "layers.9.attention.W_out.weight", "layers.9.attention.W_out.bias", "layers.9.linear0.weight", "layers.9.linear0.bias", "layers.9.linear1.weight", "layers.9.linear1.bias", "layers.9.norm1.weight", "layers.9.norm1.bias", "layers.9.norm0.weight", "layers.9.norm0.bias". 
	size mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 512]) from checkpoint, the shape in current model is torch.Size([101, 64]).
	size mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 512]) from checkpoint, the shape in current model is torch.Size([100, 64]).
	size mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([188, 64]).
	size mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([188]).
	size mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([64, 94]).
	size mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([188, 64]).
	size mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([188]).
	size mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([64, 94]).
	size mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.2.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.2.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.2.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.2.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.2.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.2.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.2.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.2.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.2.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([188, 64]).
	size mismatch for layers.2.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([188]).
	size mismatch for layers.2.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([64, 94]).
	size mismatch for layers.2.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.2.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.2.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.2.norm0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.2.norm0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.3.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.3.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.3.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.3.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.3.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.3.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.3.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.3.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.3.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([188, 64]).
	size mismatch for layers.3.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([188]).
	size mismatch for layers.3.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([64, 94]).
	size mismatch for layers.3.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.3.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.3.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.3.norm0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.3.norm0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.4.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.4.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.4.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.4.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.4.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.4.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.4.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.4.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.4.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([188, 64]).
	size mismatch for layers.4.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([188]).
	size mismatch for layers.4.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([64, 94]).
	size mismatch for layers.4.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.4.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.4.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.4.norm0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.4.norm0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.5.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.5.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.5.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.5.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.5.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.5.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.5.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.5.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.5.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([188, 64]).
	size mismatch for layers.5.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([188]).
	size mismatch for layers.5.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([64, 94]).
	size mismatch for layers.5.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.5.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.5.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.5.norm0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.5.norm0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.6.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.6.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.6.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.6.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.6.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.6.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.6.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.6.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.6.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([188, 64]).
	size mismatch for layers.6.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([188]).
	size mismatch for layers.6.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([64, 94]).
	size mismatch for layers.6.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.6.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.6.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.6.norm0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.6.norm0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.7.attention.W_q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.7.attention.W_q.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.7.attention.W_k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.7.attention.W_k.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.7.attention.W_v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.7.attention.W_v.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.7.attention.W_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for layers.7.attention.W_out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.7.linear0.weight: copying a param with shape torch.Size([681, 512]) from checkpoint, the shape in current model is torch.Size([188, 64]).
	size mismatch for layers.7.linear0.bias: copying a param with shape torch.Size([681]) from checkpoint, the shape in current model is torch.Size([188]).
	size mismatch for layers.7.linear1.weight: copying a param with shape torch.Size([512, 681]) from checkpoint, the shape in current model is torch.Size([64, 94]).
	size mismatch for layers.7.linear1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.7.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.7.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.7.norm0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.7.norm0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for last_normalization.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for last_normalization.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for head.weight: copying a param with shape torch.Size([1, 512]) from checkpoint, the shape in current model is torch.Size([1, 64]).
[W 2025-08-12 18:25:58,550] Trial 3 failed with value None.
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:26:03 INFO]: New best epoch, val score: -0.6619455551175778
[08/12/2025 18:26:03 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:26:22 INFO]: Training loss at epoch 114: 0.5080308020114899
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:26:30 INFO]: Training loss at epoch 57: 0.5507809519767761
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:26:57 INFO]: Training loss at epoch 104: 0.38348188996315
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:27:12 INFO]: Training loss at epoch 73: 0.29459601640701294
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:27:19 INFO]: Training loss at epoch 115: 0.365483358502388
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:27:22 INFO]: Training loss at epoch 72: 0.3153277635574341
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:27:23 INFO]: New best epoch, val score: -0.6878734767226293
[08/12/2025 18:27:23 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:27:30 INFO]: Training loss at epoch 30: 1.0115154683589935
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
[08/12/2025 18:27:33 INFO]: New best epoch, val score: -0.6592845513179906
[08/12/2025 18:27:33 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:27:57 INFO]: New best epoch, val score: -0.9101954951187935
[08/12/2025 18:27:57 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
[08/12/2025 18:28:02 INFO]: Training loss at epoch 105: 0.27871596068143845
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:28:18 INFO]: Training loss at epoch 116: 0.3909645676612854
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:28:22 INFO]: Training loss at epoch 58: 0.5244808942079544
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:28:35 INFO]: New best epoch, val score: -0.7761914866685409
[08/12/2025 18:28:35 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:28:42 INFO]: Training loss at epoch 74: 0.3693079501390457
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:28:54 INFO]: Training loss at epoch 73: 0.31061114370822906
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:29:04 INFO]: Training loss at epoch 106: 0.35686154663562775
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:29:15 INFO]: Training loss at epoch 117: 0.3059713840484619
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:30:06 INFO]: Training loss at epoch 107: 0.49638718366622925
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:30:10 INFO]: Training loss at epoch 75: 0.41012734174728394
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:30:11 INFO]: Training loss at epoch 118: 0.41833382844924927
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:30:15 INFO]: Training loss at epoch 59: 0.4674380421638489
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:30:23 INFO]: Training loss at epoch 74: 0.2826412171125412
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:30:53 INFO]: Training stats: {
    "score": -0.6967008243229076,
    "rmse": 0.6967008243229076
}
[08/12/2025 18:30:53 INFO]: Val stats: {
    "score": -0.7926642502459549,
    "rmse": 0.7926642502459549
}
[08/12/2025 18:30:53 INFO]: Test stats: {
    "score": -0.6994691982280133,
    "rmse": 0.6994691982280133
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:31:06 INFO]: Training loss at epoch 31: 1.0195261538028717
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
[08/12/2025 18:31:08 INFO]: Training loss at epoch 108: 0.35626180469989777
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:31:08 INFO]: Training loss at epoch 119: 0.3534848093986511
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:31:27 INFO]: Training stats: {
    "score": -0.6544901103687875,
    "rmse": 0.6544901103687875
}
[08/12/2025 18:31:27 INFO]: Val stats: {
    "score": -0.781776310485318,
    "rmse": 0.781776310485318
}
[08/12/2025 18:31:27 INFO]: Test stats: {
    "score": -0.7057812480009914,
    "rmse": 0.7057812480009914
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:31:37 INFO]: Training loss at epoch 76: 0.36068879067897797
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:31:53 INFO]: Training loss at epoch 75: 0.23682452738285065
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:32:09 INFO]: Training loss at epoch 109: 0.4035913646221161
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:32:23 INFO]: Training loss at epoch 120: 0.47927461564540863
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:32:30 INFO]: Training stats: {
    "score": -0.6094079882205564,
    "rmse": 0.6094079882205564
}
[08/12/2025 18:32:30 INFO]: Val stats: {
    "score": -0.7765935865414944,
    "rmse": 0.7765935865414944
}
[08/12/2025 18:32:30 INFO]: Test stats: {
    "score": -0.7513653919388353,
    "rmse": 0.7513653919388353
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:32:44 INFO]: Training loss at epoch 60: 0.5024705529212952
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:32:57 INFO]: New best epoch, val score: -0.762789475017111
[08/12/2025 18:32:57 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:33:05 INFO]: Training loss at epoch 77: 0.4771481156349182
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:33:20 INFO]: Training loss at epoch 121: 0.3677881509065628
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:33:23 INFO]: Training loss at epoch 76: 0.4298197478055954
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:33:27 INFO]: New best epoch, val score: -0.7573087505190148
[08/12/2025 18:33:27 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:33:32 INFO]: Training loss at epoch 110: 0.32081905007362366
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:34:17 INFO]: Training loss at epoch 122: 0.4006074517965317
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:34:23 INFO]: New best epoch, val score: -0.7301035585410187
[08/12/2025 18:34:23 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:34:33 INFO]: Training loss at epoch 111: 0.40030622482299805
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:34:33 INFO]: Training loss at epoch 78: 0.3035258874297142
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:34:37 INFO]: Training loss at epoch 61: 0.4639611542224884
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:34:39 INFO]: Training loss at epoch 32: 0.9294750094413757
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:34:44 INFO]: New best epoch, val score: -0.679414080156605
[08/12/2025 18:34:44 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
[08/12/2025 18:34:52 INFO]: Training loss at epoch 77: 0.37453386187553406
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:35:13 INFO]: Training loss at epoch 123: 0.3521661013364792
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:35:20 INFO]: New best epoch, val score: -0.7288612114461033
[08/12/2025 18:35:20 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:35:35 INFO]: Training loss at epoch 112: 0.3657867908477783
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:36:01 INFO]: Training loss at epoch 79: 0.3086753711104393
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:36:10 INFO]: Training loss at epoch 124: 0.31510621309280396
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:36:22 INFO]: Training loss at epoch 78: 0.43203073740005493
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:36:28 INFO]: Training loss at epoch 62: 0.41369472444057465
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:36:32 INFO]: Training stats: {
    "score": -0.5888465711067904,
    "rmse": 0.5888465711067904
}
[08/12/2025 18:36:32 INFO]: Val stats: {
    "score": -0.674864254847113,
    "rmse": 0.674864254847113
}
[08/12/2025 18:36:32 INFO]: Test stats: {
    "score": -0.6298363812499546,
    "rmse": 0.6298363812499546
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:36:33 INFO]: New best epoch, val score: -0.6578428416323616
[08/12/2025 18:36:33 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:36:37 INFO]: Training loss at epoch 113: 0.364359587430954
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:36:43 INFO]: New best epoch, val score: -0.674864254847113
[08/12/2025 18:36:43 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:37:06 INFO]: Training loss at epoch 125: 0.31434354186058044
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:37:39 INFO]: Training loss at epoch 114: 0.25358471274375916
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:37:51 INFO]: Training loss at epoch 79: 0.30479925870895386
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:38:01 INFO]: Training loss at epoch 80: 0.39855216443538666
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:38:03 INFO]: Training loss at epoch 126: 0.2918471693992615
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:38:09 INFO]: New best epoch, val score: -0.7241961290365655
[08/12/2025 18:38:09 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:38:12 INFO]: Training loss at epoch 33: 0.7499132752418518
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:38:20 INFO]: Training loss at epoch 63: 0.5714734792709351
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:38:22 INFO]: Training stats: {
    "score": -0.5491341638659143,
    "rmse": 0.5491341638659143
}
[08/12/2025 18:38:22 INFO]: Val stats: {
    "score": -0.6599941536634785,
    "rmse": 0.6599941536634785
}
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:38:22 INFO]: Test stats: {
    "score": -0.6178454151501561,
    "rmse": 0.6178454151501561
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
[08/12/2025 18:38:40 INFO]: Training loss at epoch 115: 0.3420831114053726
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:38:59 INFO]: Training loss at epoch 127: 0.4204360395669937
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
[08/12/2025 18:39:28 INFO]: Training loss at epoch 81: 0.30065523087978363
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:39:42 INFO]: Training loss at epoch 116: 0.2897470146417618
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:39:52 INFO]: Training loss at epoch 80: 0.2875644415616989
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:39:56 INFO]: Training loss at epoch 128: 0.3086392432451248
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:40:12 INFO]: Training loss at epoch 64: 0.5211325585842133
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:40:25 INFO]: New best epoch, val score: -0.7330129420424072
[08/12/2025 18:40:25 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:40:43 INFO]: Training loss at epoch 117: 0.3127019852399826
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:40:52 INFO]: Training loss at epoch 129: 0.27451251447200775
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:40:56 INFO]: Training loss at epoch 82: 0.39414162933826447
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:41:11 INFO]: Training stats: {
    "score": -0.6640954816216001,
    "rmse": 0.6640954816216001
}
[08/12/2025 18:41:11 INFO]: Val stats: {
    "score": -0.8145826167109844,
    "rmse": 0.8145826167109844
}
[08/12/2025 18:41:11 INFO]: Test stats: {
    "score": -0.7388835963555569,
    "rmse": 0.7388835963555569
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:41:21 INFO]: Training loss at epoch 81: 0.275886707007885
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:41:44 INFO]: Training loss at epoch 34: 0.8348799347877502
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
[08/12/2025 18:41:45 INFO]: Training loss at epoch 118: 0.25174491852521896
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:42:03 INFO]: Training loss at epoch 65: 0.5480281710624695
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:42:07 INFO]: Training loss at epoch 130: 0.3277035355567932
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:42:09 INFO]: New best epoch, val score: -0.9077768925485449
[08/12/2025 18:42:09 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:42:25 INFO]: Training loss at epoch 83: 0.25797075033187866
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:42:47 INFO]: Training loss at epoch 119: 0.24097701162099838
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:42:52 INFO]: Training loss at epoch 82: 0.26184166222810745
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:43:05 INFO]: Training loss at epoch 131: 0.321634441614151
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:43:08 INFO]: Training stats: {
    "score": -0.6394737563632273,
    "rmse": 0.6394737563632273
}
[08/12/2025 18:43:08 INFO]: Val stats: {
    "score": -0.782812517164365,
    "rmse": 0.782812517164365
}
[08/12/2025 18:43:08 INFO]: Test stats: {
    "score": -0.7965085990820862,
    "rmse": 0.7965085990820862
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:43:52 INFO]: Training loss at epoch 84: 0.4106768071651459
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:43:56 INFO]: Training loss at epoch 66: 0.5624389350414276
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:44:01 INFO]: Training loss at epoch 132: 0.46384747326374054
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:44:03 INFO]: New best epoch, val score: -0.6717841214640594
[08/12/2025 18:44:03 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:44:10 INFO]: Training loss at epoch 120: 0.36331064999103546
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:44:21 INFO]: Training loss at epoch 83: 0.2543851062655449
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:44:58 INFO]: Training loss at epoch 133: 0.3389967083930969
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:45:11 INFO]: Training loss at epoch 121: 0.28082774579524994
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:45:17 INFO]: Training loss at epoch 35: 0.8587665557861328
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:45:20 INFO]: Training loss at epoch 85: 0.29013004153966904
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:45:43 INFO]: New best epoch, val score: -0.8935706421910246
[08/12/2025 18:45:43 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:45:49 INFO]: Training loss at epoch 67: 0.5646806359291077
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:45:50 INFO]: Training loss at epoch 84: 0.3090771287679672
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:45:54 INFO]: Training loss at epoch 134: 0.5009725391864777
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:46:13 INFO]: Training loss at epoch 122: 0.3139685243368149
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:46:49 INFO]: Training loss at epoch 86: 0.2823575362563133
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:46:50 INFO]: Training loss at epoch 135: 0.38331276178359985
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:46:57 INFO]: New best epoch, val score: -0.7135271493379034
[08/12/2025 18:46:57 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:47:15 INFO]: Training loss at epoch 123: 0.2868959829211235
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:47:19 INFO]: Training loss at epoch 85: 0.31824061274528503
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:47:40 INFO]: Training loss at epoch 68: 0.3675697222352028
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:47:47 INFO]: Training loss at epoch 136: 0.3399537652730942
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:47:54 INFO]: New best epoch, val score: -0.7042700482331343
[08/12/2025 18:47:54 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:48:17 INFO]: Training loss at epoch 124: 0.4473562091588974
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:48:17 INFO]: Training loss at epoch 87: 0.41816820204257965
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:48:44 INFO]: Training loss at epoch 137: 0.36408287286758423
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:48:49 INFO]: Training loss at epoch 86: 0.34095677733421326
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:48:50 INFO]: Training loss at epoch 36: 0.7838562726974487
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:49:16 INFO]: New best epoch, val score: -0.8662573472458149
[08/12/2025 18:49:16 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
[08/12/2025 18:49:19 INFO]: Training loss at epoch 125: 0.4718140810728073
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:49:33 INFO]: Training loss at epoch 69: 0.5328655242919922
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:49:41 INFO]: Training loss at epoch 138: 0.37585748732089996
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:49:46 INFO]: Training loss at epoch 88: 0.2934481054544449
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:49:56 INFO]: New best epoch, val score: -0.6597043969089978
[08/12/2025 18:49:56 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:50:11 INFO]: Training stats: {
    "score": -0.6990221338734667,
    "rmse": 0.6990221338734667
}
[08/12/2025 18:50:11 INFO]: Val stats: {
    "score": -0.7282690900633291,
    "rmse": 0.7282690900633291
}
[08/12/2025 18:50:11 INFO]: Test stats: {
    "score": -0.7110822293569447,
    "rmse": 0.7110822293569447
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:50:19 INFO]: Training loss at epoch 87: 0.3046853244304657
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:50:21 INFO]: Training loss at epoch 126: 0.28105227649211884
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:50:24 INFO]: New best epoch, val score: -0.7282690900633291
[08/12/2025 18:50:24 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:50:28 INFO]: Running Final Evaluation...
[W 2025-08-12 18:50:29,185] Trial 2 failed with parameters: {'embedding_head_pair': (384, 8), 'n_layers': 4, 'd_ffn_factor': 1.4385528778624406, 'attention_dropout': 0.36743552563799553, 'activation': 'gelu', 'lr': 0.00015248333260109957} because of the following error: RuntimeError('Error(s) in loading state_dict for FTTransformer:\n\tUnexpected key(s) in state_dict: "layers.4.attention.W_q.weight", "layers.4.attention.W_q.bias", "layers.4.attention.W_k.weight", "layers.4.attention.W_k.bias", "layers.4.attention.W_v.weight", "layers.4.attention.W_v.bias", "layers.4.attention.W_out.weight", "layers.4.attention.W_out.bias", "layers.4.linear0.weight", "layers.4.linear0.bias", "layers.4.linear1.weight", "layers.4.linear1.bias", "layers.4.norm1.weight", "layers.4.norm1.bias", "layers.4.norm0.weight", "layers.4.norm0.bias", "layers.5.attention.W_q.weight", "layers.5.attention.W_q.bias", "layers.5.attention.W_k.weight", "layers.5.attention.W_k.bias", "layers.5.attention.W_v.weight", "layers.5.attention.W_v.bias", "layers.5.attention.W_out.weight", "layers.5.attention.W_out.bias", "layers.5.linear0.weight", "layers.5.linear0.bias", "layers.5.linear1.weight", "layers.5.linear1.bias", "layers.5.norm1.weight", "layers.5.norm1.bias", "layers.5.norm0.weight", "layers.5.norm0.bias", "layers.6.attention.W_q.weight", "layers.6.attention.W_q.bias", "layers.6.attention.W_k.weight", "layers.6.attention.W_k.bias", "layers.6.attention.W_v.weight", "layers.6.attention.W_v.bias", "layers.6.attention.W_out.weight", "layers.6.attention.W_out.bias", "layers.6.linear0.weight", "layers.6.linear0.bias", "layers.6.linear1.weight", "layers.6.linear1.bias", "layers.6.norm1.weight", "layers.6.norm1.bias", "layers.6.norm0.weight", "layers.6.norm0.bias", "layers.7.attention.W_q.weight", "layers.7.attention.W_q.bias", "layers.7.attention.W_k.weight", "layers.7.attention.W_k.bias", "layers.7.attention.W_v.weight", "layers.7.attention.W_v.bias", "layers.7.attention.W_out.weight", "layers.7.attention.W_out.bias", "layers.7.linear0.weight", "layers.7.linear0.bias", "layers.7.linear1.weight", "layers.7.linear1.bias", "layers.7.norm1.weight", "layers.7.norm1.bias", "layers.7.norm0.weight", "layers.7.norm0.bias", "layers.8.attention.W_q.weight", "layers.8.attention.W_q.bias", "layers.8.attention.W_k.weight", "layers.8.attention.W_k.bias", "layers.8.attention.W_v.weight", "layers.8.attention.W_v.bias", "layers.8.attention.W_out.weight", "layers.8.attention.W_out.bias", "layers.8.linear0.weight", "layers.8.linear0.bias", "layers.8.linear1.weight", "layers.8.linear1.bias", "layers.8.norm1.weight", "layers.8.norm1.bias", "layers.8.norm0.weight", "layers.8.norm0.bias", "layers.9.attention.W_q.weight", "layers.9.attention.W_q.bias", "layers.9.attention.W_k.weight", "layers.9.attention.W_k.bias", "layers.9.attention.W_v.weight", "layers.9.attention.W_v.bias", "layers.9.attention.W_out.weight", "layers.9.attention.W_out.bias", "layers.9.linear0.weight", "layers.9.linear0.bias", "layers.9.linear1.weight", "layers.9.linear1.bias", "layers.9.norm1.weight", "layers.9.norm1.bias", "layers.9.norm0.weight", "layers.9.norm0.bias". \n\tsize mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 256]) from checkpoint, the shape in current model is torch.Size([101, 384]).\n\tsize mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 256]) from checkpoint, the shape in current model is torch.Size([100, 384]).\n\tsize mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([552, 384]).\n\tsize mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([552]).\n\tsize mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([384, 552]).\n\tsize mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([552, 384]).\n\tsize mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([552]).\n\tsize mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([384, 552]).\n\tsize mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.2.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.2.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.2.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.2.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([552, 384]).\n\tsize mismatch for layers.2.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([552]).\n\tsize mismatch for layers.2.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([384, 552]).\n\tsize mismatch for layers.2.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.3.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.3.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.3.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.3.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([552, 384]).\n\tsize mismatch for layers.3.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([552]).\n\tsize mismatch for layers.3.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([384, 552]).\n\tsize mismatch for layers.3.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for last_normalization.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for last_normalization.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for head.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 384]).').
Traceback (most recent call last):
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 145, in <lambda>
    func = lambda trial: objective(trial, cfg, trial_stats, trial_counter, n_optuna_trials,
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 104, in objective
    stats = train_net_for_optuna.main(config, loaders, unique_categories, n_numerical, n_classes)
  File "/temporario2/14570653/tabular-transfer-learning/train_net_for_optuna.py", line 127, in main
    net.load_state_dict(torch.load(checkpoint_path)["net"])
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2215, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for FTTransformer:
	Unexpected key(s) in state_dict: "layers.4.attention.W_q.weight", "layers.4.attention.W_q.bias", "layers.4.attention.W_k.weight", "layers.4.attention.W_k.bias", "layers.4.attention.W_v.weight", "layers.4.attention.W_v.bias", "layers.4.attention.W_out.weight", "layers.4.attention.W_out.bias", "layers.4.linear0.weight", "layers.4.linear0.bias", "layers.4.linear1.weight", "layers.4.linear1.bias", "layers.4.norm1.weight", "layers.4.norm1.bias", "layers.4.norm0.weight", "layers.4.norm0.bias", "layers.5.attention.W_q.weight", "layers.5.attention.W_q.bias", "layers.5.attention.W_k.weight", "layers.5.attention.W_k.bias", "layers.5.attention.W_v.weight", "layers.5.attention.W_v.bias", "layers.5.attention.W_out.weight", "layers.5.attention.W_out.bias", "layers.5.linear0.weight", "layers.5.linear0.bias", "layers.5.linear1.weight", "layers.5.linear1.bias", "layers.5.norm1.weight", "layers.5.norm1.bias", "layers.5.norm0.weight", "layers.5.norm0.bias", "layers.6.attention.W_q.weight", "layers.6.attention.W_q.bias", "layers.6.attention.W_k.weight", "layers.6.attention.W_k.bias", "layers.6.attention.W_v.weight", "layers.6.attention.W_v.bias", "layers.6.attention.W_out.weight", "layers.6.attention.W_out.bias", "layers.6.linear0.weight", "layers.6.linear0.bias", "layers.6.linear1.weight", "layers.6.linear1.bias", "layers.6.norm1.weight", "layers.6.norm1.bias", "layers.6.norm0.weight", "layers.6.norm0.bias", "layers.7.attention.W_q.weight", "layers.7.attention.W_q.bias", "layers.7.attention.W_k.weight", "layers.7.attention.W_k.bias", "layers.7.attention.W_v.weight", "layers.7.attention.W_v.bias", "layers.7.attention.W_out.weight", "layers.7.attention.W_out.bias", "layers.7.linear0.weight", "layers.7.linear0.bias", "layers.7.linear1.weight", "layers.7.linear1.bias", "layers.7.norm1.weight", "layers.7.norm1.bias", "layers.7.norm0.weight", "layers.7.norm0.bias", "layers.8.attention.W_q.weight", "layers.8.attention.W_q.bias", "layers.8.attention.W_k.weight", "layers.8.attention.W_k.bias", "layers.8.attention.W_v.weight", "layers.8.attention.W_v.bias", "layers.8.attention.W_out.weight", "layers.8.attention.W_out.bias", "layers.8.linear0.weight", "layers.8.linear0.bias", "layers.8.linear1.weight", "layers.8.linear1.bias", "layers.8.norm1.weight", "layers.8.norm1.bias", "layers.8.norm0.weight", "layers.8.norm0.bias", "layers.9.attention.W_q.weight", "layers.9.attention.W_q.bias", "layers.9.attention.W_k.weight", "layers.9.attention.W_k.bias", "layers.9.attention.W_v.weight", "layers.9.attention.W_v.bias", "layers.9.attention.W_out.weight", "layers.9.attention.W_out.bias", "layers.9.linear0.weight", "layers.9.linear0.bias", "layers.9.linear1.weight", "layers.9.linear1.bias", "layers.9.norm1.weight", "layers.9.norm1.bias", "layers.9.norm0.weight", "layers.9.norm0.bias". 
	size mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 256]) from checkpoint, the shape in current model is torch.Size([101, 384]).
	size mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 256]) from checkpoint, the shape in current model is torch.Size([100, 384]).
	size mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([552, 384]).
	size mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([552]).
	size mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([384, 552]).
	size mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([552, 384]).
	size mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([552]).
	size mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([384, 552]).
	size mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.2.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.2.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.2.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.2.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.2.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.2.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.2.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.2.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.2.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([552, 384]).
	size mismatch for layers.2.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([552]).
	size mismatch for layers.2.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([384, 552]).
	size mismatch for layers.2.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.2.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.2.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.2.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.2.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.3.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.3.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.3.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.3.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.3.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.3.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.3.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.3.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.3.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([552, 384]).
	size mismatch for layers.3.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([552]).
	size mismatch for layers.3.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([384, 552]).
	size mismatch for layers.3.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.3.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.3.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.3.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.3.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for last_normalization.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for last_normalization.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for head.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 384]).
[W 2025-08-12 18:50:29,191] Trial 2 failed with value None.
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:50:38 INFO]: Training loss at epoch 139: 0.4648010730743408
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:50:57 INFO]: Training stats: {
    "score": -0.612095678979508,
    "rmse": 0.612095678979508
}
[08/12/2025 18:50:57 INFO]: Val stats: {
    "score": -0.6997521595732382,
    "rmse": 0.6997521595732382
}
[08/12/2025 18:50:57 INFO]: Test stats: {
    "score": -0.6841077434384388,
    "rmse": 0.6841077434384388
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:51:03 INFO]: New best epoch, val score: -0.6997521595732382
[08/12/2025 18:51:03 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:51:15 INFO]: Training loss at epoch 89: 0.26142194867134094
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:51:45 INFO]: Training stats: {
    "score": -0.5985674591521615,
    "rmse": 0.5985674591521615
}
[08/12/2025 18:51:45 INFO]: Val stats: {
    "score": -0.659364632160852,
    "rmse": 0.659364632160852
}
[08/12/2025 18:51:45 INFO]: Test stats: {
    "score": -0.6222776307406895,
    "rmse": 0.6222776307406895
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:51:49 INFO]: Training loss at epoch 88: 0.2639472484588623
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:51:53 INFO]: Training loss at epoch 140: 0.33743157982826233
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:51:56 INFO]: New best epoch, val score: -0.659364632160852
[08/12/2025 18:51:56 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:52:04 INFO]: Training loss at epoch 70: 0.48578882217407227
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:52:24 INFO]: Training loss at epoch 37: 0.7075597047805786
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:52:49 INFO]: New best epoch, val score: -0.8390232940250633
[08/12/2025 18:52:49 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
[08/12/2025 18:52:50 INFO]: Training loss at epoch 141: 0.44026803970336914
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:53:15 INFO]: Training loss at epoch 90: 0.30770719796419144
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:53:19 INFO]: Training loss at epoch 89: 0.36307474970817566
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:53:47 INFO]: Training loss at epoch 142: 0.27730026841163635
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:53:49 INFO]: Training stats: {
    "score": -0.5185563621329932,
    "rmse": 0.5185563621329932
}
[08/12/2025 18:53:49 INFO]: Val stats: {
    "score": -0.6944353410337335,
    "rmse": 0.6944353410337335
}
[08/12/2025 18:53:49 INFO]: Test stats: {
    "score": -0.6482142939756148,
    "rmse": 0.6482142939756148
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:53:57 INFO]: Training loss at epoch 71: 0.5070092082023621
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:54:43 INFO]: Training loss at epoch 143: 0.4285966008901596
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:54:43 INFO]: Training loss at epoch 91: 0.3070312738418579
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:55:19 INFO]: Training loss at epoch 90: 0.4192245155572891
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:55:39 INFO]: Training loss at epoch 144: 0.3320987820625305
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 18:55:51 INFO]: Training loss at epoch 72: 0.47926995158195496
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:55:57 INFO]: Training loss at epoch 38: 0.9298075437545776
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:56:04 INFO]: New best epoch, val score: -0.7143300820003616
[08/12/2025 18:56:04 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:56:13 INFO]: Training loss at epoch 92: 0.37676766514778137
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:56:22 INFO]: New best epoch, val score: -0.8265008645872838
[08/12/2025 18:56:22 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:56:36 INFO]: Training loss at epoch 145: 0.3753783255815506
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:56:48 INFO]: Training loss at epoch 91: 0.32198566198349
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:57:32 INFO]: Training loss at epoch 146: 0.2739838510751724
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:57:43 INFO]: Training loss at epoch 93: 0.3929472416639328
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:57:45 INFO]: Training loss at epoch 73: 0.5529090464115143
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:57:59 INFO]: New best epoch, val score: -0.7115085670543252
[08/12/2025 18:57:59 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:58:18 INFO]: Training loss at epoch 92: 0.24228686839342117
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:58:28 INFO]: Training loss at epoch 147: 0.368101567029953
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:59:12 INFO]: Training loss at epoch 94: 0.30843645334243774
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:59:26 INFO]: Training loss at epoch 148: 0.3254629820585251
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 18:59:32 INFO]: Training loss at epoch 39: 0.5933210849761963
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 18:59:37 INFO]: Training loss at epoch 74: 0.4225582331418991
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
[08/12/2025 18:59:48 INFO]: Training loss at epoch 93: 0.294744148850441
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 19:00:22 INFO]: Training loss at epoch 149: 0.35351143777370453
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:00:39 INFO]: Training loss at epoch 95: 0.24239147454500198
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:00:41 INFO]: Training stats: {
    "score": -0.6414616406663036,
    "rmse": 0.6414616406663036
}
[08/12/2025 19:00:41 INFO]: Val stats: {
    "score": -0.730931762368547,
    "rmse": 0.730931762368547
}
[08/12/2025 19:00:41 INFO]: Test stats: {
    "score": -0.6892304850725398,
    "rmse": 0.6892304850725398
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:00:45 INFO]: Training stats: {
    "score": -0.8159626904377975,
    "rmse": 0.8159626904377975
}
[08/12/2025 19:00:45 INFO]: Val stats: {
    "score": -0.8345788778521825,
    "rmse": 0.8345788778521825
}
[08/12/2025 19:00:45 INFO]: Test stats: {
    "score": -0.7696703369094939,
    "rmse": 0.7696703369094939
}
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 19:01:17 INFO]: Training loss at epoch 94: 0.2648516744375229
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 19:01:28 INFO]: Training loss at epoch 75: 0.39477571845054626
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:01:37 INFO]: Training loss at epoch 150: 0.34669622778892517
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:02:07 INFO]: Training loss at epoch 96: 0.28481660783290863
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:02:36 INFO]: Training loss at epoch 151: 0.36910830438137054
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 19:02:48 INFO]: Training loss at epoch 95: 0.29338011145591736
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 19:03:20 INFO]: Training loss at epoch 76: 0.48549604415893555
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:03:32 INFO]: Training loss at epoch 152: 0.32878316938877106
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:03:35 INFO]: Training loss at epoch 97: 0.2874596267938614
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 19:04:17 INFO]: Training loss at epoch 96: 0.22309938073158264
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 19:04:18 INFO]: Training loss at epoch 40: 0.6396419703960419
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:04:28 INFO]: Training loss at epoch 153: 0.3548582047224045
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:05:02 INFO]: Training loss at epoch 98: 0.3695371448993683
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:05:11 INFO]: Training loss at epoch 77: 0.3686688169836998
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:05:29 INFO]: Training loss at epoch 154: 0.3266247361898422
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:05:51 INFO]: Training loss at epoch 97: 0.29026326537132263
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:06:26 INFO]: Training loss at epoch 155: 0.283095046877861
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:06:30 INFO]: Training loss at epoch 99: 0.45052410662174225
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:07:00 INFO]: Training stats: {
    "score": -0.545963835961714,
    "rmse": 0.545963835961714
}
[08/12/2025 19:07:00 INFO]: Val stats: {
    "score": -0.6762848342457755,
    "rmse": 0.6762848342457755
}
[08/12/2025 19:07:00 INFO]: Test stats: {
    "score": -0.6468561705074694,
    "rmse": 0.6468561705074694
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:07:02 INFO]: Training loss at epoch 78: 0.41981540620326996
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 19:07:24 INFO]: Training loss at epoch 156: 0.32846564054489136
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:07:28 INFO]: Training loss at epoch 98: 0.3521041125059128
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:07:50 INFO]: Training loss at epoch 41: 0.8038803040981293
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
[08/12/2025 19:08:15 INFO]: New best epoch, val score: -0.8070983596834123
[08/12/2025 19:08:15 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:08:22 INFO]: Training loss at epoch 157: 0.3754294514656067
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:08:28 INFO]: Training loss at epoch 100: 0.315205380320549
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 19:08:54 INFO]: Training loss at epoch 79: 0.48400840163230896
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:08:59 INFO]: Training loss at epoch 99: 0.239591583609581
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:09:20 INFO]: Training loss at epoch 158: 0.3763897120952606
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:09:31 INFO]: Training stats: {
    "score": -0.4983843862948869,
    "rmse": 0.4983843862948869
}
[08/12/2025 19:09:31 INFO]: Val stats: {
    "score": -0.717044091327776,
    "rmse": 0.717044091327776
}
[08/12/2025 19:09:31 INFO]: Test stats: {
    "score": -0.6733541663998117,
    "rmse": 0.6733541663998117
}
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 19:09:32 INFO]: Training stats: {
    "score": -0.6882890527852454,
    "rmse": 0.6882890527852454
}
[08/12/2025 19:09:32 INFO]: Val stats: {
    "score": -0.8155080600129947,
    "rmse": 0.8155080600129947
}
[08/12/2025 19:09:32 INFO]: Test stats: {
    "score": -0.7176378435126978,
    "rmse": 0.7176378435126978
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:09:55 INFO]: Training loss at epoch 101: 0.3976444900035858
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:10:16 INFO]: Training loss at epoch 159: 0.32836008071899414
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:10:36 INFO]: Training stats: {
    "score": -0.5891760049710099,
    "rmse": 0.5891760049710099
}
[08/12/2025 19:10:36 INFO]: Val stats: {
    "score": -0.7541867323507345,
    "rmse": 0.7541867323507345
}
[08/12/2025 19:10:36 INFO]: Test stats: {
    "score": -0.6870949938808447,
    "rmse": 0.6870949938808447
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 19:11:01 INFO]: Training loss at epoch 100: 0.2401246652007103
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:11:22 INFO]: Training loss at epoch 42: 0.5282436460256577
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
[08/12/2025 19:11:23 INFO]: Training loss at epoch 102: 0.3723800629377365
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:11:23 INFO]: Training loss at epoch 80: 0.43935534358024597
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:11:32 INFO]: Training loss at epoch 160: 0.3252704441547394
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:11:48 INFO]: New best epoch, val score: -0.7862280450541493
[08/12/2025 19:11:48 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:12:32 INFO]: Training loss at epoch 161: 0.2944899797439575
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:12:33 INFO]: Training loss at epoch 101: 0.24897516518831253
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:12:52 INFO]: Training loss at epoch 103: 0.32521258294582367
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:13:17 INFO]: Training loss at epoch 81: 0.41746973991394043
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:13:31 INFO]: Training loss at epoch 162: 0.2804935947060585
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 19:14:05 INFO]: Training loss at epoch 102: 0.2399565875530243
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 19:14:20 INFO]: Training loss at epoch 104: 0.34522484242916107
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:14:27 INFO]: Training loss at epoch 163: 0.31200531125068665
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:14:56 INFO]: Training loss at epoch 43: 0.49099959433078766
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
[08/12/2025 19:15:08 INFO]: Training loss at epoch 82: 0.46382224559783936
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:15:21 INFO]: New best epoch, val score: -0.7068712701616808
[08/12/2025 19:15:21 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
[08/12/2025 19:15:25 INFO]: Training loss at epoch 164: 0.2633994594216347
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:15:37 INFO]: Training loss at epoch 103: 0.24814468622207642
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 19:15:48 INFO]: Training loss at epoch 105: 0.29630571603775024
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:16:23 INFO]: Training loss at epoch 165: 0.2867397665977478
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:17:00 INFO]: Training loss at epoch 83: 0.5231181681156158
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:17:08 INFO]: Training loss at epoch 104: 0.1918947771191597
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:17:16 INFO]: Training loss at epoch 106: 0.31479310989379883
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 19:17:20 INFO]: Training loss at epoch 166: 0.34352171421051025
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
[08/12/2025 19:18:19 INFO]: Training loss at epoch 167: 0.3431422859430313
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:18:29 INFO]: Training loss at epoch 44: 0.5225502848625183
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:18:39 INFO]: Training loss at epoch 105: 0.26725853234529495
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
[08/12/2025 19:18:44 INFO]: Training loss at epoch 107: 0.31117716431617737
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 19:18:51 INFO]: Training loss at epoch 84: 0.46381546556949615
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:19:15 INFO]: Training loss at epoch 168: 0.3492758572101593
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:20:09 INFO]: Training loss at epoch 106: 0.31815557926893234
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
[08/12/2025 19:20:11 INFO]: Training loss at epoch 108: 0.3700484186410904
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:20:12 INFO]: Training loss at epoch 169: 0.3920563757419586
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:20:30 INFO]: Training stats: {
    "score": -0.6560163979403387,
    "rmse": 0.6560163979403387
}
[08/12/2025 19:20:30 INFO]: Val stats: {
    "score": -0.759430713024664,
    "rmse": 0.759430713024664
}
[08/12/2025 19:20:30 INFO]: Test stats: {
    "score": -0.7247916586420626,
    "rmse": 0.7247916586420626
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:20:42 INFO]: Training loss at epoch 85: 0.5061334669589996
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:21:29 INFO]: Training loss at epoch 170: 0.4631878137588501
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 19:21:36 INFO]: Running Final Evaluation...
[W 2025-08-12 19:21:36,456] Trial 1 failed with parameters: {'embedding_head_pair': (320, 8), 'n_layers': 4, 'd_ffn_factor': 2.550511424520133, 'attention_dropout': 0.382955390962517, 'activation': 'relu', 'lr': 0.00019323685933336145} because of the following error: RuntimeError('Error(s) in loading state_dict for FTTransformer:\n\tUnexpected key(s) in state_dict: "layers.4.attention.W_q.weight", "layers.4.attention.W_q.bias", "layers.4.attention.W_k.weight", "layers.4.attention.W_k.bias", "layers.4.attention.W_v.weight", "layers.4.attention.W_v.bias", "layers.4.attention.W_out.weight", "layers.4.attention.W_out.bias", "layers.4.linear0.weight", "layers.4.linear0.bias", "layers.4.linear1.weight", "layers.4.linear1.bias", "layers.4.norm1.weight", "layers.4.norm1.bias", "layers.4.norm0.weight", "layers.4.norm0.bias", "layers.5.attention.W_q.weight", "layers.5.attention.W_q.bias", "layers.5.attention.W_k.weight", "layers.5.attention.W_k.bias", "layers.5.attention.W_v.weight", "layers.5.attention.W_v.bias", "layers.5.attention.W_out.weight", "layers.5.attention.W_out.bias", "layers.5.linear0.weight", "layers.5.linear0.bias", "layers.5.linear1.weight", "layers.5.linear1.bias", "layers.5.norm1.weight", "layers.5.norm1.bias", "layers.5.norm0.weight", "layers.5.norm0.bias", "layers.6.attention.W_q.weight", "layers.6.attention.W_q.bias", "layers.6.attention.W_k.weight", "layers.6.attention.W_k.bias", "layers.6.attention.W_v.weight", "layers.6.attention.W_v.bias", "layers.6.attention.W_out.weight", "layers.6.attention.W_out.bias", "layers.6.linear0.weight", "layers.6.linear0.bias", "layers.6.linear1.weight", "layers.6.linear1.bias", "layers.6.norm1.weight", "layers.6.norm1.bias", "layers.6.norm0.weight", "layers.6.norm0.bias", "layers.7.attention.W_q.weight", "layers.7.attention.W_q.bias", "layers.7.attention.W_k.weight", "layers.7.attention.W_k.bias", "layers.7.attention.W_v.weight", "layers.7.attention.W_v.bias", "layers.7.attention.W_out.weight", "layers.7.attention.W_out.bias", "layers.7.linear0.weight", "layers.7.linear0.bias", "layers.7.linear1.weight", "layers.7.linear1.bias", "layers.7.norm1.weight", "layers.7.norm1.bias", "layers.7.norm0.weight", "layers.7.norm0.bias", "layers.8.attention.W_q.weight", "layers.8.attention.W_q.bias", "layers.8.attention.W_k.weight", "layers.8.attention.W_k.bias", "layers.8.attention.W_v.weight", "layers.8.attention.W_v.bias", "layers.8.attention.W_out.weight", "layers.8.attention.W_out.bias", "layers.8.linear0.weight", "layers.8.linear0.bias", "layers.8.linear1.weight", "layers.8.linear1.bias", "layers.8.norm1.weight", "layers.8.norm1.bias", "layers.8.norm0.weight", "layers.8.norm0.bias", "layers.9.attention.W_q.weight", "layers.9.attention.W_q.bias", "layers.9.attention.W_k.weight", "layers.9.attention.W_k.bias", "layers.9.attention.W_v.weight", "layers.9.attention.W_v.bias", "layers.9.attention.W_out.weight", "layers.9.attention.W_out.bias", "layers.9.linear0.weight", "layers.9.linear0.bias", "layers.9.linear1.weight", "layers.9.linear1.bias", "layers.9.norm1.weight", "layers.9.norm1.bias", "layers.9.norm0.weight", "layers.9.norm0.bias". \n\tsize mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 256]) from checkpoint, the shape in current model is torch.Size([101, 320]).\n\tsize mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 256]) from checkpoint, the shape in current model is torch.Size([100, 320]).\n\tsize mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([816, 320]).\n\tsize mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([816]).\n\tsize mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([320, 816]).\n\tsize mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([816, 320]).\n\tsize mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([816]).\n\tsize mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([320, 816]).\n\tsize mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.2.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.2.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.2.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.2.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.2.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.2.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.2.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.2.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.2.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([816, 320]).\n\tsize mismatch for layers.2.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([816]).\n\tsize mismatch for layers.2.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([320, 816]).\n\tsize mismatch for layers.2.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.2.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.2.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.2.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.2.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.3.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.3.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.3.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.3.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.3.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.3.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.3.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.3.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.3.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([816, 320]).\n\tsize mismatch for layers.3.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([816]).\n\tsize mismatch for layers.3.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([320, 816]).\n\tsize mismatch for layers.3.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.3.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.3.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.3.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.3.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for last_normalization.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for last_normalization.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for head.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 320]).').
Traceback (most recent call last):
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 145, in <lambda>
    func = lambda trial: objective(trial, cfg, trial_stats, trial_counter, n_optuna_trials,
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 104, in objective
    stats = train_net_for_optuna.main(config, loaders, unique_categories, n_numerical, n_classes)
  File "/temporario2/14570653/tabular-transfer-learning/train_net_for_optuna.py", line 127, in main
    net.load_state_dict(torch.load(checkpoint_path)["net"])
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2215, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for FTTransformer:
	Unexpected key(s) in state_dict: "layers.4.attention.W_q.weight", "layers.4.attention.W_q.bias", "layers.4.attention.W_k.weight", "layers.4.attention.W_k.bias", "layers.4.attention.W_v.weight", "layers.4.attention.W_v.bias", "layers.4.attention.W_out.weight", "layers.4.attention.W_out.bias", "layers.4.linear0.weight", "layers.4.linear0.bias", "layers.4.linear1.weight", "layers.4.linear1.bias", "layers.4.norm1.weight", "layers.4.norm1.bias", "layers.4.norm0.weight", "layers.4.norm0.bias", "layers.5.attention.W_q.weight", "layers.5.attention.W_q.bias", "layers.5.attention.W_k.weight", "layers.5.attention.W_k.bias", "layers.5.attention.W_v.weight", "layers.5.attention.W_v.bias", "layers.5.attention.W_out.weight", "layers.5.attention.W_out.bias", "layers.5.linear0.weight", "layers.5.linear0.bias", "layers.5.linear1.weight", "layers.5.linear1.bias", "layers.5.norm1.weight", "layers.5.norm1.bias", "layers.5.norm0.weight", "layers.5.norm0.bias", "layers.6.attention.W_q.weight", "layers.6.attention.W_q.bias", "layers.6.attention.W_k.weight", "layers.6.attention.W_k.bias", "layers.6.attention.W_v.weight", "layers.6.attention.W_v.bias", "layers.6.attention.W_out.weight", "layers.6.attention.W_out.bias", "layers.6.linear0.weight", "layers.6.linear0.bias", "layers.6.linear1.weight", "layers.6.linear1.bias", "layers.6.norm1.weight", "layers.6.norm1.bias", "layers.6.norm0.weight", "layers.6.norm0.bias", "layers.7.attention.W_q.weight", "layers.7.attention.W_q.bias", "layers.7.attention.W_k.weight", "layers.7.attention.W_k.bias", "layers.7.attention.W_v.weight", "layers.7.attention.W_v.bias", "layers.7.attention.W_out.weight", "layers.7.attention.W_out.bias", "layers.7.linear0.weight", "layers.7.linear0.bias", "layers.7.linear1.weight", "layers.7.linear1.bias", "layers.7.norm1.weight", "layers.7.norm1.bias", "layers.7.norm0.weight", "layers.7.norm0.bias", "layers.8.attention.W_q.weight", "layers.8.attention.W_q.bias", "layers.8.attention.W_k.weight", "layers.8.attention.W_k.bias", "layers.8.attention.W_v.weight", "layers.8.attention.W_v.bias", "layers.8.attention.W_out.weight", "layers.8.attention.W_out.bias", "layers.8.linear0.weight", "layers.8.linear0.bias", "layers.8.linear1.weight", "layers.8.linear1.bias", "layers.8.norm1.weight", "layers.8.norm1.bias", "layers.8.norm0.weight", "layers.8.norm0.bias", "layers.9.attention.W_q.weight", "layers.9.attention.W_q.bias", "layers.9.attention.W_k.weight", "layers.9.attention.W_k.bias", "layers.9.attention.W_v.weight", "layers.9.attention.W_v.bias", "layers.9.attention.W_out.weight", "layers.9.attention.W_out.bias", "layers.9.linear0.weight", "layers.9.linear0.bias", "layers.9.linear1.weight", "layers.9.linear1.bias", "layers.9.norm1.weight", "layers.9.norm1.bias", "layers.9.norm0.weight", "layers.9.norm0.bias". 
	size mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 256]) from checkpoint, the shape in current model is torch.Size([101, 320]).
	size mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 256]) from checkpoint, the shape in current model is torch.Size([100, 320]).
	size mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([816, 320]).
	size mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([816]).
	size mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([320, 816]).
	size mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([816, 320]).
	size mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([816]).
	size mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([320, 816]).
	size mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.2.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.2.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.2.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.2.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.2.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.2.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.2.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.2.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.2.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([816, 320]).
	size mismatch for layers.2.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([816]).
	size mismatch for layers.2.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([320, 816]).
	size mismatch for layers.2.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.2.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.2.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.2.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.2.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.3.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.3.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.3.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.3.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.3.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.3.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.3.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.3.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.3.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([816, 320]).
	size mismatch for layers.3.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([816]).
	size mismatch for layers.3.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([320, 816]).
	size mismatch for layers.3.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.3.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.3.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.3.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.3.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for last_normalization.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for last_normalization.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for head.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 320]).
[W 2025-08-12 19:21:36,463] Trial 1 failed with value None.
[08/12/2025 19:21:38 INFO]: Training loss at epoch 109: 0.3362298458814621
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:21:41 INFO]: Training loss at epoch 107: 0.2513221949338913
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
[08/12/2025 19:22:00 INFO]: Training loss at epoch 45: 0.5317573100328445
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:22:08 INFO]: Training stats: {
    "score": -0.5333167414516957,
    "rmse": 0.5333167414516957
}
[08/12/2025 19:22:08 INFO]: Val stats: {
    "score": -0.7170872146341839,
    "rmse": 0.7170872146341839
}
[08/12/2025 19:22:08 INFO]: Test stats: {
    "score": -0.6223654314003423,
    "rmse": 0.6223654314003423
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:22:36 INFO]: Training loss at epoch 86: 0.4460107535123825
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:22:50 INFO]: New best epoch, val score: -0.6831238740567122
[08/12/2025 19:22:50 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
[08/12/2025 19:23:14 INFO]: Training loss at epoch 108: 0.2583000212907791
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:23:46 INFO]: Training loss at epoch 110: 0.2761233150959015
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:24:51 INFO]: Training loss at epoch 87: 0.4059835225343704
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:25:02 INFO]: Training loss at epoch 109: 0.2701867073774338
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:25:34 INFO]: Training loss at epoch 111: 0.3057383596897125
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:25:39 INFO]: Training stats: {
    "score": -0.4655586637557077,
    "rmse": 0.4655586637557077
}
[08/12/2025 19:25:39 INFO]: Val stats: {
    "score": -0.6907405055119142,
    "rmse": 0.6907405055119142
}
[08/12/2025 19:25:39 INFO]: Test stats: {
    "score": -0.6635351743499766,
    "rmse": 0.6635351743499766
}
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([87, 101, 384])
Tokenizer  -1: 384
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
[08/12/2025 19:25:52 INFO]: Running Final Evaluation...
[W 2025-08-12 19:25:52,986] Trial 9 failed with parameters: {'embedding_head_pair': (384, 12), 'n_layers': 6, 'd_ffn_factor': 0.7515143465354147, 'attention_dropout': 0.027179623953505427, 'activation': 'gelu', 'lr': 0.000279333299572177} because of the following error: RuntimeError('Error(s) in loading state_dict for FTTransformer:\n\tUnexpected key(s) in state_dict: "layers.6.attention.W_q.weight", "layers.6.attention.W_q.bias", "layers.6.attention.W_k.weight", "layers.6.attention.W_k.bias", "layers.6.attention.W_v.weight", "layers.6.attention.W_v.bias", "layers.6.attention.W_out.weight", "layers.6.attention.W_out.bias", "layers.6.linear0.weight", "layers.6.linear0.bias", "layers.6.linear1.weight", "layers.6.linear1.bias", "layers.6.norm1.weight", "layers.6.norm1.bias", "layers.6.norm0.weight", "layers.6.norm0.bias", "layers.7.attention.W_q.weight", "layers.7.attention.W_q.bias", "layers.7.attention.W_k.weight", "layers.7.attention.W_k.bias", "layers.7.attention.W_v.weight", "layers.7.attention.W_v.bias", "layers.7.attention.W_out.weight", "layers.7.attention.W_out.bias", "layers.7.linear0.weight", "layers.7.linear0.bias", "layers.7.linear1.weight", "layers.7.linear1.bias", "layers.7.norm1.weight", "layers.7.norm1.bias", "layers.7.norm0.weight", "layers.7.norm0.bias", "layers.8.attention.W_q.weight", "layers.8.attention.W_q.bias", "layers.8.attention.W_k.weight", "layers.8.attention.W_k.bias", "layers.8.attention.W_v.weight", "layers.8.attention.W_v.bias", "layers.8.attention.W_out.weight", "layers.8.attention.W_out.bias", "layers.8.linear0.weight", "layers.8.linear0.bias", "layers.8.linear1.weight", "layers.8.linear1.bias", "layers.8.norm1.weight", "layers.8.norm1.bias", "layers.8.norm0.weight", "layers.8.norm0.bias", "layers.9.attention.W_q.weight", "layers.9.attention.W_q.bias", "layers.9.attention.W_k.weight", "layers.9.attention.W_k.bias", "layers.9.attention.W_v.weight", "layers.9.attention.W_v.bias", "layers.9.attention.W_out.weight", "layers.9.attention.W_out.bias", "layers.9.linear0.weight", "layers.9.linear0.bias", "layers.9.linear1.weight", "layers.9.linear1.bias", "layers.9.norm1.weight", "layers.9.norm1.bias", "layers.9.norm0.weight", "layers.9.norm0.bias". \n\tsize mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 256]) from checkpoint, the shape in current model is torch.Size([101, 384]).\n\tsize mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 256]) from checkpoint, the shape in current model is torch.Size([100, 384]).\n\tsize mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([288, 384]).\n\tsize mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([288]).\n\tsize mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([384, 288]).\n\tsize mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([288, 384]).\n\tsize mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([288]).\n\tsize mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([384, 288]).\n\tsize mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.2.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.2.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.2.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.2.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([288, 384]).\n\tsize mismatch for layers.2.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([288]).\n\tsize mismatch for layers.2.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([384, 288]).\n\tsize mismatch for layers.2.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.3.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.3.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.3.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.3.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([288, 384]).\n\tsize mismatch for layers.3.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([288]).\n\tsize mismatch for layers.3.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([384, 288]).\n\tsize mismatch for layers.3.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.4.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.4.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.4.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.4.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.4.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.4.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.4.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.4.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.4.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([288, 384]).\n\tsize mismatch for layers.4.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([288]).\n\tsize mismatch for layers.4.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([384, 288]).\n\tsize mismatch for layers.4.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.4.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.4.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.4.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.4.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.5.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.5.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.5.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.5.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.5.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.5.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.5.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for layers.5.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.5.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([288, 384]).\n\tsize mismatch for layers.5.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([288]).\n\tsize mismatch for layers.5.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([384, 288]).\n\tsize mismatch for layers.5.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.5.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.5.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.5.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.5.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for last_normalization.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for last_normalization.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for head.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 384]).').
Traceback (most recent call last):
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 145, in <lambda>
    func = lambda trial: objective(trial, cfg, trial_stats, trial_counter, n_optuna_trials,
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 104, in objective
    stats = train_net_for_optuna.main(config, loaders, unique_categories, n_numerical, n_classes)
  File "/temporario2/14570653/tabular-transfer-learning/train_net_for_optuna.py", line 127, in main
    net.load_state_dict(torch.load(checkpoint_path)["net"])
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2215, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for FTTransformer:
	Unexpected key(s) in state_dict: "layers.6.attention.W_q.weight", "layers.6.attention.W_q.bias", "layers.6.attention.W_k.weight", "layers.6.attention.W_k.bias", "layers.6.attention.W_v.weight", "layers.6.attention.W_v.bias", "layers.6.attention.W_out.weight", "layers.6.attention.W_out.bias", "layers.6.linear0.weight", "layers.6.linear0.bias", "layers.6.linear1.weight", "layers.6.linear1.bias", "layers.6.norm1.weight", "layers.6.norm1.bias", "layers.6.norm0.weight", "layers.6.norm0.bias", "layers.7.attention.W_q.weight", "layers.7.attention.W_q.bias", "layers.7.attention.W_k.weight", "layers.7.attention.W_k.bias", "layers.7.attention.W_v.weight", "layers.7.attention.W_v.bias", "layers.7.attention.W_out.weight", "layers.7.attention.W_out.bias", "layers.7.linear0.weight", "layers.7.linear0.bias", "layers.7.linear1.weight", "layers.7.linear1.bias", "layers.7.norm1.weight", "layers.7.norm1.bias", "layers.7.norm0.weight", "layers.7.norm0.bias", "layers.8.attention.W_q.weight", "layers.8.attention.W_q.bias", "layers.8.attention.W_k.weight", "layers.8.attention.W_k.bias", "layers.8.attention.W_v.weight", "layers.8.attention.W_v.bias", "layers.8.attention.W_out.weight", "layers.8.attention.W_out.bias", "layers.8.linear0.weight", "layers.8.linear0.bias", "layers.8.linear1.weight", "layers.8.linear1.bias", "layers.8.norm1.weight", "layers.8.norm1.bias", "layers.8.norm0.weight", "layers.8.norm0.bias", "layers.9.attention.W_q.weight", "layers.9.attention.W_q.bias", "layers.9.attention.W_k.weight", "layers.9.attention.W_k.bias", "layers.9.attention.W_v.weight", "layers.9.attention.W_v.bias", "layers.9.attention.W_out.weight", "layers.9.attention.W_out.bias", "layers.9.linear0.weight", "layers.9.linear0.bias", "layers.9.linear1.weight", "layers.9.linear1.bias", "layers.9.norm1.weight", "layers.9.norm1.bias", "layers.9.norm0.weight", "layers.9.norm0.bias". 
	size mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 256]) from checkpoint, the shape in current model is torch.Size([101, 384]).
	size mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 256]) from checkpoint, the shape in current model is torch.Size([100, 384]).
	size mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([288, 384]).
	size mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([288]).
	size mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([384, 288]).
	size mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([288, 384]).
	size mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([288]).
	size mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([384, 288]).
	size mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.2.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.2.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.2.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.2.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.2.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.2.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.2.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.2.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.2.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([288, 384]).
	size mismatch for layers.2.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([288]).
	size mismatch for layers.2.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([384, 288]).
	size mismatch for layers.2.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.2.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.2.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.2.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.2.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.3.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.3.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.3.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.3.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.3.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.3.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.3.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.3.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.3.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([288, 384]).
	size mismatch for layers.3.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([288]).
	size mismatch for layers.3.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([384, 288]).
	size mismatch for layers.3.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.3.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.3.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.3.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.3.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.4.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.4.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.4.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.4.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.4.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.4.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.4.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.4.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.4.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([288, 384]).
	size mismatch for layers.4.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([288]).
	size mismatch for layers.4.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([384, 288]).
	size mismatch for layers.4.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.4.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.4.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.4.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.4.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.5.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.5.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.5.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.5.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.5.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.5.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.5.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([384, 384]).
	size mismatch for layers.5.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.5.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([288, 384]).
	size mismatch for layers.5.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([288]).
	size mismatch for layers.5.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([384, 288]).
	size mismatch for layers.5.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.5.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.5.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.5.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for layers.5.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for last_normalization.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for last_normalization.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for head.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 384]).
[W 2025-08-12 19:25:52,990] Trial 9 failed with value None.
[08/12/2025 19:26:05 INFO]: Training loss at epoch 46: 0.4984375685453415
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:27:07 INFO]: Training loss at epoch 88: 0.35000909864902496
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:27:21 INFO]: Training loss at epoch 112: 0.29308561980724335
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:29:09 INFO]: Training loss at epoch 113: 0.3238265663385391
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:29:22 INFO]: Training loss at epoch 89: 0.34542208909988403
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:30:08 INFO]: Training stats: {
    "score": -0.6795982542952466,
    "rmse": 0.6795982542952466
}
[08/12/2025 19:30:08 INFO]: Val stats: {
    "score": -0.8109225911176182,
    "rmse": 0.8109225911176182
}
[08/12/2025 19:30:08 INFO]: Test stats: {
    "score": -0.738675879837512,
    "rmse": 0.738675879837512
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:30:22 INFO]: Training loss at epoch 47: 0.5938573777675629
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
[08/12/2025 19:30:56 INFO]: Training loss at epoch 114: 0.24765978008508682
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:32:23 INFO]: Training loss at epoch 90: 0.4288705289363861
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:32:44 INFO]: Training loss at epoch 115: 0.3289985954761505
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:34:34 INFO]: Training loss at epoch 116: 0.24250950664281845
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:34:40 INFO]: Training loss at epoch 91: 0.37168754637241364
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:34:40 INFO]: Training loss at epoch 48: 0.4796750694513321
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:36:21 INFO]: Training loss at epoch 117: 0.32388487458229065
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:36:57 INFO]: Training loss at epoch 92: 0.48982295393943787
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:38:07 INFO]: Training loss at epoch 118: 0.36849863827228546
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:38:58 INFO]: Training loss at epoch 49: 0.5386103242635727
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:39:14 INFO]: Training loss at epoch 93: 0.4346413016319275
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:39:55 INFO]: Training loss at epoch 119: 0.29528096318244934
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
[08/12/2025 19:40:26 INFO]: Training stats: {
    "score": -0.6807307065638819,
    "rmse": 0.6807307065638819
}
[08/12/2025 19:40:26 INFO]: Val stats: {
    "score": -0.8002896506511243,
    "rmse": 0.8002896506511243
}
[08/12/2025 19:40:26 INFO]: Test stats: {
    "score": -0.6881687644912943,
    "rmse": 0.6881687644912943
}
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:40:33 INFO]: Training stats: {
    "score": -0.5710626875844279,
    "rmse": 0.5710626875844279
}
[08/12/2025 19:40:33 INFO]: Val stats: {
    "score": -0.7637673144772298,
    "rmse": 0.7637673144772298
}
[08/12/2025 19:40:33 INFO]: Test stats: {
    "score": -0.7311249835034237,
    "rmse": 0.7311249835034237
}
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:41:31 INFO]: Training loss at epoch 94: 0.4422016888856888
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:42:21 INFO]: Training loss at epoch 120: 0.3078780919313431
Tokenizer output shape: torch.Size([66, 101, 320])
Tokenizer  -1: 320
Tokenizer output shape: torch.Size([87, 101, 320])
Tokenizer  -1: 320
[08/12/2025 19:42:34 INFO]: Running Final Evaluation...
[W 2025-08-12 19:42:35,028] Trial 8 failed with parameters: {'embedding_head_pair': (320, 8), 'n_layers': 8, 'd_ffn_factor': 0.697669239482776, 'attention_dropout': 0.04764511346879208, 'activation': 'gelu', 'lr': 0.0003613241327333436} because of the following error: RuntimeError('Error(s) in loading state_dict for FTTransformer:\n\tUnexpected key(s) in state_dict: "layers.8.attention.W_q.weight", "layers.8.attention.W_q.bias", "layers.8.attention.W_k.weight", "layers.8.attention.W_k.bias", "layers.8.attention.W_v.weight", "layers.8.attention.W_v.bias", "layers.8.attention.W_out.weight", "layers.8.attention.W_out.bias", "layers.8.linear0.weight", "layers.8.linear0.bias", "layers.8.linear1.weight", "layers.8.linear1.bias", "layers.8.norm1.weight", "layers.8.norm1.bias", "layers.8.norm0.weight", "layers.8.norm0.bias", "layers.9.attention.W_q.weight", "layers.9.attention.W_q.bias", "layers.9.attention.W_k.weight", "layers.9.attention.W_k.bias", "layers.9.attention.W_v.weight", "layers.9.attention.W_v.bias", "layers.9.attention.W_out.weight", "layers.9.attention.W_out.bias", "layers.9.linear0.weight", "layers.9.linear0.bias", "layers.9.linear1.weight", "layers.9.linear1.bias", "layers.9.norm1.weight", "layers.9.norm1.bias", "layers.9.norm0.weight", "layers.9.norm0.bias". \n\tsize mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 256]) from checkpoint, the shape in current model is torch.Size([101, 320]).\n\tsize mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 256]) from checkpoint, the shape in current model is torch.Size([100, 320]).\n\tsize mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([223, 320]).\n\tsize mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([223]).\n\tsize mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([320, 223]).\n\tsize mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([223, 320]).\n\tsize mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([223]).\n\tsize mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([320, 223]).\n\tsize mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.2.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.2.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.2.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.2.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.2.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.2.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.2.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.2.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.2.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([223, 320]).\n\tsize mismatch for layers.2.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([223]).\n\tsize mismatch for layers.2.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([320, 223]).\n\tsize mismatch for layers.2.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.2.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.2.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.2.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.2.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.3.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.3.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.3.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.3.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.3.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.3.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.3.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.3.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.3.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([223, 320]).\n\tsize mismatch for layers.3.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([223]).\n\tsize mismatch for layers.3.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([320, 223]).\n\tsize mismatch for layers.3.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.3.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.3.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.3.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.3.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.4.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.4.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.4.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.4.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.4.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.4.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.4.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.4.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.4.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([223, 320]).\n\tsize mismatch for layers.4.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([223]).\n\tsize mismatch for layers.4.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([320, 223]).\n\tsize mismatch for layers.4.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.4.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.4.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.4.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.4.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.5.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.5.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.5.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.5.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.5.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.5.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.5.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.5.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.5.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([223, 320]).\n\tsize mismatch for layers.5.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([223]).\n\tsize mismatch for layers.5.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([320, 223]).\n\tsize mismatch for layers.5.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.5.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.5.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.5.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.5.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.6.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.6.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.6.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.6.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.6.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.6.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.6.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.6.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.6.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([223, 320]).\n\tsize mismatch for layers.6.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([223]).\n\tsize mismatch for layers.6.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([320, 223]).\n\tsize mismatch for layers.6.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.6.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.6.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.6.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.6.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.7.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.7.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.7.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.7.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.7.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.7.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.7.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).\n\tsize mismatch for layers.7.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.7.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([223, 320]).\n\tsize mismatch for layers.7.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([223]).\n\tsize mismatch for layers.7.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([320, 223]).\n\tsize mismatch for layers.7.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.7.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.7.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.7.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for layers.7.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for last_normalization.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for last_normalization.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for head.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 320]).').
Traceback (most recent call last):
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 145, in <lambda>
    func = lambda trial: objective(trial, cfg, trial_stats, trial_counter, n_optuna_trials,
  File "/temporario2/14570653/tabular-transfer-learning/optune_from_scratch.py", line 104, in objective
    stats = train_net_for_optuna.main(config, loaders, unique_categories, n_numerical, n_classes)
  File "/temporario2/14570653/tabular-transfer-learning/train_net_for_optuna.py", line 127, in main
    net.load_state_dict(torch.load(checkpoint_path)["net"])
  File "/temporario2/14570653/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2215, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for FTTransformer:
	Unexpected key(s) in state_dict: "layers.8.attention.W_q.weight", "layers.8.attention.W_q.bias", "layers.8.attention.W_k.weight", "layers.8.attention.W_k.bias", "layers.8.attention.W_v.weight", "layers.8.attention.W_v.bias", "layers.8.attention.W_out.weight", "layers.8.attention.W_out.bias", "layers.8.linear0.weight", "layers.8.linear0.bias", "layers.8.linear1.weight", "layers.8.linear1.bias", "layers.8.norm1.weight", "layers.8.norm1.bias", "layers.8.norm0.weight", "layers.8.norm0.bias", "layers.9.attention.W_q.weight", "layers.9.attention.W_q.bias", "layers.9.attention.W_k.weight", "layers.9.attention.W_k.bias", "layers.9.attention.W_v.weight", "layers.9.attention.W_v.bias", "layers.9.attention.W_out.weight", "layers.9.attention.W_out.bias", "layers.9.linear0.weight", "layers.9.linear0.bias", "layers.9.linear1.weight", "layers.9.linear1.bias", "layers.9.norm1.weight", "layers.9.norm1.bias", "layers.9.norm0.weight", "layers.9.norm0.bias". 
	size mismatch for tokenizer.weight: copying a param with shape torch.Size([101, 256]) from checkpoint, the shape in current model is torch.Size([101, 320]).
	size mismatch for tokenizer.bias: copying a param with shape torch.Size([100, 256]) from checkpoint, the shape in current model is torch.Size([100, 320]).
	size mismatch for layers.0.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.0.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.0.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.0.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.0.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.0.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.0.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.0.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.0.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([223, 320]).
	size mismatch for layers.0.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([223]).
	size mismatch for layers.0.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([320, 223]).
	size mismatch for layers.0.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.0.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.0.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.1.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.1.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.1.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.1.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.1.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.1.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.1.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.1.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.1.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([223, 320]).
	size mismatch for layers.1.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([223]).
	size mismatch for layers.1.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([320, 223]).
	size mismatch for layers.1.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.1.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.1.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.1.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.1.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.2.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.2.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.2.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.2.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.2.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.2.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.2.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.2.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.2.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([223, 320]).
	size mismatch for layers.2.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([223]).
	size mismatch for layers.2.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([320, 223]).
	size mismatch for layers.2.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.2.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.2.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.2.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.2.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.3.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.3.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.3.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.3.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.3.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.3.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.3.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.3.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.3.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([223, 320]).
	size mismatch for layers.3.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([223]).
	size mismatch for layers.3.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([320, 223]).
	size mismatch for layers.3.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.3.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.3.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.3.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.3.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.4.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.4.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.4.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.4.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.4.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.4.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.4.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.4.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.4.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([223, 320]).
	size mismatch for layers.4.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([223]).
	size mismatch for layers.4.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([320, 223]).
	size mismatch for layers.4.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.4.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.4.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.4.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.4.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.5.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.5.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.5.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.5.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.5.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.5.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.5.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.5.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.5.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([223, 320]).
	size mismatch for layers.5.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([223]).
	size mismatch for layers.5.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([320, 223]).
	size mismatch for layers.5.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.5.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.5.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.5.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.5.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.6.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.6.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.6.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.6.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.6.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.6.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.6.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.6.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.6.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([223, 320]).
	size mismatch for layers.6.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([223]).
	size mismatch for layers.6.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([320, 223]).
	size mismatch for layers.6.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.6.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.6.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.6.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.6.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.7.attention.W_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.7.attention.W_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.7.attention.W_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.7.attention.W_k.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.7.attention.W_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.7.attention.W_v.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.7.attention.W_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([320, 320]).
	size mismatch for layers.7.attention.W_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.7.linear0.weight: copying a param with shape torch.Size([562, 256]) from checkpoint, the shape in current model is torch.Size([223, 320]).
	size mismatch for layers.7.linear0.bias: copying a param with shape torch.Size([562]) from checkpoint, the shape in current model is torch.Size([223]).
	size mismatch for layers.7.linear1.weight: copying a param with shape torch.Size([256, 562]) from checkpoint, the shape in current model is torch.Size([320, 223]).
	size mismatch for layers.7.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.7.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.7.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.7.norm0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for layers.7.norm0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for last_normalization.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for last_normalization.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for head.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 320]).
[W 2025-08-12 19:42:35,033] Trial 8 failed with value None.
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:43:48 INFO]: Training loss at epoch 95: 0.36707818508148193
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:44:42 INFO]: Training loss at epoch 50: 0.45210734009742737
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
[08/12/2025 19:45:13 INFO]: New best epoch, val score: -0.7623236508646213
[08/12/2025 19:45:13 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:46:06 INFO]: Training loss at epoch 96: 0.38910478353500366
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:48:22 INFO]: Training loss at epoch 97: 0.36104443669319153
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
[08/12/2025 19:49:00 INFO]: Training loss at epoch 51: 0.47537195682525635
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:50:40 INFO]: Training loss at epoch 98: 0.33759352564811707
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:52:58 INFO]: Training loss at epoch 99: 0.4144538789987564
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:53:18 INFO]: Training loss at epoch 52: 0.48102526366710663
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:53:45 INFO]: Training stats: {
    "score": -0.6311993454276102,
    "rmse": 0.6311993454276102
}
[08/12/2025 19:53:45 INFO]: Val stats: {
    "score": -0.7652190710642942,
    "rmse": 0.7652190710642942
}
[08/12/2025 19:53:45 INFO]: Test stats: {
    "score": -0.7033574238057846,
    "rmse": 0.7033574238057846
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:53:49 INFO]: New best epoch, val score: -0.7124622778017103
[08/12/2025 19:53:49 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:56:03 INFO]: Training loss at epoch 100: 0.3391086012125015
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
[08/12/2025 19:57:38 INFO]: Training loss at epoch 53: 0.3854457587003708
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 19:58:20 INFO]: Training loss at epoch 101: 0.3381151854991913
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:00:38 INFO]: Training loss at epoch 102: 0.42728424072265625
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
[08/12/2025 20:01:57 INFO]: Training loss at epoch 54: 0.5529161393642426
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:02:55 INFO]: Training loss at epoch 103: 0.3854590505361557
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:05:14 INFO]: Training loss at epoch 104: 0.41267387568950653
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:06:15 INFO]: Training loss at epoch 55: 0.42893776297569275
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:07:31 INFO]: Training loss at epoch 105: 0.4745033085346222
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:09:49 INFO]: Training loss at epoch 106: 0.40656088292598724
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
[08/12/2025 20:10:34 INFO]: Training loss at epoch 56: 0.4231356233358383
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
[08/12/2025 20:11:05 INFO]: New best epoch, val score: -0.6917383424612442
[08/12/2025 20:11:05 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
[08/12/2025 20:12:06 INFO]: Training loss at epoch 107: 0.35770612955093384
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:14:23 INFO]: Training loss at epoch 108: 0.29384902864694595
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:14:53 INFO]: Training loss at epoch 57: 0.4452556073665619
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:16:41 INFO]: Training loss at epoch 109: 0.3762071430683136
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:17:28 INFO]: Training stats: {
    "score": -0.604623542948333,
    "rmse": 0.604623542948333
}
[08/12/2025 20:17:28 INFO]: Val stats: {
    "score": -0.7276617818400027,
    "rmse": 0.7276617818400027
}
[08/12/2025 20:17:28 INFO]: Test stats: {
    "score": -0.7000317220135606,
    "rmse": 0.7000317220135606
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
[08/12/2025 20:19:11 INFO]: Training loss at epoch 58: 0.5420764684677124
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
[08/12/2025 20:19:46 INFO]: Training loss at epoch 110: 0.49368155002593994
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:22:04 INFO]: Training loss at epoch 111: 0.3637276589870453
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
[08/12/2025 20:23:29 INFO]: Training loss at epoch 59: 0.4456115663051605
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:24:22 INFO]: Training loss at epoch 112: 0.32045990228652954
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
[08/12/2025 20:24:57 INFO]: Training stats: {
    "score": -0.623132425052918,
    "rmse": 0.623132425052918
}
[08/12/2025 20:24:57 INFO]: Val stats: {
    "score": -0.7560095537182964,
    "rmse": 0.7560095537182964
}
[08/12/2025 20:24:57 INFO]: Test stats: {
    "score": -0.6621374338564585,
    "rmse": 0.6621374338564585
}
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:26:38 INFO]: Training loss at epoch 113: 0.3819824606180191
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:28:56 INFO]: Training loss at epoch 114: 0.4112504720687866
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:29:16 INFO]: Training loss at epoch 60: 0.3685770779848099
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:29:47 INFO]: New best epoch, val score: -0.6872823757738878
[08/12/2025 20:29:47 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:31:13 INFO]: Training loss at epoch 115: 0.4408668130636215
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:33:31 INFO]: Training loss at epoch 116: 0.46269460022449493
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:33:34 INFO]: Training loss at epoch 61: 0.41274210810661316
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:33:47 INFO]: New best epoch, val score: -0.6804790669126579
[08/12/2025 20:33:47 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:35:53 INFO]: Training loss at epoch 117: 0.4462403357028961
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:36:09 INFO]: New best epoch, val score: -0.6789711226588289
[08/12/2025 20:36:09 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
[08/12/2025 20:37:57 INFO]: Training loss at epoch 62: 0.37702685594558716
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:38:10 INFO]: Training loss at epoch 118: 0.3594072014093399
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:40:27 INFO]: Training loss at epoch 119: 0.3307984173297882
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:41:14 INFO]: Training stats: {
    "score": -0.5733088785052629,
    "rmse": 0.5733088785052629
}
[08/12/2025 20:41:14 INFO]: Val stats: {
    "score": -0.6951817176634483,
    "rmse": 0.6951817176634483
}
[08/12/2025 20:41:14 INFO]: Test stats: {
    "score": -0.6797445034425851,
    "rmse": 0.6797445034425851
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:42:16 INFO]: Training loss at epoch 63: 0.3410418778657913
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:43:33 INFO]: Training loss at epoch 120: 0.3583161234855652
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
[08/12/2025 20:43:49 INFO]: New best epoch, val score: -0.6707032840038777
[08/12/2025 20:43:49 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:45:51 INFO]: Training loss at epoch 121: 0.39053812623023987
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
[08/12/2025 20:46:32 INFO]: Training loss at epoch 64: 0.35202300548553467
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:48:08 INFO]: Training loss at epoch 122: 0.3197571039199829
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:50:26 INFO]: Training loss at epoch 123: 0.3827155977487564
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:50:51 INFO]: Training loss at epoch 65: 0.3643791079521179
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:52:43 INFO]: Training loss at epoch 124: 0.2984143793582916
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:55:00 INFO]: Training loss at epoch 125: 0.3340858519077301
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:55:08 INFO]: Training loss at epoch 66: 0.2887391000986099
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:57:17 INFO]: Training loss at epoch 126: 0.32998208701610565
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 20:59:27 INFO]: Training loss at epoch 67: 0.3278321623802185
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
[08/12/2025 20:59:34 INFO]: Training loss at epoch 127: 0.35329142212867737
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 21:01:51 INFO]: Training loss at epoch 128: 0.30251574516296387
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
[08/12/2025 21:03:46 INFO]: Training loss at epoch 68: 0.37686997652053833
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 21:04:08 INFO]: Training loss at epoch 129: 0.3101138547062874
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 21:04:54 INFO]: Training stats: {
    "score": -0.5747891395155811,
    "rmse": 0.5747891395155811
}
[08/12/2025 21:04:54 INFO]: Val stats: {
    "score": -0.7095075728801232,
    "rmse": 0.7095075728801232
}
[08/12/2025 21:04:54 INFO]: Test stats: {
    "score": -0.7003522463386951,
    "rmse": 0.7003522463386951
}
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 21:07:11 INFO]: Training loss at epoch 130: 0.31410928070545197
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
[08/12/2025 21:08:05 INFO]: Training loss at epoch 69: 0.42917636036872864
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
[08/12/2025 21:09:28 INFO]: Training loss at epoch 131: 0.36230234801769257
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
[08/12/2025 21:09:32 INFO]: Training stats: {
    "score": -0.5743739778723266,
    "rmse": 0.5743739778723266
}
[08/12/2025 21:09:32 INFO]: Val stats: {
    "score": -0.7112962956416737,
    "rmse": 0.7112962956416737
}
[08/12/2025 21:09:32 INFO]: Test stats: {
    "score": -0.6203123301652691,
    "rmse": 0.6203123301652691
}
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 21:11:45 INFO]: Training loss at epoch 132: 0.38108237087726593
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([26, 101, 512])
Tokenizer  -1: 512
[08/12/2025 21:13:50 INFO]: Training loss at epoch 70: 0.2931249141693115
Tokenizer output shape: torch.Size([66, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([26, 101, 256])
Tokenizer  -1: 256
[08/12/2025 21:14:03 INFO]: Training loss at epoch 133: 0.27642980217933655
Tokenizer output shape: torch.Size([66, 101, 256])
Tokenizer  -1: 256
Tokenizer output shape: torch.Size([87, 101, 512])
Tokenizer  -1: 512
Tokenizer output shape: torch.Size([87, 101, 256])
Tokenizer  -1: 256
[08/12/2025 21:14:19 INFO]: New best epoch, val score: -0.6672453095809817
[08/12/2025 21:14:19 INFO]: Saving model to: model_best.pth
Tokenizer output shape: torch.Size([256, 101, 256])
Tokenizer  -1: 256
