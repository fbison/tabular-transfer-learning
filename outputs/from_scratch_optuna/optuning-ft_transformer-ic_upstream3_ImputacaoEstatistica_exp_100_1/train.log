[08/27/2025 14:34:58 INFO]: Building Dataset
[08/27/2025 14:34:58 INFO]: pre normalizer.fit

[08/27/2025 14:34:58 INFO]: pos normalizer.fit

[08/27/2025 14:35:00 INFO]: Task: regression, Dataset: ic_upstream3_ImputacaoEstatistica_exp_100_1, n_numerical: 165, n_categorical: 0, n_classes: 1, n_train_samples: 282, n_val_samples: 66, n_test_samples: 87
[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 1.9185478467797514
  attention_dropout: 0.09150747772900486
  ffn_dropout: 0.09150747772900486
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0005410738202370414
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: -1
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_1

[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 1.2032399474725262
  attention_dropout: 0.24564957158123651
  ffn_dropout: 0.24564957158123651
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00038960559289361496
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: -1
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_3

[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 0.7684173693461319
  attention_dropout: 0.08278286430266157
  ffn_dropout: 0.08278286430266157
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00033836542834434364
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: -1
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_2

[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.0232105868581312
  attention_dropout: 0.4801232997271212
  ffn_dropout: 0.4801232997271212
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.9716262985171643e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_4

[08/27/2025 14:35:07 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 1.7892899534497606
  attention_dropout: 0.2900440609648356
  ffn_dropout: 0.2900440609648356
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.3183817627114994e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_5

[08/27/2025 14:35:07 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 0.9848187475455704
  attention_dropout: 0.06818766786053854
  ffn_dropout: 0.06818766786053854
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00017336228613877311
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_6

[08/27/2025 14:35:07 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 2.378572060602841
  attention_dropout: 0.4801936713533806
  ffn_dropout: 0.4801936713533806
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.9489491442597975e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_7

[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 0.7120219208903297
  attention_dropout: 0.35480868899360946
  ffn_dropout: 0.35480868899360946
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00015330390636987925
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_8

[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: 
_________________________________________________

[08/27/2025 14:35:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:07 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 1.8245690617331252
  attention_dropout: 0.46219124557707675
  ffn_dropout: 0.46219124557707675
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.866520811832112e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_11

[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 1.3973497578958325
  attention_dropout: 0.24755970254018134
  ffn_dropout: 0.24755970254018134
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.6668216532830602e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_10

[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.827975645959261
  attention_dropout: 0.05108492323907471
  ffn_dropout: 0.05108492323907471
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.046224629750705e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_12

[08/27/2025 14:35:08 INFO]: 
_________________________________________________

[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 1.5679397900790621
  attention_dropout: 0.0737563196786668
  ffn_dropout: 0.0737563196786668
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0008798024215129145
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_9

[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 2.6065662567653973
  attention_dropout: 0.19524831422141836
  ffn_dropout: 0.19524831422141836
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00042528050115962985
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_13

[08/27/2025 14:35:08 INFO]: 
_________________________________________________

[08/27/2025 14:35:08 INFO]: 
_________________________________________________

[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: 
_________________________________________________

[08/27/2025 14:35:08 INFO]: 
_________________________________________________

[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: 
_________________________________________________

[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 1.699889698003409
  attention_dropout: 0.11362409884617525
  ffn_dropout: 0.11362409884617525
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.4941584867600954e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_15

[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: 
_________________________________________________

[08/27/2025 14:35:08 INFO]: 
_________________________________________________

[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 1.8162806645306078
  attention_dropout: 0.28200364984299087
  ffn_dropout: 0.28200364984299087
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 6.538324149361138e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_14

[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.4811500683451269
  attention_dropout: 0.24427702040891858
  ffn_dropout: 0.24427702040891858
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00047211896592905917
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_17

[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 16
  d_ffn_factor: 1.606345249214495
  attention_dropout: 0.17746778200954305
  ffn_dropout: 0.17746778200954305
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.412405114626581e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_16

[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 2.5760167778622147
  attention_dropout: 0.24974937827344806
  ffn_dropout: 0.24974937827344806
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.733833903368078e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_20

[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 0.8311404301852819
  attention_dropout: 0.28528415234992277
  ffn_dropout: 0.28528415234992277
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00016061273198750392
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_19

[08/27/2025 14:35:08 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 1.4382236531499095
  attention_dropout: 0.49964526570395096
  ffn_dropout: 0.49964526570395096
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00032710809802743296
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_18

[08/27/2025 14:35:08 INFO]: This ft_transformer has 3.885 million parameters.
[08/27/2025 14:35:08 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:09 INFO]: This ft_transformer has 1.680 million parameters.
[08/27/2025 14:35:09 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:10 INFO]: This ft_transformer has 0.152 million parameters.
[08/27/2025 14:35:10 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:10 INFO]: This ft_transformer has 1.003 million parameters.
[08/27/2025 14:35:10 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:10 INFO]: This ft_transformer has 4.506 million parameters.
[08/27/2025 14:35:10 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:11 INFO]: This ft_transformer has 10.277 million parameters.
[08/27/2025 14:35:11 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:11 INFO]: This ft_transformer has 8.728 million parameters.
[08/27/2025 14:35:11 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:11 INFO]: This ft_transformer has 4.300 million parameters.
[08/27/2025 14:35:11 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:12 INFO]: This ft_transformer has 9.100 million parameters.
[08/27/2025 14:35:12 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:12 INFO]: This ft_transformer has 0.260 million parameters.
[08/27/2025 14:35:12 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:12 INFO]: This ft_transformer has 7.308 million parameters.
[08/27/2025 14:35:12 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:12 INFO]: This ft_transformer has 9.180 million parameters.
[08/27/2025 14:35:12 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:12 INFO]: This ft_transformer has 25.009 million parameters.
[08/27/2025 14:35:12 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:12 INFO]: This ft_transformer has 14.631 million parameters.
[08/27/2025 14:35:12 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:12 INFO]: This ft_transformer has 0.903 million parameters.
[08/27/2025 14:35:12 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:12 INFO]: This ft_transformer has 7.877 million parameters.
[08/27/2025 14:35:12 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:12 INFO]: This ft_transformer has 8.663 million parameters.
[08/27/2025 14:35:12 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:12 INFO]: This ft_transformer has 8.329 million parameters.
[08/27/2025 14:35:12 INFO]: This ft_transformer has 1.033 million parameters.
[08/27/2025 14:35:12 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:12 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:12 INFO]: This ft_transformer has 13.666 million parameters.
[08/27/2025 14:35:12 INFO]: Training will start at epoch 0.
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:26 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:35:52 INFO]: Training loss at epoch 0: 1.6352810859680176
[08/27/2025 14:35:55 INFO]: New best epoch, val score: -1.135625652050921
[08/27/2025 14:35:55 INFO]: Saving model to: unhanged-Shanesha_trial_17/model_best.pth
[08/27/2025 14:36:02 INFO]: Training loss at epoch 0: 0.9743237197399139
[08/27/2025 14:36:07 INFO]: New best epoch, val score: -0.6925972963188692
[08/27/2025 14:36:07 INFO]: Saving model to: unhanged-Shanesha_trial_12/model_best.pth
[08/27/2025 14:36:12 INFO]: Training loss at epoch 0: 1.0932963192462921
[08/27/2025 14:36:19 INFO]: New best epoch, val score: -0.8494466034194285
[08/27/2025 14:36:19 INFO]: Saving model to: unhanged-Shanesha_trial_1/model_best.pth
[08/27/2025 14:36:20 INFO]: Training loss at epoch 1: 1.4106378555297852
[08/27/2025 14:36:28 INFO]: Training loss at epoch 0: 0.981944739818573
[08/27/2025 14:36:37 INFO]: Training loss at epoch 0: 1.121390163898468
[08/27/2025 14:36:38 INFO]: New best epoch, val score: -0.6708029802529708
[08/27/2025 14:36:38 INFO]: Saving model to: unhanged-Shanesha_trial_9/model_best.pth
[08/27/2025 14:36:39 INFO]: Training loss at epoch 0: 1.7598299384117126
[08/27/2025 14:36:45 INFO]: Training loss at epoch 1: 1.0423532724380493
[08/27/2025 14:36:46 INFO]: New best epoch, val score: -0.6652745817488454
[08/27/2025 14:36:46 INFO]: Saving model to: unhanged-Shanesha_trial_4/model_best.pth
[08/27/2025 14:36:48 INFO]: Training loss at epoch 0: 1.2534659504890442
[08/27/2025 14:36:49 INFO]: New best epoch, val score: -0.9335854513063967
[08/27/2025 14:36:49 INFO]: Saving model to: unhanged-Shanesha_trial_6/model_best.pth
[08/27/2025 14:36:50 INFO]: New best epoch, val score: -0.6705790281542909
[08/27/2025 14:36:50 INFO]: Saving model to: unhanged-Shanesha_trial_12/model_best.pth
[08/27/2025 14:36:51 INFO]: Training loss at epoch 2: 1.685594081878662
[08/27/2025 14:36:54 INFO]: New best epoch, val score: -0.7719637640185381
[08/27/2025 14:36:54 INFO]: Saving model to: unhanged-Shanesha_trial_17/model_best.pth
[08/27/2025 14:36:59 INFO]: New best epoch, val score: -0.8089162120198276
[08/27/2025 14:36:59 INFO]: Saving model to: unhanged-Shanesha_trial_19/model_best.pth
[08/27/2025 14:37:07 INFO]: Training loss at epoch 1: 1.044315218925476
[08/27/2025 14:37:13 INFO]: New best epoch, val score: -0.6713112048594697
[08/27/2025 14:37:13 INFO]: Saving model to: unhanged-Shanesha_trial_1/model_best.pth
[08/27/2025 14:37:18 INFO]: Training loss at epoch 0: 0.9259320497512817
[08/27/2025 14:37:19 INFO]: Training loss at epoch 3: 0.8890190124511719
[08/27/2025 14:37:25 INFO]: Training loss at epoch 2: 1.0910869240760803
[08/27/2025 14:37:30 INFO]: New best epoch, val score: -0.6663450246001237
[08/27/2025 14:37:30 INFO]: Saving model to: unhanged-Shanesha_trial_12/model_best.pth
[08/27/2025 14:37:32 INFO]: New best epoch, val score: -0.6726612036080075
[08/27/2025 14:37:32 INFO]: Saving model to: unhanged-Shanesha_trial_15/model_best.pth
[08/27/2025 14:37:37 INFO]: Training loss at epoch 1: 1.175467848777771
[08/27/2025 14:37:47 INFO]: Training loss at epoch 4: 1.5166822671890259
[08/27/2025 14:37:53 INFO]: Training loss at epoch 1: 1.0126819014549255
[08/27/2025 14:37:58 INFO]: Training loss at epoch 1: 1.2407965660095215
[08/27/2025 14:37:59 INFO]: Training loss at epoch 2: 1.01301771402359
[08/27/2025 14:38:05 INFO]: Training loss at epoch 3: 1.0262799859046936
[08/27/2025 14:38:06 INFO]: New best epoch, val score: -0.6583399696619493
[08/27/2025 14:38:06 INFO]: Saving model to: unhanged-Shanesha_trial_1/model_best.pth
[08/27/2025 14:38:08 INFO]: New best epoch, val score: -0.6812842903788197
[08/27/2025 14:38:08 INFO]: Saving model to: unhanged-Shanesha_trial_6/model_best.pth
[08/27/2025 14:38:15 INFO]: Training loss at epoch 5: 1.106002688407898
[08/27/2025 14:38:17 INFO]: Training loss at epoch 1: 1.1309621036052704
[08/27/2025 14:38:23 INFO]: Training loss at epoch 0: 1.0434291362762451
[08/27/2025 14:38:43 INFO]: Training loss at epoch 6: 1.0653572380542755
[08/27/2025 14:38:44 INFO]: Training loss at epoch 2: 1.3026402592658997
[08/27/2025 14:38:44 INFO]: Training loss at epoch 4: 0.9583352506160736
[08/27/2025 14:38:47 INFO]: New best epoch, val score: -0.8235869003466069
[08/27/2025 14:38:47 INFO]: Saving model to: unhanged-Shanesha_trial_10/model_best.pth
[08/27/2025 14:38:50 INFO]: Training loss at epoch 3: 1.1125346422195435
[08/27/2025 14:38:52 INFO]: New best epoch, val score: -0.6660856476489758
[08/27/2025 14:38:52 INFO]: Saving model to: unhanged-Shanesha_trial_9/model_best.pth
[08/27/2025 14:39:09 INFO]: Training loss at epoch 0: 1.2296617031097412
[08/27/2025 14:39:09 INFO]: Training loss at epoch 2: 0.8305643498897552
[08/27/2025 14:39:10 INFO]: Training loss at epoch 7: 1.381888747215271
[08/27/2025 14:39:19 INFO]: Training loss at epoch 2: 1.2633691430091858
[08/27/2025 14:39:20 INFO]: Training loss at epoch 1: 1.0473618507385254
[08/27/2025 14:39:20 INFO]: Training loss at epoch 0: 1.2785146236419678
[08/27/2025 14:39:24 INFO]: Training loss at epoch 5: 0.9392682611942291
[08/27/2025 14:39:35 INFO]: Training loss at epoch 0: 1.231566071510315
[08/27/2025 14:39:37 INFO]: New best epoch, val score: -0.6691291951079705
[08/27/2025 14:39:37 INFO]: Saving model to: unhanged-Shanesha_trial_14/model_best.pth
[08/27/2025 14:39:38 INFO]: Training loss at epoch 0: 1.1476743817329407
[08/27/2025 14:39:39 INFO]: Training loss at epoch 8: 1.3232216835021973
[08/27/2025 14:39:41 INFO]: Training loss at epoch 4: 1.211764633655548
[08/27/2025 14:39:46 INFO]: Training loss at epoch 2: 1.3035385012626648
[08/27/2025 14:39:51 INFO]: New best epoch, val score: -0.6730772633997071
[08/27/2025 14:39:51 INFO]: Saving model to: unhanged-Shanesha_trial_7/model_best.pth
[08/27/2025 14:39:51 INFO]: Training loss at epoch 3: 1.075778841972351
[08/27/2025 14:39:53 INFO]: Training loss at epoch 0: 1.1449716091156006
[08/27/2025 14:39:56 INFO]: Training loss at epoch 0: 1.014811098575592
[08/27/2025 14:40:00 INFO]: Training loss at epoch 0: 0.9504721164703369
[08/27/2025 14:40:04 INFO]: Training loss at epoch 6: 1.1000850796699524
[08/27/2025 14:40:06 INFO]: Training loss at epoch 9: 1.0984365940093994
[08/27/2025 14:40:08 INFO]: New best epoch, val score: -0.6612635026878857
[08/27/2025 14:40:08 INFO]: Saving model to: unhanged-Shanesha_trial_3/model_best.pth
[08/27/2025 14:40:12 INFO]: New best epoch, val score: -0.850231640726166
[08/27/2025 14:40:12 INFO]: Saving model to: unhanged-Shanesha_trial_2/model_best.pth
[08/27/2025 14:40:16 INFO]: Training stats: {
    "score": -1.0846307484281574,
    "rmse": 1.0846307484281574
}
[08/27/2025 14:40:16 INFO]: Val stats: {
    "score": -0.8848923110472608,
    "rmse": 0.8848923110472608
}
[08/27/2025 14:40:16 INFO]: Test stats: {
    "score": -1.0127284375994576,
    "rmse": 1.0127284375994576
}
[08/27/2025 14:40:24 INFO]: Training loss at epoch 3: 1.077637493610382
[08/27/2025 14:40:30 INFO]: New best epoch, val score: -0.6595725754984487
[08/27/2025 14:40:30 INFO]: Saving model to: unhanged-Shanesha_trial_8/model_best.pth
[08/27/2025 14:40:33 INFO]: Training loss at epoch 0: 0.8407784402370453
[08/27/2025 14:40:33 INFO]: Training loss at epoch 5: 0.7938258051872253
[08/27/2025 14:40:33 INFO]: New best epoch, val score: -0.6622614155487772
[08/27/2025 14:40:33 INFO]: Saving model to: unhanged-Shanesha_trial_5/model_best.pth
[08/27/2025 14:40:39 INFO]: New best epoch, val score: -0.7483031585949143
[08/27/2025 14:40:39 INFO]: Saving model to: unhanged-Shanesha_trial_11/model_best.pth
[08/27/2025 14:40:41 INFO]: Training loss at epoch 3: 1.1489730477333069
[08/27/2025 14:40:46 INFO]: Training loss at epoch 7: 0.988764226436615
[08/27/2025 14:40:47 INFO]: Training loss at epoch 10: 0.9792489409446716
[08/27/2025 14:40:50 INFO]: New best epoch, val score: -0.719334872021906
[08/27/2025 14:40:50 INFO]: Saving model to: unhanged-Shanesha_trial_17/model_best.pth
[08/27/2025 14:40:51 INFO]: New best epoch, val score: -0.6641426274557848
[08/27/2025 14:40:51 INFO]: Saving model to: unhanged-Shanesha_trial_6/model_best.pth
[08/27/2025 14:41:00 INFO]: Training loss at epoch 4: 1.0541256964206696
[08/27/2025 14:41:09 INFO]: New best epoch, val score: -0.6647375605001647
[08/27/2025 14:41:09 INFO]: Saving model to: unhanged-Shanesha_trial_9/model_best.pth
[08/27/2025 14:41:15 INFO]: Training loss at epoch 11: 0.9109373986721039
[08/27/2025 14:41:16 INFO]: New best epoch, val score: -0.7213519462057586
[08/27/2025 14:41:16 INFO]: Saving model to: unhanged-Shanesha_trial_16/model_best.pth
[08/27/2025 14:41:19 INFO]: Training loss at epoch 3: 1.1517633199691772
[08/27/2025 14:41:20 INFO]: New best epoch, val score: -0.6617509018501858
[08/27/2025 14:41:20 INFO]: Saving model to: unhanged-Shanesha_trial_17/model_best.pth
[08/27/2025 14:41:25 INFO]: Training loss at epoch 2: 1.13320654630661
[08/27/2025 14:41:27 INFO]: Training loss at epoch 6: 1.017814815044403
[08/27/2025 14:41:27 INFO]: Training loss at epoch 8: 0.9930124878883362
[08/27/2025 14:41:31 INFO]: New best epoch, val score: -0.7358038780358551
[08/27/2025 14:41:31 INFO]: Saving model to: unhanged-Shanesha_trial_19/model_best.pth
[08/27/2025 14:41:31 INFO]: Training loss at epoch 0: 1.188107430934906
[08/27/2025 14:41:37 INFO]: Training loss at epoch 0: 0.9617076516151428
[08/27/2025 14:41:43 INFO]: Training loss at epoch 4: 1.0970717072486877
[08/27/2025 14:41:44 INFO]: Training loss at epoch 12: 0.9978969991207123
[08/27/2025 14:41:45 INFO]: Training loss at epoch 1: 1.207909345626831
[08/27/2025 14:42:02 INFO]: Training loss at epoch 4: 1.0765863060951233
[08/27/2025 14:42:06 INFO]: Training loss at epoch 9: 0.8229864537715912
[08/27/2025 14:42:08 INFO]: Training loss at epoch 5: 0.8580622375011444
[08/27/2025 14:42:09 INFO]: New best epoch, val score: -0.6605919773462774
[08/27/2025 14:42:09 INFO]: Saving model to: unhanged-Shanesha_trial_10/model_best.pth
[08/27/2025 14:42:12 INFO]: Training loss at epoch 13: 1.0701310634613037
[08/27/2025 14:42:17 INFO]: Training loss at epoch 7: 0.8701536357402802
[08/27/2025 14:42:19 INFO]: New best epoch, val score: -0.6991074735658063
[08/27/2025 14:42:19 INFO]: Saving model to: unhanged-Shanesha_trial_18/model_best.pth
[08/27/2025 14:42:20 INFO]: Training stats: {
    "score": -1.001497276294877,
    "rmse": 1.001497276294877
}
[08/27/2025 14:42:20 INFO]: Val stats: {
    "score": -0.7139911196058784,
    "rmse": 0.7139911196058784
}
[08/27/2025 14:42:20 INFO]: Test stats: {
    "score": -0.8930956062354695,
    "rmse": 0.8930956062354695
}
[08/27/2025 14:42:25 INFO]: New best epoch, val score: -0.6874256356835879
[08/27/2025 14:42:25 INFO]: Saving model to: unhanged-Shanesha_trial_20/model_best.pth
[08/27/2025 14:42:41 INFO]: Training loss at epoch 14: 1.2091671228408813
[08/27/2025 14:42:49 INFO]: Training loss at epoch 4: 1.032192587852478
[08/27/2025 14:43:01 INFO]: Training loss at epoch 5: 1.1271171569824219
[08/27/2025 14:43:01 INFO]: Training loss at epoch 10: 1.0288299918174744
[08/27/2025 14:43:02 INFO]: New best epoch, val score: -0.6664876076646269
[08/27/2025 14:43:02 INFO]: Saving model to: unhanged-Shanesha_trial_19/model_best.pth
[08/27/2025 14:43:09 INFO]: Training loss at epoch 8: 1.0400993824005127
[08/27/2025 14:43:11 INFO]: Training loss at epoch 15: 1.240586280822754
[08/27/2025 14:43:19 INFO]: Training loss at epoch 6: 1.0695825219154358
[08/27/2025 14:43:23 INFO]: Training loss at epoch 5: 0.8176288604736328
[08/27/2025 14:43:25 INFO]: Training loss at epoch 1: 1.3134570717811584
[08/27/2025 14:43:30 INFO]: Training loss at epoch 3: 1.0567155480384827
[08/27/2025 14:43:39 INFO]: Training loss at epoch 16: 1.2623426914215088
[08/27/2025 14:43:43 INFO]: Training loss at epoch 11: 0.9915544390678406
[08/27/2025 14:43:48 INFO]: Training loss at epoch 1: 1.2955613732337952
[08/27/2025 14:44:03 INFO]: Training loss at epoch 9: 0.8797075748443604
[08/27/2025 14:44:09 INFO]: Training loss at epoch 17: 0.9378728866577148
[08/27/2025 14:44:13 INFO]: New best epoch, val score: -0.6612331214211792
[08/27/2025 14:44:13 INFO]: Saving model to: unhanged-Shanesha_trial_17/model_best.pth
[08/27/2025 14:44:21 INFO]: Training loss at epoch 6: 1.2790991067886353
[08/27/2025 14:44:23 INFO]: Training loss at epoch 5: 1.1236074566841125
[08/27/2025 14:44:23 INFO]: Training stats: {
    "score": -0.9999255967584302,
    "rmse": 0.9999255967584302
}
[08/27/2025 14:44:23 INFO]: Val stats: {
    "score": -0.6723638216091175,
    "rmse": 0.6723638216091175
}
[08/27/2025 14:44:23 INFO]: Test stats: {
    "score": -0.8713464914960489,
    "rmse": 0.8713464914960489
}
[08/27/2025 14:44:23 INFO]: Training loss at epoch 1: 5.322809398174286
[08/27/2025 14:44:25 INFO]: Training loss at epoch 12: 0.8439792692661285
[08/27/2025 14:44:27 INFO]: Training loss at epoch 7: 0.986699640750885
[08/27/2025 14:44:27 INFO]: Training loss at epoch 1: 3.7587108612060547
[08/27/2025 14:44:35 INFO]: New best epoch, val score: -0.6662834538697167
[08/27/2025 14:44:35 INFO]: Saving model to: unhanged-Shanesha_trial_19/model_best.pth
[08/27/2025 14:44:38 INFO]: Training loss at epoch 18: 1.162517249584198
[08/27/2025 14:44:44 INFO]: Training loss at epoch 6: 0.9801385402679443
[08/27/2025 14:45:05 INFO]: Training loss at epoch 1: 1.0783597230911255
[08/27/2025 14:45:08 INFO]: Training loss at epoch 13: 0.8950397670269012
[08/27/2025 14:45:10 INFO]: Training loss at epoch 19: 0.9736651182174683
[08/27/2025 14:45:11 INFO]: Training loss at epoch 2: 1.1157053709030151
[08/27/2025 14:45:11 INFO]: Training loss at epoch 1: 0.9996126294136047
[08/27/2025 14:45:17 INFO]: Training loss at epoch 10: 1.018713116645813
[08/27/2025 14:45:19 INFO]: Training loss at epoch 1: 1.1204243302345276
[08/27/2025 14:45:21 INFO]: Training stats: {
    "score": -0.9990099529257316,
    "rmse": 0.9990099529257316
}
[08/27/2025 14:45:21 INFO]: Val stats: {
    "score": -0.6899003393941319,
    "rmse": 0.6899003393941319
}
[08/27/2025 14:45:21 INFO]: Test stats: {
    "score": -0.8783330903456371,
    "rmse": 0.8783330903456371
}
[08/27/2025 14:45:27 INFO]: Training loss at epoch 0: 1.0276165306568146
[08/27/2025 14:45:37 INFO]: Training loss at epoch 8: 1.0228068828582764
[08/27/2025 14:45:38 INFO]: Training loss at epoch 4: 1.0073360204696655
[08/27/2025 14:45:43 INFO]: Training loss at epoch 7: 1.3882991671562195
[08/27/2025 14:45:50 INFO]: Training loss at epoch 20: 1.1494911313056946
[08/27/2025 14:45:50 INFO]: Training loss at epoch 14: 1.0334846377372742
[08/27/2025 14:45:54 INFO]: New best epoch, val score: -0.6655456791556166
[08/27/2025 14:45:54 INFO]: Saving model to: unhanged-Shanesha_trial_15/model_best.pth
[08/27/2025 14:45:58 INFO]: New best epoch, val score: -0.6696717237400492
[08/27/2025 14:45:58 INFO]: Saving model to: unhanged-Shanesha_trial_11/model_best.pth
[08/27/2025 14:45:59 INFO]: Training loss at epoch 6: 0.8932490944862366
[08/27/2025 14:46:08 INFO]: Training loss at epoch 7: 0.9724905490875244
[08/27/2025 14:46:10 INFO]: Training loss at epoch 11: 1.0251353085041046
[08/27/2025 14:46:10 INFO]: New best epoch, val score: -0.6658425648417111
[08/27/2025 14:46:10 INFO]: Saving model to: unhanged-Shanesha_trial_19/model_best.pth
[08/27/2025 14:46:19 INFO]: Training loss at epoch 21: 0.9907479584217072
[08/27/2025 14:46:31 INFO]: Training loss at epoch 1: 0.9391929507255554
[08/27/2025 14:46:32 INFO]: Training loss at epoch 15: 1.1156326532363892
[08/27/2025 14:46:46 INFO]: Training loss at epoch 9: 1.062915861606598
[08/27/2025 14:46:47 INFO]: New best epoch, val score: -0.6939739470185765
[08/27/2025 14:46:47 INFO]: Saving model to: unhanged-Shanesha_trial_13/model_best.pth
[08/27/2025 14:46:47 INFO]: Training loss at epoch 22: 1.0914262533187866
[08/27/2025 14:47:02 INFO]: Training loss at epoch 8: 1.0254181921482086
[08/27/2025 14:47:02 INFO]: Training loss at epoch 12: 0.9647833108901978
[08/27/2025 14:47:11 INFO]: Training stats: {
    "score": -1.0024560991574136,
    "rmse": 1.0024560991574136
}
[08/27/2025 14:47:11 INFO]: Val stats: {
    "score": -0.6657700514264647,
    "rmse": 0.6657700514264647
}
[08/27/2025 14:47:11 INFO]: Test stats: {
    "score": -0.8674531580883551,
    "rmse": 0.8674531580883551
}
[08/27/2025 14:47:12 INFO]: Training loss at epoch 16: 1.221956193447113
[08/27/2025 14:47:13 INFO]: New best epoch, val score: -0.6973795558955451
[08/27/2025 14:47:13 INFO]: Saving model to: unhanged-Shanesha_trial_16/model_best.pth
[08/27/2025 14:47:16 INFO]: Training loss at epoch 23: 0.8873895704746246
[08/27/2025 14:47:26 INFO]: Training loss at epoch 8: 0.803615927696228
[08/27/2025 14:47:30 INFO]: Training loss at epoch 7: 1.0612151622772217
[08/27/2025 14:47:42 INFO]: New best epoch, val score: -0.6653794641244765
[08/27/2025 14:47:42 INFO]: Saving model to: unhanged-Shanesha_trial_19/model_best.pth
[08/27/2025 14:47:43 INFO]: Training loss at epoch 2: 1.1840253472328186
[08/27/2025 14:47:44 INFO]: Training loss at epoch 5: 1.0307957828044891
[08/27/2025 14:47:45 INFO]: Training loss at epoch 24: 0.8138814568519592
[08/27/2025 14:47:53 INFO]: Training loss at epoch 17: 1.1481146812438965
[08/27/2025 14:47:55 INFO]: Training loss at epoch 13: 0.9343489408493042
[08/27/2025 14:48:15 INFO]: Training loss at epoch 25: 1.1687471568584442
[08/27/2025 14:48:20 INFO]: Training loss at epoch 10: 1.0749348998069763
[08/27/2025 14:48:22 INFO]: Training loss at epoch 2: 1.3832830786705017
[08/27/2025 14:48:23 INFO]: Training loss at epoch 9: 1.1603130102157593
[08/27/2025 14:48:35 INFO]: Training loss at epoch 1: 2.573780119419098
[08/27/2025 14:48:36 INFO]: Training loss at epoch 18: 1.024903118610382
[08/27/2025 14:48:37 INFO]: Training loss at epoch 3: 0.9137288331985474
[08/27/2025 14:48:44 INFO]: Training loss at epoch 26: 0.9097166657447815
[08/27/2025 14:48:45 INFO]: Training loss at epoch 1: 1.094171404838562
[08/27/2025 14:48:47 INFO]: Training loss at epoch 14: 1.0726793110370636
[08/27/2025 14:48:48 INFO]: Training loss at epoch 9: 0.9020068645477295
[08/27/2025 14:48:49 INFO]: Training stats: {
    "score": -1.0308121214184571,
    "rmse": 1.0308121214184571
}
[08/27/2025 14:48:49 INFO]: Val stats: {
    "score": -0.7778721100965645,
    "rmse": 0.7778721100965645
}
[08/27/2025 14:48:49 INFO]: Test stats: {
    "score": -0.9235891773269477,
    "rmse": 0.9235891773269477
}
[08/27/2025 14:49:03 INFO]: Training loss at epoch 8: 1.218911051750183
[08/27/2025 14:49:13 INFO]: Training loss at epoch 27: 0.8956244885921478
[08/27/2025 14:49:15 INFO]: Training loss at epoch 2: 2.5498006939888
[08/27/2025 14:49:15 INFO]: New best epoch, val score: -0.6649612990205235
[08/27/2025 14:49:15 INFO]: Saving model to: unhanged-Shanesha_trial_19/model_best.pth
[08/27/2025 14:49:16 INFO]: Training stats: {
    "score": -1.0112141364208136,
    "rmse": 1.0112141364208136
}
[08/27/2025 14:49:16 INFO]: Val stats: {
    "score": -0.7294545369569586,
    "rmse": 0.7294545369569586
}
[08/27/2025 14:49:16 INFO]: Test stats: {
    "score": -0.9049140756768819,
    "rmse": 0.9049140756768819
}
[08/27/2025 14:49:17 INFO]: Training loss at epoch 19: 1.0320715010166168
[08/27/2025 14:49:20 INFO]: Training loss at epoch 2: 1.7930923700332642
[08/27/2025 14:49:29 INFO]: Training loss at epoch 11: 0.9668695330619812
[08/27/2025 14:49:32 INFO]: Training stats: {
    "score": -0.9971571366755885,
    "rmse": 0.9971571366755885
}
[08/27/2025 14:49:32 INFO]: Val stats: {
    "score": -0.6788761971402527,
    "rmse": 0.6788761971402527
}
[08/27/2025 14:49:32 INFO]: Test stats: {
    "score": -0.8742914323410617,
    "rmse": 0.8742914323410617
}
[08/27/2025 14:49:36 INFO]: New best epoch, val score: -0.6824212883850299
[08/27/2025 14:49:36 INFO]: Saving model to: unhanged-Shanesha_trial_20/model_best.pth
[08/27/2025 14:49:40 INFO]: Training loss at epoch 15: 1.1097718477249146
[08/27/2025 14:49:43 INFO]: Training loss at epoch 28: 1.1697494685649872
[08/27/2025 14:49:52 INFO]: Training loss at epoch 6: 1.0021955072879791
[08/27/2025 14:50:07 INFO]: Training loss at epoch 10: 1.1905492544174194
[08/27/2025 14:50:11 INFO]: Training loss at epoch 29: 1.0690568089485168
[08/27/2025 14:50:12 INFO]: Training loss at epoch 20: 0.9680458903312683
[08/27/2025 14:50:16 INFO]: Training loss at epoch 2: 1.0404012203216553
[08/27/2025 14:50:22 INFO]: Training stats: {
    "score": -0.99793439126811,
    "rmse": 0.99793439126811
}
[08/27/2025 14:50:22 INFO]: Val stats: {
    "score": -0.6765525460933371,
    "rmse": 0.6765525460933371
}
[08/27/2025 14:50:22 INFO]: Test stats: {
    "score": -0.8722144233562142,
    "rmse": 0.8722144233562142
}
[08/27/2025 14:50:26 INFO]: Training loss at epoch 2: 1.4151694774627686
[08/27/2025 14:50:32 INFO]: Training loss at epoch 16: 1.0173602998256683
[08/27/2025 14:50:36 INFO]: Training loss at epoch 9: 0.9124704599380493
[08/27/2025 14:50:37 INFO]: Training loss at epoch 2: 1.0084309875965118
[08/27/2025 14:50:38 INFO]: Training loss at epoch 12: 1.0787250697612762
[08/27/2025 14:50:38 INFO]: Training loss at epoch 10: 1.1115705966949463
[08/27/2025 14:50:51 INFO]: Training loss at epoch 30: 1.0060992538928986
[08/27/2025 14:50:54 INFO]: Training loss at epoch 21: 1.0534470677375793
[08/27/2025 14:51:08 INFO]: Training stats: {
    "score": -1.0155196689765569,
    "rmse": 1.0155196689765569
}
[08/27/2025 14:51:08 INFO]: Val stats: {
    "score": -0.6646620254450489,
    "rmse": 0.6646620254450489
}
[08/27/2025 14:51:08 INFO]: Test stats: {
    "score": -0.8721934806032174,
    "rmse": 0.8721934806032174
}
[08/27/2025 14:51:20 INFO]: Training loss at epoch 31: 0.9073722958564758
[08/27/2025 14:51:20 INFO]: New best epoch, val score: -0.6646620254450489
[08/27/2025 14:51:20 INFO]: Saving model to: unhanged-Shanesha_trial_19/model_best.pth
[08/27/2025 14:51:23 INFO]: Training loss at epoch 17: 0.97535839676857
[08/27/2025 14:51:28 INFO]: Training loss at epoch 11: 1.0797224640846252
[08/27/2025 14:51:36 INFO]: Training loss at epoch 22: 0.9739684760570526
[08/27/2025 14:51:46 INFO]: Training loss at epoch 13: 1.0717856884002686
[08/27/2025 14:51:49 INFO]: Training loss at epoch 32: 0.8731387257575989
[08/27/2025 14:52:00 INFO]: Training loss at epoch 7: 1.1845442950725555
[08/27/2025 14:52:00 INFO]: Training loss at epoch 11: 0.8642400801181793
[08/27/2025 14:52:03 INFO]: Training loss at epoch 4: 1.115129292011261
[08/27/2025 14:52:05 INFO]: Training loss at epoch 3: 1.0728703737258911
[08/27/2025 14:52:17 INFO]: Training loss at epoch 18: 0.9306398034095764
[08/27/2025 14:52:19 INFO]: Training loss at epoch 23: 1.243064820766449
[08/27/2025 14:52:20 INFO]: Training loss at epoch 33: 1.1586403548717499
[08/27/2025 14:52:30 INFO]: Training loss at epoch 2: 1.0984784364700317
[08/27/2025 14:52:36 INFO]: New best epoch, val score: -0.6618876095312907
[08/27/2025 14:52:36 INFO]: Saving model to: unhanged-Shanesha_trial_14/model_best.pth
[08/27/2025 14:52:43 INFO]: Training loss at epoch 10: 0.9948307871818542
[08/27/2025 14:52:49 INFO]: Training loss at epoch 12: 1.212848961353302
[08/27/2025 14:52:51 INFO]: Training loss at epoch 34: 0.9732391238212585
[08/27/2025 14:52:55 INFO]: Training loss at epoch 3: 1.0650001168251038
[08/27/2025 14:52:59 INFO]: Training loss at epoch 14: 0.8927742540836334
[08/27/2025 14:53:01 INFO]: Training loss at epoch 24: 1.1086719036102295
[08/27/2025 14:53:09 INFO]: Training loss at epoch 19: 1.0908542275428772
[08/27/2025 14:53:14 INFO]: New best epoch, val score: -0.690580488533912
[08/27/2025 14:53:14 INFO]: Saving model to: unhanged-Shanesha_trial_16/model_best.pth
[08/27/2025 14:53:20 INFO]: Training loss at epoch 35: 1.0002894401550293
[08/27/2025 14:53:24 INFO]: Training loss at epoch 12: 1.0085829198360443
[08/27/2025 14:53:30 INFO]: Training stats: {
    "score": -1.0008145508193087,
    "rmse": 1.0008145508193087
}
[08/27/2025 14:53:30 INFO]: Val stats: {
    "score": -0.6963094011831548,
    "rmse": 0.6963094011831548
}
[08/27/2025 14:53:30 INFO]: Test stats: {
    "score": -0.883478644550152,
    "rmse": 0.883478644550152
}
[08/27/2025 14:53:43 INFO]: Training loss at epoch 25: 0.879846602678299
[08/27/2025 14:53:52 INFO]: Training loss at epoch 36: 1.0807058215141296
[08/27/2025 14:54:07 INFO]: Training loss at epoch 13: 1.224941074848175
[08/27/2025 14:54:08 INFO]: Training loss at epoch 3: 1.9342911839485168
[08/27/2025 14:54:08 INFO]: Training loss at epoch 15: 1.2051687240600586
[08/27/2025 14:54:09 INFO]: Training loss at epoch 8: 0.8961158394813538
[08/27/2025 14:54:14 INFO]: Training loss at epoch 11: 0.9542402625083923
[08/27/2025 14:54:15 INFO]: Training loss at epoch 3: 1.3340701460838318
[08/27/2025 14:54:22 INFO]: Training loss at epoch 20: 0.9735740721225739
[08/27/2025 14:54:22 INFO]: Training loss at epoch 37: 1.0369213819503784
[08/27/2025 14:54:28 INFO]: Training loss at epoch 26: 1.1081109642982483
[08/27/2025 14:54:44 INFO]: Training loss at epoch 13: 1.1497739851474762
[08/27/2025 14:54:50 INFO]: New best epoch, val score: -0.7063778948819455
[08/27/2025 14:54:50 INFO]: Saving model to: unhanged-Shanesha_trial_2/model_best.pth
[08/27/2025 14:54:51 INFO]: Training loss at epoch 38: 0.9419634938240051
[08/27/2025 14:55:09 INFO]: Training loss at epoch 27: 0.9595441222190857
[08/27/2025 14:55:14 INFO]: Training loss at epoch 21: 1.0516145825386047
[08/27/2025 14:55:18 INFO]: Training loss at epoch 16: 1.147192656993866
[08/27/2025 14:55:21 INFO]: Training loss at epoch 39: 1.0422618389129639
[08/27/2025 14:55:29 INFO]: Training loss at epoch 14: 1.1095778942108154
[08/27/2025 14:55:32 INFO]: Training stats: {
    "score": -0.997091043577433,
    "rmse": 0.997091043577433
}
[08/27/2025 14:55:32 INFO]: Val stats: {
    "score": -0.6760302813619792,
    "rmse": 0.6760302813619792
}
[08/27/2025 14:55:32 INFO]: Test stats: {
    "score": -0.8720115664358036,
    "rmse": 0.8720115664358036
}
[08/27/2025 14:55:32 INFO]: Training loss at epoch 3: 1.3522881865501404
[08/27/2025 14:55:32 INFO]: Training loss at epoch 5: 1.0365259647369385
[08/27/2025 14:55:40 INFO]: Training loss at epoch 2: 1.6632120609283447
[08/27/2025 14:55:43 INFO]: Training loss at epoch 3: 1.0374658107757568
[08/27/2025 14:55:47 INFO]: Training loss at epoch 12: 0.9421380162239075
[08/27/2025 14:55:50 INFO]: Training loss at epoch 28: 1.0292847156524658
[08/27/2025 14:55:58 INFO]: Training loss at epoch 2: 1.0532037615776062
[08/27/2025 14:56:00 INFO]: Training loss at epoch 3: 1.0293219089508057
[08/27/2025 14:56:01 INFO]: Training loss at epoch 40: 1.0623244941234589
[08/27/2025 14:56:05 INFO]: Training loss at epoch 14: 0.841367244720459
[08/27/2025 14:56:06 INFO]: Training loss at epoch 22: 1.0116008520126343
[08/27/2025 14:56:20 INFO]: Training loss at epoch 9: 0.8854244947433472
[08/27/2025 14:56:27 INFO]: Training loss at epoch 17: 0.9892989993095398
[08/27/2025 14:56:28 INFO]: Training loss at epoch 4: 1.0831045508384705
[08/27/2025 14:56:32 INFO]: Training loss at epoch 41: 1.0171173214912415
[08/27/2025 14:56:34 INFO]: Training loss at epoch 29: 1.0818040370941162
[08/27/2025 14:56:48 INFO]: Training stats: {
    "score": -0.9956658913981824,
    "rmse": 0.9956658913981824
}
[08/27/2025 14:56:48 INFO]: Val stats: {
    "score": -0.6827701262143137,
    "rmse": 0.6827701262143137
}
[08/27/2025 14:56:48 INFO]: Test stats: {
    "score": -0.8760916354419547,
    "rmse": 0.8760916354419547
}
[08/27/2025 14:56:49 INFO]: Training loss at epoch 15: 1.1554952263832092
[08/27/2025 14:56:50 INFO]: New best epoch, val score: -0.6673616645476539
[08/27/2025 14:56:50 INFO]: Saving model to: unhanged-Shanesha_trial_20/model_best.pth
[08/27/2025 14:57:00 INFO]: Training loss at epoch 23: 1.326787382364273
[08/27/2025 14:57:01 INFO]: Training loss at epoch 42: 1.1126501560211182
[08/27/2025 14:57:03 INFO]: Training stats: {
    "score": -0.9922780933562524,
    "rmse": 0.9922780933562524
}
[08/27/2025 14:57:03 INFO]: Val stats: {
    "score": -0.6775281048530338,
    "rmse": 0.6775281048530338
}
[08/27/2025 14:57:03 INFO]: Test stats: {
    "score": -0.8760256505276448,
    "rmse": 0.8760256505276448
}
[08/27/2025 14:57:04 INFO]: Training loss at epoch 1: 3.8669933080673218
[08/27/2025 14:57:22 INFO]: Training loss at epoch 13: 0.9842249751091003
[08/27/2025 14:57:28 INFO]: Training loss at epoch 15: 0.9652710855007172
[08/27/2025 14:57:30 INFO]: Training loss at epoch 4: 0.9724114835262299
[08/27/2025 14:57:31 INFO]: Training loss at epoch 30: 0.9696140885353088
[08/27/2025 14:57:31 INFO]: Training loss at epoch 43: 0.8263867199420929
[08/27/2025 14:57:37 INFO]: Training loss at epoch 18: 1.0760955214500427
[08/27/2025 14:57:54 INFO]: Training loss at epoch 24: 0.8769469261169434
[08/27/2025 14:58:01 INFO]: Training loss at epoch 44: 0.8254826962947845
[08/27/2025 14:58:04 INFO]: New best epoch, val score: -0.6639577952238787
[08/27/2025 14:58:04 INFO]: Saving model to: unhanged-Shanesha_trial_7/model_best.pth
[08/27/2025 14:58:11 INFO]: Training loss at epoch 16: 0.8855756223201752
[08/27/2025 14:58:13 INFO]: Training loss at epoch 31: 0.9625682830810547
[08/27/2025 14:58:30 INFO]: Training loss at epoch 45: 0.9070855975151062
[08/27/2025 14:58:31 INFO]: Training loss at epoch 3: 1.1069104671478271
[08/27/2025 14:58:46 INFO]: Training loss at epoch 25: 1.0459557473659515
[08/27/2025 14:58:47 INFO]: Training loss at epoch 19: 1.189087688922882
[08/27/2025 14:58:51 INFO]: Training loss at epoch 16: 1.0371378660202026
[08/27/2025 14:58:55 INFO]: Training loss at epoch 32: 0.748264268040657
[08/27/2025 14:58:56 INFO]: Training loss at epoch 14: 0.9285813271999359
[08/27/2025 14:59:00 INFO]: Training loss at epoch 46: 1.1492339670658112
[08/27/2025 14:59:02 INFO]: Training loss at epoch 6: 1.3312031626701355
[08/27/2025 14:59:03 INFO]: Training loss at epoch 4: 1.2246055603027344
[08/27/2025 14:59:11 INFO]: Training loss at epoch 4: 0.9773333072662354
[08/27/2025 14:59:12 INFO]: Training stats: {
    "score": -1.003561365701212,
    "rmse": 1.003561365701212
}
[08/27/2025 14:59:12 INFO]: Val stats: {
    "score": -0.6638742936143709,
    "rmse": 0.6638742936143709
}
[08/27/2025 14:59:12 INFO]: Test stats: {
    "score": -0.8676767788203144,
    "rmse": 0.8676767788203144
}
[08/27/2025 14:59:13 INFO]: Training loss at epoch 10: 1.0789910554885864
[08/27/2025 14:59:15 INFO]: New best epoch, val score: -0.6688054579962376
[08/27/2025 14:59:15 INFO]: Saving model to: unhanged-Shanesha_trial_16/model_best.pth
[08/27/2025 14:59:21 INFO]: New best epoch, val score: -0.6638742936143709
[08/27/2025 14:59:21 INFO]: Saving model to: unhanged-Shanesha_trial_9/model_best.pth
[08/27/2025 14:59:28 INFO]: Training loss at epoch 47: 1.1007388830184937
[08/27/2025 14:59:30 INFO]: Training loss at epoch 17: 1.0642218589782715
[08/27/2025 14:59:35 INFO]: Training loss at epoch 33: 0.9813517034053802
[08/27/2025 14:59:37 INFO]: Training loss at epoch 26: 0.9176561236381531
[08/27/2025 14:59:40 INFO]: Running Final Evaluation...
[08/27/2025 14:59:54 INFO]: Training accuracy: {
    "score": -1.0097856426393357,
    "rmse": 1.0097856426393357
}
[08/27/2025 14:59:54 INFO]: Val accuracy: {
    "score": -0.6663450246001237,
    "rmse": 0.6663450246001237
}
[08/27/2025 14:59:54 INFO]: Test accuracy: {
    "score": -0.8722816974226177,
    "rmse": 0.8722816974226177
}
[08/27/2025 14:59:54 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_12",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8722816974226177,
        "rmse": 0.8722816974226177
    },
    "train_stats": {
        "score": -1.0097856426393357,
        "rmse": 1.0097856426393357
    },
    "val_stats": {
        "score": -0.6663450246001237,
        "rmse": 0.6663450246001237
    }
}
[08/27/2025 14:59:54 INFO]: Procewss finished for trial unhanged-Shanesha_trial_12
[08/27/2025 14:59:54 INFO]: 
_________________________________________________

[08/27/2025 14:59:54 INFO]: train_net_for_optune.py main() running.
[08/27/2025 14:59:54 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 1.3329318563581283
  attention_dropout: 0.04372221691049877
  ffn_dropout: 0.04372221691049877
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00017310485300543084
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_21

[08/27/2025 14:59:54 INFO]: This ft_transformer has 9.990 million parameters.
[08/27/2025 14:59:54 INFO]: Training will start at epoch 0.
[08/27/2025 14:59:54 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 14:59:57 INFO]: Training loss at epoch 48: 1.1533869802951813
[08/27/2025 15:00:01 INFO]: Running Final Evaluation...
[08/27/2025 15:00:12 INFO]: Training accuracy: {
    "score": -1.011018239448587,
    "rmse": 1.011018239448587
}
[08/27/2025 15:00:12 INFO]: Val accuracy: {
    "score": -0.6612331214211792,
    "rmse": 0.6612331214211792
}
[08/27/2025 15:00:12 INFO]: Test accuracy: {
    "score": -0.8706552615052038,
    "rmse": 0.8706552615052038
}
[08/27/2025 15:00:12 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_17",
    "best_epoch": 17,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8706552615052038,
        "rmse": 0.8706552615052038
    },
    "train_stats": {
        "score": -1.011018239448587,
        "rmse": 1.011018239448587
    },
    "val_stats": {
        "score": -0.6612331214211792,
        "rmse": 0.6612331214211792
    }
}
[08/27/2025 15:00:12 INFO]: Procewss finished for trial unhanged-Shanesha_trial_17
[08/27/2025 15:00:12 INFO]: 
_________________________________________________

[08/27/2025 15:00:12 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:00:12 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 16
  d_ffn_factor: 1.7312742672711807
  attention_dropout: 0.09463402709836388
  ffn_dropout: 0.09463402709836388
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.141400528860798e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_22

[08/27/2025 15:00:12 INFO]: This ft_transformer has 19.783 million parameters.
[08/27/2025 15:00:12 INFO]: Training will start at epoch 0.
[08/27/2025 15:00:12 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:00:14 INFO]: Training loss at epoch 17: 0.8467911779880524
[08/27/2025 15:00:20 INFO]: Training loss at epoch 20: 1.1965301930904388
[08/27/2025 15:00:26 INFO]: Training loss at epoch 15: 0.9152053594589233
[08/27/2025 15:00:29 INFO]: New best epoch, val score: -0.6613311120766199
[08/27/2025 15:00:29 INFO]: Saving model to: unhanged-Shanesha_trial_9/model_best.pth
[08/27/2025 15:00:30 INFO]: Training loss at epoch 27: 1.122986763715744
[08/27/2025 15:00:46 INFO]: Training loss at epoch 4: 1.2144009470939636
[08/27/2025 15:00:47 INFO]: Training loss at epoch 5: 1.0286850929260254
[08/27/2025 15:00:48 INFO]: Training loss at epoch 18: 1.0848044157028198
[08/27/2025 15:01:02 INFO]: Training loss at epoch 4: 1.1044009923934937
[08/27/2025 15:01:21 INFO]: Training loss at epoch 11: 0.9894082844257355
[08/27/2025 15:01:25 INFO]: Training loss at epoch 28: 0.9900743961334229
[08/27/2025 15:01:25 INFO]: Training loss at epoch 4: 1.009565830230713
[08/27/2025 15:01:30 INFO]: Training loss at epoch 21: 1.180312603712082
[08/27/2025 15:01:37 INFO]: Training loss at epoch 18: 1.0017836689949036
[08/27/2025 15:01:39 INFO]: New best epoch, val score: -0.661113826458219
[08/27/2025 15:01:39 INFO]: Saving model to: unhanged-Shanesha_trial_9/model_best.pth
[08/27/2025 15:01:41 INFO]: New best epoch, val score: -0.6616960226309365
[08/27/2025 15:01:41 INFO]: Saving model to: unhanged-Shanesha_trial_5/model_best.pth
[08/27/2025 15:02:00 INFO]: Training loss at epoch 16: 0.8871662020683289
[08/27/2025 15:02:06 INFO]: Training loss at epoch 5: 0.8179951012134552
[08/27/2025 15:02:12 INFO]: Training loss at epoch 19: 0.8291537761688232
[08/27/2025 15:02:19 INFO]: Training loss at epoch 29: 1.1706591844558716
[08/27/2025 15:02:29 INFO]: Training loss at epoch 7: 0.9467264711856842
[08/27/2025 15:02:37 INFO]: New best epoch, val score: -0.66384917277531
[08/27/2025 15:02:37 INFO]: Saving model to: unhanged-Shanesha_trial_7/model_best.pth
[08/27/2025 15:02:39 INFO]: Training stats: {
    "score": -1.0156527026320266,
    "rmse": 1.0156527026320266
}
[08/27/2025 15:02:39 INFO]: Val stats: {
    "score": -0.7465088846718693,
    "rmse": 0.7465088846718693
}
[08/27/2025 15:02:39 INFO]: Test stats: {
    "score": -0.9018868978093748,
    "rmse": 0.9018868978093748
}
[08/27/2025 15:02:39 INFO]: Training stats: {
    "score": -0.9995705722690743,
    "rmse": 0.9995705722690743
}
[08/27/2025 15:02:39 INFO]: Val stats: {
    "score": -0.6698706704177404,
    "rmse": 0.6698706704177404
}
[08/27/2025 15:02:39 INFO]: Test stats: {
    "score": -0.8709857314102346,
    "rmse": 0.8709857314102346
}
[08/27/2025 15:02:40 INFO]: Training loss at epoch 22: 1.0053771436214447
[08/27/2025 15:02:50 INFO]: Training loss at epoch 3: 1.6759811043739319
[08/27/2025 15:02:58 INFO]: Training loss at epoch 19: 1.1902420818805695
[08/27/2025 15:03:11 INFO]: Training loss at epoch 3: 0.95777428150177
[08/27/2025 15:03:27 INFO]: Training stats: {
    "score": -1.001125085078029,
    "rmse": 1.001125085078029
}
[08/27/2025 15:03:27 INFO]: Val stats: {
    "score": -0.6677148564235369,
    "rmse": 0.6677148564235369
}
[08/27/2025 15:03:27 INFO]: Test stats: {
    "score": -0.8713748671748586,
    "rmse": 0.8713748671748586
}
[08/27/2025 15:03:28 INFO]: Training loss at epoch 12: 1.1688231229782104
[08/27/2025 15:03:31 INFO]: Training loss at epoch 30: 1.047124058008194
[08/27/2025 15:03:32 INFO]: Training loss at epoch 17: 0.9850800931453705
[08/27/2025 15:03:47 INFO]: Training loss at epoch 23: 0.7726535201072693
[08/27/2025 15:03:53 INFO]: Training loss at epoch 5: 1.3572619557380676
[08/27/2025 15:03:57 INFO]: Training loss at epoch 20: 1.0244902670383453
[08/27/2025 15:04:01 INFO]: Training loss at epoch 5: 1.410617172718048
[08/27/2025 15:04:23 INFO]: Training loss at epoch 31: 1.0355406403541565
[08/27/2025 15:04:33 INFO]: Training loss at epoch 4: 1.0020164251327515
[08/27/2025 15:04:47 INFO]: Training loss at epoch 20: 1.0360147655010223
[08/27/2025 15:04:52 INFO]: Training loss at epoch 0: 1.548628330230713
[08/27/2025 15:04:55 INFO]: Training loss at epoch 24: 0.87760129570961
[08/27/2025 15:05:02 INFO]: Training loss at epoch 18: 1.2757569551467896
[08/27/2025 15:05:06 INFO]: Training loss at epoch 6: 0.9123697578907013
[08/27/2025 15:05:16 INFO]: Training loss at epoch 21: 0.8726294934749603
[08/27/2025 15:05:17 INFO]: Training loss at epoch 32: 1.1586199700832367
[08/27/2025 15:05:32 INFO]: New best epoch, val score: -0.7714330622745845
[08/27/2025 15:05:32 INFO]: Saving model to: unhanged-Shanesha_trial_21/model_best.pth
[08/27/2025 15:05:36 INFO]: Training loss at epoch 13: 0.9904307723045349
[08/27/2025 15:05:54 INFO]: Training loss at epoch 8: 0.9927189648151398
[08/27/2025 15:06:02 INFO]: Training loss at epoch 5: 0.9663494825363159
[08/27/2025 15:06:05 INFO]: Training loss at epoch 25: 0.9441113173961639
[08/27/2025 15:06:09 INFO]: Training loss at epoch 21: 1.0937042832374573
[08/27/2025 15:06:11 INFO]: Training loss at epoch 33: 0.9744968116283417
[08/27/2025 15:06:18 INFO]: Running Final Evaluation...
[08/27/2025 15:06:20 INFO]: Training loss at epoch 5: 1.1376599669456482
[08/27/2025 15:06:35 INFO]: Training loss at epoch 19: 0.9269481003284454
[08/27/2025 15:06:37 INFO]: Training loss at epoch 22: 1.0424054265022278
[08/27/2025 15:06:37 INFO]: Training loss at epoch 6: 1.1050074696540833
[08/27/2025 15:06:37 INFO]: Training accuracy: {
    "score": -1.0084005864447272,
    "rmse": 1.0084005864447272
}
[08/27/2025 15:06:37 INFO]: Val accuracy: {
    "score": -0.6583399696619493,
    "rmse": 0.6583399696619493
}
[08/27/2025 15:06:37 INFO]: Test accuracy: {
    "score": -0.870554116911559,
    "rmse": 0.870554116911559
}
[08/27/2025 15:06:37 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_1",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.870554116911559,
        "rmse": 0.870554116911559
    },
    "train_stats": {
        "score": -1.0084005864447272,
        "rmse": 1.0084005864447272
    },
    "val_stats": {
        "score": -0.6583399696619493,
        "rmse": 0.6583399696619493
    }
}
[08/27/2025 15:06:37 INFO]: Procewss finished for trial unhanged-Shanesha_trial_1
[08/27/2025 15:06:37 INFO]: 
_________________________________________________

[08/27/2025 15:06:37 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:06:37 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 2.6591071373042316
  attention_dropout: 0.06290730762840546
  ffn_dropout: 0.06290730762840546
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00013785730584374632
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_23

[08/27/2025 15:06:37 INFO]: This ft_transformer has 2.538 million parameters.
[08/27/2025 15:06:37 INFO]: Training will start at epoch 0.
[08/27/2025 15:06:37 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:06:42 INFO]: Training loss at epoch 5: 1.039743721485138
[08/27/2025 15:07:09 INFO]: Training stats: {
    "score": -1.003351360656845,
    "rmse": 1.003351360656845
}
[08/27/2025 15:07:09 INFO]: Val stats: {
    "score": -0.7114398483311166,
    "rmse": 0.7114398483311166
}
[08/27/2025 15:07:09 INFO]: Test stats: {
    "score": -0.8873753865039341,
    "rmse": 0.8873753865039341
}
[08/27/2025 15:07:10 INFO]: New best epoch, val score: -0.6634080324093162
[08/27/2025 15:07:10 INFO]: Saving model to: unhanged-Shanesha_trial_7/model_best.pth
[08/27/2025 15:07:13 INFO]: Training loss at epoch 26: 1.0621137619018555
[08/27/2025 15:07:28 INFO]: Training loss at epoch 22: 1.106206238269806
[08/27/2025 15:07:38 INFO]: New best epoch, val score: -0.6624763911854852
[08/27/2025 15:07:38 INFO]: Saving model to: unhanged-Shanesha_trial_6/model_best.pth
[08/27/2025 15:07:42 INFO]: Training loss at epoch 14: 0.9610065817832947
[08/27/2025 15:07:46 INFO]: Training loss at epoch 0: 0.9405851662158966
[08/27/2025 15:07:57 INFO]: Training loss at epoch 23: 0.8376500606536865
[08/27/2025 15:07:57 INFO]: New best epoch, val score: -0.6592059241466465
[08/27/2025 15:07:57 INFO]: Saving model to: unhanged-Shanesha_trial_23/model_best.pth
[08/27/2025 15:08:21 INFO]: Training loss at epoch 27: 0.9483545422554016
[08/27/2025 15:08:41 INFO]: Training loss at epoch 20: 0.9302466809749603
[08/27/2025 15:08:42 INFO]: Training loss at epoch 2: 1.2276748716831207
[08/27/2025 15:08:45 INFO]: Training loss at epoch 6: 1.2329621315002441
[08/27/2025 15:08:50 INFO]: Training loss at epoch 23: 0.9386817812919617
[08/27/2025 15:08:54 INFO]: Training loss at epoch 6: 1.5968167185783386
[08/27/2025 15:08:59 INFO]: New best epoch, val score: -0.6609070503707603
[08/27/2025 15:08:59 INFO]: Saving model to: unhanged-Shanesha_trial_6/model_best.pth
[08/27/2025 15:09:08 INFO]: Training loss at epoch 1: 1.3632787466049194
[08/27/2025 15:09:16 INFO]: Training loss at epoch 24: 1.1082477569580078
[08/27/2025 15:09:17 INFO]: Training loss at epoch 9: 0.9558093845844269
[08/27/2025 15:09:27 INFO]: Training loss at epoch 7: 0.7961448431015015
[08/27/2025 15:09:30 INFO]: Training loss at epoch 28: 0.8145840764045715
[08/27/2025 15:09:51 INFO]: Training loss at epoch 15: 1.2702344357967377
[08/27/2025 15:09:55 INFO]: Training loss at epoch 4: 1.4976832866668701
[08/27/2025 15:10:10 INFO]: Training loss at epoch 24: 0.9882976710796356
[08/27/2025 15:10:14 INFO]: Training loss at epoch 21: 0.9732871353626251
[08/27/2025 15:10:16 INFO]: Training loss at epoch 0: 1.5120403170585632
[08/27/2025 15:10:23 INFO]: Training loss at epoch 4: 1.0973173379898071
[08/27/2025 15:10:29 INFO]: Training loss at epoch 2: 1.2852388620376587
[08/27/2025 15:10:31 INFO]: Training loss at epoch 1: 4.202776372432709
[08/27/2025 15:10:31 INFO]: Training stats: {
    "score": -1.0310554382638215,
    "rmse": 1.0310554382638215
}
[08/27/2025 15:10:31 INFO]: Val stats: {
    "score": -0.7849480635642733,
    "rmse": 0.7849480635642733
}
[08/27/2025 15:10:31 INFO]: Test stats: {
    "score": -0.93815092304667,
    "rmse": 0.93815092304667
}
[08/27/2025 15:10:34 INFO]: Training loss at epoch 5: 1.0606811940670013
[08/27/2025 15:10:37 INFO]: Training loss at epoch 25: 1.1064073145389557
[08/27/2025 15:10:39 INFO]: Training loss at epoch 29: 0.9961910843849182
[08/27/2025 15:10:45 INFO]: New best epoch, val score: -0.6623907648372059
[08/27/2025 15:10:45 INFO]: Saving model to: unhanged-Shanesha_trial_18/model_best.pth
[08/27/2025 15:11:04 INFO]: Training stats: {
    "score": -0.998184305775962,
    "rmse": 0.998184305775962
}
[08/27/2025 15:11:04 INFO]: Val stats: {
    "score": -0.6790115983396678,
    "rmse": 0.6790115983396678
}
[08/27/2025 15:11:04 INFO]: Test stats: {
    "score": -0.8725181161412272,
    "rmse": 0.8725181161412272
}
[08/27/2025 15:11:08 INFO]: Training loss at epoch 7: 0.9998131990432739
[08/27/2025 15:11:11 INFO]: Training loss at epoch 6: 1.0428542494773865
[08/27/2025 15:11:28 INFO]: Training loss at epoch 25: 1.0209431052207947
[08/27/2025 15:11:33 INFO]: Training loss at epoch 6: 1.13747900724411
[08/27/2025 15:11:39 INFO]: New best epoch, val score: -0.7983175016213407
[08/27/2025 15:11:39 INFO]: Saving model to: unhanged-Shanesha_trial_22/model_best.pth
[08/27/2025 15:11:42 INFO]: New best epoch, val score: -0.6628765028844813
[08/27/2025 15:11:42 INFO]: Saving model to: unhanged-Shanesha_trial_7/model_best.pth
[08/27/2025 15:11:46 INFO]: Training loss at epoch 22: 0.8223016262054443
[08/27/2025 15:11:48 INFO]: Training loss at epoch 3: 1.2839362621307373
[08/27/2025 15:11:55 INFO]: Training loss at epoch 26: 0.9893936216831207
[08/27/2025 15:11:58 INFO]: Training loss at epoch 16: 1.170407623052597
[08/27/2025 15:11:59 INFO]: Training loss at epoch 6: 0.9807330965995789
[08/27/2025 15:12:14 INFO]: New best epoch, val score: -0.6621485125825881
[08/27/2025 15:12:14 INFO]: Saving model to: unhanged-Shanesha_trial_15/model_best.pth
[08/27/2025 15:12:14 INFO]: Training loss at epoch 30: 1.157185822725296
[08/27/2025 15:12:52 INFO]: Training loss at epoch 26: 0.9549100697040558
[08/27/2025 15:13:10 INFO]: Training loss at epoch 4: 1.0016628801822662
[08/27/2025 15:13:16 INFO]: Training loss at epoch 27: 0.8453166484832764
[08/27/2025 15:13:19 INFO]: Training loss at epoch 23: 1.0456320345401764
[08/27/2025 15:13:23 INFO]: Training loss at epoch 31: 0.9575327336788177
[08/27/2025 15:13:34 INFO]: Training loss at epoch 7: 1.116605430841446
[08/27/2025 15:13:44 INFO]: Training loss at epoch 7: 1.1965932250022888
[08/27/2025 15:13:44 INFO]: Training loss at epoch 8: 0.8937180638313293
[08/27/2025 15:13:56 INFO]: Training loss at epoch 10: 1.0858458280563354
[08/27/2025 15:14:07 INFO]: Training loss at epoch 17: 0.9358028173446655
[08/27/2025 15:14:12 INFO]: Training loss at epoch 27: 0.9442124962806702
[08/27/2025 15:14:31 INFO]: Training loss at epoch 5: 1.0265007019042969
[08/27/2025 15:14:32 INFO]: Training loss at epoch 32: 0.9534311592578888
[08/27/2025 15:14:36 INFO]: Training loss at epoch 28: 1.2206818461418152
[08/27/2025 15:14:52 INFO]: Training loss at epoch 24: 0.9051398634910583
[08/27/2025 15:15:31 INFO]: Training loss at epoch 28: 1.081961989402771
[08/27/2025 15:15:38 INFO]: Training loss at epoch 33: 0.8459874987602234
[08/27/2025 15:15:41 INFO]: Training loss at epoch 8: 1.0049469769001007
[08/27/2025 15:15:47 INFO]: Training loss at epoch 6: 1.1205933690071106
[08/27/2025 15:15:52 INFO]: Training loss at epoch 29: 0.8897270262241364
[08/27/2025 15:16:04 INFO]: Training loss at epoch 2: 1.979980707168579
[08/27/2025 15:16:12 INFO]: Training loss at epoch 18: 1.0262235403060913
[08/27/2025 15:16:13 INFO]: New best epoch, val score: -0.6628169354264378
[08/27/2025 15:16:13 INFO]: Saving model to: unhanged-Shanesha_trial_7/model_best.pth
[08/27/2025 15:16:20 INFO]: Training stats: {
    "score": -1.0072476874310914,
    "rmse": 1.0072476874310914
}
[08/27/2025 15:16:20 INFO]: Val stats: {
    "score": -0.7283290158656753,
    "rmse": 0.7283290158656753
}
[08/27/2025 15:16:20 INFO]: Test stats: {
    "score": -0.8916120466966451,
    "rmse": 0.8916120466966451
}
[08/27/2025 15:16:23 INFO]: Training loss at epoch 25: 1.1505511105060577
[08/27/2025 15:16:24 INFO]: Training loss at epoch 7: 1.0173803567886353
[08/27/2025 15:16:33 INFO]: Training loss at epoch 6: 1.0070781409740448
[08/27/2025 15:16:49 INFO]: Training loss at epoch 34: 0.8879076242446899
[08/27/2025 15:16:52 INFO]: Training loss at epoch 7: 1.1145504713058472
[08/27/2025 15:16:52 INFO]: Training loss at epoch 29: 0.9956439733505249
[08/27/2025 15:17:00 INFO]: Training loss at epoch 5: 1.1272848844528198
[08/27/2025 15:17:10 INFO]: Training loss at epoch 7: 0.9872072339057922
[08/27/2025 15:17:19 INFO]: Training loss at epoch 7: 1.0707181692123413
[08/27/2025 15:17:22 INFO]: Training stats: {
    "score": -0.9993467112162758,
    "rmse": 0.9993467112162758
}
[08/27/2025 15:17:22 INFO]: Val stats: {
    "score": -0.6847348922236174,
    "rmse": 0.6847348922236174
}
[08/27/2025 15:17:22 INFO]: Test stats: {
    "score": -0.8785706497893556,
    "rmse": 0.8785706497893556
}
[08/27/2025 15:17:24 INFO]: Training loss at epoch 11: 1.0416311025619507
[08/27/2025 15:17:33 INFO]: Training loss at epoch 5: 0.8266161680221558
[08/27/2025 15:17:42 INFO]: Training loss at epoch 30: 1.282447338104248
[08/27/2025 15:17:57 INFO]: Training loss at epoch 26: 0.8558450043201447
[08/27/2025 15:17:59 INFO]: Training loss at epoch 35: 0.9121333956718445
[08/27/2025 15:18:05 INFO]: Training loss at epoch 9: 0.8314440548419952
[08/27/2025 15:18:26 INFO]: Training loss at epoch 19: 1.0751248002052307
[08/27/2025 15:18:29 INFO]: Training loss at epoch 8: 0.9889582693576813
[08/27/2025 15:18:33 INFO]: Training loss at epoch 8: 1.0080808401107788
[08/27/2025 15:18:40 INFO]: Training loss at epoch 8: 1.2572303414344788
[08/27/2025 15:18:43 INFO]: Training loss at epoch 30: 1.2830162048339844
[08/27/2025 15:19:05 INFO]: Training loss at epoch 31: 0.9234580993652344
[08/27/2025 15:19:10 INFO]: Training stats: {
    "score": -0.9947382629276725,
    "rmse": 0.9947382629276725
}
[08/27/2025 15:19:10 INFO]: Val stats: {
    "score": -0.6636420013321677,
    "rmse": 0.6636420013321677
}
[08/27/2025 15:19:10 INFO]: Test stats: {
    "score": -0.8707053622594437,
    "rmse": 0.8707053622594437
}
[08/27/2025 15:19:11 INFO]: Training loss at epoch 36: 0.7791125178337097
[08/27/2025 15:19:15 INFO]: Running Final Evaluation...
[08/27/2025 15:19:31 INFO]: Training loss at epoch 27: 0.8867382407188416
[08/27/2025 15:19:32 INFO]: Training stats: {
    "score": -0.9968995688583622,
    "rmse": 0.9968995688583622
}
[08/27/2025 15:19:32 INFO]: Val stats: {
    "score": -0.7206628249835833,
    "rmse": 0.7206628249835833
}
[08/27/2025 15:19:32 INFO]: Test stats: {
    "score": -0.9177920805959738,
    "rmse": 0.9177920805959738
}
[08/27/2025 15:19:42 INFO]: Training accuracy: {
    "score": -1.016760623815682,
    "rmse": 1.016760623815682
}
[08/27/2025 15:19:42 INFO]: Val accuracy: {
    "score": -0.6652745817488454,
    "rmse": 0.6652745817488454
}
[08/27/2025 15:19:42 INFO]: Test accuracy: {
    "score": -0.8675815870819233,
    "rmse": 0.8675815870819233
}
[08/27/2025 15:19:42 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_4",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8675815870819233,
        "rmse": 0.8675815870819233
    },
    "train_stats": {
        "score": -1.016760623815682,
        "rmse": 1.016760623815682
    },
    "val_stats": {
        "score": -0.6652745817488454,
        "rmse": 0.6652745817488454
    }
}
[08/27/2025 15:19:42 INFO]: Procewss finished for trial unhanged-Shanesha_trial_4
[08/27/2025 15:19:42 INFO]: 
_________________________________________________

[08/27/2025 15:19:42 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:19:42 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 1.292093535199959
  attention_dropout: 0.29646475961519425
  ffn_dropout: 0.29646475961519425
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.860468306912919e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_24

[08/27/2025 15:19:42 INFO]: This ft_transformer has 0.131 million parameters.
[08/27/2025 15:19:42 INFO]: Training will start at epoch 0.
[08/27/2025 15:19:42 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:19:54 INFO]: Training loss at epoch 9: 0.9272901117801666
[08/27/2025 15:20:04 INFO]: Training loss at epoch 31: 0.922796368598938
[08/27/2025 15:20:07 INFO]: Training loss at epoch 0: 1.1216926574707031
[08/27/2025 15:20:11 INFO]: New best epoch, val score: -0.7359392334464235
[08/27/2025 15:20:11 INFO]: Saving model to: unhanged-Shanesha_trial_24/model_best.pth
[08/27/2025 15:20:19 INFO]: Training loss at epoch 3: 1.0446412563323975
[08/27/2025 15:20:19 INFO]: Training stats: {
    "score": -1.0300297760646242,
    "rmse": 1.0300297760646242
}
[08/27/2025 15:20:19 INFO]: Val stats: {
    "score": -0.780785895114767,
    "rmse": 0.780785895114767
}
[08/27/2025 15:20:19 INFO]: Test stats: {
    "score": -0.9377315012807136,
    "rmse": 0.9377315012807136
}
[08/27/2025 15:20:19 INFO]: Training loss at epoch 37: 0.8735979199409485
[08/27/2025 15:20:20 INFO]: Training loss at epoch 9: 0.9882540702819824
[08/27/2025 15:20:36 INFO]: Training loss at epoch 1: 0.9781675040721893
[08/27/2025 15:20:41 INFO]: New best epoch, val score: -0.7199212826737995
[08/27/2025 15:20:41 INFO]: Saving model to: unhanged-Shanesha_trial_24/model_best.pth
[08/27/2025 15:20:54 INFO]: Training loss at epoch 12: 1.1871182322502136
[08/27/2025 15:21:04 INFO]: Training loss at epoch 28: 0.8168560564517975
[08/27/2025 15:21:08 INFO]: Training loss at epoch 2: 0.9960694909095764
[08/27/2025 15:21:12 INFO]: New best epoch, val score: -0.7065146494100457
[08/27/2025 15:21:12 INFO]: Saving model to: unhanged-Shanesha_trial_24/model_best.pth
[08/27/2025 15:21:17 INFO]: Training loss at epoch 20: 0.8152860403060913
[08/27/2025 15:21:25 INFO]: Training loss at epoch 32: 1.0948445796966553
[08/27/2025 15:21:29 INFO]: Training loss at epoch 38: 1.223651647567749
[08/27/2025 15:21:38 INFO]: Training loss at epoch 3: 0.8874570727348328
[08/27/2025 15:21:42 INFO]: Training loss at epoch 10: 1.0139229595661163
[08/27/2025 15:21:42 INFO]: New best epoch, val score: -0.6962873109584337
[08/27/2025 15:21:42 INFO]: Saving model to: unhanged-Shanesha_trial_24/model_best.pth
[08/27/2025 15:21:44 INFO]: Training loss at epoch 8: 1.0710156559944153
[08/27/2025 15:21:48 INFO]: Training loss at epoch 1: 1.1308092176914215
[08/27/2025 15:21:50 INFO]: Training loss at epoch 3: 1.6976117491722107
[08/27/2025 15:21:55 INFO]: Training stats: {
    "score": -1.0080450391267333,
    "rmse": 1.0080450391267333
}
[08/27/2025 15:21:55 INFO]: Val stats: {
    "score": -0.6633356510189227,
    "rmse": 0.6633356510189227
}
[08/27/2025 15:21:55 INFO]: Test stats: {
    "score": -0.8712563017597309,
    "rmse": 0.8712563017597309
}
[08/27/2025 15:22:07 INFO]: Training loss at epoch 4: 1.052454560995102
[08/27/2025 15:22:10 INFO]: New best epoch, val score: -0.6862850437284258
[08/27/2025 15:22:10 INFO]: Saving model to: unhanged-Shanesha_trial_24/model_best.pth
[08/27/2025 15:22:12 INFO]: Training loss at epoch 8: 1.0678280591964722
[08/27/2025 15:22:27 INFO]: New best epoch, val score: -0.7319069083939729
[08/27/2025 15:22:27 INFO]: Saving model to: unhanged-Shanesha_trial_21/model_best.pth
[08/27/2025 15:22:35 INFO]: Training loss at epoch 29: 1.0207983255386353
[08/27/2025 15:22:35 INFO]: Training loss at epoch 5: 1.096720278263092
[08/27/2025 15:22:37 INFO]: Training loss at epoch 7: 0.8492892384529114
[08/27/2025 15:22:37 INFO]: Training loss at epoch 8: 1.1090829372406006
[08/27/2025 15:22:38 INFO]: Training loss at epoch 39: 0.8685717284679413
[08/27/2025 15:22:39 INFO]: New best epoch, val score: -0.6853620537975146
[08/27/2025 15:22:39 INFO]: Saving model to: unhanged-Shanesha_trial_24/model_best.pth
[08/27/2025 15:22:44 INFO]: Training loss at epoch 33: 0.9467401206493378
[08/27/2025 15:22:58 INFO]: Training loss at epoch 11: 0.7805491387844086
[08/27/2025 15:23:02 INFO]: Training stats: {
    "score": -1.0027541771444528,
    "rmse": 1.0027541771444528
}
[08/27/2025 15:23:02 INFO]: Val stats: {
    "score": -0.7129486444436408,
    "rmse": 0.7129486444436408
}
[08/27/2025 15:23:02 INFO]: Test stats: {
    "score": -0.8911722796963929,
    "rmse": 0.8911722796963929
}
[08/27/2025 15:23:04 INFO]: Training loss at epoch 6: 0.9497382044792175
[08/27/2025 15:23:07 INFO]: Training stats: {
    "score": -0.9974359355137434,
    "rmse": 0.9974359355137434
}
[08/27/2025 15:23:07 INFO]: Val stats: {
    "score": -0.6771917125787827,
    "rmse": 0.6771917125787827
}
[08/27/2025 15:23:07 INFO]: Test stats: {
    "score": -0.8697562860202758,
    "rmse": 0.8697562860202758
}
[08/27/2025 15:23:07 INFO]: New best epoch, val score: -0.6845607578003013
[08/27/2025 15:23:07 INFO]: Saving model to: unhanged-Shanesha_trial_24/model_best.pth
[08/27/2025 15:23:19 INFO]: Training loss at epoch 9: 1.2721397280693054
[08/27/2025 15:23:21 INFO]: Training loss at epoch 21: 0.984119176864624
[08/27/2025 15:23:28 INFO]: Training loss at epoch 9: 1.2309370636940002
[08/27/2025 15:23:32 INFO]: Training loss at epoch 7: 1.0444749593734741
[08/27/2025 15:23:37 INFO]: New best epoch, val score: -0.683682381939365
[08/27/2025 15:23:37 INFO]: Saving model to: unhanged-Shanesha_trial_24/model_best.pth
[08/27/2025 15:23:51 INFO]: Training loss at epoch 10: 1.1268692016601562
[08/27/2025 15:24:04 INFO]: Training loss at epoch 8: 1.0392035841941833
[08/27/2025 15:24:04 INFO]: Training loss at epoch 34: 1.0741060078144073
[08/27/2025 15:24:08 INFO]: New best epoch, val score: -0.6828349661191679
[08/27/2025 15:24:08 INFO]: Saving model to: unhanged-Shanesha_trial_24/model_best.pth
[08/27/2025 15:24:09 INFO]: Training loss at epoch 6: 0.9536916613578796
[08/27/2025 15:24:12 INFO]: Training loss at epoch 40: 0.8918201923370361
[08/27/2025 15:24:18 INFO]: Training loss at epoch 13: 1.0254111886024475
[08/27/2025 15:24:19 INFO]: Training loss at epoch 12: 0.9892334043979645
[08/27/2025 15:24:34 INFO]: Training loss at epoch 9: 1.0028848946094513
[08/27/2025 15:24:41 INFO]: Training loss at epoch 30: 1.0570293962955475
[08/27/2025 15:24:45 INFO]: Training loss at epoch 6: 1.2186427116394043
[08/27/2025 15:24:46 INFO]: Training stats: {
    "score": -1.000880365634148,
    "rmse": 1.000880365634148
}
[08/27/2025 15:24:46 INFO]: Val stats: {
    "score": -0.6822734202671356,
    "rmse": 0.6822734202671356
}
[08/27/2025 15:24:46 INFO]: Test stats: {
    "score": -0.8683659343568688,
    "rmse": 0.8683659343568688
}
[08/27/2025 15:24:50 INFO]: New best epoch, val score: -0.6822734202671356
[08/27/2025 15:24:50 INFO]: Saving model to: unhanged-Shanesha_trial_24/model_best.pth
[08/27/2025 15:25:01 INFO]: Training stats: {
    "score": -1.095148195990178,
    "rmse": 1.095148195990178
}
[08/27/2025 15:25:01 INFO]: Val stats: {
    "score": -0.9000219633238441,
    "rmse": 0.9000219633238441
}
[08/27/2025 15:25:01 INFO]: Test stats: {
    "score": -1.025621970323732,
    "rmse": 1.025621970323732
}
[08/27/2025 15:25:13 INFO]: Training stats: {
    "score": -1.078970199842508,
    "rmse": 1.078970199842508
}
[08/27/2025 15:25:13 INFO]: Val stats: {
    "score": -0.702815596152873,
    "rmse": 0.702815596152873
}
[08/27/2025 15:25:13 INFO]: Test stats: {
    "score": -0.921004332938059,
    "rmse": 0.921004332938059
}
[08/27/2025 15:25:16 INFO]: Training loss at epoch 10: 1.0524352490901947
[08/27/2025 15:25:22 INFO]: Training loss at epoch 41: 0.8935926854610443
[08/27/2025 15:25:27 INFO]: Training loss at epoch 35: 0.994634747505188
[08/27/2025 15:25:34 INFO]: Training loss at epoch 22: 0.9040338695049286
[08/27/2025 15:25:36 INFO]: New best epoch, val score: -0.6668026213451733
[08/27/2025 15:25:36 INFO]: Saving model to: unhanged-Shanesha_trial_20/model_best.pth
[08/27/2025 15:25:41 INFO]: Training loss at epoch 13: 0.8290939480066299
[08/27/2025 15:25:45 INFO]: Training loss at epoch 11: 0.9122593402862549
[08/27/2025 15:25:47 INFO]: New best epoch, val score: -0.702815596152873
[08/27/2025 15:25:47 INFO]: Saving model to: unhanged-Shanesha_trial_2/model_best.pth
[08/27/2025 15:26:14 INFO]: Training loss at epoch 31: 0.9873558878898621
[08/27/2025 15:26:14 INFO]: Training loss at epoch 12: 1.1378870606422424
[08/27/2025 15:26:27 INFO]: Training loss at epoch 10: 1.0964504480361938
[08/27/2025 15:26:31 INFO]: Training loss at epoch 42: 0.914432942867279
[08/27/2025 15:26:43 INFO]: Training loss at epoch 13: 0.9952724874019623
[08/27/2025 15:26:47 INFO]: Training loss at epoch 36: 0.94777911901474
[08/27/2025 15:26:55 INFO]: Training loss at epoch 9: 0.8865794241428375
[08/27/2025 15:26:56 INFO]: New best epoch, val score: -0.6596467339786175
[08/27/2025 15:26:56 INFO]: Saving model to: unhanged-Shanesha_trial_6/model_best.pth
[08/27/2025 15:27:00 INFO]: Training loss at epoch 14: 1.2595584988594055
[08/27/2025 15:27:11 INFO]: Training loss at epoch 14: 1.103489637374878
[08/27/2025 15:27:25 INFO]: Training loss at epoch 4: 1.097612977027893
[08/27/2025 15:27:26 INFO]: Training loss at epoch 9: 0.826185405254364
[08/27/2025 15:27:38 INFO]: Training loss at epoch 43: 1.3100447058677673
[08/27/2025 15:27:41 INFO]: Training loss at epoch 23: 0.9120409190654755
[08/27/2025 15:27:42 INFO]: Training loss at epoch 15: 0.9149605333805084
[08/27/2025 15:27:44 INFO]: Training loss at epoch 14: 0.8643315136432648
[08/27/2025 15:27:48 INFO]: Training loss at epoch 32: 0.8048142194747925
[08/27/2025 15:27:55 INFO]: Training loss at epoch 9: 0.9297417998313904
[08/27/2025 15:28:06 INFO]: Training loss at epoch 37: 1.1387091279029846
[08/27/2025 15:28:10 INFO]: Training loss at epoch 16: 0.925456166267395
[08/27/2025 15:28:12 INFO]: Training loss at epoch 11: 0.9615351259708405
[08/27/2025 15:28:19 INFO]: Training loss at epoch 15: 1.1214087009429932
[08/27/2025 15:28:37 INFO]: Training loss at epoch 8: 1.0154582858085632
[08/27/2025 15:28:39 INFO]: Training loss at epoch 17: 1.048242211341858
[08/27/2025 15:28:44 INFO]: Training stats: {
    "score": -1.0265072415720893,
    "rmse": 1.0265072415720893
}
[08/27/2025 15:28:44 INFO]: Val stats: {
    "score": -0.6670334920291106,
    "rmse": 0.6670334920291106
}
[08/27/2025 15:28:44 INFO]: Test stats: {
    "score": -0.8803958405727568,
    "rmse": 0.8803958405727568
}
[08/27/2025 15:28:46 INFO]: Training loss at epoch 44: 0.8597683608531952
[08/27/2025 15:29:08 INFO]: Training loss at epoch 18: 0.8968599438667297
[08/27/2025 15:29:12 INFO]: Training stats: {
    "score": -1.0167307410277562,
    "rmse": 1.0167307410277562
}
[08/27/2025 15:29:12 INFO]: Val stats: {
    "score": -0.6621748130884543,
    "rmse": 0.6621748130884543
}
[08/27/2025 15:29:12 INFO]: Test stats: {
    "score": -0.8640850146961472,
    "rmse": 0.8640850146961472
}
[08/27/2025 15:29:19 INFO]: Training loss at epoch 33: 1.108379602432251
[08/27/2025 15:29:24 INFO]: Training loss at epoch 38: 0.9778857231140137
[08/27/2025 15:29:37 INFO]: Training loss at epoch 19: 1.1186737418174744
[08/27/2025 15:29:38 INFO]: Training loss at epoch 16: 1.1672004461288452
[08/27/2025 15:29:41 INFO]: Training stats: {
    "score": -1.0184201304582807,
    "rmse": 1.0184201304582807
}
[08/27/2025 15:29:41 INFO]: Val stats: {
    "score": -0.7609959335496387,
    "rmse": 0.7609959335496387
}
[08/27/2025 15:29:41 INFO]: Test stats: {
    "score": -0.9187260297878123,
    "rmse": 0.9187260297878123
}
[08/27/2025 15:29:47 INFO]: Training loss at epoch 24: 1.0525903105735779
[08/27/2025 15:29:49 INFO]: Training stats: {
    "score": -1.001302194502913,
    "rmse": 1.001302194502913
}
[08/27/2025 15:29:49 INFO]: Val stats: {
    "score": -0.6931647055023372,
    "rmse": 0.6931647055023372
}
[08/27/2025 15:29:49 INFO]: Test stats: {
    "score": -0.8738727422702101,
    "rmse": 0.8738727422702101
}
[08/27/2025 15:29:53 INFO]: Training loss at epoch 10: 0.9240744411945343
[08/27/2025 15:29:54 INFO]: Training loss at epoch 45: 1.1200075447559357
[08/27/2025 15:30:06 INFO]: Training loss at epoch 10: 0.8941491544246674
[08/27/2025 15:30:17 INFO]: Training loss at epoch 20: 0.8514828681945801
[08/27/2025 15:30:41 INFO]: New best epoch, val score: -0.691927477979903
[08/27/2025 15:30:41 INFO]: Saving model to: unhanged-Shanesha_trial_2/model_best.pth
[08/27/2025 15:30:45 INFO]: Training loss at epoch 39: 1.0196425318717957
[08/27/2025 15:30:48 INFO]: Training loss at epoch 21: 1.0974984467029572
[08/27/2025 15:30:51 INFO]: Training loss at epoch 34: 1.1141794323921204
[08/27/2025 15:30:57 INFO]: Training loss at epoch 11: 0.8516484797000885
[08/27/2025 15:30:58 INFO]: Training loss at epoch 17: 0.9405021667480469
[08/27/2025 15:31:03 INFO]: Training loss at epoch 46: 1.1446869671344757
[08/27/2025 15:31:07 INFO]: Training loss at epoch 15: 1.5169305801391602
[08/27/2025 15:31:12 INFO]: Training loss at epoch 7: 1.131101816892624
[08/27/2025 15:31:13 INFO]: Training stats: {
    "score": -0.999417115392687,
    "rmse": 0.999417115392687
}
[08/27/2025 15:31:13 INFO]: Val stats: {
    "score": -0.6697788097984947,
    "rmse": 0.6697788097984947
}
[08/27/2025 15:31:13 INFO]: Test stats: {
    "score": -0.8720792826228044,
    "rmse": 0.8720792826228044
}
[08/27/2025 15:31:16 INFO]: Training loss at epoch 22: 1.1336976289749146
[08/27/2025 15:31:44 INFO]: Training loss at epoch 23: 1.074676275253296
[08/27/2025 15:31:50 INFO]: Training loss at epoch 7: 1.075082540512085
[08/27/2025 15:31:51 INFO]: Training loss at epoch 25: 0.9140677452087402
[08/27/2025 15:31:53 INFO]: Training loss at epoch 4: 1.2948507070541382
[08/27/2025 15:32:12 INFO]: Training loss at epoch 47: 1.0221188068389893
[08/27/2025 15:32:15 INFO]: Training loss at epoch 24: 0.8719574213027954
[08/27/2025 15:32:17 INFO]: Training loss at epoch 18: 0.894727349281311
[08/27/2025 15:32:21 INFO]: Training loss at epoch 35: 1.287811666727066
[08/27/2025 15:32:28 INFO]: Training loss at epoch 12: 1.0942695438861847
[08/27/2025 15:32:32 INFO]: Training loss at epoch 40: 0.8399677574634552
[08/27/2025 15:32:40 INFO]: New best epoch, val score: -0.6661755738907291
[08/27/2025 15:32:40 INFO]: Saving model to: unhanged-Shanesha_trial_20/model_best.pth
[08/27/2025 15:32:44 INFO]: Training loss at epoch 25: 0.9580345749855042
[08/27/2025 15:32:58 INFO]: Training loss at epoch 5: 1.3009424209594727
[08/27/2025 15:33:06 INFO]: Training loss at epoch 2: 2.277635931968689
[08/27/2025 15:33:13 INFO]: Training loss at epoch 26: 1.0799999237060547
[08/27/2025 15:33:16 INFO]: New best epoch, val score: -0.6626381743601105
[08/27/2025 15:33:16 INFO]: Saving model to: unhanged-Shanesha_trial_13/model_best.pth
[08/27/2025 15:33:23 INFO]: Training loss at epoch 48: 1.333566129207611
[08/27/2025 15:33:37 INFO]: Training loss at epoch 19: 1.0353832244873047
[08/27/2025 15:33:45 INFO]: Training loss at epoch 27: 0.8737434148788452
[08/27/2025 15:33:53 INFO]: Training loss at epoch 10: 1.01841339468956
[08/27/2025 15:33:56 INFO]: Training loss at epoch 41: 0.8796834945678711
[08/27/2025 15:33:58 INFO]: Training loss at epoch 36: 1.0403504967689514
[08/27/2025 15:34:01 INFO]: Training loss at epoch 26: 0.9165844619274139
[08/27/2025 15:34:05 INFO]: Training stats: {
    "score": -1.0215905155952039,
    "rmse": 1.0215905155952039
}
[08/27/2025 15:34:05 INFO]: Val stats: {
    "score": -0.7638964100686316,
    "rmse": 0.7638964100686316
}
[08/27/2025 15:34:05 INFO]: Test stats: {
    "score": -0.925752049797724,
    "rmse": 0.925752049797724
}
[08/27/2025 15:34:14 INFO]: Training loss at epoch 28: 1.113448143005371
[08/27/2025 15:34:27 INFO]: Training loss at epoch 10: 0.9656924307346344
[08/27/2025 15:34:34 INFO]: Training loss at epoch 16: 1.0899913311004639
[08/27/2025 15:34:35 INFO]: Training loss at epoch 49: 0.9865264594554901
[08/27/2025 15:34:36 INFO]: Training loss at epoch 9: 1.1709719896316528
[08/27/2025 15:34:44 INFO]: Training loss at epoch 11: 1.058721125125885
[08/27/2025 15:34:45 INFO]: Training loss at epoch 29: 0.9405879974365234
[08/27/2025 15:34:55 INFO]: Training stats: {
    "score": -1.0045077197857812,
    "rmse": 1.0045077197857812
}
[08/27/2025 15:34:55 INFO]: Val stats: {
    "score": -0.7120296185048531,
    "rmse": 0.7120296185048531
}
[08/27/2025 15:34:55 INFO]: Test stats: {
    "score": -0.8845583922908998,
    "rmse": 0.8845583922908998
}
[08/27/2025 15:34:58 INFO]: Training loss at epoch 10: 1.3298037350177765
[08/27/2025 15:34:59 INFO]: Training loss at epoch 11: 0.9177185297012329
[08/27/2025 15:35:00 INFO]: Training stats: {
    "score": -0.9988579023228238,
    "rmse": 0.9988579023228238
}
[08/27/2025 15:35:00 INFO]: Val stats: {
    "score": -0.6992684628960911,
    "rmse": 0.6992684628960911
}
[08/27/2025 15:35:00 INFO]: Test stats: {
    "score": -0.8832642374351642,
    "rmse": 0.8832642374351642
}
[08/27/2025 15:35:16 INFO]: Training loss at epoch 42: 1.069243609905243
[08/27/2025 15:35:23 INFO]: Training loss at epoch 30: 1.1545317769050598
[08/27/2025 15:35:23 INFO]: Training loss at epoch 20: 0.9540513455867767
[08/27/2025 15:35:27 INFO]: Training loss at epoch 12: 1.1422017216682434
[08/27/2025 15:35:29 INFO]: Training loss at epoch 37: 1.2308290302753448
[08/27/2025 15:35:51 INFO]: Training loss at epoch 31: 0.9926797151565552
[08/27/2025 15:36:06 INFO]: Training loss at epoch 27: 0.9958327114582062
[08/27/2025 15:36:08 INFO]: Training loss at epoch 50: 1.1527962684631348
[08/27/2025 15:36:21 INFO]: Training loss at epoch 32: 1.0763053894042969
[08/27/2025 15:36:36 INFO]: Training loss at epoch 43: 0.9664188623428345
[08/27/2025 15:36:38 INFO]: Training stats: {
    "score": -1.0022431034596713,
    "rmse": 1.0022431034596713
}
[08/27/2025 15:36:38 INFO]: Val stats: {
    "score": -0.7147622009770813,
    "rmse": 0.7147622009770813
}
[08/27/2025 15:36:38 INFO]: Test stats: {
    "score": -0.8989531093889411,
    "rmse": 0.8989531093889411
}
[08/27/2025 15:36:42 INFO]: Training loss at epoch 21: 1.1277331709861755
[08/27/2025 15:36:47 INFO]: Training loss at epoch 13: 0.9307506382465363
[08/27/2025 15:36:50 INFO]: Training loss at epoch 33: 0.8519080579280853
[08/27/2025 15:37:02 INFO]: Training loss at epoch 38: 0.8403896391391754
[08/27/2025 15:37:17 INFO]: Training loss at epoch 51: 0.8683052957057953
[08/27/2025 15:37:20 INFO]: Training loss at epoch 34: 1.0280662775039673
[08/27/2025 15:37:49 INFO]: Training loss at epoch 35: 1.1612852811813354
[08/27/2025 15:37:54 INFO]: Training loss at epoch 17: 0.8930792808532715
[08/27/2025 15:37:56 INFO]: Training loss at epoch 44: 1.0044438242912292
[08/27/2025 15:38:01 INFO]: Training loss at epoch 22: 0.8522192239761353
[08/27/2025 15:38:11 INFO]: Training loss at epoch 28: 1.420386552810669
[08/27/2025 15:38:13 INFO]: Training loss at epoch 8: 1.0599316954612732
[08/27/2025 15:38:18 INFO]: Training loss at epoch 36: 0.9114121198654175
[08/27/2025 15:38:23 INFO]: Training loss at epoch 52: 1.089914232492447
[08/27/2025 15:38:32 INFO]: Training loss at epoch 39: 0.9640683233737946
[08/27/2025 15:38:32 INFO]: Running Final Evaluation...
[08/27/2025 15:38:35 INFO]: Training loss at epoch 6: 1.4040890336036682
[08/27/2025 15:38:47 INFO]: Training loss at epoch 37: 0.9786760807037354
[08/27/2025 15:38:58 INFO]: Training accuracy: {
    "score": -1.008093076085264,
    "rmse": 1.008093076085264
}
[08/27/2025 15:38:58 INFO]: Val accuracy: {
    "score": -0.661113826458219,
    "rmse": 0.661113826458219
}
[08/27/2025 15:38:58 INFO]: Test accuracy: {
    "score": -0.8685810033252932,
    "rmse": 0.8685810033252932
}
[08/27/2025 15:38:58 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_9",
    "best_epoch": 21,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8685810033252932,
        "rmse": 0.8685810033252932
    },
    "train_stats": {
        "score": -1.008093076085264,
        "rmse": 1.008093076085264
    },
    "val_stats": {
        "score": -0.661113826458219,
        "rmse": 0.661113826458219
    }
}
[08/27/2025 15:38:58 INFO]: Procewss finished for trial unhanged-Shanesha_trial_9
[08/27/2025 15:38:59 INFO]: 
_________________________________________________

[08/27/2025 15:38:59 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:38:59 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 4
  d_ffn_factor: 0.9664449232479996
  attention_dropout: 0.11592579633061056
  ffn_dropout: 0.11592579633061056
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00037573137523750646
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_25

[08/27/2025 15:38:59 INFO]: This ft_transformer has 1.526 million parameters.
[08/27/2025 15:38:59 INFO]: Training will start at epoch 0.
[08/27/2025 15:38:59 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:39:02 INFO]: Training loss at epoch 8: 0.926058292388916
[08/27/2025 15:39:03 INFO]: Training loss at epoch 11: 1.2735915184020996
[08/27/2025 15:39:07 INFO]: Training stats: {
    "score": -0.9949681240849495,
    "rmse": 0.9949681240849495
}
[08/27/2025 15:39:07 INFO]: Val stats: {
    "score": -0.6778307280222973,
    "rmse": 0.6778307280222973
}
[08/27/2025 15:39:07 INFO]: Test stats: {
    "score": -0.8703664193836176,
    "rmse": 0.8703664193836176
}
[08/27/2025 15:39:17 INFO]: Training loss at epoch 38: 1.0318191051483154
[08/27/2025 15:39:18 INFO]: Training loss at epoch 45: 1.2169425785541534
[08/27/2025 15:39:23 INFO]: Training loss at epoch 23: 1.17790687084198
[08/27/2025 15:39:28 INFO]: Training loss at epoch 0: 1.4082958102226257
[08/27/2025 15:39:31 INFO]: Training loss at epoch 12: 1.0492239892482758
[08/27/2025 15:39:32 INFO]: New best epoch, val score: -0.728952254382849
[08/27/2025 15:39:32 INFO]: Saving model to: unhanged-Shanesha_trial_25/model_best.pth
[08/27/2025 15:39:39 INFO]: Training loss at epoch 11: 0.8689223229885101
[08/27/2025 15:39:46 INFO]: Training loss at epoch 39: 1.1182580590248108
[08/27/2025 15:39:48 INFO]: Training loss at epoch 12: 1.0674358904361725
[08/27/2025 15:39:58 INFO]: Training stats: {
    "score": -1.0024065979905192,
    "rmse": 1.0024065979905192
}
[08/27/2025 15:39:58 INFO]: Val stats: {
    "score": -0.7051142100778164,
    "rmse": 0.7051142100778164
}
[08/27/2025 15:39:58 INFO]: Test stats: {
    "score": -0.8805013047302676,
    "rmse": 0.8805013047302676
}
[08/27/2025 15:39:58 INFO]: Training loss at epoch 13: 0.8570937514305115
[08/27/2025 15:40:04 INFO]: Training loss at epoch 1: 1.1684823632240295
[08/27/2025 15:40:14 INFO]: Training loss at epoch 11: 1.3424351513385773
[08/27/2025 15:40:20 INFO]: Training loss at epoch 29: 1.1515873670578003
[08/27/2025 15:40:27 INFO]: Training loss at epoch 40: 1.0031429827213287
[08/27/2025 15:40:31 INFO]: Running Final Evaluation...
[08/27/2025 15:40:39 INFO]: Training loss at epoch 40: 0.9289904236793518
[08/27/2025 15:40:39 INFO]: Training loss at epoch 46: 0.8784733414649963
[08/27/2025 15:40:39 INFO]: Training loss at epoch 2: 1.630678951740265
[08/27/2025 15:40:42 INFO]: Training accuracy: {
    "score": -1.0008803655800742,
    "rmse": 1.0008803655800742
}
[08/27/2025 15:40:42 INFO]: Val accuracy: {
    "score": -0.6822734202671356,
    "rmse": 0.6822734202671356
}
[08/27/2025 15:40:42 INFO]: Test accuracy: {
    "score": -0.8683659343568688,
    "rmse": 0.8683659343568688
}
[08/27/2025 15:40:42 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_24",
    "best_epoch": 9,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8683659343568688,
        "rmse": 0.8683659343568688
    },
    "train_stats": {
        "score": -1.0008803655800742,
        "rmse": 1.0008803655800742
    },
    "val_stats": {
        "score": -0.6822734202671356,
        "rmse": 0.6822734202671356
    }
}
[08/27/2025 15:40:42 INFO]: Procewss finished for trial unhanged-Shanesha_trial_24
[08/27/2025 15:40:42 INFO]: 
_________________________________________________

[08/27/2025 15:40:42 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:40:42 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 16
  d_ffn_factor: 1.372715197791206
  attention_dropout: 0.2445553822336632
  ffn_dropout: 0.2445553822336632
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0009905983114493059
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_26

[08/27/2025 15:40:42 INFO]: This ft_transformer has 14.352 million parameters.
[08/27/2025 15:40:42 INFO]: Training will start at epoch 0.
[08/27/2025 15:40:42 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:40:43 INFO]: New best epoch, val score: -0.6695021363780054
[08/27/2025 15:40:43 INFO]: Saving model to: unhanged-Shanesha_trial_25/model_best.pth
[08/27/2025 15:40:44 INFO]: Training loss at epoch 24: 0.9825994074344635
[08/27/2025 15:40:50 INFO]: Running Final Evaluation...
[08/27/2025 15:41:03 INFO]: Training stats: {
    "score": -0.9966333975278883,
    "rmse": 0.9966333975278883
}
[08/27/2025 15:41:03 INFO]: Val stats: {
    "score": -0.6637706614838385,
    "rmse": 0.6637706614838385
}
[08/27/2025 15:41:03 INFO]: Test stats: {
    "score": -0.8747535723805621,
    "rmse": 0.8747535723805621
}
[08/27/2025 15:41:04 INFO]: Training loss at epoch 14: 0.9847286939620972
[08/27/2025 15:41:13 INFO]: Training loss at epoch 3: 1.3554543256759644
[08/27/2025 15:41:19 INFO]: Training loss at epoch 18: 1.001825988292694
[08/27/2025 15:41:22 INFO]: Training accuracy: {
    "score": -1.0155196690464265,
    "rmse": 1.0155196690464265
}
[08/27/2025 15:41:22 INFO]: Val accuracy: {
    "score": -0.6646620254450489,
    "rmse": 0.6646620254450489
}
[08/27/2025 15:41:22 INFO]: Test accuracy: {
    "score": -0.8721934806032174,
    "rmse": 0.8721934806032174
}
[08/27/2025 15:41:22 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_19",
    "best_epoch": 9,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8721934806032174,
        "rmse": 0.8721934806032174
    },
    "train_stats": {
        "score": -1.0155196690464265,
        "rmse": 1.0155196690464265
    },
    "val_stats": {
        "score": -0.6646620254450489,
        "rmse": 0.6646620254450489
    }
}
[08/27/2025 15:41:22 INFO]: Procewss finished for trial unhanged-Shanesha_trial_19
[08/27/2025 15:41:22 INFO]: 
_________________________________________________

[08/27/2025 15:41:22 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:41:22 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 1.8127891432122674
  attention_dropout: 0.31019376890919687
  ffn_dropout: 0.31019376890919687
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0006319585096409879
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_27

[08/27/2025 15:41:22 INFO]: This ft_transformer has 0.416 million parameters.
[08/27/2025 15:41:22 INFO]: Training will start at epoch 0.
[08/27/2025 15:41:22 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:41:48 INFO]: Training loss at epoch 4: 1.3653056025505066
[08/27/2025 15:41:53 INFO]: New best epoch, val score: -0.6639020225420286
[08/27/2025 15:41:53 INFO]: Saving model to: unhanged-Shanesha_trial_25/model_best.pth
[08/27/2025 15:41:58 INFO]: Training loss at epoch 47: 0.8798181712627411
[08/27/2025 15:42:03 INFO]: Training loss at epoch 25: 1.0122088193893433
[08/27/2025 15:42:22 INFO]: Training loss at epoch 5: 1.1288900971412659
[08/27/2025 15:42:26 INFO]: New best epoch, val score: -0.662833191889585
[08/27/2025 15:42:26 INFO]: Saving model to: unhanged-Shanesha_trial_25/model_best.pth
[08/27/2025 15:42:39 INFO]: Training loss at epoch 10: 1.2383527755737305
[08/27/2025 15:42:42 INFO]: Training loss at epoch 0: 0.7965729534626007
[08/27/2025 15:42:54 INFO]: New best epoch, val score: -0.7030688440290798
[08/27/2025 15:42:54 INFO]: Saving model to: unhanged-Shanesha_trial_27/model_best.pth
[08/27/2025 15:42:57 INFO]: Training loss at epoch 6: 0.9238426089286804
[08/27/2025 15:43:01 INFO]: New best epoch, val score: -0.6619488986250791
[08/27/2025 15:43:01 INFO]: Saving model to: unhanged-Shanesha_trial_25/model_best.pth
[08/27/2025 15:43:09 INFO]: Training loss at epoch 30: 1.2723016440868378
[08/27/2025 15:43:18 INFO]: Training loss at epoch 48: 0.8743707239627838
[08/27/2025 15:43:23 INFO]: Training loss at epoch 26: 0.9659510850906372
[08/27/2025 15:43:33 INFO]: Training loss at epoch 7: 0.8249930441379547
[08/27/2025 15:43:35 INFO]: Training loss at epoch 5: 1.1059331893920898
[08/27/2025 15:43:37 INFO]: New best epoch, val score: -0.6615246291790499
[08/27/2025 15:43:37 INFO]: Saving model to: unhanged-Shanesha_trial_25/model_best.pth
[08/27/2025 15:44:06 INFO]: Training loss at epoch 8: 0.990348607301712
[08/27/2025 15:44:10 INFO]: Training loss at epoch 7: 0.9914963245391846
[08/27/2025 15:44:13 INFO]: Training loss at epoch 1: 0.8129705786705017
[08/27/2025 15:44:13 INFO]: Training loss at epoch 12: 1.150334656238556
[08/27/2025 15:44:22 INFO]: Training loss at epoch 13: 1.4427611231803894
[08/27/2025 15:44:28 INFO]: Training loss at epoch 3: 1.3671030402183533
[08/27/2025 15:44:29 INFO]: Training loss at epoch 14: 1.0960694551467896
[08/27/2025 15:44:37 INFO]: Training loss at epoch 49: 1.0565311014652252
[08/27/2025 15:44:37 INFO]: Training loss at epoch 13: 1.2876331806182861
[08/27/2025 15:44:38 INFO]: Training loss at epoch 9: 0.8580331802368164
[08/27/2025 15:44:40 INFO]: Training loss at epoch 27: 0.8183225095272064
[08/27/2025 15:44:41 INFO]: Training loss at epoch 19: 1.4269262850284576
[08/27/2025 15:44:50 INFO]: Training stats: {
    "score": -1.003949148901519,
    "rmse": 1.003949148901519
}
[08/27/2025 15:44:50 INFO]: Val stats: {
    "score": -0.6628277997844053,
    "rmse": 0.6628277997844053
}
[08/27/2025 15:44:50 INFO]: Test stats: {
    "score": -0.870073016998717,
    "rmse": 0.870073016998717
}
[08/27/2025 15:44:54 INFO]: New best epoch, val score: -0.6619164539586625
[08/27/2025 15:44:54 INFO]: Saving model to: unhanged-Shanesha_trial_13/model_best.pth
[08/27/2025 15:44:55 INFO]: Training loss at epoch 12: 0.9290118515491486
[08/27/2025 15:45:06 INFO]: Training stats: {
    "score": -0.9985495748811096,
    "rmse": 0.9985495748811096
}
[08/27/2025 15:45:06 INFO]: Val stats: {
    "score": -0.6874061191471799,
    "rmse": 0.6874061191471799
}
[08/27/2025 15:45:06 INFO]: Test stats: {
    "score": -0.8802184760437155,
    "rmse": 0.8802184760437155
}
[08/27/2025 15:45:16 INFO]: Training loss at epoch 31: 0.9315786361694336
[08/27/2025 15:45:19 INFO]: Training loss at epoch 15: 0.9856836199760437
[08/27/2025 15:45:21 INFO]: Training loss at epoch 9: 0.849553644657135
[08/27/2025 15:45:27 INFO]: Training loss at epoch 10: 1.0372983813285828
[08/27/2025 15:45:34 INFO]: Training loss at epoch 12: 1.044339656829834
[08/27/2025 15:45:45 INFO]: Training loss at epoch 2: 0.943645566701889
[08/27/2025 15:45:55 INFO]: New best epoch, val score: -0.6605544453207195
[08/27/2025 15:45:55 INFO]: Saving model to: unhanged-Shanesha_trial_22/model_best.pth
[08/27/2025 15:45:59 INFO]: Training stats: {
    "score": -1.0081076397696132,
    "rmse": 1.0081076397696132
}
[08/27/2025 15:45:59 INFO]: Val stats: {
    "score": -0.7328526423169128,
    "rmse": 0.7328526423169128
}
[08/27/2025 15:45:59 INFO]: Test stats: {
    "score": -0.9018418495461137,
    "rmse": 0.9018418495461137
}
[08/27/2025 15:46:03 INFO]: Training loss at epoch 11: 0.8474681079387665
[08/27/2025 15:46:04 INFO]: Training loss at epoch 28: 1.0448050796985626
[08/27/2025 15:46:12 INFO]: Training loss at epoch 9: 0.9083325564861298
[08/27/2025 15:46:32 INFO]: Training loss at epoch 50: 0.8510930836200714
[08/27/2025 15:46:39 INFO]: Training loss at epoch 12: 1.0712798833847046
[08/27/2025 15:47:12 INFO]: Training loss at epoch 13: 0.8933188617229462
[08/27/2025 15:47:18 INFO]: Training loss at epoch 3: 1.0087107419967651
[08/27/2025 15:47:25 INFO]: Training loss at epoch 29: 1.0836091339588165
[08/27/2025 15:47:27 INFO]: Training loss at epoch 32: 1.1493076086044312
[08/27/2025 15:47:47 INFO]: Training loss at epoch 14: 1.14844810962677
[08/27/2025 15:47:47 INFO]: Training stats: {
    "score": -0.9998521364427508,
    "rmse": 0.9998521364427508
}
[08/27/2025 15:47:47 INFO]: Val stats: {
    "score": -0.6759560045137593,
    "rmse": 0.6759560045137593
}
[08/27/2025 15:47:47 INFO]: Test stats: {
    "score": -0.8722640122417558,
    "rmse": 0.8722640122417558
}
[08/27/2025 15:47:51 INFO]: Training stats: {
    "score": -0.9971367165524794,
    "rmse": 0.9971367165524794
}
[08/27/2025 15:47:51 INFO]: Val stats: {
    "score": -0.6761897492464197,
    "rmse": 0.6761897492464197
}
[08/27/2025 15:47:51 INFO]: Test stats: {
    "score": -0.8725702732063524,
    "rmse": 0.8725702732063524
}
[08/27/2025 15:47:55 INFO]: Training loss at epoch 51: 1.0170550346374512
[08/27/2025 15:48:01 INFO]: Training loss at epoch 0: 1.066303938627243
[08/27/2025 15:48:22 INFO]: Training loss at epoch 15: 0.8260809481143951
[08/27/2025 15:48:38 INFO]: Training stats: {
    "score": -0.9958016069705634,
    "rmse": 0.9958016069705634
}
[08/27/2025 15:48:38 INFO]: Val stats: {
    "score": -0.6674409145738341,
    "rmse": 0.6674409145738341
}
[08/27/2025 15:48:38 INFO]: Test stats: {
    "score": -0.8703211564896323,
    "rmse": 0.8703211564896323
}
[08/27/2025 15:48:40 INFO]: Training loss at epoch 11: 1.1177507638931274
[08/27/2025 15:48:52 INFO]: Training loss at epoch 4: 0.9083981513977051
[08/27/2025 15:48:58 INFO]: Training loss at epoch 16: 0.7974191009998322
[08/27/2025 15:49:03 INFO]: New best epoch, val score: -0.6709604732778585
[08/27/2025 15:49:03 INFO]: Saving model to: unhanged-Shanesha_trial_26/model_best.pth
[08/27/2025 15:49:03 INFO]: New best epoch, val score: -0.6648908296628676
[08/27/2025 15:49:03 INFO]: Saving model to: unhanged-Shanesha_trial_27/model_best.pth
[08/27/2025 15:49:06 INFO]: Training loss at epoch 15: 0.9739896357059479
[08/27/2025 15:49:12 INFO]: Training loss at epoch 30: 0.9474482536315918
[08/27/2025 15:49:18 INFO]: Training loss at epoch 52: 1.0669998824596405
[08/27/2025 15:49:20 INFO]: Training loss at epoch 14: 1.577936828136444
[08/27/2025 15:49:25 INFO]: Training loss at epoch 20: 1.1234877109527588
[08/27/2025 15:49:29 INFO]: Training loss at epoch 13: 1.0207135677337646
[08/27/2025 15:49:30 INFO]: Training loss at epoch 17: 0.8826153874397278
[08/27/2025 15:49:33 INFO]: Training loss at epoch 14: 1.2164430618286133
[08/27/2025 15:49:34 INFO]: Training loss at epoch 33: 0.8622570931911469
[08/27/2025 15:49:39 INFO]: Training loss at epoch 16: 0.8148851692676544
[08/27/2025 15:49:49 INFO]: Training loss at epoch 8: 1.0680461525917053
[08/27/2025 15:50:05 INFO]: Training loss at epoch 18: 0.8328207731246948
[08/27/2025 15:50:16 INFO]: Training loss at epoch 13: 1.0683937072753906
[08/27/2025 15:50:24 INFO]: Training loss at epoch 5: 1.0011612474918365
[08/27/2025 15:50:33 INFO]: Training loss at epoch 31: 0.8978758454322815
[08/27/2025 15:50:36 INFO]: New best epoch, val score: -0.6648484830755069
[08/27/2025 15:50:36 INFO]: Saving model to: unhanged-Shanesha_trial_27/model_best.pth
[08/27/2025 15:50:39 INFO]: Training loss at epoch 53: 0.9432851076126099
[08/27/2025 15:50:41 INFO]: Training loss at epoch 19: 0.8762940466403961
[08/27/2025 15:50:42 INFO]: Running Final Evaluation...
[08/27/2025 15:50:52 INFO]: Training stats: {
    "score": -0.9978514734858863,
    "rmse": 0.9978514734858863
}
[08/27/2025 15:50:52 INFO]: Val stats: {
    "score": -0.6657474931051246,
    "rmse": 0.6657474931051246
}
[08/27/2025 15:50:52 INFO]: Test stats: {
    "score": -0.87093444480698,
    "rmse": 0.87093444480698
}
[08/27/2025 15:50:58 INFO]: Training loss at epoch 13: 1.4268145859241486
[08/27/2025 15:51:13 INFO]: Training accuracy: {
    "score": -1.0130750270411637,
    "rmse": 1.0130750270411637
}
[08/27/2025 15:51:13 INFO]: Val accuracy: {
    "score": -0.6592059241466465,
    "rmse": 0.6592059241466465
}
[08/27/2025 15:51:13 INFO]: Test accuracy: {
    "score": -0.86920383593592,
    "rmse": 0.86920383593592
}
[08/27/2025 15:51:13 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_23",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.86920383593592,
        "rmse": 0.86920383593592
    },
    "train_stats": {
        "score": -1.0130750270411637,
        "rmse": 1.0130750270411637
    },
    "val_stats": {
        "score": -0.6592059241466465,
        "rmse": 0.6592059241466465
    }
}
[08/27/2025 15:51:13 INFO]: Procewss finished for trial unhanged-Shanesha_trial_23
[08/27/2025 15:51:13 INFO]: 
_________________________________________________

[08/27/2025 15:51:13 INFO]: train_net_for_optune.py main() running.
[08/27/2025 15:51:13 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 0.8818042864285488
  attention_dropout: 0.3305303124548957
  ffn_dropout: 0.3305303124548957
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0001701634197464226
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_28

[08/27/2025 15:51:13 INFO]: This ft_transformer has 2.711 million parameters.
[08/27/2025 15:51:13 INFO]: Training will start at epoch 0.
[08/27/2025 15:51:13 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 15:51:27 INFO]: Training loss at epoch 20: 0.9414564967155457
[08/27/2025 15:51:45 INFO]: Training loss at epoch 34: 1.1390330493450165
[08/27/2025 15:51:57 INFO]: Training loss at epoch 6: 1.0788788199424744
[08/27/2025 15:52:01 INFO]: Training loss at epoch 54: 1.1564422845840454
[08/27/2025 15:52:03 INFO]: Training loss at epoch 21: 0.9465878009796143
[08/27/2025 15:52:35 INFO]: Training loss at epoch 22: 1.0875752568244934
[08/27/2025 15:52:53 INFO]: Training loss at epoch 21: 0.9799226224422455
[08/27/2025 15:53:11 INFO]: Training loss at epoch 23: 0.8931224942207336
[08/27/2025 15:53:19 INFO]: Training loss at epoch 55: 0.8084710836410522
[08/27/2025 15:53:27 INFO]: Training loss at epoch 7: 1.0045454502105713
[08/27/2025 15:53:33 INFO]: Training loss at epoch 0: 1.1296618580818176
[08/27/2025 15:53:38 INFO]: Training loss at epoch 16: 0.9129164814949036
[08/27/2025 15:53:46 INFO]: Training loss at epoch 24: 1.0212838053703308
[08/27/2025 15:53:52 INFO]: Training loss at epoch 35: 0.8166987597942352
[08/27/2025 15:53:53 INFO]: New best epoch, val score: -0.8936582895011841
[08/27/2025 15:53:53 INFO]: Saving model to: unhanged-Shanesha_trial_28/model_best.pth
[08/27/2025 15:53:59 INFO]: Training loss at epoch 17: 0.954076737165451
[08/27/2025 15:54:12 INFO]: Training loss at epoch 15: 1.3959441781044006
[08/27/2025 15:54:19 INFO]: Training loss at epoch 25: 0.9914819896221161
[08/27/2025 15:54:25 INFO]: Training loss at epoch 15: 1.2716723084449768
[08/27/2025 15:54:39 INFO]: Training loss at epoch 12: 0.9998054802417755
[08/27/2025 15:54:40 INFO]: Training loss at epoch 56: 1.0473071038722992
[08/27/2025 15:54:43 INFO]: Training loss at epoch 14: 0.9863055348396301
[08/27/2025 15:54:54 INFO]: Training loss at epoch 26: 0.991470217704773
[08/27/2025 15:54:54 INFO]: Training loss at epoch 10: 0.9535978436470032
[08/27/2025 15:54:59 INFO]: Training loss at epoch 8: 0.9612735211849213
[08/27/2025 15:55:17 INFO]: Training loss at epoch 6: 1.1502180695533752
[08/27/2025 15:55:29 INFO]: Training loss at epoch 9: 1.2605665922164917
[08/27/2025 15:55:29 INFO]: Training loss at epoch 27: 1.2674359679222107
[08/27/2025 15:55:33 INFO]: Training loss at epoch 14: 0.9810525476932526
[08/27/2025 15:55:49 INFO]: Training loss at epoch 10: 1.0518112778663635
[08/27/2025 15:56:00 INFO]: Training loss at epoch 36: 1.025507390499115
[08/27/2025 15:56:01 INFO]: Training loss at epoch 4: 1.2274295687675476
[08/27/2025 15:56:01 INFO]: Training loss at epoch 57: 0.9984830021858215
[08/27/2025 15:56:04 INFO]: Training loss at epoch 28: 0.8773474097251892
[08/27/2025 15:56:15 INFO]: Training loss at epoch 1: 1.0939644277095795
[08/27/2025 15:56:16 INFO]: Training loss at epoch 14: 0.991800993680954
[08/27/2025 15:56:18 INFO]: Training loss at epoch 22: 1.0725565552711487
[08/27/2025 15:56:27 INFO]: Training loss at epoch 1: 6.39262330532074
[08/27/2025 15:56:33 INFO]: Training loss at epoch 9: 0.9754728078842163
[08/27/2025 15:56:40 INFO]: Training loss at epoch 29: 0.976742684841156
[08/27/2025 15:56:40 INFO]: New best epoch, val score: -0.6611999999489295
[08/27/2025 15:56:40 INFO]: Saving model to: unhanged-Shanesha_trial_13/model_best.pth
[08/27/2025 15:56:52 INFO]: Training stats: {
    "score": -0.9909413225006377,
    "rmse": 0.9909413225006377
}
[08/27/2025 15:56:52 INFO]: Val stats: {
    "score": -0.6835911490164249,
    "rmse": 0.6835911490164249
}
[08/27/2025 15:56:52 INFO]: Test stats: {
    "score": -0.8777840657935424,
    "rmse": 0.8777840657935424
}
[08/27/2025 15:57:09 INFO]: Training stats: {
    "score": -1.0029050706174287,
    "rmse": 1.0029050706174287
}
[08/27/2025 15:57:09 INFO]: Val stats: {
    "score": -0.6655649330346582,
    "rmse": 0.6655649330346582
}
[08/27/2025 15:57:09 INFO]: Test stats: {
    "score": -0.8729466043775371,
    "rmse": 0.8729466043775371
}
[08/27/2025 15:57:25 INFO]: Training loss at epoch 58: 0.8410453200340271
[08/27/2025 15:57:27 INFO]: Training stats: {
    "score": -1.030150969231438,
    "rmse": 1.030150969231438
}
[08/27/2025 15:57:27 INFO]: Val stats: {
    "score": -0.7802763693452452,
    "rmse": 0.7802763693452452
}
[08/27/2025 15:57:27 INFO]: Test stats: {
    "score": -0.936761133264454,
    "rmse": 0.936761133264454
}
[08/27/2025 15:57:28 INFO]: Training loss at epoch 30: 0.8287839591503143
[08/27/2025 15:58:04 INFO]: Training loss at epoch 31: 1.091854989528656
[08/27/2025 15:58:10 INFO]: Training loss at epoch 37: 0.9936591982841492
[08/27/2025 15:58:12 INFO]: Training loss at epoch 17: 1.0468161702156067
[08/27/2025 15:58:20 INFO]: Training loss at epoch 18: 1.064432829618454
[08/27/2025 15:58:40 INFO]: Training loss at epoch 32: 1.156733214855194
[08/27/2025 15:58:41 INFO]: Training loss at epoch 10: 1.1713013648986816
[08/27/2025 15:58:46 INFO]: Training loss at epoch 59: 1.4153543710708618
[08/27/2025 15:58:59 INFO]: Training loss at epoch 2: 1.5495370030403137
[08/27/2025 15:59:08 INFO]: Training loss at epoch 16: 1.0543818473815918
[08/27/2025 15:59:13 INFO]: Training loss at epoch 33: 0.7802093327045441
[08/27/2025 15:59:13 INFO]: Training stats: {
    "score": -0.9980049160322398,
    "rmse": 0.9980049160322398
}
[08/27/2025 15:59:13 INFO]: Val stats: {
    "score": -0.6858788631636179,
    "rmse": 0.6858788631636179
}
[08/27/2025 15:59:13 INFO]: Test stats: {
    "score": -0.8798919985341406,
    "rmse": 0.8798919985341406
}
[08/27/2025 15:59:18 INFO]: New best epoch, val score: -0.669300453897624
[08/27/2025 15:59:18 INFO]: Saving model to: unhanged-Shanesha_trial_28/model_best.pth
[08/27/2025 15:59:22 INFO]: Training loss at epoch 16: 1.1284099221229553
[08/27/2025 15:59:45 INFO]: Training loss at epoch 23: 0.8588474988937378
[08/27/2025 15:59:51 INFO]: Training loss at epoch 34: 0.9985272288322449
[08/27/2025 15:59:59 INFO]: Training loss at epoch 15: 0.9221030175685883
[08/27/2025 16:00:12 INFO]: Training loss at epoch 11: 1.0627253651618958
[08/27/2025 16:00:20 INFO]: Training loss at epoch 38: 0.8908064961433411
[08/27/2025 16:00:27 INFO]: Training loss at epoch 35: 0.7895345091819763
[08/27/2025 16:00:36 INFO]: Training loss at epoch 60: 0.9534030258655548
[08/27/2025 16:00:43 INFO]: Training loss at epoch 13: 1.3570467233657837
[08/27/2025 16:00:54 INFO]: Training loss at epoch 15: 0.9091166853904724
[08/27/2025 16:01:03 INFO]: Training loss at epoch 36: 1.1755793988704681
[08/27/2025 16:01:38 INFO]: Training loss at epoch 37: 0.9326878786087036
[08/27/2025 16:01:40 INFO]: Training loss at epoch 15: 1.1838852763175964
[08/27/2025 16:01:43 INFO]: Training loss at epoch 3: 1.1205887198448181
[08/27/2025 16:01:49 INFO]: Training loss at epoch 12: 0.8939022421836853
[08/27/2025 16:01:57 INFO]: Training loss at epoch 61: 0.913413792848587
[08/27/2025 16:02:05 INFO]: Training loss at epoch 11: 0.8888513743877411
[08/27/2025 16:02:11 INFO]: Training loss at epoch 38: 0.9300429821014404
[08/27/2025 16:02:15 INFO]: Running Final Evaluation...
[08/27/2025 16:02:28 INFO]: Training accuracy: {
    "score": -1.0106518922470216,
    "rmse": 1.0106518922470216
}
[08/27/2025 16:02:28 INFO]: Val accuracy: {
    "score": -0.6615246291790499,
    "rmse": 0.6615246291790499
}
[08/27/2025 16:02:28 INFO]: Test accuracy: {
    "score": -0.8726493163410004,
    "rmse": 0.8726493163410004
}
[08/27/2025 16:02:28 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_25",
    "best_epoch": 7,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8726493163410004,
        "rmse": 0.8726493163410004
    },
    "train_stats": {
        "score": -1.0106518922470216,
        "rmse": 1.0106518922470216
    },
    "val_stats": {
        "score": -0.6615246291790499,
        "rmse": 0.6615246291790499
    }
}
[08/27/2025 16:02:28 INFO]: Procewss finished for trial unhanged-Shanesha_trial_25
[08/27/2025 16:02:28 INFO]: 
_________________________________________________

[08/27/2025 16:02:28 INFO]: train_net_for_optune.py main() running.
[08/27/2025 16:02:28 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 0.9574671488257872
  attention_dropout: 0.23644365489240066
  ffn_dropout: 0.23644365489240066
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.000129575561888516
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_29

[08/27/2025 16:02:28 INFO]: This ft_transformer has 4.975 million parameters.
[08/27/2025 16:02:28 INFO]: Training will start at epoch 0.
[08/27/2025 16:02:28 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 16:02:29 INFO]: Training loss at epoch 39: 1.0111159086227417
[08/27/2025 16:02:40 INFO]: Training loss at epoch 19: 0.9223380982875824
[08/27/2025 16:02:47 INFO]: Training loss at epoch 18: 1.0774392485618591
[08/27/2025 16:03:05 INFO]: Training loss at epoch 11: 1.184948205947876
[08/27/2025 16:03:05 INFO]: Training loss at epoch 10: 1.1138808727264404
[08/27/2025 16:03:11 INFO]: Training loss at epoch 24: 0.9212615489959717
[08/27/2025 16:03:12 INFO]: Training stats: {
    "score": -0.9912595095505066,
    "rmse": 0.9912595095505066
}
[08/27/2025 16:03:12 INFO]: Val stats: {
    "score": -0.6648191918945283,
    "rmse": 0.6648191918945283
}
[08/27/2025 16:03:12 INFO]: Test stats: {
    "score": -0.8759633543899816,
    "rmse": 0.8759633543899816
}
[08/27/2025 16:03:18 INFO]: Training loss at epoch 62: 1.1541897058486938
[08/27/2025 16:03:23 INFO]: Training loss at epoch 13: 1.0208119750022888
[08/27/2025 16:03:46 INFO]: New best epoch, val score: -0.681066922529996
[08/27/2025 16:03:46 INFO]: Saving model to: unhanged-Shanesha_trial_21/model_best.pth
[08/27/2025 16:04:02 INFO]: Training loss at epoch 17: 1.012255847454071
[08/27/2025 16:04:04 INFO]: Training stats: {
    "score": -0.9742351938498249,
    "rmse": 0.9742351938498249
}
[08/27/2025 16:04:04 INFO]: Val stats: {
    "score": -0.6987357953196377,
    "rmse": 0.6987357953196377
}
[08/27/2025 16:04:04 INFO]: Test stats: {
    "score": -0.9051766747461695,
    "rmse": 0.9051766747461695
}
[08/27/2025 16:04:17 INFO]: Training loss at epoch 17: 0.8841346502304077
[08/27/2025 16:04:23 INFO]: Training loss at epoch 4: 1.2055377960205078
[08/27/2025 16:04:38 INFO]: Training loss at epoch 63: 1.0771751403808594
[08/27/2025 16:04:51 INFO]: New best epoch, val score: -0.6813055015768253
[08/27/2025 16:04:51 INFO]: Saving model to: unhanged-Shanesha_trial_2/model_best.pth
[08/27/2025 16:04:54 INFO]: Training loss at epoch 2: 2.861287295818329
[08/27/2025 16:04:56 INFO]: Training loss at epoch 14: 1.299592286348343
[08/27/2025 16:05:13 INFO]: Training loss at epoch 16: 1.1656075716018677
[08/27/2025 16:05:20 INFO]: Training loss at epoch 40: 0.9988190233707428
[08/27/2025 16:05:34 INFO]: Training loss at epoch 0: 1.010353147983551
[08/27/2025 16:05:57 INFO]: Training loss at epoch 64: 0.9322132468223572
[08/27/2025 16:05:58 INFO]: New best epoch, val score: -0.7206593392847785
[08/27/2025 16:05:58 INFO]: Saving model to: unhanged-Shanesha_trial_29/model_best.pth
[08/27/2025 16:06:09 INFO]: Training loss at epoch 16: 0.8396375179290771
[08/27/2025 16:06:29 INFO]: Training loss at epoch 15: 0.9504774510860443
[08/27/2025 16:06:36 INFO]: Training loss at epoch 25: 1.1125040650367737
[08/27/2025 16:06:42 INFO]: Training loss at epoch 14: 1.1656405925750732
[08/27/2025 16:06:56 INFO]: Training loss at epoch 16: 1.2960854172706604
[08/27/2025 16:07:01 INFO]: Training loss at epoch 7: 0.9338025450706482
[08/27/2025 16:07:05 INFO]: Training loss at epoch 5: 0.8429974913597107
[08/27/2025 16:07:18 INFO]: Training loss at epoch 19: 1.0773522555828094
[08/27/2025 16:07:19 INFO]: Training loss at epoch 65: 1.0180831551551819
[08/27/2025 16:07:26 INFO]: Training loss at epoch 41: 0.8309971988201141
[08/27/2025 16:07:29 INFO]: Training loss at epoch 5: 1.6649314165115356
[08/27/2025 16:08:02 INFO]: Training loss at epoch 16: 0.9476534128189087
[08/27/2025 16:08:22 INFO]: Training loss at epoch 20: 0.9447528719902039
[08/27/2025 16:08:23 INFO]: New best epoch, val score: -0.6611709299330296
[08/27/2025 16:08:23 INFO]: Saving model to: unhanged-Shanesha_trial_13/model_best.pth
[08/27/2025 16:08:39 INFO]: Training loss at epoch 66: 0.8280392289161682
[08/27/2025 16:08:42 INFO]: Training loss at epoch 11: 1.0494125485420227
[08/27/2025 16:08:50 INFO]: Training stats: {
    "score": -0.9986915881702901,
    "rmse": 0.9986915881702901
}
[08/27/2025 16:08:50 INFO]: Val stats: {
    "score": -0.6766281903164886,
    "rmse": 0.6766281903164886
}
[08/27/2025 16:08:50 INFO]: Test stats: {
    "score": -0.8732831545469935,
    "rmse": 0.8732831545469935
}
[08/27/2025 16:08:54 INFO]: Training loss at epoch 18: 0.9757311344146729
[08/27/2025 16:09:05 INFO]: Training loss at epoch 1: 1.0544716119766235
[08/27/2025 16:09:09 INFO]: Training loss at epoch 18: 1.2009239792823792
[08/27/2025 16:09:09 INFO]: Training loss at epoch 12: 0.8905296623706818
[08/27/2025 16:09:24 INFO]: New best epoch, val score: -0.6630298554733649
[08/27/2025 16:09:24 INFO]: Saving model to: unhanged-Shanesha_trial_21/model_best.pth
[08/27/2025 16:09:35 INFO]: Training loss at epoch 42: 0.9480929374694824
[08/27/2025 16:09:36 INFO]: Training loss at epoch 17: 1.1671134233474731
[08/27/2025 16:09:44 INFO]: New best epoch, val score: -0.6607318601033959
[08/27/2025 16:09:44 INFO]: Saving model to: unhanged-Shanesha_trial_2/model_best.pth
[08/27/2025 16:09:47 INFO]: Training loss at epoch 6: 1.1932953000068665
[08/27/2025 16:10:00 INFO]: Training loss at epoch 26: 1.0097139775753021
[08/27/2025 16:10:01 INFO]: Training loss at epoch 67: 0.9979007840156555
[08/27/2025 16:10:12 INFO]: Running Final Evaluation...
[08/27/2025 16:10:17 INFO]: Training loss at epoch 12: 1.031656563282013
[08/27/2025 16:10:25 INFO]: Training loss at epoch 17: 1.1283557415008545
[08/27/2025 16:10:41 INFO]: Training accuracy: {
    "score": -1.0068139183478653,
    "rmse": 1.0068139183478653
}
[08/27/2025 16:10:41 INFO]: Val accuracy: {
    "score": -0.6596467339786175,
    "rmse": 0.6596467339786175
}
[08/27/2025 16:10:41 INFO]: Test accuracy: {
    "score": -0.8708428206976148,
    "rmse": 0.8708428206976148
}
[08/27/2025 16:10:41 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_6",
    "best_epoch": 36,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8708428206976148,
        "rmse": 0.8708428206976148
    },
    "train_stats": {
        "score": -1.0068139183478653,
        "rmse": 1.0068139183478653
    },
    "val_stats": {
        "score": -0.6596467339786175,
        "rmse": 0.6596467339786175
    }
}
[08/27/2025 16:10:41 INFO]: Procewss finished for trial unhanged-Shanesha_trial_6
[08/27/2025 16:10:41 INFO]: 
_________________________________________________

[08/27/2025 16:10:41 INFO]: train_net_for_optune.py main() running.
[08/27/2025 16:10:41 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 1.9926327754867499
  attention_dropout: 0.21700153302518022
  ffn_dropout: 0.21700153302518022
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.820558089281844e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_30

[08/27/2025 16:10:41 INFO]: This ft_transformer has 7.217 million parameters.
[08/27/2025 16:10:41 INFO]: Training will start at epoch 0.
[08/27/2025 16:10:41 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 16:11:10 INFO]: Training loss at epoch 18: 0.9572740197181702
[08/27/2025 16:11:28 INFO]: Training loss at epoch 17: 0.8388720750808716
[08/27/2025 16:11:44 INFO]: Training loss at epoch 43: 1.0021187961101532
[08/27/2025 16:12:06 INFO]: New best epoch, val score: -0.6609069778035969
[08/27/2025 16:12:06 INFO]: Saving model to: unhanged-Shanesha_trial_5/model_best.pth
[08/27/2025 16:12:14 INFO]: Training loss at epoch 17: 1.0498633980751038
[08/27/2025 16:12:28 INFO]: Training loss at epoch 7: 1.0278558731079102
[08/27/2025 16:12:38 INFO]: Training loss at epoch 2: 0.9933001697063446
[08/27/2025 16:12:40 INFO]: Training loss at epoch 21: 0.9438459873199463
[08/27/2025 16:12:41 INFO]: Training loss at epoch 15: 0.9397872090339661
[08/27/2025 16:12:42 INFO]: Training loss at epoch 19: 0.9200057983398438
[08/27/2025 16:13:14 INFO]: Training loss at epoch 3: 1.1588932871818542
[08/27/2025 16:13:15 INFO]: Training stats: {
    "score": -0.9978517700732565,
    "rmse": 0.9978517700732565
}
[08/27/2025 16:13:15 INFO]: Val stats: {
    "score": -0.6676032373920966,
    "rmse": 0.6676032373920966
}
[08/27/2025 16:13:15 INFO]: Test stats: {
    "score": -0.8724028205788266,
    "rmse": 0.8724028205788266
}
[08/27/2025 16:13:19 INFO]: Training loss at epoch 20: 1.0908330082893372
[08/27/2025 16:13:22 INFO]: Training loss at epoch 27: 0.9055051207542419
[08/27/2025 16:13:41 INFO]: Training loss at epoch 19: 0.941882997751236
[08/27/2025 16:13:46 INFO]: Training loss at epoch 44: 0.7598936259746552
[08/27/2025 16:13:58 INFO]: Training loss at epoch 19: 0.9487775564193726
[08/27/2025 16:14:19 INFO]: Training loss at epoch 12: 1.0493091344833374
[08/27/2025 16:14:38 INFO]: Training loss at epoch 0: 1.7387372851371765
[08/27/2025 16:14:48 INFO]: Training loss at epoch 20: 1.2192109823226929
[08/27/2025 16:15:06 INFO]: Training loss at epoch 8: 0.8988890647888184
[08/27/2025 16:15:10 INFO]: New best epoch, val score: -1.2126682500962316
[08/27/2025 16:15:10 INFO]: Saving model to: unhanged-Shanesha_trial_30/model_best.pth
[08/27/2025 16:15:21 INFO]: Training stats: {
    "score": -1.0799714566698078,
    "rmse": 1.0799714566698078
}
[08/27/2025 16:15:21 INFO]: Val stats: {
    "score": -0.8745162216601029,
    "rmse": 0.8745162216601029
}
[08/27/2025 16:15:21 INFO]: Test stats: {
    "score": -1.0057503555120844,
    "rmse": 1.0057503555120844
}
[08/27/2025 16:15:32 INFO]: Training loss at epoch 18: 0.972823977470398
[08/27/2025 16:15:36 INFO]: Training stats: {
    "score": -1.0214384848800473,
    "rmse": 1.0214384848800473
}
[08/27/2025 16:15:36 INFO]: Val stats: {
    "score": -0.6615640883091453,
    "rmse": 0.6615640883091453
}
[08/27/2025 16:15:36 INFO]: Test stats: {
    "score": -0.8754280539395445,
    "rmse": 0.8754280539395445
}
[08/27/2025 16:15:52 INFO]: Training loss at epoch 45: 0.9358391165733337
[08/27/2025 16:16:06 INFO]: Training loss at epoch 3: 1.4119811654090881
[08/27/2025 16:16:10 INFO]: Training loss at epoch 13: 0.9904820024967194
[08/27/2025 16:16:17 INFO]: Training loss at epoch 21: 0.9660408198833466
[08/27/2025 16:16:30 INFO]: New best epoch, val score: -0.6632722599211397
[08/27/2025 16:16:30 INFO]: Saving model to: unhanged-Shanesha_trial_29/model_best.pth
[08/27/2025 16:16:38 INFO]: Training loss at epoch 18: 0.9316376447677612
[08/27/2025 16:16:45 INFO]: Training loss at epoch 28: 0.8623456954956055
[08/27/2025 16:16:54 INFO]: Training loss at epoch 22: 1.0853268504142761
[08/27/2025 16:17:21 INFO]: Training loss at epoch 13: 0.8891125619411469
[08/27/2025 16:17:26 INFO]: Training loss at epoch 18: 1.1508753299713135
[08/27/2025 16:17:47 INFO]: Training loss at epoch 9: 1.123795509338379
[08/27/2025 16:17:49 INFO]: Training loss at epoch 22: 1.0153525471687317
[08/27/2025 16:17:50 INFO]: Training loss at epoch 21: 0.9712252020835876
[08/27/2025 16:18:01 INFO]: Training loss at epoch 46: 1.136604905128479
[08/27/2025 16:18:35 INFO]: Training loss at epoch 8: 0.8628525733947754
[08/27/2025 16:18:37 INFO]: Training loss at epoch 16: 0.975618302822113
[08/27/2025 16:18:44 INFO]: Training stats: {
    "score": -0.9985472945114215,
    "rmse": 0.9985472945114215
}
[08/27/2025 16:18:44 INFO]: Val stats: {
    "score": -0.68807765142849,
    "rmse": 0.68807765142849
}
[08/27/2025 16:18:44 INFO]: Test stats: {
    "score": -0.879676350417102,
    "rmse": 0.879676350417102
}
[08/27/2025 16:18:52 INFO]: Training loss at epoch 6: 1.1835747361183167
[08/27/2025 16:19:09 INFO]: Training loss at epoch 1: 1.2777721881866455
[08/27/2025 16:19:21 INFO]: Training loss at epoch 23: 0.8957460820674896
[08/27/2025 16:19:37 INFO]: Training loss at epoch 4: 1.2903042435646057
[08/27/2025 16:19:41 INFO]: New best epoch, val score: -0.879922583738892
[08/27/2025 16:19:41 INFO]: Saving model to: unhanged-Shanesha_trial_30/model_best.pth
[08/27/2025 16:19:54 INFO]: Training loss at epoch 13: 1.0145362317562103
[08/27/2025 16:20:10 INFO]: Training loss at epoch 47: 1.1556088626384735
[08/27/2025 16:20:11 INFO]: Training loss at epoch 29: 0.8693710267543793
[08/27/2025 16:20:12 INFO]: Training loss at epoch 20: 1.3145486116409302
[08/27/2025 16:20:27 INFO]: Running Final Evaluation...
[08/27/2025 16:20:31 INFO]: Training loss at epoch 20: 1.0953211188316345
[08/27/2025 16:20:46 INFO]: Training loss at epoch 19: 0.9010380208492279
[08/27/2025 16:20:53 INFO]: Training loss at epoch 24: 0.8518842458724976
[08/27/2025 16:21:11 INFO]: Training loss at epoch 23: 1.0894064605236053
[08/27/2025 16:21:20 INFO]: Training accuracy: {
    "score": -1.0080564716780946,
    "rmse": 1.0080564716780946
}
[08/27/2025 16:21:20 INFO]: Val accuracy: {
    "score": -0.6621485125825881,
    "rmse": 0.6621485125825881
}
[08/27/2025 16:21:20 INFO]: Test accuracy: {
    "score": -0.8751420339165753,
    "rmse": 0.8751420339165753
}
[08/27/2025 16:21:20 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_15",
    "best_epoch": 16,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8751420339165753,
        "rmse": 0.8751420339165753
    },
    "train_stats": {
        "score": -1.0080564716780946,
        "rmse": 1.0080564716780946
    },
    "val_stats": {
        "score": -0.6621485125825881,
        "rmse": 0.6621485125825881
    }
}
[08/27/2025 16:21:20 INFO]: Procewss finished for trial unhanged-Shanesha_trial_15
[08/27/2025 16:21:20 INFO]: 
_________________________________________________

[08/27/2025 16:21:20 INFO]: train_net_for_optune.py main() running.
[08/27/2025 16:21:20 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 2.5284661651978313
  attention_dropout: 0.188314439209544
  ffn_dropout: 0.188314439209544
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 7.35378599901534e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_31

[08/27/2025 16:21:20 INFO]: This ft_transformer has 3.663 million parameters.
[08/27/2025 16:21:20 INFO]: Training will start at epoch 0.
[08/27/2025 16:21:20 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 16:21:23 INFO]: Training stats: {
    "score": -0.9981000064687432,
    "rmse": 0.9981000064687432
}
[08/27/2025 16:21:23 INFO]: Val stats: {
    "score": -0.701709481137131,
    "rmse": 0.701709481137131
}
[08/27/2025 16:21:23 INFO]: Test stats: {
    "score": -0.8834324286626344,
    "rmse": 0.8834324286626344
}
[08/27/2025 16:21:25 INFO]: Training loss at epoch 10: 0.9337826073169708
[08/27/2025 16:21:35 INFO]: Training loss at epoch 4: 1.1004132628440857
[08/27/2025 16:21:54 INFO]: Training loss at epoch 19: 0.8678776323795319
[08/27/2025 16:22:21 INFO]: Training loss at epoch 22: 0.8629878163337708
[08/27/2025 16:22:23 INFO]: Training loss at epoch 25: 1.0733093917369843
[08/27/2025 16:22:32 INFO]: Training stats: {
    "score": -0.9976457720789095,
    "rmse": 0.9976457720789095
}
[08/27/2025 16:22:32 INFO]: Val stats: {
    "score": -0.6954073841997437,
    "rmse": 0.6954073841997437
}
[08/27/2025 16:22:32 INFO]: Test stats: {
    "score": -0.8805466608461182,
    "rmse": 0.8805466608461182
}
[08/27/2025 16:22:35 INFO]: New best epoch, val score: -0.6639074138799101
[08/27/2025 16:22:35 INFO]: Saving model to: unhanged-Shanesha_trial_26/model_best.pth
[08/27/2025 16:22:43 INFO]: Training loss at epoch 19: 1.030184805393219
[08/27/2025 16:23:07 INFO]: Training loss at epoch 5: 1.031306505203247
[08/27/2025 16:23:14 INFO]: Training loss at epoch 14: 0.9933411180973053
[08/27/2025 16:23:40 INFO]: Training loss at epoch 2: 1.835937738418579
[08/27/2025 16:23:44 INFO]: Training stats: {
    "score": -1.0178821978494579,
    "rmse": 1.0178821978494579
}
[08/27/2025 16:23:44 INFO]: Val stats: {
    "score": -0.6626201424610468,
    "rmse": 0.6626201424610468
}
[08/27/2025 16:23:44 INFO]: Test stats: {
    "score": -0.8665380197582008,
    "rmse": 0.8665380197582008
}
[08/27/2025 16:23:55 INFO]: Training loss at epoch 26: 0.9330122172832489
[08/27/2025 16:24:04 INFO]: Training loss at epoch 0: 1.4330103993415833
[08/27/2025 16:24:07 INFO]: Training loss at epoch 11: 1.087231069803238
[08/27/2025 16:24:26 INFO]: New best epoch, val score: -0.750937870997801
[08/27/2025 16:24:26 INFO]: Saving model to: unhanged-Shanesha_trial_31/model_best.pth
[08/27/2025 16:24:30 INFO]: Training loss at epoch 14: 1.1615375578403473
[08/27/2025 16:24:32 INFO]: Training stats: {
    "score": -0.9963647223082317,
    "rmse": 0.9963647223082317
}
[08/27/2025 16:24:32 INFO]: Val stats: {
    "score": -0.6770067381474746,
    "rmse": 0.6770067381474746
}
[08/27/2025 16:24:32 INFO]: Test stats: {
    "score": -0.8690538026107149,
    "rmse": 0.8690538026107149
}
[08/27/2025 16:24:33 INFO]: Training loss at epoch 17: 1.2198506593704224
[08/27/2025 16:24:48 INFO]: Training loss at epoch 30: 0.9000391662120819
[08/27/2025 16:25:02 INFO]: Training loss at epoch 21: 1.3612666726112366
[08/27/2025 16:25:21 INFO]: New best epoch, val score: -0.6630251515246026
[08/27/2025 16:25:21 INFO]: Saving model to: unhanged-Shanesha_trial_20/model_best.pth
[08/27/2025 16:25:32 INFO]: Training loss at epoch 21: 0.8945982754230499
[08/27/2025 16:25:37 INFO]: Training loss at epoch 27: 0.9880783259868622
[08/27/2025 16:25:38 INFO]: Training loss at epoch 24: 0.8797291219234467
[08/27/2025 16:25:40 INFO]: Training loss at epoch 14: 1.1059832572937012
[08/27/2025 16:26:08 INFO]: New best epoch, val score: -0.6607157995662696
[08/27/2025 16:26:08 INFO]: Saving model to: unhanged-Shanesha_trial_2/model_best.pth
[08/27/2025 16:26:47 INFO]: Training loss at epoch 6: 1.0956558585166931
[08/27/2025 16:26:58 INFO]: Training loss at epoch 12: 0.9525499045848846
[08/27/2025 16:27:03 INFO]: Training loss at epoch 23: 1.1443392634391785
[08/27/2025 16:27:10 INFO]: Training loss at epoch 28: 0.7100393772125244
[08/27/2025 16:27:18 INFO]: New best epoch, val score: -0.6660893486722619
[08/27/2025 16:27:18 INFO]: Saving model to: unhanged-Shanesha_trial_28/model_best.pth
[08/27/2025 16:27:21 INFO]: Training loss at epoch 1: 0.8658270835876465
[08/27/2025 16:27:53 INFO]: Training loss at epoch 20: 1.0599527955055237
[08/27/2025 16:28:18 INFO]: Training loss at epoch 3: 1.7718455791473389
[08/27/2025 16:28:23 INFO]: Training loss at epoch 31: 0.876834362745285
[08/27/2025 16:28:40 INFO]: Training loss at epoch 29: 0.872788816690445
[08/27/2025 16:28:50 INFO]: New best epoch, val score: -0.705058296894288
[08/27/2025 16:28:50 INFO]: Saving model to: unhanged-Shanesha_trial_30/model_best.pth
[08/27/2025 16:29:07 INFO]: Training loss at epoch 20: 0.9137011170387268
[08/27/2025 16:29:12 INFO]: Training stats: {
    "score": -0.9891174142157154,
    "rmse": 0.9891174142157154
}
[08/27/2025 16:29:12 INFO]: Val stats: {
    "score": -0.6808730114925262,
    "rmse": 0.6808730114925262
}
[08/27/2025 16:29:12 INFO]: Test stats: {
    "score": -0.8768290836223597,
    "rmse": 0.8768290836223597
}
[08/27/2025 16:29:35 INFO]: Training loss at epoch 13: 1.2532143592834473
[08/27/2025 16:29:52 INFO]: Training loss at epoch 25: 1.0197563767433167
[08/27/2025 16:29:59 INFO]: Training loss at epoch 20: 1.2895904779434204
[08/27/2025 16:30:00 INFO]: Training loss at epoch 22: 1.172081172466278
[08/27/2025 16:30:03 INFO]: Training loss at epoch 5: 1.2456239461898804
[08/27/2025 16:30:14 INFO]: Training loss at epoch 7: 0.9884784519672394
[08/27/2025 16:30:15 INFO]: Training loss at epoch 9: 1.0390369296073914
[08/27/2025 16:30:20 INFO]: Training loss at epoch 7: 1.423463523387909
[08/27/2025 16:30:23 INFO]: Training loss at epoch 2: 1.3823710680007935
[08/27/2025 16:30:24 INFO]: Training loss at epoch 22: 0.9467661380767822
[08/27/2025 16:30:24 INFO]: Training loss at epoch 15: 0.8125830292701721
[08/27/2025 16:30:34 INFO]: New best epoch, val score: -0.666942392903078
[08/27/2025 16:30:34 INFO]: Saving model to: unhanged-Shanesha_trial_11/model_best.pth
[08/27/2025 16:30:39 INFO]: Training loss at epoch 18: 1.1298035085201263
[08/27/2025 16:30:42 INFO]: Training loss at epoch 30: 1.1397631764411926
[08/27/2025 16:31:03 INFO]: New best epoch, val score: -0.662565585538652
[08/27/2025 16:31:03 INFO]: Saving model to: unhanged-Shanesha_trial_26/model_best.pth
[08/27/2025 16:31:12 INFO]: Training loss at epoch 15: 0.9648277759552002
[08/27/2025 16:31:29 INFO]: Training loss at epoch 24: 0.9891141653060913
[08/27/2025 16:31:43 INFO]: Training loss at epoch 32: 1.029102623462677
[08/27/2025 16:31:45 INFO]: Training loss at epoch 15: 1.0431727170944214
[08/27/2025 16:31:50 INFO]: New best epoch, val score: -0.6607989814301645
[08/27/2025 16:31:50 INFO]: Saving model to: unhanged-Shanesha_trial_21/model_best.pth
[08/27/2025 16:32:11 INFO]: Running Final Evaluation...
[08/27/2025 16:32:16 INFO]: Training loss at epoch 31: 1.2159125208854675
[08/27/2025 16:32:16 INFO]: Training loss at epoch 14: 1.0070783495903015
[08/27/2025 16:32:36 INFO]: New best epoch, val score: -0.662008942383784
[08/27/2025 16:32:36 INFO]: Saving model to: unhanged-Shanesha_trial_20/model_best.pth
[08/27/2025 16:32:47 INFO]: Training loss at epoch 4: 1.2112042903900146
[08/27/2025 16:33:02 INFO]: Training loss at epoch 21: 1.033629834651947
[08/27/2025 16:33:29 INFO]: Training accuracy: {
    "score": -1.017594508220139,
    "rmse": 1.017594508220139
}
[08/27/2025 16:33:29 INFO]: Val accuracy: {
    "score": -0.6605919773462774,
    "rmse": 0.6605919773462774
}
[08/27/2025 16:33:29 INFO]: Test accuracy: {
    "score": -0.8707600372106724,
    "rmse": 0.8707600372106724
}
[08/27/2025 16:33:29 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_10",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8707600372106724,
        "rmse": 0.8707600372106724
    },
    "train_stats": {
        "score": -1.017594508220139,
        "rmse": 1.017594508220139
    },
    "val_stats": {
        "score": -0.6605919773462774,
        "rmse": 0.6605919773462774
    }
}
[08/27/2025 16:33:29 INFO]: Procewss finished for trial unhanged-Shanesha_trial_10
[08/27/2025 16:33:29 INFO]: 
_________________________________________________

[08/27/2025 16:33:29 INFO]: train_net_for_optune.py main() running.
[08/27/2025 16:33:29 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.596873012949027
  attention_dropout: 0.16431248780374266
  ffn_dropout: 0.16431248780374266
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 8.170831366208954e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_32

[08/27/2025 16:33:29 INFO]: This ft_transformer has 2.505 million parameters.
[08/27/2025 16:33:29 INFO]: Training will start at epoch 0.
[08/27/2025 16:33:29 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 16:33:31 INFO]: Training loss at epoch 3: 1.160407543182373
[08/27/2025 16:33:42 INFO]: Training loss at epoch 8: 1.2431916892528534
[08/27/2025 16:33:47 INFO]: Training loss at epoch 32: 1.13273686170578
[08/27/2025 16:33:52 INFO]: New best epoch, val score: -0.6821760520282459
[08/27/2025 16:33:52 INFO]: Saving model to: unhanged-Shanesha_trial_31/model_best.pth
[08/27/2025 16:34:03 INFO]: Training stats: {
    "score": -1.002366325759856,
    "rmse": 1.002366325759856
}
[08/27/2025 16:34:03 INFO]: Val stats: {
    "score": -0.6650916481493679,
    "rmse": 0.6650916481493679
}
[08/27/2025 16:34:03 INFO]: Test stats: {
    "score": -0.8688763901866265,
    "rmse": 0.8688763901866265
}
[08/27/2025 16:34:06 INFO]: Training loss at epoch 26: 1.0773437917232513
[08/27/2025 16:34:06 INFO]: New best epoch, val score: -0.6631472754279985
[08/27/2025 16:34:06 INFO]: Saving model to: unhanged-Shanesha_trial_29/model_best.pth
[08/27/2025 16:34:19 INFO]: Training loss at epoch 21: 0.8688443303108215
[08/27/2025 16:34:47 INFO]: Training loss at epoch 23: 0.8371099233627319
[08/27/2025 16:34:54 INFO]: Training loss at epoch 15: 0.9098837673664093
[08/27/2025 16:35:07 INFO]: Training loss at epoch 0: 1.0286139845848083
[08/27/2025 16:35:11 INFO]: Training loss at epoch 21: 0.8755191266536713
[08/27/2025 16:35:16 INFO]: Training loss at epoch 23: 1.0592842102050781
[08/27/2025 16:35:17 INFO]: Training loss at epoch 33: 0.9441869854927063
[08/27/2025 16:35:20 INFO]: New best epoch, val score: -0.6721371004455089
[08/27/2025 16:35:20 INFO]: Saving model to: unhanged-Shanesha_trial_32/model_best.pth
[08/27/2025 16:36:02 INFO]: Training loss at epoch 25: 1.0916158556938171
[08/27/2025 16:36:36 INFO]: Training loss at epoch 19: 0.9481965899467468
[08/27/2025 16:36:37 INFO]: Training loss at epoch 4: 1.2434982657432556
[08/27/2025 16:36:48 INFO]: Training loss at epoch 16: 0.8940743803977966
[08/27/2025 16:36:52 INFO]: Training loss at epoch 34: 1.0433411598205566
[08/27/2025 16:37:03 INFO]: Training loss at epoch 1: 1.1288761496543884
[08/27/2025 16:37:13 INFO]: Training loss at epoch 9: 1.044537901878357
[08/27/2025 16:37:21 INFO]: Training loss at epoch 5: 0.885604977607727
[08/27/2025 16:37:27 INFO]: Training loss at epoch 16: 0.9897233545780182
[08/27/2025 16:37:35 INFO]: Training loss at epoch 16: 0.8946392238140106
[08/27/2025 16:38:09 INFO]: Training loss at epoch 22: 0.9287026822566986
[08/27/2025 16:38:17 INFO]: New best epoch, val score: -0.6616708223401393
[08/27/2025 16:38:17 INFO]: Saving model to: unhanged-Shanesha_trial_18/model_best.pth
[08/27/2025 16:38:24 INFO]: Training loss at epoch 6: 1.1218581795692444
[08/27/2025 16:38:24 INFO]: Training loss at epoch 35: 0.8655067384243011
[08/27/2025 16:38:26 INFO]: Training loss at epoch 27: 0.9582100510597229
[08/27/2025 16:38:27 INFO]: Training stats: {
    "score": -1.0131423342126424,
    "rmse": 1.0131423342126424
}
[08/27/2025 16:38:27 INFO]: Val stats: {
    "score": -0.6625024448127595,
    "rmse": 0.6625024448127595
}
[08/27/2025 16:38:27 INFO]: Test stats: {
    "score": -0.8712835267207993,
    "rmse": 0.8712835267207993
}
[08/27/2025 16:38:42 INFO]: Training stats: {
    "score": -1.0109584585752176,
    "rmse": 1.0109584585752176
}
[08/27/2025 16:38:42 INFO]: Val stats: {
    "score": -0.7498446539908573,
    "rmse": 0.7498446539908573
}
[08/27/2025 16:38:42 INFO]: Test stats: {
    "score": -0.9236772657831473,
    "rmse": 0.9236772657831473
}
[08/27/2025 16:38:51 INFO]: New best epoch, val score: -0.6625024448127595
[08/27/2025 16:38:51 INFO]: Saving model to: unhanged-Shanesha_trial_29/model_best.pth
[08/27/2025 16:38:53 INFO]: Training loss at epoch 2: 0.9983336627483368
[08/27/2025 16:38:54 INFO]: Training loss at epoch 16: 0.9460726976394653
[08/27/2025 16:39:06 INFO]: New best epoch, val score: -0.6645973295300468
[08/27/2025 16:39:06 INFO]: Saving model to: unhanged-Shanesha_trial_32/model_best.pth
[08/27/2025 16:39:22 INFO]: New best epoch, val score: -0.6611421110270382
[08/27/2025 16:39:22 INFO]: Saving model to: unhanged-Shanesha_trial_26/model_best.pth
[08/27/2025 16:39:36 INFO]: Training loss at epoch 22: 1.0659546852111816
[08/27/2025 16:39:36 INFO]: Training loss at epoch 24: 1.0303296446800232
[08/27/2025 16:39:42 INFO]: Training loss at epoch 5: 1.1724148988723755
[08/27/2025 16:39:55 INFO]: Training loss at epoch 36: 0.9286981821060181
[08/27/2025 16:40:06 INFO]: Running Final Evaluation...
[08/27/2025 16:40:10 INFO]: Training loss at epoch 24: 0.9756911396980286
[08/27/2025 16:40:15 INFO]: Training loss at epoch 17: 0.8604558706283569
[08/27/2025 16:40:30 INFO]: Training loss at epoch 22: 1.1925318241119385
[08/27/2025 16:40:35 INFO]: Training loss at epoch 26: 0.8612886369228363
[08/27/2025 16:40:41 INFO]: Training accuracy: {
    "score": -1.0043536584255823,
    "rmse": 1.0043536584255823
}
[08/27/2025 16:40:41 INFO]: Val accuracy: {
    "score": -0.6648484830755069,
    "rmse": 0.6648484830755069
}
[08/27/2025 16:40:41 INFO]: Test accuracy: {
    "score": -0.8733283943286035,
    "rmse": 0.8733283943286035
}
[08/27/2025 16:40:41 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_27",
    "best_epoch": 5,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8733283943286035,
        "rmse": 0.8733283943286035
    },
    "train_stats": {
        "score": -1.0043536584255823,
        "rmse": 1.0043536584255823
    },
    "val_stats": {
        "score": -0.6648484830755069,
        "rmse": 0.6648484830755069
    }
}
[08/27/2025 16:40:41 INFO]: Procewss finished for trial unhanged-Shanesha_trial_27
[08/27/2025 16:40:41 INFO]: 
_________________________________________________

[08/27/2025 16:40:41 INFO]: train_net_for_optune.py main() running.
[08/27/2025 16:40:41 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.553554077430203
  attention_dropout: 0.00908852120519979
  ffn_dropout: 0.00908852120519979
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 6.99804701237124e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_33

[08/27/2025 16:40:41 INFO]: This ft_transformer has 2.483 million parameters.
[08/27/2025 16:40:41 INFO]: Training will start at epoch 0.
[08/27/2025 16:40:41 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 16:40:47 INFO]: Training loss at epoch 3: 1.0186803340911865
[08/27/2025 16:41:38 INFO]: Training loss at epoch 8: 1.31327623128891
[08/27/2025 16:41:53 INFO]: Training loss at epoch 6: 0.9379915297031403
[08/27/2025 16:41:55 INFO]: Training loss at epoch 10: 1.0744785070419312
[08/27/2025 16:42:17 INFO]: Training loss at epoch 0: 1.0730732679367065
[08/27/2025 16:42:20 INFO]: Training loss at epoch 17: 1.1091578006744385
[08/27/2025 16:42:31 INFO]: New best epoch, val score: -0.7110222893740419
[08/27/2025 16:42:31 INFO]: Saving model to: unhanged-Shanesha_trial_33/model_best.pth
[08/27/2025 16:42:39 INFO]: Training loss at epoch 4: 0.814505934715271
[08/27/2025 16:42:41 INFO]: Training loss at epoch 28: 0.8426762223243713
[08/27/2025 16:42:46 INFO]: Training loss at epoch 6: 1.2736051678657532
[08/27/2025 16:42:57 INFO]: Training loss at epoch 18: 0.8097691237926483
[08/27/2025 16:43:22 INFO]: Training loss at epoch 23: 1.1060625314712524
[08/27/2025 16:44:11 INFO]: Training loss at epoch 1: 1.2243322730064392
[08/27/2025 16:44:26 INFO]: New best epoch, val score: -0.6939569803876962
[08/27/2025 16:44:26 INFO]: Saving model to: unhanged-Shanesha_trial_33/model_best.pth
[08/27/2025 16:44:31 INFO]: Training loss at epoch 25: 1.0022506415843964
[08/27/2025 16:44:33 INFO]: Training loss at epoch 17: 0.8988378942012787
[08/27/2025 16:44:35 INFO]: Training loss at epoch 5: 0.7659882009029388
[08/27/2025 16:44:41 INFO]: Training loss at epoch 20: 1.0485605001449585
[08/27/2025 16:44:54 INFO]: Training loss at epoch 23: 1.3755114376544952
[08/27/2025 16:45:04 INFO]: Training loss at epoch 25: 1.2469383776187897
[08/27/2025 16:45:08 INFO]: Training loss at epoch 27: 0.9433368444442749
[08/27/2025 16:45:28 INFO]: Training loss at epoch 11: 0.9909683167934418
[08/27/2025 16:45:39 INFO]: Training loss at epoch 19: 1.056833654642105
[08/27/2025 16:45:43 INFO]: Training loss at epoch 10: 1.0636824369430542
[08/27/2025 16:45:50 INFO]: Training loss at epoch 23: 1.0548726618289948
[08/27/2025 16:45:55 INFO]: Training loss at epoch 7: 1.1016831994056702
[08/27/2025 16:46:04 INFO]: Training loss at epoch 2: 1.0027707517147064
[08/27/2025 16:46:06 INFO]: Training loss at epoch 17: 1.116983711719513
[08/27/2025 16:46:29 INFO]: New best epoch, val score: -0.6655376551226425
[08/27/2025 16:46:29 INFO]: Saving model to: unhanged-Shanesha_trial_11/model_best.pth
[08/27/2025 16:46:30 INFO]: Training loss at epoch 7: 0.949137806892395
[08/27/2025 16:46:30 INFO]: Training loss at epoch 6: 0.9056068360805511
[08/27/2025 16:46:39 INFO]: Training stats: {
    "score": -1.0003091240517843,
    "rmse": 1.0003091240517843
}
[08/27/2025 16:46:39 INFO]: Val stats: {
    "score": -0.7131477960035025,
    "rmse": 0.7131477960035025
}
[08/27/2025 16:46:39 INFO]: Test stats: {
    "score": -0.894822945971102,
    "rmse": 0.894822945971102
}
[08/27/2025 16:46:48 INFO]: Training loss at epoch 7: 0.903326153755188
[08/27/2025 16:47:01 INFO]: Training loss at epoch 29: 0.897304117679596
[08/27/2025 16:47:51 INFO]: New best epoch, val score: -0.6604202617512478
[08/27/2025 16:47:51 INFO]: Saving model to: unhanged-Shanesha_trial_26/model_best.pth
[08/27/2025 16:47:59 INFO]: Training loss at epoch 3: 0.8528409600257874
[08/27/2025 16:48:03 INFO]: Training loss at epoch 18: 0.7974240779876709
[08/27/2025 16:48:25 INFO]: Training loss at epoch 7: 1.2779698371887207
[08/27/2025 16:48:28 INFO]: Training stats: {
    "score": -0.968513915067796,
    "rmse": 0.968513915067796
}
[08/27/2025 16:48:28 INFO]: Val stats: {
    "score": -0.7330219089373979,
    "rmse": 0.7330219089373979
}
[08/27/2025 16:48:28 INFO]: Test stats: {
    "score": -0.9346279982461267,
    "rmse": 0.9346279982461267
}
[08/27/2025 16:48:36 INFO]: Training loss at epoch 24: 1.1871428489685059
[08/27/2025 16:49:00 INFO]: Training loss at epoch 8: 0.9178295433521271
[08/27/2025 16:49:01 INFO]: Training loss at epoch 12: 1.0604673027992249
[08/27/2025 16:49:20 INFO]: Training loss at epoch 20: 0.9696221053600311
[08/27/2025 16:49:23 INFO]: Training loss at epoch 26: 0.9502403140068054
[08/27/2025 16:49:37 INFO]: Training loss at epoch 28: 0.8991548717021942
[08/27/2025 16:49:45 INFO]: Training loss at epoch 4: 1.0280219316482544
[08/27/2025 16:49:55 INFO]: Training loss at epoch 26: 0.9952075779438019
[08/27/2025 16:49:56 INFO]: New best epoch, val score: -0.6606878032744552
[08/27/2025 16:49:56 INFO]: Saving model to: unhanged-Shanesha_trial_3/model_best.pth
[08/27/2025 16:49:59 INFO]: New best epoch, val score: -0.6694719629410729
[08/27/2025 16:49:59 INFO]: Saving model to: unhanged-Shanesha_trial_33/model_best.pth
[08/27/2025 16:50:07 INFO]: Training loss at epoch 24: 0.934219092130661
[08/27/2025 16:50:15 INFO]: Training loss at epoch 8: 1.1670122742652893
[08/27/2025 16:50:41 INFO]: Training loss at epoch 21: 1.0569644570350647
[08/27/2025 16:51:01 INFO]: Training loss at epoch 8: 1.090685784816742
[08/27/2025 16:51:06 INFO]: Training loss at epoch 24: 1.275787889957428
[08/27/2025 16:51:34 INFO]: Training loss at epoch 18: 0.861635148525238
[08/27/2025 16:51:37 INFO]: Training loss at epoch 5: 1.0791247487068176
[08/27/2025 16:51:51 INFO]: New best epoch, val score: -0.6692236806918918
[08/27/2025 16:51:51 INFO]: Saving model to: unhanged-Shanesha_trial_33/model_best.pth
[08/27/2025 16:52:00 INFO]: Training loss at epoch 21: 0.9619060456752777
[08/27/2025 16:52:01 INFO]: Training loss at epoch 9: 1.0769165754318237
[08/27/2025 16:52:06 INFO]: Training loss at epoch 9: 1.010425865650177
[08/27/2025 16:52:27 INFO]: Training loss at epoch 13: 0.8576803505420685
[08/27/2025 16:52:37 INFO]: Training loss at epoch 30: 0.9300741255283356
[08/27/2025 16:52:44 INFO]: Training stats: {
    "score": -1.0008421388698638,
    "rmse": 1.0008421388698638
}
[08/27/2025 16:52:44 INFO]: Val stats: {
    "score": -0.7044758234986414,
    "rmse": 0.7044758234986414
}
[08/27/2025 16:52:44 INFO]: Test stats: {
    "score": -0.8857173641031949,
    "rmse": 0.8857173641031949
}
[08/27/2025 16:52:58 INFO]: Training loss at epoch 9: 1.2178041338920593
[08/27/2025 16:53:05 INFO]: Training stats: {
    "score": -1.063364045736857,
    "rmse": 1.063364045736857
}
[08/27/2025 16:53:05 INFO]: Val stats: {
    "score": -0.6920991617084936,
    "rmse": 0.6920991617084936
}
[08/27/2025 16:53:05 INFO]: Test stats: {
    "score": -0.9057002408692709,
    "rmse": 0.9057002408692709
}
[08/27/2025 16:53:10 INFO]: Training loss at epoch 18: 1.1802715063095093
[08/27/2025 16:53:27 INFO]: Training loss at epoch 6: 0.7753287851810455
[08/27/2025 16:53:29 INFO]: Training loss at epoch 19: 0.9908272624015808
[08/27/2025 16:53:40 INFO]: Training loss at epoch 25: 1.0420742630958557
[08/27/2025 16:54:00 INFO]: New best epoch, val score: -0.6618032312583098
[08/27/2025 16:54:00 INFO]: Saving model to: unhanged-Shanesha_trial_20/model_best.pth
[08/27/2025 16:54:05 INFO]: Training loss at epoch 29: 1.2480034828186035
[08/27/2025 16:54:11 INFO]: Training loss at epoch 27: 0.9217256903648376
[08/27/2025 16:54:38 INFO]: Training loss at epoch 10: 1.1278717517852783
[08/27/2025 16:54:41 INFO]: Training loss at epoch 22: 1.176314353942871
[08/27/2025 16:54:45 INFO]: Training loss at epoch 27: 1.0299983024597168
[08/27/2025 16:54:52 INFO]: New best epoch, val score: -0.6632291095931424
[08/27/2025 16:54:52 INFO]: Saving model to: unhanged-Shanesha_trial_32/model_best.pth
[08/27/2025 16:55:02 INFO]: Training loss at epoch 8: 1.218782514333725
[08/27/2025 16:55:21 INFO]: Training loss at epoch 7: 0.9281720519065857
[08/27/2025 16:55:22 INFO]: Training loss at epoch 25: 0.9143029153347015
[08/27/2025 16:55:26 INFO]: Training stats: {
    "score": -1.0281404092516002,
    "rmse": 1.0281404092516002
}
[08/27/2025 16:55:26 INFO]: Val stats: {
    "score": -0.7758820039236888,
    "rmse": 0.7758820039236888
}
[08/27/2025 16:55:26 INFO]: Test stats: {
    "score": -0.9336269185044158,
    "rmse": 0.9336269185044158
}
[08/27/2025 16:55:29 INFO]: Training loss at epoch 9: 1.3627228736877441
[08/27/2025 16:55:39 INFO]: Training stats: {
    "score": -0.9984195722789987,
    "rmse": 0.9984195722789987
}
[08/27/2025 16:55:39 INFO]: Val stats: {
    "score": -0.6943839401374748,
    "rmse": 0.6943839401374748
}
[08/27/2025 16:55:39 INFO]: Test stats: {
    "score": -0.8823369858999636,
    "rmse": 0.8823369858999636
}
[08/27/2025 16:55:59 INFO]: Training loss at epoch 14: 1.0017104744911194
[08/27/2025 16:56:12 INFO]: Training loss at epoch 10: 0.8704024851322174
[08/27/2025 16:56:20 INFO]: Training loss at epoch 25: 1.3116416931152344
[08/27/2025 16:56:32 INFO]: Training loss at epoch 11: 0.8686237633228302
[08/27/2025 16:56:34 INFO]: Training loss at epoch 22: 1.0358750224113464
[08/27/2025 16:56:34 INFO]: New best epoch, val score: -0.6649109928758975
[08/27/2025 16:56:34 INFO]: Saving model to: unhanged-Shanesha_trial_31/model_best.pth
[08/27/2025 16:56:55 INFO]: Training loss at epoch 31: 0.9624457359313965
[08/27/2025 16:56:55 INFO]: Training stats: {
    "score": -1.135773449526151,
    "rmse": 1.135773449526151
}
[08/27/2025 16:56:55 INFO]: Val stats: {
    "score": -0.7645413188785636,
    "rmse": 0.7645413188785636
}
[08/27/2025 16:56:55 INFO]: Test stats: {
    "score": -0.9750025114885441,
    "rmse": 0.9750025114885441
}
[08/27/2025 16:57:04 INFO]: Training stats: {
    "score": -1.0224399755567195,
    "rmse": 1.0224399755567195
}
[08/27/2025 16:57:04 INFO]: Val stats: {
    "score": -0.7648172111309022,
    "rmse": 0.7648172111309022
}
[08/27/2025 16:57:04 INFO]: Test stats: {
    "score": -0.9238848486713097,
    "rmse": 0.9238848486713097
}
[08/27/2025 16:57:14 INFO]: Training loss at epoch 8: 0.8394223749637604
[08/27/2025 16:57:16 INFO]: Training loss at epoch 11: 1.2233397960662842
[08/27/2025 16:57:22 INFO]: Training loss at epoch 23: 1.104078084230423
[08/27/2025 16:58:25 INFO]: Training loss at epoch 12: 1.1829628944396973
[08/27/2025 16:58:35 INFO]: Training loss at epoch 19: 0.955333411693573
[08/27/2025 16:58:50 INFO]: Training loss at epoch 26: 0.8407144546508789
[08/27/2025 16:59:03 INFO]: Training loss at epoch 28: 1.1992022395133972
[08/27/2025 16:59:05 INFO]: Training loss at epoch 9: 0.9611140787601471
[08/27/2025 16:59:19 INFO]: Training loss at epoch 11: 0.8615568280220032
[08/27/2025 16:59:29 INFO]: Training loss at epoch 15: 1.130222499370575
[08/27/2025 16:59:38 INFO]: Training loss at epoch 28: 0.8721325993537903
[08/27/2025 16:59:45 INFO]: Training stats: {
    "score": -0.9966637445157354,
    "rmse": 0.9966637445157354
}
[08/27/2025 16:59:45 INFO]: Val stats: {
    "score": -0.6723720909090972,
    "rmse": 0.6723720909090972
}
[08/27/2025 16:59:45 INFO]: Test stats: {
    "score": -0.8722369982965118,
    "rmse": 0.8722369982965118
}
[08/27/2025 17:00:04 INFO]: Training loss at epoch 24: 1.0992302894592285
[08/27/2025 17:00:09 INFO]: Training loss at epoch 30: 1.0444934368133545
[08/27/2025 17:00:19 INFO]: Training loss at epoch 13: 1.0197566151618958
[08/27/2025 17:00:19 INFO]: Training loss at epoch 19: 1.0475892126560211
[08/27/2025 17:00:36 INFO]: Training loss at epoch 26: 1.083315908908844
[08/27/2025 17:00:59 INFO]: Training stats: {
    "score": -1.000220286300533,
    "rmse": 1.000220286300533
}
[08/27/2025 17:00:59 INFO]: Val stats: {
    "score": -0.6896998632198148,
    "rmse": 0.6896998632198148
}
[08/27/2025 17:00:59 INFO]: Test stats: {
    "score": -0.8787895844791933,
    "rmse": 0.8787895844791933
}
[08/27/2025 17:01:01 INFO]: Training loss at epoch 20: 1.1931986212730408
[08/27/2025 17:01:12 INFO]: Training loss at epoch 32: 1.028904676437378
[08/27/2025 17:01:36 INFO]: Training loss at epoch 10: 1.001084327697754
[08/27/2025 17:01:38 INFO]: Training loss at epoch 10: 0.9583982527256012
[08/27/2025 17:01:38 INFO]: Training loss at epoch 26: 0.8618132472038269
[08/27/2025 17:02:10 INFO]: New best epoch, val score: -0.7025030855660604
[08/27/2025 17:02:10 INFO]: Saving model to: unhanged-Shanesha_trial_30/model_best.pth
[08/27/2025 17:02:15 INFO]: Training loss at epoch 14: 1.0662423372268677
[08/27/2025 17:02:28 INFO]: Training loss at epoch 12: 1.298429250717163
[08/27/2025 17:02:33 INFO]: Training loss at epoch 23: 0.8852352499961853
[08/27/2025 17:02:44 INFO]: Training stats: {
    "score": -0.9912451799403119,
    "rmse": 0.9912451799403119
}
[08/27/2025 17:02:44 INFO]: Val stats: {
    "score": -0.6764565745034953,
    "rmse": 0.6764565745034953
}
[08/27/2025 17:02:44 INFO]: Test stats: {
    "score": -0.8754008576355248,
    "rmse": 0.8754008576355248
}
[08/27/2025 17:02:45 INFO]: Training loss at epoch 25: 1.1060749292373657
[08/27/2025 17:02:59 INFO]: Training loss at epoch 16: 0.9850921928882599
[08/27/2025 17:03:22 INFO]: Training loss at epoch 9: 1.0010649859905243
[08/27/2025 17:03:29 INFO]: Training loss at epoch 11: 1.0541590452194214
[08/27/2025 17:03:52 INFO]: Training loss at epoch 29: 0.839590311050415
[08/27/2025 17:04:01 INFO]: Training loss at epoch 27: 1.137368381023407
[08/27/2025 17:04:07 INFO]: Training loss at epoch 15: 0.8923374712467194
[08/27/2025 17:04:28 INFO]: Training loss at epoch 29: 0.8681627213954926
[08/27/2025 17:04:40 INFO]: Training loss at epoch 31: 0.992417722940445
[08/27/2025 17:05:24 INFO]: Training loss at epoch 12: 1.1723078489303589
[08/27/2025 17:05:27 INFO]: Training loss at epoch 26: 0.9499683976173401
[08/27/2025 17:05:29 INFO]: Training loss at epoch 33: 0.7908580005168915
[08/27/2025 17:05:31 INFO]: Training loss at epoch 13: 1.0674232244491577
[08/27/2025 17:05:32 INFO]: Training stats: {
    "score": -1.0000107690036628,
    "rmse": 1.0000107690036628
}
[08/27/2025 17:05:32 INFO]: Val stats: {
    "score": -0.6885435460770615,
    "rmse": 0.6885435460770615
}
[08/27/2025 17:05:32 INFO]: Test stats: {
    "score": -0.877968010989415,
    "rmse": 0.877968010989415
}
[08/27/2025 17:05:52 INFO]: Training loss at epoch 27: 0.9740328788757324
[08/27/2025 17:06:00 INFO]: Training loss at epoch 16: 1.1620869636535645
[08/27/2025 17:06:05 INFO]: Training stats: {
    "score": -1.00005654651936,
    "rmse": 1.00005654651936
}
[08/27/2025 17:06:05 INFO]: Val stats: {
    "score": -0.6857027435658852,
    "rmse": 0.6857027435658852
}
[08/27/2025 17:06:05 INFO]: Test stats: {
    "score": -0.8767339300917991,
    "rmse": 0.8767339300917991
}
[08/27/2025 17:06:06 INFO]: Training loss at epoch 11: 1.1253998279571533
[08/27/2025 17:06:17 INFO]: Training stats: {
    "score": -1.0053971077339605,
    "rmse": 1.0053971077339605
}
[08/27/2025 17:06:17 INFO]: Val stats: {
    "score": -0.6630720594759206,
    "rmse": 0.6630720594759206
}
[08/27/2025 17:06:17 INFO]: Test stats: {
    "score": -0.8685656375974391,
    "rmse": 0.8685656375974391
}
[08/27/2025 17:06:29 INFO]: Training loss at epoch 17: 1.007712334394455
[08/27/2025 17:06:35 INFO]: Training loss at epoch 21: 0.9810666143894196
[08/27/2025 17:06:41 INFO]: New best epoch, val score: -0.6707682616223036
[08/27/2025 17:06:41 INFO]: Saving model to: unhanged-Shanesha_trial_30/model_best.pth
[08/27/2025 17:06:54 INFO]: Training loss at epoch 27: 0.9777854681015015
[08/27/2025 17:06:56 INFO]: New best epoch, val score: -0.6616450157744782
[08/27/2025 17:06:56 INFO]: Saving model to: unhanged-Shanesha_trial_29/model_best.pth
[08/27/2025 17:07:18 INFO]: Training loss at epoch 13: 0.882880300283432
[08/27/2025 17:07:57 INFO]: Training loss at epoch 17: 1.1449512839317322
[08/27/2025 17:08:05 INFO]: Training loss at epoch 20: 0.8785708546638489
[08/27/2025 17:08:10 INFO]: Training loss at epoch 27: 1.4435672461986542
[08/27/2025 17:08:20 INFO]: Training loss at epoch 10: 1.3929781913757324
[08/27/2025 17:08:31 INFO]: Training loss at epoch 24: 1.0388357043266296
[08/27/2025 17:08:38 INFO]: Training loss at epoch 14: 0.9733839929103851
[08/27/2025 17:08:54 INFO]: Training loss at epoch 12: 1.2402541041374207
[08/27/2025 17:09:10 INFO]: Training loss at epoch 14: 1.0130316019058228
[08/27/2025 17:09:14 INFO]: Training loss at epoch 32: 0.9724273383617401
[08/27/2025 17:09:17 INFO]: New best epoch, val score: -0.6672770563582879
[08/27/2025 17:09:17 INFO]: Saving model to: unhanged-Shanesha_trial_16/model_best.pth
[08/27/2025 17:09:18 INFO]: Training loss at epoch 28: 1.0066891312599182
[08/27/2025 17:09:50 INFO]: Training loss at epoch 34: 0.8104121088981628
[08/27/2025 17:09:52 INFO]: Training loss at epoch 18: 1.0356040298938751
[08/27/2025 17:09:58 INFO]: Training loss at epoch 20: 1.262020468711853
[08/27/2025 17:10:05 INFO]: Training loss at epoch 18: 0.802278071641922
[08/27/2025 17:10:21 INFO]: Running Final Evaluation...
[08/27/2025 17:10:29 INFO]: Training loss at epoch 30: 1.0029560327529907
[08/27/2025 17:10:45 INFO]: Training loss at epoch 12: 0.9096183776855469
[08/27/2025 17:10:53 INFO]: Training loss at epoch 28: 1.249104082584381
[08/27/2025 17:11:03 INFO]: Training loss at epoch 30: 1.18961763381958
[08/27/2025 17:11:04 INFO]: Training loss at epoch 15: 0.9636060893535614
[08/27/2025 17:11:14 INFO]: Training loss at epoch 28: 1.0703901052474976
[08/27/2025 17:11:16 INFO]: New best epoch, val score: -0.6620882705178937
[08/27/2025 17:11:16 INFO]: Saving model to: unhanged-Shanesha_trial_30/model_best.pth
[08/27/2025 17:11:43 INFO]: Training loss at epoch 19: 1.1026636958122253
[08/27/2025 17:11:45 INFO]: Training loss at epoch 15: 0.9532711207866669
[08/27/2025 17:11:49 INFO]: Training accuracy: {
    "score": -0.9975736958323025,
    "rmse": 0.9975736958323025
}
[08/27/2025 17:11:49 INFO]: Val accuracy: {
    "score": -0.6618876095312907,
    "rmse": 0.6618876095312907
}
[08/27/2025 17:11:49 INFO]: Test accuracy: {
    "score": -0.8829422124890303,
    "rmse": 0.8829422124890303
}
[08/27/2025 17:11:49 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_14",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8829422124890303,
        "rmse": 0.8829422124890303
    },
    "train_stats": {
        "score": -0.9975736958323025,
        "rmse": 0.9975736958323025
    },
    "val_stats": {
        "score": -0.6618876095312907,
        "rmse": 0.6618876095312907
    }
}
[08/27/2025 17:11:49 INFO]: Procewss finished for trial unhanged-Shanesha_trial_14
[08/27/2025 17:11:49 INFO]: 
_________________________________________________

[08/27/2025 17:11:49 INFO]: train_net_for_optune.py main() running.
[08/27/2025 17:11:49 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.5615505914313643
  attention_dropout: 0.004447414875412614
  ffn_dropout: 0.004447414875412614
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.000252547804775551
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_34

[08/27/2025 17:11:49 INFO]: This ft_transformer has 2.487 million parameters.
[08/27/2025 17:11:49 INFO]: Training will start at epoch 0.
[08/27/2025 17:11:49 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 17:12:14 INFO]: Training loss at epoch 28: 1.0088117718696594
[08/27/2025 17:12:18 INFO]: Training loss at epoch 22: 1.0966068506240845
[08/27/2025 17:12:23 INFO]: Training stats: {
    "score": -1.0010653803978904,
    "rmse": 1.0010653803978904
}
[08/27/2025 17:12:23 INFO]: Val stats: {
    "score": -0.6678199116153046,
    "rmse": 0.6678199116153046
}
[08/27/2025 17:12:23 INFO]: Test stats: {
    "score": -0.8689022552903971,
    "rmse": 0.8689022552903971
}
[08/27/2025 17:12:55 INFO]: Training loss at epoch 16: 0.9139717519283295
[08/27/2025 17:13:28 INFO]: Training loss at epoch 0: 1.85385262966156
[08/27/2025 17:13:31 INFO]: Training loss at epoch 29: 1.0495260655879974
[08/27/2025 17:13:33 INFO]: Training loss at epoch 19: 1.0346927642822266
[08/27/2025 17:13:41 INFO]: New best epoch, val score: -1.069328853892529
[08/27/2025 17:13:41 INFO]: Saving model to: unhanged-Shanesha_trial_34/model_best.pth
[08/27/2025 17:13:44 INFO]: Training loss at epoch 33: 0.9986773133277893
[08/27/2025 17:14:14 INFO]: Training loss at epoch 20: 0.9926940500736237
[08/27/2025 17:14:26 INFO]: Training stats: {
    "score": -0.9915827924415583,
    "rmse": 0.9915827924415583
}
[08/27/2025 17:14:26 INFO]: Val stats: {
    "score": -0.6797666403478284,
    "rmse": 0.6797666403478284
}
[08/27/2025 17:14:26 INFO]: Test stats: {
    "score": -0.8775764065093591,
    "rmse": 0.8775764065093591
}
[08/27/2025 17:14:28 INFO]: Training loss at epoch 25: 1.0311152935028076
[08/27/2025 17:14:28 INFO]: Training loss at epoch 29: 1.0560176968574524
[08/27/2025 17:14:37 INFO]: Training loss at epoch 10: 0.9113507270812988
[08/27/2025 17:14:42 INFO]: Training loss at epoch 17: 1.0071660280227661
[08/27/2025 17:14:44 INFO]: Training stats: {
    "score": -1.0134646992922496,
    "rmse": 1.0134646992922496
}
[08/27/2025 17:14:44 INFO]: Val stats: {
    "score": -0.661507719921587,
    "rmse": 0.661507719921587
}
[08/27/2025 17:14:44 INFO]: Test stats: {
    "score": -0.8729968313589651,
    "rmse": 0.8729968313589651
}
[08/27/2025 17:14:50 INFO]: Training loss at epoch 16: 1.2023560404777527
[08/27/2025 17:15:07 INFO]: Training loss at epoch 21: 1.0824098587036133
[08/27/2025 17:15:11 INFO]: New best epoch, val score: -0.661507719921587
[08/27/2025 17:15:11 INFO]: Saving model to: unhanged-Shanesha_trial_29/model_best.pth
[08/27/2025 17:15:12 INFO]: Training loss at epoch 13: 0.8559352457523346
[08/27/2025 17:15:14 INFO]: Training loss at epoch 31: 1.1174467504024506
[08/27/2025 17:15:20 INFO]: Training loss at epoch 1: 1.601915955543518
[08/27/2025 17:15:35 INFO]: New best epoch, val score: -1.017051514673781
[08/27/2025 17:15:35 INFO]: Saving model to: unhanged-Shanesha_trial_34/model_best.pth
[08/27/2025 17:15:46 INFO]: New best epoch, val score: -0.6616821104308296
[08/27/2025 17:15:46 INFO]: Saving model to: unhanged-Shanesha_trial_30/model_best.pth
[08/27/2025 17:15:52 INFO]: Training loss at epoch 31: 1.0616534650325775
[08/27/2025 17:16:09 INFO]: Training loss at epoch 21: 0.792059451341629
[08/27/2025 17:16:21 INFO]: Training stats: {
    "score": -1.0023812833623686,
    "rmse": 1.0023812833623686
}
[08/27/2025 17:16:21 INFO]: Val stats: {
    "score": -0.6637739079141806,
    "rmse": 0.6637739079141806
}
[08/27/2025 17:16:21 INFO]: Test stats: {
    "score": -0.8689538686656628,
    "rmse": 0.8689538686656628
}
[08/27/2025 17:16:30 INFO]: Training loss at epoch 29: 1.1105596423149109
[08/27/2025 17:16:36 INFO]: Training loss at epoch 18: 0.8296105265617371
[08/27/2025 17:17:02 INFO]: Training loss at epoch 21: 1.0201032757759094
[08/27/2025 17:17:08 INFO]: Training loss at epoch 30: 0.8320244252681732
[08/27/2025 17:17:13 INFO]: Training loss at epoch 2: 1.8269252181053162
[08/27/2025 17:17:29 INFO]: New best epoch, val score: -0.6695480159855416
[08/27/2025 17:17:29 INFO]: Saving model to: unhanged-Shanesha_trial_34/model_best.pth
[08/27/2025 17:17:29 INFO]: Training loss at epoch 29: 1.2672244608402252
[08/27/2025 17:17:54 INFO]: Training loss at epoch 23: 1.0193872451782227
[08/27/2025 17:18:00 INFO]: Training loss at epoch 17: 1.0687390565872192
[08/27/2025 17:18:03 INFO]: Training loss at epoch 22: 1.0081037282943726
[08/27/2025 17:18:15 INFO]: Training loss at epoch 34: 0.8535929322242737
[08/27/2025 17:18:18 INFO]: Training loss at epoch 20: 0.845232218503952
[08/27/2025 17:18:18 INFO]: Training stats: {
    "score": -1.0027104923752,
    "rmse": 1.0027104923752
}
[08/27/2025 17:18:18 INFO]: Val stats: {
    "score": -0.6649816554769411,
    "rmse": 0.6649816554769411
}
[08/27/2025 17:18:18 INFO]: Test stats: {
    "score": -0.8620301673108819,
    "rmse": 0.8620301673108819
}
[08/27/2025 17:18:27 INFO]: Training loss at epoch 19: 1.0048874914646149
[08/27/2025 17:19:07 INFO]: Training stats: {
    "score": -0.994808534214776,
    "rmse": 0.994808534214776
}
[08/27/2025 17:19:07 INFO]: Val stats: {
    "score": -0.7032093463504572,
    "rmse": 0.7032093463504572
}
[08/27/2025 17:19:07 INFO]: Test stats: {
    "score": -0.8871224916348536,
    "rmse": 0.8871224916348536
}
[08/27/2025 17:19:08 INFO]: Training loss at epoch 3: 1.227363407611847
[08/27/2025 17:19:17 INFO]: Training stats: {
    "score": -1.0137882977487098,
    "rmse": 1.0137882977487098
}
[08/27/2025 17:19:17 INFO]: Val stats: {
    "score": -0.7563972328462502,
    "rmse": 0.7563972328462502
}
[08/27/2025 17:19:17 INFO]: Test stats: {
    "score": -0.9185947303527929,
    "rmse": 0.9185947303527929
}
[08/27/2025 17:19:43 INFO]: Training loss at epoch 11: 1.0423988103866577
[08/27/2025 17:19:45 INFO]: Training loss at epoch 14: 0.8901472389698029
[08/27/2025 17:19:51 INFO]: Training loss at epoch 31: 0.9453423917293549
[08/27/2025 17:19:56 INFO]: Training loss at epoch 23: 0.8673820793628693
[08/27/2025 17:20:07 INFO]: Training loss at epoch 32: 0.8927686214447021
[08/27/2025 17:20:27 INFO]: Training loss at epoch 26: 0.9558795094490051
[08/27/2025 17:20:30 INFO]: Training loss at epoch 13: 1.055022418498993
[08/27/2025 17:20:45 INFO]: Training loss at epoch 32: 0.8459998965263367
[08/27/2025 17:20:59 INFO]: Training loss at epoch 20: 0.9436521530151367
[08/27/2025 17:21:01 INFO]: Training loss at epoch 4: 1.2776535153388977
[08/27/2025 17:21:04 INFO]: Training loss at epoch 18: 0.9635902643203735
[08/27/2025 17:21:30 INFO]: Training loss at epoch 30: 1.0492906272411346
[08/27/2025 17:21:48 INFO]: Training loss at epoch 21: 1.1746615171432495
[08/27/2025 17:21:49 INFO]: Training loss at epoch 24: 1.1547305881977081
[08/27/2025 17:22:13 INFO]: Training loss at epoch 22: 0.9503307938575745
[08/27/2025 17:22:32 INFO]: Training loss at epoch 32: 0.8810402452945709
[08/27/2025 17:22:46 INFO]: Training loss at epoch 35: 1.105928659439087
[08/27/2025 17:22:51 INFO]: Training loss at epoch 21: 0.9486702978610992
[08/27/2025 17:22:55 INFO]: Training loss at epoch 5: 0.9332299828529358
[08/27/2025 17:23:00 INFO]: Training loss at epoch 11: 1.0379993319511414
[08/27/2025 17:23:30 INFO]: Training loss at epoch 24: 1.224658727645874
[08/27/2025 17:23:36 INFO]: Training loss at epoch 30: 0.8981728851795197
[08/27/2025 17:23:44 INFO]: Training loss at epoch 25: 0.9053215086460114
[08/27/2025 17:24:10 INFO]: Training loss at epoch 19: 1.107008457183838
[08/27/2025 17:24:15 INFO]: Training loss at epoch 22: 1.1583307981491089
[08/27/2025 17:24:19 INFO]: Training loss at epoch 15: 1.260928452014923
[08/27/2025 17:24:37 INFO]: Training loss at epoch 30: 1.0002384185791016
[08/27/2025 17:24:45 INFO]: Training loss at epoch 22: 1.10121288895607
[08/27/2025 17:24:49 INFO]: Training loss at epoch 6: 1.0472049713134766
[08/27/2025 17:24:59 INFO]: Training loss at epoch 33: 1.0201503038406372
[08/27/2025 17:25:13 INFO]: Training loss at epoch 33: 0.806460440158844
[08/27/2025 17:25:14 INFO]: Training stats: {
    "score": -1.0062397420425524,
    "rmse": 1.0062397420425524
}
[08/27/2025 17:25:14 INFO]: Val stats: {
    "score": -0.6636978587024394,
    "rmse": 0.6636978587024394
}
[08/27/2025 17:25:14 INFO]: Test stats: {
    "score": -0.8674138088137558,
    "rmse": 0.8674138088137558
}
[08/27/2025 17:25:20 INFO]: Training loss at epoch 22: 0.8965655863285065
[08/27/2025 17:25:37 INFO]: Training loss at epoch 33: 1.0008091032505035
[08/27/2025 17:25:37 INFO]: Training loss at epoch 26: 0.8621473014354706
[08/27/2025 17:25:38 INFO]: New best epoch, val score: -0.6636978587024394
[08/27/2025 17:25:38 INFO]: Saving model to: unhanged-Shanesha_trial_31/model_best.pth
[08/27/2025 17:26:27 INFO]: Training loss at epoch 27: 1.1288875341415405
[08/27/2025 17:26:36 INFO]: Training loss at epoch 23: 0.917160838842392
[08/27/2025 17:26:44 INFO]: Training loss at epoch 7: 1.1211212277412415
[08/27/2025 17:26:44 INFO]: Training loss at epoch 31: 1.0393242835998535
[08/27/2025 17:26:49 INFO]: New best epoch, val score: -0.6659875742470356
[08/27/2025 17:26:49 INFO]: Saving model to: unhanged-Shanesha_trial_33/model_best.pth
[08/27/2025 17:27:17 INFO]: Training loss at epoch 36: 1.1017398834228516
[08/27/2025 17:27:23 INFO]: Running Final Evaluation...
[08/27/2025 17:27:30 INFO]: Training loss at epoch 27: 0.9293292164802551
[08/27/2025 17:27:56 INFO]: Training loss at epoch 34: 1.0993794798851013
[08/27/2025 17:28:21 INFO]: Training loss at epoch 20: 1.0477821230888367
[08/27/2025 17:28:28 INFO]: Training loss at epoch 24: 1.2327708005905151
[08/27/2025 17:28:37 INFO]: Training loss at epoch 8: 1.0749613046646118
[08/27/2025 17:28:41 INFO]: New best epoch, val score: -0.6656822605556257
[08/27/2025 17:28:41 INFO]: Saving model to: unhanged-Shanesha_trial_33/model_best.pth
[08/27/2025 17:28:42 INFO]: New best epoch, val score: -0.662921329111435
[08/27/2025 17:28:42 INFO]: Saving model to: unhanged-Shanesha_trial_31/model_best.pth
[08/27/2025 17:28:50 INFO]: Training loss at epoch 23: 0.8880827128887177
[08/27/2025 17:28:50 INFO]: Training loss at epoch 31: 0.9585743248462677
[08/27/2025 17:28:51 INFO]: Training loss at epoch 16: 1.0345687866210938
[08/27/2025 17:29:07 INFO]: Training loss at epoch 25: 1.0451059937477112
[08/27/2025 17:29:17 INFO]: Training loss at epoch 23: 1.0515220761299133
[08/27/2025 17:29:19 INFO]: Training accuracy: {
    "score": -1.0089624317580013,
    "rmse": 1.0089624317580013
}
[08/27/2025 17:29:19 INFO]: Val accuracy: {
    "score": -0.6595725754984487,
    "rmse": 0.6595725754984487
}
[08/27/2025 17:29:19 INFO]: Test accuracy: {
    "score": -0.8705222382854387,
    "rmse": 0.8705222382854387
}
[08/27/2025 17:29:19 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_8",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8705222382854387,
        "rmse": 0.8705222382854387
    },
    "train_stats": {
        "score": -1.0089624317580013,
        "rmse": 1.0089624317580013
    },
    "val_stats": {
        "score": -0.6595725754984487,
        "rmse": 0.6595725754984487
    }
}
[08/27/2025 17:29:19 INFO]: Procewss finished for trial unhanged-Shanesha_trial_8
[08/27/2025 17:29:19 INFO]: 
_________________________________________________

[08/27/2025 17:29:19 INFO]: train_net_for_optune.py main() running.
[08/27/2025 17:29:19 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.645140286444895
  attention_dropout: 0.17197959576158123
  ffn_dropout: 0.17197959576158123
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00026909078628462924
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_35

[08/27/2025 17:29:19 INFO]: This ft_transformer has 2.532 million parameters.
[08/27/2025 17:29:19 INFO]: Training will start at epoch 0.
[08/27/2025 17:29:19 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 17:29:22 INFO]: Training loss at epoch 28: 1.1040731966495514
[08/27/2025 17:29:51 INFO]: Training loss at epoch 34: 1.1512499451637268
[08/27/2025 17:29:53 INFO]: Training loss at epoch 31: 0.9211776554584503
[08/27/2025 17:30:21 INFO]: Training loss at epoch 25: 0.8194463849067688
[08/27/2025 17:30:29 INFO]: Training loss at epoch 34: 1.0052027702331543
[08/27/2025 17:30:32 INFO]: Training loss at epoch 9: 1.0171528458595276
[08/27/2025 17:30:37 INFO]: Training loss at epoch 35: 0.8547995090484619
[08/27/2025 17:30:59 INFO]: Training loss at epoch 0: 1.3016247749328613
[08/27/2025 17:31:10 INFO]: Training stats: {
    "score": -0.9994725638522207,
    "rmse": 0.9994725638522207
}
[08/27/2025 17:31:10 INFO]: Val stats: {
    "score": -0.6834046940721483,
    "rmse": 0.6834046940721483
}
[08/27/2025 17:31:10 INFO]: Test stats: {
    "score": -0.8742648329300958,
    "rmse": 0.8742648329300958
}
[08/27/2025 17:31:12 INFO]: Training loss at epoch 12: 0.8597772419452667
[08/27/2025 17:31:13 INFO]: New best epoch, val score: -0.7188495210838732
[08/27/2025 17:31:13 INFO]: Saving model to: unhanged-Shanesha_trial_35/model_best.pth
[08/27/2025 17:31:16 INFO]: Training loss at epoch 29: 0.9992348551750183
[08/27/2025 17:31:22 INFO]: Training loss at epoch 12: 1.114711582660675
[08/27/2025 17:31:27 INFO]: Training loss at epoch 21: 0.8785552084445953
[08/27/2025 17:31:27 INFO]: Training loss at epoch 23: 1.342735230922699
[08/27/2025 17:31:52 INFO]: Training loss at epoch 37: 0.9497982561588287
[08/27/2025 17:31:58 INFO]: Training stats: {
    "score": -0.998323008756625,
    "rmse": 0.998323008756625
}
[08/27/2025 17:31:58 INFO]: Val stats: {
    "score": -0.7020303368415601,
    "rmse": 0.7020303368415601
}
[08/27/2025 17:31:58 INFO]: Test stats: {
    "score": -0.8849288204962696,
    "rmse": 0.8849288204962696
}
[08/27/2025 17:32:08 INFO]: Training loss at epoch 14: 1.0674470663070679
[08/27/2025 17:32:14 INFO]: Training loss at epoch 26: 0.9511770308017731
[08/27/2025 17:32:23 INFO]: Training loss at epoch 24: 0.9517163634300232
[08/27/2025 17:32:25 INFO]: Training loss at epoch 28: 1.1085265576839447
[08/27/2025 17:32:56 INFO]: Training loss at epoch 1: 1.8434451222419739
[08/27/2025 17:33:05 INFO]: Training loss at epoch 10: 1.0039062798023224
[08/27/2025 17:33:18 INFO]: Training loss at epoch 36: 1.1740776598453522
[08/27/2025 17:33:25 INFO]: Training loss at epoch 17: 1.1328371167182922
[08/27/2025 17:33:49 INFO]: Training loss at epoch 30: 0.9944957196712494
[08/27/2025 17:33:59 INFO]: New best epoch, val score: -0.6615516569628243
[08/27/2025 17:33:59 INFO]: Saving model to: unhanged-Shanesha_trial_30/model_best.pth
[08/27/2025 17:34:06 INFO]: Training loss at epoch 27: 1.0805778503417969
[08/27/2025 17:34:09 INFO]: Training loss at epoch 32: 1.0488553047180176
[08/27/2025 17:34:33 INFO]: Training loss at epoch 22: 0.993163675069809
[08/27/2025 17:34:43 INFO]: Training loss at epoch 35: 0.9766060411930084
[08/27/2025 17:34:45 INFO]: Training loss at epoch 26: 0.9231107532978058
[08/27/2025 17:34:50 INFO]: Training loss at epoch 2: 1.9777374863624573
[08/27/2025 17:34:59 INFO]: Training loss at epoch 11: 1.1090048551559448
[08/27/2025 17:35:03 INFO]: New best epoch, val score: -0.6846578574921736
[08/27/2025 17:35:03 INFO]: Saving model to: unhanged-Shanesha_trial_35/model_best.pth
[08/27/2025 17:35:10 INFO]: Training loss at epoch 32: 1.0128088593482971
[08/27/2025 17:35:13 INFO]: New best epoch, val score: -0.66709647005937
[08/27/2025 17:35:13 INFO]: Saving model to: unhanged-Shanesha_trial_34/model_best.pth
[08/27/2025 17:35:21 INFO]: Training loss at epoch 35: 1.0976666808128357
[08/27/2025 17:35:42 INFO]: Training loss at epoch 31: 0.8951278030872345
[08/27/2025 17:35:53 INFO]: Training loss at epoch 25: 1.0095877051353455
[08/27/2025 17:35:57 INFO]: Training loss at epoch 28: 0.9438753426074982
[08/27/2025 17:35:58 INFO]: Training loss at epoch 37: 0.7962131798267365
[08/27/2025 17:36:20 INFO]: Training loss at epoch 24: 1.029390573501587
[08/27/2025 17:36:20 INFO]: Training loss at epoch 38: 1.0723312497138977
[08/27/2025 17:36:44 INFO]: Training loss at epoch 3: 0.987126499414444
[08/27/2025 17:36:54 INFO]: Training loss at epoch 12: 0.9089918732643127
[08/27/2025 17:37:08 INFO]: New best epoch, val score: -0.6656275141968566
[08/27/2025 17:37:08 INFO]: Saving model to: unhanged-Shanesha_trial_34/model_best.pth
[08/27/2025 17:37:10 INFO]: New best epoch, val score: -0.6611643006245881
[08/27/2025 17:37:10 INFO]: Saving model to: unhanged-Shanesha_trial_18/model_best.pth
[08/27/2025 17:37:37 INFO]: Training loss at epoch 32: 0.9389254152774811
[08/27/2025 17:37:38 INFO]: Training loss at epoch 23: 1.1664708852767944
[08/27/2025 17:37:51 INFO]: Training loss at epoch 29: 1.0521107614040375
[08/27/2025 17:38:01 INFO]: Training loss at epoch 18: 1.115630179643631
[08/27/2025 17:38:02 INFO]: New best epoch, val score: -0.6628216332613188
[08/27/2025 17:38:02 INFO]: Saving model to: unhanged-Shanesha_trial_31/model_best.pth
[08/27/2025 17:38:24 INFO]: Training loss at epoch 29: 0.9667656123638153
[08/27/2025 17:38:30 INFO]: Training stats: {
    "score": -0.9911114565208567,
    "rmse": 0.9911114565208567
}
[08/27/2025 17:38:30 INFO]: Val stats: {
    "score": -0.7015270838067967,
    "rmse": 0.7015270838067967
}
[08/27/2025 17:38:30 INFO]: Test stats: {
    "score": -0.8867571797412082,
    "rmse": 0.8867571797412082
}
[08/27/2025 17:38:37 INFO]: Training loss at epoch 24: 0.9071691930294037
[08/27/2025 17:38:38 INFO]: Training loss at epoch 4: 1.282277524471283
[08/27/2025 17:38:40 INFO]: Training loss at epoch 38: 1.0907507538795471
[08/27/2025 17:38:48 INFO]: Training loss at epoch 13: 1.0071080923080444
[08/27/2025 17:38:51 INFO]: New best epoch, val score: -0.660523873463063
[08/27/2025 17:38:51 INFO]: Saving model to: unhanged-Shanesha_trial_35/model_best.pth
[08/27/2025 17:39:25 INFO]: Training loss at epoch 33: 1.0398792028427124
[08/27/2025 17:39:26 INFO]: Training loss at epoch 26: 1.0903038382530212
[08/27/2025 17:39:27 INFO]: New best epoch, val score: -0.6612303523488031
[08/27/2025 17:39:27 INFO]: Saving model to: unhanged-Shanesha_trial_20/model_best.pth
[08/27/2025 17:39:30 INFO]: Training loss at epoch 33: 1.1008821725845337
[08/27/2025 17:39:33 INFO]: Training loss at epoch 36: 1.2737926542758942
[08/27/2025 17:39:41 INFO]: Training loss at epoch 13: 1.2765508890151978
[08/27/2025 17:40:13 INFO]: Training loss at epoch 36: 1.0605215430259705
[08/27/2025 17:40:22 INFO]: Training loss at epoch 27: 1.010704517364502
[08/27/2025 17:40:22 INFO]: Training loss at epoch 30: 0.982284665107727
[08/27/2025 17:40:27 INFO]: Training loss at epoch 33: 1.1561862230300903
[08/27/2025 17:40:29 INFO]: Training stats: {
    "score": -0.9956399381889623,
    "rmse": 0.9956399381889623
}
[08/27/2025 17:40:29 INFO]: Val stats: {
    "score": -0.7132751176216946,
    "rmse": 0.7132751176216946
}
[08/27/2025 17:40:29 INFO]: Test stats: {
    "score": -0.9014064865552173,
    "rmse": 0.9014064865552173
}
[08/27/2025 17:40:33 INFO]: Training loss at epoch 5: 1.1949609220027924
[08/27/2025 17:40:41 INFO]: Training loss at epoch 14: 0.9164975583553314
[08/27/2025 17:40:42 INFO]: Training loss at epoch 24: 0.9481252431869507
[08/27/2025 17:40:46 INFO]: New best epoch, val score: -0.6603195507512868
[08/27/2025 17:40:46 INFO]: Saving model to: unhanged-Shanesha_trial_35/model_best.pth
[08/27/2025 17:40:52 INFO]: Training loss at epoch 39: 1.152323305606842
[08/27/2025 17:41:20 INFO]: Training loss at epoch 39: 1.0515666604042053
[08/27/2025 17:41:24 INFO]: Training loss at epoch 34: 1.267669141292572
[08/27/2025 17:42:12 INFO]: Training loss at epoch 31: 1.206722378730774
[08/27/2025 17:42:17 INFO]: Training stats: {
    "score": -0.9885583074827224,
    "rmse": 0.9885583074827224
}
[08/27/2025 17:42:17 INFO]: Val stats: {
    "score": -0.6711481039905096,
    "rmse": 0.6711481039905096
}
[08/27/2025 17:42:17 INFO]: Test stats: {
    "score": -0.8759206458566035,
    "rmse": 0.8759206458566035
}
[08/27/2025 17:42:24 INFO]: Training stats: {
    "score": -0.9971806107809061,
    "rmse": 0.9971806107809061
}
[08/27/2025 17:42:24 INFO]: Val stats: {
    "score": -0.6905940820237123,
    "rmse": 0.6905940820237123
}
[08/27/2025 17:42:24 INFO]: Test stats: {
    "score": -0.8800784612850808,
    "rmse": 0.8800784612850808
}
[08/27/2025 17:42:28 INFO]: Training loss at epoch 6: 0.9157116413116455
[08/27/2025 17:42:30 INFO]: Training loss at epoch 19: 0.8206025958061218
[08/27/2025 17:42:34 INFO]: Training loss at epoch 15: 1.1095830798149109
[08/27/2025 17:42:36 INFO]: Training loss at epoch 13: 0.9239190518856049
[08/27/2025 17:42:41 INFO]: New best epoch, val score: -0.6602635795068172
[08/27/2025 17:42:41 INFO]: Saving model to: unhanged-Shanesha_trial_35/model_best.pth
[08/27/2025 17:42:55 INFO]: Running Final Evaluation...
[08/27/2025 17:42:55 INFO]: Training loss at epoch 27: 0.8409384787082672
[08/27/2025 17:43:16 INFO]: Training loss at epoch 35: 1.0538602769374847
[08/27/2025 17:43:24 INFO]: Training loss at epoch 25: 1.1694827675819397
[08/27/2025 17:43:42 INFO]: Training loss at epoch 15: 1.2432796657085419
[08/27/2025 17:43:48 INFO]: Training loss at epoch 25: 0.9947851300239563
[08/27/2025 17:44:04 INFO]: Training stats: {
    "score": -1.0033345661428614,
    "rmse": 1.0033345661428614
}
[08/27/2025 17:44:04 INFO]: Val stats: {
    "score": -0.6645583400930526,
    "rmse": 0.6645583400930526
}
[08/27/2025 17:44:04 INFO]: Test stats: {
    "score": -0.8685407805651518,
    "rmse": 0.8685407805651518
}
[08/27/2025 17:44:05 INFO]: Training loss at epoch 32: 0.8679259419441223
[08/27/2025 17:44:14 INFO]: New best epoch, val score: -0.6604868067232093
[08/27/2025 17:44:14 INFO]: Saving model to: unhanged-Shanesha_trial_18/model_best.pth
[08/27/2025 17:44:23 INFO]: Training loss at epoch 7: 1.1025812029838562
[08/27/2025 17:44:24 INFO]: Training loss at epoch 37: 0.9879240393638611
[08/27/2025 17:44:29 INFO]: Training loss at epoch 16: 1.046124905347824
[08/27/2025 17:44:40 INFO]: Training accuracy: {
    "score": -1.0108601766513985,
    "rmse": 1.0108601766513985
}
[08/27/2025 17:44:40 INFO]: Val accuracy: {
    "score": -0.6628169354264378,
    "rmse": 0.6628169354264378
}
[08/27/2025 17:44:40 INFO]: Test accuracy: {
    "score": -0.8723862838470249,
    "rmse": 0.8723862838470249
}
[08/27/2025 17:44:40 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_7",
    "best_epoch": 8,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8723862838470249,
        "rmse": 0.8723862838470249
    },
    "train_stats": {
        "score": -1.0108601766513985,
        "rmse": 1.0108601766513985
    },
    "val_stats": {
        "score": -0.6628169354264378,
        "rmse": 0.6628169354264378
    }
}
[08/27/2025 17:44:40 INFO]: Procewss finished for trial unhanged-Shanesha_trial_7
[08/27/2025 17:44:40 INFO]: 
_________________________________________________

[08/27/2025 17:44:40 INFO]: train_net_for_optune.py main() running.
[08/27/2025 17:44:40 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.5098945047255175
  attention_dropout: 0.15979799419113055
  ffn_dropout: 0.15979799419113055
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.000280838884094487
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_36

[08/27/2025 17:44:40 INFO]: This ft_transformer has 2.460 million parameters.
[08/27/2025 17:44:40 INFO]: Training will start at epoch 0.
[08/27/2025 17:44:40 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 17:44:42 INFO]: Training loss at epoch 34: 0.9277269244194031
[08/27/2025 17:45:00 INFO]: Training loss at epoch 40: 1.0705241560935974
[08/27/2025 17:45:06 INFO]: Training loss at epoch 37: 0.8604246079921722
[08/27/2025 17:45:14 INFO]: Training loss at epoch 36: 0.8619228601455688
[08/27/2025 17:45:47 INFO]: Training loss at epoch 34: 0.8835025131702423
[08/27/2025 17:45:50 INFO]: Training loss at epoch 25: 0.9934673011302948
[08/27/2025 17:45:59 INFO]: Training loss at epoch 28: 0.8238619267940521
[08/27/2025 17:46:01 INFO]: Training loss at epoch 33: 0.9198341965675354
[08/27/2025 17:46:19 INFO]: Training loss at epoch 0: 1.5974588990211487
[08/27/2025 17:46:19 INFO]: Training loss at epoch 8: 1.1796275973320007
[08/27/2025 17:46:25 INFO]: Training loss at epoch 17: 1.1038188934326172
[08/27/2025 17:46:27 INFO]: Training loss at epoch 30: 0.8888604044914246
[08/27/2025 17:46:31 INFO]: Training loss at epoch 28: 0.8956634104251862
[08/27/2025 17:46:32 INFO]: New best epoch, val score: -1.1499184818027062
[08/27/2025 17:46:32 INFO]: Saving model to: unhanged-Shanesha_trial_36/model_best.pth
[08/27/2025 17:46:56 INFO]: Training loss at epoch 26: 1.0400847792625427
[08/27/2025 17:47:08 INFO]: Training loss at epoch 37: 1.104825496673584
[08/27/2025 17:47:40 INFO]: Training loss at epoch 41: 1.0339105129241943
[08/27/2025 17:47:51 INFO]: Training loss at epoch 34: 0.9696658253669739
[08/27/2025 17:48:03 INFO]: Training loss at epoch 14: 0.8816478550434113
[08/27/2025 17:48:11 INFO]: Training loss at epoch 1: 1.6221843361854553
[08/27/2025 17:48:13 INFO]: Training loss at epoch 9: 0.9542663097381592
[08/27/2025 17:48:19 INFO]: Training loss at epoch 18: 1.1153908967971802
[08/27/2025 17:48:24 INFO]: New best epoch, val score: -1.0677390006099348
[08/27/2025 17:48:24 INFO]: Saving model to: unhanged-Shanesha_trial_36/model_best.pth
[08/27/2025 17:48:39 INFO]: Training loss at epoch 20: 1.0077146887779236
[08/27/2025 17:48:54 INFO]: Training stats: {
    "score": -1.0065872251401176,
    "rmse": 1.0065872251401176
}
[08/27/2025 17:48:54 INFO]: Val stats: {
    "score": -0.6618801628452035,
    "rmse": 0.6618801628452035
}
[08/27/2025 17:48:54 INFO]: Test stats: {
    "score": -0.8683852494523888,
    "rmse": 0.8683852494523888
}
[08/27/2025 17:49:01 INFO]: Training loss at epoch 38: 1.1202179789543152
[08/27/2025 17:49:17 INFO]: Training loss at epoch 38: 1.0909696817398071
[08/27/2025 17:49:44 INFO]: Training loss at epoch 35: 0.989043116569519
[08/27/2025 17:49:58 INFO]: Training loss at epoch 38: 0.9690176248550415
[08/27/2025 17:49:59 INFO]: Training loss at epoch 35: 0.9135984480381012
[08/27/2025 17:50:01 INFO]: Training loss at epoch 27: 0.8864929676055908
[08/27/2025 17:50:02 INFO]: Training loss at epoch 29: 0.9596121907234192
[08/27/2025 17:50:03 INFO]: Training loss at epoch 2: 1.8577470779418945
[08/27/2025 17:50:12 INFO]: Training loss at epoch 19: 0.8163941502571106
[08/27/2025 17:50:16 INFO]: New best epoch, val score: -0.679230714239985
[08/27/2025 17:50:16 INFO]: Saving model to: unhanged-Shanesha_trial_36/model_best.pth
[08/27/2025 17:50:21 INFO]: Training loss at epoch 42: 1.0068469643592834
[08/27/2025 17:50:31 INFO]: Training loss at epoch 26: 0.9766072928905487
[08/27/2025 17:50:48 INFO]: Training loss at epoch 10: 1.0725187063217163
[08/27/2025 17:50:51 INFO]: Training stats: {
    "score": -1.0020506906706383,
    "rmse": 1.0020506906706383
}
[08/27/2025 17:50:51 INFO]: Val stats: {
    "score": -0.666823864992418,
    "rmse": 0.666823864992418
}
[08/27/2025 17:50:51 INFO]: Test stats: {
    "score": -0.868214844516911,
    "rmse": 0.868214844516911
}
[08/27/2025 17:50:55 INFO]: Training loss at epoch 39: 1.006996512413025
[08/27/2025 17:51:02 INFO]: Training loss at epoch 35: 0.954727828502655
[08/27/2025 17:51:14 INFO]: Training stats: {
    "score": -1.0010084700868898,
    "rmse": 1.0010084700868898
}
[08/27/2025 17:51:14 INFO]: Val stats: {
    "score": -0.6639507808251988,
    "rmse": 0.6639507808251988
}
[08/27/2025 17:51:14 INFO]: Test stats: {
    "score": -0.8691819513222144,
    "rmse": 0.8691819513222144
}
[08/27/2025 17:51:34 INFO]: Training loss at epoch 29: 0.9843766689300537
[08/27/2025 17:51:35 INFO]: Training stats: {
    "score": -0.9955054227467659,
    "rmse": 0.9955054227467659
}
[08/27/2025 17:51:35 INFO]: Val stats: {
    "score": -0.6786610647661374,
    "rmse": 0.6786610647661374
}
[08/27/2025 17:51:35 INFO]: Test stats: {
    "score": -0.8726397077066744,
    "rmse": 0.8726397077066744
}
[08/27/2025 17:51:36 INFO]: Training loss at epoch 36: 0.868416041135788
[08/27/2025 17:51:55 INFO]: Training loss at epoch 3: 0.9022684991359711
[08/27/2025 17:52:26 INFO]: Training loss at epoch 31: 0.962042897939682
[08/27/2025 17:52:43 INFO]: Training loss at epoch 11: 1.0599399507045746
[08/27/2025 17:52:46 INFO]: Training loss at epoch 20: 0.8657567203044891
[08/27/2025 17:53:00 INFO]: Training loss at epoch 26: 1.4375262260437012
[08/27/2025 17:53:00 INFO]: New best epoch, val score: -0.6636588172106607
[08/27/2025 17:53:00 INFO]: Saving model to: unhanged-Shanesha_trial_34/model_best.pth
[08/27/2025 17:53:05 INFO]: Training loss at epoch 43: 0.9978123605251312
[08/27/2025 17:53:08 INFO]: Training loss at epoch 28: 1.0930935442447662
[08/27/2025 17:53:14 INFO]: Training loss at epoch 21: 0.9200555980205536
[08/27/2025 17:53:24 INFO]: Running Final Evaluation...
[08/27/2025 17:53:27 INFO]: Training loss at epoch 40: 0.9218515753746033
[08/27/2025 17:53:28 INFO]: Training loss at epoch 37: 0.7987709939479828
[08/27/2025 17:53:28 INFO]: Training stats: {
    "score": -1.0019585189260722,
    "rmse": 1.0019585189260722
}
[08/27/2025 17:53:28 INFO]: Val stats: {
    "score": -0.7038438636837875,
    "rmse": 0.7038438636837875
}
[08/27/2025 17:53:28 INFO]: Test stats: {
    "score": -0.886664212422795,
    "rmse": 0.886664212422795
}
[08/27/2025 17:53:47 INFO]: Training loss at epoch 4: 1.2885573506355286
[08/27/2025 17:54:01 INFO]: Training loss at epoch 14: 0.925213485956192
[08/27/2025 17:54:07 INFO]: Training loss at epoch 39: 0.9372720718383789
[08/27/2025 17:54:28 INFO]: Training accuracy: {
    "score": -1.0031291742959518,
    "rmse": 1.0031291742959518
}
[08/27/2025 17:54:28 INFO]: Val accuracy: {
    "score": -0.6660893486722619,
    "rmse": 0.6660893486722619
}
[08/27/2025 17:54:28 INFO]: Test accuracy: {
    "score": -0.871901917079775,
    "rmse": 0.871901917079775
}
[08/27/2025 17:54:28 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_28",
    "best_epoch": 12,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.871901917079775,
        "rmse": 0.871901917079775
    },
    "train_stats": {
        "score": -1.0031291742959518,
        "rmse": 1.0031291742959518
    },
    "val_stats": {
        "score": -0.6660893486722619,
        "rmse": 0.6660893486722619
    }
}
[08/27/2025 17:54:28 INFO]: Procewss finished for trial unhanged-Shanesha_trial_28
[08/27/2025 17:54:28 INFO]: 
_________________________________________________

[08/27/2025 17:54:28 INFO]: train_net_for_optune.py main() running.
[08/27/2025 17:54:28 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.5054251676593013
  attention_dropout: 0.0012134219671777607
  ffn_dropout: 0.0012134219671777607
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00027379834905418715
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_37

[08/27/2025 17:54:29 INFO]: This ft_transformer has 2.458 million parameters.
[08/27/2025 17:54:29 INFO]: Training will start at epoch 0.
[08/27/2025 17:54:29 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 17:54:33 INFO]: Training loss at epoch 12: 1.0637088418006897
[08/27/2025 17:54:36 INFO]: Training loss at epoch 21: 0.8878132104873657
[08/27/2025 17:54:44 INFO]: Training loss at epoch 30: 0.9196596443653107
[08/27/2025 17:54:47 INFO]: Training loss at epoch 39: 0.8394131660461426
[08/27/2025 17:55:13 INFO]: Training loss at epoch 36: 1.1302064657211304
[08/27/2025 17:55:19 INFO]: Training loss at epoch 38: 0.896267831325531
[08/27/2025 17:55:20 INFO]: Training loss at epoch 41: 1.1594375371932983
[08/27/2025 17:55:20 INFO]: Training loss at epoch 16: 1.1886641085147858
[08/27/2025 17:55:36 INFO]: Running Final Evaluation...
[08/27/2025 17:55:41 INFO]: Training loss at epoch 5: 1.125292420387268
[08/27/2025 17:55:46 INFO]: Training stats: {
    "score": -0.9998389966998251,
    "rmse": 0.9998389966998251
}
[08/27/2025 17:55:46 INFO]: Val stats: {
    "score": -0.6748109145332718,
    "rmse": 0.6748109145332718
}
[08/27/2025 17:55:46 INFO]: Test stats: {
    "score": -0.8715618965177524,
    "rmse": 0.8715618965177524
}
[08/27/2025 17:56:09 INFO]: Training loss at epoch 0: 1.6063420176506042
[08/27/2025 17:56:11 INFO]: Training loss at epoch 29: 1.1632017493247986
[08/27/2025 17:56:21 INFO]: Training loss at epoch 15: 1.02565199136734
[08/27/2025 17:56:22 INFO]: Training loss at epoch 36: 0.8404552638530731
[08/27/2025 17:56:23 INFO]: New best epoch, val score: -0.9974195523950485
[08/27/2025 17:56:23 INFO]: Saving model to: unhanged-Shanesha_trial_37/model_best.pth
[08/27/2025 17:56:24 INFO]: Training accuracy: {
    "score": -1.0082201094817935,
    "rmse": 1.0082201094817935
}
[08/27/2025 17:56:24 INFO]: Val accuracy: {
    "score": -0.6632291095931424,
    "rmse": 0.6632291095931424
}
[08/27/2025 17:56:24 INFO]: Test accuracy: {
    "score": -0.8693511594829392,
    "rmse": 0.8693511594829392
}
[08/27/2025 17:56:24 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_32",
    "best_epoch": 10,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8693511594829392,
        "rmse": 0.8693511594829392
    },
    "train_stats": {
        "score": -1.0082201094817935,
        "rmse": 1.0082201094817935
    },
    "val_stats": {
        "score": -0.6632291095931424,
        "rmse": 0.6632291095931424
    }
}
[08/27/2025 17:56:24 INFO]: Procewss finished for trial unhanged-Shanesha_trial_32
[08/27/2025 17:56:24 INFO]: 
_________________________________________________

[08/27/2025 17:56:24 INFO]: train_net_for_optune.py main() running.
[08/27/2025 17:56:24 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.0970900671743653
  attention_dropout: 0.005480527637451146
  ffn_dropout: 0.005480527637451146
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0003085513305757288
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_38

[08/27/2025 17:56:24 INFO]: This ft_transformer has 4.977 million parameters.
[08/27/2025 17:56:24 INFO]: Training will start at epoch 0.
[08/27/2025 17:56:24 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 17:56:28 INFO]: Training stats: {
    "score": -1.0011173995121971,
    "rmse": 1.0011173995121971
}
[08/27/2025 17:56:28 INFO]: Val stats: {
    "score": -0.669404005373164,
    "rmse": 0.669404005373164
}
[08/27/2025 17:56:28 INFO]: Test stats: {
    "score": -0.8696860937084502,
    "rmse": 0.8696860937084502
}
[08/27/2025 17:56:31 INFO]: Training loss at epoch 13: 1.0441181063652039
[08/27/2025 17:56:34 INFO]: Training loss at epoch 22: 1.252772331237793
[08/27/2025 17:57:14 INFO]: Training loss at epoch 39: 0.8815176784992218
[08/27/2025 17:57:17 INFO]: Training stats: {
    "score": -1.0060965989785846,
    "rmse": 1.0060965989785846
}
[08/27/2025 17:57:17 INFO]: Val stats: {
    "score": -0.7226225244225452,
    "rmse": 0.7226225244225452
}
[08/27/2025 17:57:17 INFO]: Test stats: {
    "score": -0.8953172803940725,
    "rmse": 0.8953172803940725
}
[08/27/2025 17:57:35 INFO]: Training loss at epoch 6: 1.0830150246620178
[08/27/2025 17:57:37 INFO]: Training loss at epoch 27: 0.9688585996627808
[08/27/2025 17:57:48 INFO]: Training loss at epoch 22: 1.127747505903244
[08/27/2025 17:57:51 INFO]: Training stats: {
    "score": -0.9849583565831395,
    "rmse": 0.9849583565831395
}
[08/27/2025 17:57:51 INFO]: Val stats: {
    "score": -0.6742233227783301,
    "rmse": 0.6742233227783301
}
[08/27/2025 17:57:51 INFO]: Test stats: {
    "score": -0.8737197941869309,
    "rmse": 0.8737197941869309
}
[08/27/2025 17:58:02 INFO]: Training loss at epoch 1: 1.904601275920868
[08/27/2025 17:58:16 INFO]: Training loss at epoch 31: 0.9240090847015381
[08/27/2025 17:58:23 INFO]: Training loss at epoch 14: 1.026247799396515
[08/27/2025 17:58:24 INFO]: Training loss at epoch 32: 0.989480048418045
[08/27/2025 17:58:25 INFO]: Training loss at epoch 23: 0.8970789909362793
[08/27/2025 17:58:53 INFO]: Training loss at epoch 0: 1.4536927938461304
[08/27/2025 17:59:02 INFO]: Training loss at epoch 30: 1.1385968029499054
[08/27/2025 17:59:13 INFO]: New best epoch, val score: -1.1233025368696474
[08/27/2025 17:59:13 INFO]: Saving model to: unhanged-Shanesha_trial_38/model_best.pth
[08/27/2025 17:59:25 INFO]: Training loss at epoch 7: 0.8959246277809143
[08/27/2025 17:59:43 INFO]: Training loss at epoch 40: 0.9018818140029907
[08/27/2025 17:59:54 INFO]: Training loss at epoch 2: 1.762341856956482
[08/27/2025 18:00:06 INFO]: Training loss at epoch 27: 0.9613392353057861
[08/27/2025 18:00:07 INFO]: New best epoch, val score: -0.7154292926981667
[08/27/2025 18:00:07 INFO]: Saving model to: unhanged-Shanesha_trial_37/model_best.pth
[08/27/2025 18:00:19 INFO]: Training loss at epoch 24: 0.82533398270607
[08/27/2025 18:00:20 INFO]: Training loss at epoch 15: 1.049626648426056
[08/27/2025 18:00:22 INFO]: Training loss at epoch 30: 1.2543138563632965
[08/27/2025 18:00:31 INFO]: Training loss at epoch 37: 1.0607880353927612
[08/27/2025 18:00:40 INFO]: Training loss at epoch 40: 0.9305543601512909
[08/27/2025 18:00:59 INFO]: New best epoch, val score: -0.660264495113937
[08/27/2025 18:00:59 INFO]: Saving model to: unhanged-Shanesha_trial_20/model_best.pth
[08/27/2025 18:01:21 INFO]: Training loss at epoch 8: 1.0290319919586182
[08/27/2025 18:01:22 INFO]: Training loss at epoch 40: 1.1567392349243164
[08/27/2025 18:01:37 INFO]: Training loss at epoch 41: 0.724954754114151
[08/27/2025 18:01:43 INFO]: Training loss at epoch 37: 0.8553374111652374
[08/27/2025 18:01:47 INFO]: Training loss at epoch 1: 3.531762719154358
[08/27/2025 18:01:50 INFO]: Training loss at epoch 32: 0.9554885029792786
[08/27/2025 18:01:50 INFO]: Training loss at epoch 3: 1.0756725072860718
[08/27/2025 18:02:16 INFO]: Training loss at epoch 25: 0.9750476777553558
[08/27/2025 18:02:16 INFO]: Training loss at epoch 16: 1.229421228170395
[08/27/2025 18:02:26 INFO]: Training loss at epoch 23: 1.10200634598732
[08/27/2025 18:03:14 INFO]: Training loss at epoch 9: 0.8547842502593994
[08/27/2025 18:03:28 INFO]: Training loss at epoch 42: 1.061023086309433
[08/27/2025 18:03:30 INFO]: Training loss at epoch 31: 1.1878626942634583
[08/27/2025 18:03:44 INFO]: Training loss at epoch 4: 0.8422406762838364
[08/27/2025 18:03:51 INFO]: Training stats: {
    "score": -1.0101722724941298,
    "rmse": 1.0101722724941298
}
[08/27/2025 18:03:51 INFO]: Val stats: {
    "score": -0.7308152838668541,
    "rmse": 0.7308152838668541
}
[08/27/2025 18:03:51 INFO]: Test stats: {
    "score": -0.9023155201247036,
    "rmse": 0.9023155201247036
}
[08/27/2025 18:03:59 INFO]: New best epoch, val score: -0.661904820922259
[08/27/2025 18:03:59 INFO]: Saving model to: unhanged-Shanesha_trial_37/model_best.pth
[08/27/2025 18:04:10 INFO]: Training loss at epoch 17: 0.9219699501991272
[08/27/2025 18:04:11 INFO]: Training loss at epoch 26: 0.9279523193836212
[08/27/2025 18:04:26 INFO]: Training loss at epoch 33: 0.8491230905056
[08/27/2025 18:04:37 INFO]: Training loss at epoch 2: 1.926994800567627
[08/27/2025 18:04:41 INFO]: Training loss at epoch 31: 1.0811923742294312
[08/27/2025 18:04:41 INFO]: Training loss at epoch 28: 0.8750045299530029
[08/27/2025 18:04:42 INFO]: Training loss at epoch 16: 1.0935512781143188
[08/27/2025 18:04:58 INFO]: New best epoch, val score: -0.9800335285335777
[08/27/2025 18:04:58 INFO]: Saving model to: unhanged-Shanesha_trial_38/model_best.pth
[08/27/2025 18:05:20 INFO]: Training loss at epoch 43: 0.9559183716773987
[08/27/2025 18:05:21 INFO]: Training loss at epoch 33: 0.8922561705112457
[08/27/2025 18:05:30 INFO]: Training loss at epoch 15: 1.270469605922699
[08/27/2025 18:05:32 INFO]: Training loss at epoch 41: 0.8739283084869385
[08/27/2025 18:05:37 INFO]: Training loss at epoch 5: 1.0007283687591553
[08/27/2025 18:05:43 INFO]: Training loss at epoch 10: 0.9254715144634247
[08/27/2025 18:05:47 INFO]: Training loss at epoch 38: 0.9306813776493073
[08/27/2025 18:05:56 INFO]: New best epoch, val score: -0.6746270818681984
[08/27/2025 18:05:56 INFO]: Saving model to: unhanged-Shanesha_trial_36/model_best.pth
[08/27/2025 18:06:04 INFO]: Training loss at epoch 18: 0.8973360955715179
[08/27/2025 18:06:04 INFO]: Training loss at epoch 27: 1.2259277999401093
[08/27/2025 18:06:13 INFO]: Training loss at epoch 41: 0.9267415404319763
[08/27/2025 18:06:34 INFO]: Training loss at epoch 32: 1.1038055717945099
[08/27/2025 18:06:58 INFO]: Training loss at epoch 24: 1.034380555152893
[08/27/2025 18:06:59 INFO]: Training loss at epoch 17: 1.0213657319545746
[08/27/2025 18:07:00 INFO]: Training loss at epoch 38: 0.869236558675766
[08/27/2025 18:07:13 INFO]: Training loss at epoch 44: 1.1893360614776611
[08/27/2025 18:07:19 INFO]: Training loss at epoch 28: 1.1517281532287598
[08/27/2025 18:07:29 INFO]: Training loss at epoch 3: 1.2850091457366943
[08/27/2025 18:07:31 INFO]: Training loss at epoch 6: 1.0131936073303223
[08/27/2025 18:07:37 INFO]: Training loss at epoch 11: 1.1181342005729675
[08/27/2025 18:07:51 INFO]: New best epoch, val score: -0.7541012082797631
[08/27/2025 18:07:51 INFO]: Saving model to: unhanged-Shanesha_trial_38/model_best.pth
[08/27/2025 18:07:55 INFO]: New best epoch, val score: -0.6608595983448524
[08/27/2025 18:07:55 INFO]: Saving model to: unhanged-Shanesha_trial_36/model_best.pth
[08/27/2025 18:08:01 INFO]: Training loss at epoch 28: 1.0560767352581024
[08/27/2025 18:08:01 INFO]: Training loss at epoch 19: 0.8676639795303345
[08/27/2025 18:08:41 INFO]: Training stats: {
    "score": -1.001482881590016,
    "rmse": 1.001482881590016
}
[08/27/2025 18:08:41 INFO]: Val stats: {
    "score": -0.6672513762697677,
    "rmse": 0.6672513762697677
}
[08/27/2025 18:08:41 INFO]: Test stats: {
    "score": -0.8688058226642869,
    "rmse": 0.8688058226642869
}
[08/27/2025 18:08:57 INFO]: Training loss at epoch 34: 0.8165827989578247
[08/27/2025 18:09:11 INFO]: Training loss at epoch 45: 1.0501557886600494
[08/27/2025 18:09:27 INFO]: Training loss at epoch 7: 0.7941862344741821
[08/27/2025 18:09:36 INFO]: Training loss at epoch 12: 1.0168853998184204
[08/27/2025 18:09:42 INFO]: Training loss at epoch 33: 0.8446932137012482
[08/27/2025 18:09:53 INFO]: Training loss at epoch 29: 1.0396342277526855
[08/27/2025 18:10:22 INFO]: Training loss at epoch 32: 1.1649361550807953
[08/27/2025 18:10:23 INFO]: Training loss at epoch 4: 1.1312124133110046
[08/27/2025 18:10:27 INFO]: Training loss at epoch 42: 1.0145073533058167
[08/27/2025 18:10:28 INFO]: Training loss at epoch 34: 1.0539641976356506
[08/27/2025 18:10:33 INFO]: Training stats: {
    "score": -0.9993394253141912,
    "rmse": 0.9993394253141912
}
[08/27/2025 18:10:33 INFO]: Val stats: {
    "score": -0.672959455072436,
    "rmse": 0.672959455072436
}
[08/27/2025 18:10:33 INFO]: Test stats: {
    "score": -0.8700229838693097,
    "rmse": 0.8700229838693097
}
[08/27/2025 18:10:35 INFO]: Training loss at epoch 20: 0.9033387303352356
[08/27/2025 18:10:44 INFO]: New best epoch, val score: -0.7116977031403724
[08/27/2025 18:10:44 INFO]: Saving model to: unhanged-Shanesha_trial_38/model_best.pth
[08/27/2025 18:11:05 INFO]: Training loss at epoch 46: 0.822541207075119
[08/27/2025 18:11:08 INFO]: Training loss at epoch 42: 1.0104084610939026
[08/27/2025 18:11:09 INFO]: Training loss at epoch 39: 0.8811585903167725
[08/27/2025 18:11:20 INFO]: Training loss at epoch 8: 1.0199006795883179
[08/27/2025 18:11:28 INFO]: Training loss at epoch 13: 1.0543723106384277
[08/27/2025 18:11:32 INFO]: Training loss at epoch 25: 0.8543851375579834
[08/27/2025 18:11:47 INFO]: Training loss at epoch 29: 1.024619221687317
[08/27/2025 18:12:18 INFO]: Training loss at epoch 39: 1.0369771718978882
[08/27/2025 18:12:28 INFO]: Training loss at epoch 30: 0.9922254979610443
[08/27/2025 18:12:29 INFO]: Training loss at epoch 21: 1.11667600274086
[08/27/2025 18:12:31 INFO]: Training loss at epoch 35: 1.0224152505397797
[08/27/2025 18:12:48 INFO]: Training loss at epoch 34: 0.8966187238693237
[08/27/2025 18:12:58 INFO]: Training stats: {
    "score": -1.0093849254160192,
    "rmse": 1.0093849254160192
}
[08/27/2025 18:12:58 INFO]: Val stats: {
    "score": -0.6660801927174153,
    "rmse": 0.6660801927174153
}
[08/27/2025 18:12:58 INFO]: Test stats: {
    "score": -0.8663009242619852,
    "rmse": 0.8663009242619852
}
[08/27/2025 18:13:00 INFO]: Training loss at epoch 47: 0.9674002528190613
[08/27/2025 18:13:05 INFO]: Training loss at epoch 17: 0.9453361332416534
[08/27/2025 18:13:12 INFO]: Training loss at epoch 9: 0.9812304973602295
[08/27/2025 18:13:15 INFO]: Training loss at epoch 5: 1.191096842288971
[08/27/2025 18:13:21 INFO]: Training loss at epoch 14: 0.9794167280197144
[08/27/2025 18:13:35 INFO]: New best epoch, val score: -0.7102466977610541
[08/27/2025 18:13:35 INFO]: Saving model to: unhanged-Shanesha_trial_38/model_best.pth
[08/27/2025 18:13:49 INFO]: Training stats: {
    "score": -1.0024376722763746,
    "rmse": 1.0024376722763746
}
[08/27/2025 18:13:49 INFO]: Val stats: {
    "score": -0.6646666775296296,
    "rmse": 0.6646666775296296
}
[08/27/2025 18:13:49 INFO]: Test stats: {
    "score": -0.8696033900656687,
    "rmse": 0.8696033900656687
}
[08/27/2025 18:14:03 INFO]: Training stats: {
    "score": -0.9985401132779107,
    "rmse": 0.9985401132779107
}
[08/27/2025 18:14:03 INFO]: Val stats: {
    "score": -0.7116220783358536,
    "rmse": 0.7116220783358536
}
[08/27/2025 18:14:03 INFO]: Test stats: {
    "score": -0.8909852023404039,
    "rmse": 0.8909852023404039
}
[08/27/2025 18:14:10 INFO]: Training stats: {
    "score": -1.00054638150755,
    "rmse": 1.00054638150755
}
[08/27/2025 18:14:10 INFO]: Val stats: {
    "score": -0.6932341601924723,
    "rmse": 0.6932341601924723
}
[08/27/2025 18:14:10 INFO]: Test stats: {
    "score": -0.8805165407313603,
    "rmse": 0.8805165407313603
}
[08/27/2025 18:14:20 INFO]: Training loss at epoch 31: 1.1590096354484558
[08/27/2025 18:14:24 INFO]: Training loss at epoch 22: 1.1126376688480377
[08/27/2025 18:14:31 INFO]: Training loss at epoch 29: 0.9006794989109039
[08/27/2025 18:14:51 INFO]: Training loss at epoch 48: 0.9918321669101715
[08/27/2025 18:15:13 INFO]: Training loss at epoch 15: 1.0722008347511292
[08/27/2025 18:15:17 INFO]: Training loss at epoch 43: 0.9992783963680267
[08/27/2025 18:15:41 INFO]: Training loss at epoch 10: 0.9972888231277466
[08/27/2025 18:15:51 INFO]: Training loss at epoch 35: 1.0181015729904175
[08/27/2025 18:15:55 INFO]: Training loss at epoch 33: 0.8675966262817383
[08/27/2025 18:15:56 INFO]: Training loss at epoch 43: 0.954815536737442
[08/27/2025 18:15:59 INFO]: Training loss at epoch 36: 1.1115964651107788
[08/27/2025 18:16:03 INFO]: Training loss at epoch 26: 0.985586017370224
[08/27/2025 18:16:03 INFO]: Training loss at epoch 6: 1.132129728794098
[08/27/2025 18:16:12 INFO]: Training loss at epoch 32: 0.9592710733413696
[08/27/2025 18:16:16 INFO]: Training loss at epoch 23: 0.8439846038818359
[08/27/2025 18:16:23 INFO]: New best epoch, val score: -0.7049907143143619
[08/27/2025 18:16:23 INFO]: Saving model to: unhanged-Shanesha_trial_38/model_best.pth
[08/27/2025 18:16:24 INFO]: Training loss at epoch 35: 0.9600046873092651
[08/27/2025 18:16:38 INFO]: Training loss at epoch 49: 0.8248108923435211
[08/27/2025 18:16:50 INFO]: Training stats: {
    "score": -0.9886939715314409,
    "rmse": 0.9886939715314409
}
[08/27/2025 18:16:50 INFO]: Val stats: {
    "score": -0.6809248622045443,
    "rmse": 0.6809248622045443
}
[08/27/2025 18:16:50 INFO]: Test stats: {
    "score": -0.879454832443642,
    "rmse": 0.879454832443642
}
[08/27/2025 18:16:52 INFO]: Training loss at epoch 16: 1.1897874474525452
[08/27/2025 18:17:01 INFO]: Training loss at epoch 16: 0.9707991480827332
[08/27/2025 18:17:16 INFO]: Training stats: {
    "score": -0.9853939382387805,
    "rmse": 0.9853939382387805
}
[08/27/2025 18:17:16 INFO]: Val stats: {
    "score": -0.666646889142572,
    "rmse": 0.666646889142572
}
[08/27/2025 18:17:16 INFO]: Test stats: {
    "score": -0.8738505597583579,
    "rmse": 0.8738505597583579
}
[08/27/2025 18:17:29 INFO]: Training loss at epoch 11: 0.9507843554019928
[08/27/2025 18:18:01 INFO]: Training loss at epoch 33: 1.1124227941036224
[08/27/2025 18:18:03 INFO]: Training loss at epoch 40: 0.9821709990501404
[08/27/2025 18:18:06 INFO]: Training loss at epoch 24: 1.1599853932857513
[08/27/2025 18:18:26 INFO]: Training loss at epoch 18: 1.075208306312561
[08/27/2025 18:18:46 INFO]: Training loss at epoch 7: 1.2105984687805176
[08/27/2025 18:18:48 INFO]: Training loss at epoch 36: 0.9969414174556732
[08/27/2025 18:18:48 INFO]: Training loss at epoch 17: 0.9919882714748383
[08/27/2025 18:19:03 INFO]: Training loss at epoch 50: 0.9207857847213745
[08/27/2025 18:19:06 INFO]: New best epoch, val score: -0.6977122386642958
[08/27/2025 18:19:06 INFO]: Saving model to: unhanged-Shanesha_trial_38/model_best.pth
[08/27/2025 18:19:09 INFO]: Training loss at epoch 40: 0.8354348540306091
[08/27/2025 18:19:17 INFO]: Training loss at epoch 12: 1.1217795610427856
[08/27/2025 18:19:19 INFO]: Training loss at epoch 37: 1.0300392508506775
[08/27/2025 18:19:52 INFO]: Training loss at epoch 34: 0.8986904323101044
[08/27/2025 18:19:56 INFO]: Training loss at epoch 44: 0.9670729637145996
[08/27/2025 18:19:58 INFO]: Training loss at epoch 25: 1.0445868372917175
[08/27/2025 18:20:25 INFO]: Training loss at epoch 27: 1.129084825515747
[08/27/2025 18:20:36 INFO]: Training loss at epoch 44: 0.9668567180633545
[08/27/2025 18:20:38 INFO]: Training loss at epoch 18: 1.0473551154136658
[08/27/2025 18:20:52 INFO]: Training loss at epoch 51: 1.2157018780708313
[08/27/2025 18:21:00 INFO]: Training loss at epoch 30: 1.0429336428642273
[08/27/2025 18:21:07 INFO]: Training loss at epoch 18: 0.9694470465183258
[08/27/2025 18:21:08 INFO]: Training loss at epoch 13: 0.8898793160915375
[08/27/2025 18:21:18 INFO]: Training loss at epoch 34: 0.9941691160202026
[08/27/2025 18:21:33 INFO]: Training loss at epoch 8: 1.0771568417549133
[08/27/2025 18:21:42 INFO]: Training loss at epoch 35: 0.9683358073234558
[08/27/2025 18:21:49 INFO]: Training loss at epoch 37: 1.184813767671585
[08/27/2025 18:21:51 INFO]: Training loss at epoch 26: 0.9449802935123444
[08/27/2025 18:21:53 INFO]: New best epoch, val score: -0.6893261697274677
[08/27/2025 18:21:53 INFO]: Saving model to: unhanged-Shanesha_trial_38/model_best.pth
[08/27/2025 18:22:11 INFO]: Training loss at epoch 36: 1.1568331718444824
[08/27/2025 18:22:28 INFO]: Training loss at epoch 19: 0.8171175122261047
[08/27/2025 18:22:41 INFO]: Training loss at epoch 52: 0.8459004759788513
[08/27/2025 18:22:45 INFO]: Training loss at epoch 38: 0.9092967510223389
[08/27/2025 18:22:58 INFO]: Training loss at epoch 14: 0.8761480748653412
[08/27/2025 18:23:05 INFO]: Training stats: {
    "score": -1.0009228359720035,
    "rmse": 1.0009228359720035
}
[08/27/2025 18:23:05 INFO]: Val stats: {
    "score": -0.6977297227820809,
    "rmse": 0.6977297227820809
}
[08/27/2025 18:23:05 INFO]: Test stats: {
    "score": -0.8825774979464797,
    "rmse": 0.8825774979464797
}
[08/27/2025 18:23:09 INFO]: Training loss at epoch 41: 0.8473086953163147
[08/27/2025 18:23:31 INFO]: Training loss at epoch 36: 1.0709217190742493
[08/27/2025 18:23:41 INFO]: Training loss at epoch 27: 1.257092535495758
[08/27/2025 18:23:47 INFO]: Training loss at epoch 30: 1.0593650937080383
[08/27/2025 18:24:15 INFO]: Training loss at epoch 9: 1.1138948202133179
[08/27/2025 18:24:15 INFO]: Training loss at epoch 41: 0.9497250318527222
[08/27/2025 18:24:27 INFO]: Training loss at epoch 53: 0.9542341232299805
[08/27/2025 18:24:34 INFO]: Training loss at epoch 45: 1.0727618336677551
[08/27/2025 18:24:46 INFO]: Training loss at epoch 15: 0.940103143453598
[08/27/2025 18:24:47 INFO]: Training loss at epoch 28: 0.8837866485118866
[08/27/2025 18:24:49 INFO]: Training loss at epoch 38: 0.8355121612548828
[08/27/2025 18:24:53 INFO]: Training loss at epoch 20: 0.9249787032604218
[08/27/2025 18:25:11 INFO]: Training stats: {
    "score": -1.053024076964527,
    "rmse": 1.053024076964527
}
[08/27/2025 18:25:11 INFO]: Val stats: {
    "score": -0.6807496213863587,
    "rmse": 0.6807496213863587
}
[08/27/2025 18:25:11 INFO]: Test stats: {
    "score": -0.8990481953939896,
    "rmse": 0.8990481953939896
}
[08/27/2025 18:25:16 INFO]: Training loss at epoch 45: 0.8919537365436554
[08/27/2025 18:25:20 INFO]: Training loss at epoch 37: 1.0166959464550018
[08/27/2025 18:25:30 INFO]: New best epoch, val score: -0.6807496213863587
[08/27/2025 18:25:30 INFO]: Saving model to: unhanged-Shanesha_trial_38/model_best.pth
[08/27/2025 18:25:31 INFO]: Training loss at epoch 28: 1.0372859239578247
[08/27/2025 18:26:04 INFO]: Training loss at epoch 39: 0.9811169803142548
[08/27/2025 18:26:14 INFO]: Training loss at epoch 54: 0.7996898293495178
[08/27/2025 18:26:36 INFO]: Training loss at epoch 16: 1.0266259014606476
[08/27/2025 18:26:37 INFO]: Training loss at epoch 35: 0.9897531867027283
[08/27/2025 18:26:41 INFO]: Training loss at epoch 21: 0.9750016331672668
[08/27/2025 18:27:08 INFO]: Training loss at epoch 38: 0.9792899191379547
[08/27/2025 18:27:15 INFO]: Training stats: {
    "score": -0.9994866931093146,
    "rmse": 0.9994866931093146
}
[08/27/2025 18:27:15 INFO]: Val stats: {
    "score": -0.6642098991390836,
    "rmse": 0.6642098991390836
}
[08/27/2025 18:27:15 INFO]: Test stats: {
    "score": -0.8695830243355841,
    "rmse": 0.8695830243355841
}
[08/27/2025 18:27:21 INFO]: Training loss at epoch 29: 0.9844180345535278
[08/27/2025 18:27:46 INFO]: Training loss at epoch 31: 1.2007311880588531
[08/27/2025 18:27:49 INFO]: Training loss at epoch 39: 1.0905463099479675
[08/27/2025 18:27:50 INFO]: Training loss at epoch 17: 1.2412542700767517
[08/27/2025 18:27:55 INFO]: Training loss at epoch 37: 1.0025777220726013
[08/27/2025 18:27:55 INFO]: Training loss at epoch 10: 1.2921492457389832
[08/27/2025 18:28:00 INFO]: Training stats: {
    "score": -1.0187327320972728,
    "rmse": 1.0187327320972728
}
[08/27/2025 18:28:00 INFO]: Val stats: {
    "score": -0.661314946391458,
    "rmse": 0.661314946391458
}
[08/27/2025 18:28:00 INFO]: Test stats: {
    "score": -0.8744416261682182,
    "rmse": 0.8744416261682182
}
[08/27/2025 18:28:02 INFO]: Training loss at epoch 55: 0.8355785310268402
[08/27/2025 18:28:13 INFO]: Training loss at epoch 42: 1.2066793143749237
[08/27/2025 18:28:15 INFO]: New best epoch, val score: -0.6771442464884028
[08/27/2025 18:28:15 INFO]: Saving model to: unhanged-Shanesha_trial_38/model_best.pth
[08/27/2025 18:28:16 INFO]: Running Final Evaluation...
[08/27/2025 18:28:30 INFO]: Training loss at epoch 17: 1.0941263735294342
[08/27/2025 18:28:30 INFO]: Training loss at epoch 22: 1.1536442041397095
[08/27/2025 18:28:51 INFO]: Training stats: {
    "score": -0.9990211566969458,
    "rmse": 0.9990211566969458
}
[08/27/2025 18:28:51 INFO]: Val stats: {
    "score": -0.6958995015634669,
    "rmse": 0.6958995015634669
}
[08/27/2025 18:28:51 INFO]: Test stats: {
    "score": -0.8795721730120905,
    "rmse": 0.8795721730120905
}
[08/27/2025 18:28:58 INFO]: Training accuracy: {
    "score": -0.9950372137291681,
    "rmse": 0.9950372137291681
}
[08/27/2025 18:28:58 INFO]: Val accuracy: {
    "score": -0.6656822605556257,
    "rmse": 0.6656822605556257
}
[08/27/2025 18:28:58 INFO]: Test accuracy: {
    "score": -0.8705822469322191,
    "rmse": 0.8705822469322191
}
[08/27/2025 18:28:58 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_33",
    "best_epoch": 24,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8705822469322191,
        "rmse": 0.8705822469322191
    },
    "train_stats": {
        "score": -0.9950372137291681,
        "rmse": 0.9950372137291681
    },
    "val_stats": {
        "score": -0.6656822605556257,
        "rmse": 0.6656822605556257
    }
}
[08/27/2025 18:28:58 INFO]: Procewss finished for trial unhanged-Shanesha_trial_33
[08/27/2025 18:28:59 INFO]: 
_________________________________________________

[08/27/2025 18:28:59 INFO]: train_net_for_optune.py main() running.
[08/27/2025 18:28:59 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 2.1751301866991932
  attention_dropout: 0.18570292479328465
  ffn_dropout: 0.18570292479328465
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00029968106913202003
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_39

[08/27/2025 18:28:59 INFO]: This ft_transformer has 5.069 million parameters.
[08/27/2025 18:28:59 INFO]: Training will start at epoch 0.
[08/27/2025 18:28:59 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 18:29:00 INFO]: Training loss at epoch 39: 0.9841399490833282
[08/27/2025 18:29:05 INFO]: Training loss at epoch 19: 0.9691146314144135
[08/27/2025 18:29:09 INFO]: Training loss at epoch 29: 1.0225634574890137
[08/27/2025 18:29:12 INFO]: Training loss at epoch 46: 0.9947441816329956
[08/27/2025 18:29:18 INFO]: Training loss at epoch 42: 0.8579602241516113
[08/27/2025 18:29:34 INFO]: Training loss at epoch 19: 1.3377816081047058
[08/27/2025 18:29:37 INFO]: Training stats: {
    "score": -0.9980784238227307,
    "rmse": 0.9980784238227307
}
[08/27/2025 18:29:37 INFO]: Val stats: {
    "score": -0.6783840847068242,
    "rmse": 0.6783840847068242
}
[08/27/2025 18:29:37 INFO]: Test stats: {
    "score": -0.8722697811674873,
    "rmse": 0.8722697811674873
}
[08/27/2025 18:29:49 INFO]: Training loss at epoch 30: 1.0938748717308044
[08/27/2025 18:29:55 INFO]: Training loss at epoch 46: 1.0816739201545715
[08/27/2025 18:30:18 INFO]: Training loss at epoch 23: 0.9226098954677582
[08/27/2025 18:30:18 INFO]: Training loss at epoch 18: 0.9131755232810974
[08/27/2025 18:30:36 INFO]: Training loss at epoch 40: 1.1030154824256897
[08/27/2025 18:30:37 INFO]: Training loss at epoch 11: 0.948670506477356
[08/27/2025 18:30:37 INFO]: Training loss at epoch 31: 1.1118022203445435
[08/27/2025 18:30:38 INFO]: Training stats: {
    "score": -1.016266682738767,
    "rmse": 1.016266682738767
}
[08/27/2025 18:30:38 INFO]: Val stats: {
    "score": -0.6620720960519734,
    "rmse": 0.6620720960519734
}
[08/27/2025 18:30:38 INFO]: Test stats: {
    "score": -0.8737040526706648,
    "rmse": 0.8737040526706648
}
[08/27/2025 18:30:54 INFO]: Training loss at epoch 0: 1.021016240119934
[08/27/2025 18:31:12 INFO]: New best epoch, val score: -0.678790583728561
[08/27/2025 18:31:12 INFO]: Saving model to: unhanged-Shanesha_trial_39/model_best.pth
[08/27/2025 18:31:28 INFO]: Training loss at epoch 40: 0.9864891469478607
[08/27/2025 18:31:41 INFO]: Training loss at epoch 31: 0.9864908754825592
[08/27/2025 18:31:52 INFO]: Training loss at epoch 40: 1.0665704607963562
[08/27/2025 18:31:53 INFO]: Training stats: {
    "score": -1.0116893783642196,
    "rmse": 1.0116893783642196
}
[08/27/2025 18:31:53 INFO]: Val stats: {
    "score": -0.6604980799447345,
    "rmse": 0.6604980799447345
}
[08/27/2025 18:31:53 INFO]: Test stats: {
    "score": -0.8703455084448419,
    "rmse": 0.8703455084448419
}
[08/27/2025 18:32:00 INFO]: Training loss at epoch 36: 0.9769006073474884
[08/27/2025 18:32:08 INFO]: Training loss at epoch 24: 0.9018433690071106
[08/27/2025 18:32:09 INFO]: Training loss at epoch 19: 0.9922113120555878
[08/27/2025 18:32:46 INFO]: Training stats: {
    "score": -1.0031136544578405,
    "rmse": 1.0031136544578405
}
[08/27/2025 18:32:46 INFO]: Val stats: {
    "score": -0.7096791787486094,
    "rmse": 0.7096791787486094
}
[08/27/2025 18:32:46 INFO]: Test stats: {
    "score": -0.8910723201269165,
    "rmse": 0.8910723201269165
}
[08/27/2025 18:33:07 INFO]: Training loss at epoch 1: 4.336637377738953
[08/27/2025 18:33:16 INFO]: Training loss at epoch 43: 0.9131782352924347
[08/27/2025 18:33:17 INFO]: Training loss at epoch 41: 1.0205676555633545
[08/27/2025 18:33:19 INFO]: Training stats: {
    "score": -1.0051288006612498,
    "rmse": 1.0051288006612498
}
[08/27/2025 18:33:19 INFO]: Val stats: {
    "score": -0.7191177446898712,
    "rmse": 0.7191177446898712
}
[08/27/2025 18:33:19 INFO]: Test stats: {
    "score": -0.8961064456585017,
    "rmse": 0.8961064456585017
}
[08/27/2025 18:33:20 INFO]: Training loss at epoch 12: 0.9132236838340759
[08/27/2025 18:33:30 INFO]: Training loss at epoch 32: 1.0804848670959473
[08/27/2025 18:33:39 INFO]: Training loss at epoch 38: 0.9266188442707062
[08/27/2025 18:33:49 INFO]: Training loss at epoch 47: 0.9449412822723389
[08/27/2025 18:33:56 INFO]: Training loss at epoch 25: 0.9402893781661987
[08/27/2025 18:33:58 INFO]: Training loss at epoch 41: 1.149033546447754
[08/27/2025 18:34:20 INFO]: Training loss at epoch 43: 0.8866245746612549
[08/27/2025 18:34:29 INFO]: Training loss at epoch 32: 0.9714881777763367
[08/27/2025 18:34:34 INFO]: Training loss at epoch 20: 0.861903190612793
[08/27/2025 18:34:37 INFO]: Training loss at epoch 47: 1.1012383103370667
[08/27/2025 18:34:53 INFO]: Training loss at epoch 41: 1.2418882548809052
[08/27/2025 18:35:04 INFO]: Training loss at epoch 30: 0.9735991060733795
[08/27/2025 18:35:09 INFO]: Training loss at epoch 42: 0.9286215007305145
[08/27/2025 18:35:19 INFO]: Training loss at epoch 2: 1.9655271172523499
[08/27/2025 18:35:22 INFO]: Training loss at epoch 33: 0.8484027981758118
[08/27/2025 18:35:46 INFO]: Training loss at epoch 26: 0.873336911201477
[08/27/2025 18:36:05 INFO]: Training loss at epoch 13: 1.2642488479614258
[08/27/2025 18:36:23 INFO]: Training loss at epoch 21: 0.888848751783371
[08/27/2025 18:36:58 INFO]: Training loss at epoch 43: 1.0802527070045471
[08/27/2025 18:37:11 INFO]: Training loss at epoch 34: 0.9702381193637848
[08/27/2025 18:37:20 INFO]: Training loss at epoch 37: 0.992880642414093
[08/27/2025 18:37:20 INFO]: Training loss at epoch 42: 0.9259889125823975
[08/27/2025 18:37:31 INFO]: Training loss at epoch 3: 1.4643178582191467
[08/27/2025 18:37:31 INFO]: Training loss at epoch 32: 0.9805128872394562
[08/27/2025 18:37:33 INFO]: Training loss at epoch 27: 1.0560079514980316
[08/27/2025 18:37:50 INFO]: Training loss at epoch 42: 0.8315325379371643
[08/27/2025 18:38:11 INFO]: Training loss at epoch 22: 1.0950694680213928
[08/27/2025 18:38:19 INFO]: Training loss at epoch 44: 0.9958459138870239
[08/27/2025 18:38:27 INFO]: Training loss at epoch 48: 1.1347678899765015
[08/27/2025 18:38:46 INFO]: Training loss at epoch 18: 1.1937946677207947
[08/27/2025 18:38:47 INFO]: Training loss at epoch 44: 1.1263063549995422
[08/27/2025 18:38:47 INFO]: Training loss at epoch 14: 1.2886736392974854
[08/27/2025 18:39:01 INFO]: Training loss at epoch 35: 1.070863127708435
[08/27/2025 18:39:15 INFO]: Training loss at epoch 48: 0.9120034277439117
[08/27/2025 18:39:21 INFO]: Training loss at epoch 28: 0.849990576505661
[08/27/2025 18:39:22 INFO]: Training loss at epoch 44: 1.2140641510486603
[08/27/2025 18:39:23 INFO]: Training loss at epoch 39: 0.9444412589073181
[08/27/2025 18:39:25 INFO]: Training loss at epoch 31: 0.9999232292175293
[08/27/2025 18:39:41 INFO]: Training loss at epoch 4: 1.2947562336921692
[08/27/2025 18:39:53 INFO]: Training loss at epoch 20: 0.9832597374916077
[08/27/2025 18:39:59 INFO]: Training loss at epoch 23: 0.846023440361023
[08/27/2025 18:40:36 INFO]: Training loss at epoch 45: 0.8753284215927124
[08/27/2025 18:40:40 INFO]: Training loss at epoch 43: 1.0056501626968384
[08/27/2025 18:40:47 INFO]: Training loss at epoch 43: 0.8229762315750122
[08/27/2025 18:40:50 INFO]: Training loss at epoch 36: 1.1372585594654083
[08/27/2025 18:41:09 INFO]: Training loss at epoch 29: 0.872665524482727
[08/27/2025 18:41:12 INFO]: Training loss at epoch 33: 1.0438596606254578
[08/27/2025 18:41:22 INFO]: Training stats: {
    "score": -0.9931677446796495,
    "rmse": 0.9931677446796495
}
[08/27/2025 18:41:22 INFO]: Val stats: {
    "score": -0.7130474205887153,
    "rmse": 0.7130474205887153
}
[08/27/2025 18:41:22 INFO]: Test stats: {
    "score": -0.9028127214429347,
    "rmse": 0.9028127214429347
}
[08/27/2025 18:41:29 INFO]: Training loss at epoch 15: 1.2194974422454834
[08/27/2025 18:41:46 INFO]: Training stats: {
    "score": -0.9988646851979527,
    "rmse": 0.9988646851979527
}
[08/27/2025 18:41:46 INFO]: Val stats: {
    "score": -0.6807306740078639,
    "rmse": 0.6807306740078639
}
[08/27/2025 18:41:46 INFO]: Test stats: {
    "score": -0.8739816395757758,
    "rmse": 0.8739816395757758
}
[08/27/2025 18:41:46 INFO]: Training loss at epoch 24: 0.8568484783172607
[08/27/2025 18:41:51 INFO]: Training loss at epoch 5: 1.2384445071220398
[08/27/2025 18:42:24 INFO]: Training loss at epoch 46: 0.9507652819156647
[08/27/2025 18:42:38 INFO]: Training loss at epoch 38: 0.9031810164451599
[08/27/2025 18:42:40 INFO]: Training loss at epoch 37: 0.9421735405921936
[08/27/2025 18:42:54 INFO]: Running Final Evaluation...
[08/27/2025 18:43:02 INFO]: Training loss at epoch 49: 1.10136279463768
[08/27/2025 18:43:20 INFO]: Training loss at epoch 45: 0.9813798367977142
[08/27/2025 18:43:34 INFO]: Training loss at epoch 30: 1.0021107196807861
[08/27/2025 18:43:34 INFO]: Training loss at epoch 25: 0.876625269651413
[08/27/2025 18:43:34 INFO]: Training accuracy: {
    "score": -1.012720373205592,
    "rmse": 1.012720373205592
}
[08/27/2025 18:43:34 INFO]: Val accuracy: {
    "score": -0.6602635795068172,
    "rmse": 0.6602635795068172
}
[08/27/2025 18:43:34 INFO]: Test accuracy: {
    "score": -0.870468471810542,
    "rmse": 0.870468471810542
}
[08/27/2025 18:43:34 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_35",
    "best_epoch": 6,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.870468471810542,
        "rmse": 0.870468471810542
    },
    "train_stats": {
        "score": -1.012720373205592,
        "rmse": 1.012720373205592
    },
    "val_stats": {
        "score": -0.6602635795068172,
        "rmse": 0.6602635795068172
    }
}
[08/27/2025 18:43:34 INFO]: Procewss finished for trial unhanged-Shanesha_trial_35
[08/27/2025 18:43:35 INFO]: 
_________________________________________________

[08/27/2025 18:43:35 INFO]: train_net_for_optune.py main() running.
[08/27/2025 18:43:35 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 2.143196893082846
  attention_dropout: 0.01047994619498649
  ffn_dropout: 0.01047994619498649
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00031935480546244624
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_40

[08/27/2025 18:43:35 INFO]: This ft_transformer has 5.029 million parameters.
[08/27/2025 18:43:35 INFO]: Training will start at epoch 0.
[08/27/2025 18:43:35 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 18:43:44 INFO]: Training loss at epoch 32: 1.0937967896461487
[08/27/2025 18:43:45 INFO]: Training loss at epoch 44: 1.1069889664649963
[08/27/2025 18:43:50 INFO]: Training loss at epoch 49: 0.9266564846038818
[08/27/2025 18:44:01 INFO]: Training loss at epoch 44: 1.1021952033042908
[08/27/2025 18:44:02 INFO]: Training loss at epoch 6: 1.2388150095939636
[08/27/2025 18:44:12 INFO]: Training loss at epoch 16: 1.2964693307876587
[08/27/2025 18:44:15 INFO]: Training loss at epoch 47: 1.0494206249713898
[08/27/2025 18:44:19 INFO]: Training loss at epoch 33: 1.0944105982780457
[08/27/2025 18:44:25 INFO]: Training loss at epoch 45: 0.905174732208252
[08/27/2025 18:44:25 INFO]: Training loss at epoch 20: 0.8738623559474945
[08/27/2025 18:44:38 INFO]: Training stats: {
    "score": -0.9993442570117376,
    "rmse": 0.9993442570117376
}
[08/27/2025 18:44:38 INFO]: Val stats: {
    "score": -0.679405864454721,
    "rmse": 0.679405864454721
}
[08/27/2025 18:44:38 INFO]: Test stats: {
    "score": -0.873583361914656,
    "rmse": 0.873583361914656
}
[08/27/2025 18:45:26 INFO]: Training loss at epoch 31: 0.9216586351394653
[08/27/2025 18:45:27 INFO]: Training loss at epoch 26: 0.8879145085811615
[08/27/2025 18:45:30 INFO]: Training stats: {
    "score": -1.003687646355363,
    "rmse": 1.003687646355363
}
[08/27/2025 18:45:30 INFO]: Val stats: {
    "score": -0.6642375249667017,
    "rmse": 0.6642375249667017
}
[08/27/2025 18:45:30 INFO]: Test stats: {
    "score": -0.8685796782693957,
    "rmse": 0.8685796782693957
}
[08/27/2025 18:45:36 INFO]: Training loss at epoch 0: 1.5043116807937622
[08/27/2025 18:45:40 INFO]: New best epoch, val score: -0.6602809971492919
[08/27/2025 18:45:40 INFO]: Saving model to: unhanged-Shanesha_trial_37/model_best.pth
[08/27/2025 18:45:51 INFO]: New best epoch, val score: -0.9777188394763556
[08/27/2025 18:45:51 INFO]: Saving model to: unhanged-Shanesha_trial_40/model_best.pth
[08/27/2025 18:46:05 INFO]: Training loss at epoch 48: 1.1007478833198547
[08/27/2025 18:46:16 INFO]: Training loss at epoch 7: 1.2233081459999084
[08/27/2025 18:46:19 INFO]: New best epoch, val score: -0.6620936691612587
[08/27/2025 18:46:19 INFO]: Saving model to: unhanged-Shanesha_trial_34/model_best.pth
[08/27/2025 18:46:49 INFO]: Training loss at epoch 45: 0.9011039733886719
[08/27/2025 18:47:00 INFO]: Training loss at epoch 17: 0.9336130619049072
[08/27/2025 18:47:10 INFO]: Training loss at epoch 40: 1.0784689784049988
[08/27/2025 18:47:16 INFO]: Training loss at epoch 32: 1.0762747526168823
[08/27/2025 18:47:17 INFO]: Training loss at epoch 27: 1.0718377828598022
[08/27/2025 18:47:19 INFO]: New best epoch, val score: -0.671744061468822
[08/27/2025 18:47:19 INFO]: Saving model to: unhanged-Shanesha_trial_38/model_best.pth
[08/27/2025 18:47:28 INFO]: Training loss at epoch 45: 1.0747581720352173
[08/27/2025 18:47:30 INFO]: New best epoch, val score: -0.660239684506462
[08/27/2025 18:47:30 INFO]: Saving model to: unhanged-Shanesha_trial_37/model_best.pth
[08/27/2025 18:47:50 INFO]: Training loss at epoch 1: 4.353673279285431
[08/27/2025 18:47:54 INFO]: Training loss at epoch 21: 0.9622797071933746
[08/27/2025 18:47:56 INFO]: Training loss at epoch 49: 0.9757108390331268
[08/27/2025 18:48:01 INFO]: Training loss at epoch 34: 1.1117812395095825
[08/27/2025 18:48:03 INFO]: Training loss at epoch 39: 0.8607557713985443
[08/27/2025 18:48:10 INFO]: Training loss at epoch 33: 1.0572779178619385
[08/27/2025 18:48:29 INFO]: Training loss at epoch 46: 0.9727246463298798
[08/27/2025 18:48:30 INFO]: Training loss at epoch 8: 1.1037282347679138
[08/27/2025 18:48:36 INFO]: Training stats: {
    "score": -1.0084275431428629,
    "rmse": 1.0084275431428629
}
[08/27/2025 18:48:36 INFO]: Val stats: {
    "score": -0.6616529833912951,
    "rmse": 0.6616529833912951
}
[08/27/2025 18:48:36 INFO]: Test stats: {
    "score": -0.8696664714093288,
    "rmse": 0.8696664714093288
}
[08/27/2025 18:48:50 INFO]: New best epoch, val score: -0.6616529833912951
[08/27/2025 18:48:50 INFO]: Saving model to: unhanged-Shanesha_trial_34/model_best.pth
[08/27/2025 18:49:07 INFO]: Training loss at epoch 33: 0.953700840473175
[08/27/2025 18:49:08 INFO]: Training loss at epoch 28: 1.1168485581874847
[08/27/2025 18:49:23 INFO]: Training loss at epoch 50: 0.9054538011550903
[08/27/2025 18:49:33 INFO]: Training loss at epoch 46: 1.2108741402626038
[08/27/2025 18:49:45 INFO]: Training loss at epoch 18: 0.9235921502113342
[08/27/2025 18:49:46 INFO]: Training loss at epoch 19: 1.3592267036437988
[08/27/2025 18:49:50 INFO]: Training loss at epoch 46: 1.1716033816337585
[08/27/2025 18:49:57 INFO]: Training stats: {
    "score": -0.9997180007227724,
    "rmse": 0.9997180007227724
}
[08/27/2025 18:49:57 INFO]: Val stats: {
    "score": -0.6700282844311838,
    "rmse": 0.6700282844311838
}
[08/27/2025 18:49:57 INFO]: Test stats: {
    "score": -0.8698405479569871,
    "rmse": 0.8698405479569871
}
[08/27/2025 18:50:04 INFO]: New best epoch, val score: -0.661289978930156
[08/27/2025 18:50:04 INFO]: Saving model to: unhanged-Shanesha_trial_38/model_best.pth
[08/27/2025 18:50:06 INFO]: Training loss at epoch 2: 2.067630559206009
[08/27/2025 18:50:12 INFO]: Training loss at epoch 50: 0.9419463276863098
[08/27/2025 18:50:27 INFO]: Training loss at epoch 50: 1.1091357469558716
[08/27/2025 18:50:42 INFO]: Training loss at epoch 9: 1.2818264961242676
[08/27/2025 18:50:52 INFO]: Training loss at epoch 46: 0.8724307119846344
[08/27/2025 18:50:56 INFO]: Training loss at epoch 34: 1.0812774300575256
[08/27/2025 18:50:57 INFO]: Training loss at epoch 29: 1.1372421383857727
[08/27/2025 18:51:16 INFO]: Training loss at epoch 34: 0.8223502933979034
[08/27/2025 18:51:28 INFO]: Training stats: {
    "score": -1.0404027803973515,
    "rmse": 1.0404027803973515
}
[08/27/2025 18:51:28 INFO]: Val stats: {
    "score": -0.8009254394820455,
    "rmse": 0.8009254394820455
}
[08/27/2025 18:51:28 INFO]: Test stats: {
    "score": -0.9502094962687149,
    "rmse": 0.9502094962687149
}
[08/27/2025 18:51:35 INFO]: Training stats: {
    "score": -0.9988698787516027,
    "rmse": 0.9988698787516027
}
[08/27/2025 18:51:35 INFO]: Val stats: {
    "score": -0.6708879885707503,
    "rmse": 0.6708879885707503
}
[08/27/2025 18:51:35 INFO]: Test stats: {
    "score": -0.8712515383492455,
    "rmse": 0.8712515383492455
}
[08/27/2025 18:52:18 INFO]: Training loss at epoch 51: 0.9665776491165161
[08/27/2025 18:52:21 INFO]: Training loss at epoch 3: 1.2954424619674683
[08/27/2025 18:52:30 INFO]: Training loss at epoch 19: 1.1857231855392456
[08/27/2025 18:52:37 INFO]: Training loss at epoch 34: 0.860079288482666
[08/27/2025 18:52:44 INFO]: Training loss at epoch 35: 0.9781154990196228
[08/27/2025 18:52:49 INFO]: Training loss at epoch 47: 0.8131677210330963
[08/27/2025 18:52:57 INFO]: Training loss at epoch 41: 1.3190115690231323
[08/27/2025 18:53:22 INFO]: Training loss at epoch 30: 1.1390803754329681
[08/27/2025 18:53:25 INFO]: Training stats: {
    "score": -1.0138858645878481,
    "rmse": 1.0138858645878481
}
[08/27/2025 18:53:25 INFO]: Val stats: {
    "score": -0.6603844252877538,
    "rmse": 0.6603844252877538
}
[08/27/2025 18:53:25 INFO]: Test stats: {
    "score": -0.8717242675575325,
    "rmse": 0.8717242675575325
}
[08/27/2025 18:53:33 INFO]: Training stats: {
    "score": -1.0012609029147583,
    "rmse": 1.0012609029147583
}
[08/27/2025 18:53:33 INFO]: Val stats: {
    "score": -0.6649446985282265,
    "rmse": 0.6649446985282265
}
[08/27/2025 18:53:33 INFO]: Test stats: {
    "score": -0.869347927879047,
    "rmse": 0.869347927879047
}
[08/27/2025 18:53:34 INFO]: Training loss at epoch 47: 0.9027031362056732
[08/27/2025 18:53:38 INFO]: Training loss at epoch 10: 0.9550833702087402
[08/27/2025 18:53:45 INFO]: New best epoch, val score: -0.6603844252877538
[08/27/2025 18:53:45 INFO]: Saving model to: unhanged-Shanesha_trial_38/model_best.pth
[08/27/2025 18:54:02 INFO]: Training loss at epoch 51: 1.112690508365631
[08/27/2025 18:54:09 INFO]: Training loss at epoch 52: 0.9001031816005707
[08/27/2025 18:54:14 INFO]: Training loss at epoch 47: 0.9154262244701385
[08/27/2025 18:54:34 INFO]: Training loss at epoch 36: 0.8836841583251953
[08/27/2025 18:54:34 INFO]: Training loss at epoch 4: 1.3706003427505493
[08/27/2025 18:54:38 INFO]: Training loss at epoch 47: 0.9545603692531586
[08/27/2025 18:54:50 INFO]: Training loss at epoch 35: 0.9270639717578888
[08/27/2025 18:54:50 INFO]: New best epoch, val score: -0.6661418714123647
[08/27/2025 18:54:50 INFO]: Saving model to: unhanged-Shanesha_trial_40/model_best.pth
[08/27/2025 18:54:55 INFO]: Training loss at epoch 51: 0.8969761431217194
[08/27/2025 18:55:14 INFO]: Training loss at epoch 31: 1.1302251517772675
[08/27/2025 18:55:23 INFO]: Training loss at epoch 40: 0.9054163992404938
[08/27/2025 18:55:41 INFO]: Training loss at epoch 21: 1.0396710932254791
[08/27/2025 18:55:51 INFO]: Training loss at epoch 48: 0.9842879176139832
[08/27/2025 18:55:54 INFO]: Training loss at epoch 11: 1.1037684381008148
[08/27/2025 18:55:57 INFO]: Training loss at epoch 22: 1.068575382232666
[08/27/2025 18:56:03 INFO]: Training loss at epoch 53: 0.8206970393657684
[08/27/2025 18:56:11 INFO]: New best epoch, val score: -0.6624894323065897
[08/27/2025 18:56:11 INFO]: Saving model to: unhanged-Shanesha_trial_39/model_best.pth
[08/27/2025 18:56:13 INFO]: Training loss at epoch 20: 1.1744693517684937
[08/27/2025 18:56:26 INFO]: Training loss at epoch 37: 1.0761934518814087
[08/27/2025 18:56:50 INFO]: Training loss at epoch 5: 0.959505707025528
[08/27/2025 18:57:01 INFO]: Training loss at epoch 35: 1.0249934494495392
[08/27/2025 18:57:02 INFO]: Training loss at epoch 32: 0.8739190101623535
[08/27/2025 18:57:06 INFO]: New best epoch, val score: -0.6643834005046859
[08/27/2025 18:57:06 INFO]: Saving model to: unhanged-Shanesha_trial_40/model_best.pth
[08/27/2025 18:57:40 INFO]: Training loss at epoch 48: 1.1335412561893463
[08/27/2025 18:57:53 INFO]: Training loss at epoch 54: 1.1736966371536255
[08/27/2025 18:58:06 INFO]: Training loss at epoch 12: 0.9013391733169556
[08/27/2025 18:58:09 INFO]: Training loss at epoch 35: 0.9817496836185455
[08/27/2025 18:58:15 INFO]: Training loss at epoch 38: 1.1539814472198486
[08/27/2025 18:58:41 INFO]: Training loss at epoch 48: 0.9229481816291809
[08/27/2025 18:58:42 INFO]: Training loss at epoch 42: 0.8044876456260681
[08/27/2025 18:58:43 INFO]: Training loss at epoch 52: 0.8331747949123383
[08/27/2025 18:58:50 INFO]: Training loss at epoch 33: 0.9818018674850464
[08/27/2025 18:58:51 INFO]: Training loss at epoch 49: 0.9348703026771545
[08/27/2025 18:58:56 INFO]: Training loss at epoch 21: 0.9035355448722839
[08/27/2025 18:59:03 INFO]: Training loss at epoch 6: 0.8471361398696899
[08/27/2025 18:59:16 INFO]: Running Final Evaluation...
[08/27/2025 18:59:19 INFO]: New best epoch, val score: -0.6631505085603229
[08/27/2025 18:59:19 INFO]: Saving model to: unhanged-Shanesha_trial_40/model_best.pth
[08/27/2025 18:59:38 INFO]: Training loss at epoch 52: 0.9439726173877716
[08/27/2025 18:59:41 INFO]: Training loss at epoch 55: 0.8870143592357635
[08/27/2025 18:59:42 INFO]: Training loss at epoch 48: 1.2136633396148682
[08/27/2025 18:59:53 INFO]: Training stats: {
    "score": -0.997315207986595,
    "rmse": 0.997315207986595
}
[08/27/2025 18:59:53 INFO]: Val stats: {
    "score": -0.6766916827554227,
    "rmse": 0.6766916827554227
}
[08/27/2025 18:59:53 INFO]: Test stats: {
    "score": -0.8706968273595888,
    "rmse": 0.8706968273595888
}
[08/27/2025 19:00:04 INFO]: Training loss at epoch 39: 1.0861813724040985
[08/27/2025 19:00:12 INFO]: Running Final Evaluation...
[08/27/2025 19:00:16 INFO]: Training loss at epoch 13: 1.1289451718330383
[08/27/2025 19:00:37 INFO]: Training loss at epoch 34: 1.0466919541358948
[08/27/2025 19:00:41 INFO]: Training stats: {
    "score": -1.008457785080727,
    "rmse": 1.008457785080727
}
[08/27/2025 19:00:41 INFO]: Val stats: {
    "score": -0.6618211277868824,
    "rmse": 0.6618211277868824
}
[08/27/2025 19:00:41 INFO]: Test stats: {
    "score": -0.8700538759401547,
    "rmse": 0.8700538759401547
}
[08/27/2025 19:00:46 INFO]: Training loss at epoch 41: 0.9913885295391083
[08/27/2025 19:01:00 INFO]: Training loss at epoch 49: 1.0404359698295593
[08/27/2025 19:01:13 INFO]: Training accuracy: {
    "score": -1.0096495876145068,
    "rmse": 1.0096495876145068
}
[08/27/2025 19:01:13 INFO]: Val accuracy: {
    "score": -0.6609069778035969,
    "rmse": 0.6609069778035969
}
[08/27/2025 19:01:13 INFO]: Test accuracy: {
    "score": -0.8617619113350098,
    "rmse": 0.8617619113350098
}
[08/27/2025 19:01:13 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_5",
    "best_epoch": 17,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8617619113350098,
        "rmse": 0.8617619113350098
    },
    "train_stats": {
        "score": -1.0096495876145068,
        "rmse": 1.0096495876145068
    },
    "val_stats": {
        "score": -0.6609069778035969,
        "rmse": 0.6609069778035969
    }
}
[08/27/2025 19:01:13 INFO]: Procewss finished for trial unhanged-Shanesha_trial_5
[08/27/2025 19:01:13 INFO]: 
_________________________________________________

[08/27/2025 19:01:13 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:01:13 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 2.0866606575287876
  attention_dropout: 0.39551946309083236
  ffn_dropout: 0.39551946309083236
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0002632923914563376
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_41

[08/27/2025 19:01:13 INFO]: This ft_transformer has 13.057 million parameters.
[08/27/2025 19:01:13 INFO]: Training will start at epoch 0.
[08/27/2025 19:01:13 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:01:15 INFO]: Training loss at epoch 7: 1.1760997772216797
[08/27/2025 19:01:20 INFO]: Training loss at epoch 36: 1.0422484278678894
[08/27/2025 19:01:29 INFO]: Training loss at epoch 56: 1.1665329039096832
[08/27/2025 19:01:31 INFO]: New best epoch, val score: -0.6625306570164923
[08/27/2025 19:01:31 INFO]: Saving model to: unhanged-Shanesha_trial_40/model_best.pth
[08/27/2025 19:01:34 INFO]: Training loss at epoch 36: 1.027607262134552
[08/27/2025 19:01:38 INFO]: Training loss at epoch 22: 0.992486834526062
[08/27/2025 19:01:56 INFO]: Training accuracy: {
    "score": -1.0183456703726699,
    "rmse": 1.0183456703726699
}
[08/27/2025 19:01:56 INFO]: Val accuracy: {
    "score": -0.6607157995662696,
    "rmse": 0.6607157995662696
}
[08/27/2025 19:01:56 INFO]: Test accuracy: {
    "score": -0.8735915315713502,
    "rmse": 0.8735915315713502
}
[08/27/2025 19:01:56 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_2",
    "best_epoch": 21,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8735915315713502,
        "rmse": 0.8735915315713502
    },
    "train_stats": {
        "score": -1.0183456703726699,
        "rmse": 1.0183456703726699
    },
    "val_stats": {
        "score": -0.6607157995662696,
        "rmse": 0.6607157995662696
    }
}
[08/27/2025 19:01:56 INFO]: Procewss finished for trial unhanged-Shanesha_trial_2
[08/27/2025 19:01:56 INFO]: 
_________________________________________________

[08/27/2025 19:01:56 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:01:56 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 2.163463031447632
  attention_dropout: 0.40746475705132806
  ffn_dropout: 0.40746475705132806
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00011642892084184199
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_42

[08/27/2025 19:01:56 INFO]: This ft_transformer has 13.297 million parameters.
[08/27/2025 19:01:56 INFO]: Training will start at epoch 0.
[08/27/2025 19:01:56 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:02:09 INFO]: Training stats: {
    "score": -0.9933304515496874,
    "rmse": 0.9933304515496874
}
[08/27/2025 19:02:09 INFO]: Val stats: {
    "score": -0.6694569885945444,
    "rmse": 0.6694569885945444
}
[08/27/2025 19:02:09 INFO]: Test stats: {
    "score": -0.8711232916100466,
    "rmse": 0.8711232916100466
}
[08/27/2025 19:02:25 INFO]: Training loss at epoch 35: 0.8470832109451294
[08/27/2025 19:02:26 INFO]: Training loss at epoch 14: 1.00172558426857
[08/27/2025 19:02:30 INFO]: Training loss at epoch 40: 0.9749010503292084
[08/27/2025 19:02:50 INFO]: Training loss at epoch 50: 1.0659624636173248
[08/27/2025 19:03:18 INFO]: Training loss at epoch 57: 1.1481936573982239
[08/27/2025 19:03:19 INFO]: Training loss at epoch 53: 0.9227572679519653
[08/27/2025 19:03:27 INFO]: Training loss at epoch 8: 0.8209381103515625
[08/27/2025 19:03:43 INFO]: New best epoch, val score: -0.6624624078318736
[08/27/2025 19:03:43 INFO]: Saving model to: unhanged-Shanesha_trial_40/model_best.pth
[08/27/2025 19:03:52 INFO]: Training loss at epoch 23: 0.950801432132721
[08/27/2025 19:04:18 INFO]: Training loss at epoch 36: 1.314077228307724
[08/27/2025 19:04:22 INFO]: Training loss at epoch 41: 0.9402295351028442
[08/27/2025 19:04:24 INFO]: Training loss at epoch 23: 1.1766210496425629
[08/27/2025 19:04:29 INFO]: Training loss at epoch 43: 1.0493496358394623
[08/27/2025 19:04:31 INFO]: Training loss at epoch 20: 0.9156032502651215
[08/27/2025 19:04:42 INFO]: Training loss at epoch 15: 1.025235891342163
[08/27/2025 19:04:51 INFO]: Training loss at epoch 49: 1.0909754633903503
[08/27/2025 19:04:57 INFO]: New best epoch, val score: -0.6610264117884191
[08/27/2025 19:04:57 INFO]: Saving model to: unhanged-Shanesha_trial_39/model_best.pth
[08/27/2025 19:05:01 INFO]: Training loss at epoch 36: 1.2475483417510986
[08/27/2025 19:05:10 INFO]: Training loss at epoch 58: 1.0849820971488953
[08/27/2025 19:05:34 INFO]: Training loss at epoch 50: 0.9617535769939423
[08/27/2025 19:05:44 INFO]: Training loss at epoch 37: 1.039520800113678
[08/27/2025 19:05:44 INFO]: Training loss at epoch 9: 1.022779405117035
[08/27/2025 19:05:52 INFO]: Training loss at epoch 51: 0.9697538018226624
[08/27/2025 19:05:58 INFO]: Running Final Evaluation...
[08/27/2025 19:06:05 INFO]: Training loss at epoch 37: 0.9663485884666443
[08/27/2025 19:06:09 INFO]: Training loss at epoch 42: 0.9563083052635193
[08/27/2025 19:06:10 INFO]: Training loss at epoch 42: 1.105969399213791
[08/27/2025 19:06:22 INFO]: Running Final Evaluation...
[08/27/2025 19:06:29 INFO]: Training stats: {
    "score": -1.0050598142023046,
    "rmse": 1.0050598142023046
}
[08/27/2025 19:06:29 INFO]: Val stats: {
    "score": -0.6629352826242282,
    "rmse": 0.6629352826242282
}
[08/27/2025 19:06:29 INFO]: Test stats: {
    "score": -0.8680455083682311,
    "rmse": 0.8680455083682311
}
[08/27/2025 19:06:35 INFO]: Training stats: {
    "score": -0.9937043056420067,
    "rmse": 0.9937043056420067
}
[08/27/2025 19:06:35 INFO]: Val stats: {
    "score": -0.6903058641191361,
    "rmse": 0.6903058641191361
}
[08/27/2025 19:06:35 INFO]: Test stats: {
    "score": -0.8779555140393838,
    "rmse": 0.8779555140393838
}
[08/27/2025 19:06:41 INFO]: Training loss at epoch 22: 1.1272576749324799
[08/27/2025 19:06:52 INFO]: Training loss at epoch 16: 0.9646824896335602
[08/27/2025 19:06:58 INFO]: Training loss at epoch 59: 1.126903474330902
[08/27/2025 19:07:00 INFO]: Training loss at epoch 0: 1.193115234375
[08/27/2025 19:07:01 INFO]: Training accuracy: {
    "score": -1.0121396321655567,
    "rmse": 1.0121396321655567
}
[08/27/2025 19:07:01 INFO]: Val accuracy: {
    "score": -0.6608595983448524,
    "rmse": 0.6608595983448524
}
[08/27/2025 19:07:01 INFO]: Test accuracy: {
    "score": -0.8706681715100459,
    "rmse": 0.8706681715100459
}
[08/27/2025 19:07:01 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_36",
    "best_epoch": 11,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8706681715100459,
        "rmse": 0.8706681715100459
    },
    "train_stats": {
        "score": -1.0121396321655567,
        "rmse": 1.0121396321655567
    },
    "val_stats": {
        "score": -0.6608595983448524,
        "rmse": 0.6608595983448524
    }
}
[08/27/2025 19:07:01 INFO]: Procewss finished for trial unhanged-Shanesha_trial_36
[08/27/2025 19:07:01 INFO]: 
_________________________________________________

[08/27/2025 19:07:01 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:07:01 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 2.1399012350143356
  attention_dropout: 0.3906486093593546
  ffn_dropout: 0.3906486093593546
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00010527820770310747
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_43

[08/27/2025 19:07:01 INFO]: This ft_transformer has 13.223 million parameters.
[08/27/2025 19:07:01 INFO]: Training will start at epoch 0.
[08/27/2025 19:07:01 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:07:06 INFO]: Training loss at epoch 24: 0.9992048144340515
[08/27/2025 19:07:10 INFO]: Training accuracy: {
    "score": -1.0134646988520837,
    "rmse": 1.0134646988520837
}
[08/27/2025 19:07:10 INFO]: Val accuracy: {
    "score": -0.661507719921587,
    "rmse": 0.661507719921587
}
[08/27/2025 19:07:10 INFO]: Test accuracy: {
    "score": -0.8729968313589651,
    "rmse": 0.8729968313589651
}
[08/27/2025 19:07:10 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_29",
    "best_epoch": 19,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8729968313589651,
        "rmse": 0.8729968313589651
    },
    "train_stats": {
        "score": -1.0134646988520837,
        "rmse": 1.0134646988520837
    },
    "val_stats": {
        "score": -0.661507719921587,
        "rmse": 0.661507719921587
    }
}
[08/27/2025 19:07:10 INFO]: Procewss finished for trial unhanged-Shanesha_trial_29
[08/27/2025 19:07:10 INFO]: 
_________________________________________________

[08/27/2025 19:07:10 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:07:10 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 2.1126969339176425
  attention_dropout: 0.381889734911788
  ffn_dropout: 0.381889734911788
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 9.970436729977251e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_44

[08/27/2025 19:07:10 INFO]: This ft_transformer has 13.137 million parameters.
[08/27/2025 19:07:10 INFO]: Training will start at epoch 0.
[08/27/2025 19:07:10 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:07:35 INFO]: Training stats: {
    "score": -0.9967680885423411,
    "rmse": 0.9967680885423411
}
[08/27/2025 19:07:35 INFO]: Val stats: {
    "score": -0.6772340711458994,
    "rmse": 0.6772340711458994
}
[08/27/2025 19:07:35 INFO]: Test stats: {
    "score": -0.871748177034595,
    "rmse": 0.871748177034595
}
[08/27/2025 19:07:46 INFO]: New best epoch, val score: -0.7589670811593813
[08/27/2025 19:07:46 INFO]: Saving model to: unhanged-Shanesha_trial_41/model_best.pth
[08/27/2025 19:07:47 INFO]: Training loss at epoch 0: 1.0656397938728333
[08/27/2025 19:07:52 INFO]: Training loss at epoch 38: 1.1855366826057434
[08/27/2025 19:08:01 INFO]: Training loss at epoch 54: 0.9895516037940979
[08/27/2025 19:08:21 INFO]: Training loss at epoch 37: 1.1684930324554443
[08/27/2025 19:08:37 INFO]: New best epoch, val score: -0.9185036401235335
[08/27/2025 19:08:37 INFO]: Saving model to: unhanged-Shanesha_trial_42/model_best.pth
[08/27/2025 19:08:43 INFO]: Training loss at epoch 10: 1.2551214098930359
[08/27/2025 19:08:52 INFO]: Training loss at epoch 52: 0.827173113822937
[08/27/2025 19:09:05 INFO]: Training loss at epoch 17: 1.0249576568603516
[08/27/2025 19:09:27 INFO]: Training loss at epoch 60: 1.103295385837555
[08/27/2025 19:09:42 INFO]: Training loss at epoch 39: 1.1804860830307007
[08/27/2025 19:09:51 INFO]: Training loss at epoch 25: 1.0523998141288757
[08/27/2025 19:10:08 INFO]: Training loss at epoch 38: 1.1222306489944458
[08/27/2025 19:10:15 INFO]: Training loss at epoch 44: 0.9083623588085175
[08/27/2025 19:10:22 INFO]: Training stats: {
    "score": -0.9972144064413487,
    "rmse": 0.9972144064413487
}
[08/27/2025 19:10:22 INFO]: Val stats: {
    "score": -0.6822378486777881,
    "rmse": 0.6822378486777881
}
[08/27/2025 19:10:22 INFO]: Test stats: {
    "score": -0.8762931729235187,
    "rmse": 0.8762931729235187
}
[08/27/2025 19:10:57 INFO]: Training loss at epoch 11: 0.9207172095775604
[08/27/2025 19:11:18 INFO]: Training loss at epoch 18: 1.1386255621910095
[08/27/2025 19:11:18 INFO]: Training loss at epoch 61: 1.1136508285999298
[08/27/2025 19:11:37 INFO]: Training loss at epoch 43: 0.9190524220466614
[08/27/2025 19:11:42 INFO]: Training loss at epoch 50: 0.9192256033420563
[08/27/2025 19:11:52 INFO]: Training loss at epoch 53: 1.083155632019043
[08/27/2025 19:11:56 INFO]: Training loss at epoch 24: 0.929313600063324
[08/27/2025 19:11:58 INFO]: Training loss at epoch 37: 0.9477565884590149
[08/27/2025 19:12:12 INFO]: Training loss at epoch 40: 1.1789022982120514
[08/27/2025 19:12:38 INFO]: Training loss at epoch 26: 1.3107141256332397
[08/27/2025 19:12:45 INFO]: Training loss at epoch 55: 0.793970376253128
[08/27/2025 19:12:53 INFO]: Training loss at epoch 0: 1.8524069786071777
[08/27/2025 19:13:00 INFO]: Training loss at epoch 0: 0.9058890044689178
[08/27/2025 19:13:08 INFO]: Training loss at epoch 62: 0.893393486738205
[08/27/2025 19:13:11 INFO]: Training loss at epoch 12: 1.075173258781433
[08/27/2025 19:13:30 INFO]: Training loss at epoch 19: 0.9740342497825623
[08/27/2025 19:13:36 INFO]: Training loss at epoch 1: 4.18907368183136
[08/27/2025 19:13:39 INFO]: New best epoch, val score: -1.4206477994519753
[08/27/2025 19:13:39 INFO]: Saving model to: unhanged-Shanesha_trial_43/model_best.pth
[08/27/2025 19:13:47 INFO]: New best epoch, val score: -0.7273351785006593
[08/27/2025 19:13:47 INFO]: Saving model to: unhanged-Shanesha_trial_44/model_best.pth
[08/27/2025 19:14:00 INFO]: Training loss at epoch 41: 0.9988594651222229
[08/27/2025 19:14:14 INFO]: Training stats: {
    "score": -1.0045195550455255,
    "rmse": 1.0045195550455255
}
[08/27/2025 19:14:14 INFO]: Val stats: {
    "score": -0.7123522682876925,
    "rmse": 0.7123522682876925
}
[08/27/2025 19:14:14 INFO]: Test stats: {
    "score": -0.890864929160647,
    "rmse": 0.890864929160647
}
[08/27/2025 19:14:27 INFO]: Training loss at epoch 1: 1.8666106462478638
[08/27/2025 19:14:30 INFO]: Training loss at epoch 39: 0.9265093207359314
[08/27/2025 19:14:48 INFO]: Training loss at epoch 54: 0.9529359042644501
[08/27/2025 19:14:58 INFO]: Training loss at epoch 63: 1.0403651297092438
[08/27/2025 19:15:11 INFO]: Training loss at epoch 38: 1.134809285402298
[08/27/2025 19:15:13 INFO]: Running Final Evaluation...
[08/27/2025 19:15:23 INFO]: Training loss at epoch 27: 0.9111035466194153
[08/27/2025 19:15:25 INFO]: Training loss at epoch 13: 0.8148974478244781
[08/27/2025 19:15:29 INFO]: Training loss at epoch 21: 1.0774449706077576
[08/27/2025 19:15:49 INFO]: Training loss at epoch 42: 0.9729503393173218
[08/27/2025 19:16:02 INFO]: Training stats: {
    "score": -0.997864706691854,
    "rmse": 0.997864706691854
}
[08/27/2025 19:16:02 INFO]: Val stats: {
    "score": -0.6911281682133495,
    "rmse": 0.6911281682133495
}
[08/27/2025 19:16:02 INFO]: Test stats: {
    "score": -0.8803267393198625,
    "rmse": 0.8803267393198625
}
[08/27/2025 19:16:02 INFO]: Training loss at epoch 45: 1.2413432598114014
[08/27/2025 19:16:17 INFO]: Training accuracy: {
    "score": -1.0160884292695547,
    "rmse": 1.0160884292695547
}
[08/27/2025 19:16:17 INFO]: Val accuracy: {
    "score": -0.6628216332613188,
    "rmse": 0.6628216332613188
}
[08/27/2025 19:16:17 INFO]: Test accuracy: {
    "score": -0.8719871075115205,
    "rmse": 0.8719871075115205
}
[08/27/2025 19:16:17 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_31",
    "best_epoch": 23,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8719871075115205,
        "rmse": 0.8719871075115205
    },
    "train_stats": {
        "score": -1.0160884292695547,
        "rmse": 1.0160884292695547
    },
    "val_stats": {
        "score": -0.6628216332613188,
        "rmse": 0.6628216332613188
    }
}
[08/27/2025 19:16:17 INFO]: Procewss finished for trial unhanged-Shanesha_trial_31
[08/27/2025 19:16:17 INFO]: 
_________________________________________________

[08/27/2025 19:16:17 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:16:17 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 2.1141599412349517
  attention_dropout: 0.3788994455492753
  ffn_dropout: 0.3788994455492753
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0009762978425453891
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_45

[08/27/2025 19:16:17 INFO]: This ft_transformer has 13.143 million parameters.
[08/27/2025 19:16:17 INFO]: Training will start at epoch 0.
[08/27/2025 19:16:17 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:16:26 INFO]: Training loss at epoch 20: 0.809958815574646
[08/27/2025 19:16:47 INFO]: Training loss at epoch 64: 1.007021427154541
[08/27/2025 19:16:49 INFO]: Training loss at epoch 51: 0.9229182004928589
[08/27/2025 19:17:01 INFO]: Training loss at epoch 44: 0.8581935465335846
[08/27/2025 19:17:24 INFO]: Training loss at epoch 56: 1.0873058140277863
[08/27/2025 19:17:38 INFO]: Training loss at epoch 43: 0.9642356336116791
[08/27/2025 19:17:39 INFO]: Training loss at epoch 14: 1.1946184039115906
[08/27/2025 19:17:52 INFO]: New best epoch, val score: -0.6600973704748018
[08/27/2025 19:17:52 INFO]: Saving model to: unhanged-Shanesha_trial_37/model_best.pth
[08/27/2025 19:17:53 INFO]: Training loss at epoch 23: 0.9958278238773346
[08/27/2025 19:18:07 INFO]: Training loss at epoch 28: 1.004219889640808
[08/27/2025 19:18:37 INFO]: Training loss at epoch 65: 0.9323572814464569
[08/27/2025 19:18:38 INFO]: Training loss at epoch 21: 0.7687894552946091
[08/27/2025 19:18:50 INFO]: Training loss at epoch 38: 1.062511384487152
[08/27/2025 19:19:26 INFO]: Training loss at epoch 44: 1.0662736296653748
[08/27/2025 19:19:29 INFO]: Training loss at epoch 1: 1.7063429355621338
[08/27/2025 19:19:35 INFO]: Training loss at epoch 1: 1.437635362148285
[08/27/2025 19:19:51 INFO]: Training loss at epoch 15: 1.01556995511055
[08/27/2025 19:19:55 INFO]: Training loss at epoch 25: 1.2067572176456451
[08/27/2025 19:20:08 INFO]: Training loss at epoch 2: 2.109571695327759
[08/27/2025 19:20:21 INFO]: New best epoch, val score: -0.667207506721703
[08/27/2025 19:20:21 INFO]: Saving model to: unhanged-Shanesha_trial_44/model_best.pth
[08/27/2025 19:20:25 INFO]: Training loss at epoch 66: 0.9126496613025665
[08/27/2025 19:20:26 INFO]: Training loss at epoch 40: 0.9420589506626129
[08/27/2025 19:20:48 INFO]: Training loss at epoch 29: 0.9079375863075256
[08/27/2025 19:20:48 INFO]: Training loss at epoch 22: 0.9831016659736633
[08/27/2025 19:21:04 INFO]: Training loss at epoch 2: 1.5128720998764038
[08/27/2025 19:21:14 INFO]: Training loss at epoch 45: 0.9644282758235931
[08/27/2025 19:21:43 INFO]: Training stats: {
    "score": -1.000249269331975,
    "rmse": 1.000249269331975
}
[08/27/2025 19:21:43 INFO]: Val stats: {
    "score": -0.6720218807546059,
    "rmse": 0.6720218807546059
}
[08/27/2025 19:21:43 INFO]: Test stats: {
    "score": -0.870853974397766,
    "rmse": 0.870853974397766
}
[08/27/2025 19:21:45 INFO]: Training loss at epoch 46: 0.951655387878418
[08/27/2025 19:21:54 INFO]: Training loss at epoch 39: 0.8732864558696747
[08/27/2025 19:21:55 INFO]: Training loss at epoch 52: 1.0819637775421143
[08/27/2025 19:22:01 INFO]: Training loss at epoch 57: 0.9512235224246979
[08/27/2025 19:22:02 INFO]: Training loss at epoch 0: 0.9957655668258667
[08/27/2025 19:22:03 INFO]: Training loss at epoch 16: 1.1367696523666382
[08/27/2025 19:22:13 INFO]: Training loss at epoch 67: 1.1207085251808167
[08/27/2025 19:22:24 INFO]: Training loss at epoch 45: 1.0481210350990295
[08/27/2025 19:22:36 INFO]: Running Final Evaluation...
[08/27/2025 19:22:52 INFO]: New best epoch, val score: -0.6590789475364406
[08/27/2025 19:22:52 INFO]: Saving model to: unhanged-Shanesha_trial_45/model_best.pth
[08/27/2025 19:23:01 INFO]: Training loss at epoch 23: 1.2592352330684662
[08/27/2025 19:23:03 INFO]: Training loss at epoch 46: 1.0518218874931335
[08/27/2025 19:24:04 INFO]: Training loss at epoch 68: 1.074357807636261
[08/27/2025 19:24:12 INFO]: Training stats: {
    "score": -1.0063966893185439,
    "rmse": 1.0063966893185439
}
[08/27/2025 19:24:12 INFO]: Val stats: {
    "score": -0.6618704405616233,
    "rmse": 0.6618704405616233
}
[08/27/2025 19:24:12 INFO]: Test stats: {
    "score": -0.8688635630489214,
    "rmse": 0.8688635630489214
}
[08/27/2025 19:24:17 INFO]: Training loss at epoch 17: 0.9369329214096069
[08/27/2025 19:24:24 INFO]: Training accuracy: {
    "score": -1.014594879689904,
    "rmse": 1.014594879689904
}
[08/27/2025 19:24:24 INFO]: Val accuracy: {
    "score": -0.6606878032744552,
    "rmse": 0.6606878032744552
}
[08/27/2025 19:24:24 INFO]: Test accuracy: {
    "score": -0.8719132927780989,
    "rmse": 0.8719132927780989
}
[08/27/2025 19:24:24 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_3",
    "best_epoch": 26,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8719132927780989,
        "rmse": 0.8719132927780989
    },
    "train_stats": {
        "score": -1.014594879689904,
        "rmse": 1.014594879689904
    },
    "val_stats": {
        "score": -0.6606878032744552,
        "rmse": 0.6606878032744552
    }
}
[08/27/2025 19:24:24 INFO]: Procewss finished for trial unhanged-Shanesha_trial_3
[08/27/2025 19:24:24 INFO]: 
_________________________________________________

[08/27/2025 19:24:24 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:24:24 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 2.166077371662075
  attention_dropout: 0.4011285272259984
  ffn_dropout: 0.4011285272259984
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00011113056421056964
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_46

[08/27/2025 19:24:24 INFO]: This ft_transformer has 13.309 million parameters.
[08/27/2025 19:24:24 INFO]: Training will start at epoch 0.
[08/27/2025 19:24:24 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:24:27 INFO]: Training loss at epoch 30: 1.2380402386188507
[08/27/2025 19:24:46 INFO]: Training loss at epoch 41: 0.9569920599460602
[08/27/2025 19:24:50 INFO]: Training loss at epoch 47: 0.9407302141189575
[08/27/2025 19:25:11 INFO]: Training loss at epoch 24: 0.8493049740791321
[08/27/2025 19:25:38 INFO]: Training loss at epoch 39: 0.9310770332813263
[08/27/2025 19:25:52 INFO]: Training loss at epoch 69: 0.9352726340293884
[08/27/2025 19:26:04 INFO]: Training loss at epoch 2: 2.559173583984375
[08/27/2025 19:26:09 INFO]: Training loss at epoch 2: 1.7976654767990112
[08/27/2025 19:26:25 INFO]: Training loss at epoch 22: 1.2304467558860779
[08/27/2025 19:26:31 INFO]: Training loss at epoch 18: 0.8078738749027252
[08/27/2025 19:26:31 INFO]: Training stats: {
    "score": -0.9964516401758695,
    "rmse": 0.9964516401758695
}
[08/27/2025 19:26:31 INFO]: Val stats: {
    "score": -0.6739716608489736,
    "rmse": 0.6739716608489736
}
[08/27/2025 19:26:31 INFO]: Test stats: {
    "score": -0.8704971813380877,
    "rmse": 0.8704971813380877
}
[08/27/2025 19:26:40 INFO]: Training loss at epoch 3: 1.5639978349208832
[08/27/2025 19:26:41 INFO]: Training loss at epoch 48: 1.1258498430252075
[08/27/2025 19:26:51 INFO]: New best epoch, val score: -0.6635699496949043
[08/27/2025 19:26:51 INFO]: Saving model to: unhanged-Shanesha_trial_43/model_best.pth
[08/27/2025 19:27:04 INFO]: Training loss at epoch 53: 1.2235841155052185
[08/27/2025 19:27:11 INFO]: Training loss at epoch 31: 0.9807183742523193
[08/27/2025 19:27:23 INFO]: Training loss at epoch 25: 0.8239633738994598
[08/27/2025 19:27:30 INFO]: Training loss at epoch 47: 0.9727900326251984
[08/27/2025 19:27:46 INFO]: Training loss at epoch 3: 1.5840423107147217
[08/27/2025 19:27:49 INFO]: Training loss at epoch 46: 1.2124264538288116
[08/27/2025 19:27:55 INFO]: Training loss at epoch 26: 1.1579184234142303
[08/27/2025 19:28:00 INFO]: Training stats: {
    "score": -0.9871626761361533,
    "rmse": 0.9871626761361533
}
[08/27/2025 19:28:00 INFO]: Val stats: {
    "score": -0.6666063689812737,
    "rmse": 0.6666063689812737
}
[08/27/2025 19:28:00 INFO]: Test stats: {
    "score": -0.8753081661937989,
    "rmse": 0.8753081661937989
}
[08/27/2025 19:28:21 INFO]: Training loss at epoch 70: 0.9339179694652557
[08/27/2025 19:28:26 INFO]: Running Final Evaluation...
[08/27/2025 19:28:31 INFO]: Training loss at epoch 49: 0.9898721873760223
[08/27/2025 19:28:42 INFO]: Training loss at epoch 1: 8.857403874397278
[08/27/2025 19:28:45 INFO]: Training loss at epoch 19: 0.9171170294284821
[08/27/2025 19:28:59 INFO]: Training loss at epoch 24: 0.8489990830421448
[08/27/2025 19:29:08 INFO]: Training stats: {
    "score": -0.9982292236976543,
    "rmse": 0.9982292236976543
}
[08/27/2025 19:29:08 INFO]: Val stats: {
    "score": -0.6973544027620321,
    "rmse": 0.6973544027620321
}
[08/27/2025 19:29:08 INFO]: Test stats: {
    "score": -0.8842905832765071,
    "rmse": 0.8842905832765071
}
[08/27/2025 19:29:09 INFO]: Training loss at epoch 42: 1.1367413997650146
[08/27/2025 19:29:30 INFO]: Training stats: {
    "score": -1.0076568831988046,
    "rmse": 1.0076568831988046
}
[08/27/2025 19:29:30 INFO]: Val stats: {
    "score": -0.6612109511490848,
    "rmse": 0.6612109511490848
}
[08/27/2025 19:29:30 INFO]: Test stats: {
    "score": -0.8687097649163712,
    "rmse": 0.8687097649163712
}
[08/27/2025 19:29:36 INFO]: Training loss at epoch 26: 0.9569175541400909
[08/27/2025 19:29:46 INFO]: New best epoch, val score: -0.6612109511490848
[08/27/2025 19:29:46 INFO]: Saving model to: unhanged-Shanesha_trial_40/model_best.pth
[08/27/2025 19:29:54 INFO]: Training loss at epoch 32: 0.8280325829982758
[08/27/2025 19:30:09 INFO]: Training loss at epoch 71: 1.1220988631248474
[08/27/2025 19:30:15 INFO]: Training loss at epoch 0: 1.057084321975708
[08/27/2025 19:30:24 INFO]: Training accuracy: {
    "score": -1.0148564692314825,
    "rmse": 1.0148564692314825
}
[08/27/2025 19:30:24 INFO]: Val accuracy: {
    "score": -0.6607989814301645,
    "rmse": 0.6607989814301645
}
[08/27/2025 19:30:24 INFO]: Test accuracy: {
    "score": -0.8725372391611006,
    "rmse": 0.8725372391611006
}
[08/27/2025 19:30:24 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_21",
    "best_epoch": 15,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8725372391611006,
        "rmse": 0.8725372391611006
    },
    "train_stats": {
        "score": -1.0148564692314825,
        "rmse": 1.0148564692314825
    },
    "val_stats": {
        "score": -0.6607989814301645,
        "rmse": 0.6607989814301645
    }
}
[08/27/2025 19:30:24 INFO]: Procewss finished for trial unhanged-Shanesha_trial_21
[08/27/2025 19:30:24 INFO]: 
_________________________________________________

[08/27/2025 19:30:24 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:30:24 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 2.226817327723273
  attention_dropout: 0.3725544781476954
  ffn_dropout: 0.3725544781476954
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0009454480110041862
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_47

[08/27/2025 19:30:24 INFO]: This ft_transformer has 13.500 million parameters.
[08/27/2025 19:30:24 INFO]: Training will start at epoch 0.
[08/27/2025 19:30:24 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:30:55 INFO]: Training loss at epoch 50: 0.8021415770053864
[08/27/2025 19:30:57 INFO]: Training loss at epoch 40: 0.8098697662353516
[08/27/2025 19:31:02 INFO]: New best epoch, val score: -0.659782939967528
[08/27/2025 19:31:02 INFO]: Saving model to: unhanged-Shanesha_trial_46/model_best.pth
[08/27/2025 19:31:44 INFO]: Training loss at epoch 20: 1.0331079959869385
[08/27/2025 19:31:48 INFO]: Training loss at epoch 27: 1.1296638250350952
[08/27/2025 19:32:00 INFO]: Training loss at epoch 72: 0.7797015011310577
[08/27/2025 19:32:00 INFO]: New best epoch, val score: -0.6605185842444363
[08/27/2025 19:32:00 INFO]: Saving model to: unhanged-Shanesha_trial_40/model_best.pth
[08/27/2025 19:32:11 INFO]: Training loss at epoch 54: 1.0553596019744873
[08/27/2025 19:32:38 INFO]: Training loss at epoch 33: 0.8972094357013702
[08/27/2025 19:32:40 INFO]: Training loss at epoch 3: 1.2148306369781494
[08/27/2025 19:32:43 INFO]: Training loss at epoch 3: 1.3312699794769287
[08/27/2025 19:32:45 INFO]: Training loss at epoch 51: 0.9641733467578888
[08/27/2025 19:32:48 INFO]: Running Final Evaluation...
[08/27/2025 19:33:13 INFO]: Training loss at epoch 4: 1.2176303267478943
[08/27/2025 19:33:15 INFO]: Training loss at epoch 48: 0.8632474541664124
[08/27/2025 19:33:30 INFO]: Training loss at epoch 43: 0.9384171664714813
[08/27/2025 19:33:49 INFO]: Training loss at epoch 73: 0.7837669253349304
[08/27/2025 19:33:56 INFO]: Training loss at epoch 21: 1.0253987312316895
[08/27/2025 19:33:58 INFO]: Training loss at epoch 28: 1.0387337803840637
[08/27/2025 19:34:23 INFO]: Training loss at epoch 4: 1.1111389994621277
[08/27/2025 19:34:31 INFO]: Training loss at epoch 52: 1.2242087423801422
[08/27/2025 19:34:43 INFO]: Training accuracy: {
    "score": -1.0037902470270716,
    "rmse": 1.0037902470270716
}
[08/27/2025 19:34:43 INFO]: Val accuracy: {
    "score": -0.6655376551226425,
    "rmse": 0.6655376551226425
}
[08/27/2025 19:34:43 INFO]: Test accuracy: {
    "score": -0.8689907527008729,
    "rmse": 0.8689907527008729
}
[08/27/2025 19:34:43 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_11",
    "best_epoch": 23,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8689907527008729,
        "rmse": 0.8689907527008729
    },
    "train_stats": {
        "score": -1.0037902470270716,
        "rmse": 1.0037902470270716
    },
    "val_stats": {
        "score": -0.6655376551226425,
        "rmse": 0.6655376551226425
    }
}
[08/27/2025 19:34:43 INFO]: Procewss finished for trial unhanged-Shanesha_trial_11
[08/27/2025 19:34:43 INFO]: 
_________________________________________________

[08/27/2025 19:34:43 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:34:43 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 2.091971895571089
  attention_dropout: 0.3901557815320454
  ffn_dropout: 0.3901557815320454
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00011251321818781464
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_48

[08/27/2025 19:34:44 INFO]: This ft_transformer has 13.076 million parameters.
[08/27/2025 19:34:44 INFO]: Training will start at epoch 0.
[08/27/2025 19:34:44 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:34:48 INFO]: Training loss at epoch 40: 0.9517304301261902
[08/27/2025 19:35:11 INFO]: New best epoch, val score: -0.8912603973167628
[08/27/2025 19:35:11 INFO]: Saving model to: unhanged-Shanesha_trial_42/model_best.pth
[08/27/2025 19:35:14 INFO]: Training loss at epoch 2: 2.2151703238487244
[08/27/2025 19:35:22 INFO]: Training loss at epoch 34: 0.9349843859672546
[08/27/2025 19:35:37 INFO]: Training loss at epoch 74: 0.8481674194335938
[08/27/2025 19:35:49 INFO]: Training loss at epoch 27: 0.9523453712463379
[08/27/2025 19:36:07 INFO]: Training loss at epoch 22: 1.040900468826294
[08/27/2025 19:36:08 INFO]: Training loss at epoch 29: 1.1062329709529877
[08/27/2025 19:36:16 INFO]: Training loss at epoch 0: 1.2852301597595215
[08/27/2025 19:36:19 INFO]: Training loss at epoch 53: 0.844741940498352
[08/27/2025 19:36:51 INFO]: Training loss at epoch 1: 2.2111865878105164
[08/27/2025 19:36:51 INFO]: Training stats: {
    "score": -1.0119825767610455,
    "rmse": 1.0119825767610455
}
[08/27/2025 19:36:51 INFO]: Val stats: {
    "score": -0.7366748558415072,
    "rmse": 0.7366748558415072
}
[08/27/2025 19:36:51 INFO]: Test stats: {
    "score": -0.9062530128824131,
    "rmse": 0.9062530128824131
}
[08/27/2025 19:37:03 INFO]: New best epoch, val score: -0.6740126776181635
[08/27/2025 19:37:03 INFO]: Saving model to: unhanged-Shanesha_trial_47/model_best.pth
[08/27/2025 19:37:18 INFO]: Training loss at epoch 23: 1.232926607131958
[08/27/2025 19:37:25 INFO]: Training loss at epoch 75: 0.9333501756191254
[08/27/2025 19:37:41 INFO]: Training loss at epoch 41: 1.2396893501281738
[08/27/2025 19:37:49 INFO]: Training loss at epoch 44: 1.0756006240844727
[08/27/2025 19:38:05 INFO]: Training loss at epoch 35: 0.9466094672679901
[08/27/2025 19:38:06 INFO]: Training loss at epoch 54: 0.7665145248174667
[08/27/2025 19:38:19 INFO]: Training loss at epoch 23: 0.9730834662914276
[08/27/2025 19:38:27 INFO]: New best epoch, val score: -0.6604744458159462
[08/27/2025 19:38:27 INFO]: Saving model to: unhanged-Shanesha_trial_18/model_best.pth
[08/27/2025 19:38:55 INFO]: Training loss at epoch 49: 0.9521162807941437
[08/27/2025 19:39:02 INFO]: Training loss at epoch 30: 0.9676767587661743
[08/27/2025 19:39:09 INFO]: Training loss at epoch 4: 1.7042990326881409
[08/27/2025 19:39:11 INFO]: Training loss at epoch 4: 1.3376550674438477
[08/27/2025 19:39:13 INFO]: Training loss at epoch 76: 1.0303497910499573
[08/27/2025 19:39:40 INFO]: Training loss at epoch 5: 1.6310142278671265
[08/27/2025 19:39:53 INFO]: Training loss at epoch 55: 0.9775587022304535
[08/27/2025 19:39:59 INFO]: Training loss at epoch 25: 0.9028764367103577
[08/27/2025 19:40:28 INFO]: Training loss at epoch 0: 1.160205364227295
[08/27/2025 19:40:31 INFO]: Training loss at epoch 24: 0.9648391902446747
[08/27/2025 19:40:48 INFO]: Training loss at epoch 36: 0.818542093038559
[08/27/2025 19:40:54 INFO]: Training stats: {
    "score": -0.9887506556055033,
    "rmse": 0.9887506556055033
}
[08/27/2025 19:40:54 INFO]: Val stats: {
    "score": -0.6770571435745028,
    "rmse": 0.6770571435745028
}
[08/27/2025 19:40:54 INFO]: Test stats: {
    "score": -0.8864096920361668,
    "rmse": 0.8864096920361668
}
[08/27/2025 19:40:58 INFO]: Training loss at epoch 5: 1.2591385245323181
[08/27/2025 19:41:01 INFO]: Training loss at epoch 77: 0.8943570554256439
[08/27/2025 19:41:11 INFO]: Training loss at epoch 31: 1.1078500747680664
[08/27/2025 19:41:15 INFO]: New best epoch, val score: -0.6865443367885338
[08/27/2025 19:41:15 INFO]: Saving model to: unhanged-Shanesha_trial_48/model_best.pth
[08/27/2025 19:41:35 INFO]: Training loss at epoch 41: 0.9195581376552582
[08/27/2025 19:41:41 INFO]: Training loss at epoch 56: 1.1427652835845947
[08/27/2025 19:41:43 INFO]: Training loss at epoch 3: 1.4464269280433655
[08/27/2025 19:41:46 INFO]: New best epoch, val score: -0.8875643782497356
[08/27/2025 19:41:46 INFO]: Saving model to: unhanged-Shanesha_trial_42/model_best.pth
[08/27/2025 19:42:09 INFO]: Training loss at epoch 45: 0.9097070395946503
[08/27/2025 19:42:45 INFO]: Training loss at epoch 25: 1.214233547449112
[08/27/2025 19:42:51 INFO]: Training loss at epoch 78: 1.1385042369365692
[08/27/2025 19:42:55 INFO]: Training loss at epoch 1: 7.840737044811249
[08/27/2025 19:43:24 INFO]: Training loss at epoch 32: 1.0212471783161163
[08/27/2025 19:43:26 INFO]: Training loss at epoch 2: 1.3758018612861633
[08/27/2025 19:43:31 INFO]: Training loss at epoch 57: 0.8989085257053375
[08/27/2025 19:43:34 INFO]: Training loss at epoch 37: 0.9612806439399719
[08/27/2025 19:43:44 INFO]: Training loss at epoch 28: 0.9327225685119629
[08/27/2025 19:44:25 INFO]: Training loss at epoch 42: 0.8598573505878448
[08/27/2025 19:44:39 INFO]: Training loss at epoch 79: 0.9406948685646057
[08/27/2025 19:44:57 INFO]: Training loss at epoch 26: 1.3075935244560242
[08/27/2025 19:45:12 INFO]: New best epoch, val score: -0.660473896685105
[08/27/2025 19:45:12 INFO]: Saving model to: unhanged-Shanesha_trial_18/model_best.pth
[08/27/2025 19:45:16 INFO]: Training stats: {
    "score": -0.9970788785963951,
    "rmse": 0.9970788785963951
}
[08/27/2025 19:45:16 INFO]: Val stats: {
    "score": -0.7033580536150316,
    "rmse": 0.7033580536150316
}
[08/27/2025 19:45:16 INFO]: Test stats: {
    "score": -0.8851581903813479,
    "rmse": 0.8851581903813479
}
[08/27/2025 19:45:18 INFO]: Training loss at epoch 58: 0.8800177276134491
[08/27/2025 19:45:34 INFO]: Training loss at epoch 33: 1.2191277742385864
[08/27/2025 19:45:42 INFO]: Training loss at epoch 5: 1.1462706327438354
[08/27/2025 19:45:42 INFO]: Training loss at epoch 5: 1.2472703456878662
[08/27/2025 19:46:09 INFO]: Training loss at epoch 6: 1.2834040522575378
[08/27/2025 19:46:16 INFO]: Training loss at epoch 38: 0.8673727512359619
[08/27/2025 19:46:29 INFO]: Training loss at epoch 46: 1.0213956236839294
[08/27/2025 19:46:36 INFO]: Training loss at epoch 50: 1.2353323996067047
[08/27/2025 19:47:01 INFO]: Training loss at epoch 1: 1.8226640820503235
[08/27/2025 19:47:04 INFO]: Training loss at epoch 80: 0.9420810341835022
[08/27/2025 19:47:06 INFO]: Training loss at epoch 59: 1.2175337672233582
[08/27/2025 19:47:08 INFO]: Training loss at epoch 27: 1.0669901967048645
[08/27/2025 19:47:17 INFO]: Running Final Evaluation...
[08/27/2025 19:47:35 INFO]: Training loss at epoch 6: 1.6527920365333557
[08/27/2025 19:47:43 INFO]: Training stats: {
    "score": -1.007005312739045,
    "rmse": 1.007005312739045
}
[08/27/2025 19:47:43 INFO]: Val stats: {
    "score": -0.7325159245518214,
    "rmse": 0.7325159245518214
}
[08/27/2025 19:47:43 INFO]: Test stats: {
    "score": -0.9058133125889067,
    "rmse": 0.9058133125889067
}
[08/27/2025 19:47:44 INFO]: Training loss at epoch 34: 1.1497216820716858
[08/27/2025 19:47:56 INFO]: Training accuracy: {
    "score": -1.008427543090026,
    "rmse": 1.008427543090026
}
[08/27/2025 19:47:56 INFO]: Val accuracy: {
    "score": -0.6616529833912951,
    "rmse": 0.6616529833912951
}
[08/27/2025 19:47:56 INFO]: Test accuracy: {
    "score": -0.8696664714093288,
    "rmse": 0.8696664714093288
}
[08/27/2025 19:47:56 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_34",
    "best_epoch": 49,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8696664714093288,
        "rmse": 0.8696664714093288
    },
    "train_stats": {
        "score": -1.008427543090026,
        "rmse": 1.008427543090026
    },
    "val_stats": {
        "score": -0.6616529833912951,
        "rmse": 0.6616529833912951
    }
}
[08/27/2025 19:47:56 INFO]: Procewss finished for trial unhanged-Shanesha_trial_34
[08/27/2025 19:47:56 INFO]: 
_________________________________________________

[08/27/2025 19:47:56 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:47:56 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 2.0770511143582824
  attention_dropout: 0.3812390105157013
  ffn_dropout: 0.3812390105157013
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00010803562196591882
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_49

[08/27/2025 19:47:56 INFO]: This ft_transformer has 13.027 million parameters.
[08/27/2025 19:47:56 INFO]: Training will start at epoch 0.
[08/27/2025 19:47:56 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:48:09 INFO]: Training loss at epoch 24: 0.9787319600582123
[08/27/2025 19:48:15 INFO]: Training loss at epoch 4: 1.2802841663360596
[08/27/2025 19:48:23 INFO]: New best epoch, val score: -0.8692676224058257
[08/27/2025 19:48:23 INFO]: Saving model to: unhanged-Shanesha_trial_42/model_best.pth
[08/27/2025 19:48:27 INFO]: Training loss at epoch 42: 0.9173381924629211
[08/27/2025 19:48:57 INFO]: Training loss at epoch 39: 0.9899899661540985
[08/27/2025 19:49:20 INFO]: Training loss at epoch 28: 1.2579062581062317
[08/27/2025 19:49:30 INFO]: Training loss at epoch 60: 0.852524071931839
[08/27/2025 19:49:33 INFO]: Training loss at epoch 2: 2.1523359417915344
[08/27/2025 19:49:53 INFO]: Training stats: {
    "score": -1.0086183777165532,
    "rmse": 1.0086183777165532
}
[08/27/2025 19:49:53 INFO]: Val stats: {
    "score": -0.7261866672380698,
    "rmse": 0.7261866672380698
}
[08/27/2025 19:49:53 INFO]: Test stats: {
    "score": -0.9004865098278615,
    "rmse": 0.9004865098278615
}
[08/27/2025 19:49:54 INFO]: Training loss at epoch 35: 1.206545114517212
[08/27/2025 19:50:03 INFO]: Training loss at epoch 3: 1.6112870573997498
[08/27/2025 19:50:53 INFO]: Training loss at epoch 47: 0.7917616665363312
[08/27/2025 19:51:04 INFO]: Training loss at epoch 26: 0.9710972011089325
[08/27/2025 19:51:13 INFO]: Training loss at epoch 43: 0.9987002611160278
[08/27/2025 19:51:21 INFO]: Training loss at epoch 61: 0.847774475812912
[08/27/2025 19:51:36 INFO]: Training loss at epoch 29: 1.2197868824005127
[08/27/2025 19:51:47 INFO]: Training loss at epoch 29: 0.823339432477951
[08/27/2025 19:52:09 INFO]: Training loss at epoch 36: 0.8397154211997986
[08/27/2025 19:52:17 INFO]: Training loss at epoch 6: 1.094788372516632
[08/27/2025 19:52:18 INFO]: Training loss at epoch 6: 0.9985127449035645
[08/27/2025 19:52:21 INFO]: Training loss at epoch 51: 1.1882540881633759
[08/27/2025 19:52:21 INFO]: Training stats: {
    "score": -1.0015231136784821,
    "rmse": 1.0015231136784821
}
[08/27/2025 19:52:21 INFO]: Val stats: {
    "score": -0.7007320806091769,
    "rmse": 0.7007320806091769
}
[08/27/2025 19:52:21 INFO]: Test stats: {
    "score": -0.8845798200623877,
    "rmse": 0.8845798200623877
}
[08/27/2025 19:52:40 INFO]: Training loss at epoch 40: 0.897115170955658
[08/27/2025 19:52:42 INFO]: Training loss at epoch 7: 1.1081695258617401
[08/27/2025 19:53:08 INFO]: Training loss at epoch 62: 0.9121032953262329
[08/27/2025 19:53:35 INFO]: Training loss at epoch 2: 1.2718081176280975
[08/27/2025 19:53:43 INFO]: Training loss at epoch 0: 1.1369262337684631
[08/27/2025 19:54:13 INFO]: Training loss at epoch 7: 1.1116285920143127
[08/27/2025 19:54:19 INFO]: Training loss at epoch 37: 1.0294714570045471
[08/27/2025 19:54:29 INFO]: New best epoch, val score: -0.6827891560922836
[08/27/2025 19:54:29 INFO]: Saving model to: unhanged-Shanesha_trial_49/model_best.pth
[08/27/2025 19:54:31 INFO]: Training stats: {
    "score": -1.0010198956887852,
    "rmse": 1.0010198956887852
}
[08/27/2025 19:54:31 INFO]: Val stats: {
    "score": -0.6939764282708517,
    "rmse": 0.6939764282708517
}
[08/27/2025 19:54:31 INFO]: Test stats: {
    "score": -0.8809732742641331,
    "rmse": 0.8809732742641331
}
[08/27/2025 19:54:32 INFO]: Training loss at epoch 30: 0.7466120421886444
[08/27/2025 19:54:50 INFO]: Training loss at epoch 5: 1.080291986465454
[08/27/2025 19:54:55 INFO]: Training loss at epoch 63: 0.9333565831184387
[08/27/2025 19:55:00 INFO]: New best epoch, val score: -0.8418596870244788
[08/27/2025 19:55:00 INFO]: Saving model to: unhanged-Shanesha_trial_42/model_best.pth
[08/27/2025 19:55:15 INFO]: Training loss at epoch 48: 0.891981691122055
[08/27/2025 19:55:19 INFO]: Training loss at epoch 43: 0.8654535114765167
[08/27/2025 19:55:21 INFO]: Training loss at epoch 41: 1.0061063468456268
[08/27/2025 19:55:47 INFO]: Running Final Evaluation...
[08/27/2025 19:56:14 INFO]: Training loss at epoch 3: 1.4941481351852417
[08/27/2025 19:56:29 INFO]: Training loss at epoch 38: 1.168429434299469
[08/27/2025 19:56:40 INFO]: Training loss at epoch 4: 1.1675233840942383
[08/27/2025 19:56:42 INFO]: Training loss at epoch 64: 0.9721332788467407
[08/27/2025 19:56:43 INFO]: Training loss at epoch 31: 1.048824965953827
[08/27/2025 19:57:18 INFO]: Training accuracy: {
    "score": -1.0125211221949337,
    "rmse": 1.0125211221949337
}
[08/27/2025 19:57:18 INFO]: Val accuracy: {
    "score": -0.6615516569628243,
    "rmse": 0.6615516569628243
}
[08/27/2025 19:57:18 INFO]: Test accuracy: {
    "score": -0.871385784773804,
    "rmse": 0.871385784773804
}
[08/27/2025 19:57:18 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_30",
    "best_epoch": 17,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.871385784773804,
        "rmse": 0.871385784773804
    },
    "train_stats": {
        "score": -1.0125211221949337,
        "rmse": 1.0125211221949337
    },
    "val_stats": {
        "score": -0.6615516569628243,
        "rmse": 0.6615516569628243
    }
}
[08/27/2025 19:57:18 INFO]: Procewss finished for trial unhanged-Shanesha_trial_30
[08/27/2025 19:57:19 INFO]: 
_________________________________________________

[08/27/2025 19:57:19 INFO]: train_net_for_optune.py main() running.
[08/27/2025 19:57:19 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 2.1857417784501223
  attention_dropout: 0.3805095103901467
  ffn_dropout: 0.3805095103901467
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0001095251187265651
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_50

[08/27/2025 19:57:19 INFO]: This ft_transformer has 13.371 million parameters.
[08/27/2025 19:57:19 INFO]: Training will start at epoch 0.
[08/27/2025 19:57:19 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 19:57:56 INFO]: Training loss at epoch 44: 1.1269773244857788
[08/27/2025 19:58:02 INFO]: Training loss at epoch 52: 1.1036638021469116
[08/27/2025 19:58:04 INFO]: Training loss at epoch 42: 0.9962706565856934
[08/27/2025 19:58:31 INFO]: Training loss at epoch 65: 0.7579118460416794
[08/27/2025 19:58:42 INFO]: Training loss at epoch 39: 1.2832150161266327
[08/27/2025 19:58:48 INFO]: Training loss at epoch 7: 1.0628458857536316
[08/27/2025 19:58:50 INFO]: Training loss at epoch 7: 1.1990146040916443
[08/27/2025 19:58:56 INFO]: Training loss at epoch 32: 1.0219644606113434
[08/27/2025 19:59:03 INFO]: Training loss at epoch 25: 0.8744950890541077
[08/27/2025 19:59:10 INFO]: Training loss at epoch 8: 1.0958252847194672
[08/27/2025 19:59:25 INFO]: Training stats: {
    "score": -0.9987540776966499,
    "rmse": 0.9987540776966499
}
[08/27/2025 19:59:25 INFO]: Val stats: {
    "score": -0.6842828197299511,
    "rmse": 0.6842828197299511
}
[08/27/2025 19:59:25 INFO]: Test stats: {
    "score": -0.8755158822151755,
    "rmse": 0.8755158822151755
}
[08/27/2025 20:00:05 INFO]: Training loss at epoch 3: 1.8461782336235046
[08/27/2025 20:00:12 INFO]: Training loss at epoch 1: 2.0005403757095337
[08/27/2025 20:00:17 INFO]: Training loss at epoch 66: 0.958015114068985
[08/27/2025 20:00:46 INFO]: Training loss at epoch 43: 1.1578886806964874
[08/27/2025 20:00:48 INFO]: Training loss at epoch 8: 1.2890387177467346
[08/27/2025 20:00:52 INFO]: New best epoch, val score: -0.6620001578329218
[08/27/2025 20:00:52 INFO]: Saving model to: unhanged-Shanesha_trial_48/model_best.pth
[08/27/2025 20:01:07 INFO]: Training loss at epoch 33: 0.9431978464126587
[08/27/2025 20:01:22 INFO]: Training loss at epoch 6: 0.8267602324485779
[08/27/2025 20:01:34 INFO]: Training loss at epoch 40: 1.0226559937000275
[08/27/2025 20:01:35 INFO]: New best epoch, val score: -0.8107526810694019
[08/27/2025 20:01:35 INFO]: Saving model to: unhanged-Shanesha_trial_42/model_best.pth
[08/27/2025 20:02:04 INFO]: Training loss at epoch 67: 0.806812047958374
[08/27/2025 20:02:05 INFO]: Training loss at epoch 27: 1.1319686472415924
[08/27/2025 20:02:08 INFO]: Training loss at epoch 44: 0.8896706104278564
[08/27/2025 20:02:26 INFO]: Training loss at epoch 30: 0.9801918864250183
[08/27/2025 20:02:50 INFO]: Training loss at epoch 4: 1.4470672607421875
[08/27/2025 20:03:07 INFO]: Training loss at epoch 0: 1.0965608954429626
[08/27/2025 20:03:15 INFO]: Training loss at epoch 5: 1.3797447085380554
[08/27/2025 20:03:19 INFO]: Training loss at epoch 34: 0.9099959433078766
[08/27/2025 20:03:27 INFO]: Training loss at epoch 44: 0.9095798432826996
[08/27/2025 20:03:41 INFO]: Training loss at epoch 53: 1.1115623712539673
[08/27/2025 20:03:45 INFO]: Training loss at epoch 41: 1.0123319625854492
[08/27/2025 20:03:51 INFO]: Training loss at epoch 68: 1.0662736296653748
[08/27/2025 20:03:54 INFO]: New best epoch, val score: -0.8076593851693372
[08/27/2025 20:03:54 INFO]: Saving model to: unhanged-Shanesha_trial_50/model_best.pth
[08/27/2025 20:04:38 INFO]: Training loss at epoch 45: 0.8279715180397034
[08/27/2025 20:05:16 INFO]: Training loss at epoch 8: 1.1236417293548584
[08/27/2025 20:05:20 INFO]: Training loss at epoch 8: 1.156278371810913
[08/27/2025 20:05:31 INFO]: Training loss at epoch 35: 0.9623666107654572
[08/27/2025 20:05:37 INFO]: Training loss at epoch 9: 1.0907115638256073
[08/27/2025 20:05:39 INFO]: Training loss at epoch 69: 1.1103002727031708
[08/27/2025 20:05:56 INFO]: Training loss at epoch 42: 1.0054814219474792
[08/27/2025 20:06:09 INFO]: Training loss at epoch 45: 0.8368995785713196
[08/27/2025 20:06:16 INFO]: Training stats: {
    "score": -1.0023225799047493,
    "rmse": 1.0023225799047493
}
[08/27/2025 20:06:16 INFO]: Val stats: {
    "score": -0.7224062681948936,
    "rmse": 0.7224062681948936
}
[08/27/2025 20:06:16 INFO]: Test stats: {
    "score": -0.8996327425925388,
    "rmse": 0.8996327425925388
}
[08/27/2025 20:06:34 INFO]: Training loss at epoch 4: 1.123350203037262
[08/27/2025 20:06:38 INFO]: Training loss at epoch 2: 1.5464472770690918
[08/27/2025 20:07:21 INFO]: Training loss at epoch 9: 1.0802069306373596
[08/27/2025 20:07:41 INFO]: Training loss at epoch 36: 0.9676789939403534
[08/27/2025 20:07:49 INFO]: Training stats: {
    "score": -1.0906560314159248,
    "rmse": 1.0906560314159248
}
[08/27/2025 20:07:49 INFO]: Val stats: {
    "score": -0.8924108122059191,
    "rmse": 0.8924108122059191
}
[08/27/2025 20:07:49 INFO]: Test stats: {
    "score": -1.019670463327512,
    "rmse": 1.019670463327512
}
[08/27/2025 20:07:52 INFO]: Training loss at epoch 7: 1.1227055788040161
[08/27/2025 20:08:03 INFO]: Training loss at epoch 70: 1.0787012875080109
[08/27/2025 20:08:06 INFO]: Training loss at epoch 43: 0.8714419007301331
[08/27/2025 20:08:50 INFO]: Training loss at epoch 46: 0.8933145701885223
[08/27/2025 20:08:55 INFO]: Training loss at epoch 45: 0.8836023509502411
[08/27/2025 20:09:20 INFO]: Training loss at epoch 54: 0.9532897770404816
[08/27/2025 20:09:25 INFO]: Training loss at epoch 5: 1.0124240517616272
[08/27/2025 20:09:34 INFO]: Training stats: {
    "score": -1.154606292299231,
    "rmse": 1.154606292299231
}
[08/27/2025 20:09:34 INFO]: Val stats: {
    "score": -0.7797198220016603,
    "rmse": 0.7797198220016603
}
[08/27/2025 20:09:34 INFO]: Test stats: {
    "score": -0.9931029025665166,
    "rmse": 0.9931029025665166
}
[08/27/2025 20:09:41 INFO]: Training loss at epoch 1: 2.18402898311615
[08/27/2025 20:09:48 INFO]: Training loss at epoch 6: 1.13711416721344
[08/27/2025 20:09:49 INFO]: Training loss at epoch 71: 0.9821735620498657
[08/27/2025 20:09:50 INFO]: Training loss at epoch 26: 0.9210178554058075
[08/27/2025 20:09:52 INFO]: Training loss at epoch 37: 0.99139004945755
[08/27/2025 20:10:16 INFO]: Training loss at epoch 44: 0.8959477543830872
[08/27/2025 20:10:18 INFO]: Training loss at epoch 31: 0.9930340647697449
[08/27/2025 20:10:21 INFO]: New best epoch, val score: -0.7797198220016603
[08/27/2025 20:10:21 INFO]: Saving model to: unhanged-Shanesha_trial_42/model_best.pth
[08/27/2025 20:11:18 INFO]: Training loss at epoch 46: 1.0292863547801971
[08/27/2025 20:11:31 INFO]: Training loss at epoch 47: 0.8544911444187164
[08/27/2025 20:11:37 INFO]: Training loss at epoch 72: 1.2547270059585571
[08/27/2025 20:11:44 INFO]: Training loss at epoch 9: 1.10627943277359
[08/27/2025 20:11:48 INFO]: Training loss at epoch 9: 0.9642049074172974
[08/27/2025 20:12:04 INFO]: Training loss at epoch 38: 1.0281287133693695
[08/27/2025 20:12:26 INFO]: Training loss at epoch 45: 0.7696125209331512
[08/27/2025 20:13:01 INFO]: Training loss at epoch 5: 1.5724144577980042
[08/27/2025 20:13:02 INFO]: Training loss at epoch 28: 0.9099209010601044
[08/27/2025 20:13:04 INFO]: Training loss at epoch 3: 1.6004046201705933
[08/27/2025 20:13:25 INFO]: Training loss at epoch 73: 1.3267530798912048
[08/27/2025 20:13:57 INFO]: Training stats: {
    "score": -1.0259429388163415,
    "rmse": 1.0259429388163415
}
[08/27/2025 20:13:57 INFO]: Val stats: {
    "score": -0.7770113294485722,
    "rmse": 0.7770113294485722
}
[08/27/2025 20:13:57 INFO]: Test stats: {
    "score": -0.931067322281598,
    "rmse": 0.931067322281598
}
[08/27/2025 20:14:01 INFO]: Training stats: {
    "score": -1.0396508554268447,
    "rmse": 1.0396508554268447
}
[08/27/2025 20:14:01 INFO]: Val stats: {
    "score": -0.7973497291192413,
    "rmse": 0.7973497291192413
}
[08/27/2025 20:14:01 INFO]: Test stats: {
    "score": -0.9483845396554567,
    "rmse": 0.9483845396554567
}
[08/27/2025 20:14:13 INFO]: Training loss at epoch 48: 0.8798412084579468
[08/27/2025 20:14:15 INFO]: Training loss at epoch 39: 0.9347160756587982
[08/27/2025 20:14:17 INFO]: Training loss at epoch 10: 1.0504863560199738
[08/27/2025 20:14:21 INFO]: Training loss at epoch 8: 1.1627964973449707
[08/27/2025 20:14:36 INFO]: Training loss at epoch 46: 1.1834743022918701
[08/27/2025 20:14:51 INFO]: Running Final Evaluation...
[08/27/2025 20:14:59 INFO]: Training stats: {
    "score": -1.0014150061599951,
    "rmse": 1.0014150061599951
}
[08/27/2025 20:14:59 INFO]: Val stats: {
    "score": -0.7022743193766721,
    "rmse": 0.7022743193766721
}
[08/27/2025 20:14:59 INFO]: Test stats: {
    "score": -0.8855154167944533,
    "rmse": 0.8855154167944533
}
[08/27/2025 20:15:01 INFO]: Training loss at epoch 55: 0.9364633560180664
[08/27/2025 20:15:03 INFO]: New best epoch, val score: -0.6650386573250157
[08/27/2025 20:15:03 INFO]: Saving model to: unhanged-Shanesha_trial_41/model_best.pth
[08/27/2025 20:15:12 INFO]: Training loss at epoch 74: 0.7816291451454163
[08/27/2025 20:15:25 INFO]: Running Final Evaluation...
[08/27/2025 20:15:36 INFO]: Training accuracy: {
    "score": -1.015188800412787,
    "rmse": 1.015188800412787
}
[08/27/2025 20:15:36 INFO]: Val accuracy: {
    "score": -0.6610264117884191,
    "rmse": 0.6610264117884191
}
[08/27/2025 20:15:36 INFO]: Test accuracy: {
    "score": -0.8718247061372519,
    "rmse": 0.8718247061372519
}
[08/27/2025 20:15:36 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_39",
    "best_epoch": 15,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8718247061372519,
        "rmse": 0.8718247061372519
    },
    "train_stats": {
        "score": -1.015188800412787,
        "rmse": 1.015188800412787
    },
    "val_stats": {
        "score": -0.6610264117884191,
        "rmse": 0.6610264117884191
    }
}
[08/27/2025 20:15:36 INFO]: Procewss finished for trial unhanged-Shanesha_trial_39
[08/27/2025 20:15:36 INFO]: 
_________________________________________________

[08/27/2025 20:15:36 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:15:36 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 0.6880887001387825
  attention_dropout: 0.37168587390801355
  ffn_dropout: 0.37168587390801355
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00014175872608081707
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_51

[08/27/2025 20:15:36 INFO]: This ft_transformer has 0.578 million parameters.
[08/27/2025 20:15:36 INFO]: Training will start at epoch 0.
[08/27/2025 20:15:36 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:15:42 INFO]: Training loss at epoch 46: 1.007792055606842
[08/27/2025 20:15:44 INFO]: Running Final Evaluation...
[08/27/2025 20:16:03 INFO]: Training loss at epoch 6: 1.065045803785324
[08/27/2025 20:16:06 INFO]: Training accuracy: {
    "score": -1.011495560657876,
    "rmse": 1.011495560657876
}
[08/27/2025 20:16:06 INFO]: Val accuracy: {
    "score": -0.6600973704748018,
    "rmse": 0.6600973704748018
}
[08/27/2025 20:16:06 INFO]: Test accuracy: {
    "score": -0.8729249768864542,
    "rmse": 0.8729249768864542
}
[08/27/2025 20:16:06 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_37",
    "best_epoch": 43,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8729249768864542,
        "rmse": 0.8729249768864542
    },
    "train_stats": {
        "score": -1.011495560657876,
        "rmse": 1.011495560657876
    },
    "val_stats": {
        "score": -0.6600973704748018,
        "rmse": 0.6600973704748018
    }
}
[08/27/2025 20:16:06 INFO]: Procewss finished for trial unhanged-Shanesha_trial_37
[08/27/2025 20:16:07 INFO]: 
_________________________________________________

[08/27/2025 20:16:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:16:07 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 0.6796536823820594
  attention_dropout: 0.3692846458689073
  ffn_dropout: 0.3692846458689073
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00011567128416415556
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_52

[08/27/2025 20:16:07 INFO]: This ft_transformer has 0.575 million parameters.
[08/27/2025 20:16:07 INFO]: Training will start at epoch 0.
[08/27/2025 20:16:07 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:16:08 INFO]: Training loss at epoch 10: 1.2717552185058594
[08/27/2025 20:16:14 INFO]: Training loss at epoch 0: 0.9716570377349854
[08/27/2025 20:16:16 INFO]: Training loss at epoch 2: 1.8780884146690369
[08/27/2025 20:16:20 INFO]: New best epoch, val score: -0.6581398393325539
[08/27/2025 20:16:20 INFO]: Saving model to: unhanged-Shanesha_trial_51/model_best.pth
[08/27/2025 20:16:23 INFO]: Training loss at epoch 7: 1.1270729303359985
[08/27/2025 20:16:45 INFO]: Training loss at epoch 0: 1.124402403831482
[08/27/2025 20:16:50 INFO]: New best epoch, val score: -0.7747484118311979
[08/27/2025 20:16:50 INFO]: Saving model to: unhanged-Shanesha_trial_52/model_best.pth
[08/27/2025 20:16:58 INFO]: Training loss at epoch 1: 0.9959390163421631
[08/27/2025 20:16:58 INFO]: New best epoch, val score: -0.6751800900631307
[08/27/2025 20:16:58 INFO]: Saving model to: unhanged-Shanesha_trial_42/model_best.pth
[08/27/2025 20:17:03 INFO]: Training loss at epoch 49: 0.8021518588066101
[08/27/2025 20:17:22 INFO]: Training loss at epoch 40: 1.051990270614624
[08/27/2025 20:17:29 INFO]: Training loss at epoch 1: 1.1178789734840393
[08/27/2025 20:17:36 INFO]: New best epoch, val score: -0.6659745955312154
[08/27/2025 20:17:36 INFO]: Saving model to: unhanged-Shanesha_trial_52/model_best.pth
[08/27/2025 20:17:44 INFO]: Training loss at epoch 2: 1.0489365458488464
[08/27/2025 20:18:03 INFO]: Training stats: {
    "score": -0.9995690316201719,
    "rmse": 0.9995690316201719
}
[08/27/2025 20:18:03 INFO]: Val stats: {
    "score": -0.690305942643043,
    "rmse": 0.690305942643043
}
[08/27/2025 20:18:03 INFO]: Test stats: {
    "score": -0.8794377713652863,
    "rmse": 0.8794377713652863
}
[08/27/2025 20:18:10 INFO]: Training loss at epoch 47: 1.0253337025642395
[08/27/2025 20:18:15 INFO]: Training loss at epoch 2: 0.9373040795326233
[08/27/2025 20:18:21 INFO]: Training loss at epoch 32: 1.072199434041977
[08/27/2025 20:18:28 INFO]: Training loss at epoch 3: 1.0580227375030518
[08/27/2025 20:18:38 INFO]: Training accuracy: {
    "score": -1.0069514155790042,
    "rmse": 1.0069514155790042
}
[08/27/2025 20:18:38 INFO]: Val accuracy: {
    "score": -0.6672770563582879,
    "rmse": 0.6672770563582879
}
[08/27/2025 20:18:38 INFO]: Test accuracy: {
    "score": -0.8846741089283283,
    "rmse": 0.8846741089283283
}
[08/27/2025 20:18:38 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_16",
    "best_epoch": 24,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8846741089283283,
        "rmse": 0.8846741089283283
    },
    "train_stats": {
        "score": -1.0069514155790042,
        "rmse": 1.0069514155790042
    },
    "val_stats": {
        "score": -0.6672770563582879,
        "rmse": 0.6672770563582879
    }
}
[08/27/2025 20:18:38 INFO]: Procewss finished for trial unhanged-Shanesha_trial_16
[08/27/2025 20:18:38 INFO]: 
_________________________________________________

[08/27/2025 20:18:38 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:18:38 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 0.6761727504319641
  attention_dropout: 0.4252427726268004
  ffn_dropout: 0.4252427726268004
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 9.503265747988047e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_53

[08/27/2025 20:18:38 INFO]: This ft_transformer has 0.575 million parameters.
[08/27/2025 20:18:38 INFO]: Training will start at epoch 0.
[08/27/2025 20:18:38 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:18:59 INFO]: Training loss at epoch 3: 0.9959448873996735
[08/27/2025 20:19:12 INFO]: Training loss at epoch 4: 0.8748548924922943
[08/27/2025 20:19:17 INFO]: Training loss at epoch 0: 1.2546056509017944
[08/27/2025 20:19:24 INFO]: New best epoch, val score: -0.9015458803966192
[08/27/2025 20:19:24 INFO]: Saving model to: unhanged-Shanesha_trial_53/model_best.pth
[08/27/2025 20:19:43 INFO]: Training loss at epoch 4: 1.289446771144867
[08/27/2025 20:19:44 INFO]: Training loss at epoch 41: 0.8881143033504486
[08/27/2025 20:19:45 INFO]: Training loss at epoch 6: 1.7684032917022705
[08/27/2025 20:19:50 INFO]: Training loss at epoch 4: 1.003384292125702
[08/27/2025 20:19:58 INFO]: Training loss at epoch 5: 0.8662054538726807
[08/27/2025 20:20:03 INFO]: Training loss at epoch 1: 1.1900184750556946
[08/27/2025 20:20:09 INFO]: New best epoch, val score: -0.7204857876490021
[08/27/2025 20:20:09 INFO]: Saving model to: unhanged-Shanesha_trial_53/model_best.pth
[08/27/2025 20:20:29 INFO]: Training loss at epoch 5: 1.0443679690361023
[08/27/2025 20:20:43 INFO]: Training loss at epoch 6: 1.0687918066978455
[08/27/2025 20:20:49 INFO]: Training loss at epoch 2: 1.0542441308498383
[08/27/2025 20:20:50 INFO]: Training loss at epoch 10: 0.9926489293575287
[08/27/2025 20:20:55 INFO]: New best epoch, val score: -0.674385136877832
[08/27/2025 20:20:55 INFO]: Saving model to: unhanged-Shanesha_trial_53/model_best.pth
[08/27/2025 20:20:57 INFO]: Training loss at epoch 10: 0.9939377903938293
[08/27/2025 20:20:59 INFO]: Training loss at epoch 50: 1.0149068236351013
[08/27/2025 20:21:04 INFO]: Training loss at epoch 27: 1.0513845384120941
[08/27/2025 20:21:10 INFO]: Training loss at epoch 11: 1.0472291707992554
[08/27/2025 20:21:13 INFO]: Training loss at epoch 6: 1.4559572339057922
[08/27/2025 20:21:16 INFO]: Training loss at epoch 9: 0.8930939435958862
[08/27/2025 20:21:19 INFO]: Running Final Evaluation...
[08/27/2025 20:21:26 INFO]: Training loss at epoch 7: 0.942361980676651
[08/27/2025 20:21:31 INFO]: Training loss at epoch 3: 1.2010884881019592
[08/27/2025 20:21:37 INFO]: New best epoch, val score: -0.6620122873061627
[08/27/2025 20:21:37 INFO]: Saving model to: unhanged-Shanesha_trial_44/model_best.pth
[08/27/2025 20:21:55 INFO]: Training loss at epoch 7: 1.067339539527893
[08/27/2025 20:22:04 INFO]: Training loss at epoch 42: 0.8734766244888306
[08/27/2025 20:22:08 INFO]: Training loss at epoch 8: 0.990102231502533
[08/27/2025 20:22:13 INFO]: Training loss at epoch 4: 0.8999112844467163
[08/27/2025 20:22:13 INFO]: New best epoch, val score: -0.6581000112436123
[08/27/2025 20:22:13 INFO]: Saving model to: unhanged-Shanesha_trial_51/model_best.pth
[08/27/2025 20:22:21 INFO]: Training accuracy: {
    "score": -1.0138858646058506,
    "rmse": 1.0138858646058506
}
[08/27/2025 20:22:21 INFO]: Val accuracy: {
    "score": -0.6603844252877538,
    "rmse": 0.6603844252877538
}
[08/27/2025 20:22:21 INFO]: Test accuracy: {
    "score": -0.8717242675575325,
    "rmse": 0.8717242675575325
}
[08/27/2025 20:22:21 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_38",
    "best_epoch": 19,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8717242675575325,
        "rmse": 0.8717242675575325
    },
    "train_stats": {
        "score": -1.0138858646058506,
        "rmse": 1.0138858646058506
    },
    "val_stats": {
        "score": -0.6603844252877538,
        "rmse": 0.6603844252877538
    }
}
[08/27/2025 20:22:21 INFO]: Procewss finished for trial unhanged-Shanesha_trial_38
[08/27/2025 20:22:21 INFO]: 
_________________________________________________

[08/27/2025 20:22:21 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:22:21 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 1.1393184964166343
  attention_dropout: 0.39501145175295266
  ffn_dropout: 0.39501145175295266
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00011232174783043599
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_54

[08/27/2025 20:22:22 INFO]: This ft_transformer has 10.075 million parameters.
[08/27/2025 20:22:22 INFO]: Training will start at epoch 0.
[08/27/2025 20:22:22 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:22:36 INFO]: Training loss at epoch 8: 0.9906502366065979
[08/27/2025 20:22:50 INFO]: Training loss at epoch 9: 1.14125257730484
[08/27/2025 20:22:52 INFO]: Training loss at epoch 47: 1.078473150730133
[08/27/2025 20:22:55 INFO]: Training loss at epoch 5: 1.094765067100525
[08/27/2025 20:23:00 INFO]: Training loss at epoch 7: 1.1027551889419556
[08/27/2025 20:23:04 INFO]: Training stats: {
    "score": -1.0204828561089023,
    "rmse": 1.0204828561089023
}
[08/27/2025 20:23:04 INFO]: Val stats: {
    "score": -0.6575611851025841,
    "rmse": 0.6575611851025841
}
[08/27/2025 20:23:04 INFO]: Test stats: {
    "score": -0.8738494638904566,
    "rmse": 0.8738494638904566
}
[08/27/2025 20:23:05 INFO]: Training loss at epoch 11: 0.9457311034202576
[08/27/2025 20:23:09 INFO]: New best epoch, val score: -0.6575611851025841
[08/27/2025 20:23:09 INFO]: Saving model to: unhanged-Shanesha_trial_51/model_best.pth
[08/27/2025 20:23:12 INFO]: Training loss at epoch 3: 1.1597957611083984
[08/27/2025 20:23:18 INFO]: Training loss at epoch 9: 0.8901912569999695
[08/27/2025 20:23:19 INFO]: Training loss at epoch 8: 1.4406402707099915
[08/27/2025 20:23:30 INFO]: Training stats: {
    "score": -1.0041865710223898,
    "rmse": 1.0041865710223898
}
[08/27/2025 20:23:30 INFO]: Val stats: {
    "score": -0.6643682289684163,
    "rmse": 0.6643682289684163
}
[08/27/2025 20:23:30 INFO]: Test stats: {
    "score": -0.8685948284907404,
    "rmse": 0.8685948284907404
}
[08/27/2025 20:23:32 INFO]: Training stats: {
    "score": -1.0252550326558068,
    "rmse": 1.0252550326558068
}
[08/27/2025 20:23:32 INFO]: Val stats: {
    "score": -0.7725266723212779,
    "rmse": 0.7725266723212779
}
[08/27/2025 20:23:32 INFO]: Test stats: {
    "score": -0.9274848121385333,
    "rmse": 0.9274848121385333
}
[08/27/2025 20:23:36 INFO]: Training loss at epoch 6: 1.0755946636199951
[08/27/2025 20:23:46 INFO]: Training loss at epoch 10: 1.1923851370811462
[08/27/2025 20:23:51 INFO]: New best epoch, val score: -0.6557663709355026
[08/27/2025 20:23:51 INFO]: Saving model to: unhanged-Shanesha_trial_51/model_best.pth
[08/27/2025 20:24:14 INFO]: Training loss at epoch 10: 1.2020621299743652
[08/27/2025 20:24:15 INFO]: Training loss at epoch 43: 1.04029381275177
[08/27/2025 20:24:18 INFO]: Training loss at epoch 7: 1.227326512336731
[08/27/2025 20:24:25 INFO]: Training loss at epoch 29: 1.1923297047615051
[08/27/2025 20:24:28 INFO]: Training loss at epoch 11: 0.9530759453773499
[08/27/2025 20:24:56 INFO]: Training loss at epoch 11: 1.1196937561035156
[08/27/2025 20:24:59 INFO]: Training loss at epoch 8: 1.2810496091842651
[08/27/2025 20:25:06 INFO]: Training loss at epoch 48: 1.0366030931472778
[08/27/2025 20:25:09 INFO]: Training loss at epoch 12: 1.1046061515808105
[08/27/2025 20:25:37 INFO]: Training loss at epoch 12: 0.9287354052066803
[08/27/2025 20:25:41 INFO]: Training loss at epoch 9: 1.0702119171619415
[08/27/2025 20:25:42 INFO]: New best epoch, val score: -0.6646650382342552
[08/27/2025 20:25:42 INFO]: Saving model to: unhanged-Shanesha_trial_52/model_best.pth
[08/27/2025 20:25:51 INFO]: Training loss at epoch 13: 1.2532275915145874
[08/27/2025 20:25:56 INFO]: Training stats: {
    "score": -1.180098249118772,
    "rmse": 1.180098249118772
}
[08/27/2025 20:25:56 INFO]: Val stats: {
    "score": -0.8119714812226022,
    "rmse": 0.8119714812226022
}
[08/27/2025 20:25:56 INFO]: Test stats: {
    "score": -1.0198172346639824,
    "rmse": 1.0198172346639824
}
[08/27/2025 20:26:21 INFO]: Training loss at epoch 13: 1.3567087650299072
[08/27/2025 20:26:24 INFO]: Training loss at epoch 7: 1.6099680662155151
[08/27/2025 20:26:26 INFO]: Training loss at epoch 5: 1.663383662700653
[08/27/2025 20:26:26 INFO]: Training loss at epoch 0: 1.5095680356025696
[08/27/2025 20:26:26 INFO]: New best epoch, val score: -0.6626621955399092
[08/27/2025 20:26:26 INFO]: Saving model to: unhanged-Shanesha_trial_52/model_best.pth
[08/27/2025 20:26:30 INFO]: Training loss at epoch 44: 1.0837363600730896
[08/27/2025 20:26:30 INFO]: Training loss at epoch 33: 1.0046148896217346
[08/27/2025 20:26:36 INFO]: Training loss at epoch 14: 0.836550384759903
[08/27/2025 20:26:40 INFO]: Training loss at epoch 10: 1.3413615822792053
[08/27/2025 20:27:01 INFO]: New best epoch, val score: -0.6771031284881697
[08/27/2025 20:27:01 INFO]: Saving model to: unhanged-Shanesha_trial_54/model_best.pth
[08/27/2025 20:27:05 INFO]: Training loss at epoch 14: 1.101544976234436
[08/27/2025 20:27:21 INFO]: Training loss at epoch 15: 1.0165863931179047
[08/27/2025 20:27:26 INFO]: Training loss at epoch 11: 1.081270456314087
[08/27/2025 20:27:29 INFO]: Training loss at epoch 11: 0.8926186859607697
[08/27/2025 20:27:36 INFO]: Training loss at epoch 11: 0.8247108459472656
[08/27/2025 20:27:46 INFO]: Training loss at epoch 12: 1.3115645051002502
[08/27/2025 20:27:50 INFO]: Training loss at epoch 15: 0.9845988750457764
[08/27/2025 20:28:05 INFO]: Training loss at epoch 16: 1.1732110977172852
[08/27/2025 20:28:10 INFO]: Training loss at epoch 12: 1.0445761680603027
[08/27/2025 20:28:18 INFO]: Training stats: {
    "score": -0.9972426223336903,
    "rmse": 0.9972426223336903
}
[08/27/2025 20:28:18 INFO]: Val stats: {
    "score": -0.6747682650223733,
    "rmse": 0.6747682650223733
}
[08/27/2025 20:28:18 INFO]: Test stats: {
    "score": -0.8719601341317383,
    "rmse": 0.8719601341317383
}
[08/27/2025 20:28:35 INFO]: Training loss at epoch 16: 0.9517087340354919
[08/27/2025 20:28:51 INFO]: Training loss at epoch 17: 1.0199528634548187
[08/27/2025 20:28:54 INFO]: Training loss at epoch 45: 1.0104569494724274
[08/27/2025 20:28:56 INFO]: Training loss at epoch 13: 1.0308590829372406
[08/27/2025 20:29:20 INFO]: Training loss at epoch 17: 1.0106021463871002
[08/27/2025 20:29:37 INFO]: Training loss at epoch 18: 1.0218268036842346
[08/27/2025 20:29:42 INFO]: Training loss at epoch 14: 1.0659120082855225
[08/27/2025 20:29:47 INFO]: New best epoch, val score: -0.6731417641520245
[08/27/2025 20:29:47 INFO]: Saving model to: unhanged-Shanesha_trial_53/model_best.pth
[08/27/2025 20:30:00 INFO]: Training loss at epoch 8: 0.9343544542789459
[08/27/2025 20:30:01 INFO]: Training loss at epoch 12: 1.1114335656166077
[08/27/2025 20:30:03 INFO]: Training loss at epoch 48: 1.0830004811286926
[08/27/2025 20:30:06 INFO]: Training loss at epoch 18: 0.8934380114078522
[08/27/2025 20:30:11 INFO]: Training loss at epoch 4: 1.5715312957763672
[08/27/2025 20:30:18 INFO]: Training loss at epoch 9: 1.073147475719452
[08/27/2025 20:30:22 INFO]: Training loss at epoch 19: 0.8673682808876038
[08/27/2025 20:30:26 INFO]: Training loss at epoch 10: 0.9183355569839478
[08/27/2025 20:30:28 INFO]: Training loss at epoch 15: 1.1648763418197632
[08/27/2025 20:30:34 INFO]: New best epoch, val score: -0.6675624273054701
[08/27/2025 20:30:34 INFO]: Saving model to: unhanged-Shanesha_trial_53/model_best.pth
[08/27/2025 20:30:38 INFO]: Training stats: {
    "score": -1.0018645233415782,
    "rmse": 1.0018645233415782
}
[08/27/2025 20:30:38 INFO]: Val stats: {
    "score": -0.6609330637154163,
    "rmse": 0.6609330637154163
}
[08/27/2025 20:30:38 INFO]: Test stats: {
    "score": -0.8665429166905807,
    "rmse": 0.8665429166905807
}
[08/27/2025 20:30:50 INFO]: Training loss at epoch 19: 0.9460261762142181
[08/27/2025 20:30:53 INFO]: New best epoch, val score: -0.6581620183569129
[08/27/2025 20:30:53 INFO]: Saving model to: unhanged-Shanesha_trial_20/model_best.pth
[08/27/2025 20:31:02 INFO]: New best epoch, val score: -0.7037966557917537
[08/27/2025 20:31:02 INFO]: Saving model to: unhanged-Shanesha_trial_50/model_best.pth
[08/27/2025 20:31:10 INFO]: Training stats: {
    "score": -1.0132477524631376,
    "rmse": 1.0132477524631376
}
[08/27/2025 20:31:10 INFO]: Val stats: {
    "score": -0.6627997758415133,
    "rmse": 0.6627997758415133
}
[08/27/2025 20:31:10 INFO]: Test stats: {
    "score": -0.8698430963702779,
    "rmse": 0.8698430963702779
}
[08/27/2025 20:31:16 INFO]: Training loss at epoch 16: 1.2454341650009155
[08/27/2025 20:31:16 INFO]: Training loss at epoch 46: 0.7990183234214783
[08/27/2025 20:31:21 INFO]: New best epoch, val score: -0.6646437173934128
[08/27/2025 20:31:21 INFO]: Saving model to: unhanged-Shanesha_trial_53/model_best.pth
[08/27/2025 20:31:25 INFO]: Training loss at epoch 20: 1.087881624698639
[08/27/2025 20:31:25 INFO]: Training loss at epoch 1: 1.5161439776420593
[08/27/2025 20:31:52 INFO]: Training loss at epoch 20: 0.9838106334209442
[08/27/2025 20:31:59 INFO]: Training loss at epoch 17: 1.1141710877418518
[08/27/2025 20:32:04 INFO]: New best epoch, val score: -0.6638639850689064
[08/27/2025 20:32:04 INFO]: Saving model to: unhanged-Shanesha_trial_53/model_best.pth
[08/27/2025 20:32:06 INFO]: Training loss at epoch 21: 0.8560321927070618
[08/27/2025 20:32:15 INFO]: Training loss at epoch 49: 1.0425184071063995
[08/27/2025 20:32:22 INFO]: Training loss at epoch 28: 1.0328885316848755
[08/27/2025 20:32:34 INFO]: Training loss at epoch 21: 0.8430356383323669
[08/27/2025 20:32:40 INFO]: Training stats: {
    "score": -1.1389266877645012,
    "rmse": 1.1389266877645012
}
[08/27/2025 20:32:40 INFO]: Val stats: {
    "score": -0.970522000160978,
    "rmse": 0.970522000160978
}
[08/27/2025 20:32:40 INFO]: Test stats: {
    "score": -1.082920697153972,
    "rmse": 1.082920697153972
}
[08/27/2025 20:32:41 INFO]: Training loss at epoch 18: 0.8724490106105804
[08/27/2025 20:32:48 INFO]: Training loss at epoch 22: 1.27857905626297
[08/27/2025 20:33:15 INFO]: Training loss at epoch 22: 1.007526844739914
[08/27/2025 20:33:17 INFO]: Training loss at epoch 6: 1.3058656454086304
[08/27/2025 20:33:18 INFO]: Training loss at epoch 8: 1.866050660610199
[08/27/2025 20:33:22 INFO]: Training loss at epoch 19: 1.0444097518920898
[08/27/2025 20:33:28 INFO]: Training loss at epoch 47: 1.0418769717216492
[08/27/2025 20:33:29 INFO]: Training loss at epoch 23: 1.0905791521072388
[08/27/2025 20:33:37 INFO]: Training stats: {
    "score": -1.0081933591070034,
    "rmse": 1.0081933591070034
}
[08/27/2025 20:33:37 INFO]: Val stats: {
    "score": -0.664138317259291,
    "rmse": 0.664138317259291
}
[08/27/2025 20:33:37 INFO]: Test stats: {
    "score": -0.8704832974783971,
    "rmse": 0.8704832974783971
}
[08/27/2025 20:33:56 INFO]: Training loss at epoch 23: 1.1829451620578766
[08/27/2025 20:34:10 INFO]: Training loss at epoch 24: 1.139820635318756
[08/27/2025 20:34:19 INFO]: Training loss at epoch 20: 1.177760362625122
[08/27/2025 20:34:20 INFO]: Training loss at epoch 12: 1.241310954093933
[08/27/2025 20:34:30 INFO]: Training loss at epoch 12: 0.970084935426712
[08/27/2025 20:34:31 INFO]: Training stats: {
    "score": -0.9991768744074886,
    "rmse": 0.9991768744074886
}
[08/27/2025 20:34:31 INFO]: Val stats: {
    "score": -0.6783264123427399,
    "rmse": 0.6783264123427399
}
[08/27/2025 20:34:31 INFO]: Test stats: {
    "score": -0.8731716348885963,
    "rmse": 0.8731716348885963
}
[08/27/2025 20:34:37 INFO]: Training loss at epoch 13: 1.1630219519138336
[08/27/2025 20:34:38 INFO]: Training loss at epoch 24: 0.9978622198104858
[08/27/2025 20:34:51 INFO]: Training loss at epoch 25: 0.9484168589115143
[08/27/2025 20:34:52 INFO]: Training loss at epoch 34: 1.040932834148407
[08/27/2025 20:35:00 INFO]: Training loss at epoch 21: 1.0182292461395264
[08/27/2025 20:35:20 INFO]: Training loss at epoch 25: 1.0052466690540314
[08/27/2025 20:35:34 INFO]: Training loss at epoch 26: 1.0577596426010132
[08/27/2025 20:35:41 INFO]: Training loss at epoch 48: 1.1140263080596924
[08/27/2025 20:35:44 INFO]: Training loss at epoch 22: 0.9555915296077728
[08/27/2025 20:36:03 INFO]: Training loss at epoch 2: 1.489195317029953
[08/27/2025 20:36:07 INFO]: Training loss at epoch 26: 1.2794684767723083
[08/27/2025 20:36:19 INFO]: Training loss at epoch 27: 1.2401348948478699
[08/27/2025 20:36:30 INFO]: Training loss at epoch 23: 0.8361127078533173
[08/27/2025 20:36:51 INFO]: Training loss at epoch 13: 1.2183132767677307
[08/27/2025 20:36:54 INFO]: Training loss at epoch 27: 1.0052659213542938
[08/27/2025 20:36:55 INFO]: Training loss at epoch 9: 1.157507836818695
[08/27/2025 20:37:04 INFO]: Training loss at epoch 28: 0.8747689425945282
[08/27/2025 20:37:05 INFO]: Training loss at epoch 5: 1.0486785173416138
[08/27/2025 20:37:12 INFO]: Training loss at epoch 49: 1.1057934761047363
[08/27/2025 20:37:16 INFO]: Training loss at epoch 24: 0.8707287311553955
[08/27/2025 20:37:16 INFO]: Training loss at epoch 11: 0.9358152151107788
[08/27/2025 20:37:40 INFO]: Training loss at epoch 28: 1.1515339016914368
[08/27/2025 20:37:50 INFO]: Training loss at epoch 29: 1.0205132365226746
[08/27/2025 20:38:01 INFO]: Training loss at epoch 25: 0.86406210064888
[08/27/2025 20:38:07 INFO]: Training stats: {
    "score": -1.0107153375805669,
    "rmse": 1.0107153375805669
}
[08/27/2025 20:38:07 INFO]: Val stats: {
    "score": -0.657240778546406,
    "rmse": 0.657240778546406
}
[08/27/2025 20:38:07 INFO]: Test stats: {
    "score": -0.8694419706882057,
    "rmse": 0.8694419706882057
}
[08/27/2025 20:38:08 INFO]: Training loss at epoch 49: 0.8365603089332581
[08/27/2025 20:38:25 INFO]: Training loss at epoch 29: 1.1388229131698608
[08/27/2025 20:38:41 INFO]: Training stats: {
    "score": -0.998442210797833,
    "rmse": 0.998442210797833
}
[08/27/2025 20:38:41 INFO]: Val stats: {
    "score": -0.6911745378699935,
    "rmse": 0.6911745378699935
}
[08/27/2025 20:38:41 INFO]: Test stats: {
    "score": -0.8758964665048333,
    "rmse": 0.8758964665048333
}
[08/27/2025 20:38:47 INFO]: Training loss at epoch 26: 1.030856966972351
[08/27/2025 20:38:52 INFO]: Training loss at epoch 30: 1.1838136315345764
[08/27/2025 20:38:57 INFO]: Training stats: {
    "score": -1.0008775716378882,
    "rmse": 1.0008775716378882
}
[08/27/2025 20:38:57 INFO]: Val stats: {
    "score": -0.7020221967347363,
    "rmse": 0.7020221967347363
}
[08/27/2025 20:38:57 INFO]: Test stats: {
    "score": -0.885337356281794,
    "rmse": 0.885337356281794
}
[08/27/2025 20:39:26 INFO]: Training loss at epoch 30: 0.9377225041389465
[08/27/2025 20:39:26 INFO]: Training stats: {
    "score": -1.032988859501026,
    "rmse": 1.032988859501026
}
[08/27/2025 20:39:26 INFO]: Val stats: {
    "score": -0.7844455515820768,
    "rmse": 0.7844455515820768
}
[08/27/2025 20:39:26 INFO]: Test stats: {
    "score": -0.939210145298791,
    "rmse": 0.939210145298791
}
[08/27/2025 20:39:32 INFO]: Training loss at epoch 27: 1.1686968803405762
[08/27/2025 20:39:36 INFO]: Training loss at epoch 31: 0.8951383531093597
[08/27/2025 20:39:40 INFO]: Training loss at epoch 10: 1.1961370706558228
[08/27/2025 20:39:44 INFO]: Training stats: {
    "score": -0.9980946609686708,
    "rmse": 0.9980946609686708
}
[08/27/2025 20:39:44 INFO]: Val stats: {
    "score": -0.6579454245634152,
    "rmse": 0.6579454245634152
}
[08/27/2025 20:39:44 INFO]: Test stats: {
    "score": -0.8777887236824281,
    "rmse": 0.8777887236824281
}
[08/27/2025 20:40:02 INFO]: Training loss at epoch 30: 0.8631115257740021
[08/27/2025 20:40:08 INFO]: Training loss at epoch 31: 0.7872145622968674
[08/27/2025 20:40:11 INFO]: Training loss at epoch 7: 1.0323229879140854
[08/27/2025 20:40:13 INFO]: Training loss at epoch 28: 1.1266127228736877
[08/27/2025 20:40:15 INFO]: Training loss at epoch 9: 1.31782466173172
[08/27/2025 20:40:18 INFO]: Training loss at epoch 32: 1.08912193775177
[08/27/2025 20:40:31 INFO]: New best epoch, val score: -0.6579454245634152
[08/27/2025 20:40:31 INFO]: Saving model to: unhanged-Shanesha_trial_20/model_best.pth
[08/27/2025 20:40:50 INFO]: Training loss at epoch 32: 0.946511447429657
[08/27/2025 20:40:55 INFO]: Training loss at epoch 29: 1.1437463164329529
[08/27/2025 20:40:57 INFO]: Training loss at epoch 3: 1.6533704996109009
[08/27/2025 20:40:59 INFO]: Training loss at epoch 33: 0.8934148252010345
[08/27/2025 20:41:11 INFO]: Training stats: {
    "score": -1.0261506669680058,
    "rmse": 1.0261506669680058
}
[08/27/2025 20:41:11 INFO]: Val stats: {
    "score": -0.6686052169256935,
    "rmse": 0.6686052169256935
}
[08/27/2025 20:41:11 INFO]: Test stats: {
    "score": -0.8811995756256092,
    "rmse": 0.8811995756256092
}
[08/27/2025 20:41:11 INFO]: Training loss at epoch 50: 1.0606928765773773
[08/27/2025 20:41:16 INFO]: Training loss at epoch 13: 1.0737416446208954
[08/27/2025 20:41:26 INFO]: Training loss at epoch 13: 1.1837473511695862
[08/27/2025 20:41:30 INFO]: Training loss at epoch 14: 1.2415101528167725
[08/27/2025 20:41:32 INFO]: Training loss at epoch 33: 1.134328842163086
[08/27/2025 20:41:38 INFO]: Training loss at epoch 50: 1.2472681105136871
[08/27/2025 20:41:41 INFO]: Training loss at epoch 34: 1.031419277191162
[08/27/2025 20:41:55 INFO]: Training loss at epoch 30: 1.1189629435539246
[08/27/2025 20:42:13 INFO]: Training loss at epoch 34: 1.0459192395210266
[08/27/2025 20:42:22 INFO]: Training loss at epoch 35: 1.3765147626399994
[08/27/2025 20:42:28 INFO]: Training stats: {
    "score": -1.241167164343968,
    "rmse": 1.241167164343968
}
[08/27/2025 20:42:28 INFO]: Val stats: {
    "score": -1.1168526668831222,
    "rmse": 1.1168526668831222
}
[08/27/2025 20:42:28 INFO]: Test stats: {
    "score": -1.20405640089779,
    "rmse": 1.20405640089779
}
[08/27/2025 20:42:37 INFO]: Training loss at epoch 31: 1.1319530010223389
[08/27/2025 20:42:54 INFO]: Training loss at epoch 35: 0.87556591629982
[08/27/2025 20:42:59 INFO]: New best epoch, val score: -0.6625048634276965
[08/27/2025 20:42:59 INFO]: Saving model to: unhanged-Shanesha_trial_52/model_best.pth
[08/27/2025 20:43:03 INFO]: Training loss at epoch 36: 0.9385639727115631
[08/27/2025 20:43:11 INFO]: Training loss at epoch 35: 1.0195212960243225
[08/27/2025 20:43:19 INFO]: Training loss at epoch 32: 0.8824286758899689
[08/27/2025 20:43:22 INFO]: Training loss at epoch 51: 1.0089613795280457
[08/27/2025 20:43:36 INFO]: Training loss at epoch 36: 1.106804072856903
[08/27/2025 20:43:36 INFO]: Training loss at epoch 29: 1.112568736076355
[08/27/2025 20:43:38 INFO]: Running Final Evaluation...
[08/27/2025 20:43:40 INFO]: Training loss at epoch 14: 1.2324818968772888
[08/27/2025 20:43:45 INFO]: Training loss at epoch 37: 1.145133376121521
[08/27/2025 20:43:55 INFO]: Training loss at epoch 6: 1.0682136416435242
[08/27/2025 20:44:02 INFO]: Training loss at epoch 12: 0.9436045587062836
[08/27/2025 20:44:03 INFO]: Training loss at epoch 33: 1.0150394439697266
[08/27/2025 20:44:20 INFO]: Training loss at epoch 37: 1.2200335264205933
[08/27/2025 20:44:30 INFO]: Training accuracy: {
    "score": -1.0100287968059236,
    "rmse": 1.0100287968059236
}
[08/27/2025 20:44:30 INFO]: Val accuracy: {
    "score": -0.6605185842444363,
    "rmse": 0.6605185842444363
}
[08/27/2025 20:44:30 INFO]: Test accuracy: {
    "score": -0.8694848175164902,
    "rmse": 0.8694848175164902
}
[08/27/2025 20:44:30 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_40",
    "best_epoch": 20,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8694848175164902,
        "rmse": 0.8694848175164902
    },
    "train_stats": {
        "score": -1.0100287968059236,
        "rmse": 1.0100287968059236
    },
    "val_stats": {
        "score": -0.6605185842444363,
        "rmse": 0.6605185842444363
    }
}
[08/27/2025 20:44:30 INFO]: Procewss finished for trial unhanged-Shanesha_trial_40
[08/27/2025 20:44:30 INFO]: 
_________________________________________________

[08/27/2025 20:44:30 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:44:30 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 0.7246519389725721
  attention_dropout: 0.36795619287398273
  ffn_dropout: 0.36795619287398273
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00011206636698218217
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_55

[08/27/2025 20:44:30 INFO]: Training loss at epoch 38: 0.8610585331916809
[08/27/2025 20:44:30 INFO]: This ft_transformer has 8.771 million parameters.
[08/27/2025 20:44:30 INFO]: Training will start at epoch 0.
[08/27/2025 20:44:30 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:44:47 INFO]: Training loss at epoch 34: 1.046052634716034
[08/27/2025 20:45:05 INFO]: Training loss at epoch 38: 1.3025610148906708
[08/27/2025 20:45:10 INFO]: New best epoch, val score: -0.6622209289516352
[08/27/2025 20:45:10 INFO]: Saving model to: unhanged-Shanesha_trial_52/model_best.pth
[08/27/2025 20:45:15 INFO]: Training loss at epoch 39: 1.155839204788208
[08/27/2025 20:45:31 INFO]: Training stats: {
    "score": -0.9960391443188661,
    "rmse": 0.9960391443188661
}
[08/27/2025 20:45:31 INFO]: Val stats: {
    "score": -0.6753096748052207,
    "rmse": 0.6753096748052207
}
[08/27/2025 20:45:31 INFO]: Test stats: {
    "score": -0.8699912418483464,
    "rmse": 0.8699912418483464
}
[08/27/2025 20:45:32 INFO]: Training loss at epoch 35: 0.973048985004425
[08/27/2025 20:45:40 INFO]: Training loss at epoch 4: 1.0697952508926392
[08/27/2025 20:45:50 INFO]: Training loss at epoch 39: 0.9236596524715424
[08/27/2025 20:46:06 INFO]: Training stats: {
    "score": -1.0113937401554567,
    "rmse": 1.0113937401554567
}
[08/27/2025 20:46:06 INFO]: Val stats: {
    "score": -0.6621182951142289,
    "rmse": 0.6621182951142289
}
[08/27/2025 20:46:06 INFO]: Test stats: {
    "score": -0.8688938013074271,
    "rmse": 0.8688938013074271
}
[08/27/2025 20:46:12 INFO]: New best epoch, val score: -0.6621182951142289
[08/27/2025 20:46:12 INFO]: Saving model to: unhanged-Shanesha_trial_52/model_best.pth
[08/27/2025 20:46:17 INFO]: Training loss at epoch 40: 1.1419241428375244
[08/27/2025 20:46:17 INFO]: Training loss at epoch 36: 0.7749519646167755
[08/27/2025 20:46:18 INFO]: Training loss at epoch 10: 1.0994723439216614
[08/27/2025 20:46:29 INFO]: Training loss at epoch 11: 1.202159583568573
[08/27/2025 20:46:52 INFO]: Training loss at epoch 40: 0.9652255177497864
[08/27/2025 20:46:52 INFO]: Training loss at epoch 50: 0.8012440204620361
[08/27/2025 20:46:58 INFO]: Training loss at epoch 8: 1.5200881361961365
[08/27/2025 20:47:01 INFO]: Training loss at epoch 41: 1.1451491713523865
[08/27/2025 20:47:03 INFO]: Training loss at epoch 37: 1.0460070371627808
[08/27/2025 20:47:07 INFO]: Running Final Evaluation...
[08/27/2025 20:47:24 INFO]: Training accuracy: {
    "score": -1.0095749285887226,
    "rmse": 1.0095749285887226
}
[08/27/2025 20:47:24 INFO]: Val accuracy: {
    "score": -0.6557663709355026,
    "rmse": 0.6557663709355026
}
[08/27/2025 20:47:24 INFO]: Test accuracy: {
    "score": -0.8680003945704756,
    "rmse": 0.8680003945704756
}
[08/27/2025 20:47:24 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_51",
    "best_epoch": 10,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8680003945704756,
        "rmse": 0.8680003945704756
    },
    "train_stats": {
        "score": -1.0095749285887226,
        "rmse": 1.0095749285887226
    },
    "val_stats": {
        "score": -0.6557663709355026,
        "rmse": 0.6557663709355026
    }
}
[08/27/2025 20:47:24 INFO]: Procewss finished for trial unhanged-Shanesha_trial_51
[08/27/2025 20:47:24 INFO]: 
_________________________________________________

[08/27/2025 20:47:24 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:47:24 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 1.1240765385303084
  attention_dropout: 0.38719011542109927
  ffn_dropout: 0.38719011542109927
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00010919255619191182
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_56

[08/27/2025 20:47:24 INFO]: This ft_transformer has 10.025 million parameters.
[08/27/2025 20:47:24 INFO]: Training will start at epoch 0.
[08/27/2025 20:47:24 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:47:37 INFO]: Training loss at epoch 41: 0.9517323672771454
[08/27/2025 20:47:43 INFO]: New best epoch, val score: -0.6620864615502109
[08/27/2025 20:47:43 INFO]: Saving model to: unhanged-Shanesha_trial_52/model_best.pth
[08/27/2025 20:47:45 INFO]: Training stats: {
    "score": -1.0019296743684551,
    "rmse": 1.0019296743684551
}
[08/27/2025 20:47:45 INFO]: Val stats: {
    "score": -0.70835465015419,
    "rmse": 0.70835465015419
}
[08/27/2025 20:47:45 INFO]: Test stats: {
    "score": -0.8911602617713302,
    "rmse": 0.8911602617713302
}
[08/27/2025 20:47:49 INFO]: Training loss at epoch 38: 0.9729789793491364
[08/27/2025 20:48:12 INFO]: Training loss at epoch 14: 0.8498047888278961
[08/27/2025 20:48:22 INFO]: Training loss at epoch 42: 1.0695221424102783
[08/27/2025 20:48:23 INFO]: Training loss at epoch 15: 0.9270460605621338
[08/27/2025 20:48:24 INFO]: Training loss at epoch 14: 1.0606715381145477
[08/27/2025 20:48:29 INFO]: Training loss at epoch 0: 1.2308728098869324
[08/27/2025 20:48:32 INFO]: Training loss at epoch 39: 1.0429129004478455
[08/27/2025 20:48:48 INFO]: Training stats: {
    "score": -1.00774091953738,
    "rmse": 1.00774091953738
}
[08/27/2025 20:48:48 INFO]: Val stats: {
    "score": -0.6647099504866418,
    "rmse": 0.6647099504866418
}
[08/27/2025 20:48:48 INFO]: Test stats: {
    "score": -0.8709903257527037,
    "rmse": 0.8709903257527037
}
[08/27/2025 20:48:48 INFO]: Training loss at epoch 51: 1.1123389601707458
[08/27/2025 20:48:57 INFO]: New best epoch, val score: -0.7253566625784443
[08/27/2025 20:48:57 INFO]: Saving model to: unhanged-Shanesha_trial_55/model_best.pth
[08/27/2025 20:49:03 INFO]: Training loss at epoch 43: 0.9948639273643494
[08/27/2025 20:49:23 INFO]: Training loss at epoch 10: 1.2228136658668518
[08/27/2025 20:49:30 INFO]: Training loss at epoch 40: 1.1069116592407227
[08/27/2025 20:49:45 INFO]: Training loss at epoch 44: 0.9372875392436981
[08/27/2025 20:50:12 INFO]: Training loss at epoch 41: 0.9708285331726074
[08/27/2025 20:50:27 INFO]: Training loss at epoch 45: 1.1074238121509552
[08/27/2025 20:50:27 INFO]: Training loss at epoch 5: 1.3840448260307312
[08/27/2025 20:50:38 INFO]: Training loss at epoch 15: 1.0903327465057373
[08/27/2025 20:50:53 INFO]: Training loss at epoch 42: 1.191008746623993
[08/27/2025 20:50:53 INFO]: Training loss at epoch 7: 1.0034327805042267
[08/27/2025 20:50:55 INFO]: Training loss at epoch 13: 1.1351544260978699
[08/27/2025 20:51:08 INFO]: Training loss at epoch 46: 0.8692778944969177
[08/27/2025 20:51:27 INFO]: Training loss at epoch 31: 0.9162103533744812
[08/27/2025 20:51:28 INFO]: Training loss at epoch 0: 1.3853596448898315
[08/27/2025 20:51:30 INFO]: Training loss at epoch 36: 0.9562288224697113
[08/27/2025 20:51:34 INFO]: Training loss at epoch 43: 1.0735275447368622
[08/27/2025 20:51:40 INFO]: New best epoch, val score: -0.6991123047926572
[08/27/2025 20:51:40 INFO]: Saving model to: unhanged-Shanesha_trial_50/model_best.pth
[08/27/2025 20:51:50 INFO]: Training loss at epoch 47: 1.0051375031471252
[08/27/2025 20:51:59 INFO]: New best epoch, val score: -0.77705026086463
[08/27/2025 20:51:59 INFO]: Saving model to: unhanged-Shanesha_trial_56/model_best.pth
[08/27/2025 20:52:17 INFO]: Training loss at epoch 44: 0.9822346568107605
[08/27/2025 20:52:34 INFO]: Training loss at epoch 48: 0.9846640825271606
[08/27/2025 20:52:36 INFO]: Training loss at epoch 1: 1.3024939894676208
[08/27/2025 20:52:59 INFO]: Training loss at epoch 45: 1.102088987827301
[08/27/2025 20:53:09 INFO]: Training loss at epoch 11: 0.9205895662307739
[08/27/2025 20:53:17 INFO]: Training loss at epoch 49: 0.8840721845626831
[08/27/2025 20:53:18 INFO]: Training loss at epoch 12: 1.2289067506790161
[08/27/2025 20:53:33 INFO]: Training stats: {
    "score": -0.9987006865085682,
    "rmse": 0.9987006865085682
}
[08/27/2025 20:53:33 INFO]: Val stats: {
    "score": -0.6942613096988608,
    "rmse": 0.6942613096988608
}
[08/27/2025 20:53:33 INFO]: Test stats: {
    "score": -0.8787706692479453,
    "rmse": 0.8787706692479453
}
[08/27/2025 20:53:40 INFO]: Training loss at epoch 9: 1.5079520344734192
[08/27/2025 20:53:45 INFO]: Training loss at epoch 46: 0.9486061036586761
[08/27/2025 20:53:55 INFO]: Training loss at epoch 51: 0.8888980448246002
[08/27/2025 20:54:01 INFO]: New best epoch, val score: -0.6635121743742985
[08/27/2025 20:54:01 INFO]: Saving model to: unhanged-Shanesha_trial_47/model_best.pth
[08/27/2025 20:54:18 INFO]: Training loss at epoch 50: 0.8487772643566132
[08/27/2025 20:54:29 INFO]: Training loss at epoch 47: 0.9447710812091827
[08/27/2025 20:54:54 INFO]: Training loss at epoch 15: 0.8734719157218933
[08/27/2025 20:55:04 INFO]: Training loss at epoch 51: 1.0599147081375122
[08/27/2025 20:55:05 INFO]: Training loss at epoch 16: 0.8996310532093048
[08/27/2025 20:55:09 INFO]: Training loss at epoch 15: 0.9312347173690796
[08/27/2025 20:55:15 INFO]: Training loss at epoch 48: 1.1104207038879395
[08/27/2025 20:55:19 INFO]: Training loss at epoch 6: 1.21121484041214
[08/27/2025 20:55:20 INFO]: Running Final Evaluation...
[08/27/2025 20:55:37 INFO]: Training accuracy: {
    "score": -1.0117592088987095,
    "rmse": 1.0117592088987095
}
[08/27/2025 20:55:37 INFO]: Val accuracy: {
    "score": -0.6638639850689064,
    "rmse": 0.6638639850689064
}
[08/27/2025 20:55:37 INFO]: Test accuracy: {
    "score": -0.8719307539332,
    "rmse": 0.8719307539332
}
[08/27/2025 20:55:37 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_53",
    "best_epoch": 17,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8719307539332,
        "rmse": 0.8719307539332
    },
    "train_stats": {
        "score": -1.0117592088987095,
        "rmse": 1.0117592088987095
    },
    "val_stats": {
        "score": -0.6638639850689064,
        "rmse": 0.6638639850689064
    }
}
[08/27/2025 20:55:37 INFO]: Procewss finished for trial unhanged-Shanesha_trial_53
[08/27/2025 20:55:37 INFO]: 
_________________________________________________

[08/27/2025 20:55:37 INFO]: train_net_for_optune.py main() running.
[08/27/2025 20:55:37 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 1.18321872659041
  attention_dropout: 0.3882650401350155
  ffn_dropout: 0.3882650401350155
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00012419872703832107
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_57

[08/27/2025 20:55:37 INFO]: This ft_transformer has 10.210 million parameters.
[08/27/2025 20:55:37 INFO]: Training will start at epoch 0.
[08/27/2025 20:55:37 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 20:55:48 INFO]: Training loss at epoch 52: 0.8799713850021362
[08/27/2025 20:55:51 INFO]: Training loss at epoch 52: 0.9464470148086548
[08/27/2025 20:56:09 INFO]: Training stats: {
    "score": -1.2449082461807899,
    "rmse": 1.2449082461807899
}
[08/27/2025 20:56:09 INFO]: Val stats: {
    "score": -1.1223008341300487,
    "rmse": 1.1223008341300487
}
[08/27/2025 20:56:09 INFO]: Test stats: {
    "score": -1.2074219273365476,
    "rmse": 1.2074219273365476
}
[08/27/2025 20:56:11 INFO]: Training loss at epoch 11: 0.9578052759170532
[08/27/2025 20:56:21 INFO]: Training loss at epoch 1: 1.4176818132400513
[08/27/2025 20:56:35 INFO]: Training loss at epoch 53: 1.3796603977680206
[08/27/2025 20:57:01 INFO]: Training loss at epoch 2: 1.0190945267677307
[08/27/2025 20:57:16 INFO]: Training loss at epoch 54: 1.0140080451965332
[08/27/2025 20:57:38 INFO]: Training loss at epoch 16: 0.8718293011188507
[08/27/2025 20:57:49 INFO]: Training loss at epoch 14: 0.9265535473823547
[08/27/2025 20:57:51 INFO]: Training loss at epoch 8: 1.0643305778503418
[08/27/2025 20:57:57 INFO]: Training loss at epoch 55: 1.201724886894226
[08/27/2025 20:58:25 INFO]: New best epoch, val score: -0.6657392182269688
[08/27/2025 20:58:25 INFO]: Saving model to: unhanged-Shanesha_trial_42/model_best.pth
[08/27/2025 20:58:38 INFO]: New best epoch, val score: -0.6910023144950921
[08/27/2025 20:58:38 INFO]: Saving model to: unhanged-Shanesha_trial_50/model_best.pth
[08/27/2025 20:58:39 INFO]: Training loss at epoch 56: 1.110459327697754
[08/27/2025 20:59:00 INFO]: Training loss at epoch 30: 0.9369978308677673
[08/27/2025 20:59:20 INFO]: Training loss at epoch 57: 1.0798056721687317
[08/27/2025 20:59:44 INFO]: Training loss at epoch 0: 1.178360939025879
[08/27/2025 20:59:46 INFO]: Training loss at epoch 37: 1.104128360748291
[08/27/2025 20:59:59 INFO]: Training loss at epoch 7: 1.522686243057251
[08/27/2025 21:00:01 INFO]: Training loss at epoch 58: 0.9180105626583099
[08/27/2025 21:00:05 INFO]: Training loss at epoch 12: 1.0123724937438965
[08/27/2025 21:00:09 INFO]: Training loss at epoch 13: 1.4406526684761047
[08/27/2025 21:00:16 INFO]: New best epoch, val score: -0.7169938174928869
[08/27/2025 21:00:16 INFO]: Saving model to: unhanged-Shanesha_trial_57/model_best.pth
[08/27/2025 21:00:42 INFO]: Training loss at epoch 59: 1.1116057634353638
[08/27/2025 21:00:52 INFO]: Training loss at epoch 2: 1.838743507862091
[08/27/2025 21:00:57 INFO]: Training stats: {
    "score": -0.9989009529089902,
    "rmse": 0.9989009529089902
}
[08/27/2025 21:00:57 INFO]: Val stats: {
    "score": -0.6687007451527296,
    "rmse": 0.6687007451527296
}
[08/27/2025 21:00:57 INFO]: Test stats: {
    "score": -0.8667176431080902,
    "rmse": 0.8667176431080902
}
[08/27/2025 21:00:58 INFO]: Training loss at epoch 52: 0.9336486756801605
[08/27/2025 21:01:04 INFO]: Training loss at epoch 3: 1.240479052066803
[08/27/2025 21:01:26 INFO]: New best epoch, val score: -0.6687818092027153
[08/27/2025 21:01:26 INFO]: Saving model to: unhanged-Shanesha_trial_56/model_best.pth
[08/27/2025 21:01:35 INFO]: Training loss at epoch 16: 1.4384359419345856
[08/27/2025 21:01:36 INFO]: New best epoch, val score: -0.6805916619617842
[08/27/2025 21:01:36 INFO]: Saving model to: unhanged-Shanesha_trial_55/model_best.pth
[08/27/2025 21:01:41 INFO]: Training loss at epoch 60: 1.0735177099704742
[08/27/2025 21:01:43 INFO]: Training loss at epoch 17: 1.0892104506492615
[08/27/2025 21:01:52 INFO]: Training loss at epoch 16: 0.8919055163860321
[08/27/2025 21:02:27 INFO]: Training loss at epoch 61: 0.7845738381147385
[08/27/2025 21:02:46 INFO]: Training loss at epoch 53: 0.9290029406547546
[08/27/2025 21:02:47 INFO]: Training loss at epoch 10: 1.7430648803710938
[08/27/2025 21:02:52 INFO]: Training loss at epoch 12: 1.4830655455589294
[08/27/2025 21:03:00 INFO]: Training loss at epoch 32: 0.9266038537025452
[08/27/2025 21:03:12 INFO]: Training loss at epoch 62: 0.9710094630718231
[08/27/2025 21:03:57 INFO]: Training loss at epoch 63: 1.0356008410453796
[08/27/2025 21:04:34 INFO]: Training loss at epoch 17: 0.9900143444538116
[08/27/2025 21:04:40 INFO]: Training loss at epoch 15: 1.1134952902793884
[08/27/2025 21:04:40 INFO]: Training loss at epoch 1: 1.2309523820877075
[08/27/2025 21:04:42 INFO]: Training loss at epoch 64: 1.0493792295455933
[08/27/2025 21:04:46 INFO]: Training loss at epoch 9: 0.9860631227493286
[08/27/2025 21:04:56 INFO]: Training loss at epoch 8: 0.8785949051380157
[08/27/2025 21:05:17 INFO]: New best epoch, val score: -0.6580100369908094
[08/27/2025 21:05:17 INFO]: Saving model to: unhanged-Shanesha_trial_57/model_best.pth
[08/27/2025 21:05:28 INFO]: Training loss at epoch 65: 0.9200522005558014
[08/27/2025 21:05:36 INFO]: Training loss at epoch 4: 1.2070374488830566
[08/27/2025 21:05:52 INFO]: Training loss at epoch 3: 1.1029652953147888
[08/27/2025 21:06:12 INFO]: Training loss at epoch 66: 0.932152271270752
[08/27/2025 21:06:53 INFO]: Training loss at epoch 67: 1.033571869134903
[08/27/2025 21:07:08 INFO]: Training stats: {
    "score": -1.0551886457262998,
    "rmse": 1.0551886457262998
}
[08/27/2025 21:07:08 INFO]: Val stats: {
    "score": -0.6828903363303468,
    "rmse": 0.6828903363303468
}
[08/27/2025 21:07:08 INFO]: Test stats: {
    "score": -0.9005638029583952,
    "rmse": 0.9005638029583952
}
[08/27/2025 21:07:10 INFO]: Training loss at epoch 13: 1.0720416903495789
[08/27/2025 21:07:11 INFO]: Training loss at epoch 14: 1.3961352109909058
[08/27/2025 21:07:34 INFO]: Training loss at epoch 68: 0.8938315808773041
[08/27/2025 21:07:55 INFO]: New best epoch, val score: -0.6828903363303468
[08/27/2025 21:07:55 INFO]: Saving model to: unhanged-Shanesha_trial_50/model_best.pth
[08/27/2025 21:08:07 INFO]: Training loss at epoch 38: 0.948703408241272
[08/27/2025 21:08:14 INFO]: Training loss at epoch 53: 0.9371449649333954
[08/27/2025 21:08:15 INFO]: Training loss at epoch 69: 0.9272068440914154
[08/27/2025 21:08:30 INFO]: Training stats: {
    "score": -0.9969155854164388,
    "rmse": 0.9969155854164388
}
[08/27/2025 21:08:30 INFO]: Val stats: {
    "score": -0.6897720524424287,
    "rmse": 0.6897720524424287
}
[08/27/2025 21:08:30 INFO]: Test stats: {
    "score": -0.8758520912727104,
    "rmse": 0.8758520912727104
}
[08/27/2025 21:08:32 INFO]: Training loss at epoch 17: 1.1242271065711975
[08/27/2025 21:08:36 INFO]: Training loss at epoch 18: 1.261669933795929
[08/27/2025 21:08:46 INFO]: Training loss at epoch 17: 0.861882895231247
[08/27/2025 21:09:04 INFO]: Running Final Evaluation...
[08/27/2025 21:09:11 INFO]: Training loss at epoch 70: 1.163074791431427
[08/27/2025 21:09:22 INFO]: Training loss at epoch 2: 1.2405217289924622
[08/27/2025 21:09:32 INFO]: Training loss at epoch 11: 1.1019100546836853
[08/27/2025 21:09:33 INFO]: Training loss at epoch 9: 1.1888241171836853
[08/27/2025 21:09:40 INFO]: Training loss at epoch 13: 1.425951361656189
[08/27/2025 21:09:41 INFO]: Training loss at epoch 5: 1.0608854293823242
[08/27/2025 21:09:47 INFO]: Training loss at epoch 54: 1.023243486881256
[08/27/2025 21:09:53 INFO]: Training loss at epoch 71: 0.9630240797996521
[08/27/2025 21:10:16 INFO]: Training loss at epoch 31: 1.01821631193161
[08/27/2025 21:10:18 INFO]: New best epoch, val score: -0.6686677200329281
[08/27/2025 21:10:18 INFO]: Saving model to: unhanged-Shanesha_trial_49/model_best.pth
[08/27/2025 21:10:21 INFO]: Training loss at epoch 4: 1.4228242635726929
[08/27/2025 21:10:35 INFO]: Training loss at epoch 72: 0.9329542517662048
[08/27/2025 21:10:41 INFO]: Running Final Evaluation...
[08/27/2025 21:10:57 INFO]: Training accuracy: {
    "score": -1.0091277191854444,
    "rmse": 1.0091277191854444
}
[08/27/2025 21:10:57 INFO]: Val accuracy: {
    "score": -0.6620864615502109,
    "rmse": 0.6620864615502109
}
[08/27/2025 21:10:57 INFO]: Test accuracy: {
    "score": -0.8678273626924025,
    "rmse": 0.8678273626924025
}
[08/27/2025 21:10:57 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_52",
    "best_epoch": 41,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8678273626924025,
        "rmse": 0.8678273626924025
    },
    "train_stats": {
        "score": -1.0091277191854444,
        "rmse": 1.0091277191854444
    },
    "val_stats": {
        "score": -0.6620864615502109,
        "rmse": 0.6620864615502109
    }
}
[08/27/2025 21:10:57 INFO]: Procewss finished for trial unhanged-Shanesha_trial_52
[08/27/2025 21:10:57 INFO]: 
_________________________________________________

[08/27/2025 21:10:57 INFO]: train_net_for_optune.py main() running.
[08/27/2025 21:10:57 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 1.1624561464951082
  attention_dropout: 0.3718917416366812
  ffn_dropout: 0.3718917416366812
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.000593866816525143
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_58

[08/27/2025 21:10:58 INFO]: This ft_transformer has 10.148 million parameters.
[08/27/2025 21:10:58 INFO]: Training will start at epoch 0.
[08/27/2025 21:10:58 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 21:11:10 INFO]: Training stats: {
    "score": -1.1268365701193819,
    "rmse": 1.1268365701193819
}
[08/27/2025 21:11:10 INFO]: Val stats: {
    "score": -0.9507101689419795,
    "rmse": 0.9507101689419795
}
[08/27/2025 21:11:10 INFO]: Test stats: {
    "score": -1.0677542924973717,
    "rmse": 1.0677542924973717
}
[08/27/2025 21:11:19 INFO]: Training loss at epoch 18: 0.9621962606906891
[08/27/2025 21:11:21 INFO]: Training loss at epoch 16: 1.1641575992107391
[08/27/2025 21:12:02 INFO]: Training accuracy: {
    "score": -1.014029499173471,
    "rmse": 1.014029499173471
}
[08/27/2025 21:12:02 INFO]: Val accuracy: {
    "score": -0.6604202617512478,
    "rmse": 0.6604202617512478
}
[08/27/2025 21:12:02 INFO]: Test accuracy: {
    "score": -0.8713763513058808,
    "rmse": 0.8713763513058808
}
[08/27/2025 21:12:02 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_26",
    "best_epoch": 7,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8713763513058808,
        "rmse": 0.8713763513058808
    },
    "train_stats": {
        "score": -1.014029499173471,
        "rmse": 1.014029499173471
    },
    "val_stats": {
        "score": -0.6604202617512478,
        "rmse": 0.6604202617512478
    }
}
[08/27/2025 21:12:02 INFO]: Procewss finished for trial unhanged-Shanesha_trial_26
[08/27/2025 21:12:03 INFO]: 
_________________________________________________

[08/27/2025 21:12:03 INFO]: train_net_for_optune.py main() running.
[08/27/2025 21:12:03 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 2.2846906071141717
  attention_dropout: 0.3867715862875619
  ffn_dropout: 0.3867715862875619
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0005393652349030118
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_59

[08/27/2025 21:12:03 INFO]: This ft_transformer has 13.678 million parameters.
[08/27/2025 21:12:03 INFO]: Training will start at epoch 0.
[08/27/2025 21:12:03 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 21:14:03 INFO]: Training loss at epoch 10: 0.9786519408226013
[08/27/2025 21:14:04 INFO]: Training loss at epoch 15: 1.0273413956165314
[08/27/2025 21:14:05 INFO]: Training loss at epoch 14: 0.9888773560523987
[08/27/2025 21:14:06 INFO]: Training loss at epoch 6: 1.2681151032447815
[08/27/2025 21:14:20 INFO]: Training loss at epoch 3: 1.2982089519500732
[08/27/2025 21:14:41 INFO]: Training loss at epoch 33: 1.000530868768692
[08/27/2025 21:14:55 INFO]: New best epoch, val score: -0.6629815176404104
[08/27/2025 21:14:55 INFO]: Saving model to: unhanged-Shanesha_trial_50/model_best.pth
[08/27/2025 21:15:18 INFO]: Training loss at epoch 5: 0.9010089933872223
[08/27/2025 21:15:22 INFO]: Training loss at epoch 0: 1.0274196863174438
[08/27/2025 21:15:26 INFO]: Training loss at epoch 54: 1.0504191517829895
[08/27/2025 21:15:28 INFO]: Training loss at epoch 19: 0.9819052219390869
[08/27/2025 21:15:29 INFO]: Training loss at epoch 18: 1.194709300994873
[08/27/2025 21:15:42 INFO]: Training loss at epoch 18: 0.9861302673816681
[08/27/2025 21:15:54 INFO]: New best epoch, val score: -0.7183985734006619
[08/27/2025 21:15:54 INFO]: Saving model to: unhanged-Shanesha_trial_58/model_best.pth
[08/27/2025 21:16:04 INFO]: Training loss at epoch 10: 0.962044358253479
[08/27/2025 21:16:25 INFO]: Training loss at epoch 12: 0.9119693636894226
[08/27/2025 21:16:34 INFO]: Training loss at epoch 14: 1.564375102519989
[08/27/2025 21:16:54 INFO]: Training loss at epoch 55: 0.9698534607887268
[08/27/2025 21:17:26 INFO]: Training loss at epoch 0: 1.289489984512329
[08/27/2025 21:17:41 INFO]: Training stats: {
    "score": -1.0262207569471757,
    "rmse": 1.0262207569471757
}
[08/27/2025 21:17:41 INFO]: Val stats: {
    "score": -0.7698808732874437,
    "rmse": 0.7698808732874437
}
[08/27/2025 21:17:41 INFO]: Test stats: {
    "score": -0.9294039697361828,
    "rmse": 0.9294039697361828
}
[08/27/2025 21:18:06 INFO]: New best epoch, val score: -0.799245916345175
[08/27/2025 21:18:06 INFO]: Saving model to: unhanged-Shanesha_trial_59/model_best.pth
[08/27/2025 21:18:12 INFO]: Training loss at epoch 17: 1.1341639161109924
[08/27/2025 21:18:12 INFO]: Training loss at epoch 19: 0.8572896420955658
[08/27/2025 21:18:14 INFO]: Training loss at epoch 7: 0.9221386015415192
[08/27/2025 21:18:56 INFO]: Training loss at epoch 4: 1.158425897359848
[08/27/2025 21:19:50 INFO]: Training loss at epoch 6: 0.9349768459796906
[08/27/2025 21:19:59 INFO]: Training loss at epoch 1: 5.818852603435516
[08/27/2025 21:20:34 INFO]: Training stats: {
    "score": -1.0775955069141536,
    "rmse": 1.0775955069141536
}
[08/27/2025 21:20:34 INFO]: Val stats: {
    "score": -0.7028846995499508,
    "rmse": 0.7028846995499508
}
[08/27/2025 21:20:34 INFO]: Test stats: {
    "score": -0.9206100095955471,
    "rmse": 0.9206100095955471
}
[08/27/2025 21:20:43 INFO]: Training loss at epoch 11: 1.090297281742096
[08/27/2025 21:20:49 INFO]: Training loss at epoch 11: 1.0870943665504456
[08/27/2025 21:20:50 INFO]: Training loss at epoch 16: 0.883410632610321
[08/27/2025 21:20:53 INFO]: Training loss at epoch 15: 1.207968831062317
[08/27/2025 21:21:43 INFO]: Training loss at epoch 32: 0.9855737388134003
[08/27/2025 21:21:45 INFO]: New best epoch, val score: -0.6629521365559073
[08/27/2025 21:21:45 INFO]: Saving model to: unhanged-Shanesha_trial_47/model_best.pth
[08/27/2025 21:22:15 INFO]: Training loss at epoch 19: 1.0026031732559204
[08/27/2025 21:22:29 INFO]: Training loss at epoch 19: 1.004151999950409
[08/27/2025 21:22:31 INFO]: Training loss at epoch 55: 1.0790963172912598
[08/27/2025 21:22:37 INFO]: Training loss at epoch 8: 1.0776225328445435
[08/27/2025 21:23:14 INFO]: Training loss at epoch 13: 1.3641947507858276
[08/27/2025 21:23:24 INFO]: Training loss at epoch 15: 1.442846953868866
[08/27/2025 21:23:39 INFO]: Training loss at epoch 1: 7.267555296421051
[08/27/2025 21:23:57 INFO]: Training loss at epoch 5: 0.7962811887264252
[08/27/2025 21:24:01 INFO]: Training loss at epoch 56: 1.315698891878128
[08/27/2025 21:24:32 INFO]: Training loss at epoch 20: 1.2900276482105255
[08/27/2025 21:24:39 INFO]: Training stats: {
    "score": -0.9974143490036543,
    "rmse": 0.9974143490036543
}
[08/27/2025 21:24:39 INFO]: Val stats: {
    "score": -0.6847907066238001,
    "rmse": 0.6847907066238001
}
[08/27/2025 21:24:39 INFO]: Test stats: {
    "score": -0.8749075394809314,
    "rmse": 0.8749075394809314
}
[08/27/2025 21:24:44 INFO]: Training loss at epoch 7: 1.122340738773346
[08/27/2025 21:24:51 INFO]: Training stats: {
    "score": -1.02371467366179,
    "rmse": 1.02371467366179
}
[08/27/2025 21:24:51 INFO]: Val stats: {
    "score": -0.7633692953608681,
    "rmse": 0.7633692953608681
}
[08/27/2025 21:24:51 INFO]: Test stats: {
    "score": -0.9248118672453363,
    "rmse": 0.9248118672453363
}
[08/27/2025 21:24:55 INFO]: Training loss at epoch 2: 2.58578097820282
[08/27/2025 21:25:06 INFO]: Training loss at epoch 18: 1.2116665244102478
[08/27/2025 21:25:33 INFO]: Training loss at epoch 12: 1.15963476896286
[08/27/2025 21:26:06 INFO]: Training loss at epoch 34: 0.9102412164211273
[08/27/2025 21:26:48 INFO]: Training loss at epoch 9: 0.9391674995422363
[08/27/2025 21:27:23 INFO]: Training loss at epoch 20: 1.0298754274845123
[08/27/2025 21:27:40 INFO]: Training loss at epoch 12: 1.0307944416999817
[08/27/2025 21:27:42 INFO]: Training loss at epoch 17: 1.0396965444087982
[08/27/2025 21:27:46 INFO]: Training loss at epoch 16: 1.07549387216568
[08/27/2025 21:28:11 INFO]: Training stats: {
    "score": -1.0635252650120828,
    "rmse": 1.0635252650120828
}
[08/27/2025 21:28:11 INFO]: Val stats: {
    "score": -0.6909204047816336,
    "rmse": 0.6909204047816336
}
[08/27/2025 21:28:11 INFO]: Test stats: {
    "score": -0.908861064191022,
    "rmse": 0.908861064191022
}
[08/27/2025 21:28:31 INFO]: Training loss at epoch 6: 0.9799138009548187
[08/27/2025 21:29:19 INFO]: Training loss at epoch 8: 0.9739325046539307
[08/27/2025 21:29:35 INFO]: Training loss at epoch 56: 1.0927664935588837
[08/27/2025 21:29:35 INFO]: Training loss at epoch 2: 2.3370010256767273
[08/27/2025 21:29:35 INFO]: Training loss at epoch 3: 1.741152822971344
[08/27/2025 21:29:52 INFO]: Training loss at epoch 14: 1.4625757336616516
[08/27/2025 21:30:04 INFO]: Training loss at epoch 16: 1.1291124820709229
[08/27/2025 21:30:17 INFO]: Training loss at epoch 13: 1.3372352123260498
[08/27/2025 21:30:56 INFO]: New best epoch, val score: -0.6616730050222556
[08/27/2025 21:30:56 INFO]: Saving model to: unhanged-Shanesha_trial_48/model_best.pth
[08/27/2025 21:30:57 INFO]: Training loss at epoch 57: 0.8504525125026703
[08/27/2025 21:31:17 INFO]: Training loss at epoch 21: 0.9337915480136871
[08/27/2025 21:31:27 INFO]: Training loss at epoch 20: 1.0434658527374268
[08/27/2025 21:31:43 INFO]: Training loss at epoch 20: 1.0483606457710266
[08/27/2025 21:31:56 INFO]: Training loss at epoch 19: 0.8695396184921265
[08/27/2025 21:32:41 INFO]: Training loss at epoch 10: 0.9299476146697998
[08/27/2025 21:33:09 INFO]: Training loss at epoch 33: 1.167428880929947
[08/27/2025 21:33:32 INFO]: Training loss at epoch 7: 0.8028818964958191
[08/27/2025 21:34:10 INFO]: Training loss at epoch 9: 0.9749546051025391
[08/27/2025 21:34:13 INFO]: Training stats: {
    "score": -1.0006272256806854,
    "rmse": 1.0006272256806854
}
[08/27/2025 21:34:13 INFO]: Val stats: {
    "score": -0.6725040155820537,
    "rmse": 0.6725040155820537
}
[08/27/2025 21:34:13 INFO]: Test stats: {
    "score": -0.8707017298314058,
    "rmse": 0.8707017298314058
}
[08/27/2025 21:34:20 INFO]: Training loss at epoch 21: 1.1088217496871948
[08/27/2025 21:34:27 INFO]: Training loss at epoch 4: 1.2841066122055054
[08/27/2025 21:34:42 INFO]: Training loss at epoch 13: 1.0305614471435547
[08/27/2025 21:34:43 INFO]: Training loss at epoch 18: 1.3518199920654297
[08/27/2025 21:34:48 INFO]: Training loss at epoch 17: 0.8955603539943695
[08/27/2025 21:34:59 INFO]: New best epoch, val score: -0.663041539838212
[08/27/2025 21:34:59 INFO]: Saving model to: unhanged-Shanesha_trial_58/model_best.pth
[08/27/2025 21:35:04 INFO]: Training loss at epoch 14: 1.1526873707771301
[08/27/2025 21:35:06 INFO]: New best epoch, val score: -0.6650498873407891
[08/27/2025 21:35:06 INFO]: Saving model to: unhanged-Shanesha_trial_42/model_best.pth
[08/27/2025 21:35:36 INFO]: New best epoch, val score: -0.6748229561646848
[08/27/2025 21:35:36 INFO]: Saving model to: unhanged-Shanesha_trial_54/model_best.pth
[08/27/2025 21:35:41 INFO]: Training loss at epoch 3: 1.7922604978084564
[08/27/2025 21:35:42 INFO]: Training stats: {
    "score": -1.0023267452809883,
    "rmse": 1.0023267452809883
}
[08/27/2025 21:35:42 INFO]: Val stats: {
    "score": -0.6989556193754196,
    "rmse": 0.6989556193754196
}
[08/27/2025 21:35:42 INFO]: Test stats: {
    "score": -0.8831331152164601,
    "rmse": 0.8831331152164601
}
[08/27/2025 21:36:35 INFO]: Training loss at epoch 15: 1.1393893361091614
[08/27/2025 21:36:43 INFO]: Training loss at epoch 57: 0.9879540205001831
[08/27/2025 21:36:44 INFO]: Training loss at epoch 11: 0.9879602789878845
[08/27/2025 21:36:47 INFO]: Training loss at epoch 17: 0.9724669456481934
[08/27/2025 21:37:31 INFO]: Training loss at epoch 35: 0.9047312140464783
[08/27/2025 21:37:33 INFO]: New best epoch, val score: -0.6575654510442783
[08/27/2025 21:37:33 INFO]: Saving model to: unhanged-Shanesha_trial_20/model_best.pth
[08/27/2025 21:37:51 INFO]: Training loss at epoch 58: 0.8727872669696808
[08/27/2025 21:37:54 INFO]: Training loss at epoch 22: 0.985396683216095
[08/27/2025 21:38:09 INFO]: Training loss at epoch 21: 1.026115208864212
[08/27/2025 21:38:11 INFO]: Training loss at epoch 8: 1.2411400079727173
[08/27/2025 21:38:24 INFO]: Training loss at epoch 21: 1.0492054224014282
[08/27/2025 21:38:47 INFO]: New best epoch, val score: -0.66274564592105
[08/27/2025 21:38:47 INFO]: Saving model to: unhanged-Shanesha_trial_41/model_best.pth
[08/27/2025 21:39:13 INFO]: Training loss at epoch 5: 0.8554370999336243
[08/27/2025 21:39:49 INFO]: New best epoch, val score: -0.6615760238045005
[08/27/2025 21:39:49 INFO]: Saving model to: unhanged-Shanesha_trial_58/model_best.pth
[08/27/2025 21:39:55 INFO]: Training loss at epoch 15: 1.2174649238586426
[08/27/2025 21:40:31 INFO]: New best epoch, val score: -0.6632599621130446
[08/27/2025 21:40:31 INFO]: Saving model to: unhanged-Shanesha_trial_54/model_best.pth
[08/27/2025 21:40:32 INFO]: Training loss at epoch 10: 0.9746478199958801
[08/27/2025 21:41:05 INFO]: Training loss at epoch 20: 0.8579947054386139
[08/27/2025 21:41:12 INFO]: Training loss at epoch 12: 1.1683616042137146
[08/27/2025 21:41:15 INFO]: Training loss at epoch 22: 1.0913617610931396
[08/27/2025 21:41:42 INFO]: Training loss at epoch 14: 0.7578300088644028
[08/27/2025 21:41:44 INFO]: Training loss at epoch 19: 0.9353967010974884
[08/27/2025 21:41:53 INFO]: Training loss at epoch 18: 0.9090835154056549
[08/27/2025 21:41:58 INFO]: Training loss at epoch 4: 1.3573808670043945
[08/27/2025 21:42:07 INFO]: New best epoch, val score: -0.663145820543026
[08/27/2025 21:42:07 INFO]: Saving model to: unhanged-Shanesha_trial_42/model_best.pth
[08/27/2025 21:42:39 INFO]: New best epoch, val score: -0.7891201449431342
[08/27/2025 21:42:39 INFO]: Saving model to: unhanged-Shanesha_trial_59/model_best.pth
[08/27/2025 21:43:11 INFO]: Training loss at epoch 9: 1.007649153470993
[08/27/2025 21:43:32 INFO]: Training loss at epoch 16: 0.9140726923942566
[08/27/2025 21:43:45 INFO]: Training loss at epoch 18: 0.879481703042984
[08/27/2025 21:43:59 INFO]: Training stats: {
    "score": -1.0603915290248684,
    "rmse": 1.0603915290248684
}
[08/27/2025 21:43:59 INFO]: Val stats: {
    "score": -0.8416051334268606,
    "rmse": 0.8416051334268606
}
[08/27/2025 21:43:59 INFO]: Test stats: {
    "score": -0.9810851216739778,
    "rmse": 0.9810851216739778
}
[08/27/2025 21:44:00 INFO]: Training loss at epoch 58: 0.9545062482357025
[08/27/2025 21:44:05 INFO]: Training loss at epoch 6: 1.0417088866233826
[08/27/2025 21:44:24 INFO]: Training loss at epoch 34: 1.090446025133133
[08/27/2025 21:44:37 INFO]: New best epoch, val score: -0.660579176426109
[08/27/2025 21:44:37 INFO]: Saving model to: unhanged-Shanesha_trial_58/model_best.pth
[08/27/2025 21:44:40 INFO]: Training loss at epoch 16: 0.9938562512397766
[08/27/2025 21:44:44 INFO]: Training stats: {
    "score": -1.0119678941345047,
    "rmse": 1.0119678941345047
}
[08/27/2025 21:44:44 INFO]: Val stats: {
    "score": -0.7356564260722819,
    "rmse": 0.7356564260722819
}
[08/27/2025 21:44:44 INFO]: Test stats: {
    "score": -0.9083479489122696,
    "rmse": 0.9083479489122696
}
[08/27/2025 21:44:47 INFO]: Training loss at epoch 23: 1.1224724650382996
[08/27/2025 21:45:00 INFO]: Training loss at epoch 59: 0.8475878238677979
[08/27/2025 21:45:02 INFO]: Training loss at epoch 22: 1.0017042756080627
[08/27/2025 21:45:11 INFO]: Training loss at epoch 11: 0.878995418548584
[08/27/2025 21:45:15 INFO]: Training loss at epoch 22: 0.951111227273941
[08/27/2025 21:45:20 INFO]: Training loss at epoch 13: 1.0871900916099548
[08/27/2025 21:45:33 INFO]: New best epoch, val score: -0.6603357770194193
[08/27/2025 21:45:33 INFO]: Saving model to: unhanged-Shanesha_trial_41/model_best.pth
[08/27/2025 21:45:43 INFO]: Running Final Evaluation...
[08/27/2025 21:47:20 INFO]: Training stats: {
    "score": -0.9988285834048446,
    "rmse": 0.9988285834048446
}
[08/27/2025 21:47:20 INFO]: Val stats: {
    "score": -0.6779995614910562,
    "rmse": 0.6779995614910562
}
[08/27/2025 21:47:20 INFO]: Test stats: {
    "score": -0.873085082949433,
    "rmse": 0.873085082949433
}
[08/27/2025 21:47:44 INFO]: Training loss at epoch 21: 1.083192616701126
[08/27/2025 21:47:50 INFO]: Training loss at epoch 5: 1.3365811109542847
[08/27/2025 21:47:58 INFO]: Training loss at epoch 23: 0.8152137100696564
[08/27/2025 21:48:24 INFO]: Training loss at epoch 15: 0.9291552007198334
[08/27/2025 21:48:36 INFO]: New best epoch, val score: -0.7774604540522103
[08/27/2025 21:48:36 INFO]: Saving model to: unhanged-Shanesha_trial_59/model_best.pth
[08/27/2025 21:48:38 INFO]: Training loss at epoch 19: 0.9656513035297394
[08/27/2025 21:48:51 INFO]: Training loss at epoch 7: 0.9035682082176208
[08/27/2025 21:49:09 INFO]: Training loss at epoch 36: 1.0175816416740417
[08/27/2025 21:49:27 INFO]: Training loss at epoch 17: 1.0271618366241455
[08/27/2025 21:49:27 INFO]: New best epoch, val score: -0.6603473216874208
[08/27/2025 21:49:27 INFO]: Saving model to: unhanged-Shanesha_trial_58/model_best.pth
[08/27/2025 21:49:35 INFO]: Training loss at epoch 10: 0.847097784280777
[08/27/2025 21:49:41 INFO]: Training loss at epoch 14: 1.1252127885818481
[08/27/2025 21:49:50 INFO]: Training accuracy: {
    "score": -1.0074634601985815,
    "rmse": 1.0074634601985815
}
[08/27/2025 21:49:50 INFO]: Val accuracy: {
    "score": -0.6605544453207195,
    "rmse": 0.6605544453207195
}
[08/27/2025 21:49:50 INFO]: Test accuracy: {
    "score": -0.8679058270333156,
    "rmse": 0.8679058270333156
}
[08/27/2025 21:49:50 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_22",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8679058270333156,
        "rmse": 0.8679058270333156
    },
    "train_stats": {
        "score": -1.0074634601985815,
        "rmse": 1.0074634601985815
    },
    "val_stats": {
        "score": -0.6605544453207195,
        "rmse": 0.6605544453207195
    }
}
[08/27/2025 21:49:50 INFO]: Procewss finished for trial unhanged-Shanesha_trial_22
[08/27/2025 21:49:50 INFO]: 
_________________________________________________

[08/27/2025 21:49:50 INFO]: train_net_for_optune.py main() running.
[08/27/2025 21:49:50 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 1.1810412297348143
  attention_dropout: 0.38573360902442455
  ffn_dropout: 0.38573360902442455
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00021059754346345228
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_60

[08/27/2025 21:49:51 INFO]: This ft_transformer has 10.204 million parameters.
[08/27/2025 21:49:51 INFO]: Training will start at epoch 0.
[08/27/2025 21:49:51 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 21:50:00 INFO]: Training loss at epoch 12: 0.9365585148334503
[08/27/2025 21:50:15 INFO]: New best epoch, val score: -0.6710453824412944
[08/27/2025 21:50:15 INFO]: Saving model to: unhanged-Shanesha_trial_55/model_best.pth
[08/27/2025 21:50:21 INFO]: Training loss at epoch 17: 0.9370019435882568
[08/27/2025 21:50:37 INFO]: Training loss at epoch 19: 1.1348691582679749
[08/27/2025 21:50:54 INFO]: Training loss at epoch 20: 0.9908773005008698
[08/27/2025 21:51:09 INFO]: Training stats: {
    "score": -1.0201188626587216,
    "rmse": 1.0201188626587216
}
[08/27/2025 21:51:09 INFO]: Val stats: {
    "score": -0.7551193473765817,
    "rmse": 0.7551193473765817
}
[08/27/2025 21:51:09 INFO]: Test stats: {
    "score": -0.9189393581292895,
    "rmse": 0.9189393581292895
}
[08/27/2025 21:51:16 INFO]: Training loss at epoch 59: 1.027648389339447
[08/27/2025 21:51:40 INFO]: Training loss at epoch 24: 0.9336040914058685
[08/27/2025 21:51:58 INFO]: Training loss at epoch 23: 0.9702142477035522
[08/27/2025 21:52:12 INFO]: Training loss at epoch 23: 1.1696631014347076
[08/27/2025 21:52:26 INFO]: New best epoch, val score: -0.6603142346557613
[08/27/2025 21:52:26 INFO]: Saving model to: unhanged-Shanesha_trial_41/model_best.pth
[08/27/2025 21:52:54 INFO]: Training stats: {
    "score": -1.0935645138762098,
    "rmse": 1.0935645138762098
}
[08/27/2025 21:52:54 INFO]: Val stats: {
    "score": -0.8988530250888412,
    "rmse": 0.8988530250888412
}
[08/27/2025 21:52:54 INFO]: Test stats: {
    "score": -1.023945534579259,
    "rmse": 1.023945534579259
}
[08/27/2025 21:53:31 INFO]: Training stats: {
    "score": -1.015437898385266,
    "rmse": 1.015437898385266
}
[08/27/2025 21:53:31 INFO]: Val stats: {
    "score": -0.6654992885441358,
    "rmse": 0.6654992885441358
}
[08/27/2025 21:53:31 INFO]: Test stats: {
    "score": -0.8915190857842971,
    "rmse": 0.8915190857842971
}
[08/27/2025 21:53:39 INFO]: Training loss at epoch 8: 0.924746960401535
[08/27/2025 21:53:55 INFO]: Training loss at epoch 15: 1.0139731764793396
[08/27/2025 21:53:59 INFO]: Training loss at epoch 6: 1.149575561285019
[08/27/2025 21:54:00 INFO]: Training loss at epoch 0: 0.8633520305156708
[08/27/2025 21:54:10 INFO]: Training loss at epoch 18: 1.1009132266044617
[08/27/2025 21:54:21 INFO]: Training loss at epoch 11: 1.1424847841262817
[08/27/2025 21:54:24 INFO]: New best epoch, val score: -0.661124581149089
[08/27/2025 21:54:24 INFO]: Saving model to: unhanged-Shanesha_trial_55/model_best.pth
[08/27/2025 21:54:25 INFO]: Training loss at epoch 60: 1.0824593901634216
[08/27/2025 21:54:33 INFO]: New best epoch, val score: -0.6685940140798846
[08/27/2025 21:54:33 INFO]: Saving model to: unhanged-Shanesha_trial_60/model_best.pth
[08/27/2025 21:54:35 INFO]: Training loss at epoch 22: 1.1961757838726044
[08/27/2025 21:54:38 INFO]: Training loss at epoch 13: 0.9671856462955475
[08/27/2025 21:54:40 INFO]: New best epoch, val score: -0.7613109760899794
[08/27/2025 21:54:40 INFO]: Saving model to: unhanged-Shanesha_trial_59/model_best.pth
[08/27/2025 21:54:49 INFO]: Training loss at epoch 24: 1.0897619724273682
[08/27/2025 21:55:14 INFO]: Training loss at epoch 16: 0.9369718432426453
[08/27/2025 21:57:03 INFO]: Training loss at epoch 18: 0.8334135711193085
[08/27/2025 21:57:45 INFO]: Training loss at epoch 21: 0.9723961651325226
[08/27/2025 21:58:02 INFO]: Training loss at epoch 20: 1.0798229575157166
[08/27/2025 21:58:18 INFO]: Training loss at epoch 16: 1.0622066855430603
[08/27/2025 21:58:28 INFO]: Training loss at epoch 25: 0.8875707387924194
[08/27/2025 21:58:32 INFO]: Training loss at epoch 9: 1.0191368460655212
[08/27/2025 21:58:48 INFO]: Training loss at epoch 24: 0.9523309171199799
[08/27/2025 21:58:59 INFO]: Training loss at epoch 1: 1.5199739336967468
[08/27/2025 21:59:05 INFO]: Training loss at epoch 24: 1.1281483173370361
[08/27/2025 21:59:06 INFO]: Training loss at epoch 19: 0.9723482429981232
[08/27/2025 21:59:21 INFO]: Training loss at epoch 12: 1.1180983781814575
[08/27/2025 21:59:35 INFO]: Training loss at epoch 14: 1.1923720240592957
[08/27/2025 21:59:50 INFO]: Training loss at epoch 20: 1.2384560108184814
[08/27/2025 22:00:14 INFO]: Training stats: {
    "score": -1.0066476497337502,
    "rmse": 1.0066476497337502
}
[08/27/2025 22:00:14 INFO]: Val stats: {
    "score": -0.6620664365428894,
    "rmse": 0.6620664365428894
}
[08/27/2025 22:00:14 INFO]: Test stats: {
    "score": -0.8687115661122617,
    "rmse": 0.8687115661122617
}
[08/27/2025 22:00:17 INFO]: Training loss at epoch 7: 1.5338674187660217
[08/27/2025 22:00:46 INFO]: Training stats: {
    "score": -1.0339785663318068,
    "rmse": 1.0339785663318068
}
[08/27/2025 22:00:46 INFO]: Val stats: {
    "score": -0.7894854244581009,
    "rmse": 0.7894854244581009
}
[08/27/2025 22:00:46 INFO]: Test stats: {
    "score": -0.9435174160094933,
    "rmse": 0.9435174160094933
}
[08/27/2025 22:00:48 INFO]: Training loss at epoch 60: 1.1113862991333008
[08/27/2025 22:00:51 INFO]: Training loss at epoch 37: 1.061708688735962
[08/27/2025 22:01:00 INFO]: New best epoch, val score: -0.7423062977771563
[08/27/2025 22:01:00 INFO]: Saving model to: unhanged-Shanesha_trial_59/model_best.pth
[08/27/2025 22:01:35 INFO]: Training loss at epoch 23: 1.0467771291732788
[08/27/2025 22:01:38 INFO]: Training loss at epoch 61: 0.9732338488101959
[08/27/2025 22:01:52 INFO]: Training loss at epoch 25: 1.4707927405834198
[08/27/2025 22:02:18 INFO]: Training loss at epoch 17: 1.0396384000778198
[08/27/2025 22:02:35 INFO]: Training loss at epoch 17: 0.8941901922225952
[08/27/2025 22:03:05 INFO]: New best epoch, val score: -0.6615254692163899
[08/27/2025 22:03:05 INFO]: Saving model to: unhanged-Shanesha_trial_50/model_best.pth
[08/27/2025 22:03:42 INFO]: Training loss at epoch 2: 1.256881594657898
[08/27/2025 22:03:50 INFO]: Training loss at epoch 19: 0.8989396393299103
[08/27/2025 22:04:01 INFO]: Training loss at epoch 13: 1.0227397680282593
[08/27/2025 22:04:10 INFO]: Training loss at epoch 15: 1.0402146577835083
[08/27/2025 22:04:33 INFO]: Training loss at epoch 22: 0.995727151632309
[08/27/2025 22:04:51 INFO]: Training loss at epoch 10: 1.0240463018417358
[08/27/2025 22:04:54 INFO]: Training loss at epoch 21: 0.8738649487495422
[08/27/2025 22:05:10 INFO]: Training loss at epoch 26: 1.2326741218566895
[08/27/2025 22:05:22 INFO]: Training loss at epoch 20: 0.9569190740585327
[08/27/2025 22:05:31 INFO]: Training loss at epoch 25: 0.8253160417079926
[08/27/2025 22:05:48 INFO]: Training loss at epoch 25: 0.9512616395950317
[08/27/2025 22:06:11 INFO]: Training stats: {
    "score": -1.090566119900749,
    "rmse": 1.090566119900749
}
[08/27/2025 22:06:11 INFO]: Val stats: {
    "score": -0.8936599099184264,
    "rmse": 0.8936599099184264
}
[08/27/2025 22:06:11 INFO]: Test stats: {
    "score": -1.0194403874622207,
    "rmse": 1.0194403874622207
}
[08/27/2025 22:06:14 INFO]: Training loss at epoch 8: 1.35893052816391
[08/27/2025 22:06:34 INFO]: Training loss at epoch 21: 1.3475425243377686
[08/27/2025 22:06:51 INFO]: Training loss at epoch 18: 0.8038196563720703
[08/27/2025 22:07:00 INFO]: New best epoch, val score: -0.7228874551084276
[08/27/2025 22:07:00 INFO]: Saving model to: unhanged-Shanesha_trial_59/model_best.pth
[08/27/2025 22:07:24 INFO]: New best epoch, val score: -0.6601165896267582
[08/27/2025 22:07:24 INFO]: Saving model to: unhanged-Shanesha_trial_55/model_best.pth
[08/27/2025 22:07:53 INFO]: Training loss at epoch 61: 0.9291328191757202
[08/27/2025 22:08:25 INFO]: Training loss at epoch 24: 1.1968185305595398
[08/27/2025 22:08:41 INFO]: Training loss at epoch 3: 0.9272026419639587
[08/27/2025 22:08:42 INFO]: Training loss at epoch 62: 1.0705739259719849
[08/27/2025 22:08:47 INFO]: Training loss at epoch 26: 1.0242930054664612
[08/27/2025 22:09:00 INFO]: Training loss at epoch 14: 0.9549877047538757
[08/27/2025 22:09:06 INFO]: Training loss at epoch 16: 1.0042907297611237
[08/27/2025 22:09:16 INFO]: New best epoch, val score: -0.662708283637575
[08/27/2025 22:09:16 INFO]: Saving model to: unhanged-Shanesha_trial_60/model_best.pth
[08/27/2025 22:09:17 INFO]: Training loss at epoch 18: 0.950941264629364
[08/27/2025 22:09:48 INFO]: Training loss at epoch 11: 1.0946252942085266
[08/27/2025 22:10:16 INFO]: Training loss at epoch 21: 1.0455236434936523
[08/27/2025 22:11:10 INFO]: Training loss at epoch 19: 1.0162736773490906
[08/27/2025 22:11:32 INFO]: Training loss at epoch 23: 1.0578613877296448
[08/27/2025 22:11:53 INFO]: Training loss at epoch 22: 0.9488855004310608
[08/27/2025 22:11:59 INFO]: Training loss at epoch 27: 0.9154766499996185
[08/27/2025 22:12:13 INFO]: Training loss at epoch 38: 1.0281238853931427
[08/27/2025 22:12:21 INFO]: Training loss at epoch 26: 1.2081463038921356
[08/27/2025 22:12:24 INFO]: Training loss at epoch 9: 1.2546292543411255
[08/27/2025 22:12:33 INFO]: Training stats: {
    "score": -0.998981589811144,
    "rmse": 0.998981589811144
}
[08/27/2025 22:12:33 INFO]: Val stats: {
    "score": -0.667706340383131,
    "rmse": 0.667706340383131
}
[08/27/2025 22:12:33 INFO]: Test stats: {
    "score": -0.8712896719386347,
    "rmse": 0.8712896719386347
}
[08/27/2025 22:12:39 INFO]: Training loss at epoch 26: 1.0076442062854767
[08/27/2025 22:12:57 INFO]: Training loss at epoch 20: 1.0523228347301483
[08/27/2025 22:13:17 INFO]: Training loss at epoch 4: 1.0240485668182373
[08/27/2025 22:13:17 INFO]: Training loss at epoch 22: 1.1320884227752686
[08/27/2025 22:13:25 INFO]: New best epoch, val score: -0.6633338247817011
[08/27/2025 22:13:25 INFO]: Saving model to: unhanged-Shanesha_trial_43/model_best.pth
[08/27/2025 22:13:30 INFO]: Running Final Evaluation...
[08/27/2025 22:13:35 INFO]: Training loss at epoch 15: 1.1639205813407898
[08/27/2025 22:13:37 INFO]: Training loss at epoch 17: 0.7853964865207672
[08/27/2025 22:14:26 INFO]: Training stats: {
    "score": -1.0809799699448337,
    "rmse": 1.0809799699448337
}
[08/27/2025 22:14:26 INFO]: Val stats: {
    "score": -0.7049366155750557,
    "rmse": 0.7049366155750557
}
[08/27/2025 22:14:26 INFO]: Test stats: {
    "score": -0.9229096035612688,
    "rmse": 0.9229096035612688
}
[08/27/2025 22:14:27 INFO]: Training loss at epoch 12: 1.1354886889457703
[08/27/2025 22:14:54 INFO]: Training loss at epoch 22: 0.9356352388858795
[08/27/2025 22:14:55 INFO]: Training loss at epoch 62: 0.8269193768501282
[08/27/2025 22:15:07 INFO]: Training loss at epoch 25: 1.0147236287593842
[08/27/2025 22:15:12 INFO]: New best epoch, val score: -0.7049366155750557
[08/27/2025 22:15:12 INFO]: Saving model to: unhanged-Shanesha_trial_59/model_best.pth
[08/27/2025 22:15:33 INFO]: Training loss at epoch 27: 0.924503892660141
[08/27/2025 22:15:37 INFO]: Training loss at epoch 63: 0.8851190209388733
[08/27/2025 22:16:05 INFO]: Training loss at epoch 19: 0.9218126833438873
[08/27/2025 22:17:00 INFO]: Training loss at epoch 20: 1.0951808094978333
[08/27/2025 22:17:47 INFO]: Training accuracy: {
    "score": -1.0097439307310236,
    "rmse": 1.0097439307310236
}
[08/27/2025 22:17:47 INFO]: Val accuracy: {
    "score": -0.6611709299330296,
    "rmse": 0.6611709299330296
}
[08/27/2025 22:17:47 INFO]: Test accuracy: {
    "score": -0.8705144486862351,
    "rmse": 0.8705144486862351
}
[08/27/2025 22:17:47 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_13",
    "best_epoch": 7,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8705144486862351,
        "rmse": 0.8705144486862351
    },
    "train_stats": {
        "score": -1.0097439307310236,
        "rmse": 1.0097439307310236
    },
    "val_stats": {
        "score": -0.6611709299330296,
        "rmse": 0.6611709299330296
    }
}
[08/27/2025 22:17:47 INFO]: Procewss finished for trial unhanged-Shanesha_trial_13
[08/27/2025 22:17:47 INFO]: 
_________________________________________________

[08/27/2025 22:17:47 INFO]: train_net_for_optune.py main() running.
[08/27/2025 22:17:47 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 1.1406312757760428
  attention_dropout: 0.37572870568128214
  ffn_dropout: 0.37572870568128214
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00022611596296090726
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_61

[08/27/2025 22:17:47 INFO]: This ft_transformer has 0.668 million parameters.
[08/27/2025 22:17:47 INFO]: Training will start at epoch 0.
[08/27/2025 22:17:47 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 22:18:18 INFO]: Training loss at epoch 5: 1.1531256437301636
[08/27/2025 22:18:26 INFO]: Training loss at epoch 0: 1.2217657566070557
[08/27/2025 22:18:30 INFO]: Training loss at epoch 24: 1.136817455291748
[08/27/2025 22:18:30 INFO]: Training stats: {
    "score": -1.0222205420582875,
    "rmse": 1.0222205420582875
}
[08/27/2025 22:18:30 INFO]: Val stats: {
    "score": -0.6629908977286635,
    "rmse": 0.6629908977286635
}
[08/27/2025 22:18:30 INFO]: Test stats: {
    "score": -0.8762165398026921,
    "rmse": 0.8762165398026921
}
[08/27/2025 22:18:31 INFO]: New best epoch, val score: -0.6762474384502548
[08/27/2025 22:18:31 INFO]: Saving model to: unhanged-Shanesha_trial_61/model_best.pth
[08/27/2025 22:18:34 INFO]: Training loss at epoch 18: 0.9328504204750061
[08/27/2025 22:18:37 INFO]: Training loss at epoch 16: 0.9791418015956879
[08/27/2025 22:18:51 INFO]: Training loss at epoch 28: 1.1610930263996124
[08/27/2025 22:18:54 INFO]: Training loss at epoch 23: 1.1141164302825928
[08/27/2025 22:19:06 INFO]: New best epoch, val score: -0.6680010266736479
[08/27/2025 22:19:06 INFO]: Saving model to: unhanged-Shanesha_trial_56/model_best.pth
[08/27/2025 22:19:10 INFO]: Training loss at epoch 1: 1.0514217615127563
[08/27/2025 22:19:16 INFO]: Training loss at epoch 27: 1.1969358623027802
[08/27/2025 22:19:22 INFO]: Training loss at epoch 13: 1.2254850566387177
[08/27/2025 22:19:35 INFO]: Training loss at epoch 27: 1.081114411354065
[08/27/2025 22:19:44 INFO]: Training loss at epoch 23: 0.9691231548786163
[08/27/2025 22:19:50 INFO]: Training loss at epoch 21: 1.0413880944252014
[08/27/2025 22:19:54 INFO]: Training loss at epoch 2: 1.0448237657546997
[08/27/2025 22:19:59 INFO]: New best epoch, val score: -0.6666405389851445
[08/27/2025 22:19:59 INFO]: Saving model to: unhanged-Shanesha_trial_61/model_best.pth
[08/27/2025 22:20:12 INFO]: Training loss at epoch 23: 1.0123032629489899
[08/27/2025 22:20:35 INFO]: Training loss at epoch 10: 1.1982325911521912
[08/27/2025 22:20:38 INFO]: Training loss at epoch 3: 1.2644621133804321
[08/27/2025 22:20:43 INFO]: New best epoch, val score: -0.6642353720542318
[08/27/2025 22:20:43 INFO]: Saving model to: unhanged-Shanesha_trial_61/model_best.pth
[08/27/2025 22:21:09 INFO]: Training loss at epoch 21: 0.895801842212677
[08/27/2025 22:21:16 INFO]: New best epoch, val score: -0.6844042375835863
[08/27/2025 22:21:16 INFO]: Saving model to: unhanged-Shanesha_trial_59/model_best.pth
[08/27/2025 22:21:23 INFO]: Training loss at epoch 4: 0.9805710315704346
[08/27/2025 22:21:52 INFO]: Training loss at epoch 26: 0.8244412839412689
[08/27/2025 22:21:59 INFO]: Training loss at epoch 63: 0.8189430236816406
[08/27/2025 22:22:08 INFO]: Training loss at epoch 5: 0.8225157856941223
[08/27/2025 22:22:18 INFO]: Training loss at epoch 28: 0.9288943111896515
[08/27/2025 22:22:32 INFO]: Training loss at epoch 64: 0.9423760771751404
[08/27/2025 22:22:53 INFO]: Training loss at epoch 6: 1.0461050271987915
[08/27/2025 22:22:54 INFO]: Training loss at epoch 6: 1.2333058714866638
[08/27/2025 22:23:07 INFO]: Training loss at epoch 19: 0.9819887280464172
[08/27/2025 22:23:14 INFO]: Training loss at epoch 17: 0.9114235043525696
[08/27/2025 22:23:40 INFO]: Training loss at epoch 7: 0.9881943166255951
[08/27/2025 22:24:04 INFO]: Training loss at epoch 14: 1.485222190618515
[08/27/2025 22:24:28 INFO]: Training loss at epoch 24: 1.0874704718589783
[08/27/2025 22:24:28 INFO]: Training loss at epoch 8: 0.9064874947071075
[08/27/2025 22:24:49 INFO]: Training stats: {
    "score": -0.9999522106481362,
    "rmse": 0.9999522106481362
}
[08/27/2025 22:24:49 INFO]: Val stats: {
    "score": -0.673157525870046,
    "rmse": 0.673157525870046
}
[08/27/2025 22:24:49 INFO]: Test stats: {
    "score": -0.8702354897004307,
    "rmse": 0.8702354897004307
}
[08/27/2025 22:25:17 INFO]: Training loss at epoch 9: 0.8916113972663879
[08/27/2025 22:25:18 INFO]: Training loss at epoch 25: 1.2483696341514587
[08/27/2025 22:25:18 INFO]: Training loss at epoch 20: 1.1619672179222107
[08/27/2025 22:25:30 INFO]: Training loss at epoch 22: 0.9363446533679962
[08/27/2025 22:25:34 INFO]: Training stats: {
    "score": -1.0051545074354793,
    "rmse": 1.0051545074354793
}
[08/27/2025 22:25:34 INFO]: Val stats: {
    "score": -0.7288262963567933,
    "rmse": 0.7288262963567933
}
[08/27/2025 22:25:34 INFO]: Test stats: {
    "score": -0.8987948225477881,
    "rmse": 0.8987948225477881
}
[08/27/2025 22:25:34 INFO]: Training loss at epoch 29: 1.2123213112354279
[08/27/2025 22:25:47 INFO]: Training loss at epoch 24: 1.195064127445221
[08/27/2025 22:26:05 INFO]: Training loss at epoch 28: 1.1702536642551422
[08/27/2025 22:26:21 INFO]: Training loss at epoch 10: 1.0942412912845612
[08/27/2025 22:26:26 INFO]: Training loss at epoch 28: 0.9364607632160187
[08/27/2025 22:26:40 INFO]: Training loss at epoch 22: 1.097303032875061
[08/27/2025 22:26:49 INFO]: Training loss at epoch 11: 1.1963125467300415
[08/27/2025 22:27:03 INFO]: Training loss at epoch 24: 1.0153701305389404
[08/27/2025 22:27:07 INFO]: Training loss at epoch 11: 0.9931995868682861
[08/27/2025 22:27:50 INFO]: Training loss at epoch 7: 1.2134517431259155
[08/27/2025 22:27:51 INFO]: Training loss at epoch 12: 0.984798789024353
[08/27/2025 22:27:52 INFO]: Training stats: {
    "score": -1.004334384616959,
    "rmse": 1.004334384616959
}
[08/27/2025 22:27:52 INFO]: Val stats: {
    "score": -0.7101132476601222,
    "rmse": 0.7101132476601222
}
[08/27/2025 22:27:52 INFO]: Test stats: {
    "score": -0.8904902748846126,
    "rmse": 0.8904902748846126
}
[08/27/2025 22:28:09 INFO]: Training loss at epoch 18: 0.8929413557052612
[08/27/2025 22:28:22 INFO]: New best epoch, val score: -0.6615575957166177
[08/27/2025 22:28:22 INFO]: Saving model to: unhanged-Shanesha_trial_60/model_best.pth
[08/27/2025 22:28:34 INFO]: Training loss at epoch 13: 0.9662095308303833
[08/27/2025 22:28:43 INFO]: Training loss at epoch 27: 1.1142870783805847
[08/27/2025 22:28:52 INFO]: Training loss at epoch 15: 1.1392184793949127
[08/27/2025 22:29:08 INFO]: Training loss at epoch 64: 1.2733905911445618
[08/27/2025 22:29:13 INFO]: Training loss at epoch 25: 1.0382787585258484
[08/27/2025 22:29:13 INFO]: Training loss at epoch 29: 1.0333486199378967
[08/27/2025 22:29:19 INFO]: Training loss at epoch 14: 0.91264408826828
[08/27/2025 22:29:30 INFO]: Training loss at epoch 20: 1.0573700070381165
[08/27/2025 22:29:37 INFO]: Training loss at epoch 65: 0.9349212944507599
[08/27/2025 22:29:40 INFO]: Training loss at epoch 23: 1.0512657165527344
[08/27/2025 22:30:03 INFO]: Training loss at epoch 15: 1.0973770022392273
[08/27/2025 22:30:47 INFO]: Training loss at epoch 16: 0.9985971748828888
[08/27/2025 22:31:29 INFO]: Training stats: {
    "score": -1.0101376831569304,
    "rmse": 1.0101376831569304
}
[08/27/2025 22:31:29 INFO]: Val stats: {
    "score": -0.6608003243137873,
    "rmse": 0.6608003243137873
}
[08/27/2025 22:31:29 INFO]: Test stats: {
    "score": -0.8700819152700557,
    "rmse": 0.8700819152700557
}
[08/27/2025 22:31:33 INFO]: Training loss at epoch 17: 0.9804480671882629
[08/27/2025 22:32:05 INFO]: Training loss at epoch 21: 1.2734180092811584
[08/27/2025 22:32:05 INFO]: Training loss at epoch 26: 1.033661425113678
[08/27/2025 22:32:21 INFO]: New best epoch, val score: -0.6608003243137873
[08/27/2025 22:32:21 INFO]: Saving model to: unhanged-Shanesha_trial_42/model_best.pth
[08/27/2025 22:32:22 INFO]: Training loss at epoch 18: 1.0094903111457825
[08/27/2025 22:32:32 INFO]: Training loss at epoch 8: 1.0924163460731506
[08/27/2025 22:32:34 INFO]: Training loss at epoch 25: 0.8992739915847778
[08/27/2025 22:32:45 INFO]: Training loss at epoch 12: 1.125970184803009
[08/27/2025 22:32:47 INFO]: Training loss at epoch 29: 1.0868360996246338
[08/27/2025 22:32:54 INFO]: Training loss at epoch 19: 0.9066030085086823
[08/27/2025 22:32:57 INFO]: New best epoch, val score: -0.6613452260389103
[08/27/2025 22:32:57 INFO]: Saving model to: unhanged-Shanesha_trial_50/model_best.pth
[08/27/2025 22:33:08 INFO]: New best epoch, val score: -0.6614071933289886
[08/27/2025 22:33:08 INFO]: Saving model to: unhanged-Shanesha_trial_60/model_best.pth
[08/27/2025 22:33:09 INFO]: Training loss at epoch 29: 0.9052920937538147
[08/27/2025 22:33:10 INFO]: Training loss at epoch 19: 1.0382035970687866
[08/27/2025 22:33:18 INFO]: Training loss at epoch 23: 1.1687750816345215
[08/27/2025 22:33:26 INFO]: Training stats: {
    "score": -1.0062545807280652,
    "rmse": 1.0062545807280652
}
[08/27/2025 22:33:26 INFO]: Val stats: {
    "score": -0.7336561988171272,
    "rmse": 0.7336561988171272
}
[08/27/2025 22:33:26 INFO]: Test stats: {
    "score": -0.9022323831534145,
    "rmse": 0.9022323831534145
}
[08/27/2025 22:33:40 INFO]: Training loss at epoch 16: 0.9170309901237488
[08/27/2025 22:33:46 INFO]: Training loss at epoch 25: 1.0119059681892395
[08/27/2025 22:34:00 INFO]: Training loss at epoch 24: 1.267878234386444
[08/27/2025 22:34:02 INFO]: Training loss at epoch 26: 1.0206283628940582
[08/27/2025 22:34:14 INFO]: Training loss at epoch 20: 1.0031078159809113
[08/27/2025 22:34:18 INFO]: Training loss at epoch 21: 0.8800286948680878
[08/27/2025 22:34:38 INFO]: Training stats: {
    "score": -0.9981263701235956,
    "rmse": 0.9981263701235956
}
[08/27/2025 22:34:38 INFO]: Val stats: {
    "score": -0.6764145550633187,
    "rmse": 0.6764145550633187
}
[08/27/2025 22:34:38 INFO]: Test stats: {
    "score": -0.8739445322772481,
    "rmse": 0.8739445322772481
}
[08/27/2025 22:34:39 INFO]: Training loss at epoch 30: 0.9933986067771912
[08/27/2025 22:35:02 INFO]: Training loss at epoch 21: 1.1434520184993744
[08/27/2025 22:35:17 INFO]: Training stats: {
    "score": -0.9978118230643033,
    "rmse": 0.9978118230643033
}
[08/27/2025 22:35:17 INFO]: Val stats: {
    "score": -0.6738405551818863,
    "rmse": 0.6738405551818863
}
[08/27/2025 22:35:17 INFO]: Test stats: {
    "score": -0.8699303238660628,
    "rmse": 0.8699303238660628
}
[08/27/2025 22:35:38 INFO]: Training loss at epoch 28: 1.175976037979126
[08/27/2025 22:35:39 INFO]: Training stats: {
    "score": -1.0014855264073836,
    "rmse": 1.0014855264073836
}
[08/27/2025 22:35:39 INFO]: Val stats: {
    "score": -0.6678929097174767,
    "rmse": 0.6678929097174767
}
[08/27/2025 22:35:39 INFO]: Test stats: {
    "score": -0.8693585518008825,
    "rmse": 0.8693585518008825
}
[08/27/2025 22:35:49 INFO]: Training loss at epoch 22: 1.0836220383644104
[08/27/2025 22:36:22 INFO]: Training loss at epoch 65: 1.1983085572719574
[08/27/2025 22:36:33 INFO]: Training loss at epoch 23: 1.0953322052955627
[08/27/2025 22:36:44 INFO]: Training loss at epoch 66: 1.0233196020126343
[08/27/2025 22:37:16 INFO]: Training loss at epoch 24: 0.8286687731742859
[08/27/2025 22:37:27 INFO]: Training loss at epoch 9: 1.0570679306983948
[08/27/2025 22:38:00 INFO]: Training loss at epoch 25: 0.9642246067523956
[08/27/2025 22:38:13 INFO]: Training loss at epoch 25: 0.9298369884490967
[08/27/2025 22:38:24 INFO]: Training loss at epoch 17: 0.9819690585136414
[08/27/2025 22:38:25 INFO]: Training loss at epoch 30: 0.8896474838256836
[08/27/2025 22:38:44 INFO]: Training loss at epoch 27: 1.2130122780799866
[08/27/2025 22:38:44 INFO]: Training loss at epoch 26: 0.9932769238948822
[08/27/2025 22:38:49 INFO]: Training loss at epoch 13: 1.5994598865509033
[08/27/2025 22:38:55 INFO]: Training loss at epoch 22: 1.4701460301876068
[08/27/2025 22:38:59 INFO]: Training loss at epoch 27: 0.8982933163642883
[08/27/2025 22:39:00 INFO]: Training loss at epoch 22: 1.0264463424682617
[08/27/2025 22:39:00 INFO]: Training stats: {
    "score": -1.0038358690937736,
    "rmse": 1.0038358690937736
}
[08/27/2025 22:39:00 INFO]: Val stats: {
    "score": -0.6633007048662186,
    "rmse": 0.6633007048662186
}
[08/27/2025 22:39:00 INFO]: Test stats: {
    "score": -0.8677434438117467,
    "rmse": 0.8677434438117467
}
[08/27/2025 22:39:17 INFO]: Training loss at epoch 20: 1.1650559902191162
[08/27/2025 22:39:26 INFO]: Training loss at epoch 26: 0.9441899359226227
[08/27/2025 22:39:27 INFO]: Training loss at epoch 27: 1.15515798330307
[08/27/2025 22:39:57 INFO]: Training loss at epoch 24: 0.8796611428260803
[08/27/2025 22:40:13 INFO]: Training loss at epoch 28: 0.8979945480823517
[08/27/2025 22:40:16 INFO]: New best epoch, val score: -0.661462336688656
[08/27/2025 22:40:16 INFO]: Saving model to: unhanged-Shanesha_trial_47/model_best.pth
[08/27/2025 22:40:27 INFO]: Training loss at epoch 26: 1.1171561479568481
[08/27/2025 22:41:00 INFO]: Training loss at epoch 29: 0.9358854591846466
[08/27/2025 22:41:14 INFO]: Training loss at epoch 31: 0.9394951164722443
[08/27/2025 22:41:16 INFO]: Training stats: {
    "score": -1.018843268771618,
    "rmse": 1.018843268771618
}
[08/27/2025 22:41:16 INFO]: Val stats: {
    "score": -0.7672551636589887,
    "rmse": 0.7672551636589887
}
[08/27/2025 22:41:16 INFO]: Test stats: {
    "score": -0.9246997614765747,
    "rmse": 0.9246997614765747
}
[08/27/2025 22:41:59 INFO]: Training loss at epoch 30: 0.9238996803760529
[08/27/2025 22:42:04 INFO]: Training loss at epoch 30: 1.0890253186225891
[08/27/2025 22:42:18 INFO]: Training loss at epoch 29: 1.1801259517669678
[08/27/2025 22:42:22 INFO]: Training loss at epoch 30: 1.1664586961269379
[08/27/2025 22:42:31 INFO]: Training loss at epoch 26: 0.9721483886241913
[08/27/2025 22:42:52 INFO]: Training loss at epoch 31: 1.332783043384552
[08/27/2025 22:43:15 INFO]: Training loss at epoch 18: 0.8240959644317627
[08/27/2025 22:43:26 INFO]: Training loss at epoch 66: 1.1883097887039185
[08/27/2025 22:43:36 INFO]: Training loss at epoch 28: 1.2219895124435425
[08/27/2025 22:43:43 INFO]: Training loss at epoch 32: 1.0830918550491333
[08/27/2025 22:43:46 INFO]: Training loss at epoch 67: 1.0080735683441162
[08/27/2025 22:43:46 INFO]: Training loss at epoch 23: 0.7973718345165253
[08/27/2025 22:43:56 INFO]: Training loss at epoch 10: 1.131637841463089
[08/27/2025 22:44:16 INFO]: Training loss at epoch 21: 1.053896963596344
[08/27/2025 22:44:32 INFO]: Training loss at epoch 33: 0.9898634850978851
[08/27/2025 22:44:47 INFO]: Training stats: {
    "score": -1.0079848711436545,
    "rmse": 1.0079848711436545
}
[08/27/2025 22:44:47 INFO]: Val stats: {
    "score": -0.7220132884579253,
    "rmse": 0.7220132884579253
}
[08/27/2025 22:44:47 INFO]: Test stats: {
    "score": -0.8974622686435201,
    "rmse": 0.8974622686435201
}
[08/27/2025 22:45:06 INFO]: Training loss at epoch 14: 1.260320007801056
[08/27/2025 22:45:20 INFO]: Training loss at epoch 34: 0.9906121790409088
[08/27/2025 22:45:24 INFO]: Training loss at epoch 31: 0.9218444526195526
[08/27/2025 22:45:25 INFO]: Running Final Evaluation...
[08/27/2025 22:45:42 INFO]: Training accuracy: {
    "score": -1.0124734390278463,
    "rmse": 1.0124734390278463
}
[08/27/2025 22:45:42 INFO]: Val accuracy: {
    "score": -0.6642353720542318,
    "rmse": 0.6642353720542318
}
[08/27/2025 22:45:42 INFO]: Test accuracy: {
    "score": -0.8695146353501092,
    "rmse": 0.8695146353501092
}
[08/27/2025 22:45:42 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_61",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8695146353501092,
        "rmse": 0.8695146353501092
    },
    "train_stats": {
        "score": -1.0124734390278463,
        "rmse": 1.0124734390278463
    },
    "val_stats": {
        "score": -0.6642353720542318,
        "rmse": 0.6642353720542318
    }
}
[08/27/2025 22:45:42 INFO]: Procewss finished for trial unhanged-Shanesha_trial_61
[08/27/2025 22:45:42 INFO]: 
_________________________________________________

[08/27/2025 22:45:42 INFO]: train_net_for_optune.py main() running.
[08/27/2025 22:45:42 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 1.1260263229899243
  attention_dropout: 0.4062004974246727
  ffn_dropout: 0.4062004974246727
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00013001650685296942
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_62

[08/27/2025 22:45:42 INFO]: This ft_transformer has 0.665 million parameters.
[08/27/2025 22:45:42 INFO]: Training will start at epoch 0.
[08/27/2025 22:45:42 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 22:46:00 INFO]: Training loss at epoch 28: 1.054057776927948
[08/27/2025 22:46:02 INFO]: Training loss at epoch 23: 0.9903580546379089
[08/27/2025 22:46:11 INFO]: New best epoch, val score: -0.6607013850567668
[08/27/2025 22:46:11 INFO]: Saving model to: unhanged-Shanesha_trial_42/model_best.pth
[08/27/2025 22:46:23 INFO]: Training loss at epoch 0: 1.0275171101093292
[08/27/2025 22:46:28 INFO]: New best epoch, val score: -0.7195111771708976
[08/27/2025 22:46:28 INFO]: Saving model to: unhanged-Shanesha_trial_62/model_best.pth
[08/27/2025 22:46:31 INFO]: Training loss at epoch 27: 1.1547639071941376
[08/27/2025 22:46:51 INFO]: Training loss at epoch 27: 1.031102180480957
[08/27/2025 22:46:52 INFO]: Training loss at epoch 25: 1.0479588508605957
[08/27/2025 22:47:07 INFO]: Training loss at epoch 1: 1.088281512260437
[08/27/2025 22:47:18 INFO]: New best epoch, val score: -0.6606617879749075
[08/27/2025 22:47:18 INFO]: Saving model to: unhanged-Shanesha_trial_47/model_best.pth
[08/27/2025 22:47:22 INFO]: Training loss at epoch 27: 1.079786479473114
[08/27/2025 22:47:52 INFO]: Training loss at epoch 2: 0.9413505494594574
[08/27/2025 22:47:58 INFO]: Training loss at epoch 19: 1.1550941467285156
[08/27/2025 22:48:04 INFO]: Training loss at epoch 32: 1.0841942727565765
[08/27/2025 22:48:16 INFO]: Training loss at epoch 29: 0.84517902135849
[08/27/2025 22:48:24 INFO]: Training loss at epoch 24: 1.068450152873993
[08/27/2025 22:48:36 INFO]: Training loss at epoch 3: 1.0617358982563019
[08/27/2025 22:48:37 INFO]: Training loss at epoch 11: 1.2732780575752258
[08/27/2025 22:48:47 INFO]: Training loss at epoch 31: 0.9546765685081482
[08/27/2025 22:48:56 INFO]: Training loss at epoch 22: 0.8874270617961884
[08/27/2025 22:49:09 INFO]: Training loss at epoch 31: 1.0269160270690918
[08/27/2025 22:49:20 INFO]: Training loss at epoch 4: 1.2069069147109985
[08/27/2025 22:49:30 INFO]: Training stats: {
    "score": -1.0002195548953086,
    "rmse": 1.0002195548953086
}
[08/27/2025 22:49:30 INFO]: Val stats: {
    "score": -0.6882917223947288,
    "rmse": 0.6882917223947288
}
[08/27/2025 22:49:30 INFO]: Test stats: {
    "score": -0.8779480983739649,
    "rmse": 0.8779480983739649
}
[08/27/2025 22:49:48 INFO]: Training stats: {
    "score": -1.0023439515414183,
    "rmse": 1.0023439515414183
}
[08/27/2025 22:49:48 INFO]: Val stats: {
    "score": -0.7086162646315547,
    "rmse": 0.7086162646315547
}
[08/27/2025 22:49:48 INFO]: Test stats: {
    "score": -0.8896085349981199,
    "rmse": 0.8896085349981199
}
[08/27/2025 22:49:56 INFO]: New best epoch, val score: -0.6629740601111324
[08/27/2025 22:49:56 INFO]: Saving model to: unhanged-Shanesha_trial_43/model_best.pth
[08/27/2025 22:50:06 INFO]: Training loss at epoch 5: 0.9813098311424255
[08/27/2025 22:50:26 INFO]: Training loss at epoch 67: 0.9286361038684845
[08/27/2025 22:50:39 INFO]: Training loss at epoch 68: 1.2025805115699768
[08/27/2025 22:50:53 INFO]: Training loss at epoch 6: 0.9631944894790649
[08/27/2025 22:51:02 INFO]: Training loss at epoch 15: 1.0331165194511414
[08/27/2025 22:51:03 INFO]: Training loss at epoch 28: 1.0456827282905579
[08/27/2025 22:51:20 INFO]: New best epoch, val score: -0.6569505307746574
[08/27/2025 22:51:20 INFO]: Saving model to: unhanged-Shanesha_trial_20/model_best.pth
[08/27/2025 22:51:27 INFO]: Training loss at epoch 30: 1.058422029018402
[08/27/2025 22:51:41 INFO]: Training loss at epoch 7: 0.9448621571063995
[08/27/2025 22:52:12 INFO]: Training loss at epoch 32: 0.8748688995838165
[08/27/2025 22:52:29 INFO]: Training loss at epoch 8: 1.0748336911201477
[08/27/2025 22:52:51 INFO]: Training loss at epoch 29: 1.0311728715896606
[08/27/2025 22:52:55 INFO]: Training loss at epoch 24: 0.8976073265075684
[08/27/2025 22:53:16 INFO]: Training loss at epoch 25: 0.9507656991481781
[08/27/2025 22:53:17 INFO]: Training loss at epoch 9: 1.0582950711250305
[08/27/2025 22:53:29 INFO]: Training loss at epoch 28: 1.1772565841674805
[08/27/2025 22:53:35 INFO]: Training stats: {
    "score": -1.0083929823368558,
    "rmse": 1.0083929823368558
}
[08/27/2025 22:53:35 INFO]: Val stats: {
    "score": -0.7235300008769058,
    "rmse": 0.7235300008769058
}
[08/27/2025 22:53:35 INFO]: Test stats: {
    "score": -0.8993382772275825,
    "rmse": 0.8993382772275825
}
[08/27/2025 22:53:35 INFO]: Training loss at epoch 12: 1.1134458780288696
[08/27/2025 22:53:42 INFO]: Training loss at epoch 26: 0.9727186262607574
[08/27/2025 22:53:57 INFO]: Training loss at epoch 23: 1.1230061650276184
[08/27/2025 22:54:16 INFO]: Training loss at epoch 28: 1.0298725366592407
[08/27/2025 22:54:20 INFO]: Training loss at epoch 10: 1.1440060138702393
[08/27/2025 22:54:26 INFO]: New best epoch, val score: -0.6924353455969419
[08/27/2025 22:54:26 INFO]: Saving model to: unhanged-Shanesha_trial_62/model_best.pth
[08/27/2025 22:54:30 INFO]: Training loss at epoch 20: 1.0762790739536285
[08/27/2025 22:54:31 INFO]: New best epoch, val score: -0.6667153501581606
[08/27/2025 22:54:31 INFO]: Saving model to: unhanged-Shanesha_trial_49/model_best.pth
[08/27/2025 22:54:48 INFO]: Training loss at epoch 30: 0.811097264289856
[08/27/2025 22:54:55 INFO]: Training loss at epoch 33: 0.9746960997581482
[08/27/2025 22:55:05 INFO]: Training loss at epoch 11: 1.025753378868103
[08/27/2025 22:55:10 INFO]: New best epoch, val score: -0.6729021847443215
[08/27/2025 22:55:10 INFO]: Saving model to: unhanged-Shanesha_trial_62/model_best.pth
[08/27/2025 22:55:14 INFO]: Training stats: {
    "score": -1.0144249321099355,
    "rmse": 1.0144249321099355
}
[08/27/2025 22:55:14 INFO]: Val stats: {
    "score": -0.7446902516023756,
    "rmse": 0.7446902516023756
}
[08/27/2025 22:55:14 INFO]: Test stats: {
    "score": -0.9121975767726598,
    "rmse": 0.9121975767726598
}
[08/27/2025 22:55:25 INFO]: Training loss at epoch 29: 0.817877471446991
[08/27/2025 22:55:41 INFO]: Training loss at epoch 32: 0.9546107351779938
[08/27/2025 22:55:48 INFO]: Training loss at epoch 12: 0.8986392915248871
[08/27/2025 22:55:54 INFO]: New best epoch, val score: -0.6631501539061954
[08/27/2025 22:55:54 INFO]: Saving model to: unhanged-Shanesha_trial_62/model_best.pth
[08/27/2025 22:56:05 INFO]: Training loss at epoch 32: 1.0322376489639282
[08/27/2025 22:56:32 INFO]: Training loss at epoch 13: 1.3789774179458618
[08/27/2025 22:56:37 INFO]: New best epoch, val score: -0.6590811184933895
[08/27/2025 22:56:37 INFO]: Saving model to: unhanged-Shanesha_trial_62/model_best.pth
[08/27/2025 22:56:48 INFO]: Training stats: {
    "score": -1.006505453722799,
    "rmse": 1.006505453722799
}
[08/27/2025 22:56:48 INFO]: Val stats: {
    "score": -0.6606675990436435,
    "rmse": 0.6606675990436435
}
[08/27/2025 22:56:48 INFO]: Test stats: {
    "score": -0.8704850051379587,
    "rmse": 0.8704850051379587
}
[08/27/2025 22:56:52 INFO]: New best epoch, val score: -0.662786450646424
[08/27/2025 22:56:52 INFO]: Saving model to: unhanged-Shanesha_trial_43/model_best.pth
[08/27/2025 22:57:09 INFO]: Training loss at epoch 16: 1.038356900215149
[08/27/2025 22:57:17 INFO]: Training loss at epoch 14: 1.0093283653259277
[08/27/2025 22:57:22 INFO]: New best epoch, val score: -0.6579778838489597
[08/27/2025 22:57:22 INFO]: Saving model to: unhanged-Shanesha_trial_62/model_best.pth
[08/27/2025 22:57:36 INFO]: Training loss at epoch 68: 1.0774077475070953
[08/27/2025 22:57:42 INFO]: Training loss at epoch 69: 1.0220037698745728
[08/27/2025 22:57:50 INFO]: Training loss at epoch 26: 1.0441192984580994
[08/27/2025 22:58:00 INFO]: Training loss at epoch 15: 0.9023971557617188
[08/27/2025 22:58:10 INFO]: Training loss at epoch 31: 0.9221007227897644
[08/27/2025 22:58:11 INFO]: Training loss at epoch 13: 0.8566565215587616
[08/27/2025 22:58:25 INFO]: New best epoch, val score: -0.6569490572514136
[08/27/2025 22:58:25 INFO]: Saving model to: unhanged-Shanesha_trial_20/model_best.pth
[08/27/2025 22:58:35 INFO]: Training loss at epoch 24: 1.1003152430057526
[08/27/2025 22:58:47 INFO]: Training loss at epoch 16: 1.0462778210639954
[08/27/2025 22:58:58 INFO]: Training loss at epoch 33: 1.0765897631645203
[08/27/2025 22:59:04 INFO]: Running Final Evaluation...
[08/27/2025 22:59:09 INFO]: Training loss at epoch 21: 1.0724337100982666
[08/27/2025 22:59:27 INFO]: Training loss at epoch 31: 1.2282562553882599
[08/27/2025 22:59:35 INFO]: Training loss at epoch 17: 0.9070643782615662
[08/27/2025 22:59:42 INFO]: Training loss at epoch 25: 1.149832010269165
[08/27/2025 23:00:09 INFO]: Training stats: {
    "score": -1.039428478608765,
    "rmse": 1.039428478608765
}
[08/27/2025 23:00:09 INFO]: Val stats: {
    "score": -0.8010137592357802,
    "rmse": 0.8010137592357802
}
[08/27/2025 23:00:09 INFO]: Test stats: {
    "score": -0.9512915085830106,
    "rmse": 0.9512915085830106
}
[08/27/2025 23:00:20 INFO]: Training loss at epoch 29: 1.0778431296348572
[08/27/2025 23:00:21 INFO]: Training loss at epoch 18: 1.181466281414032
[08/27/2025 23:00:24 INFO]: Training loss at epoch 27: 1.045707643032074
[08/27/2025 23:00:58 INFO]: Training loss at epoch 29: 0.8416791558265686
[08/27/2025 23:01:09 INFO]: Training loss at epoch 19: 0.8348545432090759
[08/27/2025 23:01:10 INFO]: Training loss at epoch 30: 1.0033583045005798
[08/27/2025 23:01:15 INFO]: New best epoch, val score: -0.6632464031165363
[08/27/2025 23:01:15 INFO]: Saving model to: unhanged-Shanesha_trial_49/model_best.pth
[08/27/2025 23:01:26 INFO]: Training stats: {
    "score": -0.9989191418008784,
    "rmse": 0.9989191418008784
}
[08/27/2025 23:01:26 INFO]: Val stats: {
    "score": -0.6868303957124909,
    "rmse": 0.6868303957124909
}
[08/27/2025 23:01:26 INFO]: Test stats: {
    "score": -0.8777677935084988,
    "rmse": 0.8777677935084988
}
[08/27/2025 23:01:40 INFO]: Training accuracy: {
    "score": -1.0120108234214087,
    "rmse": 1.0120108234214087
}
[08/27/2025 23:01:40 INFO]: Val accuracy: {
    "score": -0.6590789475364406,
    "rmse": 0.6590789475364406
}
[08/27/2025 23:01:40 INFO]: Test accuracy: {
    "score": -0.8698265193007285,
    "rmse": 0.8698265193007285
}
[08/27/2025 23:01:40 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_45",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8698265193007285,
        "rmse": 0.8698265193007285
    },
    "train_stats": {
        "score": -1.0120108234214087,
        "rmse": 1.0120108234214087
    },
    "val_stats": {
        "score": -0.6590789475364406,
        "rmse": 0.6590789475364406
    }
}
[08/27/2025 23:01:40 INFO]: Procewss finished for trial unhanged-Shanesha_trial_45
[08/27/2025 23:01:40 INFO]: 
_________________________________________________

[08/27/2025 23:01:40 INFO]: train_net_for_optune.py main() running.
[08/27/2025 23:01:40 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 2.3000880773928682
  attention_dropout: 0.39711279988916276
  ffn_dropout: 0.39711279988916276
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.000592451530954158
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_63

[08/27/2025 23:01:40 INFO]: This ft_transformer has 13.728 million parameters.
[08/27/2025 23:01:40 INFO]: Training will start at epoch 0.
[08/27/2025 23:01:40 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 23:01:44 INFO]: Training loss at epoch 34: 1.0389853715896606
[08/27/2025 23:02:09 INFO]: Training loss at epoch 30: 1.1477487087249756
[08/27/2025 23:02:14 INFO]: Training loss at epoch 20: 0.9016067087650299
[08/27/2025 23:02:34 INFO]: Training loss at epoch 33: 0.9561657607555389
[08/27/2025 23:02:44 INFO]: Training loss at epoch 27: 0.8854853808879852
[08/27/2025 23:02:48 INFO]: Training stats: {
    "score": -1.0056684759001309,
    "rmse": 1.0056684759001309
}
[08/27/2025 23:02:48 INFO]: Val stats: {
    "score": -0.6629453635117807,
    "rmse": 0.6629453635117807
}
[08/27/2025 23:02:48 INFO]: Test stats: {
    "score": -0.8685753766665905,
    "rmse": 0.8685753766665905
}
[08/27/2025 23:02:58 INFO]: Training loss at epoch 21: 0.9065621793270111
[08/27/2025 23:03:00 INFO]: Training loss at epoch 33: 0.8360761106014252
[08/27/2025 23:03:10 INFO]: Training loss at epoch 14: 1.4656873941421509
[08/27/2025 23:03:16 INFO]: New best epoch, val score: -0.6629835876665866
[08/27/2025 23:03:16 INFO]: Saving model to: unhanged-Shanesha_trial_56/model_best.pth
[08/27/2025 23:03:20 INFO]: Training loss at epoch 17: 1.0039601922035217
[08/27/2025 23:03:21 INFO]: Training stats: {
    "score": -1.00104504250606,
    "rmse": 1.00104504250606
}
[08/27/2025 23:03:21 INFO]: Val stats: {
    "score": -0.6690015616305911,
    "rmse": 0.6690015616305911
}
[08/27/2025 23:03:21 INFO]: Test stats: {
    "score": -0.8692210497181748,
    "rmse": 0.8692210497181748
}
[08/27/2025 23:03:31 INFO]: Training loss at epoch 25: 1.047756016254425
[08/27/2025 23:03:42 INFO]: Training loss at epoch 22: 1.1301363110542297
[08/27/2025 23:04:01 INFO]: New best epoch, val score: -0.6816102699886788
[08/27/2025 23:04:01 INFO]: Saving model to: unhanged-Shanesha_trial_59/model_best.pth
[08/27/2025 23:04:01 INFO]: Training loss at epoch 22: 1.1647799909114838
[08/27/2025 23:04:16 INFO]: Training loss at epoch 32: 0.9707778692245483
[08/27/2025 23:04:26 INFO]: Training loss at epoch 23: 1.0621458292007446
[08/27/2025 23:04:48 INFO]: Training loss at epoch 69: 1.031644344329834
[08/27/2025 23:05:10 INFO]: Training loss at epoch 24: 1.1279820203781128
[08/27/2025 23:05:21 INFO]: Training loss at epoch 31: 1.1693268418312073
[08/27/2025 23:05:47 INFO]: Training loss at epoch 34: 1.0654721558094025
[08/27/2025 23:05:53 INFO]: Training loss at epoch 25: 1.0893455743789673
[08/27/2025 23:06:30 INFO]: Training loss at epoch 26: 0.9778148233890533
[08/27/2025 23:06:37 INFO]: Training loss at epoch 26: 1.1619232892990112
[08/27/2025 23:06:51 INFO]: Training loss at epoch 0: 1.1953306198120117
[08/27/2025 23:07:03 INFO]: Training loss at epoch 28: 0.9051998257637024
[08/27/2025 23:07:04 INFO]: Training loss at epoch 70: 1.2347912788391113
[08/27/2025 23:07:04 INFO]: Training stats: {
    "score": -0.9853121647072302,
    "rmse": 0.9853121647072302
}
[08/27/2025 23:07:04 INFO]: Val stats: {
    "score": -0.6580458142733892,
    "rmse": 0.6580458142733892
}
[08/27/2025 23:07:04 INFO]: Test stats: {
    "score": -0.8781746048518447,
    "rmse": 0.8781746048518447
}
[08/27/2025 23:07:14 INFO]: Training loss at epoch 28: 1.258326768875122
[08/27/2025 23:07:20 INFO]: Training loss at epoch 27: 0.8978241384029388
[08/27/2025 23:07:32 INFO]: New best epoch, val score: -0.7371031726319042
[08/27/2025 23:07:32 INFO]: Saving model to: unhanged-Shanesha_trial_63/model_best.pth
[08/27/2025 23:07:46 INFO]: Training loss at epoch 15: 0.9840264320373535
[08/27/2025 23:08:07 INFO]: Training loss at epoch 28: 1.091743528842926
[08/27/2025 23:08:09 INFO]: Training loss at epoch 26: 0.9458044469356537
[08/27/2025 23:08:20 INFO]: Training loss at epoch 35: 0.9515297710895538
[08/27/2025 23:08:43 INFO]: Training loss at epoch 23: 0.8334700763225555
[08/27/2025 23:08:50 INFO]: Training loss at epoch 31: 1.0621607303619385
[08/27/2025 23:08:55 INFO]: Training loss at epoch 29: 1.0925831198692322
[08/27/2025 23:08:59 INFO]: Training loss at epoch 33: 0.9336243271827698
[08/27/2025 23:09:12 INFO]: Training stats: {
    "score": -0.9983067054207329,
    "rmse": 0.9983067054207329
}
[08/27/2025 23:09:12 INFO]: Val stats: {
    "score": -0.6839629128576384,
    "rmse": 0.6839629128576384
}
[08/27/2025 23:09:12 INFO]: Test stats: {
    "score": -0.8760591058712981,
    "rmse": 0.8760591058712981
}
[08/27/2025 23:09:14 INFO]: Training loss at epoch 34: 1.130955696105957
[08/27/2025 23:09:19 INFO]: Training loss at epoch 18: 0.9779756963253021
[08/27/2025 23:09:35 INFO]: Training loss at epoch 30: 0.9941456019878387
[08/27/2025 23:09:38 INFO]: Training loss at epoch 32: 1.0964970588684082
[08/27/2025 23:09:43 INFO]: Running Final Evaluation...
[08/27/2025 23:09:43 INFO]: Training loss at epoch 34: 0.9648254811763763
[08/27/2025 23:10:01 INFO]: Training loss at epoch 30: 1.1124078631401062
[08/27/2025 23:10:04 INFO]: New best epoch, val score: -0.6614107481377812
[08/27/2025 23:10:04 INFO]: Saving model to: unhanged-Shanesha_trial_59/model_best.pth
[08/27/2025 23:10:04 INFO]: Training loss at epoch 30: 1.2627733647823334
[08/27/2025 23:10:49 INFO]: Training loss at epoch 31: 1.2032182216644287
[08/27/2025 23:11:37 INFO]: Training loss at epoch 32: 1.0196162462234497
[08/27/2025 23:12:11 INFO]: Training loss at epoch 29: 1.224536120891571
[08/27/2025 23:12:20 INFO]: Training accuracy: {
    "score": -1.0045296298832194,
    "rmse": 1.0045296298832194
}
[08/27/2025 23:12:20 INFO]: Val accuracy: {
    "score": -0.659782939967528,
    "rmse": 0.659782939967528
}
[08/27/2025 23:12:20 INFO]: Test accuracy: {
    "score": -0.8697049795320206,
    "rmse": 0.8697049795320206
}
[08/27/2025 23:12:20 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_46",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8697049795320206,
        "rmse": 0.8697049795320206
    },
    "train_stats": {
        "score": -1.0045296298832194,
        "rmse": 1.0045296298832194
    },
    "val_stats": {
        "score": -0.659782939967528,
        "rmse": 0.659782939967528
    }
}
[08/27/2025 23:12:21 INFO]: Procewss finished for trial unhanged-Shanesha_trial_46
[08/27/2025 23:12:21 INFO]: 
_________________________________________________

[08/27/2025 23:12:21 INFO]: train_net_for_optune.py main() running.
[08/27/2025 23:12:21 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 2.331270256230932
  attention_dropout: 0.45443202848128517
  ffn_dropout: 0.45443202848128517
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0007427364806739356
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_64

[08/27/2025 23:12:21 INFO]: Training loss at epoch 33: 1.034616470336914
[08/27/2025 23:12:21 INFO]: This ft_transformer has 13.826 million parameters.
[08/27/2025 23:12:21 INFO]: Training will start at epoch 0.
[08/27/2025 23:12:21 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 23:12:44 INFO]: Training loss at epoch 35: 0.8180179297924042
[08/27/2025 23:12:45 INFO]: Training loss at epoch 16: 0.8456708490848541
[08/27/2025 23:13:04 INFO]: Training loss at epoch 34: 1.2844960689544678
[08/27/2025 23:13:07 INFO]: Training loss at epoch 27: 0.9962219595909119
[08/27/2025 23:13:08 INFO]: Training loss at epoch 1: 1.2514788508415222
[08/27/2025 23:13:30 INFO]: Training loss at epoch 27: 1.1368348896503448
[08/27/2025 23:13:34 INFO]: Training loss at epoch 24: 0.9618374705314636
[08/27/2025 23:13:42 INFO]: Training stats: {
    "score": -0.9988281288633183,
    "rmse": 0.9988281288633183
}
[08/27/2025 23:13:42 INFO]: Val stats: {
    "score": -0.6801773350541812,
    "rmse": 0.6801773350541812
}
[08/27/2025 23:13:42 INFO]: Test stats: {
    "score": -0.873769051295629,
    "rmse": 0.873769051295629
}
[08/27/2025 23:13:48 INFO]: Training loss at epoch 35: 0.9892828166484833
[08/27/2025 23:13:48 INFO]: Training loss at epoch 34: 1.0043285489082336
[08/27/2025 23:13:55 INFO]: Training loss at epoch 33: 0.9020179510116577
[08/27/2025 23:13:57 INFO]: Training loss at epoch 29: 1.0077477097511292
[08/27/2025 23:14:11 INFO]: Training loss at epoch 71: 0.8331992626190186
[08/27/2025 23:14:18 INFO]: Training loss at epoch 70: 0.9691709876060486
[08/27/2025 23:14:31 INFO]: Training loss at epoch 36: 1.1134425401687622
[08/27/2025 23:15:07 INFO]: Training loss at epoch 36: 1.0242167711257935
[08/27/2025 23:15:15 INFO]: Training loss at epoch 37: 0.9851873815059662
[08/27/2025 23:15:21 INFO]: Training loss at epoch 19: 0.8807084262371063
[08/27/2025 23:16:00 INFO]: Training loss at epoch 38: 0.9667814671993256
[08/27/2025 23:16:06 INFO]: Training loss at epoch 35: 1.2364605069160461
[08/27/2025 23:16:11 INFO]: Training stats: {
    "score": -1.002212621577059,
    "rmse": 1.002212621577059
}
[08/27/2025 23:16:11 INFO]: Val stats: {
    "score": -0.6674882598686948,
    "rmse": 0.6674882598686948
}
[08/27/2025 23:16:11 INFO]: Test stats: {
    "score": -0.8690017065329521,
    "rmse": 0.8690017065329521
}
[08/27/2025 23:16:25 INFO]: Training loss at epoch 31: 1.0828503668308258
[08/27/2025 23:16:31 INFO]: Training loss at epoch 35: 1.1465429067611694
[08/27/2025 23:16:45 INFO]: Training loss at epoch 39: 0.8945059776306152
[08/27/2025 23:16:46 INFO]: Training loss at epoch 31: 1.1341318190097809
[08/27/2025 23:17:02 INFO]: Training stats: {
    "score": -0.9988795466180969,
    "rmse": 0.9988795466180969
}
[08/27/2025 23:17:02 INFO]: Val stats: {
    "score": -0.6920854168802669,
    "rmse": 0.6920854168802669
}
[08/27/2025 23:17:02 INFO]: Test stats: {
    "score": -0.8803394828659766,
    "rmse": 0.8803394828659766
}
[08/27/2025 23:17:23 INFO]: Training loss at epoch 17: 1.1513900756835938
[08/27/2025 23:17:25 INFO]: Training stats: {
    "score": -1.02014584814709,
    "rmse": 1.02014584814709
}
[08/27/2025 23:17:25 INFO]: Val stats: {
    "score": -0.6616222166736969,
    "rmse": 0.6616222166736969
}
[08/27/2025 23:17:25 INFO]: Test stats: {
    "score": -0.8746901558815339,
    "rmse": 0.8746901558815339
}
[08/27/2025 23:17:41 INFO]: Training loss at epoch 0: 1.208966314792633
[08/27/2025 23:17:49 INFO]: Training loss at epoch 40: 0.9981118142604828
[08/27/2025 23:17:52 INFO]: Training loss at epoch 28: 0.9806692004203796
[08/27/2025 23:18:08 INFO]: Training loss at epoch 34: 0.9240961372852325
[08/27/2025 23:18:17 INFO]: Training loss at epoch 25: 1.1666516065597534
[08/27/2025 23:18:23 INFO]: Training loss at epoch 30: 0.9868420362472534
[08/27/2025 23:18:27 INFO]: New best epoch, val score: -0.6628768569385449
[08/27/2025 23:18:27 INFO]: Saving model to: unhanged-Shanesha_trial_64/model_best.pth
[08/27/2025 23:18:37 INFO]: Training loss at epoch 35: 1.0782237648963928
[08/27/2025 23:18:42 INFO]: Training loss at epoch 41: 0.7885838150978088
[08/27/2025 23:19:16 INFO]: Training loss at epoch 2: 3.7922335863113403
[08/27/2025 23:19:32 INFO]: Training loss at epoch 36: 1.068928301334381
[08/27/2025 23:19:41 INFO]: Training loss at epoch 42: 0.8539031744003296
[08/27/2025 23:20:29 INFO]: Training loss at epoch 43: 0.8662210404872894
[08/27/2025 23:20:30 INFO]: Training loss at epoch 28: 0.979463666677475
[08/27/2025 23:21:26 INFO]: Training loss at epoch 44: 0.9002692103385925
[08/27/2025 23:21:26 INFO]: Training loss at epoch 72: 0.8999685347080231
[08/27/2025 23:21:36 INFO]: Training loss at epoch 71: 1.075935274362564
[08/27/2025 23:22:09 INFO]: Training loss at epoch 45: 1.0315224528312683
[08/27/2025 23:22:12 INFO]: Training loss at epoch 37: 1.0613541007041931
[08/27/2025 23:22:18 INFO]: Running Final Evaluation...
[08/27/2025 23:22:29 INFO]: Training loss at epoch 18: 1.290711134672165
[08/27/2025 23:22:35 INFO]: Training accuracy: {
    "score": -1.0075579063905937,
    "rmse": 1.0075579063905937
}
[08/27/2025 23:22:35 INFO]: Val accuracy: {
    "score": -0.6579778838489597,
    "rmse": 0.6579778838489597
}
[08/27/2025 23:22:35 INFO]: Test accuracy: {
    "score": -0.8682816728521011,
    "rmse": 0.8682816728521011
}
[08/27/2025 23:22:35 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_62",
    "best_epoch": 14,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8682816728521011,
        "rmse": 0.8682816728521011
    },
    "train_stats": {
        "score": -1.0075579063905937,
        "rmse": 1.0075579063905937
    },
    "val_stats": {
        "score": -0.6579778838489597,
        "rmse": 0.6579778838489597
    }
}
[08/27/2025 23:22:35 INFO]: Procewss finished for trial unhanged-Shanesha_trial_62
[08/27/2025 23:22:35 INFO]: 
_________________________________________________

[08/27/2025 23:22:35 INFO]: train_net_for_optune.py main() running.
[08/27/2025 23:22:35 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 2.355702266840846
  attention_dropout: 0.4980664596166241
  ffn_dropout: 0.4980664596166241
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0007684130533586414
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_65

[08/27/2025 23:22:35 INFO]: This ft_transformer has 0.239 million parameters.
[08/27/2025 23:22:35 INFO]: Training will start at epoch 0.
[08/27/2025 23:22:35 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 23:22:47 INFO]: Training loss at epoch 35: 0.9249188303947449
[08/27/2025 23:22:51 INFO]: Training loss at epoch 29: 0.9584866464138031
[08/27/2025 23:23:01 INFO]: Training loss at epoch 0: 1.2439210414886475
[08/27/2025 23:23:03 INFO]: Training loss at epoch 36: 1.105672299861908
[08/27/2025 23:23:05 INFO]: New best epoch, val score: -0.7435212027601774
[08/27/2025 23:23:05 INFO]: Saving model to: unhanged-Shanesha_trial_65/model_best.pth
[08/27/2025 23:23:05 INFO]: Training loss at epoch 30: 0.9197869598865509
[08/27/2025 23:23:16 INFO]: Training loss at epoch 31: 1.0933462083339691
[08/27/2025 23:23:17 INFO]: Training loss at epoch 26: 0.8217845559120178
[08/27/2025 23:23:25 INFO]: Training loss at epoch 36: 0.9536847770214081
[08/27/2025 23:23:29 INFO]: Training loss at epoch 36: 1.03703111410141
[08/27/2025 23:23:32 INFO]: Training loss at epoch 1: 1.2643194198608398
[08/27/2025 23:23:32 INFO]: Training loss at epoch 32: 0.995464026927948
[08/27/2025 23:23:39 INFO]: Training loss at epoch 20: 1.0710907578468323
[08/27/2025 23:23:47 INFO]: Training loss at epoch 32: 1.0199541449546814
[08/27/2025 23:23:57 INFO]: Training loss at epoch 1: 7.82987904548645
[08/27/2025 23:24:02 INFO]: Training loss at epoch 2: 1.0432088673114777
[08/27/2025 23:24:29 INFO]: Training stats: {
    "score": -1.018627558092224,
    "rmse": 1.018627558092224
}
[08/27/2025 23:24:29 INFO]: Val stats: {
    "score": -0.6609262529850466,
    "rmse": 0.6609262529850466
}
[08/27/2025 23:24:29 INFO]: Test stats: {
    "score": -0.876629292068673,
    "rmse": 0.876629292068673
}
[08/27/2025 23:24:32 INFO]: Training loss at epoch 3: 1.2178040146827698
[08/27/2025 23:24:35 INFO]: New best epoch, val score: -0.6876074063103705
[08/27/2025 23:24:35 INFO]: Saving model to: unhanged-Shanesha_trial_65/model_best.pth
[08/27/2025 23:25:02 INFO]: Training loss at epoch 4: 0.9066035151481628
[08/27/2025 23:25:06 INFO]: New best epoch, val score: -0.6620418420375555
[08/27/2025 23:25:06 INFO]: Saving model to: unhanged-Shanesha_trial_65/model_best.pth
[08/27/2025 23:25:18 INFO]: Training loss at epoch 3: 1.2241592407226562
[08/27/2025 23:25:32 INFO]: Training loss at epoch 5: 1.0091732144355774
[08/27/2025 23:26:07 INFO]: Training loss at epoch 6: 1.018225073814392
[08/27/2025 23:26:18 INFO]: Training loss at epoch 37: 1.2088156342506409
[08/27/2025 23:26:39 INFO]: Training loss at epoch 7: 1.0124562978744507
[08/27/2025 23:26:59 INFO]: Training loss at epoch 36: 1.3547961115837097
[08/27/2025 23:27:13 INFO]: Training loss at epoch 8: 1.1413289904594421
[08/27/2025 23:27:13 INFO]: Training loss at epoch 19: 0.9303153157234192
[08/27/2025 23:27:21 INFO]: Training loss at epoch 29: 1.1762139797210693
[08/27/2025 23:27:46 INFO]: Training loss at epoch 9: 0.7827506065368652
[08/27/2025 23:27:57 INFO]: Training stats: {
    "score": -1.0094655938858543,
    "rmse": 1.0094655938858543
}
[08/27/2025 23:27:57 INFO]: Val stats: {
    "score": -0.6623398107046099,
    "rmse": 0.6623398107046099
}
[08/27/2025 23:27:57 INFO]: Test stats: {
    "score": -0.8683921476508957,
    "rmse": 0.8683921476508957
}
[08/27/2025 23:28:00 INFO]: Training loss at epoch 32: 0.9032979309558868
[08/27/2025 23:28:06 INFO]: Training loss at epoch 27: 0.8743632733821869
[08/27/2025 23:28:14 INFO]: Training loss at epoch 37: 0.8513814210891724
[08/27/2025 23:28:25 INFO]: Training loss at epoch 73: 0.8955273628234863
[08/27/2025 23:28:29 INFO]: Training loss at epoch 10: 1.0414170026779175
[08/27/2025 23:28:39 INFO]: Training loss at epoch 72: 1.0161058604717255
[08/27/2025 23:28:58 INFO]: Training stats: {
    "score": -1.0160957809294808,
    "rmse": 1.0160957809294808
}
[08/27/2025 23:28:58 INFO]: Val stats: {
    "score": -0.6612833433950215,
    "rmse": 0.6612833433950215
}
[08/27/2025 23:28:58 INFO]: Test stats: {
    "score": -0.8736840604550103,
    "rmse": 0.8736840604550103
}
[08/27/2025 23:29:02 INFO]: Training loss at epoch 11: 0.9975146949291229
[08/27/2025 23:29:02 INFO]: Training loss at epoch 38: 1.052787721157074
[08/27/2025 23:29:18 INFO]: Running Final Evaluation...
[08/27/2025 23:29:30 INFO]: Training loss at epoch 30: 1.1409534513950348
[08/27/2025 23:29:35 INFO]: Training loss at epoch 12: 1.2384570837020874
[08/27/2025 23:29:35 INFO]: New best epoch, val score: -0.6612833433950215
[08/27/2025 23:29:35 INFO]: Saving model to: unhanged-Shanesha_trial_60/model_best.pth
[08/27/2025 23:29:49 INFO]: Training stats: {
    "score": -1.0124449990182385,
    "rmse": 1.0124449990182385
}
[08/27/2025 23:29:49 INFO]: Val stats: {
    "score": -0.6607785530051009,
    "rmse": 0.6607785530051009
}
[08/27/2025 23:29:49 INFO]: Test stats: {
    "score": -0.8711395681698029,
    "rmse": 0.8711395681698029
}
[08/27/2025 23:29:57 INFO]: Training loss at epoch 31: 0.8593320250511169
[08/27/2025 23:29:58 INFO]: Training loss at epoch 37: 1.071455955505371
[08/27/2025 23:30:01 INFO]: Training loss at epoch 21: 1.136569619178772
[08/27/2025 23:30:05 INFO]: Training loss at epoch 13: 1.027413010597229
[08/27/2025 23:30:12 INFO]: Training loss at epoch 2: 2.362950384616852
[08/27/2025 23:30:23 INFO]: Training loss at epoch 37: 0.8482770323753357
[08/27/2025 23:30:35 INFO]: Training loss at epoch 14: 0.8866523206233978
[08/27/2025 23:30:36 INFO]: New best epoch, val score: -0.6607785530051009
[08/27/2025 23:30:36 INFO]: Saving model to: unhanged-Shanesha_trial_50/model_best.pth
[08/27/2025 23:30:41 INFO]: Training loss at epoch 33: 0.8350743055343628
[08/27/2025 23:30:44 INFO]: Training loss at epoch 33: 0.8720163106918335
[08/27/2025 23:30:45 INFO]: New best epoch, val score: -0.6606763703006643
[08/27/2025 23:30:45 INFO]: Saving model to: unhanged-Shanesha_trial_59/model_best.pth
[08/27/2025 23:31:24 INFO]: Training loss at epoch 15: 0.9644156098365784
[08/27/2025 23:31:31 INFO]: Training loss at epoch 37: 0.962454229593277
[08/27/2025 23:31:45 INFO]: Training loss at epoch 4: 1.2976403832435608
[08/27/2025 23:31:54 INFO]: Training loss at epoch 16: 0.8637945055961609
[08/27/2025 23:32:14 INFO]: Training accuracy: {
    "score": -1.0107836471132505,
    "rmse": 1.0107836471132505
}
[08/27/2025 23:32:14 INFO]: Val accuracy: {
    "score": -0.660473896685105,
    "rmse": 0.660473896685105
}
[08/27/2025 23:32:14 INFO]: Test accuracy: {
    "score": -0.8702672669779042,
    "rmse": 0.8702672669779042
}
[08/27/2025 23:32:14 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_18",
    "best_epoch": 42,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8702672669779042,
        "rmse": 0.8702672669779042
    },
    "train_stats": {
        "score": -1.0107836471132505,
        "rmse": 1.0107836471132505
    },
    "val_stats": {
        "score": -0.660473896685105,
        "rmse": 0.660473896685105
    }
}
[08/27/2025 23:32:14 INFO]: Procewss finished for trial unhanged-Shanesha_trial_18
[08/27/2025 23:32:15 INFO]: 
_________________________________________________

[08/27/2025 23:32:15 INFO]: train_net_for_optune.py main() running.
[08/27/2025 23:32:15 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 2.3207223119436104
  attention_dropout: 0.43874102075138544
  ffn_dropout: 0.43874102075138544
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0006958201264320543
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_66

[08/27/2025 23:32:15 INFO]: This ft_transformer has 0.238 million parameters.
[08/27/2025 23:32:15 INFO]: Training will start at epoch 0.
[08/27/2025 23:32:15 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 23:32:24 INFO]: Training loss at epoch 17: 1.0197086334228516
[08/27/2025 23:32:29 INFO]: New best epoch, val score: -0.6626717107718704
[08/27/2025 23:32:29 INFO]: Saving model to: unhanged-Shanesha_trial_63/model_best.pth
[08/27/2025 23:32:42 INFO]: Training loss at epoch 0: 1.2876638770103455
[08/27/2025 23:32:46 INFO]: New best epoch, val score: -0.6881633851851351
[08/27/2025 23:32:46 INFO]: Saving model to: unhanged-Shanesha_trial_66/model_best.pth
[08/27/2025 23:32:52 INFO]: Training loss at epoch 33: 1.0215941667556763
[08/27/2025 23:32:56 INFO]: Training loss at epoch 18: 0.9244321882724762
[08/27/2025 23:33:09 INFO]: Training loss at epoch 28: 1.0038020610809326
[08/27/2025 23:33:10 INFO]: Training loss at epoch 38: 1.1172411441802979
[08/27/2025 23:33:13 INFO]: Training loss at epoch 1: 1.0166330933570862
[08/27/2025 23:33:30 INFO]: Training loss at epoch 38: 0.9118058383464813
[08/27/2025 23:33:37 INFO]: Training loss at epoch 19: 1.228319674730301
[08/27/2025 23:33:43 INFO]: Training loss at epoch 2: 1.3076929450035095
[08/27/2025 23:33:47 INFO]: Training stats: {
    "score": -1.000608277790541,
    "rmse": 1.000608277790541
}
[08/27/2025 23:33:47 INFO]: Val stats: {
    "score": -0.6912890186400277,
    "rmse": 0.6912890186400277
}
[08/27/2025 23:33:47 INFO]: Test stats: {
    "score": -0.8779531532299456,
    "rmse": 0.8779531532299456
}
[08/27/2025 23:33:51 INFO]: Training loss at epoch 20: 0.9417319893836975
[08/27/2025 23:34:13 INFO]: Training loss at epoch 3: 1.0730406641960144
[08/27/2025 23:34:16 INFO]: New best epoch, val score: -0.6869093772976949
[08/27/2025 23:34:16 INFO]: Saving model to: unhanged-Shanesha_trial_66/model_best.pth
[08/27/2025 23:34:21 INFO]: Training loss at epoch 20: 1.0653295516967773
[08/27/2025 23:34:22 INFO]: Training loss at epoch 31: 1.0708110332489014
[08/27/2025 23:34:45 INFO]: Training loss at epoch 4: 1.2183337211608887
[08/27/2025 23:34:49 INFO]: New best epoch, val score: -0.6807209796484233
[08/27/2025 23:34:49 INFO]: Saving model to: unhanged-Shanesha_trial_66/model_best.pth
[08/27/2025 23:34:52 INFO]: Training loss at epoch 21: 1.0776690244674683
[08/27/2025 23:35:18 INFO]: Training loss at epoch 5: 1.2641002535820007
[08/27/2025 23:35:22 INFO]: New best epoch, val score: -0.6799159469236389
[08/27/2025 23:35:22 INFO]: Saving model to: unhanged-Shanesha_trial_66/model_best.pth
[08/27/2025 23:35:25 INFO]: Training loss at epoch 22: 0.9975078701972961
[08/27/2025 23:35:45 INFO]: Training loss at epoch 38: 0.9062569439411163
[08/27/2025 23:35:51 INFO]: Training loss at epoch 6: 1.0587358474731445
[08/27/2025 23:35:54 INFO]: Training loss at epoch 73: 1.143269568681717
[08/27/2025 23:35:55 INFO]: New best epoch, val score: -0.6787467265636956
[08/27/2025 23:35:55 INFO]: Saving model to: unhanged-Shanesha_trial_66/model_best.pth
[08/27/2025 23:35:58 INFO]: Training loss at epoch 23: 1.1594150066375732
[08/27/2025 23:36:05 INFO]: Training loss at epoch 39: 0.8385788202285767
[08/27/2025 23:36:16 INFO]: Training loss at epoch 22: 1.0499611496925354
[08/27/2025 23:36:25 INFO]: Training loss at epoch 7: 0.8656598329544067
[08/27/2025 23:36:29 INFO]: New best epoch, val score: -0.6774244631012012
[08/27/2025 23:36:29 INFO]: Saving model to: unhanged-Shanesha_trial_66/model_best.pth
[08/27/2025 23:36:31 INFO]: Training loss at epoch 24: 0.942531943321228
[08/27/2025 23:36:35 INFO]: Training loss at epoch 3: 1.787495493888855
[08/27/2025 23:36:51 INFO]: Training loss at epoch 32: 1.009635329246521
[08/27/2025 23:36:53 INFO]: Training loss at epoch 30: 1.0850316286087036
[08/27/2025 23:36:57 INFO]: Training loss at epoch 38: 0.9194939136505127
[08/27/2025 23:36:59 INFO]: Training loss at epoch 8: 1.356867253780365
[08/27/2025 23:37:03 INFO]: New best epoch, val score: -0.6759963010710816
[08/27/2025 23:37:03 INFO]: Saving model to: unhanged-Shanesha_trial_66/model_best.pth
[08/27/2025 23:37:06 INFO]: Training loss at epoch 25: 1.0786329805850983
[08/27/2025 23:37:27 INFO]: Training loss at epoch 38: 0.9511509835720062
[08/27/2025 23:37:35 INFO]: Training loss at epoch 9: 1.2644022703170776
[08/27/2025 23:37:39 INFO]: Training loss at epoch 26: 1.1654712557792664
[08/27/2025 23:37:42 INFO]: Training loss at epoch 34: 0.919383704662323
[08/27/2025 23:37:45 INFO]: Training loss at epoch 34: 0.9855526685714722
[08/27/2025 23:37:49 INFO]: Training stats: {
    "score": -1.042190527496634,
    "rmse": 1.042190527496634
}
[08/27/2025 23:37:49 INFO]: Val stats: {
    "score": -0.6744677442048097,
    "rmse": 0.6744677442048097
}
[08/27/2025 23:37:49 INFO]: Test stats: {
    "score": -0.8907505862226892,
    "rmse": 0.8907505862226892
}
[08/27/2025 23:37:54 INFO]: New best epoch, val score: -0.6744677442048097
[08/27/2025 23:37:54 INFO]: Saving model to: unhanged-Shanesha_trial_66/model_best.pth
[08/27/2025 23:37:55 INFO]: Training loss at epoch 34: 0.8719427585601807
[08/27/2025 23:37:58 INFO]: Training loss at epoch 5: 1.1228811144828796
[08/27/2025 23:38:08 INFO]: Training loss at epoch 39: 1.1151058971881866
[08/27/2025 23:38:08 INFO]: Training loss at epoch 29: 1.178286373615265
[08/27/2025 23:38:11 INFO]: Training loss at epoch 27: 1.1181377172470093
[08/27/2025 23:38:24 INFO]: Training loss at epoch 10: 1.1207749843597412
[08/27/2025 23:38:28 INFO]: New best epoch, val score: -0.6641537614821242
[08/27/2025 23:38:28 INFO]: Saving model to: unhanged-Shanesha_trial_66/model_best.pth
[08/27/2025 23:38:33 INFO]: Training stats: {
    "score": -1.001360588892565,
    "rmse": 1.001360588892565
}
[08/27/2025 23:38:33 INFO]: Val stats: {
    "score": -0.6682955900609672,
    "rmse": 0.6682955900609672
}
[08/27/2025 23:38:33 INFO]: Test stats: {
    "score": -0.869510957577322,
    "rmse": 0.869510957577322
}
[08/27/2025 23:38:44 INFO]: Training loss at epoch 28: 1.1723817586898804
[08/27/2025 23:38:53 INFO]: Training loss at epoch 21: 1.0305421948432922
[08/27/2025 23:38:57 INFO]: Training loss at epoch 11: 1.2110968232154846
[08/27/2025 23:39:16 INFO]: Training loss at epoch 29: 0.996036171913147
[08/27/2025 23:39:26 INFO]: Training loss at epoch 32: 1.0511502027511597
[08/27/2025 23:39:27 INFO]: Training stats: {
    "score": -1.0019847155094252,
    "rmse": 1.0019847155094252
}
[08/27/2025 23:39:27 INFO]: Val stats: {
    "score": -0.7014787444653219,
    "rmse": 0.7014787444653219
}
[08/27/2025 23:39:27 INFO]: Test stats: {
    "score": -0.8834072784454466,
    "rmse": 0.8834072784454466
}
[08/27/2025 23:39:30 INFO]: Training loss at epoch 12: 1.1129207015037537
[08/27/2025 23:39:47 INFO]: Training stats: {
    "score": -1.0129475863393411,
    "rmse": 1.0129475863393411
}
[08/27/2025 23:39:47 INFO]: Val stats: {
    "score": -0.7424492866858996,
    "rmse": 0.7424492866858996
}
[08/27/2025 23:39:47 INFO]: Test stats: {
    "score": -0.9105977057387622,
    "rmse": 0.9105977057387622
}
[08/27/2025 23:39:47 INFO]: Training stats: {
    "score": -1.0041895721515715,
    "rmse": 1.0041895721515715
}
[08/27/2025 23:39:47 INFO]: Val stats: {
    "score": -0.7097125819061942,
    "rmse": 0.7097125819061942
}
[08/27/2025 23:39:47 INFO]: Test stats: {
    "score": -0.8899558992025507,
    "rmse": 0.8899558992025507
}
[08/27/2025 23:39:57 INFO]: Training loss at epoch 30: 1.0925187170505524
[08/27/2025 23:39:58 INFO]: Running Final Evaluation...
[08/27/2025 23:40:01 INFO]: Training loss at epoch 13: 1.0040037631988525
[08/27/2025 23:40:08 INFO]: Training loss at epoch 39: 0.9231256246566772
[08/27/2025 23:40:27 INFO]: Training loss at epoch 39: 0.7922221720218658
[08/27/2025 23:40:28 INFO]: Training loss at epoch 31: 0.9066943228244781
[08/27/2025 23:40:32 INFO]: Training loss at epoch 14: 0.9488970935344696
[08/27/2025 23:41:00 INFO]: Training loss at epoch 32: 0.8373366296291351
[08/27/2025 23:41:02 INFO]: Training loss at epoch 15: 0.7779898792505264
[08/27/2025 23:41:30 INFO]: Training loss at epoch 33: 0.9635301530361176
[08/27/2025 23:41:33 INFO]: Training loss at epoch 16: 0.8617640137672424
[08/27/2025 23:41:35 INFO]: Training stats: {
    "score": -0.9991850310335376,
    "rmse": 0.9991850310335376
}
[08/27/2025 23:41:35 INFO]: Val stats: {
    "score": -0.7028561735189836,
    "rmse": 0.7028561735189836
}
[08/27/2025 23:41:35 INFO]: Test stats: {
    "score": -0.8883208082261493,
    "rmse": 0.8883208082261493
}
[08/27/2025 23:42:03 INFO]: Training loss at epoch 34: 0.7729747742414474
[08/27/2025 23:42:03 INFO]: Training loss at epoch 17: 1.1664074063301086
[08/27/2025 23:42:17 INFO]: Training loss at epoch 35: 0.8114069998264313
[08/27/2025 23:42:17 INFO]: Training loss at epoch 23: 0.9684422314167023
[08/27/2025 23:42:19 INFO]: Training accuracy: {
    "score": -1.0060861343473726,
    "rmse": 1.0060861343473726
}
[08/27/2025 23:42:19 INFO]: Val accuracy: {
    "score": -0.6580100369908094,
    "rmse": 0.6580100369908094
}
[08/27/2025 23:42:19 INFO]: Test accuracy: {
    "score": -0.8693019043747637,
    "rmse": 0.8693019043747637
}
[08/27/2025 23:42:20 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_57",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8693019043747637,
        "rmse": 0.8693019043747637
    },
    "train_stats": {
        "score": -1.0060861343473726,
        "rmse": 1.0060861343473726
    },
    "val_stats": {
        "score": -0.6580100369908094,
        "rmse": 0.6580100369908094
    }
}
[08/27/2025 23:42:20 INFO]: Procewss finished for trial unhanged-Shanesha_trial_57
[08/27/2025 23:42:21 INFO]: 
_________________________________________________

[08/27/2025 23:42:21 INFO]: train_net_for_optune.py main() running.
[08/27/2025 23:42:21 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 1.1029929102825176
  attention_dropout: 0.4430718574288536
  ffn_dropout: 0.4430718574288536
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0007260966637782261
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_67

[08/27/2025 23:42:21 INFO]: This ft_transformer has 0.177 million parameters.
[08/27/2025 23:42:21 INFO]: Training will start at epoch 0.
[08/27/2025 23:42:21 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 23:42:35 INFO]: Training loss at epoch 18: 1.0027922689914703
[08/27/2025 23:42:36 INFO]: Training loss at epoch 35: 1.0490164756774902
[08/27/2025 23:42:39 INFO]: Training loss at epoch 4: 1.3907209038734436
[08/27/2025 23:42:39 INFO]: Running Final Evaluation...
[08/27/2025 23:42:43 INFO]: Training stats: {
    "score": -0.9990100729814019,
    "rmse": 0.9990100729814019
}
[08/27/2025 23:42:43 INFO]: Val stats: {
    "score": -0.6870246015914063,
    "rmse": 0.6870246015914063
}
[08/27/2025 23:42:43 INFO]: Test stats: {
    "score": -0.8774206646979116,
    "rmse": 0.8774206646979116
}
[08/27/2025 23:42:45 INFO]: Training loss at epoch 0: 1.8898698091506958
[08/27/2025 23:42:49 INFO]: New best epoch, val score: -1.2744914208020495
[08/27/2025 23:42:49 INFO]: Saving model to: unhanged-Shanesha_trial_67/model_best.pth
[08/27/2025 23:42:50 INFO]: Training accuracy: {
    "score": -1.0106530814992136,
    "rmse": 1.0106530814992136
}
[08/27/2025 23:42:50 INFO]: Val accuracy: {
    "score": -0.6620418420375555,
    "rmse": 0.6620418420375555
}
[08/27/2025 23:42:50 INFO]: Test accuracy: {
    "score": -0.8688073899593496,
    "rmse": 0.8688073899593496
}
[08/27/2025 23:42:50 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_65",
    "best_epoch": 4,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8688073899593496,
        "rmse": 0.8688073899593496
    },
    "train_stats": {
        "score": -1.0106530814992136,
        "rmse": 1.0106530814992136
    },
    "val_stats": {
        "score": -0.6620418420375555,
        "rmse": 0.6620418420375555
    }
}
[08/27/2025 23:42:50 INFO]: Procewss finished for trial unhanged-Shanesha_trial_65
[08/27/2025 23:42:50 INFO]: 
_________________________________________________

[08/27/2025 23:42:50 INFO]: train_net_for_optune.py main() running.
[08/27/2025 23:42:50 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 1.1354011079970916
  attention_dropout: 0.4377992373205034
  ffn_dropout: 0.4377992373205034
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0004739417840943636
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_68

[08/27/2025 23:42:50 INFO]: This ft_transformer has 0.666 million parameters.
[08/27/2025 23:42:50 INFO]: Training will start at epoch 0.
[08/27/2025 23:42:50 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 23:42:58 INFO]: Training loss at epoch 74: 1.035548210144043
[08/27/2025 23:43:07 INFO]: Training loss at epoch 19: 1.047806203365326
[08/27/2025 23:43:13 INFO]: Training loss at epoch 1: 1.2244654297828674
[08/27/2025 23:43:17 INFO]: New best epoch, val score: -0.6890325358301516
[08/27/2025 23:43:17 INFO]: Saving model to: unhanged-Shanesha_trial_67/model_best.pth
[08/27/2025 23:43:31 INFO]: Training stats: {
    "score": -1.0063484706506494,
    "rmse": 1.0063484706506494
}
[08/27/2025 23:43:31 INFO]: Val stats: {
    "score": -0.7185010370709252,
    "rmse": 0.7185010370709252
}
[08/27/2025 23:43:31 INFO]: Test stats: {
    "score": -0.8956373301144152,
    "rmse": 0.8956373301144152
}
[08/27/2025 23:43:42 INFO]: Training loss at epoch 0: 1.1020992994308472
[08/27/2025 23:43:44 INFO]: Training loss at epoch 22: 0.974642276763916
[08/27/2025 23:43:48 INFO]: New best epoch, val score: -0.6608921793487637
[08/27/2025 23:43:48 INFO]: Saving model to: unhanged-Shanesha_trial_68/model_best.pth
[08/27/2025 23:43:49 INFO]: Training loss at epoch 33: 1.0567578673362732
[08/27/2025 23:43:52 INFO]: Training loss at epoch 39: 1.1578053832054138
[08/27/2025 23:43:55 INFO]: Training loss at epoch 2: 0.9499617516994476
[08/27/2025 23:43:55 INFO]: Training loss at epoch 31: 0.9736869037151337
[08/27/2025 23:44:01 INFO]: Training loss at epoch 20: 0.9645129442214966
[08/27/2025 23:44:12 INFO]: Training loss at epoch 6: 1.073068618774414
[08/27/2025 23:44:15 INFO]: Training loss at epoch 39: 0.9199732840061188
[08/27/2025 23:44:24 INFO]: Training loss at epoch 3: 1.1943695545196533
[08/27/2025 23:44:28 INFO]: Training loss at epoch 1: 1.359428584575653
[08/27/2025 23:44:31 INFO]: Training loss at epoch 30: 1.1427207589149475
[08/27/2025 23:44:31 INFO]: Training loss at epoch 40: 1.2442311942577362
[08/27/2025 23:44:38 INFO]: Training loss at epoch 35: 1.2744078040122986
[08/27/2025 23:44:41 INFO]: Training loss at epoch 21: 0.9484667778015137
[08/27/2025 23:44:54 INFO]: Training loss at epoch 35: 0.9877365827560425
[08/27/2025 23:44:55 INFO]: Training loss at epoch 4: 1.0148605406284332
[08/27/2025 23:44:58 INFO]: New best epoch, val score: -0.667084465843259
[08/27/2025 23:44:58 INFO]: Saving model to: unhanged-Shanesha_trial_67/model_best.pth
[08/27/2025 23:44:59 INFO]: New best epoch, val score: -0.6623787709291442
[08/27/2025 23:44:59 INFO]: Saving model to: unhanged-Shanesha_trial_63/model_best.pth
[08/27/2025 23:45:17 INFO]: Training loss at epoch 22: 1.0951143503189087
[08/27/2025 23:45:21 INFO]: Training loss at epoch 2: 1.1495124101638794
[08/27/2025 23:45:25 INFO]: Training loss at epoch 40: 0.9925993084907532
[08/27/2025 23:45:28 INFO]: Training loss at epoch 5: 1.0978965163230896
[08/27/2025 23:45:50 INFO]: Training loss at epoch 23: 1.0216884016990662
[08/27/2025 23:46:04 INFO]: Training loss at epoch 6: 1.231568694114685
[08/27/2025 23:46:04 INFO]: Training loss at epoch 40: 1.1742170453071594
[08/27/2025 23:46:07 INFO]: Training loss at epoch 3: 0.9809820652008057
[08/27/2025 23:46:22 INFO]: Training loss at epoch 24: 1.0763427019119263
[08/27/2025 23:46:23 INFO]: Training stats: {
    "score": -1.0045304905495407,
    "rmse": 1.0045304905495407
}
[08/27/2025 23:46:23 INFO]: Val stats: {
    "score": -0.6638655813878316,
    "rmse": 0.6638655813878316
}
[08/27/2025 23:46:23 INFO]: Test stats: {
    "score": -0.8684129602379083,
    "rmse": 0.8684129602379083
}
[08/27/2025 23:46:26 INFO]: New best epoch, val score: -0.6632795018882222
[08/27/2025 23:46:26 INFO]: Saving model to: unhanged-Shanesha_trial_66/model_best.pth
[08/27/2025 23:46:34 INFO]: Training loss at epoch 7: 1.2683782279491425
[08/27/2025 23:46:49 INFO]: Training stats: {
    "score": -1.0018312102399773,
    "rmse": 1.0018312102399773
}
[08/27/2025 23:46:49 INFO]: Val stats: {
    "score": -0.7006715779482912,
    "rmse": 0.7006715779482912
}
[08/27/2025 23:46:49 INFO]: Test stats: {
    "score": -0.8850830345267764,
    "rmse": 0.8850830345267764
}
[08/27/2025 23:46:55 INFO]: Training loss at epoch 25: 0.8971276581287384
[08/27/2025 23:46:59 INFO]: Training loss at epoch 4: 0.9944933652877808
[08/27/2025 23:47:08 INFO]: Training loss at epoch 8: 1.2930607199668884
[08/27/2025 23:47:22 INFO]: Training loss at epoch 36: 0.8819792568683624
[08/27/2025 23:47:28 INFO]: Training loss at epoch 26: 0.8239492177963257
[08/27/2025 23:47:38 INFO]: Training loss at epoch 9: 1.1264406442642212
[08/27/2025 23:47:53 INFO]: Training loss at epoch 5: 1.1370931267738342
[08/27/2025 23:47:54 INFO]: Training stats: {
    "score": -1.0022897346936837,
    "rmse": 1.0022897346936837
}
[08/27/2025 23:47:54 INFO]: Val stats: {
    "score": -0.6701871704562183,
    "rmse": 0.6701871704562183
}
[08/27/2025 23:47:54 INFO]: Test stats: {
    "score": -0.8685426478331972,
    "rmse": 0.8685426478331972
}
[08/27/2025 23:48:07 INFO]: 
_________________________________________________

[08/27/2025 23:48:07 INFO]: train_net_for_optune.py main() running.
[08/27/2025 23:48:07 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 1.9768073417041416
  attention_dropout: 0.3971857875671269
  ffn_dropout: 0.3971857875671269
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 5.0453186900977374e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_69

[08/27/2025 23:48:08 INFO]: This ft_transformer has 12.713 million parameters.
[08/27/2025 23:48:08 INFO]: Training will start at epoch 0.
[08/27/2025 23:48:08 INFO]: ==> Starting training for 200 epochs...
[08/27/2025 23:48:08 INFO]: Training loss at epoch 27: 1.047178030014038
[08/27/2025 23:48:31 INFO]: Training loss at epoch 10: 1.1901061236858368
[08/27/2025 23:48:39 INFO]: Training loss at epoch 28: 0.9846560060977936
[08/27/2025 23:48:41 INFO]: Training loss at epoch 24: 0.9835673868656158
[08/27/2025 23:48:44 INFO]: Training loss at epoch 23: 0.8740476369857788
[08/27/2025 23:48:59 INFO]: Training loss at epoch 11: 1.0387746095657349
[08/27/2025 23:49:08 INFO]: Training loss at epoch 5: 0.9730279445648193
[08/27/2025 23:49:08 INFO]: Training loss at epoch 29: 1.126315414905548
[08/27/2025 23:49:19 INFO]: Training stats: {
    "score": -1.0000101537384507,
    "rmse": 1.0000101537384507
}
[08/27/2025 23:49:19 INFO]: Val stats: {
    "score": -0.6742503185263813,
    "rmse": 0.6742503185263813
}
[08/27/2025 23:49:19 INFO]: Test stats: {
    "score": -0.8716539024975387,
    "rmse": 0.8716539024975387
}
[08/27/2025 23:49:35 INFO]: Training loss at epoch 31: 0.8728932738304138
[08/27/2025 23:49:35 INFO]: Training loss at epoch 41: 1.0741071701049805
[08/27/2025 23:49:37 INFO]: Training loss at epoch 12: 0.9033822417259216
[08/27/2025 23:49:59 INFO]: Training loss at epoch 40: 1.0437689423561096
[08/27/2025 23:50:01 INFO]: Training loss at epoch 30: 0.9871451258659363
[08/27/2025 23:50:05 INFO]: Training loss at epoch 13: 1.0503695607185364
[08/27/2025 23:50:19 INFO]: Training loss at epoch 41: 0.9927662909030914
[08/27/2025 23:50:20 INFO]: Training loss at epoch 75: 0.932802677154541
[08/27/2025 23:50:27 INFO]: Training loss at epoch 7: 0.7357458174228668
[08/27/2025 23:50:32 INFO]: Training loss at epoch 31: 0.966938704252243
[08/27/2025 23:50:33 INFO]: Training loss at epoch 14: 1.0073537528514862
[08/27/2025 23:50:40 INFO]: Training loss at epoch 34: 0.8538011014461517
[08/27/2025 23:50:50 INFO]: Training loss at epoch 32: 0.8739556670188904
[08/27/2025 23:51:00 INFO]: Training loss at epoch 15: 1.025827169418335
[08/27/2025 23:51:02 INFO]: Training loss at epoch 32: 1.0636414289474487
[08/27/2025 23:51:12 INFO]: New best epoch, val score: -0.6614122924855141
[08/27/2025 23:51:12 INFO]: Saving model to: unhanged-Shanesha_trial_63/model_best.pth
[08/27/2025 23:51:28 INFO]: Training loss at epoch 16: 0.968454897403717
[08/27/2025 23:51:29 INFO]: Training loss at epoch 36: 0.8373510539531708
[08/27/2025 23:51:32 INFO]: Training loss at epoch 33: 1.0939364433288574
[08/27/2025 23:51:47 INFO]: Training loss at epoch 36: 0.8811966776847839
[08/27/2025 23:51:56 INFO]: Training loss at epoch 17: 0.8796182572841644
[08/27/2025 23:52:02 INFO]: Training loss at epoch 34: 1.1104682385921478
[08/27/2025 23:52:04 INFO]: Training loss at epoch 37: 1.0055765211582184
[08/27/2025 23:52:05 INFO]: Training loss at epoch 41: 1.0854222476482391
[08/27/2025 23:52:24 INFO]: Training loss at epoch 18: 0.8909733593463898
[08/27/2025 23:52:32 INFO]: Training loss at epoch 35: 1.1790671944618225
[08/27/2025 23:52:54 INFO]: Training loss at epoch 19: 1.2083474397659302
[08/27/2025 23:52:56 INFO]: Training loss at epoch 0: 1.2357922792434692
[08/27/2025 23:53:04 INFO]: Training loss at epoch 40: 1.1604494452476501
[08/27/2025 23:53:04 INFO]: Training loss at epoch 36: 1.0519993305206299
[08/27/2025 23:53:05 INFO]: Training stats: {
    "score": -1.0005114752840123,
    "rmse": 1.0005114752840123
}
[08/27/2025 23:53:05 INFO]: Val stats: {
    "score": -0.6886251457397167,
    "rmse": 0.6886251457397167
}
[08/27/2025 23:53:05 INFO]: Test stats: {
    "score": -0.8771185216055526,
    "rmse": 0.8771185216055526
}
[08/27/2025 23:53:21 INFO]: Training loss at epoch 24: 1.0541470646858215
[08/27/2025 23:53:26 INFO]: Training loss at epoch 40: 0.9179930686950684
[08/27/2025 23:53:35 INFO]: Training loss at epoch 20: 1.0432541966438293
[08/27/2025 23:53:35 INFO]: Training loss at epoch 37: 0.8945934772491455
[08/27/2025 23:53:38 INFO]: New best epoch, val score: -0.6698906896025487
[08/27/2025 23:53:38 INFO]: Saving model to: unhanged-Shanesha_trial_69/model_best.pth
[08/27/2025 23:54:06 INFO]: Training loss at epoch 21: 0.920187771320343
[08/27/2025 23:54:09 INFO]: Training loss at epoch 38: 1.2425742149353027
[08/27/2025 23:54:17 INFO]: Training loss at epoch 42: 0.9500733017921448
[08/27/2025 23:54:19 INFO]: Training loss at epoch 32: 0.9900228083133698
[08/27/2025 23:54:34 INFO]: Training loss at epoch 42: 1.0149739682674408
[08/27/2025 23:54:36 INFO]: Training loss at epoch 22: 0.9934414923191071
[08/27/2025 23:54:41 INFO]: Training loss at epoch 39: 0.9968715310096741
[08/27/2025 23:54:58 INFO]: Training loss at epoch 25: 0.8104219138622284
[08/27/2025 23:54:59 INFO]: Training stats: {
    "score": -0.9996302925717748,
    "rmse": 0.9996302925717748
}
[08/27/2025 23:54:59 INFO]: Val stats: {
    "score": -0.6774108732429452,
    "rmse": 0.6774108732429452
}
[08/27/2025 23:54:59 INFO]: Test stats: {
    "score": -0.8725583891504837,
    "rmse": 0.8725583891504837
}
[08/27/2025 23:55:04 INFO]: Training loss at epoch 23: 0.9456126689910889
[08/27/2025 23:55:24 INFO]: Training loss at epoch 6: 1.19782292842865
[08/27/2025 23:55:32 INFO]: Training loss at epoch 40: 1.0174370110034943
[08/27/2025 23:55:35 INFO]: Training loss at epoch 24: 1.017071545124054
[08/27/2025 23:56:05 INFO]: Training loss at epoch 25: 0.8804247677326202
[08/27/2025 23:56:05 INFO]: Training loss at epoch 41: 1.045317530632019
[08/27/2025 23:56:42 INFO]: Training loss at epoch 26: 1.151427298784256
[08/27/2025 23:56:42 INFO]: Training loss at epoch 8: 1.0027303993701935
[08/27/2025 23:56:43 INFO]: Training loss at epoch 42: 0.946275919675827
[08/27/2025 23:56:58 INFO]: Training loss at epoch 41: 0.8435468673706055
[08/27/2025 23:57:01 INFO]: Training loss at epoch 38: 0.9703304767608643
[08/27/2025 23:57:12 INFO]: Training loss at epoch 27: 1.0458247065544128
[08/27/2025 23:57:16 INFO]: Training loss at epoch 43: 0.854180246591568
[08/27/2025 23:57:29 INFO]: New best epoch, val score: -0.6606370095048352
[08/27/2025 23:57:29 INFO]: Saving model to: unhanged-Shanesha_trial_63/model_best.pth
[08/27/2025 23:57:34 INFO]: Training loss at epoch 35: 1.1902390122413635
[08/27/2025 23:57:35 INFO]: Training loss at epoch 76: 0.9597607254981995
[08/27/2025 23:57:43 INFO]: Training loss at epoch 28: 0.8709777891635895
[08/27/2025 23:57:49 INFO]: Training loss at epoch 44: 0.8662024736404419
[08/27/2025 23:58:01 INFO]: Training loss at epoch 33: 0.9246893525123596
[08/27/2025 23:58:12 INFO]: Training loss at epoch 29: 1.0224210321903229
[08/27/2025 23:58:22 INFO]: Training stats: {
    "score": -1.008663730411987,
    "rmse": 1.008663730411987
}
[08/27/2025 23:58:22 INFO]: Val stats: {
    "score": -0.7244360616826179,
    "rmse": 0.7244360616826179
}
[08/27/2025 23:58:22 INFO]: Test stats: {
    "score": -0.8980779639776013,
    "rmse": 0.8980779639776013
}
[08/27/2025 23:58:24 INFO]: Training loss at epoch 25: 1.1082278490066528
[08/27/2025 23:58:26 INFO]: Training loss at epoch 37: 1.142832100391388
[08/27/2025 23:58:32 INFO]: Training loss at epoch 45: 0.9070519804954529
[08/27/2025 23:58:49 INFO]: Training loss at epoch 30: 1.2346684634685516
[08/27/2025 23:58:51 INFO]: Training loss at epoch 37: 0.9986677169799805
[08/27/2025 23:58:52 INFO]: Training loss at epoch 1: 1.0317554473876953
[08/27/2025 23:59:02 INFO]: Training loss at epoch 46: 1.0373790562152863
[08/27/2025 23:59:02 INFO]: Training loss at epoch 42: 1.1010108590126038
[08/27/2025 23:59:09 INFO]: Training loss at epoch 43: 1.0145289301872253
[08/27/2025 23:59:17 INFO]: Training loss at epoch 31: 0.917828381061554
[08/27/2025 23:59:18 INFO]: Training loss at epoch 43: 1.0598833560943604
[08/27/2025 23:59:18 INFO]: Training loss at epoch 33: 0.9422872066497803
[08/27/2025 23:59:32 INFO]: Training loss at epoch 47: 1.0401809811592102
[08/27/2025 23:59:48 INFO]: Training loss at epoch 32: 0.975225031375885
[08/28/2025 00:00:00 INFO]: Training loss at epoch 41: 0.9488553106784821
[08/28/2025 00:00:02 INFO]: Training loss at epoch 48: 0.9739724397659302
[08/28/2025 00:00:17 INFO]: Training loss at epoch 33: 1.2491300404071808
[08/28/2025 00:00:24 INFO]: Training loss at epoch 41: 0.9659943580627441
[08/28/2025 00:00:32 INFO]: Training loss at epoch 49: 1.1481214463710785
[08/28/2025 00:00:43 INFO]: Training stats: {
    "score": -1.002852051143938,
    "rmse": 1.002852051143938
}
[08/28/2025 00:00:43 INFO]: Val stats: {
    "score": -0.7057375715221964,
    "rmse": 0.7057375715221964
}
[08/28/2025 00:00:43 INFO]: Test stats: {
    "score": -0.8874249460247994,
    "rmse": 0.8874249460247994
}
[08/28/2025 00:00:44 INFO]: Training loss at epoch 34: 0.9791988134384155
[08/28/2025 00:00:47 INFO]: Running Final Evaluation...
[08/28/2025 00:01:06 INFO]: Training loss at epoch 26: 0.9359771609306335
[08/28/2025 00:01:12 INFO]: Training loss at epoch 35: 0.955001950263977
[08/28/2025 00:01:13 INFO]: Training loss at epoch 50: 0.9217517971992493
[08/28/2025 00:01:15 INFO]: Running Final Evaluation...
[08/28/2025 00:01:25 INFO]: Training accuracy: {
    "score": -1.0039278611986884,
    "rmse": 1.0039278611986884
}
[08/28/2025 00:01:25 INFO]: Val accuracy: {
    "score": -0.667084465843259,
    "rmse": 0.667084465843259
}
[08/28/2025 00:01:25 INFO]: Test accuracy: {
    "score": -0.8679115076440351,
    "rmse": 0.8679115076440351
}
[08/28/2025 00:01:25 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_67",
    "best_epoch": 4,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8679115076440351,
        "rmse": 0.8679115076440351
    },
    "train_stats": {
        "score": -1.0039278611986884,
        "rmse": 1.0039278611986884
    },
    "val_stats": {
        "score": -0.667084465843259,
        "rmse": 0.667084465843259
    }
}
[08/28/2025 00:01:25 INFO]: Procewss finished for trial unhanged-Shanesha_trial_67
[08/28/2025 00:01:26 INFO]: 
_________________________________________________

[08/28/2025 00:01:26 INFO]: train_net_for_optune.py main() running.
[08/28/2025 00:01:26 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 2.007674252320962
  attention_dropout: 0.40269410912887355
  ffn_dropout: 0.40269410912887355
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0005276537430686682
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_70

[08/28/2025 00:01:26 INFO]: This ft_transformer has 12.805 million parameters.
[08/28/2025 00:01:26 INFO]: Training will start at epoch 0.
[08/28/2025 00:01:26 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 00:01:31 INFO]: Training loss at epoch 7: 1.0754024982452393
[08/28/2025 00:01:40 INFO]: Training loss at epoch 39: 1.0466922521591187
[08/28/2025 00:01:43 INFO]: Training loss at epoch 51: 1.140365868806839
[08/28/2025 00:02:12 INFO]: Training loss at epoch 52: 1.0649687945842743
[08/28/2025 00:02:40 INFO]: Training loss at epoch 9: 1.110946774482727
[08/28/2025 00:02:42 INFO]: Training loss at epoch 53: 0.7695841789245605
[08/28/2025 00:03:00 INFO]: Training loss at epoch 26: 1.0535386800765991
[08/28/2025 00:03:13 INFO]: Training stats: {
    "score": -1.002219952807469,
    "rmse": 1.002219952807469
}
[08/28/2025 00:03:13 INFO]: Val stats: {
    "score": -0.6641321300254684,
    "rmse": 0.6641321300254684
}
[08/28/2025 00:03:13 INFO]: Test stats: {
    "score": -0.8680057464105214,
    "rmse": 0.8680057464105214
}
[08/28/2025 00:03:13 INFO]: Training loss at epoch 54: 1.04092139005661
[08/28/2025 00:03:14 INFO]: Training loss at epoch 44: 1.22461599111557
[08/28/2025 00:03:30 INFO]: Training accuracy: {
    "score": -1.0112944813363405,
    "rmse": 1.0112944813363405
}
[08/28/2025 00:03:30 INFO]: Val accuracy: {
    "score": -0.6620122873061627,
    "rmse": 0.6620122873061627
}
[08/28/2025 00:03:30 INFO]: Test accuracy: {
    "score": -0.8697049334146785,
    "rmse": 0.8697049334146785
}
[08/28/2025 00:03:30 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_44",
    "best_epoch": 10,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8697049334146785,
        "rmse": 0.8697049334146785
    },
    "train_stats": {
        "score": -1.0112944813363405,
        "rmse": 1.0112944813363405
    },
    "val_stats": {
        "score": -0.6620122873061627,
        "rmse": 0.6620122873061627
    }
}
[08/28/2025 00:03:30 INFO]: Procewss finished for trial unhanged-Shanesha_trial_44
[08/28/2025 00:03:30 INFO]: 
_________________________________________________

[08/28/2025 00:03:30 INFO]: train_net_for_optune.py main() running.
[08/28/2025 00:03:30 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 4
  d_ffn_factor: 1.9360368680399644
  attention_dropout: 0.3436104295144882
  ffn_dropout: 0.3436104295144882
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0005965487370567503
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_71

[08/28/2025 00:03:30 INFO]: This ft_transformer has 0.303 million parameters.
[08/28/2025 00:03:30 INFO]: Training will start at epoch 0.
[08/28/2025 00:03:30 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 00:03:41 INFO]: Training loss at epoch 0: 1.2130865454673767
[08/28/2025 00:03:43 INFO]: New best epoch, val score: -0.683218688409396
[08/28/2025 00:03:43 INFO]: Saving model to: unhanged-Shanesha_trial_71/model_best.pth
[08/28/2025 00:03:46 INFO]: Training loss at epoch 55: 1.0421954989433289
[08/28/2025 00:03:51 INFO]: Running Final Evaluation...
[08/28/2025 00:03:51 INFO]: Training loss at epoch 42: 1.136111080646515
[08/28/2025 00:03:53 INFO]: Training loss at epoch 1: 1.0998548865318298
[08/28/2025 00:03:54 INFO]: Training loss at epoch 44: 1.2117401957511902
[08/28/2025 00:03:57 INFO]: Training loss at epoch 34: 1.065935730934143
[08/28/2025 00:04:03 INFO]: Training accuracy: {
    "score": -1.0056301908402494,
    "rmse": 1.0056301908402494
}
[08/28/2025 00:04:03 INFO]: Val accuracy: {
    "score": -0.6632795018882222,
    "rmse": 0.6632795018882222
}
[08/28/2025 00:04:03 INFO]: Test accuracy: {
    "score": -0.8694358004645926,
    "rmse": 0.8694358004645926
}
[08/28/2025 00:04:03 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_66",
    "best_epoch": 24,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8694358004645926,
        "rmse": 0.8694358004645926
    },
    "train_stats": {
        "score": -1.0056301908402494,
        "rmse": 1.0056301908402494
    },
    "val_stats": {
        "score": -0.6632795018882222,
        "rmse": 0.6632795018882222
    }
}
[08/28/2025 00:04:03 INFO]: Procewss finished for trial unhanged-Shanesha_trial_66
[08/28/2025 00:04:04 INFO]: 
_________________________________________________

[08/28/2025 00:04:04 INFO]: train_net_for_optune.py main() running.
[08/28/2025 00:04:04 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 4
  d_ffn_factor: 1.9288459208865054
  attention_dropout: 0.3447948762720073
  ffn_dropout: 0.3447948762720073
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 5.5397591628230624e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_72

[08/28/2025 00:04:04 INFO]: This ft_transformer has 0.302 million parameters.
[08/28/2025 00:04:04 INFO]: Training will start at epoch 0.
[08/28/2025 00:04:04 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 00:04:05 INFO]: Training loss at epoch 2: 1.3253107070922852
[08/28/2025 00:04:14 INFO]: Training loss at epoch 0: 1.24384143948555
[08/28/2025 00:04:16 INFO]: New best epoch, val score: -0.8223124072994124
[08/28/2025 00:04:16 INFO]: Saving model to: unhanged-Shanesha_trial_72/model_best.pth
[08/28/2025 00:04:16 INFO]: Training loss at epoch 36: 1.0683494210243225
[08/28/2025 00:04:17 INFO]: Training loss at epoch 3: 0.8501014113426208
[08/28/2025 00:04:26 INFO]: Training loss at epoch 1: 1.1260663270950317
[08/28/2025 00:04:26 INFO]: Training loss at epoch 2: 1.1988708972930908
[08/28/2025 00:04:28 INFO]: New best epoch, val score: -0.7409553496075186
[08/28/2025 00:04:28 INFO]: Saving model to: unhanged-Shanesha_trial_72/model_best.pth
[08/28/2025 00:04:28 INFO]: Training loss at epoch 4: 1.1467689275741577
[08/28/2025 00:04:36 INFO]: Training loss at epoch 77: 0.9433075189590454
[08/28/2025 00:04:38 INFO]: Training loss at epoch 2: 1.1797668933868408
[08/28/2025 00:04:40 INFO]: New best epoch, val score: -0.6702769458941588
[08/28/2025 00:04:40 INFO]: Saving model to: unhanged-Shanesha_trial_72/model_best.pth
[08/28/2025 00:04:41 INFO]: Training loss at epoch 5: 0.9927556812763214
[08/28/2025 00:04:45 INFO]: Training loss at epoch 34: 1.1269904673099518
[08/28/2025 00:04:50 INFO]: Training loss at epoch 3: 1.364302158355713
[08/28/2025 00:04:50 INFO]: Training stats: {
    "score": -1.0126636758522984,
    "rmse": 1.0126636758522984
}
[08/28/2025 00:04:50 INFO]: Val stats: {
    "score": -0.6604620452625619,
    "rmse": 0.6604620452625619
}
[08/28/2025 00:04:50 INFO]: Test stats: {
    "score": -0.870776613637437,
    "rmse": 0.870776613637437
}
[08/28/2025 00:04:52 INFO]: Training loss at epoch 6: 1.0385128259658813
[08/28/2025 00:05:02 INFO]: Training loss at epoch 4: 1.2072932124137878
[08/28/2025 00:05:05 INFO]: Training loss at epoch 7: 1.0815863609313965
[08/28/2025 00:05:07 INFO]: Training loss at epoch 38: 1.2132205367088318
[08/28/2025 00:05:14 INFO]: Training loss at epoch 5: 0.887007474899292
[08/28/2025 00:05:17 INFO]: Training loss at epoch 8: 0.9247194826602936
[08/28/2025 00:05:18 INFO]: New best epoch, val score: -0.6830269438862036
[08/28/2025 00:05:18 INFO]: Saving model to: unhanged-Shanesha_trial_71/model_best.pth
[08/28/2025 00:05:26 INFO]: Training loss at epoch 6: 1.0433568358421326
[08/28/2025 00:05:29 INFO]: Training loss at epoch 9: 1.2239702939987183
[08/28/2025 00:05:33 INFO]: Training stats: {
    "score": -0.9990738011560268,
    "rmse": 0.9990738011560268
}
[08/28/2025 00:05:33 INFO]: Val stats: {
    "score": -0.6821564376944844,
    "rmse": 0.6821564376944844
}
[08/28/2025 00:05:33 INFO]: Test stats: {
    "score": -0.8744066812802536,
    "rmse": 0.8744066812802536
}
[08/28/2025 00:05:34 INFO]: New best epoch, val score: -0.6821564376944844
[08/28/2025 00:05:34 INFO]: Saving model to: unhanged-Shanesha_trial_71/model_best.pth
[08/28/2025 00:05:42 INFO]: New best epoch, val score: -0.6604620452625619
[08/28/2025 00:05:42 INFO]: Saving model to: unhanged-Shanesha_trial_63/model_best.pth
[08/28/2025 00:05:45 INFO]: Training loss at epoch 7: 0.9888864755630493
[08/28/2025 00:05:56 INFO]: Training loss at epoch 38: 1.1371676921844482
[08/28/2025 00:05:57 INFO]: Training loss at epoch 10: 0.7699300050735474
[08/28/2025 00:05:57 INFO]: Training loss at epoch 43: 0.950368344783783
[08/28/2025 00:05:59 INFO]: New best epoch, val score: -0.6756608635357355
[08/28/2025 00:05:59 INFO]: Saving model to: unhanged-Shanesha_trial_71/model_best.pth
[08/28/2025 00:06:05 INFO]: Training loss at epoch 8: 1.261420488357544
[08/28/2025 00:06:11 INFO]: Training loss at epoch 11: 1.237148493528366
[08/28/2025 00:06:12 INFO]: New best epoch, val score: -0.6742201128280189
[08/28/2025 00:06:12 INFO]: Saving model to: unhanged-Shanesha_trial_71/model_best.pth
[08/28/2025 00:06:17 INFO]: Training loss at epoch 9: 0.9881711304187775
[08/28/2025 00:06:21 INFO]: Training stats: {
    "score": -1.0545529842744243,
    "rmse": 1.0545529842744243
}
[08/28/2025 00:06:21 INFO]: Val stats: {
    "score": -0.827857573524931,
    "rmse": 0.827857573524931
}
[08/28/2025 00:06:21 INFO]: Test stats: {
    "score": -0.9664604303356703,
    "rmse": 0.9664604303356703
}
[08/28/2025 00:06:23 INFO]: Training loss at epoch 12: 0.9578344225883484
[08/28/2025 00:06:24 INFO]: New best epoch, val score: -0.6737840296234251
[08/28/2025 00:06:24 INFO]: Saving model to: unhanged-Shanesha_trial_71/model_best.pth
[08/28/2025 00:06:34 INFO]: Training loss at epoch 10: 0.9176386296749115
[08/28/2025 00:06:36 INFO]: Training loss at epoch 13: 0.9002004563808441
[08/28/2025 00:06:37 INFO]: New best epoch, val score: -0.6713987939660906
[08/28/2025 00:06:37 INFO]: Saving model to: unhanged-Shanesha_trial_71/model_best.pth
[08/28/2025 00:06:47 INFO]: Training loss at epoch 0: 1.5505958199501038
[08/28/2025 00:06:48 INFO]: Training loss at epoch 11: 0.9674482643604279
[08/28/2025 00:06:49 INFO]: Training loss at epoch 14: 1.1406453251838684
[08/28/2025 00:06:50 INFO]: New best epoch, val score: -0.6704115803488709
[08/28/2025 00:06:50 INFO]: Saving model to: unhanged-Shanesha_trial_71/model_best.pth
[08/28/2025 00:06:59 INFO]: Training loss at epoch 12: 1.0930192470550537
[08/28/2025 00:07:00 INFO]: Training loss at epoch 15: 0.9554293155670166
[08/28/2025 00:07:01 INFO]: New best epoch, val score: -0.6692655751550932
[08/28/2025 00:07:01 INFO]: Saving model to: unhanged-Shanesha_trial_71/model_best.pth
[08/28/2025 00:07:11 INFO]: Training loss at epoch 13: 0.9876261949539185
[08/28/2025 00:07:12 INFO]: Training loss at epoch 16: 1.0417298674583435
[08/28/2025 00:07:14 INFO]: New best epoch, val score: -0.6681540775977514
[08/28/2025 00:07:14 INFO]: Saving model to: unhanged-Shanesha_trial_71/model_best.pth
[08/28/2025 00:07:23 INFO]: Training loss at epoch 14: 0.8401623666286469
[08/28/2025 00:07:25 INFO]: Training loss at epoch 17: 0.9359830319881439
[08/28/2025 00:07:28 INFO]: Training loss at epoch 42: 0.91797935962677
[08/28/2025 00:07:29 INFO]: New best epoch, val score: -0.9232345087800589
[08/28/2025 00:07:29 INFO]: Saving model to: unhanged-Shanesha_trial_70/model_best.pth
[08/28/2025 00:07:33 INFO]: Training loss at epoch 27: 1.0297435522079468
[08/28/2025 00:07:34 INFO]: Training loss at epoch 15: 1.2754859626293182
[08/28/2025 00:07:36 INFO]: Training loss at epoch 18: 1.150302767753601
[08/28/2025 00:07:45 INFO]: Training loss at epoch 16: 1.0774108171463013
[08/28/2025 00:07:47 INFO]: Training loss at epoch 19: 1.0997994542121887
[08/28/2025 00:07:50 INFO]: Training stats: {
    "score": -1.0009475651159156,
    "rmse": 1.0009475651159156
}
[08/28/2025 00:07:50 INFO]: Val stats: {
    "score": -0.6667465283047861,
    "rmse": 0.6667465283047861
}
[08/28/2025 00:07:50 INFO]: Test stats: {
    "score": -0.8689379539790002,
    "rmse": 0.8689379539790002
}
[08/28/2025 00:07:52 INFO]: New best epoch, val score: -0.6667465283047861
[08/28/2025 00:07:52 INFO]: Saving model to: unhanged-Shanesha_trial_71/model_best.pth
[08/28/2025 00:07:53 INFO]: Training loss at epoch 45: 1.067200779914856
[08/28/2025 00:07:59 INFO]: Training loss at epoch 8: 0.9651285409927368
[08/28/2025 00:07:59 INFO]: Training loss at epoch 17: 1.0236687660217285
[08/28/2025 00:08:01 INFO]: Training loss at epoch 20: 1.0123013257980347
[08/28/2025 00:08:02 INFO]: New best epoch, val score: -0.6654076384344999
[08/28/2025 00:08:02 INFO]: Saving model to: unhanged-Shanesha_trial_71/model_best.pth
[08/28/2025 00:08:10 INFO]: Training loss at epoch 18: 1.1927731037139893
[08/28/2025 00:08:12 INFO]: Training loss at epoch 21: 0.8629753291606903
[08/28/2025 00:08:13 INFO]: New best epoch, val score: -0.664913976058847
[08/28/2025 00:08:13 INFO]: Saving model to: unhanged-Shanesha_trial_71/model_best.pth
[08/28/2025 00:08:16 INFO]: Training loss at epoch 27: 1.578790932893753
[08/28/2025 00:08:19 INFO]: Training loss at epoch 40: 1.1454175114631653
[08/28/2025 00:08:21 INFO]: Training loss at epoch 19: 0.7783130705356598
[08/28/2025 00:08:23 INFO]: Training loss at epoch 22: 0.8218497335910797
[08/28/2025 00:08:25 INFO]: Training stats: {
    "score": -1.0195030118144432,
    "rmse": 1.0195030118144432
}
[08/28/2025 00:08:25 INFO]: Val stats: {
    "score": -0.7541492922857101,
    "rmse": 0.7541492922857101
}
[08/28/2025 00:08:25 INFO]: Test stats: {
    "score": -0.9136923017466416,
    "rmse": 0.9136923017466416
}
[08/28/2025 00:08:34 INFO]: Training loss at epoch 23: 1.1514107584953308
[08/28/2025 00:08:36 INFO]: Training loss at epoch 20: 1.2307189106941223
[08/28/2025 00:08:45 INFO]: Training loss at epoch 24: 0.9162171483039856
[08/28/2025 00:08:46 INFO]: New best epoch, val score: -0.664217541592461
[08/28/2025 00:08:46 INFO]: Saving model to: unhanged-Shanesha_trial_71/model_best.pth
[08/28/2025 00:08:47 INFO]: Training loss at epoch 21: 1.1548463702201843
[08/28/2025 00:08:49 INFO]: New best epoch, val score: -0.6608792087871268
[08/28/2025 00:08:49 INFO]: Saving model to: unhanged-Shanesha_trial_60/model_best.pth
[08/28/2025 00:08:56 INFO]: Training loss at epoch 25: 0.7989276051521301
[08/28/2025 00:08:57 INFO]: New best epoch, val score: -0.662155055762469
[08/28/2025 00:08:57 INFO]: Saving model to: unhanged-Shanesha_trial_71/model_best.pth
[08/28/2025 00:08:58 INFO]: Training loss at epoch 22: 0.9685018062591553
[08/28/2025 00:08:59 INFO]: Training loss at epoch 45: 0.9260960221290588
[08/28/2025 00:09:01 INFO]: Training loss at epoch 35: 0.9650960862636566
[08/28/2025 00:09:07 INFO]: Training loss at epoch 26: 1.1899773478507996
[08/28/2025 00:09:08 INFO]: New best epoch, val score: -0.6609679330182124
[08/28/2025 00:09:08 INFO]: Saving model to: unhanged-Shanesha_trial_71/model_best.pth
[08/28/2025 00:09:09 INFO]: Training loss at epoch 23: 0.9929132461547852
[08/28/2025 00:09:18 INFO]: Training loss at epoch 27: 1.3477358222007751
[08/28/2025 00:09:19 INFO]: New best epoch, val score: -0.6605810358679791
[08/28/2025 00:09:19 INFO]: Saving model to: unhanged-Shanesha_trial_71/model_best.pth
[08/28/2025 00:09:20 INFO]: Training loss at epoch 24: 0.9523655474185944
[08/28/2025 00:09:29 INFO]: Training loss at epoch 28: 0.8408651649951935
[08/28/2025 00:09:31 INFO]: Training loss at epoch 25: 1.1908615231513977
[08/28/2025 00:09:40 INFO]: Training loss at epoch 29: 0.9874357581138611
[08/28/2025 00:09:42 INFO]: Training loss at epoch 26: 0.9484596252441406
[08/28/2025 00:09:43 INFO]: Training stats: {
    "score": -1.0016124282216359,
    "rmse": 1.0016124282216359
}
[08/28/2025 00:09:43 INFO]: Val stats: {
    "score": -0.6652408327975402,
    "rmse": 0.6652408327975402
}
[08/28/2025 00:09:43 INFO]: Test stats: {
    "score": -0.8686140118841692,
    "rmse": 0.8686140118841692
}
[08/28/2025 00:09:52 INFO]: Training loss at epoch 27: 1.2425326108932495
[08/28/2025 00:09:54 INFO]: Training loss at epoch 30: 0.8752288818359375
[08/28/2025 00:10:03 INFO]: Training loss at epoch 28: 0.9897722899913788
[08/28/2025 00:10:05 INFO]: Training loss at epoch 31: 1.2052190899848938
[08/28/2025 00:10:14 INFO]: Training loss at epoch 29: 0.8866727948188782
[08/28/2025 00:10:16 INFO]: Training loss at epoch 32: 0.8766224980354309
[08/28/2025 00:10:17 INFO]: Training stats: {
    "score": -1.0248960442825594,
    "rmse": 1.0248960442825594
}
[08/28/2025 00:10:17 INFO]: Val stats: {
    "score": -0.7675752779245802,
    "rmse": 0.7675752779245802
}
[08/28/2025 00:10:17 INFO]: Test stats: {
    "score": -0.9236735122985066,
    "rmse": 0.9236735122985066
}
[08/28/2025 00:10:21 INFO]: Training loss at epoch 3: 1.1955459713935852
[08/28/2025 00:10:26 INFO]: Training loss at epoch 33: 0.952282577753067
[08/28/2025 00:10:28 INFO]: Training loss at epoch 30: 0.9843584597110748
[08/28/2025 00:10:37 INFO]: Training loss at epoch 34: 1.085376501083374
[08/28/2025 00:10:39 INFO]: Training loss at epoch 31: 1.251522570848465
[08/28/2025 00:10:48 INFO]: Training loss at epoch 35: 1.0742685794830322
[08/28/2025 00:10:50 INFO]: Training loss at epoch 32: 0.9953041076660156
[08/28/2025 00:10:53 INFO]: Training loss at epoch 43: 0.9798532128334045
[08/28/2025 00:10:59 INFO]: Training loss at epoch 36: 0.9031556248664856
[08/28/2025 00:11:03 INFO]: Training loss at epoch 33: 1.030800700187683
[08/28/2025 00:11:04 INFO]: Running Final Evaluation...
[08/28/2025 00:11:08 INFO]: Training accuracy: {
    "score": -1.0382925662554807,
    "rmse": 1.0382925662554807
}
[08/28/2025 00:11:08 INFO]: Val accuracy: {
    "score": -0.6702769458941588,
    "rmse": 0.6702769458941588
}
[08/28/2025 00:11:08 INFO]: Test accuracy: {
    "score": -0.8816972887477337,
    "rmse": 0.8816972887477337
}
[08/28/2025 00:11:08 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_72",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8816972887477337,
        "rmse": 0.8816972887477337
    },
    "train_stats": {
        "score": -1.0382925662554807,
        "rmse": 1.0382925662554807
    },
    "val_stats": {
        "score": -0.6702769458941588,
        "rmse": 0.6702769458941588
    }
}
[08/28/2025 00:11:08 INFO]: Procewss finished for trial unhanged-Shanesha_trial_72
[08/28/2025 00:11:09 INFO]: 
_________________________________________________

[08/28/2025 00:11:09 INFO]: train_net_for_optune.py main() running.
[08/28/2025 00:11:09 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 1.5280930966432817
  attention_dropout: 0.3962213753184539
  ffn_dropout: 0.3962213753184539
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00021327938855824596
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_73

[08/28/2025 00:11:09 INFO]: This ft_transformer has 11.298 million parameters.
[08/28/2025 00:11:09 INFO]: Training will start at epoch 0.
[08/28/2025 00:11:09 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 00:11:09 INFO]: Training loss at epoch 10: 0.9713743627071381
[08/28/2025 00:11:09 INFO]: Training loss at epoch 37: 0.9853094816207886
[08/28/2025 00:11:10 INFO]: Training loss at epoch 37: 0.85333251953125
[08/28/2025 00:11:20 INFO]: Training loss at epoch 38: 1.09599107503891
[08/28/2025 00:11:31 INFO]: Training loss at epoch 39: 0.7782900631427765
[08/28/2025 00:11:35 INFO]: Training stats: {
    "score": -1.0010872890725357,
    "rmse": 1.0010872890725357
}
[08/28/2025 00:11:35 INFO]: Val stats: {
    "score": -0.7071700682850103,
    "rmse": 0.7071700682850103
}
[08/28/2025 00:11:35 INFO]: Test stats: {
    "score": -0.888103634789331,
    "rmse": 0.888103634789331
}
[08/28/2025 00:11:47 INFO]: Training loss at epoch 40: 1.00382861495018
[08/28/2025 00:11:47 INFO]: Training loss at epoch 35: 1.1459320187568665
[08/28/2025 00:11:50 INFO]: Training loss at epoch 78: 0.9285456240177155
[08/28/2025 00:11:59 INFO]: Training loss at epoch 41: 0.9215224385261536
[08/28/2025 00:12:01 INFO]: Training loss at epoch 46: 1.0083160102367401
[08/28/2025 00:12:02 INFO]: Training loss at epoch 39: 1.4371075332164764
[08/28/2025 00:12:10 INFO]: Training loss at epoch 42: 0.9996520280838013
[08/28/2025 00:12:22 INFO]: Training loss at epoch 43: 1.2575892806053162
[08/28/2025 00:12:24 INFO]: Training loss at epoch 1: 5.8063753843307495
[08/28/2025 00:12:33 INFO]: Training loss at epoch 44: 1.0522749423980713
[08/28/2025 00:12:39 INFO]: Training loss at epoch 44: 1.1552192568778992
[08/28/2025 00:12:45 INFO]: Training loss at epoch 45: 0.8756111562252045
[08/28/2025 00:12:48 INFO]: Training loss at epoch 39: 1.174031376838684
[08/28/2025 00:12:56 INFO]: Training loss at epoch 41: 1.1733593344688416
[08/28/2025 00:12:57 INFO]: Training loss at epoch 46: 1.0910266637802124
[08/28/2025 00:12:59 INFO]: Training loss at epoch 28: 0.9862284660339355
[08/28/2025 00:13:08 INFO]: Training loss at epoch 47: 0.8244794607162476
[08/28/2025 00:13:21 INFO]: Training loss at epoch 48: 0.939404159784317
[08/28/2025 00:13:32 INFO]: Training loss at epoch 49: 0.8919158577919006
[08/28/2025 00:13:32 INFO]: Training loss at epoch 28: 0.8291221559047699
[08/28/2025 00:13:37 INFO]: Training stats: {
    "score": -0.9965402924061871,
    "rmse": 0.9965402924061871
}
[08/28/2025 00:13:37 INFO]: Val stats: {
    "score": -0.6850650306101644,
    "rmse": 0.6850650306101644
}
[08/28/2025 00:13:37 INFO]: Test stats: {
    "score": -0.8759859437032238,
    "rmse": 0.8759859437032238
}
[08/28/2025 00:13:44 INFO]: Training loss at epoch 46: 0.8567605912685394
[08/28/2025 00:13:46 INFO]: Training loss at epoch 36: 0.8792744576931
[08/28/2025 00:13:49 INFO]: Training loss at epoch 50: 1.063112884759903
[08/28/2025 00:14:00 INFO]: Training loss at epoch 51: 0.8917441964149475
[08/28/2025 00:14:06 INFO]: Training loss at epoch 9: 1.2493029832839966
[08/28/2025 00:14:12 INFO]: Training loss at epoch 52: 0.8370718359947205
[08/28/2025 00:14:16 INFO]: Training loss at epoch 43: 0.9869700372219086
[08/28/2025 00:14:19 INFO]: Running Final Evaluation...
[08/28/2025 00:14:23 INFO]: Training loss at epoch 53: 0.8827160596847534
[08/28/2025 00:14:29 INFO]: Training stats: {
    "score": -1.001541249461716,
    "rmse": 1.001541249461716
}
[08/28/2025 00:14:29 INFO]: Val stats: {
    "score": -0.6678496354797182,
    "rmse": 0.6678496354797182
}
[08/28/2025 00:14:29 INFO]: Test stats: {
    "score": -0.8692745022441173,
    "rmse": 0.8692745022441173
}
[08/28/2025 00:14:35 INFO]: Training loss at epoch 54: 0.844535768032074
[08/28/2025 00:14:48 INFO]: Training loss at epoch 55: 1.343654453754425
[08/28/2025 00:15:00 INFO]: Training loss at epoch 56: 0.7733656466007233
[08/28/2025 00:15:12 INFO]: Training loss at epoch 57: 0.9730017781257629
[08/28/2025 00:15:18 INFO]: Training stats: {
    "score": -1.000736493143046,
    "rmse": 1.000736493143046
}
[08/28/2025 00:15:18 INFO]: Val stats: {
    "score": -0.6720444234474017,
    "rmse": 0.6720444234474017
}
[08/28/2025 00:15:18 INFO]: Test stats: {
    "score": -0.8704926698314095,
    "rmse": 0.8704926698314095
}
[08/28/2025 00:15:24 INFO]: Training loss at epoch 58: 0.8816006779670715
[08/28/2025 00:15:25 INFO]: Running Final Evaluation...
[08/28/2025 00:15:30 INFO]: Training accuracy: {
    "score": -1.010213504989558,
    "rmse": 1.010213504989558
}
[08/28/2025 00:15:30 INFO]: Val accuracy: {
    "score": -0.6605810358679791,
    "rmse": 0.6605810358679791
}
[08/28/2025 00:15:30 INFO]: Test accuracy: {
    "score": -0.8705523318675505,
    "rmse": 0.8705523318675505
}
[08/28/2025 00:15:30 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_71",
    "best_epoch": 27,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8705523318675505,
        "rmse": 0.8705523318675505
    },
    "train_stats": {
        "score": -1.010213504989558,
        "rmse": 1.010213504989558
    },
    "val_stats": {
        "score": -0.6605810358679791,
        "rmse": 0.6605810358679791
    }
}
[08/28/2025 00:15:30 INFO]: Procewss finished for trial unhanged-Shanesha_trial_71
[08/28/2025 00:15:30 INFO]: 
_________________________________________________

[08/28/2025 00:15:30 INFO]: train_net_for_optune.py main() running.
[08/28/2025 00:15:30 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 1.5641130521428805
  attention_dropout: 0.40454445670877043
  ffn_dropout: 0.40454445670877043
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00020445233939665774
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_74

[08/28/2025 00:15:30 INFO]: This ft_transformer has 11.409 million parameters.
[08/28/2025 00:15:30 INFO]: Training will start at epoch 0.
[08/28/2025 00:15:30 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 00:15:55 INFO]: Training loss at epoch 0: 1.0704855918884277
[08/28/2025 00:16:06 INFO]: Training accuracy: {
    "score": -1.0034103180646619,
    "rmse": 1.0034103180646619
}
[08/28/2025 00:16:06 INFO]: Val accuracy: {
    "score": -0.6632599621130446,
    "rmse": 0.6632599621130446
}
[08/28/2025 00:16:06 INFO]: Test accuracy: {
    "score": -0.8689811117425958,
    "rmse": 0.8689811117425958
}
[08/28/2025 00:16:06 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_54",
    "best_epoch": 15,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8689811117425958,
        "rmse": 0.8689811117425958
    },
    "train_stats": {
        "score": -1.0034103180646619,
        "rmse": 1.0034103180646619
    },
    "val_stats": {
        "score": -0.6632599621130446,
        "rmse": 0.6632599621130446
    }
}
[08/28/2025 00:16:06 INFO]: Procewss finished for trial unhanged-Shanesha_trial_54
[08/28/2025 00:16:06 INFO]: 
_________________________________________________

[08/28/2025 00:16:06 INFO]: train_net_for_optune.py main() running.
[08/28/2025 00:16:06 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 4
  d_ffn_factor: 1.5210474704049148
  attention_dropout: 0.40643192806801126
  ffn_dropout: 0.40643192806801126
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00021941123750741546
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_75

[08/28/2025 00:16:07 INFO]: This ft_transformer has 11.274 million parameters.
[08/28/2025 00:16:07 INFO]: Training will start at epoch 0.
[08/28/2025 00:16:07 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 00:16:14 INFO]: Training loss at epoch 4: 1.347359836101532
[08/28/2025 00:16:16 INFO]: Training stats: {
    "score": -1.0169411531662031,
    "rmse": 1.0169411531662031
}
[08/28/2025 00:16:16 INFO]: Val stats: {
    "score": -0.7471048447015691,
    "rmse": 0.7471048447015691
}
[08/28/2025 00:16:16 INFO]: Test stats: {
    "score": -0.9135646792181223,
    "rmse": 0.9135646792181223
}
[08/28/2025 00:16:29 INFO]: Training loss at epoch 47: 0.9100454151630402
[08/28/2025 00:16:33 INFO]: New best epoch, val score: -0.6744335858429534
[08/28/2025 00:16:33 INFO]: Saving model to: unhanged-Shanesha_trial_73/model_best.pth
[08/28/2025 00:17:24 INFO]: Training loss at epoch 11: 0.8497492372989655
[08/28/2025 00:17:45 INFO]: Training loss at epoch 42: 1.0248465240001678
[08/28/2025 00:17:52 INFO]: Training loss at epoch 44: 1.050454556941986
[08/28/2025 00:17:54 INFO]: Training loss at epoch 29: 0.8672365546226501
[08/28/2025 00:18:01 INFO]: Training loss at epoch 38: 1.2445203065872192
[08/28/2025 00:18:15 INFO]: Training loss at epoch 2: 2.8854283690452576
[08/28/2025 00:18:31 INFO]: Training loss at epoch 37: 0.925838828086853
[08/28/2025 00:18:45 INFO]: Training loss at epoch 36: 1.015893816947937
[08/28/2025 00:19:00 INFO]: Training loss at epoch 79: 1.0670810043811798
[08/28/2025 00:19:27 INFO]: Training stats: {
    "score": -0.9966219772035821,
    "rmse": 0.9966219772035821
}
[08/28/2025 00:19:27 INFO]: Val stats: {
    "score": -0.68681488700876,
    "rmse": 0.68681488700876
}
[08/28/2025 00:19:27 INFO]: Test stats: {
    "score": -0.8767655489765546,
    "rmse": 0.8767655489765546
}
[08/28/2025 00:19:28 INFO]: Training loss at epoch 45: 0.8455027937889099
[08/28/2025 00:19:35 INFO]: Training loss at epoch 29: 1.067922830581665
[08/28/2025 00:19:56 INFO]: Training loss at epoch 0: 0.9584895670413971
[08/28/2025 00:20:28 INFO]: Training loss at epoch 0: 1.236825704574585
[08/28/2025 00:20:31 INFO]: New best epoch, val score: -0.6742209095585245
[08/28/2025 00:20:31 INFO]: Saving model to: unhanged-Shanesha_trial_74/model_best.pth
[08/28/2025 00:20:32 INFO]: Training loss at epoch 48: 0.8441382348537445
[08/28/2025 00:20:58 INFO]: Training loss at epoch 44: 0.9384480118751526
[08/28/2025 00:20:58 INFO]: Training loss at epoch 1: 1.5003549456596375
[08/28/2025 00:21:06 INFO]: New best epoch, val score: -0.794614461810202
[08/28/2025 00:21:06 INFO]: Saving model to: unhanged-Shanesha_trial_75/model_best.pth
[08/28/2025 00:21:10 INFO]: Training loss at epoch 40: 1.081544578075409
[08/28/2025 00:21:19 INFO]: Training stats: {
    "score": -0.987498309041087,
    "rmse": 0.987498309041087
}
[08/28/2025 00:21:19 INFO]: Val stats: {
    "score": -0.6563948635138408,
    "rmse": 0.6563948635138408
}
[08/28/2025 00:21:19 INFO]: Test stats: {
    "score": -0.8814482005060698,
    "rmse": 0.8814482005060698
}
[08/28/2025 00:21:36 INFO]: New best epoch, val score: -0.6660042567939077
[08/28/2025 00:21:36 INFO]: Saving model to: unhanged-Shanesha_trial_73/model_best.pth
[08/28/2025 00:21:37 INFO]: Training stats: {
    "score": -1.0045641517255037,
    "rmse": 1.0045641517255037
}
[08/28/2025 00:21:37 INFO]: Val stats: {
    "score": -0.7102749620237832,
    "rmse": 0.7102749620237832
}
[08/28/2025 00:21:37 INFO]: Test stats: {
    "score": -0.8901869008904533,
    "rmse": 0.8901869008904533
}
[08/28/2025 00:21:44 INFO]: Training loss at epoch 5: 0.9121757745742798
[08/28/2025 00:22:05 INFO]: Training loss at epoch 40: 0.9472161531448364
[08/28/2025 00:22:12 INFO]: New best epoch, val score: -0.6563948635138408
[08/28/2025 00:22:12 INFO]: Saving model to: unhanged-Shanesha_trial_20/model_best.pth
[08/28/2025 00:22:15 INFO]: Training loss at epoch 10: 0.936202883720398
[08/28/2025 00:22:24 INFO]: Training loss at epoch 43: 1.0714012384414673
[08/28/2025 00:23:19 INFO]: Training loss at epoch 38: 1.1581249237060547
[08/28/2025 00:23:28 INFO]: Training loss at epoch 12: 1.1286386251449585
[08/28/2025 00:23:55 INFO]: Running Final Evaluation...
[08/28/2025 00:24:04 INFO]: Training loss at epoch 3: 1.98030686378479
[08/28/2025 00:24:26 INFO]: Training loss at epoch 30: 1.1659120917320251
[08/28/2025 00:24:47 INFO]: Training loss at epoch 45: 0.9273107647895813
[08/28/2025 00:24:48 INFO]: New best epoch, val score: -0.7643877855743372
[08/28/2025 00:24:48 INFO]: Saving model to: unhanged-Shanesha_trial_70/model_best.pth
[08/28/2025 00:24:51 INFO]: Training loss at epoch 39: 1.055273175239563
[08/28/2025 00:25:03 INFO]: Training loss at epoch 49: 0.9023780822753906
[08/28/2025 00:25:23 INFO]: Training loss at epoch 1: 1.293847769498825
[08/28/2025 00:25:40 INFO]: Training accuracy: {
    "score": -1.0126580463397528,
    "rmse": 1.0126580463397528
}
[08/28/2025 00:25:40 INFO]: Val accuracy: {
    "score": -0.6603473216874208,
    "rmse": 0.6603473216874208
}
[08/28/2025 00:25:40 INFO]: Test accuracy: {
    "score": -0.8707262203094743,
    "rmse": 0.8707262203094743
}
[08/28/2025 00:25:40 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_58",
    "best_epoch": 7,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8707262203094743,
        "rmse": 0.8707262203094743
    },
    "train_stats": {
        "score": -1.0126580463397528,
        "rmse": 1.0126580463397528
    },
    "val_stats": {
        "score": -0.6603473216874208,
        "rmse": 0.6603473216874208
    }
}
[08/28/2025 00:25:40 INFO]: Procewss finished for trial unhanged-Shanesha_trial_58
[08/28/2025 00:25:40 INFO]: 
_________________________________________________

[08/28/2025 00:25:40 INFO]: train_net_for_optune.py main() running.
[08/28/2025 00:25:40 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 1.2357344204845573
  attention_dropout: 0.4047256554763903
  ffn_dropout: 0.4047256554763903
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00014210292555088617
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_76

[08/28/2025 00:25:40 INFO]: This ft_transformer has 0.472 million parameters.
[08/28/2025 00:25:40 INFO]: Training will start at epoch 0.
[08/28/2025 00:25:40 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 00:25:45 INFO]: Training loss at epoch 37: 0.9975458979606628
[08/28/2025 00:25:54 INFO]: Training loss at epoch 1: 3.233578324317932
[08/28/2025 00:26:04 INFO]: Training loss at epoch 0: 1.3294594287872314
[08/28/2025 00:26:07 INFO]: New best epoch, val score: -0.9461863760292291
[08/28/2025 00:26:07 INFO]: Saving model to: unhanged-Shanesha_trial_76/model_best.pth
[08/28/2025 00:26:21 INFO]: Training loss at epoch 46: 1.0208159685134888
[08/28/2025 00:26:22 INFO]: Training loss at epoch 2: 1.2125422358512878
[08/28/2025 00:26:27 INFO]: Training stats: {
    "score": -1.0004997562452023,
    "rmse": 1.0004997562452023
}
[08/28/2025 00:26:27 INFO]: Val stats: {
    "score": -0.7105608810255123,
    "rmse": 0.7105608810255123
}
[08/28/2025 00:26:27 INFO]: Test stats: {
    "score": -0.8921300824502998,
    "rmse": 0.8921300824502998
}
[08/28/2025 00:26:31 INFO]: Training loss at epoch 1: 1.2693804502487183
[08/28/2025 00:26:35 INFO]: New best epoch, val score: -0.6766444345111615
[08/28/2025 00:26:35 INFO]: Saving model to: unhanged-Shanesha_trial_76/model_best.pth
[08/28/2025 00:26:56 INFO]: Running Final Evaluation...
[08/28/2025 00:26:59 INFO]: Training loss at epoch 2: 1.0626497864723206
[08/28/2025 00:27:06 INFO]: Training stats: {
    "score": -1.0012371649513887,
    "rmse": 1.0012371649513887
}
[08/28/2025 00:27:06 INFO]: Val stats: {
    "score": -0.6996093593585575,
    "rmse": 0.6996093593585575
}
[08/28/2025 00:27:06 INFO]: Test stats: {
    "score": -0.8834351785911064,
    "rmse": 0.8834351785911064
}
[08/28/2025 00:27:12 INFO]: Training loss at epoch 44: 1.03671133518219
[08/28/2025 00:27:26 INFO]: Training loss at epoch 3: 1.2426663041114807
[08/28/2025 00:27:31 INFO]: Training loss at epoch 6: 0.7664762586355209
[08/28/2025 00:27:45 INFO]: Training loss at epoch 30: 1.0906589031219482
[08/28/2025 00:27:53 INFO]: Training loss at epoch 4: 1.233297884464264
[08/28/2025 00:27:53 INFO]: Training loss at epoch 45: 0.8006380200386047
[08/28/2025 00:28:01 INFO]: Training loss at epoch 41: 0.9895824491977692
[08/28/2025 00:28:20 INFO]: Training loss at epoch 5: 1.0173579156398773
[08/28/2025 00:28:21 INFO]: Training accuracy: {
    "score": -1.0142894527941295,
    "rmse": 1.0142894527941295
}
[08/28/2025 00:28:21 INFO]: Val accuracy: {
    "score": -0.6601165896267582,
    "rmse": 0.6601165896267582
}
[08/28/2025 00:28:21 INFO]: Test accuracy: {
    "score": -0.8741962545217591,
    "rmse": 0.8741962545217591
}
[08/28/2025 00:28:21 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_55",
    "best_epoch": 18,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8741962545217591,
        "rmse": 0.8741962545217591
    },
    "train_stats": {
        "score": -1.0142894527941295,
        "rmse": 1.0142894527941295
    },
    "val_stats": {
        "score": -0.6601165896267582,
        "rmse": 0.6601165896267582
    }
}
[08/28/2025 00:28:21 INFO]: Procewss finished for trial unhanged-Shanesha_trial_55
[08/28/2025 00:28:21 INFO]: 
_________________________________________________

[08/28/2025 00:28:21 INFO]: train_net_for_optune.py main() running.
[08/28/2025 00:28:21 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 1.531547352664348
  attention_dropout: 0.42107148767869995
  ffn_dropout: 0.42107148767869995
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0002095510396840672
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_77

[08/28/2025 00:28:21 INFO]: This ft_transformer has 0.511 million parameters.
[08/28/2025 00:28:21 INFO]: Training will start at epoch 0.
[08/28/2025 00:28:21 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 00:28:23 INFO]: Training loss at epoch 11: 1.183838427066803
[08/28/2025 00:28:30 INFO]: Training loss at epoch 80: 0.8004773259162903
[08/28/2025 00:28:46 INFO]: Training loss at epoch 0: 1.6838011145591736
[08/28/2025 00:28:47 INFO]: Training loss at epoch 6: 1.012683480978012
[08/28/2025 00:28:49 INFO]: New best epoch, val score: -0.762282396381263
[08/28/2025 00:28:49 INFO]: Saving model to: unhanged-Shanesha_trial_77/model_best.pth
[08/28/2025 00:28:58 INFO]: Training loss at epoch 41: 0.9876652956008911
[08/28/2025 00:29:06 INFO]: Training loss at epoch 31: 0.9928839206695557
[08/28/2025 00:29:14 INFO]: Training loss at epoch 1: 1.148272693157196
[08/28/2025 00:29:14 INFO]: Training loss at epoch 7: 1.2664701342582703
[08/28/2025 00:29:17 INFO]: New best epoch, val score: -0.6563494493117326
[08/28/2025 00:29:17 INFO]: Saving model to: unhanged-Shanesha_trial_20/model_best.pth
[08/28/2025 00:29:18 INFO]: New best epoch, val score: -0.732011558638251
[08/28/2025 00:29:18 INFO]: Saving model to: unhanged-Shanesha_trial_77/model_best.pth
[08/28/2025 00:29:18 INFO]: New best epoch, val score: -0.6749528260565762
[08/28/2025 00:29:18 INFO]: Saving model to: unhanged-Shanesha_trial_76/model_best.pth
[08/28/2025 00:29:28 INFO]: Training loss at epoch 13: 0.8474113643169403
[08/28/2025 00:29:41 INFO]: Training loss at epoch 4: 1.3300184607505798
[08/28/2025 00:29:42 INFO]: Training loss at epoch 8: 1.0201641023159027
[08/28/2025 00:29:42 INFO]: Training loss at epoch 2: 1.2860034704208374
[08/28/2025 00:29:45 INFO]: New best epoch, val score: -0.6728284122483859
[08/28/2025 00:29:45 INFO]: Saving model to: unhanged-Shanesha_trial_76/model_best.pth
[08/28/2025 00:30:10 INFO]: Training loss at epoch 9: 0.869197815656662
[08/28/2025 00:30:11 INFO]: Training loss at epoch 3: 1.4213503003120422
[08/28/2025 00:30:21 INFO]: Training stats: {
    "score": -1.0329170472377598,
    "rmse": 1.0329170472377598
}
[08/28/2025 00:30:21 INFO]: Val stats: {
    "score": -0.670839682822251,
    "rmse": 0.670839682822251
}
[08/28/2025 00:30:21 INFO]: Test stats: {
    "score": -0.8850190146081466,
    "rmse": 0.8850190146081466
}
[08/28/2025 00:30:24 INFO]: Training loss at epoch 2: 1.1308560371398926
[08/28/2025 00:30:25 INFO]: New best epoch, val score: -0.670839682822251
[08/28/2025 00:30:25 INFO]: Saving model to: unhanged-Shanesha_trial_76/model_best.pth
[08/28/2025 00:30:43 INFO]: Training loss at epoch 4: 0.9419777095317841
[08/28/2025 00:30:46 INFO]: New best epoch, val score: -0.7258409613270752
[08/28/2025 00:30:46 INFO]: Saving model to: unhanged-Shanesha_trial_77/model_best.pth
[08/28/2025 00:30:51 INFO]: Training loss at epoch 10: 1.076592206954956
[08/28/2025 00:30:54 INFO]: New best epoch, val score: -0.6667278745232128
[08/28/2025 00:30:54 INFO]: Saving model to: unhanged-Shanesha_trial_76/model_best.pth
[08/28/2025 00:30:55 INFO]: Training loss at epoch 2: 1.9626476764678955
[08/28/2025 00:31:14 INFO]: Training loss at epoch 5: 1.0697882175445557
[08/28/2025 00:31:17 INFO]: New best epoch, val score: -0.7230484292365265
[08/28/2025 00:31:17 INFO]: Saving model to: unhanged-Shanesha_trial_77/model_best.pth
[08/28/2025 00:31:21 INFO]: Training loss at epoch 11: 1.2157931327819824
[08/28/2025 00:31:27 INFO]: Training loss at epoch 3: 1.5680163502693176
[08/28/2025 00:31:30 INFO]: Training loss at epoch 46: 0.9027973711490631
[08/28/2025 00:31:44 INFO]: Training loss at epoch 6: 0.9502138495445251
[08/28/2025 00:31:48 INFO]: New best epoch, val score: -0.719955320825082
[08/28/2025 00:31:48 INFO]: Saving model to: unhanged-Shanesha_trial_77/model_best.pth
[08/28/2025 00:31:50 INFO]: Training loss at epoch 12: 1.027321994304657
[08/28/2025 00:31:53 INFO]: Training loss at epoch 45: 0.959223747253418
[08/28/2025 00:32:15 INFO]: Training loss at epoch 7: 0.8900227546691895
[08/28/2025 00:32:19 INFO]: New best epoch, val score: -0.7168415458161831
[08/28/2025 00:32:19 INFO]: Saving model to: unhanged-Shanesha_trial_77/model_best.pth
[08/28/2025 00:32:20 INFO]: Training loss at epoch 13: 1.1454541385173798
[08/28/2025 00:32:35 INFO]: Training loss at epoch 38: 0.7937096953392029
[08/28/2025 00:32:46 INFO]: Training loss at epoch 8: 0.9514457285404205
[08/28/2025 00:32:50 INFO]: New best epoch, val score: -0.7140304748440344
[08/28/2025 00:32:50 INFO]: Saving model to: unhanged-Shanesha_trial_77/model_best.pth
[08/28/2025 00:32:51 INFO]: Training loss at epoch 14: 1.0169292092323303
[08/28/2025 00:33:08 INFO]: Training loss at epoch 47: 0.8605993390083313
[08/28/2025 00:33:17 INFO]: Training loss at epoch 9: 1.4143743515014648
[08/28/2025 00:33:20 INFO]: Training loss at epoch 7: 0.9507739841938019
[08/28/2025 00:33:20 INFO]: Training loss at epoch 15: 1.0793949365615845
[08/28/2025 00:33:28 INFO]: Training stats: {
    "score": -1.0038390137438873,
    "rmse": 1.0038390137438873
}
[08/28/2025 00:33:28 INFO]: Val stats: {
    "score": -0.711329192415659,
    "rmse": 0.711329192415659
}
[08/28/2025 00:33:28 INFO]: Test stats: {
    "score": -0.8907551691437859,
    "rmse": 0.8907551691437859
}
[08/28/2025 00:33:31 INFO]: New best epoch, val score: -0.711329192415659
[08/28/2025 00:33:31 INFO]: Saving model to: unhanged-Shanesha_trial_77/model_best.pth
[08/28/2025 00:33:50 INFO]: Training loss at epoch 16: 0.9837964177131653
[08/28/2025 00:33:56 INFO]: Training loss at epoch 40: 0.8960268795490265
[08/28/2025 00:33:57 INFO]: Training loss at epoch 31: 1.2489096522331238
[08/28/2025 00:33:58 INFO]: Training loss at epoch 10: 0.9816774725914001
[08/28/2025 00:34:01 INFO]: New best epoch, val score: -0.6864390350648971
[08/28/2025 00:34:01 INFO]: Saving model to: unhanged-Shanesha_trial_77/model_best.pth
[08/28/2025 00:34:06 INFO]: Training loss at epoch 32: 1.0219811499118805
[08/28/2025 00:34:19 INFO]: Training loss at epoch 17: 1.1706389784812927
[08/28/2025 00:34:28 INFO]: Training loss at epoch 11: 0.986522912979126
[08/28/2025 00:34:32 INFO]: New best epoch, val score: -0.6697028533485408
[08/28/2025 00:34:32 INFO]: Saving model to: unhanged-Shanesha_trial_77/model_best.pth
[08/28/2025 00:34:40 INFO]: Training loss at epoch 12: 1.0534714460372925
[08/28/2025 00:34:46 INFO]: Training loss at epoch 18: 1.0752944350242615
[08/28/2025 00:34:52 INFO]: Training loss at epoch 46: 0.9993861615657806
[08/28/2025 00:34:56 INFO]: Training loss at epoch 42: 0.8604466319084167
[08/28/2025 00:34:57 INFO]: Training loss at epoch 12: 0.9595274329185486
[08/28/2025 00:35:00 INFO]: New best epoch, val score: -0.662677196564689
[08/28/2025 00:35:00 INFO]: Saving model to: unhanged-Shanesha_trial_77/model_best.pth
[08/28/2025 00:35:14 INFO]: Training loss at epoch 19: 1.3708623349666595
[08/28/2025 00:35:23 INFO]: Training stats: {
    "score": -1.001798836045962,
    "rmse": 1.001798836045962
}
[08/28/2025 00:35:23 INFO]: Val stats: {
    "score": -0.7076988209693613,
    "rmse": 0.7076988209693613
}
[08/28/2025 00:35:23 INFO]: Test stats: {
    "score": -0.8862827970914321,
    "rmse": 0.8862827970914321
}
[08/28/2025 00:35:25 INFO]: Training loss at epoch 13: 0.9524589478969574
[08/28/2025 00:35:28 INFO]: New best epoch, val score: -0.6612831009595397
[08/28/2025 00:35:28 INFO]: Saving model to: unhanged-Shanesha_trial_77/model_best.pth
[08/28/2025 00:35:38 INFO]: Training loss at epoch 5: 1.4198978543281555
[08/28/2025 00:35:43 INFO]: Training loss at epoch 81: 1.047648847103119
[08/28/2025 00:35:46 INFO]: Training loss at epoch 14: 0.8866290748119354
[08/28/2025 00:35:47 INFO]: Training loss at epoch 3: 0.8937986195087433
[08/28/2025 00:35:50 INFO]: Training loss at epoch 20: 1.1181998252868652
[08/28/2025 00:35:53 INFO]: Training loss at epoch 14: 1.0996107459068298
[08/28/2025 00:35:59 INFO]: Training loss at epoch 42: 1.1496191024780273
[08/28/2025 00:36:14 INFO]: Training loss at epoch 3: 1.3937942385673523
[08/28/2025 00:36:17 INFO]: Training loss at epoch 21: 1.0279740989208221
[08/28/2025 00:36:21 INFO]: Training loss at epoch 15: 1.0399256944656372
[08/28/2025 00:36:39 INFO]: Training loss at epoch 46: 1.1022071242332458
[08/28/2025 00:36:43 INFO]: Training loss at epoch 4: 1.1725029945373535
[08/28/2025 00:36:44 INFO]: Training loss at epoch 22: 1.1441576480865479
[08/28/2025 00:36:48 INFO]: New best epoch, val score: -0.6643330719178246
[08/28/2025 00:36:48 INFO]: Saving model to: unhanged-Shanesha_trial_76/model_best.pth
[08/28/2025 00:36:48 INFO]: New best epoch, val score: -0.6673365066481496
[08/28/2025 00:36:48 INFO]: Saving model to: unhanged-Shanesha_trial_75/model_best.pth
[08/28/2025 00:36:50 INFO]: Training loss at epoch 16: 0.8326303362846375
[08/28/2025 00:37:12 INFO]: Training loss at epoch 23: 0.9446492493152618
[08/28/2025 00:37:16 INFO]: New best epoch, val score: -0.6619987170304031
[08/28/2025 00:37:16 INFO]: Saving model to: unhanged-Shanesha_trial_76/model_best.pth
[08/28/2025 00:37:18 INFO]: Training loss at epoch 17: 1.0601821541786194
[08/28/2025 00:37:40 INFO]: Training loss at epoch 24: 1.0283815562725067
[08/28/2025 00:37:43 INFO]: New best epoch, val score: -0.6617062293100546
[08/28/2025 00:37:43 INFO]: Saving model to: unhanged-Shanesha_trial_76/model_best.pth
[08/28/2025 00:37:46 INFO]: Training loss at epoch 18: 1.115158349275589
[08/28/2025 00:38:07 INFO]: Training loss at epoch 25: 1.1621492505073547
[08/28/2025 00:38:14 INFO]: Training loss at epoch 19: 1.090259611606598
[08/28/2025 00:38:20 INFO]: Training loss at epoch 47: 1.0671800076961517
[08/28/2025 00:38:24 INFO]: Training stats: {
    "score": -1.0001785204306852,
    "rmse": 1.0001785204306852
}
[08/28/2025 00:38:24 INFO]: Val stats: {
    "score": -0.6716848166737647,
    "rmse": 0.6716848166737647
}
[08/28/2025 00:38:24 INFO]: Test stats: {
    "score": -0.8702025786936054,
    "rmse": 0.8702025786936054
}
[08/28/2025 00:38:34 INFO]: Training loss at epoch 26: 1.101819097995758
[08/28/2025 00:38:42 INFO]: Training loss at epoch 33: 1.1245314478874207
[08/28/2025 00:38:52 INFO]: Training loss at epoch 20: 0.9773003458976746
[08/28/2025 00:38:53 INFO]: Training loss at epoch 8: 1.1007917523384094
[08/28/2025 00:39:02 INFO]: Training loss at epoch 27: 0.9855001270771027
[08/28/2025 00:39:21 INFO]: Training loss at epoch 39: 1.0038450062274933
[08/28/2025 00:39:22 INFO]: Training loss at epoch 21: 0.891767144203186
[08/28/2025 00:39:32 INFO]: Training loss at epoch 28: 0.8615517020225525
[08/28/2025 00:39:48 INFO]: Training loss at epoch 48: 1.1594831347465515
[08/28/2025 00:39:52 INFO]: Training loss at epoch 32: 0.9558559358119965
[08/28/2025 00:39:52 INFO]: Training loss at epoch 22: 0.9573636949062347
[08/28/2025 00:40:02 INFO]: Training loss at epoch 29: 1.1512187719345093
[08/28/2025 00:40:13 INFO]: Training stats: {
    "score": -1.0022861314973137,
    "rmse": 1.0022861314973137
}
[08/28/2025 00:40:13 INFO]: Val stats: {
    "score": -0.6659621762566013,
    "rmse": 0.6659621762566013
}
[08/28/2025 00:40:13 INFO]: Test stats: {
    "score": -0.8689353162721406,
    "rmse": 0.8689353162721406
}
[08/28/2025 00:40:23 INFO]: Training loss at epoch 23: 0.9763414859771729
[08/28/2025 00:40:35 INFO]: Training loss at epoch 41: 1.0904434323310852
[08/28/2025 00:40:40 INFO]: Training loss at epoch 13: 1.4739928841590881
[08/28/2025 00:40:42 INFO]: Training loss at epoch 30: 0.9171042144298553
[08/28/2025 00:40:54 INFO]: Training loss at epoch 24: 1.3503062725067139
[08/28/2025 00:40:58 INFO]: Training loss at epoch 4: 1.5531351268291473
[08/28/2025 00:41:12 INFO]: Training loss at epoch 31: 0.9155316948890686
[08/28/2025 00:41:24 INFO]: Training loss at epoch 6: 1.3671958446502686
[08/28/2025 00:41:25 INFO]: Training loss at epoch 25: 1.2718909680843353
[08/28/2025 00:41:25 INFO]: Training loss at epoch 47: 0.8659386038780212
[08/28/2025 00:41:29 INFO]: Training loss at epoch 4: 1.2536851167678833
[08/28/2025 00:41:38 INFO]: Training loss at epoch 47: 1.0414593815803528
[08/28/2025 00:41:38 INFO]: Training loss at epoch 43: 0.9941416382789612
[08/28/2025 00:41:42 INFO]: Training loss at epoch 32: 1.0513008832931519
[08/28/2025 00:41:51 INFO]: Training stats: {
    "score": -0.9994239548405895,
    "rmse": 0.9994239548405895
}
[08/28/2025 00:41:51 INFO]: Val stats: {
    "score": -0.6951975512709454,
    "rmse": 0.6951975512709454
}
[08/28/2025 00:41:51 INFO]: Test stats: {
    "score": -0.8813327825337788,
    "rmse": 0.8813327825337788
}
[08/28/2025 00:41:54 INFO]: Training loss at epoch 15: 0.9978760182857513
[08/28/2025 00:41:56 INFO]: Training loss at epoch 26: 1.0202295780181885
[08/28/2025 00:42:00 INFO]: Training loss at epoch 5: 1.1176834106445312
[08/28/2025 00:42:12 INFO]: Training loss at epoch 33: 0.8180395066738129
[08/28/2025 00:42:26 INFO]: Training loss at epoch 27: 1.0224049985408783
[08/28/2025 00:42:42 INFO]: Training loss at epoch 34: 0.9625345468521118
[08/28/2025 00:42:53 INFO]: Training loss at epoch 82: 0.8796780407428741
[08/28/2025 00:42:57 INFO]: Training loss at epoch 28: 1.3147911727428436
[08/28/2025 00:43:00 INFO]: Training loss at epoch 43: 0.9851103723049164
[08/28/2025 00:43:12 INFO]: Training loss at epoch 35: 1.0583534836769104
[08/28/2025 00:43:28 INFO]: Training loss at epoch 29: 1.1192731857299805
[08/28/2025 00:43:39 INFO]: Training stats: {
    "score": -1.0066316001967655,
    "rmse": 1.0066316001967655
}
[08/28/2025 00:43:39 INFO]: Val stats: {
    "score": -0.6624453404026648,
    "rmse": 0.6624453404026648
}
[08/28/2025 00:43:39 INFO]: Test stats: {
    "score": -0.8694205544863811,
    "rmse": 0.8694205544863811
}
[08/28/2025 00:43:41 INFO]: Training loss at epoch 36: 0.9619396924972534
[08/28/2025 00:43:46 INFO]: Training loss at epoch 34: 0.874791145324707
[08/28/2025 00:44:10 INFO]: Training loss at epoch 30: 1.0211988985538483
[08/28/2025 00:44:11 INFO]: Training loss at epoch 37: 1.112290859222412
[08/28/2025 00:44:38 INFO]: Training loss at epoch 31: 1.165400892496109
[08/28/2025 00:44:38 INFO]: Training loss at epoch 38: 1.2869160771369934
[08/28/2025 00:44:52 INFO]: Training loss at epoch 9: 0.8095195591449738
[08/28/2025 00:45:05 INFO]: Training loss at epoch 39: 0.9605154991149902
[08/28/2025 00:45:06 INFO]: Training loss at epoch 32: 1.025856077671051
[08/28/2025 00:45:15 INFO]: Training stats: {
    "score": -1.0002691005638675,
    "rmse": 1.0002691005638675
}
[08/28/2025 00:45:15 INFO]: Val stats: {
    "score": -0.7015880989161549,
    "rmse": 0.7015880989161549
}
[08/28/2025 00:45:15 INFO]: Test stats: {
    "score": -0.8843028584016578,
    "rmse": 0.8843028584016578
}
[08/28/2025 00:45:24 INFO]: Training loss at epoch 48: 0.986522376537323
[08/28/2025 00:45:33 INFO]: Training loss at epoch 33: 1.0709048509597778
[08/28/2025 00:45:42 INFO]: Training loss at epoch 40: 0.9675793051719666
[08/28/2025 00:46:01 INFO]: Training loss at epoch 34: 0.9760236144065857
[08/28/2025 00:46:06 INFO]: Training loss at epoch 33: 0.9128299951553345
[08/28/2025 00:46:09 INFO]: Training loss at epoch 41: 1.0879388451576233
[08/28/2025 00:46:12 INFO]: Training loss at epoch 48: 1.1340596675872803
[08/28/2025 00:46:17 INFO]: Training loss at epoch 5: 1.1688750982284546
[08/28/2025 00:46:29 INFO]: Training loss at epoch 35: 1.1169338822364807
[08/28/2025 00:46:36 INFO]: Training loss at epoch 42: 1.0872767567634583
[08/28/2025 00:46:41 INFO]: Training stats: {
    "score": -1.0328834562822247,
    "rmse": 1.0328834562822247
}
[08/28/2025 00:46:41 INFO]: Val stats: {
    "score": -0.7877828005531992,
    "rmse": 0.7877828005531992
}
[08/28/2025 00:46:41 INFO]: Test stats: {
    "score": -0.939817873582699,
    "rmse": 0.939817873582699
}
[08/28/2025 00:46:42 INFO]: Training loss at epoch 5: 1.6047726273536682
[08/28/2025 00:46:44 INFO]: Training loss at epoch 49: 0.8149510025978088
[08/28/2025 00:46:54 INFO]: Training loss at epoch 14: 1.1090545058250427
[08/28/2025 00:46:57 INFO]: Training loss at epoch 36: 1.0120890140533447
[08/28/2025 00:47:04 INFO]: Training loss at epoch 43: 0.9718129336833954
[08/28/2025 00:47:10 INFO]: Training loss at epoch 6: 1.2359825372695923
[08/28/2025 00:47:10 INFO]: Training loss at epoch 7: 1.3124498128890991
[08/28/2025 00:47:24 INFO]: Training loss at epoch 42: 1.0491477251052856
[08/28/2025 00:47:25 INFO]: Training loss at epoch 37: 0.9706084132194519
[08/28/2025 00:47:32 INFO]: Training loss at epoch 44: 0.8429783880710602
[08/28/2025 00:47:53 INFO]: Training loss at epoch 38: 1.120703935623169
[08/28/2025 00:47:57 INFO]: Training loss at epoch 16: 1.078879565000534
[08/28/2025 00:48:00 INFO]: Training loss at epoch 45: 1.178515076637268
[08/28/2025 00:48:22 INFO]: Training loss at epoch 39: 1.0068288147449493
[08/28/2025 00:48:22 INFO]: Training loss at epoch 44: 1.0882798433303833
[08/28/2025 00:48:23 INFO]: Training loss at epoch 35: 0.9702029228210449
[08/28/2025 00:48:24 INFO]: Training loss at epoch 48: 0.9208760261535645
[08/28/2025 00:48:27 INFO]: Training loss at epoch 46: 0.9546078443527222
[08/28/2025 00:48:31 INFO]: Training stats: {
    "score": -0.9987644356419212,
    "rmse": 0.9987644356419212
}
[08/28/2025 00:48:31 INFO]: Val stats: {
    "score": -0.6772768837325536,
    "rmse": 0.6772768837325536
}
[08/28/2025 00:48:31 INFO]: Test stats: {
    "score": -0.8722274872475213,
    "rmse": 0.8722274872475213
}
[08/28/2025 00:48:37 INFO]: Training loss at epoch 40: 1.2314856052398682
[08/28/2025 00:48:56 INFO]: Training loss at epoch 47: 0.8847257792949677
[08/28/2025 00:48:58 INFO]: Training stats: {
    "score": -0.999905330366859,
    "rmse": 0.999905330366859
}
[08/28/2025 00:48:58 INFO]: Val stats: {
    "score": -0.6898914070120067,
    "rmse": 0.6898914070120067
}
[08/28/2025 00:48:58 INFO]: Test stats: {
    "score": -0.87889330028724,
    "rmse": 0.87889330028724
}
[08/28/2025 00:49:01 INFO]: Training loss at epoch 40: 1.1119744181632996
[08/28/2025 00:49:26 INFO]: Training loss at epoch 48: 0.9570308327674866
[08/28/2025 00:49:32 INFO]: Training loss at epoch 41: 1.1236042380332947
[08/28/2025 00:49:48 INFO]: Training loss at epoch 44: 0.9923864305019379
[08/28/2025 00:49:55 INFO]: Training loss at epoch 83: 0.9631111919879913
[08/28/2025 00:49:55 INFO]: Training loss at epoch 49: 1.0649801790714264
[08/28/2025 00:50:02 INFO]: Training loss at epoch 42: 1.0588127970695496
[08/28/2025 00:50:05 INFO]: Training stats: {
    "score": -0.9999518465937737,
    "rmse": 0.9999518465937737
}
[08/28/2025 00:50:05 INFO]: Val stats: {
    "score": -0.7015023340489384,
    "rmse": 0.7015023340489384
}
[08/28/2025 00:50:05 INFO]: Test stats: {
    "score": -0.8839337262522491,
    "rmse": 0.8839337262522491
}
[08/28/2025 00:50:32 INFO]: Training loss at epoch 43: 0.9154628217220306
[08/28/2025 00:50:35 INFO]: Training loss at epoch 50: 0.9786756932735443
[08/28/2025 00:50:55 INFO]: Training loss at epoch 49: 0.8651304543018341
[08/28/2025 00:51:03 INFO]: Training loss at epoch 44: 1.0833837389945984
[08/28/2025 00:51:04 INFO]: Training loss at epoch 51: 1.2247610092163086
[08/28/2025 00:51:07 INFO]: Running Final Evaluation...
[08/28/2025 00:51:18 INFO]: Training accuracy: {
    "score": -1.011151573676501,
    "rmse": 1.011151573676501
}
[08/28/2025 00:51:18 INFO]: Val accuracy: {
    "score": -0.6612831009595397,
    "rmse": 0.6612831009595397
}
[08/28/2025 00:51:18 INFO]: Test accuracy: {
    "score": -0.8707805812619618,
    "rmse": 0.8707805812619618
}
[08/28/2025 00:51:18 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_77",
    "best_epoch": 13,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8707805812619618,
        "rmse": 0.8707805812619618
    },
    "train_stats": {
        "score": -1.011151573676501,
        "rmse": 1.011151573676501
    },
    "val_stats": {
        "score": -0.6612831009595397,
        "rmse": 0.6612831009595397
    }
}
[08/28/2025 00:51:18 INFO]: Procewss finished for trial unhanged-Shanesha_trial_77
[08/28/2025 00:51:18 INFO]: 
_________________________________________________

[08/28/2025 00:51:18 INFO]: train_net_for_optune.py main() running.
[08/28/2025 00:51:18 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 2.270429725479678
  attention_dropout: 0.14343721366011092
  ffn_dropout: 0.14343721366011092
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00013384088141933225
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_78

[08/28/2025 00:51:18 INFO]: This ft_transformer has 0.307 million parameters.
[08/28/2025 00:51:18 INFO]: Training will start at epoch 0.
[08/28/2025 00:51:18 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 00:51:32 INFO]: Training loss at epoch 6: 1.5523459911346436
[08/28/2025 00:51:35 INFO]: Training loss at epoch 52: 1.3715511858463287
[08/28/2025 00:51:58 INFO]: Training loss at epoch 0: 1.4510195851325989
[08/28/2025 00:52:00 INFO]: Training loss at epoch 6: 1.0387890040874481
[08/28/2025 00:52:04 INFO]: New best epoch, val score: -1.099091673852297
[08/28/2025 00:52:04 INFO]: Saving model to: unhanged-Shanesha_trial_78/model_best.pth
[08/28/2025 00:52:05 INFO]: Training loss at epoch 53: 1.0059981048107147
[08/28/2025 00:52:14 INFO]: Training loss at epoch 34: 1.1436941921710968
[08/28/2025 00:52:17 INFO]: Training loss at epoch 49: 0.940795511007309
[08/28/2025 00:52:30 INFO]: Training loss at epoch 10: 1.0812547206878662
[08/28/2025 00:52:32 INFO]: Training loss at epoch 7: 1.0279273986816406
[08/28/2025 00:52:35 INFO]: Training loss at epoch 54: 0.8925116658210754
[08/28/2025 00:52:38 INFO]: Training stats: {
    "score": -1.0052119770653616,
    "rmse": 1.0052119770653616
}
[08/28/2025 00:52:38 INFO]: Val stats: {
    "score": -0.7205120645329952,
    "rmse": 0.7205120645329952
}
[08/28/2025 00:52:38 INFO]: Test stats: {
    "score": -0.8965900291779144,
    "rmse": 0.8965900291779144
}
[08/28/2025 00:52:43 INFO]: Training loss at epoch 1: 1.3114124536514282
[08/28/2025 00:52:49 INFO]: New best epoch, val score: -0.9033954794250354
[08/28/2025 00:52:49 INFO]: Saving model to: unhanged-Shanesha_trial_78/model_best.pth
[08/28/2025 00:53:04 INFO]: Training loss at epoch 55: 0.9348889887332916
[08/28/2025 00:53:07 INFO]: Training loss at epoch 8: 1.3761643767356873
[08/28/2025 00:53:08 INFO]: Running Final Evaluation...
[08/28/2025 00:53:11 INFO]: Training loss at epoch 15: 1.13624769449234
[08/28/2025 00:53:18 INFO]: Training accuracy: {
    "score": -1.0110892089373642,
    "rmse": 1.0110892089373642
}
[08/28/2025 00:53:18 INFO]: Val accuracy: {
    "score": -0.6617062293100546,
    "rmse": 0.6617062293100546
}
[08/28/2025 00:53:18 INFO]: Test accuracy: {
    "score": -0.8711985670170913,
    "rmse": 0.8711985670170913
}
[08/28/2025 00:53:18 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_76",
    "best_epoch": 24,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8711985670170913,
        "rmse": 0.8711985670170913
    },
    "train_stats": {
        "score": -1.0110892089373642,
        "rmse": 1.0110892089373642
    },
    "val_stats": {
        "score": -0.6617062293100546,
        "rmse": 0.6617062293100546
    }
}
[08/28/2025 00:53:18 INFO]: Procewss finished for trial unhanged-Shanesha_trial_76
[08/28/2025 00:53:19 INFO]: 
_________________________________________________

[08/28/2025 00:53:19 INFO]: train_net_for_optune.py main() running.
[08/28/2025 00:53:19 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 2.2602078184268883
  attention_dropout: 0.1368013425900629
  ffn_dropout: 0.1368013425900629
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0002020716813761348
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_79

[08/28/2025 00:53:19 INFO]: This ft_transformer has 0.380 million parameters.
[08/28/2025 00:53:19 INFO]: Training will start at epoch 0.
[08/28/2025 00:53:19 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 00:53:28 INFO]: Training loss at epoch 36: 0.9563521444797516
[08/28/2025 00:53:29 INFO]: Training loss at epoch 2: 1.0900258421897888
[08/28/2025 00:53:34 INFO]: New best epoch, val score: -0.7076446308212064
[08/28/2025 00:53:34 INFO]: Saving model to: unhanged-Shanesha_trial_78/model_best.pth
[08/28/2025 00:53:59 INFO]: Training loss at epoch 0: 1.0696510076522827
[08/28/2025 00:54:04 INFO]: New best epoch, val score: -0.6607047245028203
[08/28/2025 00:54:04 INFO]: Saving model to: unhanged-Shanesha_trial_79/model_best.pth
[08/28/2025 00:54:11 INFO]: Training loss at epoch 3: 1.2691714465618134
[08/28/2025 00:54:16 INFO]: New best epoch, val score: -0.6616012130527493
[08/28/2025 00:54:16 INFO]: Saving model to: unhanged-Shanesha_trial_78/model_best.pth
[08/28/2025 00:54:16 INFO]: Training loss at epoch 17: 0.7878747880458832
[08/28/2025 00:54:19 INFO]: Training loss at epoch 43: 1.1405959129333496
[08/28/2025 00:54:39 INFO]: Training stats: {
    "score": -0.9992856218447496,
    "rmse": 0.9992856218447496
}
[08/28/2025 00:54:39 INFO]: Val stats: {
    "score": -0.6918721881425568,
    "rmse": 0.6918721881425568
}
[08/28/2025 00:54:39 INFO]: Test stats: {
    "score": -0.8795729231769184,
    "rmse": 0.8795729231769184
}
[08/28/2025 00:54:44 INFO]: Training loss at epoch 1: 1.2275295853614807
[08/28/2025 00:54:53 INFO]: Training loss at epoch 4: 0.9534071385860443
[08/28/2025 00:55:18 INFO]: Training loss at epoch 45: 0.9629413187503815
[08/28/2025 00:55:23 INFO]: Training loss at epoch 49: 1.0395148992538452
[08/28/2025 00:55:28 INFO]: Training loss at epoch 2: 1.1910516023635864
[08/28/2025 00:55:34 INFO]: Training loss at epoch 5: 1.0034402310848236
[08/28/2025 00:55:40 INFO]: Training loss at epoch 41: 0.8291103839874268
[08/28/2025 00:55:52 INFO]: Training loss at epoch 50: 1.1047660112380981
[08/28/2025 00:56:13 INFO]: Training loss at epoch 3: 0.9178660809993744
[08/28/2025 00:56:15 INFO]: Training loss at epoch 6: 1.186028778553009
[08/28/2025 00:56:18 INFO]: New best epoch, val score: -0.6599894014383417
[08/28/2025 00:56:18 INFO]: Saving model to: unhanged-Shanesha_trial_79/model_best.pth
[08/28/2025 00:56:43 INFO]: Training loss at epoch 7: 1.2106741070747375
[08/28/2025 00:56:45 INFO]: Training loss at epoch 45: 1.0184513330459595
[08/28/2025 00:56:57 INFO]: Training loss at epoch 7: 1.1251047253608704
[08/28/2025 00:56:57 INFO]: Training loss at epoch 4: 0.8466275036334991
[08/28/2025 00:57:02 INFO]: Training loss at epoch 84: 0.9925403296947479
[08/28/2025 00:57:06 INFO]: Training loss at epoch 7: 1.0976730585098267
[08/28/2025 00:57:14 INFO]: Training loss at epoch 50: 1.0176950693130493
[08/28/2025 00:57:35 INFO]: Training loss at epoch 8: 0.9771918058395386
[08/28/2025 00:57:36 INFO]: Training stats: {
    "score": -1.0001503020914913,
    "rmse": 1.0001503020914913
}
[08/28/2025 00:57:36 INFO]: Val stats: {
    "score": -0.6715908287730867,
    "rmse": 0.6715908287730867
}
[08/28/2025 00:57:36 INFO]: Test stats: {
    "score": -0.8704011688478338,
    "rmse": 0.8704011688478338
}
[08/28/2025 00:57:39 INFO]: Training loss at epoch 8: 1.0889146327972412
[08/28/2025 00:57:42 INFO]: Training loss at epoch 5: 1.174778550863266
[08/28/2025 00:58:04 INFO]: Training loss at epoch 11: 1.0222972631454468
[08/28/2025 00:58:06 INFO]: Training loss at epoch 37: 1.0754767954349518
[08/28/2025 00:58:13 INFO]: Training loss at epoch 35: 0.9719569981098175
[08/28/2025 00:58:25 INFO]: Training loss at epoch 9: 1.1449880003929138
[08/28/2025 00:58:31 INFO]: Training loss at epoch 6: 1.0999691486358643
[08/28/2025 00:58:40 INFO]: Training stats: {
    "score": -1.0405820242098318,
    "rmse": 1.0405820242098318
}
[08/28/2025 00:58:40 INFO]: Val stats: {
    "score": -0.6699356767240925,
    "rmse": 0.6699356767240925
}
[08/28/2025 00:58:40 INFO]: Test stats: {
    "score": -0.8882385788904884,
    "rmse": 0.8882385788904884
}
[08/28/2025 00:58:45 INFO]: Training loss at epoch 9: 1.6083571314811707
[08/28/2025 00:59:14 INFO]: Training loss at epoch 16: 1.184574007987976
[08/28/2025 00:59:19 INFO]: Training loss at epoch 7: 1.0555850863456726
[08/28/2025 00:59:26 INFO]: Training loss at epoch 10: 0.9782148897647858
[08/28/2025 00:59:31 INFO]: New best epoch, val score: -0.6610335258352859
[08/28/2025 00:59:31 INFO]: Saving model to: unhanged-Shanesha_trial_78/model_best.pth
[08/28/2025 01:00:07 INFO]: Training loss at epoch 8: 1.061282068490982
[08/28/2025 01:00:11 INFO]: Training loss at epoch 11: 1.0877030491828918
[08/28/2025 01:00:17 INFO]: New best epoch, val score: -0.6580068501678836
[08/28/2025 01:00:17 INFO]: Saving model to: unhanged-Shanesha_trial_78/model_best.pth
[08/28/2025 01:00:21 INFO]: Training loss at epoch 18: 1.0244867205619812
[08/28/2025 01:00:50 INFO]: Training stats: {
    "score": -1.1261901453056773,
    "rmse": 1.1261901453056773
}
[08/28/2025 01:00:50 INFO]: Val stats: {
    "score": -0.9496570643005962,
    "rmse": 0.9496570643005962
}
[08/28/2025 01:00:50 INFO]: Test stats: {
    "score": -1.065044792332435,
    "rmse": 1.065044792332435
}
[08/28/2025 01:00:56 INFO]: Training loss at epoch 9: 1.0518551468849182
[08/28/2025 01:00:57 INFO]: Training loss at epoch 12: 0.8491424918174744
[08/28/2025 01:01:07 INFO]: Training loss at epoch 44: 0.9859417378902435
[08/28/2025 01:01:13 INFO]: Training stats: {
    "score": -0.9971647611961874,
    "rmse": 0.9971647611961874
}
[08/28/2025 01:01:13 INFO]: Val stats: {
    "score": -0.6851691934879914,
    "rmse": 0.6851691934879914
}
[08/28/2025 01:01:13 INFO]: Test stats: {
    "score": -0.8793338601275887,
    "rmse": 0.8793338601275887
}
[08/28/2025 01:01:35 INFO]: Training loss at epoch 50: 1.1637107133865356
[08/28/2025 01:01:41 INFO]: Training loss at epoch 13: 1.0668089985847473
[08/28/2025 01:02:01 INFO]: Training loss at epoch 10: 0.8495067358016968
[08/28/2025 01:02:11 INFO]: Training loss at epoch 8: 1.4342575669288635
[08/28/2025 01:02:13 INFO]: Training loss at epoch 51: 1.006223738193512
[08/28/2025 01:02:13 INFO]: Training loss at epoch 46: 1.1077130734920502
[08/28/2025 01:02:27 INFO]: Training loss at epoch 14: 0.9221665263175964
[08/28/2025 01:02:33 INFO]: Training loss at epoch 8: 1.5081965923309326
[08/28/2025 01:02:42 INFO]: Training loss at epoch 42: 1.0527254939079285
[08/28/2025 01:02:48 INFO]: Training loss at epoch 51: 0.7947358191013336
[08/28/2025 01:02:48 INFO]: Training loss at epoch 11: 0.9932713806629181
[08/28/2025 01:03:04 INFO]: Training loss at epoch 9: 1.2219604849815369
[08/28/2025 01:03:08 INFO]: Training loss at epoch 15: 1.0180067420005798
[08/28/2025 01:03:08 INFO]: Training loss at epoch 38: 0.9303776919841766
[08/28/2025 01:03:32 INFO]: Training loss at epoch 12: 1.0221181511878967
[08/28/2025 01:03:49 INFO]: Training loss at epoch 46: 1.1163168549537659
[08/28/2025 01:03:50 INFO]: Training loss at epoch 16: 1.1376128792762756
[08/28/2025 01:03:57 INFO]: Training loss at epoch 12: 1.0531521439552307
[08/28/2025 01:04:17 INFO]: Training loss at epoch 13: 1.0994109809398651
[08/28/2025 01:04:18 INFO]: Training loss at epoch 85: 0.8412517309188843
[08/28/2025 01:04:27 INFO]: Training loss at epoch 36: 1.0379319489002228
[08/28/2025 01:04:31 INFO]: Training loss at epoch 17: 0.9145763516426086
[08/28/2025 01:04:35 INFO]: Training loss at epoch 50: 1.1675783395767212
[08/28/2025 01:04:44 INFO]: Training stats: {
    "score": -1.05106356425541,
    "rmse": 1.05106356425541
}
[08/28/2025 01:04:44 INFO]: Val stats: {
    "score": -0.6798746616473187,
    "rmse": 0.6798746616473187
}
[08/28/2025 01:04:44 INFO]: Test stats: {
    "score": -0.8984887292019461,
    "rmse": 0.8984887292019461
}
[08/28/2025 01:05:01 INFO]: Training loss at epoch 14: 0.9025759100914001
[08/28/2025 01:05:05 INFO]: New best epoch, val score: -0.65542711551482
[08/28/2025 01:05:05 INFO]: Saving model to: unhanged-Shanesha_trial_20/model_best.pth
[08/28/2025 01:05:13 INFO]: Training loss at epoch 18: 1.022683322429657
[08/28/2025 01:05:25 INFO]: Training loss at epoch 17: 1.0606553256511688
[08/28/2025 01:05:46 INFO]: Training loss at epoch 15: 1.0106874108314514
[08/28/2025 01:05:55 INFO]: Training loss at epoch 19: 1.031020700931549
[08/28/2025 01:06:10 INFO]: Training stats: {
    "score": -1.0015784678941069,
    "rmse": 1.0015784678941069
}
[08/28/2025 01:06:10 INFO]: Val stats: {
    "score": -0.6991974097686219,
    "rmse": 0.6991974097686219
}
[08/28/2025 01:06:10 INFO]: Test stats: {
    "score": -0.8853788002402229,
    "rmse": 0.8853788002402229
}
[08/28/2025 01:06:23 INFO]: Training loss at epoch 19: 0.8790564835071564
[08/28/2025 01:06:31 INFO]: Training loss at epoch 10: 1.1304934322834015
[08/28/2025 01:06:31 INFO]: Training loss at epoch 16: 0.8228991031646729
[08/28/2025 01:06:49 INFO]: Training loss at epoch 52: 0.8935213387012482
[08/28/2025 01:06:53 INFO]: Training loss at epoch 20: 1.1408042907714844
[08/28/2025 01:07:13 INFO]: New best epoch, val score: -0.6844243472295812
[08/28/2025 01:07:13 INFO]: Saving model to: unhanged-Shanesha_trial_70/model_best.pth
[08/28/2025 01:07:17 INFO]: Training loss at epoch 9: 1.0321599543094635
[08/28/2025 01:07:19 INFO]: Training loss at epoch 17: 1.0378330647945404
[08/28/2025 01:07:36 INFO]: Training loss at epoch 9: 1.022507220506668
[08/28/2025 01:07:39 INFO]: Training loss at epoch 21: 0.8794153034687042
[08/28/2025 01:07:50 INFO]: Training loss at epoch 45: 0.826166033744812
[08/28/2025 01:07:51 INFO]: Training loss at epoch 39: 1.0292944312095642
[08/28/2025 01:08:07 INFO]: Training loss at epoch 18: 0.9691274762153625
[08/28/2025 01:08:24 INFO]: Training loss at epoch 22: 0.9362297356128693
[08/28/2025 01:08:25 INFO]: Training loss at epoch 51: 0.9636619687080383
[08/28/2025 01:08:35 INFO]: Training stats: {
    "score": -0.9998266312501257,
    "rmse": 0.9998266312501257
}
[08/28/2025 01:08:35 INFO]: Val stats: {
    "score": -0.6788879679402494,
    "rmse": 0.6788879679402494
}
[08/28/2025 01:08:35 INFO]: Test stats: {
    "score": -0.8733383952026671,
    "rmse": 0.8733383952026671
}
[08/28/2025 01:08:56 INFO]: Training loss at epoch 19: 1.0819288790225983
[08/28/2025 01:08:58 INFO]: Training loss at epoch 47: 1.1371415853500366
[08/28/2025 01:09:09 INFO]: Training loss at epoch 23: 1.1188345551490784
[08/28/2025 01:09:09 INFO]: Training stats: {
    "score": -1.0630600722902235,
    "rmse": 1.0630600722902235
}
[08/28/2025 01:09:09 INFO]: Val stats: {
    "score": -0.6901766051979616,
    "rmse": 0.6901766051979616
}
[08/28/2025 01:09:09 INFO]: Test stats: {
    "score": -0.9092312248249885,
    "rmse": 0.9092312248249885
}
[08/28/2025 01:09:13 INFO]: Training stats: {
    "score": -0.9953092797703552,
    "rmse": 0.9953092797703552
}
[08/28/2025 01:09:13 INFO]: Val stats: {
    "score": -0.687731700630488,
    "rmse": 0.687731700630488
}
[08/28/2025 01:09:13 INFO]: Test stats: {
    "score": -0.8802754376126586,
    "rmse": 0.8802754376126586
}
[08/28/2025 01:09:28 INFO]: Training stats: {
    "score": -1.0667245638872314,
    "rmse": 1.0667245638872314
}
[08/28/2025 01:09:28 INFO]: Val stats: {
    "score": -0.6914173937622629,
    "rmse": 0.6914173937622629
}
[08/28/2025 01:09:28 INFO]: Test stats: {
    "score": -0.9101640826765773,
    "rmse": 0.9101640826765773
}
[08/28/2025 01:09:31 INFO]: Training loss at epoch 52: 0.9274082183837891
[08/28/2025 01:09:33 INFO]: Training loss at epoch 43: 0.8627449572086334
[08/28/2025 01:09:35 INFO]: Training stats: {
    "score": -0.9954172891369272,
    "rmse": 0.9954172891369272
}
[08/28/2025 01:09:35 INFO]: Val stats: {
    "score": -0.6833467565660112,
    "rmse": 0.6833467565660112
}
[08/28/2025 01:09:35 INFO]: Test stats: {
    "score": -0.8751211417498984,
    "rmse": 0.8751211417498984
}
[08/28/2025 01:09:44 INFO]: Training loss at epoch 13: 1.1391615867614746
[08/28/2025 01:09:51 INFO]: Running Final Evaluation...
[08/28/2025 01:09:54 INFO]: Training loss at epoch 24: 0.9899410307407379
[08/28/2025 01:10:02 INFO]: Training loss at epoch 20: 1.0555750131607056
[08/28/2025 01:10:06 INFO]: Training loss at epoch 10: 1.0180948674678802
[08/28/2025 01:10:40 INFO]: Training loss at epoch 25: 0.9949621558189392
[08/28/2025 01:10:40 INFO]: Training loss at epoch 37: 1.2573183476924896
[08/28/2025 01:10:51 INFO]: Training loss at epoch 47: 0.8297606706619263
[08/28/2025 01:10:51 INFO]: Training loss at epoch 21: 0.8791102468967438
[08/28/2025 01:11:24 INFO]: Training loss at epoch 26: 1.0959028601646423
[08/28/2025 01:11:35 INFO]: Training loss at epoch 51: 1.048422932624817
[08/28/2025 01:11:36 INFO]: Training loss at epoch 86: 0.9275091886520386
[08/28/2025 01:11:39 INFO]: Training loss at epoch 22: 0.8607256412506104
[08/28/2025 01:11:48 INFO]: Training loss at epoch 18: 1.1263957023620605
[08/28/2025 01:11:51 INFO]: Training loss at epoch 53: 1.0502769351005554
[08/28/2025 01:12:09 INFO]: Training loss at epoch 27: 0.973027378320694
[08/28/2025 01:12:26 INFO]: Training accuracy: {
    "score": -1.0083632500015283,
    "rmse": 1.0083632500015283
}
[08/28/2025 01:12:26 INFO]: Val accuracy: {
    "score": -0.6616730050222556,
    "rmse": 0.6616730050222556
}
[08/28/2025 01:12:26 INFO]: Test accuracy: {
    "score": -0.8696096448948817,
    "rmse": 0.8696096448948817
}
[08/28/2025 01:12:26 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_48",
    "best_epoch": 16,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8696096448948817,
        "rmse": 0.8696096448948817
    },
    "train_stats": {
        "score": -1.0083632500015283,
        "rmse": 1.0083632500015283
    },
    "val_stats": {
        "score": -0.6616730050222556,
        "rmse": 0.6616730050222556
    }
}
[08/28/2025 01:12:26 INFO]: Procewss finished for trial unhanged-Shanesha_trial_48
[08/28/2025 01:12:26 INFO]: 
_________________________________________________

[08/28/2025 01:12:26 INFO]: train_net_for_optune.py main() running.
[08/28/2025 01:12:26 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 0.8481424684935858
  attention_dropout: 0.4659132578168565
  ffn_dropout: 0.4659132578168565
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00019185239862361275
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_80

[08/28/2025 01:12:26 INFO]: This ft_transformer has 0.241 million parameters.
[08/28/2025 01:12:26 INFO]: Training will start at epoch 0.
[08/28/2025 01:12:26 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 01:12:27 INFO]: Training loss at epoch 23: 1.0110256671905518
[08/28/2025 01:12:35 INFO]: Training loss at epoch 11: 0.9079055786132812
[08/28/2025 01:12:51 INFO]: Training loss at epoch 28: 1.0463318824768066
[08/28/2025 01:13:01 INFO]: Training loss at epoch 0: 0.9158046543598175
[08/28/2025 01:13:06 INFO]: New best epoch, val score: -0.7272462992284013
[08/28/2025 01:13:06 INFO]: Saving model to: unhanged-Shanesha_trial_80/model_best.pth
[08/28/2025 01:13:11 INFO]: Training loss at epoch 24: 1.169863075017929
[08/28/2025 01:13:14 INFO]: New best epoch, val score: -0.6841700361866349
[08/28/2025 01:13:14 INFO]: Saving model to: unhanged-Shanesha_trial_70/model_best.pth
[08/28/2025 01:13:33 INFO]: Training loss at epoch 29: 1.107578605413437
[08/28/2025 01:13:41 INFO]: Training loss at epoch 1: 1.2483683824539185
[08/28/2025 01:13:46 INFO]: New best epoch, val score: -0.7263728835511264
[08/28/2025 01:13:46 INFO]: Saving model to: unhanged-Shanesha_trial_80/model_best.pth
[08/28/2025 01:13:48 INFO]: Training stats: {
    "score": -1.0009104240812126,
    "rmse": 1.0009104240812126
}
[08/28/2025 01:13:48 INFO]: Val stats: {
    "score": -0.6682204236544568,
    "rmse": 0.6682204236544568
}
[08/28/2025 01:13:48 INFO]: Test stats: {
    "score": -0.8699616252772105,
    "rmse": 0.8699616252772105
}
[08/28/2025 01:13:56 INFO]: Training loss at epoch 25: 0.8464817702770233
[08/28/2025 01:14:20 INFO]: Training loss at epoch 2: 1.240335762500763
[08/28/2025 01:14:25 INFO]: Training loss at epoch 40: 1.0126498639583588
[08/28/2025 01:14:25 INFO]: New best epoch, val score: -0.675424838987941
[08/28/2025 01:14:25 INFO]: Saving model to: unhanged-Shanesha_trial_80/model_best.pth
[08/28/2025 01:14:27 INFO]: Training loss at epoch 10: 0.9694174826145172
[08/28/2025 01:14:29 INFO]: Training loss at epoch 30: 1.0448604822158813
[08/28/2025 01:14:41 INFO]: Training loss at epoch 26: 1.126325935125351
[08/28/2025 01:14:41 INFO]: Training loss at epoch 10: 1.2466470003128052
[08/28/2025 01:14:43 INFO]: Training loss at epoch 46: 1.019712507724762
[08/28/2025 01:14:45 INFO]: Training loss at epoch 20: 1.0528584718704224
[08/28/2025 01:14:59 INFO]: Training loss at epoch 3: 1.167763888835907
[08/28/2025 01:15:04 INFO]: New best epoch, val score: -0.6624529614978771
[08/28/2025 01:15:04 INFO]: Saving model to: unhanged-Shanesha_trial_80/model_best.pth
[08/28/2025 01:15:11 INFO]: Training loss at epoch 31: 0.8154897689819336
[08/28/2025 01:15:16 INFO]: Training loss at epoch 11: 0.9119541943073273
[08/28/2025 01:15:19 INFO]: Training loss at epoch 52: 1.0531872510910034
[08/28/2025 01:15:25 INFO]: Training loss at epoch 27: 0.9258635342121124
[08/28/2025 01:15:26 INFO]: Training loss at epoch 14: 0.8847464919090271
[08/28/2025 01:15:39 INFO]: Training loss at epoch 4: 1.2665079236030579
[08/28/2025 01:15:53 INFO]: Training loss at epoch 32: 0.9687298238277435
[08/28/2025 01:16:11 INFO]: Training loss at epoch 28: 0.87239870429039
[08/28/2025 01:16:17 INFO]: Training loss at epoch 53: 1.1862953901290894
[08/28/2025 01:16:20 INFO]: Training loss at epoch 5: 0.8979914784431458
[08/28/2025 01:16:26 INFO]: Training loss at epoch 44: 1.034028172492981
[08/28/2025 01:16:27 INFO]: Training loss at epoch 54: 1.012522041797638
[08/28/2025 01:16:37 INFO]: Training loss at epoch 33: 0.90901979804039
[08/28/2025 01:16:40 INFO]: Training loss at epoch 38: 1.063859760761261
[08/28/2025 01:16:58 INFO]: Training loss at epoch 29: 0.827536404132843
[08/28/2025 01:17:02 INFO]: Training loss at epoch 6: 1.1608269810676575
[08/28/2025 01:17:16 INFO]: Training stats: {
    "score": -0.9933922722294349,
    "rmse": 0.9933922722294349
}
[08/28/2025 01:17:16 INFO]: Val stats: {
    "score": -0.6950339455347802,
    "rmse": 0.6950339455347802
}
[08/28/2025 01:17:16 INFO]: Test stats: {
    "score": -0.8834882067916904,
    "rmse": 0.8834882067916904
}
[08/28/2025 01:17:23 INFO]: Training loss at epoch 34: 0.9293080568313599
[08/28/2025 01:17:44 INFO]: Training loss at epoch 48: 1.1182488203048706
[08/28/2025 01:17:45 INFO]: Training loss at epoch 7: 0.9285925924777985
[08/28/2025 01:17:52 INFO]: Training loss at epoch 19: 1.502694845199585
[08/28/2025 01:18:04 INFO]: Training loss at epoch 30: 0.9438987374305725
[08/28/2025 01:18:08 INFO]: Training loss at epoch 35: 0.9617901742458344
[08/28/2025 01:18:21 INFO]: Training loss at epoch 12: 1.5655877590179443
[08/28/2025 01:18:24 INFO]: Training loss at epoch 52: 1.1729127168655396
[08/28/2025 01:18:30 INFO]: Training loss at epoch 8: 1.0970261096954346
[08/28/2025 01:18:43 INFO]: Training loss at epoch 87: 0.9284586608409882
[08/28/2025 01:18:52 INFO]: Training loss at epoch 31: 1.237516164779663
[08/28/2025 01:18:54 INFO]: Training loss at epoch 36: 1.1912589073181152
[08/28/2025 01:19:12 INFO]: Training loss at epoch 9: 1.1272003054618835
[08/28/2025 01:19:22 INFO]: Training loss at epoch 41: 1.1647034287452698
[08/28/2025 01:19:29 INFO]: Training stats: {
    "score": -0.9993200420591084,
    "rmse": 0.9993200420591084
}
[08/28/2025 01:19:29 INFO]: Val stats: {
    "score": -0.6787591642343312,
    "rmse": 0.6787591642343312
}
[08/28/2025 01:19:29 INFO]: Test stats: {
    "score": -0.8733071239740267,
    "rmse": 0.8733071239740267
}
[08/28/2025 01:19:39 INFO]: Training loss at epoch 37: 0.9649829268455505
[08/28/2025 01:19:41 INFO]: Training loss at epoch 32: 1.03971067070961
[08/28/2025 01:19:51 INFO]: Training loss at epoch 11: 1.8695029020309448
[08/28/2025 01:20:03 INFO]: Training loss at epoch 11: 1.0637040734291077
[08/28/2025 01:20:05 INFO]: Training stats: {
    "score": -1.011244541043844,
    "rmse": 1.011244541043844
}
[08/28/2025 01:20:05 INFO]: Val stats: {
    "score": -0.7316940809216366,
    "rmse": 0.7316940809216366
}
[08/28/2025 01:20:05 INFO]: Test stats: {
    "score": -0.9035220453670109,
    "rmse": 0.9035220453670109
}
[08/28/2025 01:20:12 INFO]: Training loss at epoch 10: 0.8918775022029877
[08/28/2025 01:20:24 INFO]: Training loss at epoch 38: 1.0440263748168945
[08/28/2025 01:20:29 INFO]: Training loss at epoch 33: 0.9158617854118347
[08/28/2025 01:20:44 INFO]: Training loss at epoch 12: 1.224283218383789
[08/28/2025 01:20:54 INFO]: Training loss at epoch 11: 1.1520344614982605
[08/28/2025 01:21:05 INFO]: Training loss at epoch 21: 1.102215588092804
[08/28/2025 01:21:08 INFO]: Training loss at epoch 39: 1.0759111940860748
[08/28/2025 01:21:15 INFO]: Training loss at epoch 34: 1.450794756412506
[08/28/2025 01:21:21 INFO]: Running Final Evaluation...
[08/28/2025 01:21:22 INFO]: Training loss at epoch 15: 0.9870818257331848
[08/28/2025 01:21:23 INFO]: Training stats: {
    "score": -1.003948038505473,
    "rmse": 1.003948038505473
}
[08/28/2025 01:21:23 INFO]: Val stats: {
    "score": -0.6626078818255156,
    "rmse": 0.6626078818255156
}
[08/28/2025 01:21:23 INFO]: Test stats: {
    "score": -0.8684271472324422,
    "rmse": 0.8684271472324422
}
[08/28/2025 01:21:24 INFO]: Training loss at epoch 55: 0.9931929111480713
[08/28/2025 01:21:33 INFO]: Training loss at epoch 12: 1.0278138220310211
[08/28/2025 01:21:37 INFO]: Training accuracy: {
    "score": -1.0109215606466746,
    "rmse": 1.0109215606466746
}
[08/28/2025 01:21:37 INFO]: Val accuracy: {
    "score": -0.6599894014383417,
    "rmse": 0.6599894014383417
}
[08/28/2025 01:21:37 INFO]: Test accuracy: {
    "score": -0.8784131023901993,
    "rmse": 0.8784131023901993
}
[08/28/2025 01:21:37 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_79",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8784131023901993,
        "rmse": 0.8784131023901993
    },
    "train_stats": {
        "score": -1.0109215606466746,
        "rmse": 1.0109215606466746
    },
    "val_stats": {
        "score": -0.6599894014383417,
        "rmse": 0.6599894014383417
    }
}
[08/28/2025 01:21:37 INFO]: Procewss finished for trial unhanged-Shanesha_trial_79
[08/28/2025 01:21:37 INFO]: 
_________________________________________________

[08/28/2025 01:21:37 INFO]: train_net_for_optune.py main() running.
[08/28/2025 01:21:37 INFO]: model:
  name: ft_transformer
  d_embedding: 512
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 16
  d_ffn_factor: 0.7899685148684392
  attention_dropout: 0.05397911023896013
  ffn_dropout: 0.05397911023896013
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.000136089906168693
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_81

[08/28/2025 01:21:37 INFO]: This ft_transformer has 8.974 million parameters.
[08/28/2025 01:21:37 INFO]: Training will start at epoch 0.
[08/28/2025 01:21:37 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 01:21:39 INFO]: Training loss at epoch 47: 0.8983981907367706
[08/28/2025 01:22:05 INFO]: Training loss at epoch 40: 0.9568564295768738
[08/28/2025 01:22:12 INFO]: Training loss at epoch 13: 1.0876190662384033
[08/28/2025 01:22:20 INFO]: Training loss at epoch 53: 1.0455675721168518
[08/28/2025 01:22:47 INFO]: Training loss at epoch 41: 1.2824424505233765
[08/28/2025 01:22:52 INFO]: Training loss at epoch 14: 0.834865391254425
[08/28/2025 01:22:54 INFO]: Training loss at epoch 39: 1.0412646532058716
[08/28/2025 01:23:11 INFO]: Training loss at epoch 54: 0.8593354523181915
[08/28/2025 01:23:27 INFO]: Training loss at epoch 45: 0.967081606388092
[08/28/2025 01:23:28 INFO]: Training loss at epoch 42: 0.9010589718818665
[08/28/2025 01:23:31 INFO]: Training loss at epoch 15: 1.020797848701477
[08/28/2025 01:23:33 INFO]: Running Final Evaluation...
[08/28/2025 01:23:48 INFO]: Training accuracy: {
    "score": -1.011750709960502,
    "rmse": 1.011750709960502
}
[08/28/2025 01:23:48 INFO]: Val accuracy: {
    "score": -0.6580068501678836,
    "rmse": 0.6580068501678836
}
[08/28/2025 01:23:48 INFO]: Test accuracy: {
    "score": -0.8702076954628257,
    "rmse": 0.8702076954628257
}
[08/28/2025 01:23:48 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_78",
    "best_epoch": 11,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8702076954628257,
        "rmse": 0.8702076954628257
    },
    "train_stats": {
        "score": -1.011750709960502,
        "rmse": 1.011750709960502
    },
    "val_stats": {
        "score": -0.6580068501678836,
        "rmse": 0.6580068501678836
    }
}
[08/28/2025 01:23:48 INFO]: Procewss finished for trial unhanged-Shanesha_trial_78
[08/28/2025 01:23:48 INFO]: 
_________________________________________________

[08/28/2025 01:23:48 INFO]: train_net_for_optune.py main() running.
[08/28/2025 01:23:48 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 0.8201505879511382
  attention_dropout: 0.052874590798270935
  ffn_dropout: 0.052874590798270935
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00013833656406112788
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_82

[08/28/2025 01:23:49 INFO]: This ft_transformer has 0.211 million parameters.
[08/28/2025 01:23:49 INFO]: Training will start at epoch 0.
[08/28/2025 01:23:49 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 01:24:05 INFO]: Training loss at epoch 42: 0.7998687028884888
[08/28/2025 01:24:10 INFO]: Training loss at epoch 13: 1.361752986907959
[08/28/2025 01:24:10 INFO]: Training loss at epoch 16: 0.9951566755771637
[08/28/2025 01:24:22 INFO]: Training loss at epoch 0: 1.8791742324829102
[08/28/2025 01:24:26 INFO]: New best epoch, val score: -1.0692785353912182
[08/28/2025 01:24:26 INFO]: Saving model to: unhanged-Shanesha_trial_82/model_best.pth
[08/28/2025 01:24:44 INFO]: Training loss at epoch 49: 0.8296607136726379
[08/28/2025 01:24:49 INFO]: Training loss at epoch 17: 1.1011722385883331
[08/28/2025 01:24:50 INFO]: Training stats: {
    "score": -1.0082580676625135,
    "rmse": 1.0082580676625135
}
[08/28/2025 01:24:50 INFO]: Val stats: {
    "score": -0.7231236684285468,
    "rmse": 0.7231236684285468
}
[08/28/2025 01:24:50 INFO]: Test stats: {
    "score": -0.8980341725119154,
    "rmse": 0.8980341725119154
}
[08/28/2025 01:24:55 INFO]: Training loss at epoch 12: 1.6784604787826538
[08/28/2025 01:25:00 INFO]: Training loss at epoch 1: 1.6815084218978882
[08/28/2025 01:25:05 INFO]: New best epoch, val score: -0.8331539670397352
[08/28/2025 01:25:05 INFO]: Saving model to: unhanged-Shanesha_trial_82/model_best.pth
[08/28/2025 01:25:07 INFO]: Training loss at epoch 12: 1.1515545845031738
[08/28/2025 01:25:10 INFO]: Training loss at epoch 53: 1.0531439781188965
[08/28/2025 01:25:30 INFO]: Training loss at epoch 18: 0.9666552543640137
[08/28/2025 01:25:40 INFO]: Training loss at epoch 2: 1.3012437224388123
[08/28/2025 01:25:43 INFO]: Training loss at epoch 13: 1.2385708093643188
[08/28/2025 01:25:45 INFO]: New best epoch, val score: -0.6738892381016338
[08/28/2025 01:25:45 INFO]: Saving model to: unhanged-Shanesha_trial_82/model_best.pth
[08/28/2025 01:25:46 INFO]: Training loss at epoch 88: 0.821047455072403
[08/28/2025 01:25:59 INFO]: Training loss at epoch 56: 1.0448052883148193
[08/28/2025 01:26:04 INFO]: Training loss at epoch 0: 0.8903058767318726
[08/28/2025 01:26:06 INFO]: Training loss at epoch 20: 0.93840092420578
[08/28/2025 01:26:13 INFO]: Training loss at epoch 19: 1.0788106322288513
[08/28/2025 01:26:21 INFO]: Training loss at epoch 3: 1.0731788277626038
[08/28/2025 01:26:29 INFO]: Training stats: {
    "score": -0.9991720977049425,
    "rmse": 0.9991720977049425
}
[08/28/2025 01:26:29 INFO]: Val stats: {
    "score": -0.6892469440836927,
    "rmse": 0.6892469440836927
}
[08/28/2025 01:26:29 INFO]: Test stats: {
    "score": -0.8792991023494908,
    "rmse": 0.8792991023494908
}
[08/28/2025 01:26:45 INFO]: New best epoch, val score: -0.6774126562570586
[08/28/2025 01:26:45 INFO]: Saving model to: unhanged-Shanesha_trial_81/model_best.pth
[08/28/2025 01:26:59 INFO]: Training loss at epoch 16: 1.0164664685726166
[08/28/2025 01:27:02 INFO]: Training loss at epoch 4: 1.0576378107070923
[08/28/2025 01:27:04 INFO]: Training loss at epoch 22: 0.8627333045005798
[08/28/2025 01:27:12 INFO]: Training loss at epoch 20: 0.9171954691410065
[08/28/2025 01:27:13 INFO]: Training stats: {
    "score": -1.0002152576825183,
    "rmse": 1.0002152576825183
}
[08/28/2025 01:27:13 INFO]: Val stats: {
    "score": -0.6869004810572926,
    "rmse": 0.6869004810572926
}
[08/28/2025 01:27:13 INFO]: Test stats: {
    "score": -0.8771902422477853,
    "rmse": 0.8771902422477853
}
[08/28/2025 01:27:44 INFO]: Training loss at epoch 5: 1.0572574138641357
[08/28/2025 01:27:55 INFO]: Training loss at epoch 21: 1.0359638929367065
[08/28/2025 01:28:25 INFO]: Training loss at epoch 48: 0.7697246968746185
[08/28/2025 01:28:26 INFO]: Training loss at epoch 6: 0.9518162906169891
[08/28/2025 01:28:38 INFO]: Training loss at epoch 22: 1.0512428879737854
[08/28/2025 01:29:04 INFO]: Training loss at epoch 43: 1.1301597356796265
[08/28/2025 01:29:07 INFO]: Training loss at epoch 7: 1.0843182802200317
[08/28/2025 01:29:16 INFO]: Training loss at epoch 54: 0.8987276256084442
[08/28/2025 01:29:21 INFO]: Training loss at epoch 23: 1.2672673761844635
[08/28/2025 01:29:48 INFO]: Training loss at epoch 8: 1.04483562707901
[08/28/2025 01:30:03 INFO]: Training loss at epoch 24: 0.8695282936096191
[08/28/2025 01:30:05 INFO]: Training loss at epoch 55: 1.046901822090149
[08/28/2025 01:30:11 INFO]: Training loss at epoch 14: 1.1653931736946106
[08/28/2025 01:30:27 INFO]: Training loss at epoch 13: 1.0926945209503174
[08/28/2025 01:30:28 INFO]: Training loss at epoch 9: 0.9886028170585632
[08/28/2025 01:30:31 INFO]: Training loss at epoch 46: 1.0736725330352783
[08/28/2025 01:30:37 INFO]: Training loss at epoch 13: 0.9717240333557129
[08/28/2025 01:30:43 INFO]: Training stats: {
    "score": -1.0384157165245953,
    "rmse": 1.0384157165245953
}
[08/28/2025 01:30:43 INFO]: Val stats: {
    "score": -0.7972421522636823,
    "rmse": 0.7972421522636823
}
[08/28/2025 01:30:43 INFO]: Test stats: {
    "score": -0.9496206519960306,
    "rmse": 0.9496206519960306
}
[08/28/2025 01:30:44 INFO]: Training loss at epoch 25: 0.8669376969337463
[08/28/2025 01:30:54 INFO]: Running Final Evaluation...
[08/28/2025 01:30:58 INFO]: Training loss at epoch 57: 0.9690314531326294
[08/28/2025 01:31:11 INFO]: Training loss at epoch 40: 0.9006561040878296
[08/28/2025 01:31:11 INFO]: Training loss at epoch 14: 1.0963842868804932
[08/28/2025 01:31:20 INFO]: Training loss at epoch 10: 0.8793449401855469
[08/28/2025 01:31:24 INFO]: Training loss at epoch 26: 0.97821444272995
[08/28/2025 01:31:30 INFO]: Training loss at epoch 1: 3.1649845242500305
[08/28/2025 01:31:58 INFO]: Training loss at epoch 11: 1.2933539748191833
[08/28/2025 01:32:03 INFO]: Training loss at epoch 27: 1.1096695065498352
[08/28/2025 01:32:13 INFO]: Training loss at epoch 54: 0.9435318410396576
[08/28/2025 01:32:24 INFO]: Training loss at epoch 21: 1.144589126110077
[08/28/2025 01:32:37 INFO]: Training loss at epoch 12: 1.3528975248336792
[08/28/2025 01:32:42 INFO]: Training loss at epoch 28: 0.918588787317276
[08/28/2025 01:32:47 INFO]: Training loss at epoch 17: 1.2979921698570251
[08/28/2025 01:33:01 INFO]: Training loss at epoch 89: 1.2459513545036316
[08/28/2025 01:33:09 INFO]: Training accuracy: {
    "score": -1.0137433635944326,
    "rmse": 1.0137433635944326
}
[08/28/2025 01:33:09 INFO]: Val accuracy: {
    "score": -0.6603142346557613,
    "rmse": 0.6603142346557613
}
[08/28/2025 01:33:09 INFO]: Test accuracy: {
    "score": -0.8715015854704578,
    "rmse": 0.8715015854704578
}
[08/28/2025 01:33:09 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_41",
    "best_epoch": 24,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8715015854704578,
        "rmse": 0.8715015854704578
    },
    "train_stats": {
        "score": -1.0137433635944326,
        "rmse": 1.0137433635944326
    },
    "val_stats": {
        "score": -0.6603142346557613,
        "rmse": 0.6603142346557613
    }
}
[08/28/2025 01:33:09 INFO]: Procewss finished for trial unhanged-Shanesha_trial_41
[08/28/2025 01:33:09 INFO]: 
_________________________________________________

[08/28/2025 01:33:09 INFO]: train_net_for_optune.py main() running.
[08/28/2025 01:33:09 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 2.432221790069989
  attention_dropout: 0.05998966863196125
  ffn_dropout: 0.05998966863196125
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00013316593356152057
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_83

[08/28/2025 01:33:09 INFO]: This ft_transformer has 0.317 million parameters.
[08/28/2025 01:33:09 INFO]: Training will start at epoch 0.
[08/28/2025 01:33:09 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 01:33:14 INFO]: Training loss at epoch 23: 1.065771222114563
[08/28/2025 01:33:16 INFO]: Training loss at epoch 13: 0.9139115512371063
[08/28/2025 01:33:21 INFO]: Training loss at epoch 29: 1.109947144985199
[08/28/2025 01:33:36 INFO]: Training stats: {
    "score": -0.9984156874200242,
    "rmse": 0.9984156874200242
}
[08/28/2025 01:33:36 INFO]: Val stats: {
    "score": -0.6776768903981834,
    "rmse": 0.6776768903981834
}
[08/28/2025 01:33:36 INFO]: Test stats: {
    "score": -0.873669287614725,
    "rmse": 0.873669287614725
}
[08/28/2025 01:33:46 INFO]: Training loss at epoch 44: 0.916622132062912
[08/28/2025 01:33:47 INFO]: Training loss at epoch 0: 1.2643309831619263
[08/28/2025 01:33:52 INFO]: New best epoch, val score: -0.7060049588667523
[08/28/2025 01:33:52 INFO]: Saving model to: unhanged-Shanesha_trial_83/model_best.pth
[08/28/2025 01:33:55 INFO]: Training loss at epoch 14: 1.0704882144927979
[08/28/2025 01:34:10 INFO]: Training loss at epoch 50: 0.8439872562885284
[08/28/2025 01:34:15 INFO]: Training loss at epoch 30: 1.0056607723236084
[08/28/2025 01:34:31 INFO]: Training loss at epoch 1: 1.20735502243042
[08/28/2025 01:34:34 INFO]: Training loss at epoch 15: 1.2130966782569885
[08/28/2025 01:34:37 INFO]: New best epoch, val score: -0.6739423461134504
[08/28/2025 01:34:37 INFO]: Saving model to: unhanged-Shanesha_trial_83/model_best.pth
[08/28/2025 01:34:57 INFO]: Training loss at epoch 31: 0.9032961428165436
[08/28/2025 01:35:07 INFO]: Training loss at epoch 49: 0.8561091125011444
[08/28/2025 01:35:15 INFO]: Training loss at epoch 16: 1.0736966133117676
[08/28/2025 01:35:17 INFO]: Training loss at epoch 2: 1.2567675113677979
[08/28/2025 01:35:22 INFO]: Training stats: {
    "score": -0.9782099041511618,
    "rmse": 0.9782099041511618
}
[08/28/2025 01:35:22 INFO]: Val stats: {
    "score": -0.65596751723178,
    "rmse": 0.65596751723178
}
[08/28/2025 01:35:22 INFO]: Test stats: {
    "score": -0.8824517189877099,
    "rmse": 0.8824517189877099
}
[08/28/2025 01:35:32 INFO]: Training loss at epoch 14: 0.8875776827335358
[08/28/2025 01:35:34 INFO]: Training loss at epoch 58: 1.3192890882492065
[08/28/2025 01:35:40 INFO]: Training loss at epoch 32: 1.0873739123344421
[08/28/2025 01:35:41 INFO]: Training loss at epoch 14: 1.0665345191955566
[08/28/2025 01:35:51 INFO]: Training loss at epoch 15: 1.1167641878128052
[08/28/2025 01:35:56 INFO]: Training loss at epoch 17: 0.9323037266731262
[08/28/2025 01:36:04 INFO]: Training loss at epoch 3: 1.3071801662445068
[08/28/2025 01:36:05 INFO]: Training loss at epoch 55: 0.9977465569972992
[08/28/2025 01:36:11 INFO]: Running Final Evaluation...
[08/28/2025 01:36:19 INFO]: Training loss at epoch 15: 0.8378849029541016
[08/28/2025 01:36:22 INFO]: Training loss at epoch 33: 1.0690382719039917
[08/28/2025 01:36:34 INFO]: New best epoch, val score: -0.6769416317148741
[08/28/2025 01:36:34 INFO]: Saving model to: unhanged-Shanesha_trial_70/model_best.pth
[08/28/2025 01:36:37 INFO]: Training loss at epoch 18: 1.178459644317627
[08/28/2025 01:36:42 INFO]: New best epoch, val score: -0.6730622867963789
[08/28/2025 01:36:42 INFO]: Saving model to: unhanged-Shanesha_trial_82/model_best.pth
[08/28/2025 01:36:43 INFO]: Training loss at epoch 2: 1.2931315898895264
[08/28/2025 01:36:50 INFO]: Training loss at epoch 4: 0.9490830600261688
[08/28/2025 01:36:59 INFO]: New best epoch, val score: -0.6621483503826819
[08/28/2025 01:36:59 INFO]: Saving model to: unhanged-Shanesha_trial_73/model_best.pth
[08/28/2025 01:37:05 INFO]: Training loss at epoch 34: 1.0581710934638977
[08/28/2025 01:37:11 INFO]: Running Final Evaluation...
[08/28/2025 01:37:14 INFO]: Training loss at epoch 41: 1.2061631977558136
[08/28/2025 01:37:18 INFO]: Training loss at epoch 19: 0.9178567826747894
[08/28/2025 01:37:20 INFO]: Training loss at epoch 47: 1.2113211750984192
[08/28/2025 01:37:27 INFO]: Training accuracy: {
    "score": -1.0090042919882665,
    "rmse": 1.0090042919882665
}
[08/28/2025 01:37:27 INFO]: Val accuracy: {
    "score": -0.6624529614978771,
    "rmse": 0.6624529614978771
}
[08/28/2025 01:37:27 INFO]: Test accuracy: {
    "score": -0.8704761210686214,
    "rmse": 0.8704761210686214
}
[08/28/2025 01:37:27 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_80",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8704761210686214,
        "rmse": 0.8704761210686214
    },
    "train_stats": {
        "score": -1.0090042919882665,
        "rmse": 1.0090042919882665
    },
    "val_stats": {
        "score": -0.6624529614978771,
        "rmse": 0.6624529614978771
    }
}
[08/28/2025 01:37:27 INFO]: Procewss finished for trial unhanged-Shanesha_trial_80
[08/28/2025 01:37:27 INFO]: 
_________________________________________________

[08/28/2025 01:37:27 INFO]: train_net_for_optune.py main() running.
[08/28/2025 01:37:27 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 1.6250854412104263
  attention_dropout: 0.08435046780517051
  ffn_dropout: 0.08435046780517051
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00013812625250823725
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_84

[08/28/2025 01:37:27 INFO]: This ft_transformer has 0.264 million parameters.
[08/28/2025 01:37:27 INFO]: Training will start at epoch 0.
[08/28/2025 01:37:27 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 01:37:35 INFO]: Training stats: {
    "score": -0.9997478139018755,
    "rmse": 0.9997478139018755
}
[08/28/2025 01:37:35 INFO]: Val stats: {
    "score": -0.6709973489754322,
    "rmse": 0.6709973489754322
}
[08/28/2025 01:37:35 INFO]: Test stats: {
    "score": -0.8703979443557084,
    "rmse": 0.8703979443557084
}
[08/28/2025 01:37:35 INFO]: Training stats: {
    "score": -1.0022048418073812,
    "rmse": 1.0022048418073812
}
[08/28/2025 01:37:35 INFO]: Val stats: {
    "score": -0.7065726924568394,
    "rmse": 0.7065726924568394
}
[08/28/2025 01:37:35 INFO]: Test stats: {
    "score": -0.8873873051774709,
    "rmse": 0.8873873051774709
}
[08/28/2025 01:37:37 INFO]: Training loss at epoch 5: 1.1082435846328735
[08/28/2025 01:37:40 INFO]: New best epoch, val score: -0.6709973489754322
[08/28/2025 01:37:40 INFO]: Saving model to: unhanged-Shanesha_trial_82/model_best.pth
[08/28/2025 01:38:05 INFO]: Training accuracy: {
    "score": -1.0038886829181353,
    "rmse": 1.0038886829181353
}
[08/28/2025 01:38:05 INFO]: Val accuracy: {
    "score": -0.6629835876665866,
    "rmse": 0.6629835876665866
}
[08/28/2025 01:38:05 INFO]: Test accuracy: {
    "score": -0.8679687965338078,
    "rmse": 0.8679687965338078
}
[08/28/2025 01:38:05 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_56",
    "best_epoch": 27,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8679687965338078,
        "rmse": 0.8679687965338078
    },
    "train_stats": {
        "score": -1.0038886829181353,
        "rmse": 1.0038886829181353
    },
    "val_stats": {
        "score": -0.6629835876665866,
        "rmse": 0.6629835876665866
    }
}
[08/28/2025 01:38:05 INFO]: Procewss finished for trial unhanged-Shanesha_trial_56
[08/28/2025 01:38:05 INFO]: 
_________________________________________________

[08/28/2025 01:38:05 INFO]: train_net_for_optune.py main() running.
[08/28/2025 01:38:05 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 1.0510331104530621
  attention_dropout: 0.05960707083402225
  ffn_dropout: 0.05960707083402225
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00014408982605774393
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_85

[08/28/2025 01:38:05 INFO]: This ft_transformer has 0.226 million parameters.
[08/28/2025 01:38:05 INFO]: Training will start at epoch 0.
[08/28/2025 01:38:05 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 01:38:07 INFO]: Training loss at epoch 0: 1.1829639673233032
[08/28/2025 01:38:13 INFO]: New best epoch, val score: -0.6891424317754496
[08/28/2025 01:38:13 INFO]: Saving model to: unhanged-Shanesha_trial_84/model_best.pth
[08/28/2025 01:38:18 INFO]: Training loss at epoch 20: 1.1619709432125092
[08/28/2025 01:38:23 INFO]: New best epoch, val score: -0.6701956839167661
[08/28/2025 01:38:23 INFO]: Saving model to: unhanged-Shanesha_trial_82/model_best.pth
[08/28/2025 01:38:25 INFO]: Training loss at epoch 6: 0.8668155372142792
[08/28/2025 01:38:40 INFO]: Training loss at epoch 18: 0.8988363742828369
[08/28/2025 01:38:41 INFO]: Training loss at epoch 22: 1.2826319932937622
[08/28/2025 01:38:43 INFO]: Training loss at epoch 0: 0.9976512491703033
[08/28/2025 01:38:48 INFO]: Training loss at epoch 45: 1.0746709406375885
[08/28/2025 01:38:49 INFO]: New best epoch, val score: -0.6805823185702239
[08/28/2025 01:38:49 INFO]: Saving model to: unhanged-Shanesha_trial_85/model_best.pth
[08/28/2025 01:38:53 INFO]: Training loss at epoch 1: 1.1917428374290466
[08/28/2025 01:38:58 INFO]: New best epoch, val score: -0.6851410446141084
[08/28/2025 01:38:58 INFO]: Saving model to: unhanged-Shanesha_trial_84/model_best.pth
[08/28/2025 01:39:00 INFO]: Training loss at epoch 21: 1.3299649953842163
[08/28/2025 01:39:06 INFO]: New best epoch, val score: -0.6693319448425765
[08/28/2025 01:39:06 INFO]: Saving model to: unhanged-Shanesha_trial_82/model_best.pth
[08/28/2025 01:39:14 INFO]: Training loss at epoch 55: 1.1616969406604767
[08/28/2025 01:39:14 INFO]: Training loss at epoch 7: 1.1501487493515015
[08/28/2025 01:39:26 INFO]: Training loss at epoch 1: 1.1023502945899963
[08/28/2025 01:39:33 INFO]: Training loss at epoch 24: 1.177315592765808
[08/28/2025 01:39:37 INFO]: Training loss at epoch 2: 1.066140741109848
[08/28/2025 01:39:42 INFO]: Training loss at epoch 22: 0.9642131924629211
[08/28/2025 01:39:46 INFO]: New best epoch, val score: -0.6679997014627292
[08/28/2025 01:39:46 INFO]: Saving model to: unhanged-Shanesha_trial_82/model_best.pth
[08/28/2025 01:39:59 INFO]: Training loss at epoch 8: 0.9583034813404083
[08/28/2025 01:40:05 INFO]: Training loss at epoch 2: 0.9024448394775391
[08/28/2025 01:40:10 INFO]: New best epoch, val score: -0.6677347672787015
[08/28/2025 01:40:10 INFO]: Saving model to: unhanged-Shanesha_trial_85/model_best.pth
[08/28/2025 01:40:19 INFO]: Training loss at epoch 3: 1.2889018654823303
[08/28/2025 01:40:20 INFO]: Training loss at epoch 23: 1.0846115052700043
[08/28/2025 01:40:25 INFO]: New best epoch, val score: -0.6669654673719796
[08/28/2025 01:40:25 INFO]: Saving model to: unhanged-Shanesha_trial_82/model_best.pth
[08/28/2025 01:40:43 INFO]: Training loss at epoch 9: 0.9108895063400269
[08/28/2025 01:40:44 INFO]: Training loss at epoch 3: 1.057990700006485
[08/28/2025 01:40:58 INFO]: Training loss at epoch 15: 1.1620924472808838
[08/28/2025 01:40:58 INFO]: Training stats: {
    "score": -1.000011974992673,
    "rmse": 1.000011974992673
}
[08/28/2025 01:40:58 INFO]: Val stats: {
    "score": -0.6852428200796061,
    "rmse": 0.6852428200796061
}
[08/28/2025 01:40:58 INFO]: Test stats: {
    "score": -0.8749813842612474,
    "rmse": 0.8749813842612474
}
[08/28/2025 01:40:59 INFO]: Training loss at epoch 24: 1.0383469760417938
[08/28/2025 01:41:01 INFO]: Training loss at epoch 4: 1.2183531522750854
[08/28/2025 01:41:02 INFO]: Training loss at epoch 15: 1.0570351481437683
[08/28/2025 01:41:04 INFO]: New best epoch, val score: -0.6663742425211443
[08/28/2025 01:41:04 INFO]: Saving model to: unhanged-Shanesha_trial_82/model_best.pth
[08/28/2025 01:41:16 INFO]: Training loss at epoch 51: 0.9846760928630829
[08/28/2025 01:41:23 INFO]: Training loss at epoch 4: 1.2453579902648926
[08/28/2025 01:41:37 INFO]: Training loss at epoch 25: 1.2959570288658142
[08/28/2025 01:41:38 INFO]: Training loss at epoch 16: 1.2322682440280914
[08/28/2025 01:41:41 INFO]: Training loss at epoch 10: 0.9402262270450592
[08/28/2025 01:41:42 INFO]: New best epoch, val score: -0.6663281988270339
[08/28/2025 01:41:42 INFO]: Saving model to: unhanged-Shanesha_trial_82/model_best.pth
[08/28/2025 01:41:44 INFO]: Training loss at epoch 5: 0.8926189243793488
[08/28/2025 01:41:45 INFO]: Training loss at epoch 16: 1.1182119846343994
[08/28/2025 01:42:00 INFO]: Training loss at epoch 3: 2.5864903926849365
[08/28/2025 01:42:03 INFO]: Training loss at epoch 5: 0.951528400182724
[08/28/2025 01:42:16 INFO]: Training loss at epoch 26: 1.0499019622802734
[08/28/2025 01:42:23 INFO]: New best epoch, val score: -0.6611394431293647
[08/28/2025 01:42:23 INFO]: Saving model to: unhanged-Shanesha_trial_70/model_best.pth
[08/28/2025 01:42:26 INFO]: Training loss at epoch 6: 1.0716228485107422
[08/28/2025 01:42:26 INFO]: Training loss at epoch 11: 1.0329142212867737
[08/28/2025 01:42:33 INFO]: Training loss at epoch 90: 0.9449919164180756
[08/28/2025 01:42:37 INFO]: New best epoch, val score: -0.6693175124234029
[08/28/2025 01:42:37 INFO]: Saving model to: unhanged-Shanesha_trial_81/model_best.pth
[08/28/2025 01:42:42 INFO]: Training loss at epoch 6: 1.068362981081009
[08/28/2025 01:42:54 INFO]: Training loss at epoch 27: 0.9564706981182098
[08/28/2025 01:42:59 INFO]: Training loss at epoch 56: 0.9421623051166534
[08/28/2025 01:43:08 INFO]: Training loss at epoch 7: 1.0541670322418213
[08/28/2025 01:43:10 INFO]: Training loss at epoch 12: 0.9107556939125061
[08/28/2025 01:43:17 INFO]: Training loss at epoch 42: 1.2774057686328888
[08/28/2025 01:43:21 INFO]: Training loss at epoch 7: 0.9470895528793335
[08/28/2025 01:43:25 INFO]: Training loss at epoch 46: 1.0801973044872284
[08/28/2025 01:43:33 INFO]: Training loss at epoch 28: 1.1314566135406494
[08/28/2025 01:43:50 INFO]: Training loss at epoch 8: 0.8345226645469666
[08/28/2025 01:43:56 INFO]: Training loss at epoch 13: 1.0315774083137512
[08/28/2025 01:44:02 INFO]: Training loss at epoch 8: 0.8766259849071503
[08/28/2025 01:44:10 INFO]: Training loss at epoch 48: 0.9997448325157166
[08/28/2025 01:44:13 INFO]: Training loss at epoch 29: 0.7380855828523636
[08/28/2025 01:44:14 INFO]: Training loss at epoch 19: 1.0551286935806274
[08/28/2025 01:44:15 INFO]: Training loss at epoch 50: 0.996992290019989
[08/28/2025 01:44:28 INFO]: Training stats: {
    "score": -1.0000740737926124,
    "rmse": 1.0000740737926124
}
[08/28/2025 01:44:28 INFO]: Val stats: {
    "score": -0.6695602366774479,
    "rmse": 0.6695602366774479
}
[08/28/2025 01:44:28 INFO]: Test stats: {
    "score": -0.8699984147437118,
    "rmse": 0.8699984147437118
}
[08/28/2025 01:44:33 INFO]: Training loss at epoch 9: 1.0083214044570923
[08/28/2025 01:44:42 INFO]: Training loss at epoch 14: 0.9139463603496552
[08/28/2025 01:44:43 INFO]: Training loss at epoch 9: 0.8663716912269592
[08/28/2025 01:44:43 INFO]: Training loss at epoch 23: 1.1842251420021057
[08/28/2025 01:44:49 INFO]: Training stats: {
    "score": -1.0020184898140423,
    "rmse": 1.0020184898140423
}
[08/28/2025 01:44:49 INFO]: Val stats: {
    "score": -0.7153379269208977,
    "rmse": 0.7153379269208977
}
[08/28/2025 01:44:49 INFO]: Test stats: {
    "score": -0.8896979396758163,
    "rmse": 0.8896979396758163
}
[08/28/2025 01:44:58 INFO]: Training stats: {
    "score": -0.999939888470964,
    "rmse": 0.999939888470964
}
[08/28/2025 01:44:58 INFO]: Val stats: {
    "score": -0.6960051708164712,
    "rmse": 0.6960051708164712
}
[08/28/2025 01:44:58 INFO]: Test stats: {
    "score": -0.8800618700811093,
    "rmse": 0.8800618700811093
}
[08/28/2025 01:45:09 INFO]: Training loss at epoch 30: 1.0489756762981415
[08/28/2025 01:45:29 INFO]: Training loss at epoch 15: 0.9544836580753326
[08/28/2025 01:45:29 INFO]: New best epoch, val score: -0.6619476569509891
[08/28/2025 01:45:29 INFO]: Saving model to: unhanged-Shanesha_trial_64/model_best.pth
[08/28/2025 01:45:33 INFO]: Training loss at epoch 25: 0.940603107213974
[08/28/2025 01:45:35 INFO]: Training loss at epoch 10: 0.9455497860908508
[08/28/2025 01:45:40 INFO]: Training loss at epoch 10: 0.8968697786331177
[08/28/2025 01:45:51 INFO]: Training loss at epoch 31: 0.9421041905879974
[08/28/2025 01:46:00 INFO]: Training loss at epoch 56: 1.077441155910492
[08/28/2025 01:46:13 INFO]: Training loss at epoch 16: 1.1507856845855713
[08/28/2025 01:46:14 INFO]: Training loss at epoch 16: 1.2888467609882355
[08/28/2025 01:46:16 INFO]: Training loss at epoch 16: 0.9090145528316498
[08/28/2025 01:46:18 INFO]: Training stats: {
    "score": -1.0345997891042797,
    "rmse": 1.0345997891042797
}
[08/28/2025 01:46:18 INFO]: Val stats: {
    "score": -0.7942821360406026,
    "rmse": 0.7942821360406026
}
[08/28/2025 01:46:18 INFO]: Test stats: {
    "score": -0.9435616818707376,
    "rmse": 0.9435616818707376
}
[08/28/2025 01:46:20 INFO]: Training loss at epoch 11: 1.072644293308258
[08/28/2025 01:46:23 INFO]: Training loss at epoch 11: 0.9720833599567413
[08/28/2025 01:46:26 INFO]: New best epoch, val score: -0.6818366622570324
[08/28/2025 01:46:26 INFO]: Saving model to: unhanged-Shanesha_trial_84/model_best.pth
[08/28/2025 01:46:29 INFO]: New best epoch, val score: -0.6644057395612948
[08/28/2025 01:46:29 INFO]: Saving model to: unhanged-Shanesha_trial_85/model_best.pth
[08/28/2025 01:46:32 INFO]: Training loss at epoch 32: 1.1574230194091797
[08/28/2025 01:46:54 INFO]: New best epoch, val score: -0.6649366672960088
[08/28/2025 01:46:54 INFO]: Saving model to: unhanged-Shanesha_trial_75/model_best.pth
[08/28/2025 01:46:57 INFO]: Training loss at epoch 17: 0.9941317439079285
[08/28/2025 01:47:05 INFO]: Training loss at epoch 17: 0.8513875007629395
[08/28/2025 01:47:06 INFO]: Training loss at epoch 12: 1.3949020206928253
[08/28/2025 01:47:07 INFO]: Training loss at epoch 12: 1.0936475098133087
[08/28/2025 01:47:12 INFO]: New best epoch, val score: -0.6611862942279876
[08/28/2025 01:47:12 INFO]: Saving model to: unhanged-Shanesha_trial_85/model_best.pth
[08/28/2025 01:47:13 INFO]: New best epoch, val score: -0.6762704040241434
[08/28/2025 01:47:13 INFO]: Saving model to: unhanged-Shanesha_trial_84/model_best.pth
[08/28/2025 01:47:14 INFO]: Training loss at epoch 33: 1.0658428370952606
[08/28/2025 01:47:22 INFO]: Training loss at epoch 4: 1.2013441920280457
[08/28/2025 01:47:40 INFO]: Training loss at epoch 17: 0.9979200065135956
[08/28/2025 01:47:48 INFO]: Training loss at epoch 13: 1.0562917590141296
[08/28/2025 01:47:52 INFO]: Training loss at epoch 18: 1.1417237222194672
[08/28/2025 01:47:53 INFO]: Training loss at epoch 13: 1.032810091972351
[08/28/2025 01:47:54 INFO]: New best epoch, val score: -0.6607389820528861
[08/28/2025 01:47:54 INFO]: Saving model to: unhanged-Shanesha_trial_85/model_best.pth
[08/28/2025 01:47:56 INFO]: Training loss at epoch 34: 0.9460744559764862
[08/28/2025 01:47:59 INFO]: New best epoch, val score: -0.6739783830582095
[08/28/2025 01:47:59 INFO]: Saving model to: unhanged-Shanesha_trial_84/model_best.pth
[08/28/2025 01:48:20 INFO]: Training loss at epoch 52: 1.234004557132721
[08/28/2025 01:48:28 INFO]: Training loss at epoch 47: 0.8484747409820557
[08/28/2025 01:48:31 INFO]: Training loss at epoch 14: 0.8440949618816376
[08/28/2025 01:48:37 INFO]: Training loss at epoch 35: 0.9201142191886902
[08/28/2025 01:48:38 INFO]: Training loss at epoch 14: 0.9287740886211395
[08/28/2025 01:48:40 INFO]: Training loss at epoch 19: 0.9545830488204956
[08/28/2025 01:48:44 INFO]: New best epoch, val score: -0.6730946568305842
[08/28/2025 01:48:44 INFO]: Saving model to: unhanged-Shanesha_trial_84/model_best.pth
[08/28/2025 01:48:55 INFO]: Training stats: {
    "score": -1.000052424857592,
    "rmse": 1.000052424857592
}
[08/28/2025 01:48:55 INFO]: Val stats: {
    "score": -0.675355548247925,
    "rmse": 0.675355548247925
}
[08/28/2025 01:48:55 INFO]: Test stats: {
    "score": -0.8705618598434712,
    "rmse": 0.8705618598434712
}
[08/28/2025 01:49:10 INFO]: Training loss at epoch 15: 1.207212895154953
[08/28/2025 01:49:16 INFO]: Training loss at epoch 36: 0.8599762916564941
[08/28/2025 01:49:20 INFO]: Training loss at epoch 15: 0.7834473550319672
[08/28/2025 01:49:37 INFO]: Training loss at epoch 43: 0.9257922172546387
[08/28/2025 01:49:39 INFO]: Training loss at epoch 20: 0.9842064678668976
[08/28/2025 01:49:49 INFO]: Training loss at epoch 16: 0.9848144948482513
[08/28/2025 01:49:50 INFO]: Training loss at epoch 91: 0.911880224943161
[08/28/2025 01:49:54 INFO]: Training loss at epoch 37: 0.8850497305393219
[08/28/2025 01:50:01 INFO]: Training loss at epoch 57: 0.9637166261672974
[08/28/2025 01:50:02 INFO]: Training loss at epoch 16: 1.2456312477588654
[08/28/2025 01:50:22 INFO]: Training loss at epoch 21: 0.9244710206985474
[08/28/2025 01:50:28 INFO]: Training loss at epoch 17: 0.9105326533317566
[08/28/2025 01:50:32 INFO]: Training loss at epoch 38: 1.0521072149276733
[08/28/2025 01:50:43 INFO]: Training loss at epoch 17: 1.068293035030365
[08/28/2025 01:51:00 INFO]: Training loss at epoch 24: 0.9480286538600922
[08/28/2025 01:51:06 INFO]: Training loss at epoch 22: 1.0316126346588135
[08/28/2025 01:51:06 INFO]: Training loss at epoch 18: 1.3482699692249298
[08/28/2025 01:51:10 INFO]: Training loss at epoch 49: 0.9700806736946106
[08/28/2025 01:51:10 INFO]: Training loss at epoch 51: 1.0358076691627502
[08/28/2025 01:51:11 INFO]: Training loss at epoch 39: 0.8746852278709412
[08/28/2025 01:51:25 INFO]: Training loss at epoch 18: 1.0707870423793793
[08/28/2025 01:51:25 INFO]: Training stats: {
    "score": -0.9987375084029573,
    "rmse": 0.9987375084029573
}
[08/28/2025 01:51:25 INFO]: Val stats: {
    "score": -0.6910123361028028,
    "rmse": 0.6910123361028028
}
[08/28/2025 01:51:25 INFO]: Test stats: {
    "score": -0.880487762218193,
    "rmse": 0.880487762218193
}
[08/28/2025 01:51:28 INFO]: Training loss at epoch 17: 1.1979807019233704
[08/28/2025 01:51:29 INFO]: Training loss at epoch 17: 1.5529659986495972
[08/28/2025 01:51:41 INFO]: New best epoch, val score: -0.6611418747527936
[08/28/2025 01:51:41 INFO]: Saving model to: unhanged-Shanesha_trial_64/model_best.pth
[08/28/2025 01:51:42 INFO]: Training loss at epoch 26: 0.9684132635593414
[08/28/2025 01:51:46 INFO]: Training loss at epoch 19: 0.9118335843086243
[08/28/2025 01:51:50 INFO]: Training loss at epoch 23: 1.09519761800766
[08/28/2025 01:52:00 INFO]: Training loss at epoch 20: 0.9253206551074982
[08/28/2025 01:52:00 INFO]: Training stats: {
    "score": -0.9988688228641789,
    "rmse": 0.9988688228641789
}
[08/28/2025 01:52:00 INFO]: Val stats: {
    "score": -0.699335638700594,
    "rmse": 0.699335638700594
}
[08/28/2025 01:52:00 INFO]: Test stats: {
    "score": -0.8822935933108805,
    "rmse": 0.8822935933108805
}
[08/28/2025 01:52:02 INFO]: New best epoch, val score: -0.6600299103508341
[08/28/2025 01:52:02 INFO]: Saving model to: unhanged-Shanesha_trial_75/model_best.pth
[08/28/2025 01:52:04 INFO]: Training loss at epoch 40: 0.8953841328620911
[08/28/2025 01:52:05 INFO]: New best epoch, val score: -0.6599308390317952
[08/28/2025 01:52:05 INFO]: Saving model to: unhanged-Shanesha_trial_74/model_best.pth
[08/28/2025 01:52:06 INFO]: Training loss at epoch 18: 1.1067842245101929
[08/28/2025 01:52:08 INFO]: Training loss at epoch 19: 0.8417120277881622
[08/28/2025 01:52:22 INFO]: Training stats: {
    "score": -0.9970493410338601,
    "rmse": 0.9970493410338601
}
[08/28/2025 01:52:22 INFO]: Val stats: {
    "score": -0.6807098509789156,
    "rmse": 0.6807098509789156
}
[08/28/2025 01:52:22 INFO]: Test stats: {
    "score": -0.8705792345551281,
    "rmse": 0.8705792345551281
}
[08/28/2025 01:52:31 INFO]: Training loss at epoch 5: 1.6222899556159973
[08/28/2025 01:52:34 INFO]: Training loss at epoch 24: 1.0258868932724
[08/28/2025 01:52:40 INFO]: Training loss at epoch 20: 0.9321538805961609
[08/28/2025 01:52:41 INFO]: New best epoch, val score: -0.6620706099853914
[08/28/2025 01:52:41 INFO]: Saving model to: unhanged-Shanesha_trial_73/model_best.pth
[08/28/2025 01:52:43 INFO]: Training loss at epoch 41: 1.1239082515239716
[08/28/2025 01:52:49 INFO]: Training loss at epoch 57: 1.0343629717826843
[08/28/2025 01:53:03 INFO]: Training loss at epoch 48: 0.9545792043209076
[08/28/2025 01:53:05 INFO]: Training loss at epoch 20: 1.0288825035095215
[08/28/2025 01:53:18 INFO]: Training loss at epoch 18: 1.1230985224246979
[08/28/2025 01:53:19 INFO]: Training loss at epoch 25: 0.9035585820674896
[08/28/2025 01:53:20 INFO]: Training loss at epoch 21: 0.7811036109924316
[08/28/2025 01:53:23 INFO]: Training loss at epoch 42: 0.8296024203300476
[08/28/2025 01:53:28 INFO]: Training stats: {
    "score": -0.998070219744069,
    "rmse": 0.998070219744069
}
[08/28/2025 01:53:28 INFO]: Val stats: {
    "score": -0.6742823315731267,
    "rmse": 0.6742823315731267
}
[08/28/2025 01:53:28 INFO]: Test stats: {
    "score": -0.8714660118534024,
    "rmse": 0.8714660118534024
}
[08/28/2025 01:53:50 INFO]: Training loss at epoch 21: 0.869057685136795
[08/28/2025 01:54:01 INFO]: Training loss at epoch 22: 0.9060885310173035
[08/28/2025 01:54:03 INFO]: Training loss at epoch 43: 1.10149285197258
[08/28/2025 01:54:07 INFO]: Training loss at epoch 26: 1.070029079914093
[08/28/2025 01:54:35 INFO]: Training loss at epoch 22: 0.7982730865478516
[08/28/2025 01:54:44 INFO]: Training loss at epoch 23: 0.9927203357219696
[08/28/2025 01:54:45 INFO]: Training loss at epoch 44: 1.0805019736289978
[08/28/2025 01:54:55 INFO]: Training loss at epoch 27: 0.8609135448932648
[08/28/2025 01:55:12 INFO]: Training loss at epoch 53: 1.0004410147666931
[08/28/2025 01:55:20 INFO]: Training loss at epoch 23: 1.2928865849971771
[08/28/2025 01:55:25 INFO]: Training loss at epoch 24: 1.171677827835083
[08/28/2025 01:55:26 INFO]: Training loss at epoch 45: 1.1213262677192688
[08/28/2025 01:55:41 INFO]: Training loss at epoch 44: 0.8349809050559998
[08/28/2025 01:55:42 INFO]: Training loss at epoch 28: 1.099926769733429
[08/28/2025 01:56:05 INFO]: Training loss at epoch 24: 1.2555144429206848
[08/28/2025 01:56:08 INFO]: Training loss at epoch 46: 0.8771004974842072
[08/28/2025 01:56:08 INFO]: Training loss at epoch 25: 0.815312922000885
[08/28/2025 01:56:30 INFO]: Training loss at epoch 29: 0.9390376210212708
[08/28/2025 01:56:47 INFO]: Training stats: {
    "score": -1.001316742331906,
    "rmse": 1.001316742331906
}
[08/28/2025 01:56:47 INFO]: Val stats: {
    "score": -0.702161585177738,
    "rmse": 0.702161585177738
}
[08/28/2025 01:56:47 INFO]: Test stats: {
    "score": -0.8842670329015601,
    "rmse": 0.8842670329015601
}
[08/28/2025 01:56:47 INFO]: Training loss at epoch 18: 1.2206982374191284
[08/28/2025 01:56:49 INFO]: Training loss at epoch 47: 0.9802700877189636
[08/28/2025 01:56:49 INFO]: Training loss at epoch 25: 1.1557632684707642
[08/28/2025 01:56:49 INFO]: Training loss at epoch 26: 0.9051309823989868
[08/28/2025 01:56:53 INFO]: Training loss at epoch 18: 1.2060924172401428
[08/28/2025 01:56:55 INFO]: Training loss at epoch 58: 1.0622100234031677
[08/28/2025 01:57:00 INFO]: Training loss at epoch 92: 0.88546422123909
[08/28/2025 01:57:18 INFO]: Training loss at epoch 25: 0.9323205649852753
[08/28/2025 01:57:31 INFO]: Training loss at epoch 48: 1.0268616080284119
[08/28/2025 01:57:32 INFO]: Training loss at epoch 27: 1.000173658132553
[08/28/2025 01:57:33 INFO]: Training loss at epoch 19: 0.9302695393562317
[08/28/2025 01:57:35 INFO]: Training loss at epoch 30: 1.0493495464324951
[08/28/2025 01:57:35 INFO]: Training loss at epoch 26: 1.1140367984771729
[08/28/2025 01:57:55 INFO]: Training loss at epoch 21: 0.8876949548721313
[08/28/2025 01:57:58 INFO]: Training loss at epoch 6: 1.2000847458839417
[08/28/2025 01:57:59 INFO]: Training loss at epoch 27: 1.076599270105362
[08/28/2025 01:58:02 INFO]: New best epoch, val score: -0.6610410468481154
[08/28/2025 01:58:02 INFO]: Saving model to: unhanged-Shanesha_trial_64/model_best.pth
[08/28/2025 01:58:06 INFO]: Training loss at epoch 52: 0.9019739329814911
[08/28/2025 01:58:06 INFO]: Training loss at epoch 49: 1.0326636731624603
[08/28/2025 01:58:11 INFO]: Training loss at epoch 49: 0.9489502310752869
[08/28/2025 01:58:14 INFO]: Training loss at epoch 28: 1.043662041425705
[08/28/2025 01:58:20 INFO]: Training loss at epoch 27: 1.363342672586441
[08/28/2025 01:58:21 INFO]: Training loss at epoch 31: 1.178896814584732
[08/28/2025 01:58:26 INFO]: Training stats: {
    "score": -0.9975901969701149,
    "rmse": 0.9975901969701149
}
[08/28/2025 01:58:26 INFO]: Val stats: {
    "score": -0.6788205928107235,
    "rmse": 0.6788205928107235
}
[08/28/2025 01:58:26 INFO]: Test stats: {
    "score": -0.8744420426076468,
    "rmse": 0.8744420426076468
}
[08/28/2025 01:58:53 INFO]: Training loss at epoch 29: 0.8742816150188446
[08/28/2025 01:59:02 INFO]: Training loss at epoch 28: 1.0105848014354706
[08/28/2025 01:59:05 INFO]: Training loss at epoch 50: 1.0770013630390167
[08/28/2025 01:59:05 INFO]: Training loss at epoch 32: 0.8879344463348389
[08/28/2025 01:59:07 INFO]: Training stats: {
    "score": -0.9965366859975193,
    "rmse": 0.9965366859975193
}
[08/28/2025 01:59:07 INFO]: Val stats: {
    "score": -0.6710683760865662,
    "rmse": 0.6710683760865662
}
[08/28/2025 01:59:07 INFO]: Test stats: {
    "score": -0.8687692281499557,
    "rmse": 0.8687692281499557
}
[08/28/2025 01:59:11 INFO]: Running Final Evaluation...
[08/28/2025 01:59:18 INFO]: Training stats: {
    "score": -0.9998838000678958,
    "rmse": 0.9998838000678958
}
[08/28/2025 01:59:18 INFO]: Val stats: {
    "score": -0.6720075240253328,
    "rmse": 0.6720075240253328
}
[08/28/2025 01:59:18 INFO]: Test stats: {
    "score": -0.8699891947063284,
    "rmse": 0.8699891947063284
}
[08/28/2025 01:59:18 INFO]: Training loss at epoch 19: 0.919343888759613
[08/28/2025 01:59:26 INFO]: Training accuracy: {
    "score": -1.0014672561343299,
    "rmse": 1.0014672561343299
}
[08/28/2025 01:59:26 INFO]: Val accuracy: {
    "score": -0.6739423461134504,
    "rmse": 0.6739423461134504
}
[08/28/2025 01:59:26 INFO]: Test accuracy: {
    "score": -0.8685425114050889,
    "rmse": 0.8685425114050889
}
[08/28/2025 01:59:26 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_83",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8685425114050889,
        "rmse": 0.8685425114050889
    },
    "train_stats": {
        "score": -1.0014672561343299,
        "rmse": 1.0014672561343299
    },
    "val_stats": {
        "score": -0.6739423461134504,
        "rmse": 0.6739423461134504
    }
}
[08/28/2025 01:59:26 INFO]: Procewss finished for trial unhanged-Shanesha_trial_83
[08/28/2025 01:59:26 INFO]: 
_________________________________________________

[08/28/2025 01:59:26 INFO]: train_net_for_optune.py main() running.
[08/28/2025 01:59:26 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 4
  d_ffn_factor: 1.0471337784234702
  attention_dropout: 0.03742974110529643
  ffn_dropout: 0.03742974110529643
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 8.5965643959655e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_86

[08/28/2025 01:59:26 INFO]: This ft_transformer has 0.226 million parameters.
[08/28/2025 01:59:26 INFO]: Training will start at epoch 0.
[08/28/2025 01:59:26 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 01:59:41 INFO]: Training stats: {
    "score": -1.0005874220208204,
    "rmse": 1.0005874220208204
}
[08/28/2025 01:59:41 INFO]: Val stats: {
    "score": -0.6624629646095629,
    "rmse": 0.6624629646095629
}
[08/28/2025 01:59:41 INFO]: Test stats: {
    "score": -0.8687441204120775,
    "rmse": 0.8687441204120775
}
[08/28/2025 01:59:43 INFO]: Training loss at epoch 51: 0.8559502363204956
[08/28/2025 01:59:43 INFO]: Training loss at epoch 29: 0.8958932161331177
[08/28/2025 01:59:46 INFO]: Training loss at epoch 30: 1.1182017922401428
[08/28/2025 01:59:50 INFO]: Training loss at epoch 58: 1.3918842375278473
[08/28/2025 01:59:58 INFO]: Training stats: {
    "score": -0.9969199991383684,
    "rmse": 0.9969199991383684
}
[08/28/2025 01:59:58 INFO]: Val stats: {
    "score": -0.6824685902226383,
    "rmse": 0.6824685902226383
}
[08/28/2025 01:59:58 INFO]: Test stats: {
    "score": -0.8716462682082801,
    "rmse": 0.8716462682082801
}
[08/28/2025 02:00:01 INFO]: Training loss at epoch 0: 1.0747683346271515
[08/28/2025 02:00:06 INFO]: New best epoch, val score: -0.7120039633145379
[08/28/2025 02:00:06 INFO]: Saving model to: unhanged-Shanesha_trial_86/model_best.pth
[08/28/2025 02:00:21 INFO]: Training loss at epoch 52: 1.0155873894691467
[08/28/2025 02:00:25 INFO]: Training loss at epoch 31: 1.0129461884498596
[08/28/2025 02:00:28 INFO]: Training loss at epoch 50: 0.7712936997413635
[08/28/2025 02:00:40 INFO]: Training loss at epoch 30: 0.8294837772846222
[08/28/2025 02:00:41 INFO]: Training loss at epoch 1: 0.8604978322982788
[08/28/2025 02:00:45 INFO]: New best epoch, val score: -0.6675847441994642
[08/28/2025 02:00:45 INFO]: Saving model to: unhanged-Shanesha_trial_86/model_best.pth
[08/28/2025 02:00:59 INFO]: Training loss at epoch 53: 0.8936856687068939
[08/28/2025 02:01:03 INFO]: Training loss at epoch 32: 0.8871150314807892
[08/28/2025 02:01:08 INFO]: Training stats: {
    "score": -1.0227023543762048,
    "rmse": 1.0227023543762048
}
[08/28/2025 02:01:08 INFO]: Val stats: {
    "score": -0.7614111323616792,
    "rmse": 0.7614111323616792
}
[08/28/2025 02:01:08 INFO]: Test stats: {
    "score": -0.9231590875720495,
    "rmse": 0.9231590875720495
}
[08/28/2025 02:01:20 INFO]: Training loss at epoch 2: 0.8671862483024597
[08/28/2025 02:01:21 INFO]: Training loss at epoch 31: 1.0505614280700684
[08/28/2025 02:01:37 INFO]: Training loss at epoch 54: 0.920398473739624
[08/28/2025 02:01:42 INFO]: Training loss at epoch 33: 0.9681491553783417
[08/28/2025 02:01:45 INFO]: Training loss at epoch 45: 0.9718289375305176
[08/28/2025 02:01:53 INFO]: Training loss at epoch 19: 1.0832387804985046
[08/28/2025 02:01:59 INFO]: Training loss at epoch 3: 0.9119874835014343
[08/28/2025 02:02:01 INFO]: Training loss at epoch 19: 1.2464342713356018
[08/28/2025 02:02:03 INFO]: Training loss at epoch 32: 0.9998742938041687
[08/28/2025 02:02:07 INFO]: Training loss at epoch 54: 0.8136592209339142
[08/28/2025 02:02:08 INFO]: New best epoch, val score: -0.6714546104812339
[08/28/2025 02:02:08 INFO]: Saving model to: unhanged-Shanesha_trial_84/model_best.pth
[08/28/2025 02:02:16 INFO]: Training loss at epoch 55: 0.8009423911571503
[08/28/2025 02:02:22 INFO]: Training loss at epoch 34: 0.8589915931224823
[08/28/2025 02:02:41 INFO]: Training loss at epoch 4: 0.8723042607307434
[08/28/2025 02:02:47 INFO]: Training loss at epoch 33: 0.9344876408576965
[08/28/2025 02:02:52 INFO]: New best epoch, val score: -0.669931744082419
[08/28/2025 02:02:52 INFO]: Saving model to: unhanged-Shanesha_trial_84/model_best.pth
[08/28/2025 02:02:56 INFO]: Training loss at epoch 56: 0.7948021292686462
[08/28/2025 02:03:01 INFO]: Running Final Evaluation...
[08/28/2025 02:03:03 INFO]: Training loss at epoch 35: 1.0018662214279175
[08/28/2025 02:03:06 INFO]: Training loss at epoch 7: 1.1887400150299072
[08/28/2025 02:03:17 INFO]: Training accuracy: {
    "score": -1.0015110144034722,
    "rmse": 1.0015110144034722
}
[08/28/2025 02:03:17 INFO]: Val accuracy: {
    "score": -0.6663281988270339,
    "rmse": 0.6663281988270339
}
[08/28/2025 02:03:17 INFO]: Test accuracy: {
    "score": -0.869069283970653,
    "rmse": 0.869069283970653
}
[08/28/2025 02:03:17 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_82",
    "best_epoch": 25,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.869069283970653,
        "rmse": 0.869069283970653
    },
    "train_stats": {
        "score": -1.0015110144034722,
        "rmse": 1.0015110144034722
    },
    "val_stats": {
        "score": -0.6663281988270339,
        "rmse": 0.6663281988270339
    }
}
[08/28/2025 02:03:17 INFO]: Procewss finished for trial unhanged-Shanesha_trial_82
[08/28/2025 02:03:17 INFO]: 
_________________________________________________

[08/28/2025 02:03:17 INFO]: train_net_for_optune.py main() running.
[08/28/2025 02:03:17 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 1.6868890108480972
  attention_dropout: 0.08650338049096962
  ffn_dropout: 0.08650338049096962
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 9.135576360751954e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_87

[08/28/2025 02:03:17 INFO]: This ft_transformer has 7.686 million parameters.
[08/28/2025 02:03:17 INFO]: Training will start at epoch 0.
[08/28/2025 02:03:17 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 02:03:21 INFO]: Training loss at epoch 26: 1.1190758645534515
[08/28/2025 02:03:23 INFO]: Training loss at epoch 5: 0.849334180355072
[08/28/2025 02:03:32 INFO]: Training loss at epoch 22: 1.4116272032260895
[08/28/2025 02:03:32 INFO]: Training loss at epoch 34: 0.9135744869709015
[08/28/2025 02:03:38 INFO]: New best epoch, val score: -0.6699211659845571
[08/28/2025 02:03:38 INFO]: Saving model to: unhanged-Shanesha_trial_84/model_best.pth
[08/28/2025 02:03:41 INFO]: Training stats: {
    "score": -1.0279569194213358,
    "rmse": 1.0279569194213358
}
[08/28/2025 02:03:41 INFO]: Val stats: {
    "score": -0.6642346539788588,
    "rmse": 0.6642346539788588
}
[08/28/2025 02:03:41 INFO]: Test stats: {
    "score": -0.8802725422256923,
    "rmse": 0.8802725422256923
}
[08/28/2025 02:03:43 INFO]: Training loss at epoch 59: 0.9670973122119904
[08/28/2025 02:03:44 INFO]: Training loss at epoch 36: 0.9251086115837097
[08/28/2025 02:03:53 INFO]: Training stats: {
    "score": -1.0282679707988958,
    "rmse": 1.0282679707988958
}
[08/28/2025 02:03:53 INFO]: Val stats: {
    "score": -0.7769250427154836,
    "rmse": 0.7769250427154836
}
[08/28/2025 02:03:53 INFO]: Test stats: {
    "score": -0.9344752870672777,
    "rmse": 0.9344752870672777
}
[08/28/2025 02:04:01 INFO]: Training loss at epoch 28: 0.9320390820503235
[08/28/2025 02:04:04 INFO]: Training loss at epoch 93: 0.9621753394603729
[08/28/2025 02:04:05 INFO]: Training loss at epoch 6: 0.8958129286766052
[08/28/2025 02:04:18 INFO]: Training loss at epoch 35: 0.9359011054039001
[08/28/2025 02:04:28 INFO]: Training loss at epoch 50: 1.0261544585227966
[08/28/2025 02:04:30 INFO]: Training loss at epoch 20: 1.1497173309326172
[08/28/2025 02:04:30 INFO]: Training loss at epoch 37: 0.9454092979431152
[08/28/2025 02:04:48 INFO]: Training loss at epoch 7: 0.8651838898658752
[08/28/2025 02:04:50 INFO]: Training loss at epoch 53: 1.0728697180747986
[08/28/2025 02:05:04 INFO]: Training loss at epoch 36: 0.8577822148799896
[08/28/2025 02:05:12 INFO]: Training loss at epoch 38: 0.8993819355964661
[08/28/2025 02:05:31 INFO]: Training loss at epoch 8: 1.246753841638565
[08/28/2025 02:05:49 INFO]: Training loss at epoch 37: 1.0787982940673828
[08/28/2025 02:05:54 INFO]: Training loss at epoch 39: 1.1490847170352936
[08/28/2025 02:06:09 INFO]: Training stats: {
    "score": -0.9946287352244824,
    "rmse": 0.9946287352244824
}
[08/28/2025 02:06:09 INFO]: Val stats: {
    "score": -0.6763023748347303,
    "rmse": 0.6763023748347303
}
[08/28/2025 02:06:09 INFO]: Test stats: {
    "score": -0.8706802740383546,
    "rmse": 0.8706802740383546
}
[08/28/2025 02:06:15 INFO]: Training loss at epoch 9: 1.2828201949596405
[08/28/2025 02:06:15 INFO]: Training stats: {
    "score": -1.0023309017486164,
    "rmse": 1.0023309017486164
}
[08/28/2025 02:06:15 INFO]: Val stats: {
    "score": -0.66487505566259,
    "rmse": 0.66487505566259
}
[08/28/2025 02:06:15 INFO]: Test stats: {
    "score": -0.8684715523398211,
    "rmse": 0.8684715523398211
}
[08/28/2025 02:06:30 INFO]: Training stats: {
    "score": -1.0220873141222433,
    "rmse": 1.0220873141222433
}
[08/28/2025 02:06:30 INFO]: Val stats: {
    "score": -0.7700927882107462,
    "rmse": 0.7700927882107462
}
[08/28/2025 02:06:30 INFO]: Test stats: {
    "score": -0.9261225463784177,
    "rmse": 0.9261225463784177
}
[08/28/2025 02:06:35 INFO]: Training loss at epoch 38: 0.9824924767017365
[08/28/2025 02:06:51 INFO]: Training loss at epoch 59: 1.0094720721244812
[08/28/2025 02:06:52 INFO]: Training loss at epoch 40: 0.8486910164356232
[08/28/2025 02:07:09 INFO]: Training loss at epoch 20: 0.9390659034252167
[08/28/2025 02:07:12 INFO]: Training loss at epoch 10: 1.027013123035431
[08/28/2025 02:07:20 INFO]: Training loss at epoch 39: 1.0017056167125702
[08/28/2025 02:07:29 INFO]: Training loss at epoch 0: 0.9278164207935333
[08/28/2025 02:07:30 INFO]: Training loss at epoch 41: 0.7967517673969269
[08/28/2025 02:07:31 INFO]: Training loss at epoch 51: 0.9810035824775696
[08/28/2025 02:07:35 INFO]: Training stats: {
    "score": -0.9964801930052091,
    "rmse": 0.9964801930052091
}
[08/28/2025 02:07:35 INFO]: Val stats: {
    "score": -0.6866897391764165,
    "rmse": 0.6866897391764165
}
[08/28/2025 02:07:35 INFO]: Test stats: {
    "score": -0.8737597351001537,
    "rmse": 0.8737597351001537
}
[08/28/2025 02:07:51 INFO]: Training loss at epoch 11: 0.8686201572418213
[08/28/2025 02:07:59 INFO]: New best epoch, val score: -0.6646638330576815
[08/28/2025 02:07:59 INFO]: Saving model to: unhanged-Shanesha_trial_87/model_best.pth
[08/28/2025 02:08:03 INFO]: Training loss at epoch 46: 1.2472930550575256
[08/28/2025 02:08:09 INFO]: Training loss at epoch 42: 0.9339151084423065
[08/28/2025 02:08:16 INFO]: Training loss at epoch 40: 0.8620677590370178
[08/28/2025 02:08:31 INFO]: Training loss at epoch 12: 1.087959110736847
[08/28/2025 02:08:31 INFO]: Training loss at epoch 8: 1.5582728385925293
[08/28/2025 02:08:44 INFO]: New best epoch, val score: -0.6606263283812059
[08/28/2025 02:08:44 INFO]: Saving model to: unhanged-Shanesha_trial_59/model_best.pth
[08/28/2025 02:08:48 INFO]: Training loss at epoch 43: 0.7679347693920135
[08/28/2025 02:08:57 INFO]: Training loss at epoch 41: 0.7906610667705536
[08/28/2025 02:09:01 INFO]: Training loss at epoch 20: 1.3484770059585571
[08/28/2025 02:09:05 INFO]: Training stats: {
    "score": -1.0020747541027841,
    "rmse": 1.0020747541027841
}
[08/28/2025 02:09:05 INFO]: Val stats: {
    "score": -0.6662779331504697,
    "rmse": 0.6662779331504697
}
[08/28/2025 02:09:05 INFO]: Test stats: {
    "score": -0.8686466712763788,
    "rmse": 0.8686466712763788
}
[08/28/2025 02:09:11 INFO]: Training loss at epoch 13: 1.1453689336776733
[08/28/2025 02:09:12 INFO]: Training loss at epoch 20: 0.980637401342392
[08/28/2025 02:09:13 INFO]: Training loss at epoch 55: 0.8739831149578094
[08/28/2025 02:09:16 INFO]: New best epoch, val score: -0.6671397615672527
[08/28/2025 02:09:16 INFO]: Saving model to: unhanged-Shanesha_trial_86/model_best.pth
[08/28/2025 02:09:19 INFO]: Training loss at epoch 51: 1.0974555313587189
[08/28/2025 02:09:22 INFO]: Training loss at epoch 23: 0.9501802921295166
[08/28/2025 02:09:27 INFO]: Training loss at epoch 44: 0.8355683088302612
[08/28/2025 02:09:32 INFO]: Running Final Evaluation...
[08/28/2025 02:09:37 INFO]: Training loss at epoch 27: 0.9865216016769409
[08/28/2025 02:09:39 INFO]: Training loss at epoch 42: 1.0768879055976868
[08/28/2025 02:09:44 INFO]: Training loss at epoch 21: 1.174700289964676
[08/28/2025 02:09:46 INFO]: Training accuracy: {
    "score": -1.005934608187498,
    "rmse": 1.005934608187498
}
[08/28/2025 02:09:46 INFO]: Val accuracy: {
    "score": -0.6607389820528861,
    "rmse": 0.6607389820528861
}
[08/28/2025 02:09:46 INFO]: Test accuracy: {
    "score": -0.866589889218976,
    "rmse": 0.866589889218976
}
[08/28/2025 02:09:46 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_85",
    "best_epoch": 13,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.866589889218976,
        "rmse": 0.866589889218976
    },
    "train_stats": {
        "score": -1.005934608187498,
        "rmse": 1.005934608187498
    },
    "val_stats": {
        "score": -0.6607389820528861,
        "rmse": 0.6607389820528861
    }
}
[08/28/2025 02:09:46 INFO]: Procewss finished for trial unhanged-Shanesha_trial_85
[08/28/2025 02:09:46 INFO]: 
_________________________________________________

[08/28/2025 02:09:46 INFO]: train_net_for_optune.py main() running.
[08/28/2025 02:09:46 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 1.7414397188069182
  attention_dropout: 0.08240924402244057
  ffn_dropout: 0.08240924402244057
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 8.572896457518794e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_88

[08/28/2025 02:09:46 INFO]: This ft_transformer has 1.280 million parameters.
[08/28/2025 02:09:46 INFO]: Training will start at epoch 0.
[08/28/2025 02:09:46 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 02:09:50 INFO]: Training loss at epoch 14: 1.03325355052948
[08/28/2025 02:09:55 INFO]: New best epoch, val score: -0.6640999841611991
[08/28/2025 02:09:55 INFO]: Saving model to: unhanged-Shanesha_trial_86/model_best.pth
[08/28/2025 02:10:09 INFO]: Training loss at epoch 29: 1.010647475719452
[08/28/2025 02:10:21 INFO]: Training loss at epoch 43: 0.9999028146266937
[08/28/2025 02:10:30 INFO]: Training loss at epoch 15: 0.9370337426662445
[08/28/2025 02:10:35 INFO]: New best epoch, val score: -0.6635036788883885
[08/28/2025 02:10:35 INFO]: Saving model to: unhanged-Shanesha_trial_86/model_best.pth
[08/28/2025 02:11:02 INFO]: Training loss at epoch 44: 1.1068148612976074
[08/28/2025 02:11:10 INFO]: Training loss at epoch 16: 1.027248740196228
[08/28/2025 02:11:11 INFO]: Training loss at epoch 94: 0.8759163618087769
[08/28/2025 02:11:32 INFO]: Training loss at epoch 0: 1.0843204259872437
[08/28/2025 02:11:33 INFO]: Training loss at epoch 54: 1.0898518860340118
[08/28/2025 02:11:46 INFO]: Training loss at epoch 45: 0.8685469031333923
[08/28/2025 02:11:48 INFO]: New best epoch, val score: -0.7184628070178654
[08/28/2025 02:11:48 INFO]: Saving model to: unhanged-Shanesha_trial_88/model_best.pth
[08/28/2025 02:11:51 INFO]: Training loss at epoch 1: 1.6471032500267029
[08/28/2025 02:11:52 INFO]: Training loss at epoch 17: 0.971367210149765
[08/28/2025 02:12:12 INFO]: Training stats: {
    "score": -1.0064769010943257,
    "rmse": 1.0064769010943257
}
[08/28/2025 02:12:12 INFO]: Val stats: {
    "score": -0.7178141343914493,
    "rmse": 0.7178141343914493
}
[08/28/2025 02:12:12 INFO]: Test stats: {
    "score": -0.894844472167014,
    "rmse": 0.894844472167014
}
[08/28/2025 02:12:31 INFO]: Training loss at epoch 46: 0.8389317393302917
[08/28/2025 02:12:34 INFO]: Training loss at epoch 18: 1.0451422333717346
[08/28/2025 02:12:47 INFO]: Training loss at epoch 21: 1.0740180611610413
[08/28/2025 02:13:02 INFO]: Training loss at epoch 60: 0.9749990403652191
[08/28/2025 02:13:15 INFO]: Training loss at epoch 47: 0.9853712022304535
[08/28/2025 02:13:17 INFO]: Training loss at epoch 19: 1.2306550741195679
[08/28/2025 02:13:33 INFO]: Training stats: {
    "score": -0.9991507784377052,
    "rmse": 0.9991507784377052
}
[08/28/2025 02:13:33 INFO]: Val stats: {
    "score": -0.672042998890146,
    "rmse": 0.672042998890146
}
[08/28/2025 02:13:33 INFO]: Test stats: {
    "score": -0.8704207317429321,
    "rmse": 0.8704207317429321
}
[08/28/2025 02:13:40 INFO]: Training loss at epoch 1: 1.1620819568634033
[08/28/2025 02:13:45 INFO]: Training loss at epoch 9: 1.2650604844093323
[08/28/2025 02:14:00 INFO]: Training loss at epoch 48: 1.0825337171554565
[08/28/2025 02:14:05 INFO]: New best epoch, val score: -0.6697297493746854
[08/28/2025 02:14:05 INFO]: Saving model to: unhanged-Shanesha_trial_84/model_best.pth
[08/28/2025 02:14:10 INFO]: Training loss at epoch 47: 1.0735920667648315
[08/28/2025 02:14:11 INFO]: Training loss at epoch 52: 1.017620861530304
[08/28/2025 02:14:16 INFO]: Training loss at epoch 20: 0.8430779576301575
[08/28/2025 02:14:16 INFO]: Training loss at epoch 21: 0.8733658790588379
[08/28/2025 02:14:24 INFO]: Training loss at epoch 52: 0.9043731093406677
[08/28/2025 02:14:32 INFO]: Training loss at epoch 21: 1.0865668654441833
[08/28/2025 02:14:45 INFO]: Training loss at epoch 49: 0.9233678877353668
[08/28/2025 02:14:56 INFO]: New best epoch, val score: -0.6606116784048846
[08/28/2025 02:14:56 INFO]: Saving model to: unhanged-Shanesha_trial_59/model_best.pth
[08/28/2025 02:14:59 INFO]: Training loss at epoch 21: 1.033251941204071
[08/28/2025 02:15:01 INFO]: Training stats: {
    "score": -1.0001553035141277,
    "rmse": 1.0001553035141277
}
[08/28/2025 02:15:01 INFO]: Val stats: {
    "score": -0.6704422818161839,
    "rmse": 0.6704422818161839
}
[08/28/2025 02:15:01 INFO]: Test stats: {
    "score": -0.8672196457041701,
    "rmse": 0.8672196457041701
}
[08/28/2025 02:15:06 INFO]: Training loss at epoch 22: 1.0401102900505066
[08/28/2025 02:15:13 INFO]: Training loss at epoch 24: 0.7966460883617401
[08/28/2025 02:15:42 INFO]: Training loss at epoch 22: 0.786495566368103
[08/28/2025 02:15:43 INFO]: Training stats: {
    "score": -1.0872158894694346,
    "rmse": 1.0872158894694346
}
[08/28/2025 02:15:43 INFO]: Val stats: {
    "score": -0.8874390399134496,
    "rmse": 0.8874390399134496
}
[08/28/2025 02:15:43 INFO]: Test stats: {
    "score": -1.0156983835644042,
    "rmse": 1.0156983835644042
}
[08/28/2025 02:15:46 INFO]: Training loss at epoch 50: 0.8954896032810211
[08/28/2025 02:15:50 INFO]: Training loss at epoch 2: 0.932325541973114
[08/28/2025 02:15:59 INFO]: Training loss at epoch 28: 0.8683038651943207
[08/28/2025 02:16:05 INFO]: Training loss at epoch 60: 0.9374482929706573
[08/28/2025 02:16:18 INFO]: Training loss at epoch 56: 1.1222440004348755
[08/28/2025 02:16:24 INFO]: Training loss at epoch 23: 0.9521645903587341
[08/28/2025 02:16:29 INFO]: Training loss at epoch 51: 1.005756139755249
[08/28/2025 02:16:36 INFO]: Training loss at epoch 2: 1.3014910817146301
[08/28/2025 02:16:52 INFO]: New best epoch, val score: -0.6611458139432947
[08/28/2025 02:16:52 INFO]: Saving model to: unhanged-Shanesha_trial_43/model_best.pth
[08/28/2025 02:17:04 INFO]: Training loss at epoch 24: 0.8922956585884094
[08/28/2025 02:17:11 INFO]: Training loss at epoch 52: 0.9771426916122437
[08/28/2025 02:17:43 INFO]: Training loss at epoch 25: 1.070467472076416
[08/28/2025 02:17:50 INFO]: Training loss at epoch 3: 1.314536452293396
[08/28/2025 02:17:52 INFO]: Training loss at epoch 53: 1.0879679024219513
[08/28/2025 02:18:23 INFO]: Training loss at epoch 26: 0.9399266242980957
[08/28/2025 02:18:26 INFO]: Training loss at epoch 30: 1.054764449596405
[08/28/2025 02:18:27 INFO]: Training loss at epoch 95: 1.027625322341919
[08/28/2025 02:18:27 INFO]: Training loss at epoch 55: 1.1269260942935944
[08/28/2025 02:18:34 INFO]: Training loss at epoch 54: 1.0833616852760315
[08/28/2025 02:18:39 INFO]: Training loss at epoch 22: 0.9978820383548737
[08/28/2025 02:18:57 INFO]: Training loss at epoch 53: 1.145960807800293
[08/28/2025 02:19:02 INFO]: Training loss at epoch 27: 0.959178626537323
[08/28/2025 02:19:15 INFO]: Training loss at epoch 55: 1.096851497888565
[08/28/2025 02:19:25 INFO]: Training loss at epoch 22: 1.031864047050476
[08/28/2025 02:19:41 INFO]: Training loss at epoch 28: 0.8799017667770386
[08/28/2025 02:19:42 INFO]: Training loss at epoch 22: 1.0478721261024475
[08/28/2025 02:19:48 INFO]: Training loss at epoch 4: 1.1146392822265625
[08/28/2025 02:19:53 INFO]: Training loss at epoch 61: 0.983541339635849
[08/28/2025 02:19:56 INFO]: Training loss at epoch 56: 0.8219854533672333
[08/28/2025 02:20:03 INFO]: New best epoch, val score: -0.6649849121076344
[08/28/2025 02:20:03 INFO]: Saving model to: unhanged-Shanesha_trial_88/model_best.pth
[08/28/2025 02:20:10 INFO]: Training loss at epoch 23: 1.060562789440155
[08/28/2025 02:20:11 INFO]: Training loss at epoch 48: 1.1865027844905853
[08/28/2025 02:20:21 INFO]: Training loss at epoch 29: 1.0350875854492188
[08/28/2025 02:20:36 INFO]: Training stats: {
    "score": -0.9970062434818961,
    "rmse": 0.9970062434818961
}
[08/28/2025 02:20:36 INFO]: Val stats: {
    "score": -0.6885567125531803,
    "rmse": 0.6885567125531803
}
[08/28/2025 02:20:36 INFO]: Test stats: {
    "score": -0.8770534247329854,
    "rmse": 0.8770534247329854
}
[08/28/2025 02:20:38 INFO]: Training loss at epoch 57: 0.9116601645946503
[08/28/2025 02:20:45 INFO]: Training loss at epoch 25: 0.9789076447486877
[08/28/2025 02:20:45 INFO]: Training loss at epoch 10: 1.1854812502861023
[08/28/2025 02:20:58 INFO]: Training loss at epoch 3: 1.5484777688980103
[08/28/2025 02:21:11 INFO]: Training loss at epoch 53: 1.2164912521839142
[08/28/2025 02:21:20 INFO]: Training loss at epoch 30: 1.084726870059967
[08/28/2025 02:21:24 INFO]: Training loss at epoch 58: 1.0586496591567993
[08/28/2025 02:21:53 INFO]: Training loss at epoch 5: 0.9241392314434052
[08/28/2025 02:21:58 INFO]: Training loss at epoch 29: 1.0393354296684265
[08/28/2025 02:22:03 INFO]: Training loss at epoch 31: 1.0342077016830444
[08/28/2025 02:22:08 INFO]: New best epoch, val score: -0.6643169610396759
[08/28/2025 02:22:08 INFO]: Saving model to: unhanged-Shanesha_trial_88/model_best.pth
[08/28/2025 02:22:09 INFO]: Training loss at epoch 59: 1.228862226009369
[08/28/2025 02:22:25 INFO]: Training stats: {
    "score": -0.9956364404679693,
    "rmse": 0.9956364404679693
}
[08/28/2025 02:22:25 INFO]: Val stats: {
    "score": -0.6835286748831264,
    "rmse": 0.6835286748831264
}
[08/28/2025 02:22:25 INFO]: Test stats: {
    "score": -0.8716793758855682,
    "rmse": 0.8716793758855682
}
[08/28/2025 02:22:46 INFO]: Training loss at epoch 32: 0.7759345769882202
[08/28/2025 02:22:50 INFO]: Training loss at epoch 61: 0.9345628619194031
[08/28/2025 02:23:08 INFO]: Training loss at epoch 57: 0.890734076499939
[08/28/2025 02:23:10 INFO]: Training loss at epoch 60: 1.1056997179985046
[08/28/2025 02:23:29 INFO]: Training loss at epoch 33: 0.8189380168914795
[08/28/2025 02:23:50 INFO]: Training loss at epoch 54: 0.976277083158493
[08/28/2025 02:23:54 INFO]: Training loss at epoch 61: 0.8724558353424072
[08/28/2025 02:24:01 INFO]: Training loss at epoch 6: 1.1717459559440613
[08/28/2025 02:24:10 INFO]: Training stats: {
    "score": -0.9999671774640735,
    "rmse": 0.9999671774640735
}
[08/28/2025 02:24:10 INFO]: Val stats: {
    "score": -0.6827720905475285,
    "rmse": 0.6827720905475285
}
[08/28/2025 02:24:10 INFO]: Test stats: {
    "score": -0.8751331352497789,
    "rmse": 0.8751331352497789
}
[08/28/2025 02:24:13 INFO]: Training loss at epoch 34: 1.091777890920639
[08/28/2025 02:24:17 INFO]: New best epoch, val score: -0.6637750509549794
[08/28/2025 02:24:17 INFO]: Saving model to: unhanged-Shanesha_trial_88/model_best.pth
[08/28/2025 02:24:34 INFO]: Training loss at epoch 23: 1.0288918018341064
[08/28/2025 02:24:39 INFO]: Training loss at epoch 62: 1.1629634201526642
[08/28/2025 02:24:40 INFO]: Training loss at epoch 31: 1.2381738722324371
[08/28/2025 02:24:47 INFO]: Training loss at epoch 23: 1.1625486612319946
[08/28/2025 02:24:56 INFO]: Training loss at epoch 35: 0.9823886752128601
[08/28/2025 02:25:09 INFO]: Training loss at epoch 23: 0.9080523550510406
[08/28/2025 02:25:21 INFO]: Training loss at epoch 56: 1.0161179304122925
[08/28/2025 02:25:23 INFO]: Training loss at epoch 63: 0.8589436113834381
[08/28/2025 02:25:39 INFO]: Training loss at epoch 24: 1.0853395462036133
[08/28/2025 02:25:39 INFO]: Training loss at epoch 36: 1.1034706830978394
[08/28/2025 02:25:43 INFO]: Training loss at epoch 96: 0.8721984326839447
[08/28/2025 02:25:44 INFO]: Training loss at epoch 4: 1.14024019241333
[08/28/2025 02:26:06 INFO]: Training loss at epoch 64: 0.9912282526493073
[08/28/2025 02:26:07 INFO]: Training loss at epoch 7: 1.0793415904045105
[08/28/2025 02:26:14 INFO]: Training loss at epoch 11: 1.1748493313789368
[08/28/2025 02:26:20 INFO]: Training loss at epoch 37: 0.9133964776992798
[08/28/2025 02:26:22 INFO]: New best epoch, val score: -0.6634890095020467
[08/28/2025 02:26:22 INFO]: Saving model to: unhanged-Shanesha_trial_88/model_best.pth
[08/28/2025 02:26:30 INFO]: Training loss at epoch 49: 1.1065918505191803
[08/28/2025 02:26:42 INFO]: Training loss at epoch 26: 0.9303965866565704
[08/28/2025 02:26:47 INFO]: Training loss at epoch 65: 0.9201344847679138
[08/28/2025 02:26:54 INFO]: Training loss at epoch 62: 1.008128434419632
[08/28/2025 02:27:01 INFO]: Training loss at epoch 38: 1.0406975448131561
[08/28/2025 02:27:28 INFO]: Training loss at epoch 66: 0.9316838085651398
[08/28/2025 02:27:41 INFO]: Training loss at epoch 39: 1.0510810613632202
[08/28/2025 02:27:41 INFO]: Running Final Evaluation...
[08/28/2025 02:27:55 INFO]: Training stats: {
    "score": -0.9964973904584011,
    "rmse": 0.9964973904584011
}
[08/28/2025 02:27:55 INFO]: Val stats: {
    "score": -0.6799967992602725,
    "rmse": 0.6799967992602725
}
[08/28/2025 02:27:55 INFO]: Test stats: {
    "score": -0.874045797093832,
    "rmse": 0.874045797093832
}
[08/28/2025 02:28:08 INFO]: Training loss at epoch 8: 1.0061351656913757
[08/28/2025 02:28:09 INFO]: Training loss at epoch 67: 0.9049034118652344
[08/28/2025 02:28:12 INFO]: Training loss at epoch 54: 0.8747480809688568
[08/28/2025 02:28:23 INFO]: New best epoch, val score: -0.663347992670512
[08/28/2025 02:28:23 INFO]: Saving model to: unhanged-Shanesha_trial_88/model_best.pth
[08/28/2025 02:28:26 INFO]: Training stats: {
    "score": -1.0007659944637919,
    "rmse": 1.0007659944637919
}
[08/28/2025 02:28:26 INFO]: Val stats: {
    "score": -0.6714572060395358,
    "rmse": 0.6714572060395358
}
[08/28/2025 02:28:26 INFO]: Test stats: {
    "score": -0.8702566456707693,
    "rmse": 0.8702566456707693
}
[08/28/2025 02:28:34 INFO]: Training loss at epoch 40: 1.3100829422473907
[08/28/2025 02:28:35 INFO]: Training loss at epoch 55: 0.8827841579914093
[08/28/2025 02:28:50 INFO]: Training loss at epoch 68: 0.8590095043182373
[08/28/2025 02:29:13 INFO]: Training loss at epoch 41: 1.1078179776668549
[08/28/2025 02:29:31 INFO]: Training loss at epoch 69: 1.0165692567825317
[08/28/2025 02:29:40 INFO]: Training loss at epoch 62: 1.1540063619613647
[08/28/2025 02:29:46 INFO]: Training stats: {
    "score": -0.9955638831459511,
    "rmse": 0.9955638831459511
}
[08/28/2025 02:29:46 INFO]: Val stats: {
    "score": -0.6787721339340875,
    "rmse": 0.6787721339340875
}
[08/28/2025 02:29:46 INFO]: Test stats: {
    "score": -0.8695559608957538,
    "rmse": 0.8695559608957538
}
[08/28/2025 02:29:49 INFO]: Training loss at epoch 24: 1.1333094835281372
[08/28/2025 02:29:54 INFO]: Training loss at epoch 42: 0.8955709040164948
[08/28/2025 02:29:59 INFO]: Training loss at epoch 58: 0.8810303807258606
[08/28/2025 02:30:00 INFO]: Training accuracy: {
    "score": -1.0139541548345867,
    "rmse": 1.0139541548345867
}
[08/28/2025 02:30:00 INFO]: Val accuracy: {
    "score": -0.6607013850567668,
    "rmse": 0.6607013850567668
}
[08/28/2025 02:30:00 INFO]: Test accuracy: {
    "score": -0.8718222726185588,
    "rmse": 0.8718222726185588
}
[08/28/2025 02:30:00 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_42",
    "best_epoch": 31,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8718222726185588,
        "rmse": 0.8718222726185588
    },
    "train_stats": {
        "score": -1.0139541548345867,
        "rmse": 1.0139541548345867
    },
    "val_stats": {
        "score": -0.6607013850567668,
        "rmse": 0.6607013850567668
    }
}
[08/28/2025 02:30:00 INFO]: Procewss finished for trial unhanged-Shanesha_trial_42
[08/28/2025 02:30:00 INFO]: 
_________________________________________________

[08/28/2025 02:30:00 INFO]: train_net_for_optune.py main() running.
[08/28/2025 02:30:00 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 1.6548560186736458
  attention_dropout: 0.08426434873850669
  ffn_dropout: 0.08426434873850669
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 8.297127200897624e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_89

[08/28/2025 02:30:00 INFO]: This ft_transformer has 3.113 million parameters.
[08/28/2025 02:30:00 INFO]: Training will start at epoch 0.
[08/28/2025 02:30:00 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 02:30:02 INFO]: Training loss at epoch 5: 1.120838701725006
[08/28/2025 02:30:08 INFO]: Training loss at epoch 9: 1.0378836393356323
[08/28/2025 02:30:10 INFO]: Training loss at epoch 30: 1.020659625530243
[08/28/2025 02:30:10 INFO]: Training loss at epoch 24: 1.258690059185028
[08/28/2025 02:30:12 INFO]: Training loss at epoch 24: 1.2184735238552094
[08/28/2025 02:30:28 INFO]: Training loss at epoch 70: 1.0839387476444244
[08/28/2025 02:30:36 INFO]: Training loss at epoch 43: 0.9528811573982239
[08/28/2025 02:30:41 INFO]: Training loss at epoch 32: 1.0020698010921478
[08/28/2025 02:30:42 INFO]: Training loss at epoch 25: 1.1315288543701172
[08/28/2025 02:30:52 INFO]: Running Final Evaluation...
[08/28/2025 02:30:54 INFO]: Training stats: {
    "score": -1.0042421753453457,
    "rmse": 1.0042421753453457
}
[08/28/2025 02:30:54 INFO]: Val stats: {
    "score": -0.6634390568455683,
    "rmse": 0.6634390568455683
}
[08/28/2025 02:30:54 INFO]: Test stats: {
    "score": -0.8694871346933291,
    "rmse": 0.8694871346933291
}
[08/28/2025 02:31:13 INFO]: Training loss at epoch 71: 0.8470185995101929
[08/28/2025 02:31:19 INFO]: Training loss at epoch 44: 0.7908823788166046
[08/28/2025 02:31:22 INFO]: Training loss at epoch 12: 1.2242053747177124
[08/28/2025 02:31:38 INFO]: Training loss at epoch 0: 1.7719169855117798
[08/28/2025 02:31:51 INFO]: New best epoch, val score: -1.1527658783710468
[08/28/2025 02:31:51 INFO]: Saving model to: unhanged-Shanesha_trial_89/model_best.pth
[08/28/2025 02:31:57 INFO]: Training loss at epoch 72: 1.095748633146286
[08/28/2025 02:32:03 INFO]: Training loss at epoch 45: 0.8824627995491028
[08/28/2025 02:32:04 INFO]: Training loss at epoch 57: 1.1189268827438354
[08/28/2025 02:32:23 INFO]: Training loss at epoch 27: 1.1116456389427185
[08/28/2025 02:32:42 INFO]: Training loss at epoch 73: 0.8347821533679962
[08/28/2025 02:32:46 INFO]: Training loss at epoch 46: 1.0456205606460571
[08/28/2025 02:32:48 INFO]: Training loss at epoch 97: 1.0622183680534363
[08/28/2025 02:32:52 INFO]: Running Final Evaluation...
[08/28/2025 02:33:03 INFO]: Training loss at epoch 10: 1.0810298919677734
[08/28/2025 02:33:07 INFO]: Training accuracy: {
    "score": -1.0098036252468798,
    "rmse": 1.0098036252468798
}
[08/28/2025 02:33:07 INFO]: Val accuracy: {
    "score": -0.6635036788883885,
    "rmse": 0.6635036788883885
}
[08/28/2025 02:33:07 INFO]: Test accuracy: {
    "score": -0.8717996335314545,
    "rmse": 0.8717996335314545
}
[08/28/2025 02:33:07 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_86",
    "best_epoch": 15,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8717996335314545,
        "rmse": 0.8717996335314545
    },
    "train_stats": {
        "score": -1.0098036252468798,
        "rmse": 1.0098036252468798
    },
    "val_stats": {
        "score": -0.6635036788883885,
        "rmse": 0.6635036788883885
    }
}
[08/28/2025 02:33:07 INFO]: Procewss finished for trial unhanged-Shanesha_trial_86
[08/28/2025 02:33:07 INFO]: 
_________________________________________________

[08/28/2025 02:33:07 INFO]: train_net_for_optune.py main() running.
[08/28/2025 02:33:07 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 1.7531560628897607
  attention_dropout: 0.08741448195059194
  ffn_dropout: 0.08741448195059194
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0009968213645880153
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_90

[08/28/2025 02:33:08 INFO]: This ft_transformer has 7.827 million parameters.
[08/28/2025 02:33:08 INFO]: Training will start at epoch 0.
[08/28/2025 02:33:08 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 02:33:26 INFO]: Training loss at epoch 74: 1.2885619103908539
[08/28/2025 02:33:30 INFO]: Training loss at epoch 1: 1.3643516898155212
[08/28/2025 02:33:30 INFO]: Training accuracy: {
    "score": -1.0109193937035947,
    "rmse": 1.0109193937035947
}
[08/28/2025 02:33:30 INFO]: Val accuracy: {
    "score": -0.6606617879749075,
    "rmse": 0.6606617879749075
}
[08/28/2025 02:33:30 INFO]: Test accuracy: {
    "score": -0.8699750813257239,
    "rmse": 0.8699750813257239
}
[08/28/2025 02:33:30 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_47",
    "best_epoch": 27,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8699750813257239,
        "rmse": 0.8699750813257239
    },
    "train_stats": {
        "score": -1.0109193937035947,
        "rmse": 1.0109193937035947
    },
    "val_stats": {
        "score": -0.6606617879749075,
        "rmse": 0.6606617879749075
    }
}
[08/28/2025 02:33:30 INFO]: Procewss finished for trial unhanged-Shanesha_trial_47
[08/28/2025 02:33:30 INFO]: 
_________________________________________________

[08/28/2025 02:33:30 INFO]: train_net_for_optune.py main() running.
[08/28/2025 02:33:30 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 2.0137924190208407
  attention_dropout: 0.10291047316047565
  ffn_dropout: 0.10291047316047565
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00015769913231509827
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_91

[08/28/2025 02:33:30 INFO]: This ft_transformer has 8.359 million parameters.
[08/28/2025 02:33:30 INFO]: Training will start at epoch 0.
[08/28/2025 02:33:30 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 02:33:31 INFO]: Training loss at epoch 56: 0.9465514421463013
[08/28/2025 02:33:43 INFO]: New best epoch, val score: -0.8235953186754
[08/28/2025 02:33:43 INFO]: Saving model to: unhanged-Shanesha_trial_89/model_best.pth
[08/28/2025 02:34:10 INFO]: Training loss at epoch 75: 0.946355402469635
[08/28/2025 02:34:41 INFO]: Training loss at epoch 50: 1.1643406450748444
[08/28/2025 02:34:48 INFO]: Training loss at epoch 6: 1.4720922708511353
[08/28/2025 02:34:53 INFO]: Training loss at epoch 76: 0.879325807094574
[08/28/2025 02:35:09 INFO]: Training loss at epoch 11: 0.9016563594341278
[08/28/2025 02:35:14 INFO]: Training loss at epoch 55: 1.0006625354290009
[08/28/2025 02:35:17 INFO]: Training loss at epoch 25: 1.0687009692192078
[08/28/2025 02:35:18 INFO]: Training loss at epoch 2: 1.826226532459259
[08/28/2025 02:35:35 INFO]: Training loss at epoch 77: 0.909663051366806
[08/28/2025 02:35:41 INFO]: Training loss at epoch 25: 1.1396803259849548
[08/28/2025 02:36:05 INFO]: Training loss at epoch 26: 1.0330343842506409
[08/28/2025 02:36:10 INFO]: Training loss at epoch 25: 0.901997834444046
[08/28/2025 02:36:16 INFO]: Training loss at epoch 78: 1.1149547696113586
[08/28/2025 02:36:30 INFO]: Training loss at epoch 31: 1.0712063610553741
[08/28/2025 02:36:41 INFO]: Training loss at epoch 63: 1.1163852214813232
[08/28/2025 02:36:43 INFO]: Training loss at epoch 13: 1.2805136442184448
[08/28/2025 02:36:56 INFO]: Training loss at epoch 33: 0.8144241571426392
[08/28/2025 02:36:56 INFO]: Training loss at epoch 79: 0.7716703712940216
[08/28/2025 02:36:58 INFO]: Training loss at epoch 3: 1.4465872645378113
[08/28/2025 02:37:07 INFO]: Training loss at epoch 12: 1.0394838452339172
[08/28/2025 02:37:10 INFO]: New best epoch, val score: -0.661658113365745
[08/28/2025 02:37:10 INFO]: Saving model to: unhanged-Shanesha_trial_89/model_best.pth
[08/28/2025 02:37:11 INFO]: Training stats: {
    "score": -0.9938028438031328,
    "rmse": 0.9938028438031328
}
[08/28/2025 02:37:11 INFO]: Val stats: {
    "score": -0.6810344069994532,
    "rmse": 0.6810344069994532
}
[08/28/2025 02:37:11 INFO]: Test stats: {
    "score": -0.870831991298343,
    "rmse": 0.870831991298343
}
[08/28/2025 02:37:16 INFO]: Running Final Evaluation...
[08/28/2025 02:37:31 INFO]: Training accuracy: {
    "score": -1.0008330340993454,
    "rmse": 1.0008330340993454
}
[08/28/2025 02:37:31 INFO]: Val accuracy: {
    "score": -0.6697297493746854,
    "rmse": 0.6697297493746854
}
[08/28/2025 02:37:31 INFO]: Test accuracy: {
    "score": -0.8671599750033591,
    "rmse": 0.8671599750033591
}
[08/28/2025 02:37:31 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_84",
    "best_epoch": 48,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8671599750033591,
        "rmse": 0.8671599750033591
    },
    "train_stats": {
        "score": -1.0008330340993454,
        "rmse": 1.0008330340993454
    },
    "val_stats": {
        "score": -0.6697297493746854,
        "rmse": 0.6697297493746854
    }
}
[08/28/2025 02:37:31 INFO]: Procewss finished for trial unhanged-Shanesha_trial_84
[08/28/2025 02:37:31 INFO]: 
_________________________________________________

[08/28/2025 02:37:31 INFO]: train_net_for_optune.py main() running.
[08/28/2025 02:37:31 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 2.0213210741393812
  attention_dropout: 0.12373947874980368
  ffn_dropout: 0.12373947874980368
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00016064623869048122
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_92

[08/28/2025 02:37:31 INFO]: This ft_transformer has 3.413 million parameters.
[08/28/2025 02:37:31 INFO]: Training will start at epoch 0.
[08/28/2025 02:37:31 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 02:37:37 INFO]: Training loss at epoch 0: 1.4930377006530762
[08/28/2025 02:38:05 INFO]: Training loss at epoch 28: 0.9049348533153534
[08/28/2025 02:38:08 INFO]: Training loss at epoch 0: 0.8577629923820496
[08/28/2025 02:38:12 INFO]: New best epoch, val score: -0.7358448533869729
[08/28/2025 02:38:12 INFO]: Saving model to: unhanged-Shanesha_trial_90/model_best.pth
[08/28/2025 02:38:12 INFO]: Training loss at epoch 57: 0.8638421297073364
[08/28/2025 02:38:38 INFO]: Training loss at epoch 4: 1.1723031997680664
[08/28/2025 02:38:44 INFO]: New best epoch, val score: -0.733666606342264
[08/28/2025 02:38:44 INFO]: Saving model to: unhanged-Shanesha_trial_91/model_best.pth
[08/28/2025 02:38:47 INFO]: Training loss at epoch 58: 1.0305789113044739
[08/28/2025 02:39:05 INFO]: Training loss at epoch 0: 1.0101910829544067
[08/28/2025 02:39:05 INFO]: Training loss at epoch 13: 0.9370938241481781
[08/28/2025 02:39:08 INFO]: Training loss at epoch 7: 1.3873044848442078
[08/28/2025 02:39:19 INFO]: New best epoch, val score: -0.6849647036420926
[08/28/2025 02:39:19 INFO]: Saving model to: unhanged-Shanesha_trial_92/model_best.pth
[08/28/2025 02:39:36 INFO]: Running Final Evaluation...
[08/28/2025 02:39:50 INFO]: Training loss at epoch 98: 1.2356303930282593
[08/28/2025 02:40:19 INFO]: Training loss at epoch 26: 1.0378251671791077
[08/28/2025 02:40:25 INFO]: Training loss at epoch 5: 1.2360379099845886
[08/28/2025 02:40:39 INFO]: Training loss at epoch 51: 1.1690215766429901
[08/28/2025 02:40:48 INFO]: Training loss at epoch 26: 1.190642923116684
[08/28/2025 02:41:02 INFO]: Training loss at epoch 1: 1.2809607088565826
[08/28/2025 02:41:12 INFO]: Training loss at epoch 14: 1.1447645723819733
[08/28/2025 02:41:15 INFO]: Training loss at epoch 27: 1.0275312662124634
[08/28/2025 02:41:56 INFO]: Training loss at epoch 26: 0.9368408620357513
[08/28/2025 02:41:59 INFO]: Training loss at epoch 14: 1.3291343450546265
[08/28/2025 02:42:03 INFO]: Training loss at epoch 56: 1.0645243525505066
[08/28/2025 02:42:13 INFO]: Training accuracy: {
    "score": -1.0060720765823485,
    "rmse": 1.0060720765823485
}
[08/28/2025 02:42:13 INFO]: Val accuracy: {
    "score": -0.6632464031165363,
    "rmse": 0.6632464031165363
}
[08/28/2025 02:42:13 INFO]: Test accuracy: {
    "score": -0.8688366440015644,
    "rmse": 0.8688366440015644
}
[08/28/2025 02:42:13 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_49",
    "best_epoch": 27,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8688366440015644,
        "rmse": 0.8688366440015644
    },
    "train_stats": {
        "score": -1.0060720765823485,
        "rmse": 1.0060720765823485
    },
    "val_stats": {
        "score": -0.6632464031165363,
        "rmse": 0.6632464031165363
    }
}
[08/28/2025 02:42:13 INFO]: Procewss finished for trial unhanged-Shanesha_trial_49
[08/28/2025 02:42:13 INFO]: 
_________________________________________________

[08/28/2025 02:42:13 INFO]: train_net_for_optune.py main() running.
[08/28/2025 02:42:13 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 1.9910826732735092
  attention_dropout: 0.2635615174449583
  ffn_dropout: 0.2635615174449583
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00038201191103035806
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_93

[08/28/2025 02:42:13 INFO]: This ft_transformer has 8.314 million parameters.
[08/28/2025 02:42:13 INFO]: Training will start at epoch 0.
[08/28/2025 02:42:13 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 02:42:17 INFO]: Training loss at epoch 6: 1.0491992831230164
[08/28/2025 02:42:43 INFO]: Training loss at epoch 32: 1.048384815454483
[08/28/2025 02:42:55 INFO]: Training loss at epoch 1: 6.563562989234924
[08/28/2025 02:43:01 INFO]: Training loss at epoch 2: 1.0368703603744507
[08/28/2025 02:43:10 INFO]: Training loss at epoch 34: 1.1201943755149841
[08/28/2025 02:43:10 INFO]: Training loss at epoch 58: 0.9992596507072449
[08/28/2025 02:43:20 INFO]: Training loss at epoch 15: 1.1933334469795227
[08/28/2025 02:43:38 INFO]: Training loss at epoch 64: 1.1208126246929169
[08/28/2025 02:43:41 INFO]: Training loss at epoch 1: 2.17234867811203
[08/28/2025 02:43:47 INFO]: Running Final Evaluation...
[08/28/2025 02:43:55 INFO]: Training loss at epoch 8: 1.2819575667381287
[08/28/2025 02:44:01 INFO]: Training loss at epoch 29: 1.1164944171905518
[08/28/2025 02:44:08 INFO]: Training loss at epoch 7: 0.9009350836277008
[08/28/2025 02:44:20 INFO]: New best epoch, val score: -0.7204797606611445
[08/28/2025 02:44:20 INFO]: Saving model to: unhanged-Shanesha_trial_91/model_best.pth
[08/28/2025 02:44:53 INFO]: Training loss at epoch 3: 0.9104112088680267
[08/28/2025 02:45:06 INFO]: New best epoch, val score: -0.6574356104553399
[08/28/2025 02:45:06 INFO]: Saving model to: unhanged-Shanesha_trial_92/model_best.pth
[08/28/2025 02:45:23 INFO]: Training loss at epoch 16: 0.9250877797603607
[08/28/2025 02:45:24 INFO]: Training accuracy: {
    "score": -1.0073780469176823,
    "rmse": 1.0073780469176823
}
[08/28/2025 02:45:24 INFO]: Val accuracy: {
    "score": -0.6608792087871268,
    "rmse": 0.6608792087871268
}
[08/28/2025 02:45:24 INFO]: Test accuracy: {
    "score": -0.8698174069033072,
    "rmse": 0.8698174069033072
}
[08/28/2025 02:45:24 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_60",
    "best_epoch": 27,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8698174069033072,
        "rmse": 0.8698174069033072
    },
    "train_stats": {
        "score": -1.0073780469176823,
        "rmse": 1.0073780469176823
    },
    "val_stats": {
        "score": -0.6608792087871268,
        "rmse": 0.6608792087871268
    }
}
[08/28/2025 02:45:24 INFO]: Procewss finished for trial unhanged-Shanesha_trial_60
[08/28/2025 02:45:25 INFO]: 
_________________________________________________

[08/28/2025 02:45:25 INFO]: train_net_for_optune.py main() running.
[08/28/2025 02:45:25 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 1.7149874530569549
  attention_dropout: 0.11706675243641317
  ffn_dropout: 0.11706675243641317
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00017912381001231956
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_94

[08/28/2025 02:45:25 INFO]: This ft_transformer has 7.744 million parameters.
[08/28/2025 02:45:25 INFO]: Training will start at epoch 0.
[08/28/2025 02:45:25 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 02:45:40 INFO]: Training loss at epoch 27: 1.2892126441001892
[08/28/2025 02:45:48 INFO]: Training loss at epoch 8: 1.0167455077171326
[08/28/2025 02:45:51 INFO]: Training stats: {
    "score": -1.0012019435816155,
    "rmse": 1.0012019435816155
}
[08/28/2025 02:45:51 INFO]: Val stats: {
    "score": -0.7118473797910301,
    "rmse": 0.7118473797910301
}
[08/28/2025 02:45:51 INFO]: Test stats: {
    "score": -0.8898652520464768,
    "rmse": 0.8898652520464768
}
[08/28/2025 02:46:09 INFO]: Training loss at epoch 27: 1.0623066425323486
[08/28/2025 02:46:31 INFO]: Training loss at epoch 28: 0.9565476775169373
[08/28/2025 02:46:40 INFO]: Training loss at epoch 4: 1.0333510637283325
[08/28/2025 02:46:49 INFO]: Training loss at epoch 52: 1.2008472383022308
[08/28/2025 02:46:58 INFO]: Training loss at epoch 0: 0.9975394010543823
[08/28/2025 02:47:03 INFO]: Training loss at epoch 99: 1.010913074016571
[08/28/2025 02:47:11 INFO]: Training loss at epoch 15: 1.2655506134033203
[08/28/2025 02:47:20 INFO]: Training loss at epoch 17: 1.003045231103897
[08/28/2025 02:47:29 INFO]: Training loss at epoch 9: 0.9197779595851898
[08/28/2025 02:47:34 INFO]: New best epoch, val score: -0.7124390558754213
[08/28/2025 02:47:34 INFO]: Saving model to: unhanged-Shanesha_trial_93/model_best.pth
[08/28/2025 02:47:41 INFO]: Training loss at epoch 27: 0.9443467557430267
[08/28/2025 02:47:48 INFO]: New best epoch, val score: -0.6687806563142017
[08/28/2025 02:47:48 INFO]: Saving model to: unhanged-Shanesha_trial_81/model_best.pth
[08/28/2025 02:47:59 INFO]: Training loss at epoch 2: 1.9570919275283813
[08/28/2025 02:48:03 INFO]: Training stats: {
    "score": -1.0356519235653852,
    "rmse": 1.0356519235653852
}
[08/28/2025 02:48:03 INFO]: Val stats: {
    "score": -0.7909445049026405,
    "rmse": 0.7909445049026405
}
[08/28/2025 02:48:03 INFO]: Test stats: {
    "score": -0.947399883850581,
    "rmse": 0.947399883850581
}
[08/28/2025 02:48:14 INFO]: Training loss at epoch 9: 1.1517435908317566
[08/28/2025 02:48:20 INFO]: New best epoch, val score: -0.6606928242113406
[08/28/2025 02:48:20 INFO]: Saving model to: unhanged-Shanesha_trial_70/model_best.pth
[08/28/2025 02:48:29 INFO]: Training loss at epoch 5: 0.9041585624217987
[08/28/2025 02:48:45 INFO]: Training loss at epoch 33: 0.8762948215007782
[08/28/2025 02:48:53 INFO]: Training loss at epoch 57: 1.1975558996200562
[08/28/2025 02:48:56 INFO]: Training loss at epoch 2: 1.384756863117218
[08/28/2025 02:49:10 INFO]: Training loss at epoch 35: 1.1626669764518738
[08/28/2025 02:49:24 INFO]: Training loss at epoch 18: 1.1136561334133148
[08/28/2025 02:49:27 INFO]: Training stats: {
    "score": -0.9691712879213311,
    "rmse": 0.9691712879213311
}
[08/28/2025 02:49:27 INFO]: Val stats: {
    "score": -0.6616334554958333,
    "rmse": 0.6616334554958333
}
[08/28/2025 02:49:27 INFO]: Test stats: {
    "score": -0.8865115026099171,
    "rmse": 0.8865115026099171
}
[08/28/2025 02:49:51 INFO]: Training stats: {
    "score": -1.0875409030054612,
    "rmse": 1.0875409030054612
}
[08/28/2025 02:49:51 INFO]: Val stats: {
    "score": -0.887443523477037,
    "rmse": 0.887443523477037
}
[08/28/2025 02:49:51 INFO]: Test stats: {
    "score": -1.016712666308415,
    "rmse": 1.016712666308415
}
[08/28/2025 02:49:53 INFO]: Training loss at epoch 10: 0.9604446589946747
[08/28/2025 02:49:56 INFO]: Training loss at epoch 0: 1.2313760817050934
[08/28/2025 02:50:25 INFO]: Training loss at epoch 65: 0.9724758863449097
[08/28/2025 02:50:27 INFO]: Training loss at epoch 6: 0.948063313961029
[08/28/2025 02:50:36 INFO]: New best epoch, val score: -0.7139798989692242
[08/28/2025 02:50:36 INFO]: Saving model to: unhanged-Shanesha_trial_94/model_best.pth
[08/28/2025 02:50:54 INFO]: Training loss at epoch 28: 1.0201616287231445
[08/28/2025 02:51:29 INFO]: Training loss at epoch 28: 0.9950098991394043
[08/28/2025 02:51:32 INFO]: Training loss at epoch 19: 1.1157433986663818
[08/28/2025 02:51:39 INFO]: Training loss at epoch 30: 0.9775576293468475
[08/28/2025 02:51:45 INFO]: Training loss at epoch 11: 0.9278513491153717
[08/28/2025 02:51:53 INFO]: Training loss at epoch 29: 1.0478594303131104
[08/28/2025 02:52:18 INFO]: Training stats: {
    "score": -1.0005276886285144,
    "rmse": 1.0005276886285144
}
[08/28/2025 02:52:18 INFO]: Val stats: {
    "score": -0.6916967468590182,
    "rmse": 0.6916967468590182
}
[08/28/2025 02:52:18 INFO]: Test stats: {
    "score": -0.8817847559043709,
    "rmse": 0.8817847559043709
}
[08/28/2025 02:52:27 INFO]: Training loss at epoch 7: 1.051539659500122
[08/28/2025 02:52:34 INFO]: Training loss at epoch 1: 4.208224833011627
[08/28/2025 02:52:39 INFO]: Training loss at epoch 16: 1.0645338296890259
[08/28/2025 02:53:07 INFO]: Training loss at epoch 53: 1.199134349822998
[08/28/2025 02:53:19 INFO]: New best epoch, val score: -0.6657667321551741
[08/28/2025 02:53:19 INFO]: Saving model to: unhanged-Shanesha_trial_81/model_best.pth
[08/28/2025 02:53:25 INFO]: Training loss at epoch 3: 1.3143877387046814
[08/28/2025 02:53:35 INFO]: Training loss at epoch 12: 0.793350487947464
[08/28/2025 02:53:42 INFO]: Training stats: {
    "score": -1.003094332863484,
    "rmse": 1.003094332863484
}
[08/28/2025 02:53:42 INFO]: Val stats: {
    "score": -0.7113389886047795,
    "rmse": 0.7113389886047795
}
[08/28/2025 02:53:42 INFO]: Test stats: {
    "score": -0.8898118714644441,
    "rmse": 0.8898118714644441
}
[08/28/2025 02:53:44 INFO]: Training loss at epoch 28: 1.09176367521286
[08/28/2025 02:53:47 INFO]: New best epoch, val score: -0.6615134685472895
[08/28/2025 02:53:47 INFO]: Saving model to: unhanged-Shanesha_trial_89/model_best.pth
[08/28/2025 02:54:19 INFO]: Training loss at epoch 8: 0.9888380765914917
[08/28/2025 02:54:21 INFO]: Training loss at epoch 20: 0.9002189338207245
[08/28/2025 02:54:29 INFO]: Training loss at epoch 3: 1.3045989573001862
[08/28/2025 02:54:31 INFO]: Training loss at epoch 10: 1.309388518333435
[08/28/2025 02:55:06 INFO]: New best epoch, val score: -0.6983701472662496
[08/28/2025 02:55:06 INFO]: Saving model to: unhanged-Shanesha_trial_91/model_best.pth
[08/28/2025 02:55:08 INFO]: Training loss at epoch 34: 0.9289067685604095
[08/28/2025 02:55:12 INFO]: Training loss at epoch 1: 3.3687857389450073
[08/28/2025 02:55:15 INFO]: Training loss at epoch 13: 1.0347455739974976
[08/28/2025 02:55:27 INFO]: Training loss at epoch 36: 1.0751941204071045
[08/28/2025 02:55:56 INFO]: Training loss at epoch 58: 1.236474335193634
[08/28/2025 02:56:06 INFO]: Training loss at epoch 9: 1.0890019237995148
[08/28/2025 02:56:07 INFO]: Training loss at epoch 29: 0.8371828496456146
[08/28/2025 02:56:19 INFO]: Training loss at epoch 21: 0.886317253112793
[08/28/2025 02:56:40 INFO]: Training loss at epoch 29: 0.998520702123642
[08/28/2025 02:56:41 INFO]: Training loss at epoch 100: 1.203074961900711
[08/28/2025 02:56:42 INFO]: Training stats: {
    "score": -1.005228710881218,
    "rmse": 1.005228710881218
}
[08/28/2025 02:56:42 INFO]: Val stats: {
    "score": -0.7203495473331643,
    "rmse": 0.7203495473331643
}
[08/28/2025 02:56:42 INFO]: Test stats: {
    "score": -0.8992884681486595,
    "rmse": 0.8992884681486595
}
[08/28/2025 02:56:55 INFO]: Training loss at epoch 14: 1.1935559511184692
[08/28/2025 02:57:14 INFO]: Training loss at epoch 66: 0.8710876703262329
[08/28/2025 02:57:18 INFO]: Training loss at epoch 31: 1.0224635899066925
[08/28/2025 02:57:44 INFO]: Training loss at epoch 17: 1.0350378155708313
[08/28/2025 02:57:46 INFO]: Training stats: {
    "score": -1.0000740322761414,
    "rmse": 1.0000740322761414
}
[08/28/2025 02:57:46 INFO]: Val stats: {
    "score": -0.6706330593031158,
    "rmse": 0.6706330593031158
}
[08/28/2025 02:57:46 INFO]: Test stats: {
    "score": -0.8704976681168273,
    "rmse": 0.8704976681168273
}
[08/28/2025 02:57:51 INFO]: Training loss at epoch 2: 2.292592167854309
[08/28/2025 02:57:56 INFO]: Running Final Evaluation...
[08/28/2025 02:58:21 INFO]: Training loss at epoch 22: 1.0029039680957794
[08/28/2025 02:58:22 INFO]: Training loss at epoch 4: 1.1511164903640747
[08/28/2025 02:58:24 INFO]: Training stats: {
    "score": -1.0024384556930077,
    "rmse": 1.0024384556930077
}
[08/28/2025 02:58:24 INFO]: Val stats: {
    "score": -0.7078683274602454,
    "rmse": 0.7078683274602454
}
[08/28/2025 02:58:24 INFO]: Test stats: {
    "score": -0.8893914119052599,
    "rmse": 0.8893914119052599
}
[08/28/2025 02:58:32 INFO]: Training loss at epoch 10: 0.8612301647663116
[08/28/2025 02:58:39 INFO]: Training loss at epoch 15: 1.0230979323387146
[08/28/2025 02:58:45 INFO]: Training loss at epoch 30: 1.0477802753448486
[08/28/2025 02:58:53 INFO]: New best epoch, val score: -0.6601930363164528
[08/28/2025 02:58:53 INFO]: Saving model to: unhanged-Shanesha_trial_89/model_best.pth
[08/28/2025 02:58:54 INFO]: Training loss at epoch 11: 1.1232976913452148
[08/28/2025 02:59:03 INFO]: Training loss at epoch 54: 1.0679603219032288
[08/28/2025 02:59:21 INFO]: Training loss at epoch 29: 0.9995608031749725
[08/28/2025 02:59:28 INFO]: New best epoch, val score: -0.6613660385724116
[08/28/2025 02:59:28 INFO]: Saving model to: unhanged-Shanesha_trial_87/model_best.pth
[08/28/2025 02:59:47 INFO]: Training loss at epoch 4: 1.1374835968017578
[08/28/2025 03:00:09 INFO]: Training accuracy: {
    "score": -1.0018208911483946,
    "rmse": 1.0018208911483946
}
[08/28/2025 03:00:09 INFO]: Val accuracy: {
    "score": -0.6698906896025487,
    "rmse": 0.6698906896025487
}
[08/28/2025 03:00:09 INFO]: Test accuracy: {
    "score": -0.8694917708310433,
    "rmse": 0.8694917708310433
}
[08/28/2025 03:00:09 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_69",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8694917708310433,
        "rmse": 0.8694917708310433
    },
    "train_stats": {
        "score": -1.0018208911483946,
        "rmse": 1.0018208911483946
    },
    "val_stats": {
        "score": -0.6698906896025487,
        "rmse": 0.6698906896025487
    }
}
[08/28/2025 03:00:09 INFO]: Procewss finished for trial unhanged-Shanesha_trial_69
[08/28/2025 03:00:09 INFO]: 
_________________________________________________

[08/28/2025 03:00:09 INFO]: train_net_for_optune.py main() running.
[08/28/2025 03:00:09 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 0.9002282365924243
  attention_dropout: 0.09956109079678643
  ffn_dropout: 0.09956109079678643
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00016954676608045444
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_95

[08/28/2025 03:00:09 INFO]: This ft_transformer has 1.005 million parameters.
[08/28/2025 03:00:09 INFO]: Training will start at epoch 0.
[08/28/2025 03:00:09 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 03:00:23 INFO]: Training loss at epoch 2: 1.67094087600708
[08/28/2025 03:00:32 INFO]: Training loss at epoch 16: 0.9165714979171753
[08/28/2025 03:00:32 INFO]: Training loss at epoch 11: 1.0828456282615662
[08/28/2025 03:00:32 INFO]: Training loss at epoch 23: 1.0715703070163727
[08/28/2025 03:01:20 INFO]: Training loss at epoch 35: 0.8791176974773407
[08/28/2025 03:01:26 INFO]: Training stats: {
    "score": -1.0229158461353114,
    "rmse": 1.0229158461353114
}
[08/28/2025 03:01:26 INFO]: Val stats: {
    "score": -0.6624974456548568,
    "rmse": 0.6624974456548568
}
[08/28/2025 03:01:26 INFO]: Test stats: {
    "score": -0.8764077039285508,
    "rmse": 0.8764077039285508
}
[08/28/2025 03:01:40 INFO]: Training loss at epoch 37: 1.0601161122322083
[08/28/2025 03:01:53 INFO]: Training loss at epoch 0: 1.1409081816673279
[08/28/2025 03:02:09 INFO]: New best epoch, val score: -1.0095904667450128
[08/28/2025 03:02:09 INFO]: Saving model to: unhanged-Shanesha_trial_95/model_best.pth
[08/28/2025 03:02:25 INFO]: Training loss at epoch 17: 0.8971060514450073
[08/28/2025 03:02:32 INFO]: Training loss at epoch 12: 0.8944608271121979
[08/28/2025 03:02:44 INFO]: Training loss at epoch 24: 1.068719208240509
[08/28/2025 03:03:01 INFO]: Training loss at epoch 59: 1.0418960452079773
[08/28/2025 03:03:13 INFO]: Training loss at epoch 18: 0.9860217571258545
[08/28/2025 03:03:13 INFO]: Training loss at epoch 30: 0.9478471875190735
[08/28/2025 03:03:33 INFO]: Training loss at epoch 3: 1.257332593202591
[08/28/2025 03:03:38 INFO]: Training loss at epoch 12: 1.1206596493721008
[08/28/2025 03:03:43 INFO]: Training loss at epoch 5: 0.8832198977470398
[08/28/2025 03:03:47 INFO]: Training loss at epoch 1: 1.1659097075462341
[08/28/2025 03:03:52 INFO]: Training loss at epoch 30: 1.0072810649871826
[08/28/2025 03:04:00 INFO]: Training loss at epoch 101: 1.0056259036064148
[08/28/2025 03:04:01 INFO]: New best epoch, val score: -0.6802867023987705
[08/28/2025 03:04:01 INFO]: Saving model to: unhanged-Shanesha_trial_95/model_best.pth
[08/28/2025 03:04:07 INFO]: Training loss at epoch 18: 1.0738351941108704
[08/28/2025 03:04:08 INFO]: Training loss at epoch 31: 0.9774425029754639
[08/28/2025 03:04:13 INFO]: Training loss at epoch 67: 1.1286627650260925
[08/28/2025 03:04:20 INFO]: Training loss at epoch 13: 0.8799905478954315
[08/28/2025 03:04:44 INFO]: Training loss at epoch 25: 0.9301011562347412
[08/28/2025 03:05:15 INFO]: Training stats: {
    "score": -1.0023546175113225,
    "rmse": 1.0023546175113225
}
[08/28/2025 03:05:15 INFO]: Val stats: {
    "score": -0.6641709031130344,
    "rmse": 0.6641709031130344
}
[08/28/2025 03:05:15 INFO]: Test stats: {
    "score": -0.8693733129979498,
    "rmse": 0.8693733129979498
}
[08/28/2025 03:05:16 INFO]: Training loss at epoch 55: 1.024555265903473
[08/28/2025 03:05:17 INFO]: Training loss at epoch 5: 0.9473313391208649
[08/28/2025 03:05:32 INFO]: Training loss at epoch 3: 0.9718940556049347
[08/28/2025 03:05:37 INFO]: Training loss at epoch 2: 0.9771026968955994
[08/28/2025 03:05:48 INFO]: Training loss at epoch 19: 0.989476889371872
[08/28/2025 03:06:08 INFO]: Training loss at epoch 14: 1.0868031978607178
[08/28/2025 03:06:08 INFO]: New best epoch, val score: -0.6728727310738016
[08/28/2025 03:06:08 INFO]: Saving model to: unhanged-Shanesha_trial_94/model_best.pth
[08/28/2025 03:06:22 INFO]: Training stats: {
    "score": -1.0065623851829026,
    "rmse": 1.0065623851829026
}
[08/28/2025 03:06:22 INFO]: Val stats: {
    "score": -0.7213579017503035,
    "rmse": 0.7213579017503035
}
[08/28/2025 03:06:22 INFO]: Test stats: {
    "score": -0.8994850793170046,
    "rmse": 0.8994850793170046
}
[08/28/2025 03:06:44 INFO]: Training loss at epoch 26: 0.9841919541358948
[08/28/2025 03:07:04 INFO]: Training loss at epoch 30: 1.1288862228393555
[08/28/2025 03:07:23 INFO]: Training loss at epoch 36: 1.1168487966060638
[08/28/2025 03:07:28 INFO]: Training loss at epoch 3: 1.0797811150550842
[08/28/2025 03:07:40 INFO]: Training loss at epoch 38: 1.1464292109012604
[08/28/2025 03:07:44 INFO]: New best epoch, val score: -0.6619761288184317
[08/28/2025 03:07:44 INFO]: Saving model to: unhanged-Shanesha_trial_95/model_best.pth
[08/28/2025 03:08:00 INFO]: Training loss at epoch 15: 0.8661715686321259
[08/28/2025 03:08:04 INFO]: Training loss at epoch 13: 1.0480929911136627
[08/28/2025 03:08:09 INFO]: Training loss at epoch 20: 0.9654445052146912
[08/28/2025 03:08:18 INFO]: Training loss at epoch 31: 0.9526118636131287
[08/28/2025 03:08:21 INFO]: Training loss at epoch 19: 1.0027640759944916
[08/28/2025 03:08:50 INFO]: Training loss at epoch 6: 1.0730710625648499
[08/28/2025 03:08:52 INFO]: Training loss at epoch 27: 1.0566667914390564
[08/28/2025 03:08:57 INFO]: Training loss at epoch 4: 1.249610722064972
[08/28/2025 03:09:05 INFO]: Training loss at epoch 31: 1.104326993227005
[08/28/2025 03:09:22 INFO]: Training loss at epoch 32: 0.9828427135944366
[08/28/2025 03:09:28 INFO]: Training loss at epoch 4: 1.1247760653495789
[08/28/2025 03:10:01 INFO]: Training loss at epoch 21: 1.0125486850738525
[08/28/2025 03:10:01 INFO]: Training loss at epoch 16: 0.9817841947078705
[08/28/2025 03:10:19 INFO]: Training stats: {
    "score": -1.025699788989988,
    "rmse": 1.025699788989988
}
[08/28/2025 03:10:19 INFO]: Val stats: {
    "score": -0.7712649920029568,
    "rmse": 0.7712649920029568
}
[08/28/2025 03:10:19 INFO]: Test stats: {
    "score": -0.9296238057929779,
    "rmse": 0.9296238057929779
}
[08/28/2025 03:10:47 INFO]: Training loss at epoch 6: 1.01307612657547
[08/28/2025 03:10:52 INFO]: Training loss at epoch 4: 0.909557968378067
[08/28/2025 03:11:01 INFO]: Training loss at epoch 28: 1.0174587070941925
[08/28/2025 03:11:09 INFO]: Training loss at epoch 68: 0.9612889885902405
[08/28/2025 03:11:13 INFO]: Training loss at epoch 102: 1.039275974035263
[08/28/2025 03:11:27 INFO]: Training loss at epoch 5: 0.8706008195877075
[08/28/2025 03:11:34 INFO]: Training loss at epoch 56: 0.9949305355548859
[08/28/2025 03:11:52 INFO]: Training loss at epoch 22: 0.9720783531665802
[08/28/2025 03:11:59 INFO]: Training loss at epoch 17: 1.1837795972824097
[08/28/2025 03:12:19 INFO]: Training loss at epoch 60: 1.0258110761642456
[08/28/2025 03:12:51 INFO]: Training loss at epoch 14: 1.3778865337371826
[08/28/2025 03:13:03 INFO]: Training loss at epoch 31: 0.8654936850070953
[08/28/2025 03:13:03 INFO]: Training loss at epoch 29: 1.0130802989006042
[08/28/2025 03:13:06 INFO]: Running Final Evaluation...
[08/28/2025 03:13:17 INFO]: Training loss at epoch 6: 1.0556825995445251
[08/28/2025 03:13:32 INFO]: Training loss at epoch 23: 1.2506601214408875
[08/28/2025 03:13:37 INFO]: Training loss at epoch 32: 1.1937053799629211
[08/28/2025 03:13:41 INFO]: Training loss at epoch 37: 0.8051953315734863
[08/28/2025 03:13:45 INFO]: Training stats: {
    "score": -1.0063622612655951,
    "rmse": 1.0063622612655951
}
[08/28/2025 03:13:45 INFO]: Val stats: {
    "score": -0.7187845865994167,
    "rmse": 0.7187845865994167
}
[08/28/2025 03:13:45 INFO]: Test stats: {
    "score": -0.8972239080933024,
    "rmse": 0.8972239080933024
}
[08/28/2025 03:13:46 INFO]: Training loss at epoch 18: 0.8210406303405762
[08/28/2025 03:13:56 INFO]: Training loss at epoch 39: 0.8956181704998016
[08/28/2025 03:14:03 INFO]: Training loss at epoch 7: 1.113116979598999
[08/28/2025 03:14:21 INFO]: Training loss at epoch 32: 0.9138216078281403
[08/28/2025 03:14:26 INFO]: Training loss at epoch 5: 1.2580780982971191
[08/28/2025 03:14:39 INFO]: Training loss at epoch 33: 0.7926796972751617
[08/28/2025 03:15:06 INFO]: Training loss at epoch 7: 1.2753173112869263
[08/28/2025 03:15:14 INFO]: Training loss at epoch 24: 0.933686375617981
[08/28/2025 03:15:27 INFO]: Training loss at epoch 20: 1.0628487467765808
[08/28/2025 03:15:31 INFO]: Training accuracy: {
    "score": -1.0124449990195477,
    "rmse": 1.0124449990195477
}
[08/28/2025 03:15:31 INFO]: Val accuracy: {
    "score": -0.6607785530051009,
    "rmse": 0.6607785530051009
}
[08/28/2025 03:15:31 INFO]: Test accuracy: {
    "score": -0.8711395681698029,
    "rmse": 0.8711395681698029
}
[08/28/2025 03:15:31 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_50",
    "best_epoch": 29,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8711395681698029,
        "rmse": 0.8711395681698029
    },
    "train_stats": {
        "score": -1.0124449990195477,
        "rmse": 1.0124449990195477
    },
    "val_stats": {
        "score": -0.6607785530051009,
        "rmse": 0.6607785530051009
    }
}
[08/28/2025 03:15:31 INFO]: Procewss finished for trial unhanged-Shanesha_trial_50
[08/28/2025 03:15:31 INFO]: 
_________________________________________________

[08/28/2025 03:15:31 INFO]: train_net_for_optune.py main() running.
[08/28/2025 03:15:31 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 0.9073112607689788
  attention_dropout: 0.11534878235255172
  ffn_dropout: 0.11534878235255172
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00015894031406132392
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_96

[08/28/2025 03:15:31 INFO]: This ft_transformer has 6.090 million parameters.
[08/28/2025 03:15:31 INFO]: Training will start at epoch 0.
[08/28/2025 03:15:31 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 03:15:33 INFO]: Training loss at epoch 19: 1.1165441870689392
[08/28/2025 03:15:45 INFO]: Training loss at epoch 30: 1.1293150782585144
[08/28/2025 03:15:53 INFO]: Training loss at epoch 5: 1.25502610206604
[08/28/2025 03:15:53 INFO]: Training stats: {
    "score": -1.0002578608177335,
    "rmse": 1.0002578608177335
}
[08/28/2025 03:15:53 INFO]: Val stats: {
    "score": -0.6726003021110502,
    "rmse": 0.6726003021110502
}
[08/28/2025 03:15:53 INFO]: Test stats: {
    "score": -0.8707548494803148,
    "rmse": 0.8707548494803148
}
[08/28/2025 03:16:00 INFO]: Training loss at epoch 7: 1.1478325724601746
[08/28/2025 03:16:09 INFO]: Training stats: {
    "score": -0.995891360511991,
    "rmse": 0.995891360511991
}
[08/28/2025 03:16:09 INFO]: Val stats: {
    "score": -0.6677270919564389,
    "rmse": 0.6677270919564389
}
[08/28/2025 03:16:09 INFO]: Test stats: {
    "score": -0.8711951037628497,
    "rmse": 0.8711951037628497
}
[08/28/2025 03:16:39 INFO]: New best epoch, val score: -0.6924618950292089
[08/28/2025 03:16:39 INFO]: Saving model to: unhanged-Shanesha_trial_91/model_best.pth
[08/28/2025 03:16:59 INFO]: Training loss at epoch 8: 1.1034675240516663
[08/28/2025 03:17:00 INFO]: Training loss at epoch 25: 1.0142852663993835
[08/28/2025 03:17:18 INFO]: Training loss at epoch 15: 1.1675167679786682
[08/28/2025 03:17:31 INFO]: Training loss at epoch 57: 0.7879876494407654
[08/28/2025 03:17:53 INFO]: Training loss at epoch 69: 1.0623514652252197
[08/28/2025 03:17:55 INFO]: Training loss at epoch 31: 0.9448844194412231
[08/28/2025 03:18:08 INFO]: Training loss at epoch 20: 1.027575671672821
[08/28/2025 03:18:16 INFO]: Training loss at epoch 103: 0.971656322479248
[08/28/2025 03:18:49 INFO]: Training loss at epoch 32: 0.8523094654083252
[08/28/2025 03:18:49 INFO]: Training loss at epoch 33: 0.9496683478355408
[08/28/2025 03:18:52 INFO]: Training loss at epoch 26: 0.9228273332118988
[08/28/2025 03:18:59 INFO]: Training loss at epoch 9: 0.8564572036266327
[08/28/2025 03:19:17 INFO]: Training loss at epoch 8: 0.8865895569324493
[08/28/2025 03:19:33 INFO]: Training loss at epoch 0: 1.165614366531372
[08/28/2025 03:19:42 INFO]: Training stats: {
    "score": -1.0190114378719564,
    "rmse": 1.0190114378719564
}
[08/28/2025 03:19:42 INFO]: Val stats: {
    "score": -0.7543713648697884,
    "rmse": 0.7543713648697884
}
[08/28/2025 03:19:42 INFO]: Test stats: {
    "score": -0.9154484754185256,
    "rmse": 0.9154484754185256
}
[08/28/2025 03:19:43 INFO]: Training loss at epoch 33: 1.0012454390525818
[08/28/2025 03:19:56 INFO]: Training loss at epoch 38: 0.9898686707019806
[08/28/2025 03:19:57 INFO]: New best epoch, val score: -0.7329598221381975
[08/28/2025 03:19:57 INFO]: Saving model to: unhanged-Shanesha_trial_90/model_best.pth
[08/28/2025 03:19:59 INFO]: Training loss at epoch 6: 1.1917697191238403
[08/28/2025 03:20:04 INFO]: Training loss at epoch 34: 1.3231181204319
[08/28/2025 03:20:05 INFO]: Training loss at epoch 32: 1.190411925315857
[08/28/2025 03:20:07 INFO]: Training loss at epoch 21: 0.9998854696750641
[08/28/2025 03:20:07 INFO]: New best epoch, val score: -0.9384572393603483
[08/28/2025 03:20:07 INFO]: Saving model to: unhanged-Shanesha_trial_96/model_best.pth
[08/28/2025 03:20:23 INFO]: Training stats: {
    "score": -0.9984666787549209,
    "rmse": 0.9984666787549209
}
[08/28/2025 03:20:23 INFO]: Val stats: {
    "score": -0.6805820984390986,
    "rmse": 0.6805820984390986
}
[08/28/2025 03:20:23 INFO]: Test stats: {
    "score": -0.873444040687731,
    "rmse": 0.873444040687731
}
[08/28/2025 03:20:43 INFO]: Training loss at epoch 27: 0.9881500899791718
[08/28/2025 03:20:55 INFO]: Training loss at epoch 21: 1.0420596599578857
[08/28/2025 03:21:17 INFO]: Training loss at epoch 6: 1.1773546934127808
[08/28/2025 03:21:36 INFO]: Training loss at epoch 8: 1.2603740692138672
[08/28/2025 03:21:38 INFO]: Training loss at epoch 10: 1.0766983032226562
[08/28/2025 03:22:00 INFO]: Training loss at epoch 22: 1.1235591173171997
[08/28/2025 03:22:00 INFO]: Training loss at epoch 16: 0.8884633779525757
[08/28/2025 03:22:10 INFO]: Training loss at epoch 33: 1.134427011013031
[08/28/2025 03:22:12 INFO]: New best epoch, val score: -0.6849811287425205
[08/28/2025 03:22:12 INFO]: Saving model to: unhanged-Shanesha_trial_91/model_best.pth
[08/28/2025 03:22:13 INFO]: Training loss at epoch 40: 1.051489531993866
[08/28/2025 03:22:26 INFO]: Training loss at epoch 28: 0.9739883244037628
[08/28/2025 03:22:54 INFO]: Running Final Evaluation...
[08/28/2025 03:23:28 INFO]: Training loss at epoch 11: 0.8764787912368774
[08/28/2025 03:23:44 INFO]: Training loss at epoch 58: 0.9330468773841858
[08/28/2025 03:23:48 INFO]: Training loss at epoch 23: 1.035848319530487
[08/28/2025 03:23:56 INFO]: Training loss at epoch 1: 1.9489096999168396
[08/28/2025 03:24:00 INFO]: Training loss at epoch 34: 0.9836339056491852
[08/28/2025 03:24:06 INFO]: Training loss at epoch 29: 0.9237801730632782
[08/28/2025 03:24:09 INFO]: Training loss at epoch 34: 0.9892258048057556
[08/28/2025 03:24:24 INFO]: Training loss at epoch 9: 1.291612148284912
[08/28/2025 03:24:26 INFO]: New best epoch, val score: -0.9064064804289165
[08/28/2025 03:24:26 INFO]: Saving model to: unhanged-Shanesha_trial_96/model_best.pth
[08/28/2025 03:24:32 INFO]: Training loss at epoch 33: 0.9962957501411438
[08/28/2025 03:24:40 INFO]: Training stats: {
    "score": -0.9973880862851845,
    "rmse": 0.9973880862851845
}
[08/28/2025 03:24:40 INFO]: Val stats: {
    "score": -0.6803446448003192,
    "rmse": 0.6803446448003192
}
[08/28/2025 03:24:40 INFO]: Test stats: {
    "score": -0.8767326785619053,
    "rmse": 0.8767326785619053
}
[08/28/2025 03:24:51 INFO]: Training loss at epoch 34: 0.7457317411899567
[08/28/2025 03:24:54 INFO]: Training accuracy: {
    "score": -1.01266367593793,
    "rmse": 1.01266367593793
}
[08/28/2025 03:24:54 INFO]: Val accuracy: {
    "score": -0.6604620452625619,
    "rmse": 0.6604620452625619
}
[08/28/2025 03:24:54 INFO]: Test accuracy: {
    "score": -0.870776613637437,
    "rmse": 0.870776613637437
}
[08/28/2025 03:24:54 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_63",
    "best_epoch": 9,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.870776613637437,
        "rmse": 0.870776613637437
    },
    "train_stats": {
        "score": -1.01266367593793,
        "rmse": 1.01266367593793
    },
    "val_stats": {
        "score": -0.6604620452625619,
        "rmse": 0.6604620452625619
    }
}
[08/28/2025 03:24:54 INFO]: Procewss finished for trial unhanged-Shanesha_trial_63
[08/28/2025 03:24:54 INFO]: 
_________________________________________________

[08/28/2025 03:24:54 INFO]: train_net_for_optune.py main() running.
[08/28/2025 03:24:54 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 1.7383842620495185
  attention_dropout: 0.11798176975620756
  ffn_dropout: 0.11798176975620756
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00017778409188251942
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_97

[08/28/2025 03:24:55 INFO]: This ft_transformer has 7.795 million parameters.
[08/28/2025 03:24:55 INFO]: Training will start at epoch 0.
[08/28/2025 03:24:55 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 03:25:10 INFO]: Training loss at epoch 35: 0.856059193611145
[08/28/2025 03:25:18 INFO]: Training loss at epoch 7: 1.4709357619285583
[08/28/2025 03:25:20 INFO]: Training loss at epoch 12: 0.9651419222354889
[08/28/2025 03:25:21 INFO]: Training loss at epoch 104: 0.9263759255409241
[08/28/2025 03:25:34 INFO]: New best epoch, val score: -0.6617237314598983
[08/28/2025 03:25:34 INFO]: Saving model to: unhanged-Shanesha_trial_95/model_best.pth
[08/28/2025 03:25:38 INFO]: Training loss at epoch 24: 0.9117003381252289
[08/28/2025 03:26:00 INFO]: Training loss at epoch 39: 0.9015463590621948
[08/28/2025 03:26:00 INFO]: Training loss at epoch 22: 0.8786387145519257
[08/28/2025 03:26:10 INFO]: Training stats: {
    "score": -1.0092327604194358,
    "rmse": 1.0092327604194358
}
[08/28/2025 03:26:10 INFO]: Val stats: {
    "score": -0.7255737697246469,
    "rmse": 0.7255737697246469
}
[08/28/2025 03:26:10 INFO]: Test stats: {
    "score": -0.8996128927893688,
    "rmse": 0.8996128927893688
}
[08/28/2025 03:26:13 INFO]: Training loss at epoch 35: 0.7898736596107483
[08/28/2025 03:26:19 INFO]: Training loss at epoch 7: 1.2013580799102783
[08/28/2025 03:26:28 INFO]: Training loss at epoch 17: 1.1233911514282227
[08/28/2025 03:26:28 INFO]: Training loss at epoch 30: 0.8936139047145844
[08/28/2025 03:26:50 INFO]: New best epoch, val score: -0.7255737697246469
[08/28/2025 03:26:50 INFO]: Saving model to: unhanged-Shanesha_trial_90/model_best.pth
[08/28/2025 03:26:54 INFO]: Training loss at epoch 9: 1.0956966876983643
[08/28/2025 03:27:09 INFO]: Training loss at epoch 70: 0.899399071931839
[08/28/2025 03:27:17 INFO]: Training loss at epoch 13: 0.9783615171909332
[08/28/2025 03:27:38 INFO]: Training loss at epoch 25: 0.9149956703186035
[08/28/2025 03:28:14 INFO]: Training stats: {
    "score": -0.999931709057311,
    "rmse": 0.999931709057311
}
[08/28/2025 03:28:14 INFO]: Val stats: {
    "score": -0.6826459647392158,
    "rmse": 0.6826459647392158
}
[08/28/2025 03:28:14 INFO]: Test stats: {
    "score": -0.8750872825844438,
    "rmse": 0.8750872825844438
}
[08/28/2025 03:28:20 INFO]: Training loss at epoch 31: 1.03309565782547
[08/28/2025 03:28:23 INFO]: Training loss at epoch 36: 1.0491658449172974
[08/28/2025 03:28:33 INFO]: Training loss at epoch 2: 1.1575609147548676
[08/28/2025 03:28:52 INFO]: Training stats: {
    "score": -1.0490444860837214,
    "rmse": 1.0490444860837214
}
[08/28/2025 03:28:52 INFO]: Val stats: {
    "score": -0.6777797344835248,
    "rmse": 0.6777797344835248
}
[08/28/2025 03:28:52 INFO]: Test stats: {
    "score": -0.896862336081793,
    "rmse": 0.896862336081793
}
[08/28/2025 03:29:17 INFO]: Training loss at epoch 14: 1.0383265018463135
[08/28/2025 03:29:25 INFO]: Training loss at epoch 35: 0.9137694239616394
[08/28/2025 03:29:31 INFO]: New best epoch, val score: -0.6777797344835248
[08/28/2025 03:29:31 INFO]: Saving model to: unhanged-Shanesha_trial_91/model_best.pth
[08/28/2025 03:29:36 INFO]: Training loss at epoch 26: 0.8637362718582153
[08/28/2025 03:29:42 INFO]: Training loss at epoch 0: 0.925355464220047
[08/28/2025 03:30:03 INFO]: Training loss at epoch 59: 0.8982694149017334
[08/28/2025 03:30:09 INFO]: Training loss at epoch 32: 1.0224612355232239
[08/28/2025 03:30:21 INFO]: New best epoch, val score: -0.6977931213071261
[08/28/2025 03:30:21 INFO]: Saving model to: unhanged-Shanesha_trial_97/model_best.pth
[08/28/2025 03:30:22 INFO]: Training loss at epoch 35: 0.999640703201294
[08/28/2025 03:30:30 INFO]: Training loss at epoch 37: 0.8834166526794434
[08/28/2025 03:30:33 INFO]: Training loss at epoch 34: 1.2398644089698792
[08/28/2025 03:30:38 INFO]: Training loss at epoch 36: 1.1019417643547058
[08/28/2025 03:30:55 INFO]: Training loss at epoch 8: 1.4457740187644958
[08/28/2025 03:31:09 INFO]: Training loss at epoch 18: 0.9485599100589752
[08/28/2025 03:31:12 INFO]: Training loss at epoch 15: 1.2237685322761536
[08/28/2025 03:31:24 INFO]: Training loss at epoch 23: 0.990795224905014
[08/28/2025 03:31:26 INFO]: New best epoch, val score: -0.661427442372794
[08/28/2025 03:31:26 INFO]: Saving model to: unhanged-Shanesha_trial_95/model_best.pth
[08/28/2025 03:31:26 INFO]: Training loss at epoch 27: 1.1674190163612366
[08/28/2025 03:31:30 INFO]: Training loss at epoch 10: 1.0423164367675781
[08/28/2025 03:31:36 INFO]: Training loss at epoch 8: 1.1838046312332153
[08/28/2025 03:31:51 INFO]: Training loss at epoch 33: 1.1273455619812012
[08/28/2025 03:32:02 INFO]: Training stats: {
    "score": -1.0010243966442587,
    "rmse": 1.0010243966442587
}
[08/28/2025 03:32:02 INFO]: Val stats: {
    "score": -0.6703612611586438,
    "rmse": 0.6703612611586438
}
[08/28/2025 03:32:02 INFO]: Test stats: {
    "score": -0.8698795354087571,
    "rmse": 0.8698795354087571
}
[08/28/2025 03:32:05 INFO]: New best epoch, val score: -0.6697111688626638
[08/28/2025 03:32:05 INFO]: Saving model to: unhanged-Shanesha_trial_90/model_best.pth
[08/28/2025 03:32:30 INFO]: Training loss at epoch 38: 1.0524293184280396
[08/28/2025 03:32:38 INFO]: Training loss at epoch 105: 0.9395914971828461
[08/28/2025 03:32:55 INFO]: Training loss at epoch 3: 1.5633354783058167
[08/28/2025 03:33:02 INFO]: Training loss at epoch 16: 0.9534290134906769
[08/28/2025 03:33:14 INFO]: Training loss at epoch 28: 0.9710914492607117
[08/28/2025 03:33:25 INFO]: New best epoch, val score: -0.8022846864533232
[08/28/2025 03:33:25 INFO]: Saving model to: unhanged-Shanesha_trial_96/model_best.pth
[08/28/2025 03:33:32 INFO]: Training loss at epoch 34: 1.0883072912693024
[08/28/2025 03:33:59 INFO]: Training loss at epoch 71: 0.9771877229213715
[08/28/2025 03:34:08 INFO]: Training loss at epoch 10: 0.8401917815208435
[08/28/2025 03:34:19 INFO]: Training loss at epoch 40: 0.9622904062271118
[08/28/2025 03:34:29 INFO]: Training loss at epoch 36: 1.0781362354755402
[08/28/2025 03:34:31 INFO]: Training loss at epoch 39: 1.0252776741981506
[08/28/2025 03:34:49 INFO]: Training loss at epoch 1: 1.860733687877655
[08/28/2025 03:34:56 INFO]: Training loss at epoch 17: 1.0989258885383606
[08/28/2025 03:35:07 INFO]: Training loss at epoch 29: 1.0313120484352112
[08/28/2025 03:35:16 INFO]: Training stats: {
    "score": -0.9987945790931787,
    "rmse": 0.9987945790931787
}
[08/28/2025 03:35:16 INFO]: Val stats: {
    "score": -0.6876642169155649,
    "rmse": 0.6876642169155649
}
[08/28/2025 03:35:16 INFO]: Test stats: {
    "score": -0.8796109004426793,
    "rmse": 0.8796109004426793
}
[08/28/2025 03:35:19 INFO]: Training loss at epoch 35: 0.9227697253227234
[08/28/2025 03:35:31 INFO]: Training loss at epoch 36: 1.052878051996231
[08/28/2025 03:35:32 INFO]: Running Final Evaluation...
[08/28/2025 03:35:38 INFO]: Training loss at epoch 19: 0.9711715281009674
[08/28/2025 03:35:47 INFO]: Training stats: {
    "score": -1.0139148808950844,
    "rmse": 1.0139148808950844
}
[08/28/2025 03:35:47 INFO]: Val stats: {
    "score": -0.6592669374244295,
    "rmse": 0.6592669374244295
}
[08/28/2025 03:35:47 INFO]: Test stats: {
    "score": -0.8759268704830943,
    "rmse": 0.8759268704830943
}
[08/28/2025 03:35:48 INFO]: Training loss at epoch 37: 1.1636417210102081
[08/28/2025 03:36:16 INFO]: Training loss at epoch 35: 1.0422682166099548
[08/28/2025 03:36:19 INFO]: Training loss at epoch 9: 1.056156724691391
[08/28/2025 03:36:19 INFO]: Training accuracy: {
    "score": -1.0043184132744782,
    "rmse": 1.0043184132744782
}
[08/28/2025 03:36:19 INFO]: Val accuracy: {
    "score": -0.663347992670512,
    "rmse": 0.663347992670512
}
[08/28/2025 03:36:19 INFO]: Test accuracy: {
    "score": -0.8694795907577136,
    "rmse": 0.8694795907577136
}
[08/28/2025 03:36:19 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_88",
    "best_epoch": 8,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8694795907577136,
        "rmse": 0.8694795907577136
    },
    "train_stats": {
        "score": -1.0043184132744782,
        "rmse": 1.0043184132744782
    },
    "val_stats": {
        "score": -0.663347992670512,
        "rmse": 0.663347992670512
    }
}
[08/28/2025 03:36:19 INFO]: Procewss finished for trial unhanged-Shanesha_trial_88
[08/28/2025 03:36:19 INFO]: 
_________________________________________________

[08/28/2025 03:36:19 INFO]: train_net_for_optune.py main() running.
[08/28/2025 03:36:19 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 0.9005418839809347
  attention_dropout: 0.11103587446250832
  ffn_dropout: 0.11103587446250832
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00015736397774336253
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_98

[08/28/2025 03:36:19 INFO]: This ft_transformer has 2.495 million parameters.
[08/28/2025 03:36:19 INFO]: Training will start at epoch 0.
[08/28/2025 03:36:19 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 03:36:41 INFO]: Training loss at epoch 24: 1.0656241178512573
[08/28/2025 03:36:41 INFO]: Training loss at epoch 11: 0.8962590396404266
[08/28/2025 03:36:47 INFO]: Training loss at epoch 9: 1.1608898043632507
[08/28/2025 03:36:55 INFO]: Training loss at epoch 18: 0.8698922395706177
[08/28/2025 03:37:11 INFO]: Training loss at epoch 36: 1.1730369925498962
[08/28/2025 03:37:14 INFO]: Training stats: {
    "score": -1.016297056914958,
    "rmse": 1.016297056914958
}
[08/28/2025 03:37:14 INFO]: Val stats: {
    "score": -0.7474299189585114,
    "rmse": 0.7474299189585114
}
[08/28/2025 03:37:14 INFO]: Test stats: {
    "score": -0.9146145459910761,
    "rmse": 0.9146145459910761
}
[08/28/2025 03:37:22 INFO]: New best epoch, val score: -0.6611376556743868
[08/28/2025 03:37:22 INFO]: Saving model to: unhanged-Shanesha_trial_90/model_best.pth
[08/28/2025 03:37:29 INFO]: Training loss at epoch 4: 1.1711938381195068
[08/28/2025 03:37:47 INFO]: Training loss at epoch 30: 0.9180796146392822
[08/28/2025 03:38:03 INFO]: New best epoch, val score: -0.6789134382440775
[08/28/2025 03:38:03 INFO]: Saving model to: unhanged-Shanesha_trial_96/model_best.pth
[08/28/2025 03:38:07 INFO]: Training loss at epoch 0: 1.6928933262825012
[08/28/2025 03:38:18 INFO]: Training loss at epoch 60: 1.0763519406318665
[08/28/2025 03:38:18 INFO]: Training stats: {
    "score": -1.104102259344589,
    "rmse": 1.104102259344589
}
[08/28/2025 03:38:18 INFO]: Val stats: {
    "score": -0.9140386898785547,
    "rmse": 0.9140386898785547
}
[08/28/2025 03:38:18 INFO]: Test stats: {
    "score": -1.037005725089249,
    "rmse": 1.037005725089249
}
[08/28/2025 03:38:21 INFO]: New best epoch, val score: -1.0049871006213371
[08/28/2025 03:38:21 INFO]: Saving model to: unhanged-Shanesha_trial_98/model_best.pth
[08/28/2025 03:38:41 INFO]: Training stats: {
    "score": -1.0688128054772212,
    "rmse": 1.0688128054772212
}
[08/28/2025 03:38:41 INFO]: Val stats: {
    "score": -0.6935017897057292,
    "rmse": 0.6935017897057292
}
[08/28/2025 03:38:41 INFO]: Test stats: {
    "score": -0.9111210909117509,
    "rmse": 0.9111210909117509
}
[08/28/2025 03:38:53 INFO]: Training loss at epoch 19: 1.2024943232536316
[08/28/2025 03:39:01 INFO]: Training loss at epoch 37: 0.9945749342441559
[08/28/2025 03:39:33 INFO]: Training stats: {
    "score": -1.0049128565570196,
    "rmse": 1.0049128565570196
}
[08/28/2025 03:39:33 INFO]: Val stats: {
    "score": -0.7145156307385632,
    "rmse": 0.7145156307385632
}
[08/28/2025 03:39:33 INFO]: Test stats: {
    "score": -0.8902032836788095,
    "rmse": 0.8902032836788095
}
[08/28/2025 03:39:40 INFO]: Training loss at epoch 31: 0.9887466430664062
[08/28/2025 03:39:40 INFO]: Training loss at epoch 11: 0.9247207045555115
[08/28/2025 03:39:52 INFO]: Training loss at epoch 37: 1.2854152023792267
[08/28/2025 03:39:54 INFO]: Training loss at epoch 106: 0.8495329022407532
[08/28/2025 03:40:00 INFO]: Training loss at epoch 1: 1.413755714893341
[08/28/2025 03:40:09 INFO]: Training loss at epoch 2: 1.7423534393310547
[08/28/2025 03:40:36 INFO]: Training loss at epoch 41: 1.017779141664505
[08/28/2025 03:40:42 INFO]: Training loss at epoch 38: 1.0499943792819977
[08/28/2025 03:40:52 INFO]: Training loss at epoch 37: 1.0502788126468658
[08/28/2025 03:40:57 INFO]: Training loss at epoch 72: 1.0028470158576965
[08/28/2025 03:41:05 INFO]: Training loss at epoch 38: 0.9385818839073181
[08/28/2025 03:41:22 INFO]: Training loss at epoch 20: 1.1102808117866516
[08/28/2025 03:41:27 INFO]: Training loss at epoch 32: 0.875247448682785
[08/28/2025 03:41:42 INFO]: Training loss at epoch 20: 0.7896046042442322
[08/28/2025 03:41:49 INFO]: Training loss at epoch 5: 1.153249979019165
[08/28/2025 03:41:50 INFO]: Training loss at epoch 2: 2.4336723685264587
[08/28/2025 03:41:51 INFO]: Training loss at epoch 12: 1.086872637271881
[08/28/2025 03:41:54 INFO]: Training loss at epoch 25: 1.19209486246109
[08/28/2025 03:42:02 INFO]: Training loss at epoch 36: 1.0423700213432312
[08/28/2025 03:42:20 INFO]: New best epoch, val score: -0.6778483190702707
[08/28/2025 03:42:20 INFO]: Saving model to: unhanged-Shanesha_trial_96/model_best.pth
[08/28/2025 03:42:22 INFO]: Training loss at epoch 39: 0.8567182421684265
[08/28/2025 03:42:58 INFO]: Training stats: {
    "score": -0.9994683797819448,
    "rmse": 0.9994683797819448
}
[08/28/2025 03:42:58 INFO]: Val stats: {
    "score": -0.6648155806216675,
    "rmse": 0.6648155806216675
}
[08/28/2025 03:42:58 INFO]: Test stats: {
    "score": -0.871395171752475,
    "rmse": 0.871395171752475
}
[08/28/2025 03:43:13 INFO]: Training loss at epoch 21: 0.8894065320491791
[08/28/2025 03:43:18 INFO]: Training loss at epoch 33: 0.8792192041873932
[08/28/2025 03:43:36 INFO]: Training loss at epoch 10: 1.1295284628868103
[08/28/2025 03:43:42 INFO]: Training loss at epoch 10: 0.9810277819633484
[08/28/2025 03:43:45 INFO]: Training loss at epoch 3: 1.4425427913665771
[08/28/2025 03:44:01 INFO]: New best epoch, val score: -0.6656014517138163
[08/28/2025 03:44:01 INFO]: Saving model to: unhanged-Shanesha_trial_98/model_best.pth
[08/28/2025 03:44:16 INFO]: New best epoch, val score: -0.7023780092979949
[08/28/2025 03:44:16 INFO]: Saving model to: unhanged-Shanesha_trial_93/model_best.pth
[08/28/2025 03:44:17 INFO]: Training loss at epoch 61: 1.279268354177475
[08/28/2025 03:44:47 INFO]: Training loss at epoch 40: 1.0997032523155212
[08/28/2025 03:44:59 INFO]: Training loss at epoch 12: 1.210779070854187
[08/28/2025 03:45:04 INFO]: Training loss at epoch 38: 1.426107794046402
[08/28/2025 03:45:11 INFO]: Training loss at epoch 22: 1.0206058621406555
[08/28/2025 03:45:14 INFO]: Training loss at epoch 34: 0.8885787427425385
[08/28/2025 03:45:18 INFO]: Training loss at epoch 3: 0.9852036535739899
[08/28/2025 03:45:29 INFO]: Running Final Evaluation...
[08/28/2025 03:45:45 INFO]: Training loss at epoch 4: 1.0517346262931824
[08/28/2025 03:46:09 INFO]: Training accuracy: {
    "score": -1.0057642259802297,
    "rmse": 1.0057642259802297
}
[08/28/2025 03:46:09 INFO]: Val accuracy: {
    "score": -0.6574356104553399,
    "rmse": 0.6574356104553399
}
[08/28/2025 03:46:09 INFO]: Test accuracy: {
    "score": -0.8709688741132602,
    "rmse": 0.8709688741132602
}
[08/28/2025 03:46:09 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_92",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8709688741132602,
        "rmse": 0.8709688741132602
    },
    "train_stats": {
        "score": -1.0057642259802297,
        "rmse": 1.0057642259802297
    },
    "val_stats": {
        "score": -0.6574356104553399,
        "rmse": 0.6574356104553399
    }
}
[08/28/2025 03:46:09 INFO]: Procewss finished for trial unhanged-Shanesha_trial_92
[08/28/2025 03:46:09 INFO]: 
_________________________________________________

[08/28/2025 03:46:09 INFO]: train_net_for_optune.py main() running.
[08/28/2025 03:46:09 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 1.8528051522239593
  attention_dropout: 0.11861402278217908
  ffn_dropout: 0.11861402278217908
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0001775924471675323
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_99

[08/28/2025 03:46:09 INFO]: This ft_transformer has 3.274 million parameters.
[08/28/2025 03:46:09 INFO]: Training will start at epoch 0.
[08/28/2025 03:46:09 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 03:46:12 INFO]: Training loss at epoch 38: 0.9998740255832672
[08/28/2025 03:46:22 INFO]: Training loss at epoch 21: 1.008437603712082
[08/28/2025 03:46:23 INFO]: Training loss at epoch 6: 1.0437049269676208
[08/28/2025 03:46:26 INFO]: Training loss at epoch 39: 0.9898633360862732
[08/28/2025 03:46:42 INFO]: Training loss at epoch 41: 0.8857618868350983
[08/28/2025 03:46:52 INFO]: Training loss at epoch 42: 1.074419766664505
[08/28/2025 03:46:58 INFO]: New best epoch, val score: -0.6747804307764609
[08/28/2025 03:46:58 INFO]: Saving model to: unhanged-Shanesha_trial_96/model_best.pth
[08/28/2025 03:47:06 INFO]: Training loss at epoch 107: 0.8723571002483368
[08/28/2025 03:47:10 INFO]: Training loss at epoch 23: 0.84620600938797
[08/28/2025 03:47:12 INFO]: Training loss at epoch 13: 1.0364683866500854
[08/28/2025 03:47:20 INFO]: Training loss at epoch 26: 0.9265040755271912
[08/28/2025 03:47:46 INFO]: Training loss at epoch 5: 1.2136294841766357
[08/28/2025 03:47:53 INFO]: Training loss at epoch 0: 1.2530637383460999
[08/28/2025 03:47:59 INFO]: Training loss at epoch 73: 0.8650406301021576
[08/28/2025 03:48:02 INFO]: New best epoch, val score: -0.6651932421637637
[08/28/2025 03:48:02 INFO]: Saving model to: unhanged-Shanesha_trial_81/model_best.pth
[08/28/2025 03:48:04 INFO]: Training loss at epoch 37: 0.9850100576877594
[08/28/2025 03:48:07 INFO]: New best epoch, val score: -0.7307369138976675
[08/28/2025 03:48:07 INFO]: Saving model to: unhanged-Shanesha_trial_99/model_best.pth
[08/28/2025 03:48:19 INFO]: Training stats: {
    "score": -0.9980732133730263,
    "rmse": 0.9980732133730263
}
[08/28/2025 03:48:19 INFO]: Val stats: {
    "score": -0.6869263113638095,
    "rmse": 0.6869263113638095
}
[08/28/2025 03:48:19 INFO]: Test stats: {
    "score": -0.8764832589899015,
    "rmse": 0.8764832589899015
}
[08/28/2025 03:48:33 INFO]: Training loss at epoch 42: 0.8165818452835083
[08/28/2025 03:49:05 INFO]: Training loss at epoch 24: 0.9971396327018738
[08/28/2025 03:49:05 INFO]: Training loss at epoch 11: 1.1313723027706146
[08/28/2025 03:49:14 INFO]: Training loss at epoch 11: 1.1088129580020905
[08/28/2025 03:49:39 INFO]: Training loss at epoch 6: 1.2861496806144714
[08/28/2025 03:49:41 INFO]: Training loss at epoch 1: 2.060562789440155
[08/28/2025 03:49:51 INFO]: New best epoch, val score: -0.6645927799245027
[08/28/2025 03:49:51 INFO]: Saving model to: unhanged-Shanesha_trial_93/model_best.pth
[08/28/2025 03:50:14 INFO]: Training loss at epoch 43: 0.7543801218271255
[08/28/2025 03:50:20 INFO]: Training loss at epoch 39: 1.0851090848445892
[08/28/2025 03:50:29 INFO]: Training loss at epoch 13: 1.1983710527420044
[08/28/2025 03:50:30 INFO]: Training loss at epoch 62: 0.8491435050964355
[08/28/2025 03:50:32 INFO]: Training loss at epoch 4: 1.157808393239975
[08/28/2025 03:50:50 INFO]: Training loss at epoch 7: 1.2983574271202087
[08/28/2025 03:50:53 INFO]: Training loss at epoch 22: 1.2574974298477173
[08/28/2025 03:50:54 INFO]: Training loss at epoch 25: 0.8298140466213226
[08/28/2025 03:51:08 INFO]: New best epoch, val score: -0.6624619996779357
[08/28/2025 03:51:08 INFO]: Saving model to: unhanged-Shanesha_trial_97/model_best.pth
[08/28/2025 03:51:21 INFO]: New best epoch, val score: -0.6708717088521965
[08/28/2025 03:51:21 INFO]: Saving model to: unhanged-Shanesha_trial_96/model_best.pth
[08/28/2025 03:51:25 INFO]: Training loss at epoch 39: 0.8709965348243713
[08/28/2025 03:51:26 INFO]: Training loss at epoch 2: 1.1830092668533325
[08/28/2025 03:51:29 INFO]: Training loss at epoch 7: 1.5038031339645386
[08/28/2025 03:51:55 INFO]: Training loss at epoch 44: 1.104898750782013
[08/28/2025 03:52:00 INFO]: Training stats: {
    "score": -1.00354970366392,
    "rmse": 1.00354970366392
}
[08/28/2025 03:52:00 INFO]: Val stats: {
    "score": -0.6633849932514002,
    "rmse": 0.6633849932514002
}
[08/28/2025 03:52:00 INFO]: Test stats: {
    "score": -0.8691449360613799,
    "rmse": 0.8691449360613799
}
[08/28/2025 03:52:16 INFO]: Training loss at epoch 14: 1.1250635981559753
[08/28/2025 03:52:29 INFO]: Training loss at epoch 27: 0.9586665332317352
[08/28/2025 03:52:47 INFO]: Training loss at epoch 26: 1.1141814291477203
[08/28/2025 03:52:54 INFO]: New best epoch, val score: -0.6610013471663535
[08/28/2025 03:52:54 INFO]: Saving model to: unhanged-Shanesha_trial_90/model_best.pth
[08/28/2025 03:52:56 INFO]: Training loss at epoch 43: 1.1812333166599274
[08/28/2025 03:53:10 INFO]: New best epoch, val score: -0.664068805176986
[08/28/2025 03:53:10 INFO]: Saving model to: unhanged-Shanesha_trial_81/model_best.pth
[08/28/2025 03:53:15 INFO]: Training stats: {
    "score": -1.004865351645552,
    "rmse": 1.004865351645552
}
[08/28/2025 03:53:15 INFO]: Val stats: {
    "score": -0.7169851675695001,
    "rmse": 0.7169851675695001
}
[08/28/2025 03:53:15 INFO]: Test stats: {
    "score": -0.8957635810254886,
    "rmse": 0.8957635810254886
}
[08/28/2025 03:53:19 INFO]: Training loss at epoch 3: 1.2142703533172607
[08/28/2025 03:53:26 INFO]: Training loss at epoch 8: 1.382383644580841
[08/28/2025 03:53:26 INFO]: Training loss at epoch 40: 1.0126671493053436
[08/28/2025 03:53:45 INFO]: Training loss at epoch 45: 0.986731618642807
[08/28/2025 03:53:48 INFO]: Training loss at epoch 38: 1.0535272359848022
[08/28/2025 03:54:12 INFO]: Training loss at epoch 108: 1.046816885471344
[08/28/2025 03:54:17 INFO]: Training loss at epoch 12: 1.162851870059967
[08/28/2025 03:54:39 INFO]: Training loss at epoch 12: 0.9542145729064941
[08/28/2025 03:54:45 INFO]: Training loss at epoch 74: 0.8944294452667236
[08/28/2025 03:54:46 INFO]: Training loss at epoch 27: 1.1012794077396393
[08/28/2025 03:55:15 INFO]: Training loss at epoch 4: 0.9295998513698578
[08/28/2025 03:55:23 INFO]: Training loss at epoch 8: 1.23142808675766
[08/28/2025 03:55:28 INFO]: Training loss at epoch 9: 1.3398634791374207
[08/28/2025 03:55:29 INFO]: New best epoch, val score: -0.6889389831529493
[08/28/2025 03:55:29 INFO]: Saving model to: unhanged-Shanesha_trial_99/model_best.pth
[08/28/2025 03:55:33 INFO]: Training loss at epoch 23: 0.8821287751197815
[08/28/2025 03:55:36 INFO]: Training loss at epoch 46: 0.9309270083904266
[08/28/2025 03:55:51 INFO]: Running Final Evaluation...
[08/28/2025 03:55:52 INFO]: Training loss at epoch 5: 0.9321579039096832
[08/28/2025 03:55:58 INFO]: New best epoch, val score: -0.6671039871875366
[08/28/2025 03:55:58 INFO]: Saving model to: unhanged-Shanesha_trial_96/model_best.pth
[08/28/2025 03:56:02 INFO]: Training loss at epoch 14: 1.3245643377304077
[08/28/2025 03:56:12 INFO]: Training stats: {
    "score": -1.1147434164371017,
    "rmse": 1.1147434164371017
}
[08/28/2025 03:56:12 INFO]: Val stats: {
    "score": -0.7412236118023949,
    "rmse": 0.7412236118023949
}
[08/28/2025 03:56:12 INFO]: Test stats: {
    "score": -0.9537430105260262,
    "rmse": 0.9537430105260262
}
[08/28/2025 03:56:28 INFO]: Training accuracy: {
    "score": -1.0098439300108966,
    "rmse": 1.0098439300108966
}
[08/28/2025 03:56:28 INFO]: Val accuracy: {
    "score": -0.6601930363164528,
    "rmse": 0.6601930363164528
}
[08/28/2025 03:56:28 INFO]: Test accuracy: {
    "score": -0.8726288679408186,
    "rmse": 0.8726288679408186
}
[08/28/2025 03:56:28 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_89",
    "best_epoch": 15,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8726288679408186,
        "rmse": 0.8726288679408186
    },
    "train_stats": {
        "score": -1.0098439300108966,
        "rmse": 1.0098439300108966
    },
    "val_stats": {
        "score": -0.6601930363164528,
        "rmse": 0.6601930363164528
    }
}
[08/28/2025 03:56:28 INFO]: Procewss finished for trial unhanged-Shanesha_trial_89
[08/28/2025 03:56:29 INFO]: 
_________________________________________________

[08/28/2025 03:56:29 INFO]: train_net_for_optune.py main() running.
[08/28/2025 03:56:29 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 1.9063290043890941
  attention_dropout: 0.1320714061010367
  ffn_dropout: 0.1320714061010367
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0001612344452836393
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_100

[08/28/2025 03:56:29 INFO]: This ft_transformer has 3.320 million parameters.
[08/28/2025 03:56:29 INFO]: Training will start at epoch 0.
[08/28/2025 03:56:29 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 03:56:44 INFO]: Training loss at epoch 28: 0.945340633392334
[08/28/2025 03:56:46 INFO]: Training loss at epoch 63: 0.8672499656677246
[08/28/2025 03:57:08 INFO]: Training loss at epoch 5: 1.1911445260047913
[08/28/2025 03:57:22 INFO]: New best epoch, val score: -0.6877241348352778
[08/28/2025 03:57:22 INFO]: Saving model to: unhanged-Shanesha_trial_99/model_best.pth
[08/28/2025 03:57:28 INFO]: Training loss at epoch 40: 0.9402708411216736
[08/28/2025 03:57:38 INFO]: Training loss at epoch 15: 0.9949901401996613
[08/28/2025 03:57:56 INFO]: Training loss at epoch 28: 1.0038687586784363
[08/28/2025 03:58:04 INFO]: Training loss at epoch 0: 1.5801676511764526
[08/28/2025 03:58:05 INFO]: Training loss at epoch 10: 1.1501551270484924
[08/28/2025 03:58:17 INFO]: New best epoch, val score: -0.7751577258013781
[08/28/2025 03:58:17 INFO]: Saving model to: unhanged-Shanesha_trial_100/model_best.pth
[08/28/2025 03:58:18 INFO]: New best epoch, val score: -0.6648255957498571
[08/28/2025 03:58:18 INFO]: Saving model to: unhanged-Shanesha_trial_98/model_best.pth
[08/28/2025 03:58:35 INFO]: Training loss at epoch 29: 1.027911126613617
[08/28/2025 03:58:37 INFO]: Training loss at epoch 40: 1.0366531014442444
[08/28/2025 03:58:46 INFO]: Training loss at epoch 41: 1.0676221251487732
[08/28/2025 03:58:55 INFO]: Training loss at epoch 6: 1.0931108593940735
[08/28/2025 03:59:07 INFO]: New best epoch, val score: -0.684087577649893
[08/28/2025 03:59:07 INFO]: Saving model to: unhanged-Shanesha_trial_99/model_best.pth
[08/28/2025 03:59:12 INFO]: Training loss at epoch 44: 1.243722915649414
[08/28/2025 03:59:14 INFO]: Training stats: {
    "score": -1.0029159708657933,
    "rmse": 1.0029159708657933
}
[08/28/2025 03:59:14 INFO]: Val stats: {
    "score": -0.6659181505900766,
    "rmse": 0.6659181505900766
}
[08/28/2025 03:59:14 INFO]: Test stats: {
    "score": -0.8670771749484296,
    "rmse": 0.8670771749484296
}
[08/28/2025 03:59:29 INFO]: Training loss at epoch 13: 1.339540183544159
[08/28/2025 03:59:39 INFO]: Training loss at epoch 39: 0.8901316523551941
[08/28/2025 03:59:45 INFO]: Training loss at epoch 9: 1.0721391439437866
[08/28/2025 03:59:50 INFO]: Training loss at epoch 1: 1.5623942017555237
[08/28/2025 03:59:55 INFO]: Training loss at epoch 11: 1.0818315744400024
[08/28/2025 04:00:00 INFO]: Training loss at epoch 24: 0.9671141505241394
[08/28/2025 04:00:03 INFO]: Training loss at epoch 13: 1.0561318695545197
[08/28/2025 04:00:42 INFO]: Training loss at epoch 7: 0.876816987991333
[08/28/2025 04:00:54 INFO]: New best epoch, val score: -0.6795619454219087
[08/28/2025 04:00:54 INFO]: Saving model to: unhanged-Shanesha_trial_99/model_best.pth
[08/28/2025 04:00:55 INFO]: Training loss at epoch 6: 1.087974488735199
[08/28/2025 04:01:04 INFO]: Training loss at epoch 30: 0.8397314250469208
[08/28/2025 04:01:12 INFO]: Training stats: {
    "score": -1.0261096372920964,
    "rmse": 1.0261096372920964
}
[08/28/2025 04:01:12 INFO]: Val stats: {
    "score": -0.6640504527517095,
    "rmse": 0.6640504527517095
}
[08/28/2025 04:01:12 INFO]: Test stats: {
    "score": -0.8792818860438351,
    "rmse": 0.8792818860438351
}
[08/28/2025 04:01:15 INFO]: Training loss at epoch 15: 0.9609385430812836
[08/28/2025 04:01:15 INFO]: Training loss at epoch 109: 0.9211365282535553
[08/28/2025 04:01:31 INFO]: Training loss at epoch 75: 0.9891231954097748
[08/28/2025 04:01:32 INFO]: Training stats: {
    "score": -1.0021658017995547,
    "rmse": 1.0021658017995547
}
[08/28/2025 04:01:32 INFO]: Val stats: {
    "score": -0.6674918937422708,
    "rmse": 0.6674918937422708
}
[08/28/2025 04:01:32 INFO]: Test stats: {
    "score": -0.8690362736132907,
    "rmse": 0.8690362736132907
}
[08/28/2025 04:01:37 INFO]: Training loss at epoch 2: 1.5386391878128052
[08/28/2025 04:01:45 INFO]: New best epoch, val score: -0.6640504527517095
[08/28/2025 04:01:45 INFO]: Saving model to: unhanged-Shanesha_trial_96/model_best.pth
[08/28/2025 04:01:48 INFO]: Training loss at epoch 12: 1.0637885928153992
[08/28/2025 04:01:50 INFO]: New best epoch, val score: -0.6822302946902546
[08/28/2025 04:01:50 INFO]: Saving model to: unhanged-Shanesha_trial_100/model_best.pth
[08/28/2025 04:02:32 INFO]: Training loss at epoch 41: 1.0717776417732239
[08/28/2025 04:02:35 INFO]: Training loss at epoch 8: 1.2855027914047241
[08/28/2025 04:02:41 INFO]: Training loss at epoch 16: 0.8665579259395599
[08/28/2025 04:02:45 INFO]: Training loss at epoch 64: 1.0776756703853607
[08/28/2025 04:02:48 INFO]: New best epoch, val score: -0.6746381792722085
[08/28/2025 04:02:48 INFO]: Saving model to: unhanged-Shanesha_trial_99/model_best.pth
[08/28/2025 04:03:01 INFO]: Training loss at epoch 31: 0.9425445795059204
[08/28/2025 04:03:09 INFO]: Training loss at epoch 29: 0.8663733005523682
[08/28/2025 04:03:34 INFO]: Training loss at epoch 3: 0.7770504355430603
[08/28/2025 04:03:47 INFO]: Training stats: {
    "score": -0.9751985174365858,
    "rmse": 0.9751985174365858
}
[08/28/2025 04:03:47 INFO]: Val stats: {
    "score": -0.7336082236846297,
    "rmse": 0.7336082236846297
}
[08/28/2025 04:03:47 INFO]: Test stats: {
    "score": -0.9301539343396461,
    "rmse": 0.9301539343396461
}
[08/28/2025 04:03:49 INFO]: Training loss at epoch 13: 1.0688782334327698
[08/28/2025 04:03:55 INFO]: Training loss at epoch 41: 0.7614472657442093
[08/28/2025 04:04:02 INFO]: Training loss at epoch 42: 0.9734929203987122
[08/28/2025 04:04:32 INFO]: Training loss at epoch 9: 1.0543806552886963
[08/28/2025 04:04:40 INFO]: Training loss at epoch 25: 0.8995404541492462
[08/28/2025 04:04:46 INFO]: Training loss at epoch 14: 1.1123299300670624
[08/28/2025 04:05:00 INFO]: Training loss at epoch 32: 1.0623690783977509
[08/28/2025 04:05:09 INFO]: Training stats: {
    "score": -0.9984539783815257,
    "rmse": 0.9984539783815257
}
[08/28/2025 04:05:09 INFO]: Val stats: {
    "score": -0.6841912543098182,
    "rmse": 0.6841912543098182
}
[08/28/2025 04:05:09 INFO]: Test stats: {
    "score": -0.8756175770642808,
    "rmse": 0.8756175770642808
}
[08/28/2025 04:05:12 INFO]: Training stats: {
    "score": -1.0349677984668242,
    "rmse": 1.0349677984668242
}
[08/28/2025 04:05:12 INFO]: Val stats: {
    "score": -0.6696780258903364,
    "rmse": 0.6696780258903364
}
[08/28/2025 04:05:12 INFO]: Test stats: {
    "score": -0.8847733667159509,
    "rmse": 0.8847733667159509
}
[08/28/2025 04:05:26 INFO]: New best epoch, val score: -0.6696780258903364
[08/28/2025 04:05:26 INFO]: Saving model to: unhanged-Shanesha_trial_99/model_best.pth
[08/28/2025 04:05:31 INFO]: Training loss at epoch 45: 0.8697635531425476
[08/28/2025 04:05:32 INFO]: Training loss at epoch 4: 1.0795659124851227
[08/28/2025 04:05:38 INFO]: Training loss at epoch 14: 1.3370047807693481
[08/28/2025 04:05:45 INFO]: New best epoch, val score: -0.6658505660429956
[08/28/2025 04:05:45 INFO]: Saving model to: unhanged-Shanesha_trial_100/model_best.pth
[08/28/2025 04:05:49 INFO]: Training loss at epoch 14: 0.9260669648647308
[08/28/2025 04:05:49 INFO]: Training loss at epoch 10: 1.0765771269798279
[08/28/2025 04:06:17 INFO]: Training loss at epoch 7: 0.9612764716148376
[08/28/2025 04:06:48 INFO]: Training loss at epoch 16: 0.9948097169399261
[08/28/2025 04:06:53 INFO]: Training loss at epoch 33: 1.073240578174591
[08/28/2025 04:07:00 INFO]: Training loss at epoch 10: 1.0387166738510132
[08/28/2025 04:07:18 INFO]: Training loss at epoch 5: 1.0314472317695618
[08/28/2025 04:07:24 INFO]: New best epoch, val score: -0.6712322480101998
[08/28/2025 04:07:24 INFO]: Saving model to: unhanged-Shanesha_trial_91/model_best.pth
[08/28/2025 04:07:27 INFO]: Training loss at epoch 40: 0.9559110403060913
[08/28/2025 04:07:31 INFO]: New best epoch, val score: -0.6649771315609257
[08/28/2025 04:07:31 INFO]: Saving model to: unhanged-Shanesha_trial_100/model_best.pth
[08/28/2025 04:07:39 INFO]: Training loss at epoch 15: 1.334743320941925
[08/28/2025 04:07:48 INFO]: Training loss at epoch 42: 0.9671519696712494
[08/28/2025 04:07:55 INFO]: Training loss at epoch 17: 0.9690922200679779
[08/28/2025 04:08:26 INFO]: Training loss at epoch 76: 0.9885854125022888
[08/28/2025 04:08:42 INFO]: Training loss at epoch 34: 0.898313045501709
[08/28/2025 04:08:45 INFO]: Training loss at epoch 11: 0.8216377794742584
[08/28/2025 04:08:52 INFO]: Training loss at epoch 65: 1.0944531559944153
[08/28/2025 04:09:04 INFO]: Training loss at epoch 6: 1.019568145275116
[08/28/2025 04:09:04 INFO]: Training loss at epoch 26: 0.9465756118297577
[08/28/2025 04:09:06 INFO]: Training loss at epoch 42: 0.9838093221187592
[08/28/2025 04:09:11 INFO]: Training loss at epoch 43: 1.1184935569763184
[08/28/2025 04:09:16 INFO]: New best epoch, val score: -0.6642166374776282
[08/28/2025 04:09:16 INFO]: Saving model to: unhanged-Shanesha_trial_100/model_best.pth
[08/28/2025 04:09:28 INFO]: Training loss at epoch 16: 0.8479525148868561
[08/28/2025 04:09:48 INFO]: Training loss at epoch 15: 1.2067999839782715
[08/28/2025 04:10:02 INFO]: Training loss at epoch 11: 1.1732097864151
[08/28/2025 04:10:15 INFO]: Training loss at epoch 30: 1.0571521520614624
[08/28/2025 04:10:34 INFO]: Training loss at epoch 12: 0.9869058728218079
[08/28/2025 04:10:34 INFO]: Training loss at epoch 35: 0.9059550166130066
[08/28/2025 04:10:51 INFO]: Training loss at epoch 110: 0.9665262699127197
[08/28/2025 04:10:53 INFO]: Training loss at epoch 15: 1.1433172821998596
[08/28/2025 04:10:54 INFO]: Training loss at epoch 7: 0.8088360726833344
[08/28/2025 04:11:09 INFO]: New best epoch, val score: -0.663719502711207
[08/28/2025 04:11:09 INFO]: Saving model to: unhanged-Shanesha_trial_100/model_best.pth
[08/28/2025 04:11:22 INFO]: Training loss at epoch 8: 0.983055830001831
[08/28/2025 04:11:27 INFO]: Training loss at epoch 17: 1.1709516644477844
[08/28/2025 04:11:33 INFO]: Training loss at epoch 46: 1.0657732486724854
[08/28/2025 04:12:10 INFO]: Training loss at epoch 17: 1.071716845035553
[08/28/2025 04:12:30 INFO]: Training loss at epoch 13: 1.0661281943321228
[08/28/2025 04:12:33 INFO]: Training loss at epoch 36: 1.1085067391395569
[08/28/2025 04:12:51 INFO]: New best epoch, val score: -0.66084784808877
[08/28/2025 04:12:51 INFO]: Saving model to: unhanged-Shanesha_trial_91/model_best.pth
[08/28/2025 04:12:53 INFO]: Training loss at epoch 8: 0.8915223181247711
[08/28/2025 04:13:05 INFO]: Training loss at epoch 43: 1.0052571892738342
[08/28/2025 04:13:08 INFO]: Training loss at epoch 18: 0.8568342924118042
[08/28/2025 04:13:18 INFO]: Training loss at epoch 41: 1.179421991109848
[08/28/2025 04:13:28 INFO]: Training loss at epoch 18: 1.4967970252037048
[08/28/2025 04:13:47 INFO]: Training loss at epoch 27: 0.9771081507205963
[08/28/2025 04:14:26 INFO]: Training loss at epoch 14: 0.9354250133037567
[08/28/2025 04:14:31 INFO]: Training loss at epoch 37: 1.0381460785865784
[08/28/2025 04:14:35 INFO]: Training loss at epoch 43: 0.8804551661014557
[08/28/2025 04:14:41 INFO]: Training loss at epoch 12: 1.0906281471252441
[08/28/2025 04:14:42 INFO]: Training loss at epoch 44: 1.0950465500354767
[08/28/2025 04:14:49 INFO]: Training loss at epoch 9: 1.0614395141601562
[08/28/2025 04:15:11 INFO]: Training loss at epoch 66: 1.1031084060668945
[08/28/2025 04:15:12 INFO]: Training loss at epoch 16: 1.1036924123764038
[08/28/2025 04:15:25 INFO]: Training stats: {
    "score": -1.0008234160957004,
    "rmse": 1.0008234160957004
}
[08/28/2025 04:15:25 INFO]: Val stats: {
    "score": -0.6645245682630703,
    "rmse": 0.6645245682630703
}
[08/28/2025 04:15:25 INFO]: Test stats: {
    "score": -0.868803793900504,
    "rmse": 0.868803793900504
}
[08/28/2025 04:15:26 INFO]: Training loss at epoch 19: 1.05152627825737
[08/28/2025 04:15:30 INFO]: Training loss at epoch 77: 0.983175128698349
[08/28/2025 04:15:41 INFO]: Training loss at epoch 31: 0.9919655025005341
[08/28/2025 04:16:05 INFO]: Training stats: {
    "score": -0.9987155070230359,
    "rmse": 0.9987155070230359
}
[08/28/2025 04:16:05 INFO]: Val stats: {
    "score": -0.6881126110440496,
    "rmse": 0.6881126110440496
}
[08/28/2025 04:16:05 INFO]: Test stats: {
    "score": -0.8768537348696334,
    "rmse": 0.8768537348696334
}
[08/28/2025 04:16:13 INFO]: Training loss at epoch 15: 1.1007974743843079
[08/28/2025 04:16:22 INFO]: Training loss at epoch 38: 0.9617731869220734
[08/28/2025 04:16:29 INFO]: Training loss at epoch 16: 1.264121949672699
[08/28/2025 04:16:39 INFO]: Training loss at epoch 9: 0.9878510534763336
[08/28/2025 04:17:10 INFO]: Training loss at epoch 10: 0.8239471018314362
[08/28/2025 04:17:33 INFO]: Training loss at epoch 18: 0.8963851034641266
[08/28/2025 04:17:43 INFO]: Training loss at epoch 47: 1.0593334138393402
[08/28/2025 04:17:56 INFO]: Training loss at epoch 20: 1.1726813316345215
[08/28/2025 04:18:01 INFO]: Training loss at epoch 16: 0.9754167795181274
[08/28/2025 04:18:02 INFO]: Training loss at epoch 111: 1.0682473182678223
[08/28/2025 04:18:11 INFO]: Training loss at epoch 39: 1.1994874477386475
[08/28/2025 04:18:12 INFO]: Training loss at epoch 44: 0.9161453545093536
[08/28/2025 04:18:12 INFO]: Training loss at epoch 28: 1.1971731185913086
[08/28/2025 04:18:13 INFO]: Training loss at epoch 19: 0.8652357757091522
[08/28/2025 04:18:20 INFO]: Training stats: {
    "score": -1.000339749990102,
    "rmse": 1.000339749990102
}
[08/28/2025 04:18:20 INFO]: Val stats: {
    "score": -0.6694351186489165,
    "rmse": 0.6694351186489165
}
[08/28/2025 04:18:20 INFO]: Test stats: {
    "score": -0.8708605584087596,
    "rmse": 0.8708605584087596
}
[08/28/2025 04:18:53 INFO]: Training stats: {
    "score": -0.9989617324494107,
    "rmse": 0.9989617324494107
}
[08/28/2025 04:18:53 INFO]: Val stats: {
    "score": -0.6950472963166884,
    "rmse": 0.6950472963166884
}
[08/28/2025 04:18:53 INFO]: Test stats: {
    "score": -0.8781076377457312,
    "rmse": 0.8781076377457312
}
[08/28/2025 04:18:55 INFO]: Training loss at epoch 13: 1.4188525080680847
[08/28/2025 04:18:58 INFO]: Training loss at epoch 11: 0.8984338641166687
[08/28/2025 04:18:58 INFO]: Training loss at epoch 42: 1.0272058248519897
[08/28/2025 04:19:40 INFO]: Training loss at epoch 44: 1.1355348229408264
[08/28/2025 04:19:47 INFO]: Training loss at epoch 45: 1.0730087161064148
[08/28/2025 04:19:53 INFO]: Training loss at epoch 21: 1.0622036457061768
[08/28/2025 04:19:53 INFO]: Training loss at epoch 17: 0.9612069427967072
[08/28/2025 04:20:02 INFO]: Training stats: {
    "score": -1.0183475781120597,
    "rmse": 1.0183475781120597
}
[08/28/2025 04:20:02 INFO]: Val stats: {
    "score": -0.7506814320223114,
    "rmse": 0.7506814320223114
}
[08/28/2025 04:20:02 INFO]: Test stats: {
    "score": -0.91596081303048,
    "rmse": 0.91596081303048
}
[08/28/2025 04:20:06 INFO]: New best epoch, val score: -0.6608942736137011
[08/28/2025 04:20:06 INFO]: Saving model to: unhanged-Shanesha_trial_99/model_best.pth
[08/28/2025 04:20:08 INFO]: New best epoch, val score: -0.6636466737674227
[08/28/2025 04:20:08 INFO]: Saving model to: unhanged-Shanesha_trial_98/model_best.pth
[08/28/2025 04:20:19 INFO]: Training loss at epoch 17: 1.1159292161464691
[08/28/2025 04:20:50 INFO]: Training loss at epoch 40: 1.089242935180664
[08/28/2025 04:20:54 INFO]: Training loss at epoch 12: 1.0407598614692688
[08/28/2025 04:20:56 INFO]: Training loss at epoch 32: 0.9908680617809296
[08/28/2025 04:20:58 INFO]: New best epoch, val score: -0.6599391991249106
[08/28/2025 04:20:58 INFO]: Saving model to: unhanged-Shanesha_trial_94/model_best.pth
[08/28/2025 04:21:16 INFO]: Training loss at epoch 67: 0.8656792640686035
[08/28/2025 04:21:50 INFO]: Training loss at epoch 18: 0.8249242305755615
[08/28/2025 04:21:54 INFO]: Training loss at epoch 22: 1.0822839140892029
[08/28/2025 04:21:59 INFO]: Training loss at epoch 17: 1.018332600593567
[08/28/2025 04:22:10 INFO]: New best epoch, val score: -0.6622312912704638
[08/28/2025 04:22:10 INFO]: Saving model to: unhanged-Shanesha_trial_98/model_best.pth
[08/28/2025 04:22:24 INFO]: Training loss at epoch 78: 0.9773330688476562
[08/28/2025 04:22:50 INFO]: Training loss at epoch 41: 0.9745546281337738
[08/28/2025 04:22:52 INFO]: Training loss at epoch 13: 0.8668161630630493
[08/28/2025 04:22:57 INFO]: Training loss at epoch 29: 0.950221449136734
[08/28/2025 04:23:08 INFO]: Training loss at epoch 19: 1.0941521525382996
[08/28/2025 04:23:30 INFO]: Training loss at epoch 14: 0.9532482028007507
[08/28/2025 04:23:36 INFO]: Training loss at epoch 45: 0.97385174036026
[08/28/2025 04:23:42 INFO]: Training loss at epoch 19: 1.188502162694931
[08/28/2025 04:23:43 INFO]: Training loss at epoch 10: 1.0069235563278198
[08/28/2025 04:23:52 INFO]: Training loss at epoch 23: 1.017458975315094
[08/28/2025 04:24:02 INFO]: Training loss at epoch 48: 0.9125387370586395
[08/28/2025 04:24:06 INFO]: New best epoch, val score: -0.6621107239847583
[08/28/2025 04:24:06 INFO]: Saving model to: unhanged-Shanesha_trial_98/model_best.pth
[08/28/2025 04:24:18 INFO]: Training stats: {
    "score": -1.023902851374874,
    "rmse": 1.023902851374874
}
[08/28/2025 04:24:18 INFO]: Val stats: {
    "score": -0.663996091241114,
    "rmse": 0.663996091241114
}
[08/28/2025 04:24:18 INFO]: Test stats: {
    "score": -0.8781043197070376,
    "rmse": 0.8781043197070376
}
[08/28/2025 04:24:24 INFO]: Training stats: {
    "score": -1.0012367130275925,
    "rmse": 1.0012367130275925
}
[08/28/2025 04:24:24 INFO]: Val stats: {
    "score": -0.6675757491588481,
    "rmse": 0.6675757491588481
}
[08/28/2025 04:24:24 INFO]: Test stats: {
    "score": -0.8702874894268713,
    "rmse": 0.8702874894268713
}
[08/28/2025 04:24:38 INFO]: Training loss at epoch 14: 1.1380880177021027
[08/28/2025 04:24:39 INFO]: Training loss at epoch 42: 0.8865611553192139
[08/28/2025 04:24:51 INFO]: Training stats: {
    "score": -1.0371961798649154,
    "rmse": 1.0371961798649154
}
[08/28/2025 04:24:51 INFO]: Val stats: {
    "score": -0.6701719494941414,
    "rmse": 0.6701719494941414
}
[08/28/2025 04:24:51 INFO]: Test stats: {
    "score": -0.8876324874912817,
    "rmse": 0.8876324874912817
}
[08/28/2025 04:24:53 INFO]: Training loss at epoch 43: 1.066063106060028
[08/28/2025 04:25:02 INFO]: Training loss at epoch 45: 0.8974242806434631
[08/28/2025 04:25:07 INFO]: Training loss at epoch 46: 1.2145794034004211
[08/28/2025 04:25:15 INFO]: Training loss at epoch 20: 1.3042954206466675
[08/28/2025 04:25:22 INFO]: Training loss at epoch 112: 0.7403520345687866
[08/28/2025 04:25:31 INFO]: Training loss at epoch 18: 1.1780964732170105
[08/28/2025 04:25:42 INFO]: Training loss at epoch 24: 0.8639333844184875
[08/28/2025 04:26:03 INFO]: Training loss at epoch 20: 1.091479241847992
[08/28/2025 04:26:11 INFO]: Training loss at epoch 33: 0.9859781563282013
[08/28/2025 04:26:15 INFO]: New best epoch, val score: -0.6606360417023133
[08/28/2025 04:26:15 INFO]: Saving model to: unhanged-Shanesha_trial_99/model_best.pth
[08/28/2025 04:26:24 INFO]: Training loss at epoch 15: 0.9476081430912018
[08/28/2025 04:26:28 INFO]: Training loss at epoch 43: 0.9076818525791168
[08/28/2025 04:26:36 INFO]: New best epoch, val score: -0.6606225590662338
[08/28/2025 04:26:36 INFO]: Saving model to: unhanged-Shanesha_trial_100/model_best.pth
[08/28/2025 04:27:15 INFO]: Training loss at epoch 68: 1.053985208272934
[08/28/2025 04:27:16 INFO]: Training loss at epoch 18: 0.9228635728359222
[08/28/2025 04:27:31 INFO]: Training loss at epoch 25: 1.0635345578193665
[08/28/2025 04:27:43 INFO]: Training loss at epoch 15: 1.068733274936676
[08/28/2025 04:27:50 INFO]: Training loss at epoch 21: 1.0543373227119446
[08/28/2025 04:28:12 INFO]: Training loss at epoch 16: 0.8681604564189911
[08/28/2025 04:28:20 INFO]: Training loss at epoch 44: 1.1631922125816345
[08/28/2025 04:28:27 INFO]: New best epoch, val score: -0.6591815630753847
[08/28/2025 04:28:27 INFO]: Saving model to: unhanged-Shanesha_trial_100/model_best.pth
[08/28/2025 04:28:39 INFO]: Training loss at epoch 46: 0.9644248783588409
[08/28/2025 04:28:50 INFO]: Training loss at epoch 30: 0.9985379874706268
[08/28/2025 04:28:54 INFO]: Training loss at epoch 11: 0.9473978281021118
[08/28/2025 04:29:07 INFO]: Training loss at epoch 79: 0.8547568917274475
[08/28/2025 04:29:30 INFO]: Training loss at epoch 26: 1.098830133676529
[08/28/2025 04:29:46 INFO]: Training loss at epoch 22: 0.9330361783504486
[08/28/2025 04:30:07 INFO]: Training loss at epoch 49: 0.9085029363632202
[08/28/2025 04:30:09 INFO]: Training loss at epoch 17: 0.9776130020618439
[08/28/2025 04:30:12 INFO]: Training loss at epoch 20: 0.9349896907806396
[08/28/2025 04:30:17 INFO]: Training loss at epoch 46: 0.7770698070526123
[08/28/2025 04:30:18 INFO]: Training loss at epoch 45: 0.927188903093338
[08/28/2025 04:30:22 INFO]: Training loss at epoch 47: 0.9081990420818329
[08/28/2025 04:30:25 INFO]: Training loss at epoch 21: 0.9468740522861481
[08/28/2025 04:30:41 INFO]: Training loss at epoch 44: 0.9882623255252838
[08/28/2025 04:30:45 INFO]: Training loss at epoch 19: 1.1013826131820679
[08/28/2025 04:31:34 INFO]: Training loss at epoch 34: 1.4102636575698853
[08/28/2025 04:31:34 INFO]: Training loss at epoch 27: 1.122677057981491
[08/28/2025 04:31:37 INFO]: Training stats: {
    "score": -1.0014517773588898,
    "rmse": 1.0014517773588898
}
[08/28/2025 04:31:37 INFO]: Val stats: {
    "score": -0.7049563122258039,
    "rmse": 0.7049563122258039
}
[08/28/2025 04:31:37 INFO]: Test stats: {
    "score": -0.8868538681035488,
    "rmse": 0.8868538681035488
}
[08/28/2025 04:31:42 INFO]: Training loss at epoch 23: 1.0785179138183594
[08/28/2025 04:32:04 INFO]: Training loss at epoch 18: 0.96875
[08/28/2025 04:32:15 INFO]: Training loss at epoch 46: 0.8833344578742981
[08/28/2025 04:32:15 INFO]: Training stats: {
    "score": -1.0025897839898443,
    "rmse": 1.0025897839898443
}
[08/28/2025 04:32:15 INFO]: Val stats: {
    "score": -0.7020208018849893,
    "rmse": 0.7020208018849893
}
[08/28/2025 04:32:15 INFO]: Test stats: {
    "score": -0.8854531592857157,
    "rmse": 0.8854531592857157
}
[08/28/2025 04:32:18 INFO]: Training loss at epoch 16: 1.048155963420868
[08/28/2025 04:32:29 INFO]: Running Final Evaluation...
[08/28/2025 04:32:34 INFO]: Training loss at epoch 113: 0.7174877822399139
[08/28/2025 04:32:34 INFO]: Training stats: {
    "score": -1.0390045870274802,
    "rmse": 1.0390045870274802
}
[08/28/2025 04:32:34 INFO]: Val stats: {
    "score": -0.6702312480870405,
    "rmse": 0.6702312480870405
}
[08/28/2025 04:32:34 INFO]: Test stats: {
    "score": -0.8874422459584733,
    "rmse": 0.8874422459584733
}
[08/28/2025 04:32:51 INFO]: Training loss at epoch 19: 1.0891234576702118
[08/28/2025 04:33:08 INFO]: Training accuracy: {
    "score": -1.0108387093346969,
    "rmse": 1.0108387093346969
}
[08/28/2025 04:33:08 INFO]: Val accuracy: {
    "score": -0.661427442372794,
    "rmse": 0.661427442372794
}
[08/28/2025 04:33:08 INFO]: Test accuracy: {
    "score": -0.867707387638215,
    "rmse": 0.867707387638215
}
[08/28/2025 04:33:09 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_95",
    "best_epoch": 15,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.867707387638215,
        "rmse": 0.867707387638215
    },
    "train_stats": {
        "score": -1.0108387093346969,
        "rmse": 1.0108387093346969
    },
    "val_stats": {
        "score": -0.661427442372794,
        "rmse": 0.661427442372794
    }
}
[08/28/2025 04:33:09 INFO]: Procewss finished for trial unhanged-Shanesha_trial_95
[08/28/2025 04:33:09 INFO]: 
_________________________________________________

[08/28/2025 04:33:09 INFO]: train_net_for_optune.py main() running.
[08/28/2025 04:33:09 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 1.8831686410154074
  attention_dropout: 0.14219106889619118
  ffn_dropout: 0.14219106889619118
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00015414342376375846
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_101

[08/28/2025 04:33:09 INFO]: This ft_transformer has 3.300 million parameters.
[08/28/2025 04:33:09 INFO]: Training will start at epoch 0.
[08/28/2025 04:33:09 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 04:33:25 INFO]: Training loss at epoch 31: 0.9825646281242371
[08/28/2025 04:33:26 INFO]: Training loss at epoch 28: 1.0666238069534302
[08/28/2025 04:33:26 INFO]: Training loss at epoch 24: 1.0097955465316772
[08/28/2025 04:33:28 INFO]: Training loss at epoch 69: 0.8771778345108032
[08/28/2025 04:33:50 INFO]: Training loss at epoch 19: 1.1210353374481201
[08/28/2025 04:33:55 INFO]: Training loss at epoch 47: 1.0378172993659973
[08/28/2025 04:34:08 INFO]: Training loss at epoch 12: 1.1506327986717224
[08/28/2025 04:34:26 INFO]: Training stats: {
    "score": -0.9973413554254043,
    "rmse": 0.9973413554254043
}
[08/28/2025 04:34:26 INFO]: Val stats: {
    "score": -0.6894376259364619,
    "rmse": 0.6894376259364619
}
[08/28/2025 04:34:26 INFO]: Test stats: {
    "score": -0.8806032917340595,
    "rmse": 0.8806032917340595
}
[08/28/2025 04:34:37 INFO]: Training stats: {
    "score": -1.017586335729755,
    "rmse": 1.017586335729755
}
[08/28/2025 04:34:37 INFO]: Val stats: {
    "score": -0.748990601142494,
    "rmse": 0.748990601142494
}
[08/28/2025 04:34:37 INFO]: Test stats: {
    "score": -0.9149352609119947,
    "rmse": 0.9149352609119947
}
[08/28/2025 04:34:40 INFO]: Training loss at epoch 0: 1.5497021079063416
[08/28/2025 04:34:53 INFO]: New best epoch, val score: -1.0394997246479012
[08/28/2025 04:34:53 INFO]: Saving model to: unhanged-Shanesha_trial_101/model_best.pth
[08/28/2025 04:35:12 INFO]: Training loss at epoch 25: 0.992624431848526
[08/28/2025 04:35:15 INFO]: Training loss at epoch 29: 1.0092400908470154
[08/28/2025 04:35:24 INFO]: Training stats: {
    "score": -1.0035141404331054,
    "rmse": 1.0035141404331054
}
[08/28/2025 04:35:24 INFO]: Val stats: {
    "score": -0.6650544198570769,
    "rmse": 0.6650544198570769
}
[08/28/2025 04:35:24 INFO]: Test stats: {
    "score": -0.8686121054721861,
    "rmse": 0.8686121054721861
}
[08/28/2025 04:35:25 INFO]: Training loss at epoch 47: 0.8150521814823151
[08/28/2025 04:35:27 INFO]: Training loss at epoch 21: 1.25612473487854
[08/28/2025 04:35:28 INFO]: Training loss at epoch 48: 0.9741404950618744
[08/28/2025 04:35:31 INFO]: Training loss at epoch 22: 1.0898619592189789
[08/28/2025 04:35:54 INFO]: Training stats: {
    "score": -0.9988958039535724,
    "rmse": 0.9988958039535724
}
[08/28/2025 04:35:54 INFO]: Val stats: {
    "score": -0.693882820179851,
    "rmse": 0.693882820179851
}
[08/28/2025 04:35:54 INFO]: Test stats: {
    "score": -0.8802333754688187,
    "rmse": 0.8802333754688187
}
[08/28/2025 04:36:03 INFO]: New best epoch, val score: -0.6612893322652331
[08/28/2025 04:36:03 INFO]: Saving model to: unhanged-Shanesha_trial_73/model_best.pth
[08/28/2025 04:36:14 INFO]: Training loss at epoch 20: 1.133292406797409
[08/28/2025 04:36:19 INFO]: Training loss at epoch 45: 1.0253165066242218
[08/28/2025 04:36:27 INFO]: Training loss at epoch 1: 1.3273322582244873
[08/28/2025 04:36:38 INFO]: Training loss at epoch 35: 0.9602789580821991
[08/28/2025 04:36:39 INFO]: Training loss at epoch 17: 0.7270937711000443
[08/28/2025 04:37:03 INFO]: Training loss at epoch 26: 1.098983883857727
[08/28/2025 04:37:41 INFO]: Training loss at epoch 20: 1.1043561100959778
[08/28/2025 04:37:55 INFO]: Training loss at epoch 30: 0.8929889500141144
[08/28/2025 04:37:55 INFO]: Training loss at epoch 32: 0.9619295597076416
[08/28/2025 04:38:11 INFO]: Training loss at epoch 21: 1.083154022693634
[08/28/2025 04:38:24 INFO]: Training loss at epoch 2: 1.7844098210334778
[08/28/2025 04:38:24 INFO]: Training loss at epoch 80: 1.0111092627048492
[08/28/2025 04:38:27 INFO]: Training loss at epoch 50: 0.7820162773132324
[08/28/2025 04:38:37 INFO]: New best epoch, val score: -0.722045740748304
[08/28/2025 04:38:37 INFO]: Saving model to: unhanged-Shanesha_trial_101/model_best.pth
[08/28/2025 04:39:00 INFO]: Training loss at epoch 27: 1.0661619901657104
[08/28/2025 04:39:13 INFO]: Training loss at epoch 48: 1.1339287757873535
[08/28/2025 04:39:26 INFO]: Training loss at epoch 13: 1.0045362710952759
[08/28/2025 04:39:47 INFO]: Training loss at epoch 114: 0.9124959707260132
[08/28/2025 04:39:52 INFO]: Running Final Evaluation...
[08/28/2025 04:39:55 INFO]: Training loss at epoch 31: 0.8303588628768921
[08/28/2025 04:40:07 INFO]: Training loss at epoch 22: 1.070930540561676
[08/28/2025 04:40:10 INFO]: Training loss at epoch 20: 0.9223659634590149
[08/28/2025 04:40:16 INFO]: Training loss at epoch 3: 1.0648746490478516
[08/28/2025 04:40:48 INFO]: Training loss at epoch 23: 1.0700830221176147
[08/28/2025 04:40:48 INFO]: Training loss at epoch 48: 1.1388992667198181
[08/28/2025 04:40:50 INFO]: Training loss at epoch 28: 0.9444429874420166
[08/28/2025 04:40:51 INFO]: Training loss at epoch 49: 0.9856878817081451
[08/28/2025 04:40:56 INFO]: Training loss at epoch 22: 0.8003151118755341
[08/28/2025 04:41:10 INFO]: Training loss at epoch 18: 1.008402943611145
[08/28/2025 04:41:23 INFO]: Running Final Evaluation...
[08/28/2025 04:41:36 INFO]: Training loss at epoch 70: 1.1992597579956055
[08/28/2025 04:41:38 INFO]: Training accuracy: {
    "score": -1.0157457696182526,
    "rmse": 1.0157457696182526
}
[08/28/2025 04:41:38 INFO]: Val accuracy: {
    "score": -0.6600299103508341,
    "rmse": 0.6600299103508341
}
[08/28/2025 04:41:38 INFO]: Test accuracy: {
    "score": -0.8726310680122942,
    "rmse": 0.8726310680122942
}
[08/28/2025 04:41:38 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_75",
    "best_epoch": 17,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8726310680122942,
        "rmse": 0.8726310680122942
    },
    "train_stats": {
        "score": -1.0157457696182526,
        "rmse": 1.0157457696182526
    },
    "val_stats": {
        "score": -0.6600299103508341,
        "rmse": 0.6600299103508341
    }
}
[08/28/2025 04:41:38 INFO]: Procewss finished for trial unhanged-Shanesha_trial_75
[08/28/2025 04:41:38 INFO]: 
_________________________________________________

[08/28/2025 04:41:38 INFO]: train_net_for_optune.py main() running.
[08/28/2025 04:41:38 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 2.0282066226045763
  attention_dropout: 0.13897710351644899
  ffn_dropout: 0.13897710351644899
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0003735676061178558
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_102

[08/28/2025 04:41:38 INFO]: This ft_transformer has 4.254 million parameters.
[08/28/2025 04:41:38 INFO]: Training will start at epoch 0.
[08/28/2025 04:41:38 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 04:41:41 INFO]: New best epoch, val score: -0.6612510683033581
[08/28/2025 04:41:41 INFO]: Saving model to: unhanged-Shanesha_trial_96/model_best.pth
[08/28/2025 04:41:45 INFO]: Training loss at epoch 32: 1.2522710859775543
[08/28/2025 04:41:53 INFO]: Training loss at epoch 23: 0.941227525472641
[08/28/2025 04:41:58 INFO]: Training loss at epoch 36: 0.8946840167045593
[08/28/2025 04:42:00 INFO]: Training loss at epoch 4: 1.1900913715362549
[08/28/2025 04:42:12 INFO]: Training loss at epoch 46: 0.9238798916339874
[08/28/2025 04:42:25 INFO]: Training loss at epoch 33: 0.9425271153450012
[08/28/2025 04:42:32 INFO]: Training stats: {
    "score": -1.0295747162843196,
    "rmse": 1.0295747162843196
}
[08/28/2025 04:42:32 INFO]: Val stats: {
    "score": -0.6674178139661836,
    "rmse": 0.6674178139661836
}
[08/28/2025 04:42:32 INFO]: Test stats: {
    "score": -0.8826094944189765,
    "rmse": 0.8826094944189765
}
[08/28/2025 04:42:35 INFO]: Training loss at epoch 29: 1.0136581063270569
[08/28/2025 04:42:50 INFO]: Training loss at epoch 21: 0.9124463200569153
[08/28/2025 04:43:07 INFO]: Training accuracy: {
    "score": -1.0170957720280682,
    "rmse": 1.0170957720280682
}
[08/28/2025 04:43:07 INFO]: Val accuracy: {
    "score": -0.6599308390317952,
    "rmse": 0.6599308390317952
}
[08/28/2025 04:43:07 INFO]: Test accuracy: {
    "score": -0.8737680655852076,
    "rmse": 0.8737680655852076
}
[08/28/2025 04:43:07 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_74",
    "best_epoch": 17,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8737680655852076,
        "rmse": 0.8737680655852076
    },
    "train_stats": {
        "score": -1.0170957720280682,
        "rmse": 1.0170957720280682
    },
    "val_stats": {
        "score": -0.6599308390317952,
        "rmse": 0.6599308390317952
    }
}
[08/28/2025 04:43:07 INFO]: Procewss finished for trial unhanged-Shanesha_trial_74
[08/28/2025 04:43:07 INFO]: 
_________________________________________________

[08/28/2025 04:43:07 INFO]: train_net_for_optune.py main() running.
[08/28/2025 04:43:07 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 1.8805005420670813
  attention_dropout: 0.13271066245583293
  ffn_dropout: 0.13271066245583293
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00016142674525884552
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_103

[08/28/2025 04:43:07 INFO]: This ft_transformer has 3.297 million parameters.
[08/28/2025 04:43:07 INFO]: Training will start at epoch 0.
[08/28/2025 04:43:07 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 04:43:10 INFO]: Training stats: {
    "score": -1.0008456685619207,
    "rmse": 1.0008456685619207
}
[08/28/2025 04:43:10 INFO]: Val stats: {
    "score": -0.6642575679569541,
    "rmse": 0.6642575679569541
}
[08/28/2025 04:43:10 INFO]: Test stats: {
    "score": -0.8681368894523268,
    "rmse": 0.8681368894523268
}
[08/28/2025 04:43:25 INFO]: New best epoch, val score: -0.6598149101234511
[08/28/2025 04:43:25 INFO]: Saving model to: unhanged-Shanesha_trial_94/model_best.pth
[08/28/2025 04:43:29 INFO]: Training loss at epoch 0: 0.9141177237033844
[08/28/2025 04:43:35 INFO]: Training loss at epoch 33: 0.9128338396549225
[08/28/2025 04:43:39 INFO]: Training loss at epoch 24: 1.026606023311615
[08/28/2025 04:43:43 INFO]: New best epoch, val score: -0.6607570597286027
[08/28/2025 04:43:43 INFO]: Saving model to: unhanged-Shanesha_trial_102/model_best.pth
[08/28/2025 04:43:45 INFO]: Training loss at epoch 5: 1.1803840398788452
[08/28/2025 04:44:26 INFO]: Training loss at epoch 14: 0.9164488613605499
[08/28/2025 04:44:29 INFO]: Training loss at epoch 51: 1.1925735771656036
[08/28/2025 04:44:43 INFO]: Training loss at epoch 0: 1.6972249746322632
[08/28/2025 04:44:57 INFO]: New best epoch, val score: -0.7478761545581201
[08/28/2025 04:44:57 INFO]: Saving model to: unhanged-Shanesha_trial_103/model_best.pth
[08/28/2025 04:44:59 INFO]: Training loss at epoch 30: 0.9420016705989838
[08/28/2025 04:45:06 INFO]: Training loss at epoch 81: 0.9992046356201172
[08/28/2025 04:45:25 INFO]: Training loss at epoch 21: 1.308402270078659
[08/28/2025 04:45:27 INFO]: Training loss at epoch 19: 0.9081651866436005
[08/28/2025 04:45:29 INFO]: Training loss at epoch 25: 0.9168625175952911
[08/28/2025 04:45:29 INFO]: Training loss at epoch 34: 1.0219595432281494
[08/28/2025 04:45:32 INFO]: Training loss at epoch 6: 1.1037081480026245
[08/28/2025 04:45:40 INFO]: Training loss at epoch 1: 4.863449990749359
[08/28/2025 04:45:48 INFO]: Training loss at epoch 24: 0.9895519912242889
[08/28/2025 04:46:10 INFO]: Training loss at epoch 23: 1.2109441459178925
[08/28/2025 04:46:27 INFO]: New best epoch, val score: -0.6606935188621728
[08/28/2025 04:46:27 INFO]: Saving model to: unhanged-Shanesha_trial_90/model_best.pth
[08/28/2025 04:46:37 INFO]: Training loss at epoch 1: 1.3738177418708801
[08/28/2025 04:46:44 INFO]: Training loss at epoch 115: 0.9264081120491028
[08/28/2025 04:46:55 INFO]: Training loss at epoch 31: 0.8410657644271851
[08/28/2025 04:46:59 INFO]: Training loss at epoch 34: 1.0775331556797028
[08/28/2025 04:47:05 INFO]: Training stats: {
    "score": -1.0089618739542776,
    "rmse": 1.0089618739542776
}
[08/28/2025 04:47:05 INFO]: Val stats: {
    "score": -0.6609537126282823,
    "rmse": 0.6609537126282823
}
[08/28/2025 04:47:05 INFO]: Test stats: {
    "score": -0.8699694828287227,
    "rmse": 0.8699694828287227
}
[08/28/2025 04:47:14 INFO]: Training loss at epoch 37: 0.937540739774704
[08/28/2025 04:47:27 INFO]: Training loss at epoch 26: 1.088865339756012
[08/28/2025 04:47:29 INFO]: Training loss at epoch 7: 0.9924916326999664
[08/28/2025 04:47:31 INFO]: Training loss at epoch 35: 1.0645408630371094
[08/28/2025 04:47:39 INFO]: New best epoch, val score: -0.6609537126282823
[08/28/2025 04:47:39 INFO]: Saving model to: unhanged-Shanesha_trial_96/model_best.pth
[08/28/2025 04:47:42 INFO]: Training loss at epoch 71: 0.8220815360546112
[08/28/2025 04:47:50 INFO]: Training loss at epoch 50: 0.9294300675392151
[08/28/2025 04:47:59 INFO]: Training loss at epoch 2: 1.8534462451934814
[08/28/2025 04:48:04 INFO]: Training loss at epoch 47: 1.0633241534233093
[08/28/2025 04:48:10 INFO]: Training loss at epoch 22: 1.0932419896125793
[08/28/2025 04:48:35 INFO]: Training loss at epoch 2: 2.2157217264175415
[08/28/2025 04:48:49 INFO]: New best epoch, val score: -0.7162825043320765
[08/28/2025 04:48:49 INFO]: Saving model to: unhanged-Shanesha_trial_103/model_best.pth
[08/28/2025 04:48:51 INFO]: Training loss at epoch 32: 0.8636529445648193
[08/28/2025 04:49:21 INFO]: Training loss at epoch 8: 1.104448825120926
[08/28/2025 04:49:22 INFO]: Training loss at epoch 27: 1.0177541971206665
[08/28/2025 04:49:29 INFO]: Training loss at epoch 36: 0.9117792546749115
[08/28/2025 04:49:47 INFO]: Training loss at epoch 15: 0.9717369377613068
[08/28/2025 04:50:09 INFO]: Training loss at epoch 3: 1.699357271194458
[08/28/2025 04:50:22 INFO]: Training loss at epoch 3: 1.0789398550987244
[08/28/2025 04:50:36 INFO]: Training loss at epoch 33: 0.8616818487644196
[08/28/2025 04:50:47 INFO]: Training loss at epoch 52: 1.1412087380886078
[08/28/2025 04:50:59 INFO]: Training loss at epoch 22: 1.2408187985420227
[08/28/2025 04:51:02 INFO]: Training loss at epoch 25: 1.1709551811218262
[08/28/2025 04:51:05 INFO]: Training loss at epoch 9: 1.1937807202339172
[08/28/2025 04:51:07 INFO]: Training loss at epoch 28: 0.9944904744625092
[08/28/2025 04:51:18 INFO]: Training loss at epoch 37: 0.9565219283103943
[08/28/2025 04:51:28 INFO]: New best epoch, val score: -0.6604457563560482
[08/28/2025 04:51:28 INFO]: Saving model to: unhanged-Shanesha_trial_64/model_best.pth
[08/28/2025 04:51:29 INFO]: Training loss at epoch 20: 1.0590282082557678
[08/28/2025 04:51:30 INFO]: Training loss at epoch 35: 0.9257557392120361
[08/28/2025 04:51:33 INFO]: Training loss at epoch 24: 0.8247872292995453
[08/28/2025 04:51:41 INFO]: Training stats: {
    "score": -1.0780098960874893,
    "rmse": 1.0780098960874893
}
[08/28/2025 04:51:41 INFO]: Val stats: {
    "score": -0.871954232667721,
    "rmse": 0.871954232667721
}
[08/28/2025 04:51:41 INFO]: Test stats: {
    "score": -1.0034053210221165,
    "rmse": 1.0034053210221165
}
[08/28/2025 04:51:58 INFO]: Training loss at epoch 82: 0.9822141528129578
[08/28/2025 04:52:07 INFO]: Training loss at epoch 4: 1.3289235830307007
[08/28/2025 04:52:14 INFO]: Training loss at epoch 4: 1.2951707243919373
[08/28/2025 04:52:20 INFO]: New best epoch, val score: -0.6792947401103266
[08/28/2025 04:52:20 INFO]: Saving model to: unhanged-Shanesha_trial_103/model_best.pth
[08/28/2025 04:52:21 INFO]: Training loss at epoch 34: 1.0263795256614685
[08/28/2025 04:52:24 INFO]: Training loss at epoch 38: 1.2220051288604736
[08/28/2025 04:52:56 INFO]: Training loss at epoch 29: 1.0877732634544373
[08/28/2025 04:52:56 INFO]: Training loss at epoch 51: 1.0123025178909302
[08/28/2025 04:53:04 INFO]: New best epoch, val score: -0.6620865538169931
[08/28/2025 04:53:04 INFO]: Saving model to: unhanged-Shanesha_trial_81/model_best.pth
[08/28/2025 04:53:12 INFO]: Training loss at epoch 38: 0.9667910635471344
[08/28/2025 04:53:14 INFO]: Training loss at epoch 23: 1.0122466087341309
[08/28/2025 04:53:32 INFO]: Training loss at epoch 10: 1.2880226373672485
[08/28/2025 04:53:37 INFO]: Training stats: {
    "score": -0.9961800900903909,
    "rmse": 0.9961800900903909
}
[08/28/2025 04:53:37 INFO]: Val stats: {
    "score": -0.6688288828275565,
    "rmse": 0.6688288828275565
}
[08/28/2025 04:53:37 INFO]: Test stats: {
    "score": -0.8713627692767103,
    "rmse": 0.8713627692767103
}
[08/28/2025 04:53:45 INFO]: Training loss at epoch 72: 0.8743046522140503
[08/28/2025 04:53:45 INFO]: New best epoch, val score: -0.705774551913632
[08/28/2025 04:53:45 INFO]: Saving model to: unhanged-Shanesha_trial_101/model_best.pth
[08/28/2025 04:53:46 INFO]: Training loss at epoch 48: 0.9323212802410126
[08/28/2025 04:53:53 INFO]: Training loss at epoch 116: 0.9480597972869873
[08/28/2025 04:54:02 INFO]: Training loss at epoch 5: 1.0850523710250854
[08/28/2025 04:54:16 INFO]: New best epoch, val score: -0.67588005075467
[08/28/2025 04:54:16 INFO]: Saving model to: unhanged-Shanesha_trial_103/model_best.pth
[08/28/2025 04:54:16 INFO]: Training loss at epoch 35: 1.0423037111759186
[08/28/2025 04:54:33 INFO]: Training loss at epoch 5: 0.8566373884677887
[08/28/2025 04:54:48 INFO]: Running Final Evaluation...
[08/28/2025 04:55:00 INFO]: Training loss at epoch 16: 0.9003768861293793
[08/28/2025 04:55:15 INFO]: Training loss at epoch 39: 0.9276944100856781
[08/28/2025 04:55:27 INFO]: Training loss at epoch 11: 0.9923833906650543
[08/28/2025 04:55:33 INFO]: Training loss at epoch 30: 1.0802385210990906
[08/28/2025 04:55:41 INFO]: New best epoch, val score: -0.6608423429754562
[08/28/2025 04:55:41 INFO]: Saving model to: unhanged-Shanesha_trial_101/model_best.pth
[08/28/2025 04:55:59 INFO]: Training loss at epoch 6: 1.1659852862358093
[08/28/2025 04:55:59 INFO]: Training stats: {
    "score": -0.9979995004896388,
    "rmse": 0.9979995004896388
}
[08/28/2025 04:55:59 INFO]: Val stats: {
    "score": -0.6727989399388932,
    "rmse": 0.6727989399388932
}
[08/28/2025 04:55:59 INFO]: Test stats: {
    "score": -0.8706371837813379,
    "rmse": 0.8706371837813379
}
[08/28/2025 04:56:03 INFO]: Training loss at epoch 21: 1.1696196496486664
[08/28/2025 04:56:11 INFO]: Training loss at epoch 36: 1.0641416609287262
[08/28/2025 04:56:13 INFO]: Training loss at epoch 36: 0.7759720087051392
[08/28/2025 04:56:13 INFO]: New best epoch, val score: -0.6719309329859906
[08/28/2025 04:56:13 INFO]: Saving model to: unhanged-Shanesha_trial_103/model_best.pth
[08/28/2025 04:56:20 INFO]: Training loss at epoch 26: 1.0256948471069336
[08/28/2025 04:56:34 INFO]: Training loss at epoch 23: 1.0538966059684753
[08/28/2025 04:56:48 INFO]: Training loss at epoch 6: 0.8445063233375549
[08/28/2025 04:57:05 INFO]: Training loss at epoch 25: 0.8617251515388489
[08/28/2025 04:57:06 INFO]: Training loss at epoch 53: 1.005400389432907
[08/28/2025 04:57:20 INFO]: Training loss at epoch 12: 0.9747438430786133
[08/28/2025 04:57:27 INFO]: Training loss at epoch 31: 0.8269844353199005
[08/28/2025 04:57:37 INFO]: Training accuracy: {
    "score": -0.9814217380474461,
    "rmse": 0.9814217380474461
}
[08/28/2025 04:57:37 INFO]: Val accuracy: {
    "score": -0.65542711551482,
    "rmse": 0.65542711551482
}
[08/28/2025 04:57:37 INFO]: Test accuracy: {
    "score": -0.8813826753451235,
    "rmse": 0.8813826753451235
}
[08/28/2025 04:57:38 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_20",
    "best_epoch": 85,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8813826753451235,
        "rmse": 0.8813826753451235
    },
    "train_stats": {
        "score": -0.9814217380474461,
        "rmse": 0.9814217380474461
    },
    "val_stats": {
        "score": -0.65542711551482,
        "rmse": 0.65542711551482
    }
}
[08/28/2025 04:57:38 INFO]: Procewss finished for trial unhanged-Shanesha_trial_20
[08/28/2025 04:57:38 INFO]: 
_________________________________________________

[08/28/2025 04:57:38 INFO]: train_net_for_optune.py main() running.
[08/28/2025 04:57:38 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 2.4270339430464265
  attention_dropout: 0.2727574713203751
  ffn_dropout: 0.2727574713203751
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.3349929774968245e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_104

[08/28/2025 04:57:38 INFO]: This ft_transformer has 3.746 million parameters.
[08/28/2025 04:57:38 INFO]: Training will start at epoch 0.
[08/28/2025 04:57:38 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 04:57:49 INFO]: Training loss at epoch 7: 1.011178344488144
[08/28/2025 04:57:52 INFO]: Training loss at epoch 39: 1.0192126631736755
[08/28/2025 04:57:54 INFO]: Training loss at epoch 40: 1.0203873217105865
[08/28/2025 04:58:01 INFO]: Training loss at epoch 37: 1.0886761844158173
[08/28/2025 04:58:02 INFO]: New best epoch, val score: -0.6680673123096975
[08/28/2025 04:58:02 INFO]: Saving model to: unhanged-Shanesha_trial_103/model_best.pth
[08/28/2025 04:58:20 INFO]: Training loss at epoch 52: 0.9600452184677124
[08/28/2025 04:58:31 INFO]: Training loss at epoch 24: 0.8243173658847809
[08/28/2025 04:58:56 INFO]: Training loss at epoch 7: 1.1602372527122498
[08/28/2025 04:58:57 INFO]: Training loss at epoch 83: 0.8282384276390076
[08/28/2025 04:59:04 INFO]: Training loss at epoch 13: 1.4597569704055786
[08/28/2025 04:59:13 INFO]: Training loss at epoch 32: 0.9546881318092346
[08/28/2025 04:59:19 INFO]: Training loss at epoch 0: 0.9867165386676788
[08/28/2025 04:59:32 INFO]: New best epoch, val score: -0.7112792754679438
[08/28/2025 04:59:32 INFO]: Saving model to: unhanged-Shanesha_trial_104/model_best.pth
[08/28/2025 04:59:35 INFO]: Training loss at epoch 8: 0.9929877519607544
[08/28/2025 04:59:36 INFO]: Training loss at epoch 49: 0.9123636484146118
[08/28/2025 04:59:37 INFO]: Training stats: {
    "score": -1.0087675436523436,
    "rmse": 1.0087675436523436
}
[08/28/2025 04:59:37 INFO]: Val stats: {
    "score": -0.6617277912075766,
    "rmse": 0.6617277912075766
}
[08/28/2025 04:59:37 INFO]: Test stats: {
    "score": -0.870266979144676,
    "rmse": 0.870266979144676
}
[08/28/2025 04:59:44 INFO]: Training loss at epoch 41: 0.999258816242218
[08/28/2025 04:59:46 INFO]: Training loss at epoch 38: 0.895829826593399
[08/28/2025 04:59:50 INFO]: New best epoch, val score: -0.6647244902856351
[08/28/2025 04:59:50 INFO]: Saving model to: unhanged-Shanesha_trial_103/model_best.pth
[08/28/2025 04:59:52 INFO]: Training loss at epoch 73: 0.9405112862586975
[08/28/2025 05:00:08 INFO]: Training loss at epoch 17: 0.7743097692728043
[08/28/2025 05:00:14 INFO]: New best epoch, val score: -0.6617277912075766
[08/28/2025 05:00:14 INFO]: Saving model to: unhanged-Shanesha_trial_81/model_best.pth
[08/28/2025 05:00:23 INFO]: Training loss at epoch 22: 0.8183949589729309
[08/28/2025 05:00:34 INFO]: Training loss at epoch 37: 0.9546783268451691
[08/28/2025 05:00:48 INFO]: Training loss at epoch 14: 1.0024055540561676
[08/28/2025 05:00:59 INFO]: Training loss at epoch 33: 0.8417896628379822
[08/28/2025 05:01:01 INFO]: Training loss at epoch 8: 0.885710597038269
[08/28/2025 05:01:14 INFO]: Training loss at epoch 1: 1.333196997642517
[08/28/2025 05:01:18 INFO]: Training loss at epoch 27: 1.3243057429790497
[08/28/2025 05:01:27 INFO]: Training loss at epoch 9: 1.2152040004730225
[08/28/2025 05:01:30 INFO]: New best epoch, val score: -0.6769243910491868
[08/28/2025 05:01:30 INFO]: Saving model to: unhanged-Shanesha_trial_104/model_best.pth
[08/28/2025 05:01:31 INFO]: Training stats: {
    "score": -1.0086940409043283,
    "rmse": 1.0086940409043283
}
[08/28/2025 05:01:31 INFO]: Val stats: {
    "score": -0.7245151487647388,
    "rmse": 0.7245151487647388
}
[08/28/2025 05:01:31 INFO]: Test stats: {
    "score": -0.8989160783761146,
    "rmse": 0.8989160783761146
}
[08/28/2025 05:01:34 INFO]: Training loss at epoch 39: 0.7977285981178284
[08/28/2025 05:01:37 INFO]: Training loss at epoch 42: 0.8514279127120972
[08/28/2025 05:01:54 INFO]: Training loss at epoch 24: 1.0262130200862885
[08/28/2025 05:01:58 INFO]: New best epoch, val score: -0.660646906816872
[08/28/2025 05:01:58 INFO]: Saving model to: unhanged-Shanesha_trial_90/model_best.pth
[08/28/2025 05:02:11 INFO]: Training stats: {
    "score": -1.0222447578265332,
    "rmse": 1.0222447578265332
}
[08/28/2025 05:02:11 INFO]: Val stats: {
    "score": -0.6621835774169075,
    "rmse": 0.6621835774169075
}
[08/28/2025 05:02:11 INFO]: Test stats: {
    "score": -0.8765834863918449,
    "rmse": 0.8765834863918449
}
[08/28/2025 05:02:17 INFO]: Training stats: {
    "score": -0.996874404531812,
    "rmse": 0.996874404531812
}
[08/28/2025 05:02:17 INFO]: Val stats: {
    "score": -0.6687118644493112,
    "rmse": 0.6687118644493112
}
[08/28/2025 05:02:17 INFO]: Test stats: {
    "score": -0.8687348593302987,
    "rmse": 0.8687348593302987
}
[08/28/2025 05:02:23 INFO]: Training loss at epoch 26: 1.2466118931770325
[08/28/2025 05:02:26 INFO]: New best epoch, val score: -0.6621835774169075
[08/28/2025 05:02:26 INFO]: Saving model to: unhanged-Shanesha_trial_103/model_best.pth
[08/28/2025 05:02:45 INFO]: Training loss at epoch 15: 0.977795273065567
[08/28/2025 05:02:58 INFO]: Training loss at epoch 34: 1.01992928981781
[08/28/2025 05:03:12 INFO]: Training loss at epoch 54: 1.0067791640758514
[08/28/2025 05:03:22 INFO]: Training loss at epoch 9: 0.9667883217334747
[08/28/2025 05:03:26 INFO]: Training loss at epoch 2: 1.2548599243164062
[08/28/2025 05:03:33 INFO]: Training loss at epoch 53: 0.8405289053916931
[08/28/2025 05:03:43 INFO]: Training loss at epoch 43: 1.1136273741722107
[08/28/2025 05:03:46 INFO]: Training loss at epoch 25: 1.0319342613220215
[08/28/2025 05:04:10 INFO]: Training stats: {
    "score": -1.010298288353754,
    "rmse": 1.010298288353754
}
[08/28/2025 05:04:10 INFO]: Val stats: {
    "score": -0.7293457796743464,
    "rmse": 0.7293457796743464
}
[08/28/2025 05:04:10 INFO]: Test stats: {
    "score": -0.9003572594630089,
    "rmse": 0.9003572594630089
}
[08/28/2025 05:04:11 INFO]: Training loss at epoch 10: 1.0921288132667542
[08/28/2025 05:04:15 INFO]: Training loss at epoch 40: 0.9095978140830994
[08/28/2025 05:04:40 INFO]: Training loss at epoch 16: 1.0968086123466492
[08/28/2025 05:04:56 INFO]: Training loss at epoch 35: 1.0165676474571228
[08/28/2025 05:05:02 INFO]: Training loss at epoch 23: 1.0245569944381714
[08/28/2025 05:05:04 INFO]: Training loss at epoch 40: 1.0927842259407043
[08/28/2025 05:05:19 INFO]: Training loss at epoch 38: 1.010762631893158
[08/28/2025 05:05:31 INFO]: Training loss at epoch 3: 0.9835571050643921
[08/28/2025 05:05:32 INFO]: Training loss at epoch 18: 0.9866662323474884
[08/28/2025 05:05:40 INFO]: Training loss at epoch 44: 0.9440926015377045
[08/28/2025 05:05:53 INFO]: Training loss at epoch 84: 0.9759484529495239
[08/28/2025 05:06:02 INFO]: Training loss at epoch 11: 1.0223122835159302
[08/28/2025 05:06:05 INFO]: Training loss at epoch 41: 0.9384716749191284
[08/28/2025 05:06:09 INFO]: Training loss at epoch 74: 0.9586283266544342
[08/28/2025 05:06:20 INFO]: Training loss at epoch 10: 0.8162796795368195
[08/28/2025 05:06:27 INFO]: Training loss at epoch 17: 1.1305644512176514
[08/28/2025 05:06:40 INFO]: New best epoch, val score: -0.6608334330843146
[08/28/2025 05:06:40 INFO]: Saving model to: unhanged-Shanesha_trial_43/model_best.pth
[08/28/2025 05:06:40 INFO]: Training loss at epoch 28: 0.9798080027103424
[08/28/2025 05:06:43 INFO]: Training loss at epoch 36: 0.954072117805481
[08/28/2025 05:07:25 INFO]: Training loss at epoch 4: 1.0555769801139832
[08/28/2025 05:07:26 INFO]: Training loss at epoch 50: 1.0589761137962341
[08/28/2025 05:07:29 INFO]: Training loss at epoch 25: 1.2269791960716248
[08/28/2025 05:07:30 INFO]: Training loss at epoch 45: 0.8345319032669067
[08/28/2025 05:07:39 INFO]: New best epoch, val score: -0.6697743000225203
[08/28/2025 05:07:39 INFO]: Saving model to: unhanged-Shanesha_trial_104/model_best.pth
[08/28/2025 05:07:46 INFO]: Training loss at epoch 27: 1.2021810114383698
[08/28/2025 05:07:47 INFO]: Training loss at epoch 12: 1.0741655826568604
[08/28/2025 05:07:49 INFO]: Training loss at epoch 42: 1.0230775475502014
[08/28/2025 05:08:11 INFO]: Training loss at epoch 18: 1.2143824994564056
[08/28/2025 05:08:25 INFO]: Training loss at epoch 11: 1.2569047510623932
[08/28/2025 05:08:28 INFO]: Training loss at epoch 37: 1.1463246047496796
[08/28/2025 05:08:40 INFO]: Training loss at epoch 54: 0.9437116980552673
[08/28/2025 05:08:49 INFO]: Training loss at epoch 26: 1.0717677474021912
[08/28/2025 05:09:16 INFO]: Training loss at epoch 55: 1.1091134548187256
[08/28/2025 05:09:16 INFO]: Training loss at epoch 24: 0.9004456400871277
[08/28/2025 05:09:21 INFO]: Training loss at epoch 46: 0.8764523863792419
[08/28/2025 05:09:22 INFO]: Training loss at epoch 5: 0.9922960698604584
[08/28/2025 05:09:35 INFO]: Training loss at epoch 13: 1.2225301861763
[08/28/2025 05:09:36 INFO]: New best epoch, val score: -0.6688065494612098
[08/28/2025 05:09:36 INFO]: Saving model to: unhanged-Shanesha_trial_104/model_best.pth
[08/28/2025 05:09:37 INFO]: Training loss at epoch 43: 0.8952732384204865
[08/28/2025 05:09:40 INFO]: Training loss at epoch 39: 1.074896663427353
[08/28/2025 05:10:01 INFO]: Training loss at epoch 19: 1.1149119138717651
[08/28/2025 05:10:11 INFO]: Training loss at epoch 41: 1.0404056310653687
[08/28/2025 05:10:21 INFO]: Training loss at epoch 38: 1.2483018338680267
[08/28/2025 05:10:39 INFO]: Training loss at epoch 12: 1.1364811658859253
[08/28/2025 05:10:40 INFO]: Training loss at epoch 19: 0.8330435156822205
[08/28/2025 05:10:41 INFO]: Training stats: {
    "score": -0.9994658254128062,
    "rmse": 0.9994658254128062
}
[08/28/2025 05:10:41 INFO]: Val stats: {
    "score": -0.6910562082538195,
    "rmse": 0.6910562082538195
}
[08/28/2025 05:10:41 INFO]: Test stats: {
    "score": -0.879282755309097,
    "rmse": 0.879282755309097
}
[08/28/2025 05:11:17 INFO]: Training stats: {
    "score": -0.9990610288037624,
    "rmse": 0.9990610288037624
}
[08/28/2025 05:11:17 INFO]: Val stats: {
    "score": -0.6899684895652578,
    "rmse": 0.6899684895652578
}
[08/28/2025 05:11:17 INFO]: Test stats: {
    "score": -0.8798512664181442,
    "rmse": 0.8798512664181442
}
[08/28/2025 05:11:22 INFO]: Training loss at epoch 47: 0.9587275981903076
[08/28/2025 05:11:31 INFO]: Training loss at epoch 6: 1.1697535514831543
[08/28/2025 05:11:32 INFO]: Training loss at epoch 14: 1.1246100664138794
[08/28/2025 05:11:35 INFO]: Training loss at epoch 44: 1.1894137263298035
[08/28/2025 05:11:47 INFO]: New best epoch, val score: -0.6684883685482573
[08/28/2025 05:11:47 INFO]: Saving model to: unhanged-Shanesha_trial_104/model_best.pth
[08/28/2025 05:11:51 INFO]: Training loss at epoch 29: 1.0705796480178833
[08/28/2025 05:12:16 INFO]: Training loss at epoch 75: 1.2987833619117737
[08/28/2025 05:12:18 INFO]: Training loss at epoch 39: 1.2293663024902344
[08/28/2025 05:12:33 INFO]: Training stats: {
    "score": -0.9981199739570831,
    "rmse": 0.9981199739570831
}
[08/28/2025 05:12:33 INFO]: Val stats: {
    "score": -0.6779141845264324,
    "rmse": 0.6779141845264324
}
[08/28/2025 05:12:33 INFO]: Test stats: {
    "score": -0.873837074698698,
    "rmse": 0.873837074698698
}
[08/28/2025 05:12:36 INFO]: Training loss at epoch 20: 1.178935557603836
[08/28/2025 05:12:46 INFO]: Training loss at epoch 85: 0.8791674971580505
[08/28/2025 05:12:58 INFO]: Training loss at epoch 13: 1.102973461151123
[08/28/2025 05:12:59 INFO]: Training stats: {
    "score": -0.9926801446922503,
    "rmse": 0.9926801446922503
}
[08/28/2025 05:12:59 INFO]: Val stats: {
    "score": -0.6724960825666617,
    "rmse": 0.6724960825666617
}
[08/28/2025 05:12:59 INFO]: Test stats: {
    "score": -0.8736943054431983,
    "rmse": 0.8736943054431983
}
[08/28/2025 05:13:03 INFO]: Training loss at epoch 26: 1.0124256014823914
[08/28/2025 05:13:15 INFO]: Training loss at epoch 28: 0.9428744316101074
[08/28/2025 05:13:21 INFO]: Training loss at epoch 48: 1.0437334775924683
[08/28/2025 05:13:21 INFO]: Training loss at epoch 51: 1.0921850800514221
[08/28/2025 05:13:27 INFO]: Training loss at epoch 15: 1.046860933303833
[08/28/2025 05:13:29 INFO]: Training loss at epoch 45: 1.002581000328064
[08/28/2025 05:13:36 INFO]: Training loss at epoch 7: 1.1105899214744568
[08/28/2025 05:13:39 INFO]: Training stats: {
    "score": -1.000550352805325,
    "rmse": 1.000550352805325
}
[08/28/2025 05:13:39 INFO]: Val stats: {
    "score": -0.6728940756774486,
    "rmse": 0.6728940756774486
}
[08/28/2025 05:13:39 INFO]: Test stats: {
    "score": -0.870800932065714,
    "rmse": 0.870800932065714
}
[08/28/2025 05:13:40 INFO]: New best epoch, val score: -0.6604834897907779
[08/28/2025 05:13:40 INFO]: Saving model to: unhanged-Shanesha_trial_93/model_best.pth
[08/28/2025 05:13:52 INFO]: Training loss at epoch 25: 1.1480997204780579
[08/28/2025 05:14:03 INFO]: Training loss at epoch 55: 1.2606987953186035
[08/28/2025 05:14:06 INFO]: Training loss at epoch 27: 0.7943998575210571
[08/28/2025 05:14:23 INFO]: Training loss at epoch 21: 1.2196029424667358
[08/28/2025 05:14:45 INFO]: Training loss at epoch 40: 0.9872647225856781
[08/28/2025 05:15:04 INFO]: Training loss at epoch 14: 1.0827053487300873
[08/28/2025 05:15:10 INFO]: Training loss at epoch 49: 1.0481458008289337
[08/28/2025 05:15:13 INFO]: Training loss at epoch 16: 1.2925991415977478
[08/28/2025 05:15:15 INFO]: Training loss at epoch 46: 1.1201861500740051
[08/28/2025 05:15:29 INFO]: Training loss at epoch 42: 1.0929332971572876
[08/28/2025 05:15:31 INFO]: Training loss at epoch 56: 1.1545278131961823
[08/28/2025 05:15:31 INFO]: Training loss at epoch 8: 1.1195583939552307
[08/28/2025 05:15:47 INFO]: Training loss at epoch 40: 0.8615202605724335
[08/28/2025 05:15:49 INFO]: Training stats: {
    "score": -0.9965181057361384,
    "rmse": 0.9965181057361384
}
[08/28/2025 05:15:49 INFO]: Val stats: {
    "score": -0.6901565949921286,
    "rmse": 0.6901565949921286
}
[08/28/2025 05:15:49 INFO]: Test stats: {
    "score": -0.879087472880489,
    "rmse": 0.879087472880489
}
[08/28/2025 05:16:08 INFO]: Training loss at epoch 22: 1.0965846478939056
[08/28/2025 05:16:31 INFO]: Training loss at epoch 41: 0.9967295825481415
[08/28/2025 05:16:58 INFO]: Training loss at epoch 17: 1.0589023530483246
[08/28/2025 05:17:00 INFO]: Training loss at epoch 47: 0.9830958247184753
[08/28/2025 05:17:09 INFO]: Training loss at epoch 15: 0.9500491321086884
[08/28/2025 05:17:26 INFO]: Training loss at epoch 9: 0.8873350322246552
[08/28/2025 05:17:34 INFO]: Training loss at epoch 20: 1.1678566932678223
[08/28/2025 05:17:40 INFO]: Training loss at epoch 50: 1.082990288734436
[08/28/2025 05:17:54 INFO]: Training loss at epoch 23: 1.0454773008823395
[08/28/2025 05:18:08 INFO]: Training loss at epoch 26: 1.1346220970153809
[08/28/2025 05:18:09 INFO]: Training stats: {
    "score": -1.0047157009073258,
    "rmse": 1.0047157009073258
}
[08/28/2025 05:18:09 INFO]: Val stats: {
    "score": -0.6704337688810212,
    "rmse": 0.6704337688810212
}
[08/28/2025 05:18:09 INFO]: Test stats: {
    "score": -0.8708631298376335,
    "rmse": 0.8708631298376335
}
[08/28/2025 05:18:14 INFO]: Training loss at epoch 76: 1.0535362362861633
[08/28/2025 05:18:19 INFO]: Training loss at epoch 27: 0.8622694611549377
[08/28/2025 05:18:22 INFO]: Training loss at epoch 42: 0.8851932883262634
[08/28/2025 05:18:27 INFO]: Training loss at epoch 29: 1.056606948375702
[08/28/2025 05:18:43 INFO]: Training loss at epoch 30: 0.8561083078384399
[08/28/2025 05:18:51 INFO]: Training loss at epoch 18: 1.1507706046104431
[08/28/2025 05:18:52 INFO]: Training loss at epoch 48: 1.0990877449512482
[08/28/2025 05:19:01 INFO]: Training loss at epoch 52: 1.1459927558898926
[08/28/2025 05:19:06 INFO]: New best epoch, val score: -0.6618300909214632
[08/28/2025 05:19:06 INFO]: Saving model to: unhanged-Shanesha_trial_103/model_best.pth
[08/28/2025 05:19:11 INFO]: Training loss at epoch 28: 0.9349315464496613
[08/28/2025 05:19:12 INFO]: Training loss at epoch 56: 1.2373011708259583
[08/28/2025 05:19:26 INFO]: Training loss at epoch 16: 1.0591524839401245
[08/28/2025 05:19:37 INFO]: Training loss at epoch 86: 1.1497520208358765
[08/28/2025 05:19:41 INFO]: Training loss at epoch 51: 0.7621197253465652
[08/28/2025 05:19:50 INFO]: Training loss at epoch 24: 0.9982117414474487
[08/28/2025 05:20:17 INFO]: Training loss at epoch 10: 1.1436278820037842
[08/28/2025 05:20:20 INFO]: Training loss at epoch 43: 0.9195721447467804
[08/28/2025 05:20:24 INFO]: Training loss at epoch 41: 1.1071785688400269
[08/28/2025 05:20:24 INFO]: Training stats: {
    "score": -1.0067982407629308,
    "rmse": 1.0067982407629308
}
[08/28/2025 05:20:24 INFO]: Val stats: {
    "score": -0.66192954061317,
    "rmse": 0.66192954061317
}
[08/28/2025 05:20:24 INFO]: Test stats: {
    "score": -0.869430041278811,
    "rmse": 0.869430041278811
}
[08/28/2025 05:20:50 INFO]: Training loss at epoch 19: 0.9728080332279205
[08/28/2025 05:20:50 INFO]: Training loss at epoch 49: 1.0796447396278381
[08/28/2025 05:20:52 INFO]: Training loss at epoch 43: 1.0252541601657867
[08/28/2025 05:21:29 INFO]: Training stats: {
    "score": -0.9940987153936888,
    "rmse": 0.9940987153936888
}
[08/28/2025 05:21:29 INFO]: Val stats: {
    "score": -0.6720836783125451,
    "rmse": 0.6720836783125451
}
[08/28/2025 05:21:29 INFO]: Test stats: {
    "score": -0.8699868580462631,
    "rmse": 0.8699868580462631
}
[08/28/2025 05:21:29 INFO]: Training stats: {
    "score": -1.0104303352908264,
    "rmse": 1.0104303352908264
}
[08/28/2025 05:21:29 INFO]: Val stats: {
    "score": -0.6606925437718847,
    "rmse": 0.6606925437718847
}
[08/28/2025 05:21:29 INFO]: Test stats: {
    "score": -0.8696651328502769,
    "rmse": 0.8696651328502769
}
[08/28/2025 05:21:42 INFO]: Training loss at epoch 52: 0.962471216917038
[08/28/2025 05:21:44 INFO]: New best epoch, val score: -0.6606925437718847
[08/28/2025 05:21:44 INFO]: Saving model to: unhanged-Shanesha_trial_103/model_best.pth
[08/28/2025 05:21:45 INFO]: Training loss at epoch 17: 1.0778717994689941
[08/28/2025 05:21:46 INFO]: Training loss at epoch 25: 0.7842543721199036
[08/28/2025 05:21:50 INFO]: Training loss at epoch 57: 1.0129455924034119
[08/28/2025 05:22:14 INFO]: Training loss at epoch 44: 0.8995288908481598
[08/28/2025 05:22:20 INFO]: Training loss at epoch 11: 0.9980376660823822
[08/28/2025 05:22:43 INFO]: Training loss at epoch 27: 0.9836329817771912
[08/28/2025 05:22:58 INFO]: Training loss at epoch 21: 1.2630941569805145
[08/28/2025 05:23:16 INFO]: Training loss at epoch 50: 0.8612535893917084
[08/28/2025 05:23:17 INFO]: Training loss at epoch 20: 1.1786197125911713
[08/28/2025 05:23:29 INFO]: Training loss at epoch 26: 0.8473866879940033
[08/28/2025 05:23:32 INFO]: Training loss at epoch 53: 0.9060056507587433
[08/28/2025 05:23:50 INFO]: Training loss at epoch 18: 0.974556177854538
[08/28/2025 05:23:51 INFO]: Training loss at epoch 28: 1.0637096762657166
[08/28/2025 05:23:57 INFO]: Training loss at epoch 31: 1.0718356668949127
[08/28/2025 05:24:00 INFO]: Training loss at epoch 45: 1.1779006719589233
[08/28/2025 05:24:15 INFO]: Training loss at epoch 12: 1.4902730584144592
[08/28/2025 05:24:21 INFO]: Training loss at epoch 29: 0.881282240152359
[08/28/2025 05:24:25 INFO]: Training loss at epoch 77: 0.953176349401474
[08/28/2025 05:24:27 INFO]: Training loss at epoch 57: 0.9278045296669006
[08/28/2025 05:24:49 INFO]: Training loss at epoch 53: 0.8246733546257019
[08/28/2025 05:24:50 INFO]: Training loss at epoch 42: 0.8973345160484314
[08/28/2025 05:25:01 INFO]: Training loss at epoch 51: 0.9643144905567169
[08/28/2025 05:25:02 INFO]: Training loss at epoch 21: 0.9919302463531494
[08/28/2025 05:25:13 INFO]: Running Final Evaluation...
[08/28/2025 05:25:14 INFO]: Training loss at epoch 27: 0.9785038828849792
[08/28/2025 05:25:21 INFO]: Running Final Evaluation...
[08/28/2025 05:25:22 INFO]: Training loss at epoch 54: 1.155387133359909
[08/28/2025 05:25:36 INFO]: Running Final Evaluation...
[08/28/2025 05:25:40 INFO]: Training loss at epoch 30: 0.9565310478210449
[08/28/2025 05:25:46 INFO]: Training loss at epoch 46: 0.9169297516345978
[08/28/2025 05:25:49 INFO]: Training accuracy: {
    "score": -1.0111501386195183,
    "rmse": 1.0111501386195183
}
[08/28/2025 05:25:49 INFO]: Val accuracy: {
    "score": -0.6606360417023133,
    "rmse": 0.6606360417023133
}
[08/28/2025 05:25:49 INFO]: Test accuracy: {
    "score": -0.8707445250638204,
    "rmse": 0.8707445250638204
}
[08/28/2025 05:25:49 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_99",
    "best_epoch": 20,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8707445250638204,
        "rmse": 0.8707445250638204
    },
    "train_stats": {
        "score": -1.0111501386195183,
        "rmse": 1.0111501386195183
    },
    "val_stats": {
        "score": -0.6606360417023133,
        "rmse": 0.6606360417023133
    }
}
[08/28/2025 05:25:49 INFO]: Procewss finished for trial unhanged-Shanesha_trial_99
[08/28/2025 05:25:49 INFO]: 
_________________________________________________

[08/28/2025 05:25:49 INFO]: train_net_for_optune.py main() running.
[08/28/2025 05:25:49 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 2.4342193715243097
  attention_dropout: 0.141070060529979
  ffn_dropout: 0.141070060529979
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 6.228084306232341e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_105

[08/28/2025 05:25:50 INFO]: This ft_transformer has 9.218 million parameters.
[08/28/2025 05:25:50 INFO]: Training will start at epoch 0.
[08/28/2025 05:25:50 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 05:25:55 INFO]: Training loss at epoch 19: 1.0013880729675293
[08/28/2025 05:25:57 INFO]: Training loss at epoch 44: 0.8809168934822083
[08/28/2025 05:26:02 INFO]: Training stats: {
    "score": -0.9997231513958945,
    "rmse": 0.9997231513958945
}
[08/28/2025 05:26:02 INFO]: Val stats: {
    "score": -0.6767919398061281,
    "rmse": 0.6767919398061281
}
[08/28/2025 05:26:02 INFO]: Test stats: {
    "score": -0.8725939626802457,
    "rmse": 0.8725939626802457
}
[08/28/2025 05:26:12 INFO]: Training loss at epoch 13: 1.163816511631012
[08/28/2025 05:26:18 INFO]: Training accuracy: {
    "score": -1.0091625619137545,
    "rmse": 1.0091625619137545
}
[08/28/2025 05:26:18 INFO]: Val accuracy: {
    "score": -0.6621107239847583,
    "rmse": 0.6621107239847583
}
[08/28/2025 05:26:18 INFO]: Test accuracy: {
    "score": -0.8698908041665301,
    "rmse": 0.8698908041665301
}
[08/28/2025 05:26:18 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_98",
    "best_epoch": 23,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8698908041665301,
        "rmse": 0.8698908041665301
    },
    "train_stats": {
        "score": -1.0091625619137545,
        "rmse": 1.0091625619137545
    },
    "val_stats": {
        "score": -0.6621107239847583,
        "rmse": 0.6621107239847583
    }
}
[08/28/2025 05:26:18 INFO]: Procewss finished for trial unhanged-Shanesha_trial_98
[08/28/2025 05:26:18 INFO]: 
_________________________________________________

[08/28/2025 05:26:18 INFO]: train_net_for_optune.py main() running.
[08/28/2025 05:26:18 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 2.653584398419122
  attention_dropout: 0.14219434125189218
  ffn_dropout: 0.14219434125189218
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.693711335643905e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_106

[08/28/2025 05:26:18 INFO]: This ft_transformer has 13.889 million parameters.
[08/28/2025 05:26:18 INFO]: Training will start at epoch 0.
[08/28/2025 05:26:18 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 05:26:27 INFO]: Training loss at epoch 87: 0.9476999640464783
[08/28/2025 05:26:41 INFO]: Training stats: {
    "score": -0.9991203477490249,
    "rmse": 0.9991203477490249
}
[08/28/2025 05:26:41 INFO]: Val stats: {
    "score": -0.694324484820739,
    "rmse": 0.694324484820739
}
[08/28/2025 05:26:41 INFO]: Test stats: {
    "score": -0.881558742563657,
    "rmse": 0.881558742563657
}
[08/28/2025 05:26:53 INFO]: Training loss at epoch 22: 0.9879325032234192
[08/28/2025 05:26:56 INFO]: Training accuracy: {
    "score": -1.019184840839795,
    "rmse": 1.019184840839795
}
[08/28/2025 05:26:56 INFO]: Val accuracy: {
    "score": -0.6613660385724116,
    "rmse": 0.6613660385724116
}
[08/28/2025 05:26:56 INFO]: Test accuracy: {
    "score": -0.8758224514738754,
    "rmse": 0.8758224514738754
}
[08/28/2025 05:26:56 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_87",
    "best_epoch": 11,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8758224514738754,
        "rmse": 0.8758224514738754
    },
    "train_stats": {
        "score": -1.019184840839795,
        "rmse": 1.019184840839795
    },
    "val_stats": {
        "score": -0.6613660385724116,
        "rmse": 0.6613660385724116
    }
}
[08/28/2025 05:26:56 INFO]: Procewss finished for trial unhanged-Shanesha_trial_87
[08/28/2025 05:26:56 INFO]: 
_________________________________________________

[08/28/2025 05:26:56 INFO]: train_net_for_optune.py main() running.
[08/28/2025 05:26:56 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 2.4140639601437597
  attention_dropout: 0.21263860872016965
  ffn_dropout: 0.21263860872016965
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.5811856335029123e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_107

[08/28/2025 05:26:56 INFO]: This ft_transformer has 5.352 million parameters.
[08/28/2025 05:26:56 INFO]: Training will start at epoch 0.
[08/28/2025 05:26:56 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 05:27:01 INFO]: Training loss at epoch 28: 0.8538265228271484
[08/28/2025 05:27:06 INFO]: Training loss at epoch 28: 0.908184140920639
[08/28/2025 05:27:42 INFO]: Training loss at epoch 47: 1.0564047992229462
[08/28/2025 05:27:54 INFO]: Training loss at epoch 58: 1.133303701877594
[08/28/2025 05:27:56 INFO]: Running Final Evaluation...
[08/28/2025 05:28:08 INFO]: Training loss at epoch 22: 1.0715879797935486
[08/28/2025 05:28:19 INFO]: Training loss at epoch 14: 1.1479578018188477
[08/28/2025 05:28:37 INFO]: Training accuracy: {
    "score": -1.0063412915782115,
    "rmse": 1.0063412915782115
}
[08/28/2025 05:28:37 INFO]: Val accuracy: {
    "score": -0.6591815630753847,
    "rmse": 0.6591815630753847
}
[08/28/2025 05:28:37 INFO]: Test accuracy: {
    "score": -0.8702184012060241,
    "rmse": 0.8702184012060241
}
[08/28/2025 05:28:37 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_100",
    "best_epoch": 16,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8702184012060241,
        "rmse": 0.8702184012060241
    },
    "train_stats": {
        "score": -1.0063412915782115,
        "rmse": 1.0063412915782115
    },
    "val_stats": {
        "score": -0.6591815630753847,
        "rmse": 0.6591815630753847
    }
}
[08/28/2025 05:28:37 INFO]: Procewss finished for trial unhanged-Shanesha_trial_100
[08/28/2025 05:28:37 INFO]: 
_________________________________________________

[08/28/2025 05:28:37 INFO]: train_net_for_optune.py main() running.
[08/28/2025 05:28:37 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 2.0252989759955007
  attention_dropout: 0.14790258755443655
  ffn_dropout: 0.14790258755443655
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.11727943788881e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_108

[08/28/2025 05:28:37 INFO]: This ft_transformer has 3.418 million parameters.
[08/28/2025 05:28:37 INFO]: Training will start at epoch 0.
[08/28/2025 05:28:37 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 05:28:49 INFO]: New best epoch, val score: -0.6623893310532938
[08/28/2025 05:28:49 INFO]: Saving model to: unhanged-Shanesha_trial_97/model_best.pth
[08/28/2025 05:28:51 INFO]: Training loss at epoch 23: 0.8061944544315338
[08/28/2025 05:29:00 INFO]: Training loss at epoch 20: 1.143060177564621
[08/28/2025 05:29:02 INFO]: Training loss at epoch 29: 1.053004801273346
[08/28/2025 05:29:12 INFO]: Training loss at epoch 32: 0.9513355195522308
[08/28/2025 05:29:22 INFO]: Training loss at epoch 29: 0.8728835582733154
[08/28/2025 05:29:24 INFO]: Training loss at epoch 0: 1.716623842716217
[08/28/2025 05:29:42 INFO]: Training stats: {
    "score": -0.9984254661848877,
    "rmse": 0.9984254661848877
}
[08/28/2025 05:29:42 INFO]: Val stats: {
    "score": -0.6794386697426048,
    "rmse": 0.6794386697426048
}
[08/28/2025 05:29:42 INFO]: Test stats: {
    "score": -0.8734994376058548,
    "rmse": 0.8734994376058548
}
[08/28/2025 05:29:44 INFO]: New best epoch, val score: -0.8940318353100325
[08/28/2025 05:29:44 INFO]: Saving model to: unhanged-Shanesha_trial_107/model_best.pth
[08/28/2025 05:29:50 INFO]: Training loss at epoch 58: 1.2583910822868347
[08/28/2025 05:30:21 INFO]: Training loss at epoch 0: 1.260091483592987
[08/28/2025 05:30:25 INFO]: Training loss at epoch 15: 0.9209221005439758
[08/28/2025 05:30:34 INFO]: New best epoch, val score: -0.7472183384025809
[08/28/2025 05:30:34 INFO]: Saving model to: unhanged-Shanesha_trial_108/model_best.pth
[08/28/2025 05:30:40 INFO]: Training loss at epoch 78: 0.7795443832874298
[08/28/2025 05:30:44 INFO]: Training loss at epoch 24: 0.8437913656234741
[08/28/2025 05:30:47 INFO]: Training loss at epoch 54: 0.955198734998703
[08/28/2025 05:31:09 INFO]: Training loss at epoch 0: 1.3504257202148438
[08/28/2025 05:31:12 INFO]: Training loss at epoch 21: 0.8510630428791046
[08/28/2025 05:31:13 INFO]: Training loss at epoch 31: 1.0443998575210571
[08/28/2025 05:31:13 INFO]: Training stats: {
    "score": -1.0106068425922863,
    "rmse": 1.0106068425922863
}
[08/28/2025 05:31:13 INFO]: Val stats: {
    "score": -0.6606571768767574,
    "rmse": 0.6606571768767574
}
[08/28/2025 05:31:13 INFO]: Test stats: {
    "score": -0.8699032376657718,
    "rmse": 0.8699032376657718
}
[08/28/2025 05:31:21 INFO]: Running Final Evaluation...
[08/28/2025 05:31:21 INFO]: Training loss at epoch 30: 1.006044864654541
[08/28/2025 05:31:23 INFO]: Training loss at epoch 45: 1.0125819444656372
[08/28/2025 05:31:29 INFO]: Training loss at epoch 30: 0.9539485275745392
[08/28/2025 05:31:34 INFO]: Training loss at epoch 29: 0.90993732213974
[08/28/2025 05:31:48 INFO]: New best epoch, val score: -1.0268549909763036
[08/28/2025 05:31:48 INFO]: Saving model to: unhanged-Shanesha_trial_105/model_best.pth
[08/28/2025 05:31:58 INFO]: Training loss at epoch 1: 1.3797040581703186
[08/28/2025 05:32:08 INFO]: Training loss at epoch 1: 1.1065733134746552
[08/28/2025 05:32:16 INFO]: New best epoch, val score: -0.6646708680110863
[08/28/2025 05:32:16 INFO]: Saving model to: unhanged-Shanesha_trial_107/model_best.pth
[08/28/2025 05:32:20 INFO]: Training loss at epoch 16: 0.9267081916332245
[08/28/2025 05:32:22 INFO]: New best epoch, val score: -0.6625308745643254
[08/28/2025 05:32:22 INFO]: Saving model to: unhanged-Shanesha_trial_108/model_best.pth
[08/28/2025 05:32:30 INFO]: Training loss at epoch 25: 0.8765436410903931
[08/28/2025 05:33:02 INFO]: Training stats: {
    "score": -1.0003379445175613,
    "rmse": 1.0003379445175613
}
[08/28/2025 05:33:02 INFO]: Val stats: {
    "score": -0.6709214472090242,
    "rmse": 0.6709214472090242
}
[08/28/2025 05:33:02 INFO]: Test stats: {
    "score": -0.87106826945521,
    "rmse": 0.87106826945521
}
[08/28/2025 05:33:13 INFO]: Training loss at epoch 31: 1.1149582266807556
[08/28/2025 05:33:15 INFO]: Training loss at epoch 0: 1.2997715473175049
[08/28/2025 05:33:17 INFO]: Training loss at epoch 22: 0.8022516965866089
[08/28/2025 05:33:20 INFO]: Training loss at epoch 23: 1.0460566282272339
[08/28/2025 05:33:23 INFO]: Training accuracy: {
    "score": -1.0120209207612518,
    "rmse": 1.0120209207612518
}
[08/28/2025 05:33:23 INFO]: Val accuracy: {
    "score": -0.6606116784048846,
    "rmse": 0.6606116784048846
}
[08/28/2025 05:33:23 INFO]: Test accuracy: {
    "score": -0.8705146312588088,
    "rmse": 0.8705146312588088
}
[08/28/2025 05:33:23 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_59",
    "best_epoch": 47,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8705146312588088,
        "rmse": 0.8705146312588088
    },
    "train_stats": {
        "score": -1.0120209207612518,
        "rmse": 1.0120209207612518
    },
    "val_stats": {
        "score": -0.6606116784048846,
        "rmse": 0.6606116784048846
    }
}
[08/28/2025 05:33:23 INFO]: Procewss finished for trial unhanged-Shanesha_trial_59
[08/28/2025 05:33:23 INFO]: 
_________________________________________________

[08/28/2025 05:33:23 INFO]: train_net_for_optune.py main() running.
[08/28/2025 05:33:23 INFO]: model:
  name: ft_transformer
  d_embedding: 320
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 2.03882717419644
  attention_dropout: 0.14765874545556834
  ffn_dropout: 0.14765874545556834
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.0813849209681041e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_109

[08/28/2025 05:33:23 INFO]: This ft_transformer has 3.428 million parameters.
[08/28/2025 05:33:23 INFO]: Training will start at epoch 0.
[08/28/2025 05:33:23 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 05:33:24 INFO]: Training loss at epoch 88: 1.130211591720581
[08/28/2025 05:33:57 INFO]: Training loss at epoch 2: 0.9960544109344482
[08/28/2025 05:33:59 INFO]: Training loss at epoch 59: 0.9583616256713867
[08/28/2025 05:34:07 INFO]: New best epoch, val score: -0.6567460218380431
[08/28/2025 05:34:07 INFO]: Saving model to: unhanged-Shanesha_trial_106/model_best.pth
[08/28/2025 05:34:14 INFO]: Training loss at epoch 33: 0.9595138132572174
[08/28/2025 05:34:16 INFO]: Training loss at epoch 26: 0.8624656796455383
[08/28/2025 05:34:17 INFO]: Training loss at epoch 17: 1.207922488451004
[08/28/2025 05:34:29 INFO]: Training loss at epoch 2: 1.0875349044799805
[08/28/2025 05:34:58 INFO]: Training loss at epoch 59: 0.9294118881225586
[08/28/2025 05:35:03 INFO]: Training loss at epoch 32: 0.9878131151199341
[08/28/2025 05:35:04 INFO]: Training loss at epoch 0: 1.2746157050132751
[08/28/2025 05:35:18 INFO]: New best epoch, val score: -0.7291099783266203
[08/28/2025 05:35:18 INFO]: Saving model to: unhanged-Shanesha_trial_109/model_best.pth
[08/28/2025 05:35:30 INFO]: Training loss at epoch 23: 0.9114479124546051
[08/28/2025 05:35:54 INFO]: Training loss at epoch 3: 0.9360245764255524
[08/28/2025 05:36:10 INFO]: Training stats: {
    "score": -0.9998566696137928,
    "rmse": 0.9998566696137928
}
[08/28/2025 05:36:10 INFO]: Val stats: {
    "score": -0.6786749262618492,
    "rmse": 0.6786749262618492
}
[08/28/2025 05:36:10 INFO]: Test stats: {
    "score": -0.8732336543330054,
    "rmse": 0.8732336543330054
}
[08/28/2025 05:36:13 INFO]: Training loss at epoch 27: 1.0449799299240112
[08/28/2025 05:36:24 INFO]: Training loss at epoch 18: 0.8681731224060059
[08/28/2025 05:36:31 INFO]: Training loss at epoch 31: 1.0249689221382141
[08/28/2025 05:36:32 INFO]: Training loss at epoch 55: 0.9477147161960602
[08/28/2025 05:36:34 INFO]: Training loss at epoch 32: 1.3474508225917816
[08/28/2025 05:36:39 INFO]: Training loss at epoch 46: 0.9087751507759094
[08/28/2025 05:36:45 INFO]: Training loss at epoch 30: 0.9777681827545166
[08/28/2025 05:36:51 INFO]: Training stats: {
    "score": -1.020252941103141,
    "rmse": 1.020252941103141
}
[08/28/2025 05:36:51 INFO]: Val stats: {
    "score": -0.6633295281593594,
    "rmse": 0.6633295281593594
}
[08/28/2025 05:36:51 INFO]: Test stats: {
    "score": -0.8767116652494094,
    "rmse": 0.8767116652494094
}
[08/28/2025 05:36:59 INFO]: Training loss at epoch 33: 1.2368413209915161
[08/28/2025 05:37:01 INFO]: Training loss at epoch 1: 1.3220751285552979
[08/28/2025 05:37:05 INFO]: Training loss at epoch 1: 1.1745268106460571
[08/28/2025 05:37:13 INFO]: Training loss at epoch 3: 1.211700201034546
[08/28/2025 05:37:18 INFO]: New best epoch, val score: -0.6578215679598707
[08/28/2025 05:37:18 INFO]: Saving model to: unhanged-Shanesha_trial_109/model_best.pth
[08/28/2025 05:37:34 INFO]: Training loss at epoch 30: 0.9643909931182861
[08/28/2025 05:37:44 INFO]: New best epoch, val score: -0.9868911313479618
[08/28/2025 05:37:44 INFO]: Saving model to: unhanged-Shanesha_trial_105/model_best.pth
[08/28/2025 05:37:48 INFO]: Training loss at epoch 24: 1.0069137811660767
[08/28/2025 05:37:52 INFO]: Training loss at epoch 4: 1.194726824760437
[08/28/2025 05:38:10 INFO]: Training loss at epoch 28: 0.8352357149124146
[08/28/2025 05:38:30 INFO]: Training loss at epoch 19: 0.9710288047790527
[08/28/2025 05:38:41 INFO]: Training loss at epoch 24: 1.1512645483016968
[08/28/2025 05:38:49 INFO]: Training loss at epoch 34: 1.1675147712230682
[08/28/2025 05:38:59 INFO]: Training loss at epoch 2: 0.975253701210022
[08/28/2025 05:39:09 INFO]: Training stats: {
    "score": -0.9984769988831094,
    "rmse": 0.9984769988831094
}
[08/28/2025 05:39:09 INFO]: Val stats: {
    "score": -0.6877103362848205,
    "rmse": 0.6877103362848205
}
[08/28/2025 05:39:09 INFO]: Test stats: {
    "score": -0.8762782074853005,
    "rmse": 0.8762782074853005
}
[08/28/2025 05:39:35 INFO]: Training loss at epoch 34: 0.9038881063461304
[08/28/2025 05:39:45 INFO]: Training loss at epoch 5: 1.178739845752716
[08/28/2025 05:39:50 INFO]: Training loss at epoch 4: 1.2225956916809082
[08/28/2025 05:39:58 INFO]: Training loss at epoch 29: 1.342781513929367
[08/28/2025 05:39:58 INFO]: Training loss at epoch 25: 0.9643127024173737
[08/28/2025 05:40:22 INFO]: Training loss at epoch 89: 1.1914462447166443
[08/28/2025 05:40:34 INFO]: Training stats: {
    "score": -1.0108322644346572,
    "rmse": 1.0108322644346572
}
[08/28/2025 05:40:34 INFO]: Val stats: {
    "score": -0.7359950861242579,
    "rmse": 0.7359950861242579
}
[08/28/2025 05:40:34 INFO]: Test stats: {
    "score": -0.9061199150626014,
    "rmse": 0.9061199150626014
}
[08/28/2025 05:40:36 INFO]: Training loss at epoch 35: 0.9457115530967712
[08/28/2025 05:40:49 INFO]: Training loss at epoch 3: 0.9810212850570679
[08/28/2025 05:41:06 INFO]: Training loss at epoch 20: 1.1301888823509216
[08/28/2025 05:41:07 INFO]: Training loss at epoch 1: 1.3815861940383911
[08/28/2025 05:41:32 INFO]: Training loss at epoch 6: 1.379713535308838
[08/28/2025 05:41:38 INFO]: Training loss at epoch 32: 1.072131872177124
[08/28/2025 05:41:50 INFO]: Training loss at epoch 47: 0.8676334619522095
[08/28/2025 05:41:53 INFO]: Training loss at epoch 33: 1.1678707599639893
[08/28/2025 05:41:54 INFO]: Training loss at epoch 31: 0.957964152097702
[08/28/2025 05:42:01 INFO]: Training loss at epoch 60: 1.1391584873199463
[08/28/2025 05:42:02 INFO]: Training loss at epoch 26: 1.008037805557251
[08/28/2025 05:42:11 INFO]: Training loss at epoch 31: 0.9832485020160675
[08/28/2025 05:42:15 INFO]: Training loss at epoch 56: 1.0026559829711914
[08/28/2025 05:42:15 INFO]: Training loss at epoch 60: 1.1103710532188416
[08/28/2025 05:42:18 INFO]: Training loss at epoch 5: 1.0632676780223846
[08/28/2025 05:42:19 INFO]: Training loss at epoch 30: 1.15780371427536
[08/28/2025 05:42:20 INFO]: Training loss at epoch 36: 0.951919287443161
[08/28/2025 05:42:38 INFO]: Training stats: {
    "score": -1.0113887585617478,
    "rmse": 1.0113887585617478
}
[08/28/2025 05:42:38 INFO]: Val stats: {
    "score": -0.7385396721807126,
    "rmse": 0.7385396721807126
}
[08/28/2025 05:42:38 INFO]: Test stats: {
    "score": -0.9074869572152617,
    "rmse": 0.9074869572152617
}
[08/28/2025 05:42:40 INFO]: Training loss at epoch 4: 1.3214397430419922
[08/28/2025 05:42:46 INFO]: Training loss at epoch 2: 1.6443758010864258
[08/28/2025 05:43:03 INFO]: Training loss at epoch 21: 1.2816851437091827
[08/28/2025 05:43:22 INFO]: Training loss at epoch 7: 1.3326426148414612
[08/28/2025 05:43:25 INFO]: New best epoch, val score: -0.6871868952723211
[08/28/2025 05:43:25 INFO]: Saving model to: unhanged-Shanesha_trial_105/model_best.pth
[08/28/2025 05:43:44 INFO]: Training loss at epoch 25: 0.8302088975906372
[08/28/2025 05:44:07 INFO]: Training loss at epoch 37: 1.1015098094940186
[08/28/2025 05:44:08 INFO]: Training loss at epoch 31: 1.0782361924648285
[08/28/2025 05:44:10 INFO]: Training loss at epoch 27: 1.028849571943283
[08/28/2025 05:44:30 INFO]: Training loss at epoch 5: 1.1590078473091125
[08/28/2025 05:44:35 INFO]: Training loss at epoch 35: 1.0186772346496582
[08/28/2025 05:44:53 INFO]: Training loss at epoch 6: 1.0151722729206085
[08/28/2025 05:45:02 INFO]: Training loss at epoch 22: 0.8749822974205017
[08/28/2025 05:45:15 INFO]: Training loss at epoch 8: 1.152772843837738
[08/28/2025 05:45:59 INFO]: Training loss at epoch 38: 0.8818946480751038
[08/28/2025 05:46:01 INFO]: Training loss at epoch 32: 1.1031833589076996
[08/28/2025 05:46:19 INFO]: Training loss at epoch 32: 1.0159366130828857
[08/28/2025 05:46:25 INFO]: Training loss at epoch 28: 0.8224433660507202
[08/28/2025 05:46:26 INFO]: Training loss at epoch 6: 1.394837200641632
[08/28/2025 05:46:47 INFO]: Training loss at epoch 33: 0.9809864163398743
[08/28/2025 05:47:04 INFO]: Training loss at epoch 23: 1.1343462467193604
[08/28/2025 05:47:07 INFO]: Training loss at epoch 48: 0.8859220743179321
[08/28/2025 05:47:10 INFO]: Training loss at epoch 9: 1.0918155312538147
[08/28/2025 05:47:16 INFO]: Training loss at epoch 34: 0.9009942710399628
[08/28/2025 05:47:17 INFO]: Training loss at epoch 61: 0.9001557230949402
[08/28/2025 05:47:33 INFO]: Training loss at epoch 7: 1.0473584532737732
[08/28/2025 05:47:42 INFO]: Training loss at epoch 32: 1.1959545016288757
[08/28/2025 05:47:51 INFO]: Training stats: {
    "score": -1.070666373606715,
    "rmse": 1.070666373606715
}
[08/28/2025 05:47:51 INFO]: Val stats: {
    "score": -0.8561402040336746,
    "rmse": 0.8561402040336746
}
[08/28/2025 05:47:51 INFO]: Test stats: {
    "score": -0.9975280273443679,
    "rmse": 0.9975280273443679
}
[08/28/2025 05:47:53 INFO]: Training loss at epoch 39: 0.9260799288749695
[08/28/2025 05:47:56 INFO]: Training loss at epoch 33: 0.9780122935771942
[08/28/2025 05:48:08 INFO]: Training loss at epoch 57: 1.1028219163417816
[08/28/2025 05:48:25 INFO]: Training loss at epoch 7: 1.4241867065429688
[08/28/2025 05:48:32 INFO]: Training stats: {
    "score": -0.997825178339733,
    "rmse": 0.997825178339733
}
[08/28/2025 05:48:32 INFO]: Val stats: {
    "score": -0.6860492439757044,
    "rmse": 0.6860492439757044
}
[08/28/2025 05:48:32 INFO]: Test stats: {
    "score": -0.8767329506503028,
    "rmse": 0.8767329506503028
}
[08/28/2025 05:48:32 INFO]: Training loss at epoch 61: 0.9633642435073853
[08/28/2025 05:48:43 INFO]: Training loss at epoch 29: 0.9982507526874542
[08/28/2025 05:48:45 INFO]: Training loss at epoch 3: 1.1021344065666199
[08/28/2025 05:48:57 INFO]: Training loss at epoch 2: 1.0453274548053741
[08/28/2025 05:49:06 INFO]: Training loss at epoch 26: 1.1210995614528656
[08/28/2025 05:49:11 INFO]: Training loss at epoch 24: 1.0650674700737
[08/28/2025 05:49:28 INFO]: Training stats: {
    "score": -1.0108282068620391,
    "rmse": 1.0108282068620391
}
[08/28/2025 05:49:28 INFO]: Val stats: {
    "score": -0.7410490373704922,
    "rmse": 0.7410490373704922
}
[08/28/2025 05:49:28 INFO]: Test stats: {
    "score": -0.9104071126774756,
    "rmse": 0.9104071126774756
}
[08/28/2025 05:49:37 INFO]: Training loss at epoch 90: 1.0932367444038391
[08/28/2025 05:49:49 INFO]: Training loss at epoch 10: 1.0822365880012512
[08/28/2025 05:49:52 INFO]: Training loss at epoch 34: 1.04453644156456
[08/28/2025 05:49:59 INFO]: Training loss at epoch 36: 0.9409584403038025
[08/28/2025 05:50:15 INFO]: Training loss at epoch 8: 1.1487368941307068
[08/28/2025 05:50:23 INFO]: Training loss at epoch 8: 1.307646095752716
[08/28/2025 05:50:24 INFO]: Training loss at epoch 40: 1.1231935024261475
[08/28/2025 05:50:50 INFO]: Training loss at epoch 33: 0.9865321516990662
[08/28/2025 05:51:09 INFO]: Training loss at epoch 25: 1.1152750849723816
[08/28/2025 05:51:23 INFO]: New best epoch, val score: -0.6673905695974665
[08/28/2025 05:51:23 INFO]: Saving model to: unhanged-Shanesha_trial_104/model_best.pth
[08/28/2025 05:51:36 INFO]: Training loss at epoch 30: 1.0958859324455261
[08/28/2025 05:51:37 INFO]: Training loss at epoch 11: 1.158721387386322
[08/28/2025 05:51:38 INFO]: Training loss at epoch 35: 0.9858990609645844
[08/28/2025 05:51:58 INFO]: Training loss at epoch 34: 1.0888165831565857
[08/28/2025 05:52:08 INFO]: Training loss at epoch 41: 1.1979005634784698
[08/28/2025 05:52:10 INFO]: Training loss at epoch 9: 1.030921310186386
[08/28/2025 05:52:24 INFO]: Training loss at epoch 49: 0.9814606308937073
[08/28/2025 05:52:30 INFO]: Training loss at epoch 62: 1.2254191040992737
[08/28/2025 05:52:37 INFO]: Training loss at epoch 35: 1.1671012043952942
[08/28/2025 05:52:43 INFO]: Training loss at epoch 9: 1.083168625831604
[08/28/2025 05:52:47 INFO]: Training stats: {
    "score": -1.0905389414063222,
    "rmse": 1.0905389414063222
}
[08/28/2025 05:52:47 INFO]: Val stats: {
    "score": -0.9046200428734124,
    "rmse": 0.9046200428734124
}
[08/28/2025 05:52:47 INFO]: Test stats: {
    "score": -1.0295876471830403,
    "rmse": 1.0295876471830403
}
[08/28/2025 05:53:04 INFO]: Training loss at epoch 26: 0.9327657222747803
[08/28/2025 05:53:05 INFO]: Training loss at epoch 33: 0.9700502157211304
[08/28/2025 05:53:23 INFO]: Training loss at epoch 36: 1.0450501441955566
[08/28/2025 05:53:24 INFO]: Training loss at epoch 12: 1.0070709586143494
[08/28/2025 05:53:32 INFO]: Training stats: {
    "score": -1.0632787100435206,
    "rmse": 1.0632787100435206
}
[08/28/2025 05:53:32 INFO]: Val stats: {
    "score": -0.8456188096609945,
    "rmse": 0.8456188096609945
}
[08/28/2025 05:53:32 INFO]: Test stats: {
    "score": -0.9865913575729492,
    "rmse": 0.9865913575729492
}
[08/28/2025 05:53:40 INFO]: Training loss at epoch 31: 1.0469244718551636
[08/28/2025 05:53:47 INFO]: Training loss at epoch 58: 1.082727700471878
[08/28/2025 05:53:54 INFO]: Training loss at epoch 42: 1.1322360038757324
[08/28/2025 05:53:55 INFO]: Running Final Evaluation...
[08/28/2025 05:54:06 INFO]: Running Final Evaluation...
[08/28/2025 05:54:08 INFO]: Training loss at epoch 27: 1.2204287648200989
[08/28/2025 05:54:08 INFO]: Training stats: {
    "score": -1.0065273533937678,
    "rmse": 1.0065273533937678
}
[08/28/2025 05:54:09 INFO]: Val stats: {
    "score": -0.6622653767325029,
    "rmse": 0.6622653767325029
}
[08/28/2025 05:54:09 INFO]: Test stats: {
    "score": -0.869688239283198,
    "rmse": 0.869688239283198
}
[08/28/2025 05:54:26 INFO]: Running Final Evaluation...
[08/28/2025 05:54:26 INFO]: Training loss at epoch 4: 1.2461031675338745
[08/28/2025 05:54:30 INFO]: Training loss at epoch 62: 1.1452780067920685
[08/28/2025 05:54:35 INFO]: Training loss at epoch 10: 0.8653184473514557
[08/28/2025 05:54:38 INFO]: Training accuracy: {
    "score": -1.0085312129312998,
    "rmse": 1.0085312129312998
}
[08/28/2025 05:54:38 INFO]: Val accuracy: {
    "score": -0.6607570597286027,
    "rmse": 0.6607570597286027
}
[08/28/2025 05:54:38 INFO]: Test accuracy: {
    "score": -0.8785172121331035,
    "rmse": 0.8785172121331035
}
[08/28/2025 05:54:38 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_102",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8785172121331035,
        "rmse": 0.8785172121331035
    },
    "train_stats": {
        "score": -1.0085312129312998,
        "rmse": 1.0085312129312998
    },
    "val_stats": {
        "score": -0.6607570597286027,
        "rmse": 0.6607570597286027
    }
}
[08/28/2025 05:54:38 INFO]: Procewss finished for trial unhanged-Shanesha_trial_102
[08/28/2025 05:54:38 INFO]: 
_________________________________________________

[08/28/2025 05:54:38 INFO]: train_net_for_optune.py main() running.
[08/28/2025 05:54:38 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 1.8781223159801474
  attention_dropout: 0.2035826036289144
  ffn_dropout: 0.2035826036289144
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.572710603446428e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_110

[08/28/2025 05:54:38 INFO]: This ft_transformer has 11.605 million parameters.
[08/28/2025 05:54:38 INFO]: Training will start at epoch 0.
[08/28/2025 05:54:38 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 05:54:42 INFO]: Training accuracy: {
    "score": -1.0179082512692408,
    "rmse": 1.0179082512692408
}
[08/28/2025 05:54:42 INFO]: Val accuracy: {
    "score": -0.6608423429754562,
    "rmse": 0.6608423429754562
}
[08/28/2025 05:54:42 INFO]: Test accuracy: {
    "score": -0.8731122413188761,
    "rmse": 0.8731122413188761
}
[08/28/2025 05:54:42 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_101",
    "best_epoch": 11,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8731122413188761,
        "rmse": 0.8731122413188761
    },
    "train_stats": {
        "score": -1.0179082512692408,
        "rmse": 1.0179082512692408
    },
    "val_stats": {
        "score": -0.6608423429754562,
        "rmse": 0.6608423429754562
    }
}
[08/28/2025 05:54:42 INFO]: Procewss finished for trial unhanged-Shanesha_trial_101
[08/28/2025 05:54:42 INFO]: 
_________________________________________________

[08/28/2025 05:54:42 INFO]: train_net_for_optune.py main() running.
[08/28/2025 05:54:42 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 2.4540330696359476
  attention_dropout: 0.2090581617824265
  ffn_dropout: 0.2090581617824265
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.4233910025425647e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_111

[08/28/2025 05:54:43 INFO]: This ft_transformer has 5.398 million parameters.
[08/28/2025 05:54:43 INFO]: Training will start at epoch 0.
[08/28/2025 05:54:43 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 05:54:59 INFO]: Training loss at epoch 37: 1.0019007921218872
[08/28/2025 05:55:01 INFO]: Training loss at epoch 27: 1.1198948621749878
[08/28/2025 05:55:04 INFO]: Training loss at epoch 34: 1.0409988462924957
[08/28/2025 05:55:12 INFO]: Training loss at epoch 37: 0.8320972919464111
[08/28/2025 05:55:15 INFO]: Training loss at epoch 13: 1.1646882891654968
[08/28/2025 05:56:11 INFO]: Training loss at epoch 10: 1.0134690999984741
[08/28/2025 05:56:21 INFO]: Training loss at epoch 91: 0.9551456272602081
[08/28/2025 05:56:36 INFO]: Training loss at epoch 11: 1.0890082716941833
[08/28/2025 05:56:38 INFO]: Training loss at epoch 3: 0.8667864799499512
[08/28/2025 05:56:43 INFO]: Training accuracy: {
    "score": -1.0160301398475515,
    "rmse": 1.0160301398475515
}
[08/28/2025 05:56:43 INFO]: Val accuracy: {
    "score": -0.6606928242113406,
    "rmse": 0.6606928242113406
}
[08/28/2025 05:56:43 INFO]: Test accuracy: {
    "score": -0.872365376174901,
    "rmse": 0.872365376174901
}
[08/28/2025 05:56:43 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_70",
    "best_epoch": 27,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.872365376174901,
        "rmse": 0.872365376174901
    },
    "train_stats": {
        "score": -1.0160301398475515,
        "rmse": 1.0160301398475515
    },
    "val_stats": {
        "score": -0.6606928242113406,
        "rmse": 0.6606928242113406
    }
}
[08/28/2025 05:56:43 INFO]: Procewss finished for trial unhanged-Shanesha_trial_70
[08/28/2025 05:56:43 INFO]: 
_________________________________________________

[08/28/2025 05:56:43 INFO]: train_net_for_optune.py main() running.
[08/28/2025 05:56:43 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 2.4454620974719647
  attention_dropout: 0.2600959184325848
  ffn_dropout: 0.2600959184325848
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.269188638303266e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_112

[08/28/2025 05:56:43 INFO]: This ft_transformer has 5.389 million parameters.
[08/28/2025 05:56:43 INFO]: Training will start at epoch 0.
[08/28/2025 05:56:43 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 05:57:08 INFO]: Training loss at epoch 28: 0.9695048034191132
[08/28/2025 05:57:08 INFO]: Training loss at epoch 35: 1.0275869369506836
[08/28/2025 05:57:09 INFO]: Training loss at epoch 0: 0.9404170513153076
[08/28/2025 05:57:10 INFO]: Training loss at epoch 38: 1.1204745769500732
[08/28/2025 05:57:14 INFO]: Training loss at epoch 14: 1.195754885673523
[08/28/2025 05:57:29 INFO]: New best epoch, val score: -0.7495348959728577
[08/28/2025 05:57:29 INFO]: Saving model to: unhanged-Shanesha_trial_111/model_best.pth
[08/28/2025 05:57:37 INFO]: New best epoch, val score: -0.6480977165632422
[08/28/2025 05:57:37 INFO]: Saving model to: unhanged-Shanesha_trial_106/model_best.pth
[08/28/2025 05:57:50 INFO]: Training loss at epoch 63: 1.0119368135929108
[08/28/2025 05:58:05 INFO]: Training loss at epoch 36: 1.086697518825531
[08/28/2025 05:58:35 INFO]: Training loss at epoch 12: 0.9626238942146301
[08/28/2025 05:58:38 INFO]: Training loss at epoch 34: 0.9413018226623535
[08/28/2025 05:58:53 INFO]: Training loss at epoch 11: 0.9552267491817474
[08/28/2025 05:59:03 INFO]: Training loss at epoch 39: 0.9466804265975952
[08/28/2025 05:59:06 INFO]: Training loss at epoch 0: 1.5762524008750916
[08/28/2025 05:59:09 INFO]: Training loss at epoch 15: 0.9714435636997223
[08/28/2025 05:59:11 INFO]: Training loss at epoch 29: 1.0036225616931915
[08/28/2025 05:59:23 INFO]: New best epoch, val score: -1.1030292700989788
[08/28/2025 05:59:23 INFO]: Saving model to: unhanged-Shanesha_trial_112/model_best.pth
[08/28/2025 05:59:29 INFO]: Training loss at epoch 28: 1.0391114950180054
[08/28/2025 05:59:33 INFO]: Training loss at epoch 50: 0.8506132662296295
[08/28/2025 05:59:37 INFO]: Training loss at epoch 35: 1.0244081020355225
[08/28/2025 05:59:39 INFO]: Training stats: {
    "score": -0.9982930682207669,
    "rmse": 0.9982930682207669
}
[08/28/2025 05:59:39 INFO]: Val stats: {
    "score": -0.6698528704784181,
    "rmse": 0.6698528704784181
}
[08/28/2025 05:59:39 INFO]: Test stats: {
    "score": -0.869426711096145,
    "rmse": 0.869426711096145
}
[08/28/2025 05:59:46 INFO]: Training loss at epoch 1: 1.0158672034740448
[08/28/2025 05:59:50 INFO]: Training stats: {
    "score": -1.0044182806954502,
    "rmse": 1.0044182806954502
}
[08/28/2025 05:59:50 INFO]: Val stats: {
    "score": -0.6689517304151673,
    "rmse": 0.6689517304151673
}
[08/28/2025 05:59:50 INFO]: Test stats: {
    "score": -0.8738846927456118,
    "rmse": 0.8738846927456118
}
[08/28/2025 06:00:04 INFO]: New best epoch, val score: -0.6921437328974485
[08/28/2025 06:00:04 INFO]: Saving model to: unhanged-Shanesha_trial_111/model_best.pth
[08/28/2025 06:00:15 INFO]: Training loss at epoch 38: 1.1554901003837585
[08/28/2025 06:00:24 INFO]: Training loss at epoch 5: 1.2432981133460999
[08/28/2025 06:00:25 INFO]: Training loss at epoch 13: 1.0668964982032776
[08/28/2025 06:00:46 INFO]: Training loss at epoch 63: 1.3639457821846008
[08/28/2025 06:00:50 INFO]: Training loss at epoch 0: 1.0434498190879822
[08/28/2025 06:00:58 INFO]: Training loss at epoch 16: 0.9911929965019226
[08/28/2025 06:01:22 INFO]: Training loss at epoch 12: 1.0484967827796936
[08/28/2025 06:01:25 INFO]: Training loss at epoch 40: 0.8599569499492645
[08/28/2025 06:01:36 INFO]: Training loss at epoch 1: 1.5718671083450317
[08/28/2025 06:01:37 INFO]: New best epoch, val score: -0.665878605738974
[08/28/2025 06:01:37 INFO]: Saving model to: unhanged-Shanesha_trial_110/model_best.pth
[08/28/2025 06:01:45 INFO]: Training loss at epoch 30: 0.905166357755661
[08/28/2025 06:01:53 INFO]: New best epoch, val score: -0.7654638837021732
[08/28/2025 06:01:53 INFO]: Saving model to: unhanged-Shanesha_trial_112/model_best.pth
[08/28/2025 06:02:16 INFO]: Training loss at epoch 14: 0.8836080133914948
[08/28/2025 06:02:16 INFO]: Training loss at epoch 36: 0.9252638816833496
[08/28/2025 06:02:16 INFO]: Training loss at epoch 2: 1.1516879200935364
[08/28/2025 06:02:47 INFO]: Training loss at epoch 17: 0.9002697169780731
[08/28/2025 06:02:54 INFO]: Training loss at epoch 64: 0.9395285844802856
[08/28/2025 06:03:09 INFO]: Training loss at epoch 92: 1.191466212272644
[08/28/2025 06:03:14 INFO]: Training loss at epoch 41: 1.189333200454712
[08/28/2025 06:03:19 INFO]: Training loss at epoch 37: 0.8865683078765869
[08/28/2025 06:03:47 INFO]: Training loss at epoch 31: 1.168546438217163
[08/28/2025 06:03:58 INFO]: Training loss at epoch 13: 0.9509490430355072
[08/28/2025 06:03:58 INFO]: Training loss at epoch 35: 1.3563069105148315
[08/28/2025 06:03:58 INFO]: Training loss at epoch 36: 1.100046843290329
[08/28/2025 06:04:14 INFO]: Training loss at epoch 15: 0.8832345604896545
[08/28/2025 06:04:16 INFO]: Training loss at epoch 2: 1.1114587187767029
[08/28/2025 06:04:25 INFO]: Training loss at epoch 4: 0.9542201161384583
[08/28/2025 06:04:36 INFO]: New best epoch, val score: -0.6871164974296238
[08/28/2025 06:04:36 INFO]: Saving model to: unhanged-Shanesha_trial_112/model_best.pth
[08/28/2025 06:04:39 INFO]: Training loss at epoch 29: 1.0272372364997864
[08/28/2025 06:04:45 INFO]: Training loss at epoch 18: 1.0848060250282288
[08/28/2025 06:04:47 INFO]: Training loss at epoch 51: 0.9953556060791016
[08/28/2025 06:04:59 INFO]: Training loss at epoch 3: 0.8486403524875641
[08/28/2025 06:05:11 INFO]: Training loss at epoch 42: 1.025180697441101
[08/28/2025 06:05:29 INFO]: Training loss at epoch 39: 0.9689436852931976
[08/28/2025 06:05:29 INFO]: New best epoch, val score: -0.6616226954555516
[08/28/2025 06:05:29 INFO]: Saving model to: unhanged-Shanesha_trial_81/model_best.pth
[08/28/2025 06:05:54 INFO]: Training loss at epoch 32: 0.9093877971172333
[08/28/2025 06:06:15 INFO]: Training loss at epoch 16: 0.971648097038269
[08/28/2025 06:06:21 INFO]: Training loss at epoch 6: 0.9861893355846405
[08/28/2025 06:06:34 INFO]: Training stats: {
    "score": -0.9986424167367061,
    "rmse": 0.9986424167367061
}
[08/28/2025 06:06:34 INFO]: Val stats: {
    "score": -0.695315623073222,
    "rmse": 0.695315623073222
}
[08/28/2025 06:06:34 INFO]: Test stats: {
    "score": -0.8831133296924495,
    "rmse": 0.8831133296924495
}
[08/28/2025 06:06:45 INFO]: Training loss at epoch 14: 0.9585475027561188
[08/28/2025 06:06:46 INFO]: Training loss at epoch 19: 0.8728929162025452
[08/28/2025 06:07:03 INFO]: Training loss at epoch 3: 1.1235317587852478
[08/28/2025 06:07:04 INFO]: New best epoch, val score: -0.664558877188208
[08/28/2025 06:07:04 INFO]: Saving model to: unhanged-Shanesha_trial_107/model_best.pth
[08/28/2025 06:07:07 INFO]: Training loss at epoch 64: 0.8438606262207031
[08/28/2025 06:07:09 INFO]: Training loss at epoch 43: 0.8151956796646118
[08/28/2025 06:07:20 INFO]: Training stats: {
    "score": -1.005925597515316,
    "rmse": 1.005925597515316
}
[08/28/2025 06:07:20 INFO]: Val stats: {
    "score": -0.6626611516143688,
    "rmse": 0.6626611516143688
}
[08/28/2025 06:07:20 INFO]: Test stats: {
    "score": -0.8686035190673437,
    "rmse": 0.8686035190673437
}
[08/28/2025 06:07:26 INFO]: Training stats: {
    "score": -1.0039487433084975,
    "rmse": 1.0039487433084975
}
[08/28/2025 06:07:26 INFO]: Val stats: {
    "score": -0.6638824943013892,
    "rmse": 0.6638824943013892
}
[08/28/2025 06:07:26 INFO]: Test stats: {
    "score": -0.8777986926668393,
    "rmse": 0.8777986926668393
}
[08/28/2025 06:07:41 INFO]: Training loss at epoch 37: 0.9771035015583038
[08/28/2025 06:07:45 INFO]: Training loss at epoch 4: 0.9347427487373352
[08/28/2025 06:07:55 INFO]: Training loss at epoch 1: 1.0138275027275085
[08/28/2025 06:08:00 INFO]: Training loss at epoch 33: 0.9990242719650269
[08/28/2025 06:08:11 INFO]: Training loss at epoch 17: 1.0123983919620514
[08/28/2025 06:08:23 INFO]: Training loss at epoch 65: 0.9888995885848999
[08/28/2025 06:08:34 INFO]: Training loss at epoch 37: 1.1176839172840118
[08/28/2025 06:08:51 INFO]: Training loss at epoch 38: 0.8618704974651337
[08/28/2025 06:08:58 INFO]: Training loss at epoch 44: 0.9245366454124451
[08/28/2025 06:09:15 INFO]: Training loss at epoch 20: 1.1649431884288788
[08/28/2025 06:09:19 INFO]: Training loss at epoch 15: 1.2892853021621704
[08/28/2025 06:09:33 INFO]: Training loss at epoch 36: 1.0563896298408508
[08/28/2025 06:09:36 INFO]: Training loss at epoch 4: 1.7541995644569397
[08/28/2025 06:09:37 INFO]: New best epoch, val score: -0.664104340320743
[08/28/2025 06:09:37 INFO]: Saving model to: unhanged-Shanesha_trial_107/model_best.pth
[08/28/2025 06:09:54 INFO]: Training loss at epoch 34: 1.0633580088615417
[08/28/2025 06:10:00 INFO]: Training loss at epoch 18: 1.0869005918502808
[08/28/2025 06:10:06 INFO]: Training loss at epoch 52: 0.9587684571743011
[08/28/2025 06:10:09 INFO]: Training loss at epoch 93: 0.9495199024677277
[08/28/2025 06:10:14 INFO]: Training loss at epoch 5: 1.0947527885437012
[08/28/2025 06:10:43 INFO]: Training loss at epoch 45: 1.0480095744132996
[08/28/2025 06:11:02 INFO]: Training loss at epoch 21: 1.0910134315490723
[08/28/2025 06:11:38 INFO]: Training loss at epoch 30: 1.0989950597286224
[08/28/2025 06:11:49 INFO]: Training loss at epoch 16: 1.117984116077423
[08/28/2025 06:11:51 INFO]: Training loss at epoch 19: 1.092057466506958
[08/28/2025 06:11:52 INFO]: Training loss at epoch 35: 1.125012218952179
[08/28/2025 06:12:05 INFO]: Training loss at epoch 7: 1.164372205734253
[08/28/2025 06:12:08 INFO]: Training loss at epoch 5: 1.2645972967147827
[08/28/2025 06:12:13 INFO]: Training loss at epoch 5: 0.9904768168926239
[08/28/2025 06:12:23 INFO]: Training loss at epoch 40: 0.9458504319190979
[08/28/2025 06:12:32 INFO]: Training stats: {
    "score": -0.9956590019987388,
    "rmse": 0.9956590019987388
}
[08/28/2025 06:12:32 INFO]: Val stats: {
    "score": -0.663954967495305,
    "rmse": 0.663954967495305
}
[08/28/2025 06:12:32 INFO]: Test stats: {
    "score": -0.8669298910402851,
    "rmse": 0.8669298910402851
}
[08/28/2025 06:12:34 INFO]: Training loss at epoch 46: 0.8332291841506958
[08/28/2025 06:12:43 INFO]: Training loss at epoch 38: 1.0056422352790833
[08/28/2025 06:12:50 INFO]: Training loss at epoch 6: 0.9780193269252777
[08/28/2025 06:12:54 INFO]: Training loss at epoch 38: 0.9569888412952423
[08/28/2025 06:12:58 INFO]: Training loss at epoch 22: 1.0805491209030151
[08/28/2025 06:13:13 INFO]: Training loss at epoch 65: 0.9126084446907043
[08/28/2025 06:13:34 INFO]: Training loss at epoch 66: 0.9299077391624451
[08/28/2025 06:14:00 INFO]: Training loss at epoch 36: 1.1813339293003082
[08/28/2025 06:14:14 INFO]: Training loss at epoch 39: 1.0428813695907593
[08/28/2025 06:14:32 INFO]: Training loss at epoch 47: 0.9772303402423859
[08/28/2025 06:14:33 INFO]: Training loss at epoch 20: 1.3184453248977661
[08/28/2025 06:14:34 INFO]: Training loss at epoch 17: 1.2815458178520203
[08/28/2025 06:14:45 INFO]: Training loss at epoch 2: 1.202233910560608
[08/28/2025 06:14:55 INFO]: Training loss at epoch 6: 1.324346661567688
[08/28/2025 06:14:58 INFO]: Training loss at epoch 23: 0.9549935162067413
[08/28/2025 06:15:06 INFO]: Training loss at epoch 37: 1.0462384223937988
[08/28/2025 06:15:31 INFO]: Training loss at epoch 53: 0.9567124545574188
[08/28/2025 06:15:38 INFO]: Training loss at epoch 7: 1.0942332744598389
[08/28/2025 06:16:08 INFO]: Training loss at epoch 37: 0.9299698173999786
[08/28/2025 06:16:10 INFO]: Training stats: {
    "score": -1.0013643936316632,
    "rmse": 1.0013643936316632
}
[08/28/2025 06:16:10 INFO]: Val stats: {
    "score": -0.6670674608918351,
    "rmse": 0.6670674608918351
}
[08/28/2025 06:16:10 INFO]: Test stats: {
    "score": -0.8694371346208055,
    "rmse": 0.8694371346208055
}
[08/28/2025 06:16:28 INFO]: Training loss at epoch 48: 1.0168969929218292
[08/28/2025 06:16:32 INFO]: Training loss at epoch 21: 1.1974198818206787
[08/28/2025 06:16:53 INFO]: Training loss at epoch 24: 1.0167388916015625
[08/28/2025 06:17:04 INFO]: Training loss at epoch 31: 1.1816100478172302
[08/28/2025 06:17:10 INFO]: Training loss at epoch 94: 0.9606410562992096
[08/28/2025 06:17:13 INFO]: Training loss at epoch 18: 1.1268954277038574
[08/28/2025 06:17:27 INFO]: Training loss at epoch 39: 0.9559026062488556
[08/28/2025 06:17:32 INFO]: Training loss at epoch 7: 1.4140204191207886
[08/28/2025 06:17:40 INFO]: New best epoch, val score: -0.6601063284662727
[08/28/2025 06:17:40 INFO]: Saving model to: unhanged-Shanesha_trial_97/model_best.pth
[08/28/2025 06:17:42 INFO]: Training loss at epoch 41: 0.8232152760028839
[08/28/2025 06:17:57 INFO]: New best epoch, val score: -0.660715414897126
[08/28/2025 06:17:57 INFO]: Saving model to: unhanged-Shanesha_trial_43/model_best.pth
[08/28/2025 06:17:59 INFO]: Training loss at epoch 39: 1.0596001148223877
[08/28/2025 06:18:03 INFO]: Training loss at epoch 38: 0.9259772002696991
[08/28/2025 06:18:06 INFO]: Training loss at epoch 8: 1.1346181631088257
[08/28/2025 06:18:09 INFO]: Training loss at epoch 8: 0.8339217603206635
[08/28/2025 06:18:13 INFO]: Training loss at epoch 49: 0.7695825695991516
[08/28/2025 06:18:20 INFO]: Training loss at epoch 22: 1.0188671350479126
[08/28/2025 06:18:40 INFO]: Training loss at epoch 25: 0.8862962424755096
[08/28/2025 06:18:49 INFO]: Training stats: {
    "score": -0.9963188194162029,
    "rmse": 0.9963188194162029
}
[08/28/2025 06:18:49 INFO]: Val stats: {
    "score": -0.6735923950852751,
    "rmse": 0.6735923950852751
}
[08/28/2025 06:18:49 INFO]: Test stats: {
    "score": -0.8710159727261645,
    "rmse": 0.8710159727261645
}
[08/28/2025 06:18:49 INFO]: Training loss at epoch 67: 0.883591502904892
[08/28/2025 06:18:54 INFO]: Training stats: {
    "score": -1.0008752257111817,
    "rmse": 1.0008752257111817
}
[08/28/2025 06:18:54 INFO]: Val stats: {
    "score": -0.667801962269864,
    "rmse": 0.667801962269864
}
[08/28/2025 06:18:54 INFO]: Test stats: {
    "score": -0.8700192925396486,
    "rmse": 0.8700192925396486
}
[08/28/2025 06:19:23 INFO]: Training loss at epoch 66: 0.992300271987915
[08/28/2025 06:19:40 INFO]: Training stats: {
    "score": -1.0038412047031293,
    "rmse": 1.0038412047031293
}
[08/28/2025 06:19:40 INFO]: Val stats: {
    "score": -0.7090159692076454,
    "rmse": 0.7090159692076454
}
[08/28/2025 06:19:40 INFO]: Test stats: {
    "score": -0.8896180971811817,
    "rmse": 0.8896180971811817
}
[08/28/2025 06:19:41 INFO]: Training loss at epoch 19: 0.885560929775238
[08/28/2025 06:19:57 INFO]: Training loss at epoch 39: 0.9376019835472107
[08/28/2025 06:20:01 INFO]: Training loss at epoch 8: 1.117845207452774
[08/28/2025 06:20:01 INFO]: Training loss at epoch 6: 0.9886112213134766
[08/28/2025 06:20:08 INFO]: Training loss at epoch 23: 1.1623317897319794
[08/28/2025 06:20:26 INFO]: Training loss at epoch 38: 1.2029038667678833
[08/28/2025 06:20:30 INFO]: Training loss at epoch 26: 1.1541741788387299
[08/28/2025 06:20:34 INFO]: Training stats: {
    "score": -0.9990866275198717,
    "rmse": 0.9990866275198717
}
[08/28/2025 06:20:34 INFO]: Val stats: {
    "score": -0.6977777630305353,
    "rmse": 0.6977777630305353
}
[08/28/2025 06:20:34 INFO]: Test stats: {
    "score": -0.8845354254958729,
    "rmse": 0.8845354254958729
}
[08/28/2025 06:20:37 INFO]: Training loss at epoch 50: 1.4652405977249146
[08/28/2025 06:20:38 INFO]: Training stats: {
    "score": -0.9997179619443017,
    "rmse": 0.9997179619443017
}
[08/28/2025 06:20:38 INFO]: Val stats: {
    "score": -0.6720285760185237,
    "rmse": 0.6720285760185237
}
[08/28/2025 06:20:38 INFO]: Test stats: {
    "score": -0.8740918130579213,
    "rmse": 0.8740918130579213
}
[08/28/2025 06:20:38 INFO]: Training loss at epoch 54: 1.0851867198944092
[08/28/2025 06:20:41 INFO]: Training loss at epoch 9: 0.8676309287548065
[08/28/2025 06:20:52 INFO]: Running Final Evaluation...
[08/28/2025 06:21:25 INFO]: Training loss at epoch 40: 0.9087954461574554
[08/28/2025 06:21:31 INFO]: Training accuracy: {
    "score": -1.0104303353455828,
    "rmse": 1.0104303353455828
}
[08/28/2025 06:21:31 INFO]: Val accuracy: {
    "score": -0.6606925437718847,
    "rmse": 0.6606925437718847
}
[08/28/2025 06:21:31 INFO]: Test accuracy: {
    "score": -0.8696651328502769,
    "rmse": 0.8696651328502769
}
[08/28/2025 06:21:32 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_103",
    "best_epoch": 19,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8696651328502769,
        "rmse": 0.8696651328502769
    },
    "train_stats": {
        "score": -1.0104303353455828,
        "rmse": 1.0104303353455828
    },
    "val_stats": {
        "score": -0.6606925437718847,
        "rmse": 0.6606925437718847
    }
}
[08/28/2025 06:21:32 INFO]: Procewss finished for trial unhanged-Shanesha_trial_103
[08/28/2025 06:21:32 INFO]: 
_________________________________________________

[08/28/2025 06:21:32 INFO]: train_net_for_optune.py main() running.
[08/28/2025 06:21:32 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 2.622986933660781
  attention_dropout: 0.03163181431711072
  ffn_dropout: 0.03163181431711072
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.149412890891859e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_113

[08/28/2025 06:21:32 INFO]: This ft_transformer has 5.598 million parameters.
[08/28/2025 06:21:32 INFO]: Training will start at epoch 0.
[08/28/2025 06:21:32 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 06:21:33 INFO]: Training loss at epoch 3: 1.0176146626472473
[08/28/2025 06:21:38 INFO]: Training stats: {
    "score": -1.0112008777596257,
    "rmse": 1.0112008777596257
}
[08/28/2025 06:21:38 INFO]: Val stats: {
    "score": -0.7412216196556135,
    "rmse": 0.7412216196556135
}
[08/28/2025 06:21:38 INFO]: Test stats: {
    "score": -0.9087653762506396,
    "rmse": 0.9087653762506396
}
[08/28/2025 06:22:09 INFO]: Training loss at epoch 24: 1.0062483549118042
[08/28/2025 06:22:15 INFO]: Training loss at epoch 32: 0.9658225178718567
[08/28/2025 06:22:32 INFO]: Training loss at epoch 27: 1.0341842770576477
[08/28/2025 06:22:46 INFO]: Training loss at epoch 40: 0.8832724094390869
[08/28/2025 06:22:46 INFO]: Training loss at epoch 9: 1.1675522923469543
[08/28/2025 06:22:54 INFO]: Training loss at epoch 42: 1.1257085502147675
[08/28/2025 06:23:20 INFO]: Training loss at epoch 20: 0.9562970399856567
[08/28/2025 06:23:27 INFO]: Training loss at epoch 40: 1.0945051312446594
[08/28/2025 06:23:43 INFO]: Training stats: {
    "score": -1.11588988160735,
    "rmse": 1.11588988160735
}
[08/28/2025 06:23:43 INFO]: Val stats: {
    "score": -0.7290381253464837,
    "rmse": 0.7290381253464837
}
[08/28/2025 06:23:43 INFO]: Test stats: {
    "score": -0.9486783919827325,
    "rmse": 0.9486783919827325
}
[08/28/2025 06:24:03 INFO]: Training loss at epoch 9: 1.0907842516899109
[08/28/2025 06:24:03 INFO]: Training loss at epoch 0: 1.0402116775512695
[08/28/2025 06:24:07 INFO]: Training loss at epoch 95: 0.98293536901474
[08/28/2025 06:24:09 INFO]: Training loss at epoch 25: 0.8117400109767914
[08/28/2025 06:24:15 INFO]: Training loss at epoch 68: 1.0568649172782898
[08/28/2025 06:24:24 INFO]: New best epoch, val score: -0.673059538123189
[08/28/2025 06:24:24 INFO]: Saving model to: unhanged-Shanesha_trial_113/model_best.pth
[08/28/2025 06:24:24 INFO]: Training loss at epoch 10: 1.0468515157699585
[08/28/2025 06:24:33 INFO]: Training loss at epoch 28: 1.018273651599884
[08/28/2025 06:24:51 INFO]: Training loss at epoch 41: 1.1978148818016052
[08/28/2025 06:25:00 INFO]: Training loss at epoch 40: 1.1543331742286682
[08/28/2025 06:25:42 INFO]: Training loss at epoch 67: 1.2929611802101135
[08/28/2025 06:25:54 INFO]: Training loss at epoch 21: 0.9095686972141266
[08/28/2025 06:25:58 INFO]: Training loss at epoch 26: 1.0061388909816742
[08/28/2025 06:25:59 INFO]: Training stats: {
    "score": -1.0200112597211008,
    "rmse": 1.0200112597211008
}
[08/28/2025 06:25:59 INFO]: Val stats: {
    "score": -0.7578486638309524,
    "rmse": 0.7578486638309524
}
[08/28/2025 06:25:59 INFO]: Test stats: {
    "score": -0.921039491735944,
    "rmse": 0.921039491735944
}
[08/28/2025 06:26:00 INFO]: Training loss at epoch 39: 1.0196989178657532
[08/28/2025 06:26:03 INFO]: Training loss at epoch 55: 0.9272650480270386
[08/28/2025 06:26:16 INFO]: Training loss at epoch 10: 1.0929244756698608
[08/28/2025 06:26:21 INFO]: Training loss at epoch 29: 0.8198940753936768
[08/28/2025 06:26:34 INFO]: New best epoch, val score: -0.677191559580367
[08/28/2025 06:26:34 INFO]: Saving model to: unhanged-Shanesha_trial_112/model_best.pth
[08/28/2025 06:26:40 INFO]: Training loss at epoch 1: 0.9464396834373474
[08/28/2025 06:26:45 INFO]: Training loss at epoch 42: 0.8975027799606323
[08/28/2025 06:26:53 INFO]: Training loss at epoch 11: 1.1237373352050781
[08/28/2025 06:26:53 INFO]: Training loss at epoch 41: 0.9375841617584229
[08/28/2025 06:26:58 INFO]: Training stats: {
    "score": -0.9991270488785978,
    "rmse": 0.9991270488785978
}
[08/28/2025 06:26:58 INFO]: Val stats: {
    "score": -0.687698463705995,
    "rmse": 0.687698463705995
}
[08/28/2025 06:26:58 INFO]: Test stats: {
    "score": -0.8861219728044186,
    "rmse": 0.8861219728044186
}
[08/28/2025 06:27:26 INFO]: Training loss at epoch 33: 0.8953331112861633
[08/28/2025 06:27:45 INFO]: Training loss at epoch 41: 0.8921872675418854
[08/28/2025 06:27:46 INFO]: Training stats: {
    "score": -1.0007773073640065,
    "rmse": 1.0007773073640065
}
[08/28/2025 06:27:46 INFO]: Val stats: {
    "score": -0.6707945204530221,
    "rmse": 0.6707945204530221
}
[08/28/2025 06:27:46 INFO]: Test stats: {
    "score": -0.8701577811647597,
    "rmse": 0.8701577811647597
}
[08/28/2025 06:27:46 INFO]: Training loss at epoch 27: 0.9238372445106506
[08/28/2025 06:27:51 INFO]: Training loss at epoch 7: 1.0984290838241577
[08/28/2025 06:27:58 INFO]: Training loss at epoch 43: 0.9297460615634918
[08/28/2025 06:28:21 INFO]: Training loss at epoch 4: 1.0130006968975067
[08/28/2025 06:28:22 INFO]: Training loss at epoch 22: 0.9011184871196747
[08/28/2025 06:28:42 INFO]: Training loss at epoch 43: 1.032634437084198
[08/28/2025 06:28:48 INFO]: Training loss at epoch 30: 0.94624924659729
[08/28/2025 06:28:49 INFO]: Training loss at epoch 11: 1.142944872379303
[08/28/2025 06:29:09 INFO]: New best epoch, val score: -0.6563641036753545
[08/28/2025 06:29:09 INFO]: Saving model to: unhanged-Shanesha_trial_112/model_best.pth
[08/28/2025 06:29:21 INFO]: Training loss at epoch 69: 1.070081651210785
[08/28/2025 06:29:21 INFO]: Training loss at epoch 2: 1.020634651184082
[08/28/2025 06:29:29 INFO]: Training loss at epoch 12: 1.0448291301727295
[08/28/2025 06:29:41 INFO]: Training loss at epoch 28: 1.1154136657714844
[08/28/2025 06:30:05 INFO]: Training loss at epoch 41: 0.9183951318264008
[08/28/2025 06:30:48 INFO]: Training loss at epoch 31: 0.9716190993785858
[08/28/2025 06:30:49 INFO]: Training loss at epoch 44: 1.1410276591777802
[08/28/2025 06:30:55 INFO]: Training loss at epoch 96: 1.1071286797523499
[08/28/2025 06:31:08 INFO]: Training loss at epoch 23: 1.068413496017456
[08/28/2025 06:31:14 INFO]: Training stats: {
    "score": -1.010002214049138,
    "rmse": 1.010002214049138
}
[08/28/2025 06:31:14 INFO]: Val stats: {
    "score": -0.6617713768185322,
    "rmse": 0.6617713768185322
}
[08/28/2025 06:31:14 INFO]: Test stats: {
    "score": -0.870936786489892,
    "rmse": 0.870936786489892
}
[08/28/2025 06:31:24 INFO]: Training loss at epoch 56: 1.0348431468009949
[08/28/2025 06:31:36 INFO]: Training loss at epoch 12: 0.9039202928543091
[08/28/2025 06:31:42 INFO]: Training loss at epoch 29: 0.9875130653381348
[08/28/2025 06:31:54 INFO]: Training loss at epoch 10: 0.9433680474758148
[08/28/2025 06:31:57 INFO]: Training loss at epoch 68: 1.1510757207870483
[08/28/2025 06:32:15 INFO]: Training loss at epoch 13: 0.8444886803627014
[08/28/2025 06:32:15 INFO]: Training loss at epoch 3: 1.061049997806549
[08/28/2025 06:32:21 INFO]: Training loss at epoch 42: 0.9911670386791229
[08/28/2025 06:32:23 INFO]: Training stats: {
    "score": -1.0079889117257916,
    "rmse": 1.0079889117257916
}
[08/28/2025 06:32:23 INFO]: Val stats: {
    "score": -0.7462140285537954,
    "rmse": 0.7462140285537954
}
[08/28/2025 06:32:23 INFO]: Test stats: {
    "score": -0.9128320109592163,
    "rmse": 0.9128320109592163
}
[08/28/2025 06:32:25 INFO]: Training loss at epoch 42: 1.0613003373146057
[08/28/2025 06:32:36 INFO]: New best epoch, val score: -0.6642257447088147
[08/28/2025 06:32:36 INFO]: Saving model to: unhanged-Shanesha_trial_113/model_best.pth
[08/28/2025 06:32:38 INFO]: New best epoch, val score: -0.6709228097801115
[08/28/2025 06:32:38 INFO]: Saving model to: unhanged-Shanesha_trial_105/model_best.pth
[08/28/2025 06:32:46 INFO]: Training loss at epoch 32: 0.9837431907653809
[08/28/2025 06:32:49 INFO]: Training loss at epoch 34: 1.0518103539943695
[08/28/2025 06:32:55 INFO]: Training loss at epoch 45: 1.0362819731235504
[08/28/2025 06:32:59 INFO]: Running Final Evaluation...
[08/28/2025 06:33:18 INFO]: Training loss at epoch 44: 0.84217768907547
[08/28/2025 06:33:22 INFO]: Training loss at epoch 40: 0.9625392556190491
[08/28/2025 06:33:38 INFO]: Training accuracy: {
    "score": -1.019636328223914,
    "rmse": 1.019636328223914
}
[08/28/2025 06:33:38 INFO]: Val accuracy: {
    "score": -0.6625308745643254,
    "rmse": 0.6625308745643254
}
[08/28/2025 06:33:38 INFO]: Test accuracy: {
    "score": -0.8866448409389818,
    "rmse": 0.8866448409389818
}
[08/28/2025 06:33:38 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_108",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8866448409389818,
        "rmse": 0.8866448409389818
    },
    "train_stats": {
        "score": -1.019636328223914,
        "rmse": 1.019636328223914
    },
    "val_stats": {
        "score": -0.6625308745643254,
        "rmse": 0.6625308745643254
    }
}
[08/28/2025 06:33:38 INFO]: Procewss finished for trial unhanged-Shanesha_trial_108
[08/28/2025 06:33:38 INFO]: 
_________________________________________________

[08/28/2025 06:33:38 INFO]: train_net_for_optune.py main() running.
[08/28/2025 06:33:38 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 2.4903732782669543
  attention_dropout: 0.2726356693755142
  ffn_dropout: 0.2726356693755142
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 6.355815885265403e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_114

[08/28/2025 06:33:38 INFO]: This ft_transformer has 2.784 million parameters.
[08/28/2025 06:33:38 INFO]: Training will start at epoch 0.
[08/28/2025 06:33:38 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 06:33:48 INFO]: Training loss at epoch 24: 1.1461411118507385
[08/28/2025 06:34:14 INFO]: Training loss at epoch 13: 0.9560287594795227
[08/28/2025 06:34:16 INFO]: Training loss at epoch 30: 1.0867271721363068
[08/28/2025 06:34:27 INFO]: Training loss at epoch 0: 1.1226321458816528
[08/28/2025 06:34:34 INFO]: New best epoch, val score: -0.6850485706020505
[08/28/2025 06:34:34 INFO]: Saving model to: unhanged-Shanesha_trial_114/model_best.pth
[08/28/2025 06:34:49 INFO]: Training loss at epoch 14: 1.044091820716858
[08/28/2025 06:34:52 INFO]: Training loss at epoch 46: 1.0206337571144104
[08/28/2025 06:34:54 INFO]: Training loss at epoch 4: 1.052447497844696
[08/28/2025 06:35:17 INFO]: Training loss at epoch 42: 0.966972827911377
[08/28/2025 06:35:21 INFO]: Training loss at epoch 5: 0.8778396546840668
[08/28/2025 06:35:23 INFO]: Training loss at epoch 1: 0.904265433549881
[08/28/2025 06:35:44 INFO]: Training loss at epoch 8: 0.9631026685237885
[08/28/2025 06:36:04 INFO]: Training loss at epoch 31: 0.9314764142036438
[08/28/2025 06:36:15 INFO]: Training loss at epoch 25: 1.0809333622455597
[08/28/2025 06:36:19 INFO]: Training loss at epoch 2: 1.1268274188041687
[08/28/2025 06:36:24 INFO]: Training loss at epoch 70: 0.9191659688949585
[08/28/2025 06:36:25 INFO]: New best epoch, val score: -0.671574356795457
[08/28/2025 06:36:25 INFO]: Saving model to: unhanged-Shanesha_trial_114/model_best.pth
[08/28/2025 06:36:36 INFO]: Training loss at epoch 57: 1.057625412940979
[08/28/2025 06:36:38 INFO]: Training loss at epoch 43: 1.1249361634254456
[08/28/2025 06:36:43 INFO]: Training loss at epoch 14: 1.031393438577652
[08/28/2025 06:36:48 INFO]: Training loss at epoch 47: 0.9821840822696686
[08/28/2025 06:37:19 INFO]: Training loss at epoch 3: 1.2599120736122131
[08/28/2025 06:37:22 INFO]: Training loss at epoch 15: 1.2734946012496948
[08/28/2025 06:37:33 INFO]: Training loss at epoch 5: 0.9532045125961304
[08/28/2025 06:37:41 INFO]: Training loss at epoch 11: 0.9858837425708771
[08/28/2025 06:37:43 INFO]: Training loss at epoch 43: 1.050262212753296
[08/28/2025 06:37:47 INFO]: Training loss at epoch 97: 0.847269207239151
[08/28/2025 06:37:57 INFO]: Training loss at epoch 35: 0.7781301140785217
[08/28/2025 06:38:00 INFO]: Training loss at epoch 32: 1.0141722559928894
[08/28/2025 06:38:05 INFO]: Training loss at epoch 69: 0.9964861273765564
[08/28/2025 06:38:15 INFO]: Running Final Evaluation...
[08/28/2025 06:38:21 INFO]: Training loss at epoch 4: 1.1314640641212463
[08/28/2025 06:38:25 INFO]: New best epoch, val score: -0.6645150255398411
[08/28/2025 06:38:25 INFO]: Saving model to: unhanged-Shanesha_trial_105/model_best.pth
[08/28/2025 06:38:26 INFO]: Training loss at epoch 45: 1.0250604152679443
[08/28/2025 06:38:48 INFO]: Training loss at epoch 41: 1.1781908571720123
[08/28/2025 06:38:54 INFO]: Training loss at epoch 48: 1.1656345129013062
[08/28/2025 06:38:57 INFO]: Training accuracy: {
    "score": -1.0005800701727194,
    "rmse": 1.0005800701727194
}
[08/28/2025 06:38:57 INFO]: Val accuracy: {
    "score": -0.6578215679598707,
    "rmse": 0.6578215679598707
}
[08/28/2025 06:38:57 INFO]: Test accuracy: {
    "score": -0.8669466502834595,
    "rmse": 0.8669466502834595
}
[08/28/2025 06:38:57 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_109",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8669466502834595,
        "rmse": 0.8669466502834595
    },
    "train_stats": {
        "score": -1.0005800701727194,
        "rmse": 1.0005800701727194
    },
    "val_stats": {
        "score": -0.6578215679598707,
        "rmse": 0.6578215679598707
    }
}
[08/28/2025 06:38:57 INFO]: Procewss finished for trial unhanged-Shanesha_trial_109
[08/28/2025 06:38:57 INFO]: 
_________________________________________________

[08/28/2025 06:38:57 INFO]: train_net_for_optune.py main() running.
[08/28/2025 06:38:57 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 2.450348878428065
  attention_dropout: 0.20514856515331348
  ffn_dropout: 0.20514856515331348
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.6984671967569443e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_115

[08/28/2025 06:38:57 INFO]: This ft_transformer has 5.392 million parameters.
[08/28/2025 06:38:57 INFO]: Training will start at epoch 0.
[08/28/2025 06:38:57 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 06:38:58 INFO]: Training loss at epoch 26: 1.0678752064704895
[08/28/2025 06:39:24 INFO]: Training loss at epoch 5: 1.1444211602210999
[08/28/2025 06:39:30 INFO]: Training loss at epoch 15: 0.9601611495018005
[08/28/2025 06:40:10 INFO]: Training loss at epoch 16: 1.0536137223243713
[08/28/2025 06:40:18 INFO]: Training stats: {
    "score": -1.001760712085293,
    "rmse": 1.001760712085293
}
[08/28/2025 06:40:18 INFO]: Val stats: {
    "score": -0.6681780101673924,
    "rmse": 0.6681780101673924
}
[08/28/2025 06:40:18 INFO]: Test stats: {
    "score": -0.869262919501677,
    "rmse": 0.869262919501677
}
[08/28/2025 06:40:25 INFO]: Training loss at epoch 6: 1.0172024369239807
[08/28/2025 06:40:26 INFO]: Training loss at epoch 6: 0.9516032636165619
[08/28/2025 06:40:36 INFO]: Training loss at epoch 43: 1.003438115119934
[08/28/2025 06:41:02 INFO]: Training loss at epoch 49: 1.0592605471611023
[08/28/2025 06:41:19 INFO]: Training loss at epoch 44: 0.891647070646286
[08/28/2025 06:41:24 INFO]: Training loss at epoch 0: 1.0722589492797852
[08/28/2025 06:41:29 INFO]: Training loss at epoch 7: 0.9889222681522369
[08/28/2025 06:41:43 INFO]: Training loss at epoch 27: 1.2329269051551819
[08/28/2025 06:41:43 INFO]: New best epoch, val score: -0.9240352806017328
[08/28/2025 06:41:43 INFO]: Saving model to: unhanged-Shanesha_trial_115/model_best.pth
[08/28/2025 06:41:45 INFO]: Training stats: {
    "score": -0.9948829586261949,
    "rmse": 0.9948829586261949
}
[08/28/2025 06:41:45 INFO]: Val stats: {
    "score": -0.697260772530385,
    "rmse": 0.697260772530385
}
[08/28/2025 06:41:45 INFO]: Test stats: {
    "score": -0.8834004429218693,
    "rmse": 0.8834004429218693
}
[08/28/2025 06:41:55 INFO]: Training loss at epoch 71: 1.1716807782649994
[08/28/2025 06:42:10 INFO]: Training loss at epoch 58: 1.0432007908821106
[08/28/2025 06:42:12 INFO]: Training loss at epoch 16: 0.8888217806816101
[08/28/2025 06:42:22 INFO]: Training loss at epoch 6: 1.168593168258667
[08/28/2025 06:42:26 INFO]: Training loss at epoch 8: 1.0263174772262573
[08/28/2025 06:42:48 INFO]: Training loss at epoch 17: 0.933643251657486
[08/28/2025 06:43:07 INFO]: Training loss at epoch 7: 1.0509918630123138
[08/28/2025 06:43:14 INFO]: Training loss at epoch 44: 1.1905386447906494
[08/28/2025 06:43:18 INFO]: Training loss at epoch 36: 0.9803701639175415
[08/28/2025 06:43:21 INFO]: Training loss at epoch 9: 0.9296850264072418
[08/28/2025 06:43:38 INFO]: Training loss at epoch 9: 0.9329819679260254
[08/28/2025 06:43:39 INFO]: Training loss at epoch 50: 0.9896796941757202
[08/28/2025 06:43:39 INFO]: Training loss at epoch 12: 1.1821447014808655
[08/28/2025 06:43:40 INFO]: Training stats: {
    "score": -1.0066604185237298,
    "rmse": 1.0066604185237298
}
[08/28/2025 06:43:40 INFO]: Val stats: {
    "score": -0.7467116608870144,
    "rmse": 0.7467116608870144
}
[08/28/2025 06:43:40 INFO]: Test stats: {
    "score": -0.9104041440453836,
    "rmse": 0.9104041440453836
}
[08/28/2025 06:43:46 INFO]: Training loss at epoch 46: 1.0173676013946533
[08/28/2025 06:43:55 INFO]: Training loss at epoch 1: 0.9600979387760162
[08/28/2025 06:44:11 INFO]: Training loss at epoch 28: 0.9249797463417053
[08/28/2025 06:44:12 INFO]: New best epoch, val score: -0.6574640191703006
[08/28/2025 06:44:12 INFO]: Saving model to: unhanged-Shanesha_trial_115/model_best.pth
[08/28/2025 06:44:20 INFO]: Training loss at epoch 42: 1.086891233921051
[08/28/2025 06:44:36 INFO]: Training loss at epoch 10: 1.1850753426551819
[08/28/2025 06:44:41 INFO]: Training loss at epoch 17: 1.3282203078269958
[08/28/2025 06:44:43 INFO]: Training loss at epoch 98: 1.1777089834213257
[08/28/2025 06:45:17 INFO]: Training loss at epoch 18: 0.9936065077781677
[08/28/2025 06:45:31 INFO]: Training loss at epoch 11: 0.8650780022144318
[08/28/2025 06:45:34 INFO]: Training loss at epoch 51: 1.0276912450790405
[08/28/2025 06:45:37 INFO]: Training loss at epoch 45: 0.8471127450466156
[08/28/2025 06:45:39 INFO]: Training loss at epoch 44: 0.8334199786186218
[08/28/2025 06:45:40 INFO]: Training loss at epoch 8: 1.2010720372200012
[08/28/2025 06:46:10 INFO]: Training stats: {
    "score": -0.9988215171431923,
    "rmse": 0.9988215171431923
}
[08/28/2025 06:46:10 INFO]: Val stats: {
    "score": -0.6677754176561237,
    "rmse": 0.6677754176561237
}
[08/28/2025 06:46:10 INFO]: Test stats: {
    "score": -0.8732607092122961,
    "rmse": 0.8732607092122961
}
[08/28/2025 06:46:22 INFO]: Training loss at epoch 70: 1.0950111150741577
[08/28/2025 06:46:29 INFO]: Training loss at epoch 2: 1.128190815448761
[08/28/2025 06:46:32 INFO]: Training loss at epoch 12: 1.0662476420402527
[08/28/2025 06:46:46 INFO]: Training loss at epoch 29: 1.0985003411769867
[08/28/2025 06:47:02 INFO]: Training loss at epoch 72: 1.190679281949997
[08/28/2025 06:47:20 INFO]: Training loss at epoch 18: 0.9551824629306793
[08/28/2025 06:47:22 INFO]: Training loss at epoch 59: 0.9918622970581055
[08/28/2025 06:47:35 INFO]: Training loss at epoch 13: 1.1346197128295898
[08/28/2025 06:47:41 INFO]: Training loss at epoch 52: 0.865715354681015
[08/28/2025 06:47:42 INFO]: Training stats: {
    "score": -0.9962163690184469,
    "rmse": 0.9962163690184469
}
[08/28/2025 06:47:42 INFO]: Val stats: {
    "score": -0.6852466114470779,
    "rmse": 0.6852466114470779
}
[08/28/2025 06:47:42 INFO]: Test stats: {
    "score": -0.8776353856347904,
    "rmse": 0.8776353856347904
}
[08/28/2025 06:48:01 INFO]: Training loss at epoch 19: 0.9028528332710266
[08/28/2025 06:48:31 INFO]: Training loss at epoch 9: 0.9794026613235474
[08/28/2025 06:48:32 INFO]: Training loss at epoch 37: 1.03386789560318
[08/28/2025 06:48:37 INFO]: Training loss at epoch 14: 1.1720058917999268
[08/28/2025 06:48:38 INFO]: Training loss at epoch 45: 1.0995927155017853
[08/28/2025 06:48:57 INFO]: Training stats: {
    "score": -1.000146828262302,
    "rmse": 1.000146828262302
}
[08/28/2025 06:48:57 INFO]: Val stats: {
    "score": -0.7155752821648125,
    "rmse": 0.7155752821648125
}
[08/28/2025 06:48:57 INFO]: Test stats: {
    "score": -0.8918388061657774,
    "rmse": 0.8918388061657774
}
[08/28/2025 06:49:03 INFO]: Training loss at epoch 47: 0.9577164649963379
[08/28/2025 06:49:15 INFO]: Training loss at epoch 7: 0.8892209529876709
[08/28/2025 06:49:15 INFO]: Training loss at epoch 3: 1.05354043841362
[08/28/2025 06:49:21 INFO]: Training stats: {
    "score": -1.005102077469463,
    "rmse": 1.005102077469463
}
[08/28/2025 06:49:21 INFO]: Val stats: {
    "score": -0.6627414755498551,
    "rmse": 0.6627414755498551
}
[08/28/2025 06:49:21 INFO]: Test stats: {
    "score": -0.8695787573776363,
    "rmse": 0.8695787573776363
}
[08/28/2025 06:49:29 INFO]: Training stats: {
    "score": -1.033050525401113,
    "rmse": 1.033050525401113
}
[08/28/2025 06:49:29 INFO]: Val stats: {
    "score": -0.6700485867109442,
    "rmse": 0.6700485867109442
}
[08/28/2025 06:49:29 INFO]: Test stats: {
    "score": -0.8824143566741992,
    "rmse": 0.8824143566741992
}
[08/28/2025 06:49:37 INFO]: Training loss at epoch 13: 1.076699674129486
[08/28/2025 06:49:40 INFO]: Training loss at epoch 15: 1.2311363518238068
[08/28/2025 06:49:49 INFO]: Training loss at epoch 53: 0.8572924733161926
[08/28/2025 06:49:57 INFO]: Training loss at epoch 43: 1.1525738537311554
[08/28/2025 06:50:08 INFO]: Training loss at epoch 19: 1.249153196811676
[08/28/2025 06:50:18 INFO]: Training loss at epoch 46: 1.0543655753135681
[08/28/2025 06:50:28 INFO]: Training loss at epoch 30: 0.883837878704071
[08/28/2025 06:50:42 INFO]: Training loss at epoch 16: 1.1051506400108337
[08/28/2025 06:51:04 INFO]: Training stats: {
    "score": -1.0169193129275245,
    "rmse": 1.0169193129275245
}
[08/28/2025 06:51:04 INFO]: Val stats: {
    "score": -0.7402961339292871,
    "rmse": 0.7402961339292871
}
[08/28/2025 06:51:04 INFO]: Test stats: {
    "score": -0.9083087165367865,
    "rmse": 0.9083087165367865
}
[08/28/2025 06:51:10 INFO]: Training loss at epoch 45: 1.1718129515647888
[08/28/2025 06:51:42 INFO]: Training loss at epoch 20: 0.8563416600227356
[08/28/2025 06:51:43 INFO]: Training loss at epoch 17: 1.1996876001358032
[08/28/2025 06:51:51 INFO]: Training loss at epoch 99: 1.000427007675171
[08/28/2025 06:51:54 INFO]: Training loss at epoch 54: 1.1745015978813171
[08/28/2025 06:51:58 INFO]: Training loss at epoch 4: 1.0202048420906067
[08/28/2025 06:52:15 INFO]: Training loss at epoch 10: 0.9328384101390839
[08/28/2025 06:52:30 INFO]: Training loss at epoch 73: 1.1245121657848358
[08/28/2025 06:52:39 INFO]: Training loss at epoch 18: 0.9673789739608765
[08/28/2025 06:52:47 INFO]: Training loss at epoch 71: 1.120731681585312
[08/28/2025 06:53:02 INFO]: Training loss at epoch 31: 1.0412089824676514
[08/28/2025 06:53:35 INFO]: Training loss at epoch 19: 1.0806897282600403
[08/28/2025 06:53:35 INFO]: Training loss at epoch 20: 0.9796765446662903
[08/28/2025 06:53:48 INFO]: Training loss at epoch 38: 0.841025173664093
[08/28/2025 06:53:48 INFO]: Training loss at epoch 55: 0.9190537631511688
[08/28/2025 06:53:53 INFO]: Training stats: {
    "score": -1.0219583620150166,
    "rmse": 1.0219583620150166
}
[08/28/2025 06:53:53 INFO]: Val stats: {
    "score": -0.78532884083975,
    "rmse": 0.78532884083975
}
[08/28/2025 06:53:53 INFO]: Test stats: {
    "score": -0.9381100602360525,
    "rmse": 0.9381100602360525
}
[08/28/2025 06:54:02 INFO]: Training loss at epoch 46: 0.9673515856266022
[08/28/2025 06:54:04 INFO]: Training stats: {
    "score": -0.9974021663558337,
    "rmse": 0.9974021663558337
}
[08/28/2025 06:54:04 INFO]: Val stats: {
    "score": -0.6835886230243118,
    "rmse": 0.6835886230243118
}
[08/28/2025 06:54:04 INFO]: Test stats: {
    "score": -0.8751157205992026,
    "rmse": 0.8751157205992026
}
[08/28/2025 06:54:07 INFO]: Training loss at epoch 10: 1.0669607520103455
[08/28/2025 06:54:11 INFO]: Training loss at epoch 21: 0.9654093384742737
[08/28/2025 06:54:14 INFO]: Training loss at epoch 48: 0.9952358901500702
[08/28/2025 06:54:26 INFO]: Training loss at epoch 5: 0.8249602317810059
[08/28/2025 06:54:28 INFO]: New best epoch, val score: -0.6866635492040337
[08/28/2025 06:54:28 INFO]: Saving model to: unhanged-Shanesha_trial_111/model_best.pth
[08/28/2025 06:54:35 INFO]: Training loss at epoch 60: 0.9654386639595032
[08/28/2025 06:54:38 INFO]: Training loss at epoch 47: 1.0109368562698364
[08/28/2025 06:54:48 INFO]: Training loss at epoch 11: 0.9604030251502991
[08/28/2025 06:54:49 INFO]: Training loss at epoch 20: 0.778477743268013
[08/28/2025 06:55:19 INFO]: Training loss at epoch 44: 0.9454270601272583
[08/28/2025 06:55:24 INFO]: Training loss at epoch 14: 1.070637047290802
[08/28/2025 06:55:33 INFO]: Training loss at epoch 32: 1.0157047510147095
[08/28/2025 06:55:47 INFO]: Training loss at epoch 56: 1.081143081188202
[08/28/2025 06:55:50 INFO]: Training loss at epoch 21: 1.0043978095054626
[08/28/2025 06:56:02 INFO]: Running Final Evaluation...
[08/28/2025 06:56:04 INFO]: Training loss at epoch 8: 1.022115170955658
[08/28/2025 06:56:07 INFO]: New best epoch, val score: -0.6628071495892596
[08/28/2025 06:56:07 INFO]: Saving model to: unhanged-Shanesha_trial_105/model_best.pth
[08/28/2025 06:56:11 INFO]: Training loss at epoch 21: 1.315503090620041
[08/28/2025 06:56:12 INFO]: Training loss at epoch 46: 1.0776284039020538
[08/28/2025 06:56:46 INFO]: Training accuracy: {
    "score": -1.0118315904110864,
    "rmse": 1.0118315904110864
}
[08/28/2025 06:56:46 INFO]: Val accuracy: {
    "score": -0.6673905695974665,
    "rmse": 0.6673905695974665
}
[08/28/2025 06:56:46 INFO]: Test accuracy: {
    "score": -0.8761876519976558,
    "rmse": 0.8761876519976558
}
[08/28/2025 06:56:46 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_104",
    "best_epoch": 25,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8761876519976558,
        "rmse": 0.8761876519976558
    },
    "train_stats": {
        "score": -1.0118315904110864,
        "rmse": 1.0118315904110864
    },
    "val_stats": {
        "score": -0.6673905695974665,
        "rmse": 0.6673905695974665
    }
}
[08/28/2025 06:56:46 INFO]: Procewss finished for trial unhanged-Shanesha_trial_104
[08/28/2025 06:56:46 INFO]: 
_________________________________________________

[08/28/2025 06:56:46 INFO]: train_net_for_optune.py main() running.
[08/28/2025 06:56:46 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.4460885471431595
  attention_dropout: 0.19881603945066678
  ffn_dropout: 0.19881603945066678
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.3475016287949184e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_116

[08/28/2025 06:56:46 INFO]: This ft_transformer has 2.168 million parameters.
[08/28/2025 06:56:46 INFO]: Training will start at epoch 0.
[08/28/2025 06:56:46 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 06:56:52 INFO]: Training loss at epoch 22: 1.1077801585197449
[08/28/2025 06:56:53 INFO]: Training loss at epoch 22: 1.0588704943656921
[08/28/2025 06:57:07 INFO]: Training loss at epoch 6: 0.9622022211551666
[08/28/2025 06:57:33 INFO]: Training loss at epoch 0: 1.3710004091262817
[08/28/2025 06:57:37 INFO]: Training loss at epoch 12: 0.8639385104179382
[08/28/2025 06:57:39 INFO]: New best epoch, val score: -0.8220656468812354
[08/28/2025 06:57:39 INFO]: Saving model to: unhanged-Shanesha_trial_116/model_best.pth
[08/28/2025 06:57:44 INFO]: Training loss at epoch 74: 0.8540589213371277
[08/28/2025 06:57:56 INFO]: Training loss at epoch 23: 1.101513147354126
[08/28/2025 06:58:03 INFO]: New best epoch, val score: -0.6701445544779334
[08/28/2025 06:58:03 INFO]: Saving model to: unhanged-Shanesha_trial_114/model_best.pth
[08/28/2025 06:58:19 INFO]: Training loss at epoch 33: 0.9423449635505676
[08/28/2025 06:58:27 INFO]: Training loss at epoch 1: 1.3555039167404175
[08/28/2025 06:58:33 INFO]: New best epoch, val score: -0.6706343407771626
[08/28/2025 06:58:33 INFO]: Saving model to: unhanged-Shanesha_trial_116/model_best.pth
[08/28/2025 06:58:57 INFO]: Training loss at epoch 22: 0.9126083850860596
[08/28/2025 06:58:58 INFO]: Training loss at epoch 24: 1.1592882871627808
[08/28/2025 06:59:04 INFO]: Training loss at epoch 72: 0.9339882135391235
[08/28/2025 06:59:09 INFO]: Training loss at epoch 39: 0.9034497737884521
[08/28/2025 06:59:15 INFO]: Training loss at epoch 48: 0.9048121273517609
[08/28/2025 06:59:20 INFO]: Training loss at epoch 2: 1.1472434401512146
[08/28/2025 06:59:37 INFO]: Training loss at epoch 49: 1.216998964548111
[08/28/2025 06:59:39 INFO]: Training loss at epoch 47: 0.9188534915447235
[08/28/2025 06:59:41 INFO]: Training loss at epoch 23: 1.0081287920475006
[08/28/2025 06:59:53 INFO]: Training loss at epoch 7: 1.0051621794700623
[08/28/2025 07:00:01 INFO]: Training loss at epoch 25: 1.130268394947052
[08/28/2025 07:00:07 INFO]: Training loss at epoch 61: 1.1180761456489563
[08/28/2025 07:00:13 INFO]: Training loss at epoch 3: 1.049607515335083
[08/28/2025 07:00:26 INFO]: Training loss at epoch 13: 1.1408252120018005
[08/28/2025 07:00:57 INFO]: Training stats: {
    "score": -0.995871064979272,
    "rmse": 0.995871064979272
}
[08/28/2025 07:00:57 INFO]: Val stats: {
    "score": -0.6846681740280843,
    "rmse": 0.6846681740280843
}
[08/28/2025 07:00:57 INFO]: Test stats: {
    "score": -0.8779246424281381,
    "rmse": 0.8779246424281381
}
[08/28/2025 07:00:57 INFO]: Training loss at epoch 26: 0.8886091411113739
[08/28/2025 07:00:58 INFO]: Training loss at epoch 45: 0.967973530292511
[08/28/2025 07:00:58 INFO]: Training loss at epoch 34: 0.9900126159191132
[08/28/2025 07:01:00 INFO]: Training loss at epoch 4: 1.227054476737976
[08/28/2025 07:01:07 INFO]: Training loss at epoch 100: 0.8282530307769775
[08/28/2025 07:01:20 INFO]: Training stats: {
    "score": -1.0049206848181425,
    "rmse": 1.0049206848181425
}
[08/28/2025 07:01:20 INFO]: Val stats: {
    "score": -0.6635009393734638,
    "rmse": 0.6635009393734638
}
[08/28/2025 07:01:20 INFO]: Test stats: {
    "score": -0.8685371921578182,
    "rmse": 0.8685371921578182
}
[08/28/2025 07:01:26 INFO]: Training loss at epoch 15: 1.1167256236076355
[08/28/2025 07:01:31 INFO]: Training loss at epoch 47: 0.8865590691566467
[08/28/2025 07:01:34 INFO]: Training loss at epoch 23: 1.1311811804771423
[08/28/2025 07:01:48 INFO]: Training loss at epoch 5: 1.2138299942016602
[08/28/2025 07:01:53 INFO]: Training loss at epoch 27: 0.9712117612361908
[08/28/2025 07:02:01 INFO]: Training loss at epoch 11: 0.8850780725479126
[08/28/2025 07:02:13 INFO]: Training loss at epoch 24: 1.0001240670681
[08/28/2025 07:02:24 INFO]: Training loss at epoch 8: 0.9919694066047668
[08/28/2025 07:02:36 INFO]: Training loss at epoch 6: 1.1611413359642029
[08/28/2025 07:02:48 INFO]: Training loss at epoch 28: 1.016289860010147
[08/28/2025 07:02:59 INFO]: Training loss at epoch 75: 1.038353979587555
[08/28/2025 07:02:59 INFO]: Training loss at epoch 14: 1.204580843448639
[08/28/2025 07:03:00 INFO]: Training loss at epoch 9: 0.9925769567489624
[08/28/2025 07:03:23 INFO]: Training loss at epoch 7: 1.0608483254909515
[08/28/2025 07:03:26 INFO]: Training loss at epoch 35: 0.9489019215106964
[08/28/2025 07:03:33 INFO]: Training loss at epoch 49: 0.9749515652656555
[08/28/2025 07:03:46 INFO]: Training loss at epoch 29: 1.0522642731666565
[08/28/2025 07:04:06 INFO]: Training stats: {
    "score": -1.0090421755160173,
    "rmse": 1.0090421755160173
}
[08/28/2025 07:04:06 INFO]: Val stats: {
    "score": -0.7569630440400338,
    "rmse": 0.7569630440400338
}
[08/28/2025 07:04:06 INFO]: Test stats: {
    "score": -0.9177467919100066,
    "rmse": 0.9177467919100066
}
[08/28/2025 07:04:08 INFO]: Training loss at epoch 24: 1.202739953994751
[08/28/2025 07:04:14 INFO]: Training loss at epoch 8: 1.5603511929512024
[08/28/2025 07:04:50 INFO]: Training loss at epoch 25: 0.8803155422210693
[08/28/2025 07:04:57 INFO]: Training loss at epoch 48: 0.9279428720474243
[08/28/2025 07:05:00 INFO]: Training loss at epoch 9: 1.0964249968528748
[08/28/2025 07:05:06 INFO]: Training loss at epoch 9: 1.3118497729301453
[08/28/2025 07:05:07 INFO]: Training loss at epoch 30: 0.9924777448177338
[08/28/2025 07:05:09 INFO]: Training stats: {
    "score": -1.0018996791163475,
    "rmse": 1.0018996791163475
}
[08/28/2025 07:05:09 INFO]: Val stats: {
    "score": -0.665069307481729,
    "rmse": 0.665069307481729
}
[08/28/2025 07:05:09 INFO]: Test stats: {
    "score": -0.8698254655099461,
    "rmse": 0.8698254655099461
}
[08/28/2025 07:05:12 INFO]: Training loss at epoch 73: 1.1825920343399048
[08/28/2025 07:05:17 INFO]: Training loss at epoch 62: 1.1412354111671448
[08/28/2025 07:05:24 INFO]: Training stats: {
    "score": -0.9961571283327129,
    "rmse": 0.9961571283327129
}
[08/28/2025 07:05:24 INFO]: Val stats: {
    "score": -0.6934495002561202,
    "rmse": 0.6934495002561202
}
[08/28/2025 07:05:24 INFO]: Test stats: {
    "score": -0.8764968092171164,
    "rmse": 0.8764968092171164
}
[08/28/2025 07:05:25 INFO]: Training stats: {
    "score": -1.1037455136826304,
    "rmse": 1.1037455136826304
}
[08/28/2025 07:05:25 INFO]: Val stats: {
    "score": -0.9139698012010925,
    "rmse": 0.9139698012010925
}
[08/28/2025 07:05:25 INFO]: Test stats: {
    "score": -1.0399292018208774,
    "rmse": 1.0399292018208774
}
[08/28/2025 07:05:38 INFO]: Running Final Evaluation...
[08/28/2025 07:05:45 INFO]: Training loss at epoch 15: 0.8747623264789581
[08/28/2025 07:05:56 INFO]: Training stats: {
    "score": -1.00140539269701,
    "rmse": 1.00140539269701
}
[08/28/2025 07:05:56 INFO]: Val stats: {
    "score": -0.6809359984734337,
    "rmse": 0.6809359984734337
}
[08/28/2025 07:05:56 INFO]: Test stats: {
    "score": -0.8845026422530858,
    "rmse": 0.8845026422530858
}
[08/28/2025 07:06:09 INFO]: Training loss at epoch 40: 0.9279698431491852
[08/28/2025 07:06:09 INFO]: Training loss at epoch 36: 1.2037751078605652
[08/28/2025 07:06:10 INFO]: Training loss at epoch 31: 0.9665830731391907
[08/28/2025 07:06:18 INFO]: Training loss at epoch 10: 1.1168912649154663
[08/28/2025 07:06:25 INFO]: Training loss at epoch 46: 0.9237077832221985
[08/28/2025 07:06:33 INFO]: Training loss at epoch 50: 1.100463718175888
[08/28/2025 07:06:43 INFO]: Training loss at epoch 48: 0.9017089903354645
[08/28/2025 07:06:56 INFO]: Training loss at epoch 25: 0.9375451803207397
[08/28/2025 07:07:12 INFO]: New best epoch, val score: -0.6606310410217328
[08/28/2025 07:07:12 INFO]: Saving model to: unhanged-Shanesha_trial_90/model_best.pth
[08/28/2025 07:07:12 INFO]: Training loss at epoch 32: 0.9133919775485992
[08/28/2025 07:07:12 INFO]: Training loss at epoch 11: 0.9622620046138763
[08/28/2025 07:07:22 INFO]: Training loss at epoch 16: 0.853760302066803
[08/28/2025 07:07:37 INFO]: Training accuracy: {
    "score": -1.0169194053059003,
    "rmse": 1.0169194053059003
}
[08/28/2025 07:07:37 INFO]: Val accuracy: {
    "score": -0.66084784808877,
    "rmse": 0.66084784808877
}
[08/28/2025 07:07:37 INFO]: Test accuracy: {
    "score": -0.8739991548495372,
    "rmse": 0.8739991548495372
}
[08/28/2025 07:07:37 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_91",
    "best_epoch": 17,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8739991548495372,
        "rmse": 0.8739991548495372
    },
    "train_stats": {
        "score": -1.0169194053059003,
        "rmse": 1.0169194053059003
    },
    "val_stats": {
        "score": -0.66084784808877,
        "rmse": 0.66084784808877
    }
}
[08/28/2025 07:07:37 INFO]: Procewss finished for trial unhanged-Shanesha_trial_91
[08/28/2025 07:07:37 INFO]: 
_________________________________________________

[08/28/2025 07:07:37 INFO]: train_net_for_optune.py main() running.
[08/28/2025 07:07:37 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.7585489465569353
  attention_dropout: 0.19870270978203106
  ffn_dropout: 0.19870270978203106
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.7721198928710535e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_117

[08/28/2025 07:07:37 INFO]: This ft_transformer has 2.352 million parameters.
[08/28/2025 07:07:37 INFO]: Training will start at epoch 0.
[08/28/2025 07:07:37 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 07:07:38 INFO]: Training loss at epoch 26: 0.9962519109249115
[08/28/2025 07:08:05 INFO]: Training loss at epoch 101: 1.1164694428443909
[08/28/2025 07:08:06 INFO]: Training loss at epoch 12: 0.8598650395870209
[08/28/2025 07:08:15 INFO]: Training loss at epoch 33: 1.01167231798172
[08/28/2025 07:08:27 INFO]: Training loss at epoch 0: 1.1160812973976135
[08/28/2025 07:08:28 INFO]: Training loss at epoch 76: 1.1384325921535492
[08/28/2025 07:08:34 INFO]: New best epoch, val score: -0.7398022446783598
[08/28/2025 07:08:34 INFO]: Saving model to: unhanged-Shanesha_trial_117/model_best.pth
[08/28/2025 07:08:37 INFO]: Training loss at epoch 16: 0.9955449402332306
[08/28/2025 07:08:43 INFO]: Training loss at epoch 10: 0.8998536467552185
[08/28/2025 07:08:56 INFO]: Training loss at epoch 37: 1.308508276939392
[08/28/2025 07:09:00 INFO]: Training loss at epoch 13: 1.0298202633857727
[08/28/2025 07:09:16 INFO]: Training loss at epoch 34: 0.8474605679512024
[08/28/2025 07:09:21 INFO]: Training loss at epoch 1: 1.2144094705581665
[08/28/2025 07:09:29 INFO]: New best epoch, val score: -0.7066469165315452
[08/28/2025 07:09:29 INFO]: Saving model to: unhanged-Shanesha_trial_117/model_best.pth
[08/28/2025 07:09:42 INFO]: Training loss at epoch 26: 0.9130163490772247
[08/28/2025 07:09:50 INFO]: Training loss at epoch 50: 1.0875572860240936
[08/28/2025 07:09:53 INFO]: Training loss at epoch 14: 1.1067021489143372
[08/28/2025 07:10:01 INFO]: Training loss at epoch 12: 1.0975916981697083
[08/28/2025 07:10:14 INFO]: Training loss at epoch 2: 1.145970106124878
[08/28/2025 07:10:15 INFO]: Training loss at epoch 35: 0.8099059462547302
[08/28/2025 07:10:19 INFO]: Training loss at epoch 27: 1.0968528985977173
[08/28/2025 07:10:20 INFO]: New best epoch, val score: -0.6646121338498524
[08/28/2025 07:10:20 INFO]: Saving model to: unhanged-Shanesha_trial_117/model_best.pth
[08/28/2025 07:10:21 INFO]: Running Final Evaluation...
[08/28/2025 07:10:40 INFO]: Training loss at epoch 15: 1.0163813829421997
[08/28/2025 07:10:45 INFO]: Training loss at epoch 63: 1.3223731517791748
[08/28/2025 07:10:46 INFO]: New best epoch, val score: -0.6663291652404997
[08/28/2025 07:10:46 INFO]: Saving model to: unhanged-Shanesha_trial_116/model_best.pth
[08/28/2025 07:11:05 INFO]: Training loss at epoch 3: 1.003295123577118
[08/28/2025 07:11:10 INFO]: Training loss at epoch 36: 0.8825926184654236
[08/28/2025 07:11:16 INFO]: Training loss at epoch 17: 1.035465657711029
[08/28/2025 07:11:17 INFO]: Training loss at epoch 11: 0.9327788650989532
[08/28/2025 07:11:27 INFO]: Training loss at epoch 41: 1.175022155046463
[08/28/2025 07:11:28 INFO]: Training loss at epoch 38: 0.9056757390499115
[08/28/2025 07:11:28 INFO]: Training loss at epoch 16: 0.9187626242637634
[08/28/2025 07:11:32 INFO]: Training loss at epoch 74: 0.9008112847805023
[08/28/2025 07:11:33 INFO]: New best epoch, val score: -0.6643230317657377
[08/28/2025 07:11:33 INFO]: Saving model to: unhanged-Shanesha_trial_116/model_best.pth
[08/28/2025 07:11:46 INFO]: Training loss at epoch 51: 1.1176385283470154
[08/28/2025 07:11:52 INFO]: Training accuracy: {
    "score": -1.0089618739034687,
    "rmse": 1.0089618739034687
}
[08/28/2025 07:11:52 INFO]: Val accuracy: {
    "score": -0.6609537126282823,
    "rmse": 0.6609537126282823
}
[08/28/2025 07:11:52 INFO]: Test accuracy: {
    "score": -0.8699694828287227,
    "rmse": 0.8699694828287227
}
[08/28/2025 07:11:52 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_96",
    "best_epoch": 19,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8699694828287227,
        "rmse": 0.8699694828287227
    },
    "train_stats": {
        "score": -1.0089618739034687,
        "rmse": 1.0089618739034687
    },
    "val_stats": {
        "score": -0.6609537126282823,
        "rmse": 0.6609537126282823
    }
}
[08/28/2025 07:11:52 INFO]: Procewss finished for trial unhanged-Shanesha_trial_96
[08/28/2025 07:11:52 INFO]: 
_________________________________________________

[08/28/2025 07:11:52 INFO]: train_net_for_optune.py main() running.
[08/28/2025 07:11:52 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.7629279045990611
  attention_dropout: 0.31535213170416077
  ffn_dropout: 0.31535213170416077
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.366130431528488e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_118

[08/28/2025 07:11:52 INFO]: This ft_transformer has 2.354 million parameters.
[08/28/2025 07:11:52 INFO]: Training will start at epoch 0.
[08/28/2025 07:11:52 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 07:11:55 INFO]: Training loss at epoch 4: 1.149640142917633
[08/28/2025 07:11:55 INFO]: Training loss at epoch 49: 1.011447787284851
[08/28/2025 07:11:57 INFO]: Training loss at epoch 47: 1.130606234073639
[08/28/2025 07:12:01 INFO]: New best epoch, val score: -0.6632551296484102
[08/28/2025 07:12:01 INFO]: Saving model to: unhanged-Shanesha_trial_117/model_best.pth
[08/28/2025 07:12:06 INFO]: Training loss at epoch 37: 1.0879804193973541
[08/28/2025 07:12:10 INFO]: Training loss at epoch 27: 1.0866973996162415
[08/28/2025 07:12:15 INFO]: Training loss at epoch 17: 0.9969868361949921
[08/28/2025 07:12:20 INFO]: Training loss at epoch 10: 0.9866715967655182
[08/28/2025 07:12:21 INFO]: New best epoch, val score: -0.6642701376811861
[08/28/2025 07:12:21 INFO]: Saving model to: unhanged-Shanesha_trial_116/model_best.pth
[08/28/2025 07:12:36 INFO]: Training loss at epoch 0: 1.7373510599136353
[08/28/2025 07:12:41 INFO]: New best epoch, val score: -1.2673630344713875
[08/28/2025 07:12:41 INFO]: Saving model to: unhanged-Shanesha_trial_118/model_best.pth
[08/28/2025 07:12:45 INFO]: Training loss at epoch 5: 1.1439281105995178
[08/28/2025 07:12:47 INFO]: Training loss at epoch 28: 0.9651935398578644
[08/28/2025 07:12:51 INFO]: New best epoch, val score: -0.6630554197617975
[08/28/2025 07:12:51 INFO]: Saving model to: unhanged-Shanesha_trial_117/model_best.pth
[08/28/2025 07:13:02 INFO]: Training loss at epoch 38: 0.913531094789505
[08/28/2025 07:13:05 INFO]: Training loss at epoch 18: 1.0010797381401062
[08/28/2025 07:13:11 INFO]: Training loss at epoch 17: 0.8601871430873871
[08/28/2025 07:13:28 INFO]: Training loss at epoch 1: 1.3467693328857422
[08/28/2025 07:13:34 INFO]: New best epoch, val score: -0.9329886224894377
[08/28/2025 07:13:34 INFO]: Saving model to: unhanged-Shanesha_trial_118/model_best.pth
[08/28/2025 07:13:36 INFO]: Training loss at epoch 77: 0.9075615704059601
[08/28/2025 07:13:38 INFO]: Training loss at epoch 6: 0.9779077768325806
[08/28/2025 07:13:38 INFO]: Training stats: {
    "score": -1.0040633553149905,
    "rmse": 1.0040633553149905
}
[08/28/2025 07:13:38 INFO]: Val stats: {
    "score": -0.7115506435085743,
    "rmse": 0.7115506435085743
}
[08/28/2025 07:13:38 INFO]: Test stats: {
    "score": -0.8910932548040228,
    "rmse": 0.8910932548040228
}
[08/28/2025 07:13:51 INFO]: Training loss at epoch 12: 0.9701098501682281
[08/28/2025 07:13:54 INFO]: Training loss at epoch 18: 0.9079765677452087
[08/28/2025 07:13:56 INFO]: Training loss at epoch 19: 1.0439810156822205
[08/28/2025 07:14:01 INFO]: Training loss at epoch 39: 1.1822263598442078
[08/28/2025 07:14:02 INFO]: Training loss at epoch 39: 0.888351559638977
[08/28/2025 07:14:14 INFO]: Training stats: {
    "score": -1.0002288154357892,
    "rmse": 1.0002288154357892
}
[08/28/2025 07:14:14 INFO]: Val stats: {
    "score": -0.6687790267993171,
    "rmse": 0.6687790267993171
}
[08/28/2025 07:14:14 INFO]: Test stats: {
    "score": -0.8727499688966811,
    "rmse": 0.8727499688966811
}
[08/28/2025 07:14:23 INFO]: Training loss at epoch 2: 0.9215851426124573
[08/28/2025 07:14:24 INFO]: Training stats: {
    "score": -1.0036376032140948,
    "rmse": 1.0036376032140948
}
[08/28/2025 07:14:24 INFO]: Val stats: {
    "score": -0.7477692944099814,
    "rmse": 0.7477692944099814
}
[08/28/2025 07:14:24 INFO]: Test stats: {
    "score": -0.9126831628169062,
    "rmse": 0.9126831628169062
}
[08/28/2025 07:14:29 INFO]: New best epoch, val score: -0.6813655576956553
[08/28/2025 07:14:29 INFO]: Saving model to: unhanged-Shanesha_trial_118/model_best.pth
[08/28/2025 07:14:35 INFO]: Training loss at epoch 7: 1.1403087377548218
[08/28/2025 07:14:50 INFO]: Training loss at epoch 28: 1.0010966956615448
[08/28/2025 07:14:57 INFO]: Training stats: {
    "score": -0.9980213968523953,
    "rmse": 0.9980213968523953
}
[08/28/2025 07:14:57 INFO]: Val stats: {
    "score": -0.7024534825365875,
    "rmse": 0.7024534825365875
}
[08/28/2025 07:14:57 INFO]: Test stats: {
    "score": -0.888197631927577,
    "rmse": 0.888197631927577
}
[08/28/2025 07:14:59 INFO]: Training loss at epoch 102: 1.0880037248134613
[08/28/2025 07:15:07 INFO]: Training loss at epoch 20: 0.9516294002532959
[08/28/2025 07:15:18 INFO]: Training loss at epoch 3: 1.1009893417358398
[08/28/2025 07:15:27 INFO]: Training loss at epoch 40: 0.9789857864379883
[08/28/2025 07:15:31 INFO]: Training loss at epoch 8: 1.3128733038902283
[08/28/2025 07:15:32 INFO]: Training loss at epoch 29: 0.8192405700683594
[08/28/2025 07:16:00 INFO]: Training loss at epoch 21: 0.9461397230625153
[08/28/2025 07:16:04 INFO]: Training loss at epoch 64: 0.7707052975893021
[08/28/2025 07:16:14 INFO]: Training loss at epoch 4: 1.2707883715629578
[08/28/2025 07:16:26 INFO]: Training loss at epoch 9: 1.27645742893219
[08/28/2025 07:16:28 INFO]: Training stats: {
    "score": -1.0138398635455659,
    "rmse": 1.0138398635455659
}
[08/28/2025 07:16:28 INFO]: Val stats: {
    "score": -0.7565248493871113,
    "rmse": 0.7565248493871113
}
[08/28/2025 07:16:28 INFO]: Test stats: {
    "score": -0.9194711320344021,
    "rmse": 0.9194711320344021
}
[08/28/2025 07:16:29 INFO]: Training loss at epoch 41: 0.9878806173801422
[08/28/2025 07:16:38 INFO]: Training loss at epoch 13: 1.171850562095642
[08/28/2025 07:16:45 INFO]: Training loss at epoch 19: 1.0272033214569092
[08/28/2025 07:16:46 INFO]: Training stats: {
    "score": -1.0008804797065365,
    "rmse": 1.0008804797065365
}
[08/28/2025 07:16:46 INFO]: Val stats: {
    "score": -0.6721049589299432,
    "rmse": 0.6721049589299432
}
[08/28/2025 07:16:46 INFO]: Test stats: {
    "score": -0.8681716661257988,
    "rmse": 0.8681716661257988
}
[08/28/2025 07:16:46 INFO]: Training loss at epoch 42: 1.0714814364910126
[08/28/2025 07:16:53 INFO]: Training loss at epoch 22: 0.9964931011199951
[08/28/2025 07:17:06 INFO]: Training loss at epoch 52: 1.1297477185726166
[08/28/2025 07:17:10 INFO]: Training loss at epoch 5: 1.4730217456817627
[08/28/2025 07:17:32 INFO]: Training loss at epoch 42: 0.8605544865131378
[08/28/2025 07:17:36 INFO]: Training loss at epoch 48: 0.9181323945522308
[08/28/2025 07:17:37 INFO]: Training loss at epoch 29: 1.0157538056373596
[08/28/2025 07:17:41 INFO]: Training loss at epoch 10: 0.9932222366333008
[08/28/2025 07:17:43 INFO]: Training loss at epoch 40: 1.0195406675338745
[08/28/2025 07:17:43 INFO]: Training stats: {
    "score": -0.9987678924192166,
    "rmse": 0.9987678924192166
}
[08/28/2025 07:17:43 INFO]: Val stats: {
    "score": -0.6724962297550041,
    "rmse": 0.6724962297550041
}
[08/28/2025 07:17:43 INFO]: Test stats: {
    "score": -0.8716869874555596,
    "rmse": 0.8716869874555596
}
[08/28/2025 07:17:47 INFO]: Training loss at epoch 23: 1.125542163848877
[08/28/2025 07:17:55 INFO]: Training loss at epoch 13: 1.1912651658058167
[08/28/2025 07:17:56 INFO]: Training loss at epoch 75: 0.9225018620491028
[08/28/2025 07:18:05 INFO]: Training loss at epoch 6: 1.0330535173416138
[08/28/2025 07:18:32 INFO]: Training stats: {
    "score": -1.0016709559744934,
    "rmse": 1.0016709559744934
}
[08/28/2025 07:18:32 INFO]: Val stats: {
    "score": -0.6695521696281421,
    "rmse": 0.6695521696281421
}
[08/28/2025 07:18:32 INFO]: Test stats: {
    "score": -0.8689494342493789,
    "rmse": 0.8689494342493789
}
[08/28/2025 07:18:32 INFO]: Training loss at epoch 43: 1.0761467218399048
[08/28/2025 07:18:36 INFO]: Training loss at epoch 11: 1.0215162634849548
[08/28/2025 07:18:39 INFO]: Training loss at epoch 24: 1.0276711583137512
[08/28/2025 07:18:57 INFO]: Training loss at epoch 7: 1.160467803478241
[08/28/2025 07:19:04 INFO]: Training loss at epoch 50: 1.206552267074585
[08/28/2025 07:19:07 INFO]: Training loss at epoch 78: 1.2044953405857086
[08/28/2025 07:19:10 INFO]: Training loss at epoch 30: 1.0984848737716675
[08/28/2025 07:19:16 INFO]: Training loss at epoch 18: 1.1381947994232178
[08/28/2025 07:19:18 INFO]: Training loss at epoch 14: 1.0254077017307281
[08/28/2025 07:19:24 INFO]: Training loss at epoch 11: 1.1727405190467834
[08/28/2025 07:19:26 INFO]: Training loss at epoch 12: 1.0836138725280762
[08/28/2025 07:19:27 INFO]: Training loss at epoch 25: 0.9781154692173004
[08/28/2025 07:19:29 INFO]: Training loss at epoch 44: 1.0809450149536133
[08/28/2025 07:19:47 INFO]: Training loss at epoch 8: 1.112326443195343
[08/28/2025 07:20:14 INFO]: Training loss at epoch 26: 1.053904414176941
[08/28/2025 07:20:16 INFO]: Training loss at epoch 41: 0.9325061440467834
[08/28/2025 07:20:16 INFO]: Training loss at epoch 13: 1.043044090270996
[08/28/2025 07:20:22 INFO]: Training loss at epoch 20: 0.9562197923660278
[08/28/2025 07:20:25 INFO]: Training loss at epoch 45: 0.9050601720809937
[08/28/2025 07:20:37 INFO]: Training loss at epoch 9: 1.3897120356559753
[08/28/2025 07:20:40 INFO]: New best epoch, val score: -0.6632758085698186
[08/28/2025 07:20:40 INFO]: Saving model to: unhanged-Shanesha_trial_113/model_best.pth
[08/28/2025 07:20:54 INFO]: Training stats: {
    "score": -1.058452662824062,
    "rmse": 1.058452662824062
}
[08/28/2025 07:20:54 INFO]: Val stats: {
    "score": -0.692108794069199,
    "rmse": 0.692108794069199
}
[08/28/2025 07:20:54 INFO]: Test stats: {
    "score": -0.9021845047845134,
    "rmse": 0.9021845047845134
}
[08/28/2025 07:21:02 INFO]: Training loss at epoch 30: 0.9860122501850128
[08/28/2025 07:21:02 INFO]: Training loss at epoch 27: 0.8654386401176453
[08/28/2025 07:21:06 INFO]: Training loss at epoch 14: 1.0741431713104248
[08/28/2025 07:21:20 INFO]: Training loss at epoch 46: 0.8484527766704559
[08/28/2025 07:21:20 INFO]: Training loss at epoch 65: 1.0596070289611816
[08/28/2025 07:21:38 INFO]: Training loss at epoch 31: 1.1477898359298706
[08/28/2025 07:21:43 INFO]: Training loss at epoch 10: 0.9257127344608307
[08/28/2025 07:21:46 INFO]: Training loss at epoch 15: 0.8827444911003113
[08/28/2025 07:21:49 INFO]: New best epoch, val score: -0.6674306845963517
[08/28/2025 07:21:49 INFO]: Saving model to: unhanged-Shanesha_trial_118/model_best.pth
[08/28/2025 07:21:50 INFO]: Training loss at epoch 28: 1.1274786591529846
[08/28/2025 07:21:53 INFO]: Training loss at epoch 103: 1.040539413690567
[08/28/2025 07:21:53 INFO]: Training loss at epoch 43: 1.0392621159553528
[08/28/2025 07:21:57 INFO]: Training loss at epoch 15: 0.897943377494812
[08/28/2025 07:22:12 INFO]: Training loss at epoch 53: 0.9697081744670868
[08/28/2025 07:22:18 INFO]: Training loss at epoch 47: 1.1829379200935364
[08/28/2025 07:22:35 INFO]: Training loss at epoch 11: 0.8014854043722153
[08/28/2025 07:22:40 INFO]: Training loss at epoch 29: 0.9051945209503174
[08/28/2025 07:22:49 INFO]: Training loss at epoch 42: 0.9065921604633331
[08/28/2025 07:22:51 INFO]: Training loss at epoch 16: 1.0484100580215454
[08/28/2025 07:22:57 INFO]: Training loss at epoch 49: 1.021951287984848
[08/28/2025 07:22:58 INFO]: Training stats: {
    "score": -1.0019944695141718,
    "rmse": 1.0019944695141718
}
[08/28/2025 07:22:58 INFO]: Val stats: {
    "score": -0.7081485804442146,
    "rmse": 0.7081485804442146
}
[08/28/2025 07:22:58 INFO]: Test stats: {
    "score": -0.8922678323226408,
    "rmse": 0.8922678323226408
}
[08/28/2025 07:23:00 INFO]: Training loss at epoch 21: 0.9194009900093079
[08/28/2025 07:23:19 INFO]: Training loss at epoch 48: 1.0056216716766357
[08/28/2025 07:23:30 INFO]: Training loss at epoch 12: 0.9893904030323029
[08/28/2025 07:23:40 INFO]: Training loss at epoch 31: 0.9640317261219025
[08/28/2025 07:23:47 INFO]: Training loss at epoch 17: 1.0118467509746552
[08/28/2025 07:23:51 INFO]: Training loss at epoch 30: 0.8292059898376465
[08/28/2025 07:24:02 INFO]: Training loss at epoch 76: 0.9956739246845245
[08/28/2025 07:24:10 INFO]: Training loss at epoch 51: 0.9579318165779114
[08/28/2025 07:24:18 INFO]: Training loss at epoch 79: 0.9522047638893127
[08/28/2025 07:24:20 INFO]: Training loss at epoch 49: 0.9446209371089935
[08/28/2025 07:24:21 INFO]: Training loss at epoch 32: 1.1172139644622803
[08/28/2025 07:24:26 INFO]: Training loss at epoch 13: 0.9507410526275635
[08/28/2025 07:24:30 INFO]: Training loss at epoch 16: 0.9973286688327789
[08/28/2025 07:24:42 INFO]: Training stats: {
    "score": -0.9868346422404795,
    "rmse": 0.9868346422404795
}
[08/28/2025 07:24:42 INFO]: Val stats: {
    "score": -0.6851130299440839,
    "rmse": 0.6851130299440839
}
[08/28/2025 07:24:42 INFO]: Test stats: {
    "score": -0.8754198548903525,
    "rmse": 0.8754198548903525
}
[08/28/2025 07:24:42 INFO]: Training loss at epoch 18: 0.9533605575561523
[08/28/2025 07:24:44 INFO]: Training loss at epoch 31: 0.8439647257328033
[08/28/2025 07:24:56 INFO]: Training stats: {
    "score": -1.0043919285197704,
    "rmse": 1.0043919285197704
}
[08/28/2025 07:24:56 INFO]: Val stats: {
    "score": -0.7105265488789151,
    "rmse": 0.7105265488789151
}
[08/28/2025 07:24:56 INFO]: Test stats: {
    "score": -0.8905577816058783,
    "rmse": 0.8905577816058783
}
[08/28/2025 07:25:07 INFO]: Training loss at epoch 19: 0.9478480815887451
[08/28/2025 07:25:21 INFO]: Training loss at epoch 14: 0.9711640477180481
[08/28/2025 07:25:34 INFO]: Training loss at epoch 43: 0.8031311631202698
[08/28/2025 07:25:37 INFO]: Training loss at epoch 32: 1.0227996706962585
[08/28/2025 07:25:38 INFO]: Training loss at epoch 19: 1.1211225986480713
[08/28/2025 07:25:43 INFO]: Training loss at epoch 14: 1.0190933346748352
[08/28/2025 07:25:44 INFO]: Training loss at epoch 50: 0.9005305171012878
[08/28/2025 07:25:51 INFO]: Training loss at epoch 22: 1.1445581316947937
[08/28/2025 07:25:57 INFO]: Training stats: {
    "score": -1.000962582748865,
    "rmse": 1.000962582748865
}
[08/28/2025 07:25:57 INFO]: Val stats: {
    "score": -0.6695876064645692,
    "rmse": 0.6695876064645692
}
[08/28/2025 07:25:57 INFO]: Test stats: {
    "score": -0.8676476360805603,
    "rmse": 0.8676476360805603
}
[08/28/2025 07:26:12 INFO]: Training stats: {
    "score": -0.9964475818268916,
    "rmse": 0.9964475818268916
}
[08/28/2025 07:26:12 INFO]: Val stats: {
    "score": -0.6789697734726103,
    "rmse": 0.6789697734726103
}
[08/28/2025 07:26:12 INFO]: Test stats: {
    "score": -0.8730673502248916,
    "rmse": 0.8730673502248916
}
[08/28/2025 07:26:16 INFO]: Training loss at epoch 15: 0.8610766530036926
[08/28/2025 07:26:22 INFO]: Training loss at epoch 12: 1.1143310070037842
[08/28/2025 07:26:26 INFO]: Training loss at epoch 32: 1.083475649356842
[08/28/2025 07:26:31 INFO]: Training loss at epoch 33: 1.1185715198516846
[08/28/2025 07:26:47 INFO]: Training loss at epoch 51: 1.087814748287201
[08/28/2025 07:26:49 INFO]: Training loss at epoch 66: 1.2069944143295288
[08/28/2025 07:26:51 INFO]: Running Final Evaluation...
[08/28/2025 07:26:53 INFO]: Training loss at epoch 20: 1.024646818637848
[08/28/2025 07:27:08 INFO]: Training loss at epoch 33: 1.0096774101257324
[08/28/2025 07:27:11 INFO]: Training loss at epoch 16: 1.1914076805114746
[08/28/2025 07:27:14 INFO]: Training stats: {
    "score": -1.023815328366312,
    "rmse": 1.023815328366312
}
[08/28/2025 07:27:14 INFO]: Val stats: {
    "score": -0.7674044841806043,
    "rmse": 0.7674044841806043
}
[08/28/2025 07:27:14 INFO]: Test stats: {
    "score": -0.9279692388419822,
    "rmse": 0.9279692388419822
}
[08/28/2025 07:27:16 INFO]: Training loss at epoch 17: 0.8563839793205261
[08/28/2025 07:27:21 INFO]: Training loss at epoch 44: 1.2145432829856873
[08/28/2025 07:27:23 INFO]: Training loss at epoch 34: 0.9240381717681885
[08/28/2025 07:27:40 INFO]: Training loss at epoch 54: 0.9639832079410553
[08/28/2025 07:27:48 INFO]: Training loss at epoch 21: 1.416798174381256
[08/28/2025 07:27:48 INFO]: Training loss at epoch 52: 1.0717523097991943
[08/28/2025 07:28:03 INFO]: Training loss at epoch 17: 0.9672497510910034
[08/28/2025 07:28:13 INFO]: Training loss at epoch 35: 1.0350886583328247
[08/28/2025 07:28:16 INFO]: Training loss at epoch 44: 0.898177981376648
[08/28/2025 07:28:36 INFO]: Training loss at epoch 23: 1.0747748613357544
[08/28/2025 07:28:38 INFO]: Training loss at epoch 22: 0.9680738151073456
[08/28/2025 07:28:40 INFO]: Training accuracy: {
    "score": -1.008545170730923,
    "rmse": 1.008545170730923
}
[08/28/2025 07:28:40 INFO]: Val accuracy: {
    "score": -0.6612893322652331,
    "rmse": 0.6612893322652331
}
[08/28/2025 07:28:40 INFO]: Test accuracy: {
    "score": -0.8700527424954125,
    "rmse": 0.8700527424954125
}
[08/28/2025 07:28:40 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_73",
    "best_epoch": 48,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8700527424954125,
        "rmse": 0.8700527424954125
    },
    "train_stats": {
        "score": -1.008545170730923,
        "rmse": 1.008545170730923
    },
    "val_stats": {
        "score": -0.6612893322652331,
        "rmse": 0.6612893322652331
    }
}
[08/28/2025 07:28:40 INFO]: Procewss finished for trial unhanged-Shanesha_trial_73
[08/28/2025 07:28:40 INFO]: 
_________________________________________________

[08/28/2025 07:28:40 INFO]: train_net_for_optune.py main() running.
[08/28/2025 07:28:40 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 2
  n_heads: 8
  d_ffn_factor: 1.7747500069526363
  attention_dropout: 0.2132913990002016
  ffn_dropout: 0.2132913990002016
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.0876147995408367e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_119

[08/28/2025 07:28:40 INFO]: This ft_transformer has 2.361 million parameters.
[08/28/2025 07:28:40 INFO]: Training will start at epoch 0.
[08/28/2025 07:28:40 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 07:28:44 INFO]: Training loss at epoch 53: 1.0552692413330078
[08/28/2025 07:28:53 INFO]: Training loss at epoch 18: 1.219926655292511
[08/28/2025 07:28:57 INFO]: Training loss at epoch 104: 0.9250487685203552
[08/28/2025 07:29:01 INFO]: Training loss at epoch 36: 0.9877364635467529
[08/28/2025 07:29:03 INFO]: Training loss at epoch 33: 1.2853565216064453
[08/28/2025 07:29:24 INFO]: Training loss at epoch 0: 0.8444398939609528
[08/28/2025 07:29:25 INFO]: Training loss at epoch 52: 0.9121982455253601
[08/28/2025 07:29:28 INFO]: Training loss at epoch 23: 1.1762130856513977
[08/28/2025 07:29:30 INFO]: New best epoch, val score: -0.6606745731170452
[08/28/2025 07:29:30 INFO]: Saving model to: unhanged-Shanesha_trial_119/model_best.pth
[08/28/2025 07:29:39 INFO]: Training loss at epoch 34: 1.126163810491562
[08/28/2025 07:29:39 INFO]: Training loss at epoch 54: 1.0753284096717834
[08/28/2025 07:29:42 INFO]: Training loss at epoch 19: 0.9946424961090088
[08/28/2025 07:29:46 INFO]: Running Final Evaluation...
[08/28/2025 07:29:47 INFO]: Training loss at epoch 18: 0.8328582644462585
[08/28/2025 07:29:48 INFO]: Training loss at epoch 37: 0.9929107129573822
[08/28/2025 07:29:59 INFO]: Training stats: {
    "score": -1.039749745376479,
    "rmse": 1.039749745376479
}
[08/28/2025 07:29:59 INFO]: Val stats: {
    "score": -0.8055031421930283,
    "rmse": 0.8055031421930283
}
[08/28/2025 07:29:59 INFO]: Test stats: {
    "score": -0.9532913420955673,
    "rmse": 0.9532913420955673
}
[08/28/2025 07:30:01 INFO]: Running Final Evaluation...
[08/28/2025 07:30:05 INFO]: Training accuracy: {
    "score": -1.0030510696916872,
    "rmse": 1.0030510696916872
}
[08/28/2025 07:30:05 INFO]: Val accuracy: {
    "score": -0.6701445544779334,
    "rmse": 0.6701445544779334
}
[08/28/2025 07:30:05 INFO]: Test accuracy: {
    "score": -0.8735848269597574,
    "rmse": 0.8735848269597574
}
[08/28/2025 07:30:05 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_114",
    "best_epoch": 23,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8735848269597574,
        "rmse": 0.8735848269597574
    },
    "train_stats": {
        "score": -1.0030510696916872,
        "rmse": 1.0030510696916872
    },
    "val_stats": {
        "score": -0.6701445544779334,
        "rmse": 0.6701445544779334
    }
}
[08/28/2025 07:30:05 INFO]: Procewss finished for trial unhanged-Shanesha_trial_114
[08/28/2025 07:30:05 INFO]: 
_________________________________________________

[08/28/2025 07:30:05 INFO]: train_net_for_optune.py main() running.
[08/28/2025 07:30:05 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 2.2135780592014487
  attention_dropout: 0.22689217089513017
  ffn_dropout: 0.22689217089513017
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.742188742911976e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_120

[08/28/2025 07:30:05 INFO]: This ft_transformer has 5.115 million parameters.
[08/28/2025 07:30:05 INFO]: Training will start at epoch 0.
[08/28/2025 07:30:05 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 07:30:15 INFO]: Training loss at epoch 1: 1.066616952419281
[08/28/2025 07:30:16 INFO]: Training loss at epoch 77: 0.8446661531925201
[08/28/2025 07:30:18 INFO]: Training loss at epoch 24: 1.0243609547615051
[08/28/2025 07:30:23 INFO]: Training loss at epoch 50: 1.0666469931602478
[08/28/2025 07:30:36 INFO]: Training loss at epoch 38: 1.213162899017334
[08/28/2025 07:30:45 INFO]: Training loss at epoch 45: 0.8126262426376343
[08/28/2025 07:30:49 INFO]: Training loss at epoch 20: 1.1178566217422485
[08/28/2025 07:31:05 INFO]: Training loss at epoch 2: 1.0087524950504303
[08/28/2025 07:31:08 INFO]: Training loss at epoch 25: 0.98565873503685
[08/28/2025 07:31:09 INFO]: Training loss at epoch 24: 1.1586531400680542
[08/28/2025 07:31:26 INFO]: Training loss at epoch 39: 0.9168603420257568
[08/28/2025 07:31:35 INFO]: Training loss at epoch 34: 0.924191951751709
[08/28/2025 07:31:43 INFO]: Training loss at epoch 21: 0.8322291374206543
[08/28/2025 07:31:43 INFO]: Training stats: {
    "score": -1.0005519238266232,
    "rmse": 1.0005519238266232
}
[08/28/2025 07:31:43 INFO]: Val stats: {
    "score": -0.7046019965119641,
    "rmse": 0.7046019965119641
}
[08/28/2025 07:31:43 INFO]: Test stats: {
    "score": -0.8901344983239524,
    "rmse": 0.8901344983239524
}
[08/28/2025 07:31:47 INFO]: Training accuracy: {
    "score": -1.014936035476028,
    "rmse": 1.014936035476028
}
[08/28/2025 07:31:47 INFO]: Val accuracy: {
    "score": -0.6598149101234511,
    "rmse": 0.6598149101234511
}
[08/28/2025 07:31:47 INFO]: Test accuracy: {
    "score": -0.8714454342775141,
    "rmse": 0.8714454342775141
}
[08/28/2025 07:31:47 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_94",
    "best_epoch": 21,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8714454342775141,
        "rmse": 0.8714454342775141
    },
    "train_stats": {
        "score": -1.014936035476028,
        "rmse": 1.014936035476028
    },
    "val_stats": {
        "score": -0.6598149101234511,
        "rmse": 0.6598149101234511
    }
}
[08/28/2025 07:31:47 INFO]: Procewss finished for trial unhanged-Shanesha_trial_94
[08/28/2025 07:31:47 INFO]: 
_________________________________________________

[08/28/2025 07:31:47 INFO]: train_net_for_optune.py main() running.
[08/28/2025 07:31:47 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 1.7802561352390092
  attention_dropout: 0.2264024882199002
  ffn_dropout: 0.2264024882199002
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.6970171946002814e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_121

[08/28/2025 07:31:47 INFO]: This ft_transformer has 2.076 million parameters.
[08/28/2025 07:31:47 INFO]: Training will start at epoch 0.
[08/28/2025 07:31:47 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 07:32:00 INFO]: Training loss at epoch 67: 0.9200811088085175
[08/28/2025 07:32:01 INFO]: Training loss at epoch 3: 1.2947124242782593
[08/28/2025 07:32:03 INFO]: Training loss at epoch 26: 1.024539053440094
[08/28/2025 07:32:15 INFO]: Training loss at epoch 35: 1.0000572204589844
[08/28/2025 07:32:20 INFO]: Training loss at epoch 0: 1.1410190165042877
[08/28/2025 07:32:24 INFO]: Training loss at epoch 19: 1.0232397317886353
[08/28/2025 07:32:29 INFO]: Training loss at epoch 45: 0.9975209832191467
[08/28/2025 07:32:37 INFO]: Training loss at epoch 40: 1.0831236839294434
[08/28/2025 07:32:38 INFO]: Training loss at epoch 22: 0.9319217205047607
[08/28/2025 07:32:39 INFO]: New best epoch, val score: -0.6865445446926586
[08/28/2025 07:32:39 INFO]: Saving model to: unhanged-Shanesha_trial_120/model_best.pth
[08/28/2025 07:32:45 INFO]: Training loss at epoch 55: 1.0045055747032166
[08/28/2025 07:33:00 INFO]: Training loss at epoch 4: 1.1489957571029663
[08/28/2025 07:33:00 INFO]: Training loss at epoch 27: 0.973186582326889
[08/28/2025 07:33:02 INFO]: Training loss at epoch 20: 1.2435623407363892
[08/28/2025 07:33:04 INFO]: Training loss at epoch 0: 1.482774019241333
[08/28/2025 07:33:13 INFO]: Training loss at epoch 13: 0.9063776433467865
[08/28/2025 07:33:14 INFO]: New best epoch, val score: -0.9484512547816849
[08/28/2025 07:33:14 INFO]: Saving model to: unhanged-Shanesha_trial_121/model_best.pth
[08/28/2025 07:33:21 INFO]: Training stats: {
    "score": -0.9997923243559363,
    "rmse": 0.9997923243559363
}
[08/28/2025 07:33:21 INFO]: Val stats: {
    "score": -0.6644442769291784,
    "rmse": 0.6644442769291784
}
[08/28/2025 07:33:21 INFO]: Test stats: {
    "score": -0.8753623058477402,
    "rmse": 0.8753623058477402
}
[08/28/2025 07:33:28 INFO]: Training loss at epoch 46: 1.1328633725643158
[08/28/2025 07:33:30 INFO]: Training loss at epoch 41: 0.9600701630115509
[08/28/2025 07:33:33 INFO]: Training loss at epoch 15: 1.3818190693855286
[08/28/2025 07:33:34 INFO]: Training loss at epoch 23: 0.9057379364967346
[08/28/2025 07:33:47 INFO]: Running Final Evaluation...
[08/28/2025 07:33:56 INFO]: Training loss at epoch 28: 1.1158176064491272
[08/28/2025 07:33:56 INFO]: Training loss at epoch 5: 0.7555639445781708
[08/28/2025 07:34:01 INFO]: Training loss at epoch 25: 0.959946483373642
[08/28/2025 07:34:22 INFO]: New best epoch, val score: -0.6624289685521199
[08/28/2025 07:34:22 INFO]: Saving model to: unhanged-Shanesha_trial_113/model_best.pth
[08/28/2025 07:34:22 INFO]: Training loss at epoch 35: 1.0077341198921204
[08/28/2025 07:34:23 INFO]: Training loss at epoch 42: 0.9932445287704468
[08/28/2025 07:34:30 INFO]: Training loss at epoch 24: 0.9341099262237549
[08/28/2025 07:34:31 INFO]: Training loss at epoch 1: 1.7105895280838013
[08/28/2025 07:34:42 INFO]: New best epoch, val score: -0.7514038310350507
[08/28/2025 07:34:42 INFO]: Saving model to: unhanged-Shanesha_trial_121/model_best.pth
[08/28/2025 07:34:48 INFO]: Training accuracy: {
    "score": -1.0082273643549944,
    "rmse": 1.0082273643549944
}
[08/28/2025 07:34:48 INFO]: Val accuracy: {
    "score": -0.664104340320743,
    "rmse": 0.664104340320743
}
[08/28/2025 07:34:48 INFO]: Test accuracy: {
    "score": -0.8711020839667466,
    "rmse": 0.8711020839667466
}
[08/28/2025 07:34:48 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_107",
    "best_epoch": 15,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8711020839667466,
        "rmse": 0.8711020839667466
    },
    "train_stats": {
        "score": -1.0082273643549944,
        "rmse": 1.0082273643549944
    },
    "val_stats": {
        "score": -0.664104340320743,
        "rmse": 0.664104340320743
    }
}
[08/28/2025 07:34:48 INFO]: Procewss finished for trial unhanged-Shanesha_trial_107
[08/28/2025 07:34:48 INFO]: 
_________________________________________________

[08/28/2025 07:34:48 INFO]: train_net_for_optune.py main() running.
[08/28/2025 07:34:48 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.2213227661036363
  attention_dropout: 0.15482487521802352
  ffn_dropout: 0.15482487521802352
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00023998563633976586
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_122

[08/28/2025 07:34:48 INFO]: This ft_transformer has 2.308 million parameters.
[08/28/2025 07:34:48 INFO]: Training will start at epoch 0.
[08/28/2025 07:34:48 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 07:34:51 INFO]: Training loss at epoch 29: 0.8761478662490845
[08/28/2025 07:34:52 INFO]: Training loss at epoch 6: 1.2122575044631958
[08/28/2025 07:35:01 INFO]: Training loss at epoch 1: 1.1805620193481445
[08/28/2025 07:35:02 INFO]: Training loss at epoch 36: 0.9836103916168213
[08/28/2025 07:35:12 INFO]: Training stats: {
    "score": -0.9989983795431281,
    "rmse": 0.9989983795431281
}
[08/28/2025 07:35:12 INFO]: Val stats: {
    "score": -0.6726202074580636,
    "rmse": 0.6726202074580636
}
[08/28/2025 07:35:12 INFO]: Test stats: {
    "score": -0.8687074170506747,
    "rmse": 0.8687074170506747
}
[08/28/2025 07:35:17 INFO]: Training loss at epoch 43: 0.8811749815940857
[08/28/2025 07:35:26 INFO]: Training loss at epoch 25: 0.869702160358429
[08/28/2025 07:35:48 INFO]: Training loss at epoch 7: 1.100856602191925
[08/28/2025 07:36:00 INFO]: Training loss at epoch 2: 1.1987613439559937
[08/28/2025 07:36:00 INFO]: Training loss at epoch 105: 0.9551113247871399
[08/28/2025 07:36:04 INFO]: Training loss at epoch 51: 1.0716042518615723
[08/28/2025 07:36:07 INFO]: Training loss at epoch 30: 0.8248803913593292
[08/28/2025 07:36:07 INFO]: Training loss at epoch 20: 1.1156346201896667
[08/28/2025 07:36:10 INFO]: New best epoch, val score: -0.6699145037305941
[08/28/2025 07:36:10 INFO]: Saving model to: unhanged-Shanesha_trial_121/model_best.pth
[08/28/2025 07:36:10 INFO]: Training loss at epoch 44: 1.0096184015274048
[08/28/2025 07:36:20 INFO]: Training loss at epoch 26: 1.2459871470928192
[08/28/2025 07:36:27 INFO]: Training loss at epoch 0: 1.1764394640922546
[08/28/2025 07:36:42 INFO]: New best epoch, val score: -0.6946535285803103
[08/28/2025 07:36:42 INFO]: Saving model to: unhanged-Shanesha_trial_122/model_best.pth
[08/28/2025 07:36:43 INFO]: Training loss at epoch 78: 0.8326442539691925
[08/28/2025 07:36:44 INFO]: Training loss at epoch 8: 1.0958884358406067
[08/28/2025 07:36:51 INFO]: Training loss at epoch 26: 1.0186039805412292
[08/28/2025 07:37:00 INFO]: Training loss at epoch 31: 1.082808256149292
[08/28/2025 07:37:00 INFO]: Training loss at epoch 45: 0.8632955253124237
[08/28/2025 07:37:06 INFO]: Training loss at epoch 36: 1.0020132958889008
[08/28/2025 07:37:12 INFO]: Training loss at epoch 27: 0.8898440003395081
[08/28/2025 07:37:21 INFO]: Training loss at epoch 3: 0.8893716335296631
[08/28/2025 07:37:30 INFO]: Training loss at epoch 68: 1.1633354425430298
[08/28/2025 07:37:34 INFO]: Training loss at epoch 2: 0.9417248368263245
[08/28/2025 07:37:35 INFO]: Training loss at epoch 9: 0.9186634123325348
[08/28/2025 07:37:40 INFO]: Training loss at epoch 37: 0.9369600415229797
[08/28/2025 07:37:48 INFO]: Training loss at epoch 46: 0.9379332959651947
[08/28/2025 07:37:50 INFO]: Training loss at epoch 32: 0.9822190403938293
[08/28/2025 07:37:51 INFO]: New best epoch, val score: -0.6713248871812996
[08/28/2025 07:37:51 INFO]: Saving model to: unhanged-Shanesha_trial_120/model_best.pth
[08/28/2025 07:37:52 INFO]: Training stats: {
    "score": -0.9993502417547342,
    "rmse": 0.9993502417547342
}
[08/28/2025 07:37:52 INFO]: Val stats: {
    "score": -0.6974046142396255,
    "rmse": 0.6974046142396255
}
[08/28/2025 07:37:52 INFO]: Test stats: {
    "score": -0.8832135421166677,
    "rmse": 0.8832135421166677
}
[08/28/2025 07:37:53 INFO]: Training loss at epoch 46: 1.1227345764636993
[08/28/2025 07:38:02 INFO]: Training loss at epoch 28: 0.9875620901584625
[08/28/2025 07:38:04 INFO]: Training loss at epoch 56: 0.7502109706401825
[08/28/2025 07:38:14 INFO]: Training loss at epoch 1: 2.315661609172821
[08/28/2025 07:38:36 INFO]: Training loss at epoch 47: 1.2548978626728058
[08/28/2025 07:38:39 INFO]: Training loss at epoch 21: 1.0167547762393951
[08/28/2025 07:38:40 INFO]: Training loss at epoch 4: 1.4009480476379395
[08/28/2025 07:38:40 INFO]: Training loss at epoch 33: 1.2563595175743103
[08/28/2025 07:38:42 INFO]: Training loss at epoch 10: 0.9394731819629669
[08/28/2025 07:38:51 INFO]: Training loss at epoch 29: 1.0589390993118286
[08/28/2025 07:38:57 INFO]: Training loss at epoch 21: 0.7918495684862137
[08/28/2025 07:39:08 INFO]: Training stats: {
    "score": -0.9972390532898522,
    "rmse": 0.9972390532898522
}
[08/28/2025 07:39:08 INFO]: Val stats: {
    "score": -0.6826912308453608,
    "rmse": 0.6826912308453608
}
[08/28/2025 07:39:08 INFO]: Test stats: {
    "score": -0.8724902828168071,
    "rmse": 0.8724902828168071
}
[08/28/2025 07:39:24 INFO]: Training loss at epoch 27: 1.1710928976535797
[08/28/2025 07:39:25 INFO]: Training loss at epoch 48: 0.9874071478843689
[08/28/2025 07:39:30 INFO]: Training loss at epoch 34: 1.144001543521881
[08/28/2025 07:39:30 INFO]: Running Final Evaluation...
[08/28/2025 07:39:33 INFO]: Training loss at epoch 11: 0.9617792367935181
[08/28/2025 07:39:36 INFO]: Training loss at epoch 37: 0.9172817766666412
[08/28/2025 07:39:47 INFO]: Training accuracy: {
    "score": -1.0029727598530007,
    "rmse": 1.0029727598530007
}
[08/28/2025 07:39:47 INFO]: Val accuracy: {
    "score": -0.6642701376811861,
    "rmse": 0.6642701376811861
}
[08/28/2025 07:39:47 INFO]: Test accuracy: {
    "score": -0.8718713041746965,
    "rmse": 0.8718713041746965
}
[08/28/2025 07:39:47 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_116",
    "best_epoch": 17,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8718713041746965,
        "rmse": 0.8718713041746965
    },
    "train_stats": {
        "score": -1.0029727598530007,
        "rmse": 1.0029727598530007
    },
    "val_stats": {
        "score": -0.6642701376811861,
        "rmse": 0.6642701376811861
    }
}
[08/28/2025 07:39:47 INFO]: Procewss finished for trial unhanged-Shanesha_trial_116
[08/28/2025 07:39:47 INFO]: 
_________________________________________________

[08/28/2025 07:39:47 INFO]: train_net_for_optune.py main() running.
[08/28/2025 07:39:47 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.660797800041394
  attention_dropout: 0.3094524854142183
  ffn_dropout: 0.3094524854142183
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00012271374351209623
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_123

[08/28/2025 07:39:47 INFO]: This ft_transformer has 2.540 million parameters.
[08/28/2025 07:39:47 INFO]: Training will start at epoch 0.
[08/28/2025 07:39:47 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 07:39:58 INFO]: Training loss at epoch 30: 0.9460931420326233
[08/28/2025 07:39:58 INFO]: Training loss at epoch 5: 1.2672637104988098
[08/28/2025 07:39:58 INFO]: Training loss at epoch 3: 0.9982348382472992
[08/28/2025 07:39:59 INFO]: Training loss at epoch 2: 1.4908708930015564
[08/28/2025 07:40:05 INFO]: Training loss at epoch 14: 0.9717132747173309
[08/28/2025 07:40:10 INFO]: Training loss at epoch 38: 1.0402314066886902
[08/28/2025 07:40:12 INFO]: New best epoch, val score: -0.6627178991676282
[08/28/2025 07:40:12 INFO]: Saving model to: unhanged-Shanesha_trial_122/model_best.pth
[08/28/2025 07:40:21 INFO]: Training loss at epoch 35: 0.8981600999832153
[08/28/2025 07:40:23 INFO]: Training loss at epoch 12: 0.9410860538482666
[08/28/2025 07:40:51 INFO]: Training loss at epoch 31: 1.1393007040023804
[08/28/2025 07:41:13 INFO]: Training loss at epoch 22: 1.154881238937378
[08/28/2025 07:41:17 INFO]: Training loss at epoch 36: 0.9004643559455872
[08/28/2025 07:41:19 INFO]: Training loss at epoch 13: 0.8637804090976715
[08/28/2025 07:41:22 INFO]: Training loss at epoch 6: 1.0018833875656128
[08/28/2025 07:41:23 INFO]: Training loss at epoch 16: 1.0243039727210999
[08/28/2025 07:41:24 INFO]: Running Final Evaluation...
[08/28/2025 07:41:26 INFO]: Training loss at epoch 52: 0.9986654818058014
[08/28/2025 07:41:31 INFO]: Training loss at epoch 0: 1.1346791982650757
[08/28/2025 07:41:43 INFO]: Training accuracy: {
    "score": -1.0122485784928204,
    "rmse": 1.0122485784928204
}
[08/28/2025 07:41:43 INFO]: Val accuracy: {
    "score": -0.6630554197617975,
    "rmse": 0.6630554197617975
}
[08/28/2025 07:41:43 INFO]: Test accuracy: {
    "score": -0.8687758068550069,
    "rmse": 0.8687758068550069
}
[08/28/2025 07:41:43 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_117",
    "best_epoch": 5,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8687758068550069,
        "rmse": 0.8687758068550069
    },
    "train_stats": {
        "score": -1.0122485784928204,
        "rmse": 1.0122485784928204
    },
    "val_stats": {
        "score": -0.6630554197617975,
        "rmse": 0.6630554197617975
    }
}
[08/28/2025 07:41:43 INFO]: Procewss finished for trial unhanged-Shanesha_trial_117
[08/28/2025 07:41:43 INFO]: 
_________________________________________________

[08/28/2025 07:41:43 INFO]: train_net_for_optune.py main() running.
[08/28/2025 07:41:43 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.6514257289764385
  attention_dropout: 0.15726625016663587
  ffn_dropout: 0.15726625016663587
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00012510639599568117
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_124

[08/28/2025 07:41:43 INFO]: This ft_transformer has 2.534 million parameters.
[08/28/2025 07:41:43 INFO]: Training will start at epoch 0.
[08/28/2025 07:41:43 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 07:41:46 INFO]: New best epoch, val score: -0.6832278121297072
[08/28/2025 07:41:46 INFO]: Saving model to: unhanged-Shanesha_trial_123/model_best.pth
[08/28/2025 07:41:46 INFO]: Training loss at epoch 32: 0.9059081077575684
[08/28/2025 07:41:52 INFO]: Training loss at epoch 3: 1.17917138338089
[08/28/2025 07:42:09 INFO]: Training loss at epoch 28: 0.9328181445598602
[08/28/2025 07:42:16 INFO]: Training loss at epoch 14: 1.0900172591209412
[08/28/2025 07:42:18 INFO]: Training loss at epoch 38: 0.8378495275974274
[08/28/2025 07:42:37 INFO]: Training loss at epoch 4: 1.3139132261276245
[08/28/2025 07:42:42 INFO]: Training loss at epoch 33: 0.9717928767204285
[08/28/2025 07:42:46 INFO]: Training loss at epoch 69: 0.9540061354637146
[08/28/2025 07:42:48 INFO]: Training loss at epoch 7: 1.1992194056510925
[08/28/2025 07:42:51 INFO]: Training loss at epoch 106: 1.0273178815841675
[08/28/2025 07:42:51 INFO]: Training loss at epoch 79: 0.9573642611503601
[08/28/2025 07:42:55 INFO]: Training loss at epoch 39: 1.1749160885810852
[08/28/2025 07:43:10 INFO]: Training loss at epoch 47: 1.0111547708511353
[08/28/2025 07:43:12 INFO]: Training loss at epoch 15: 0.9090730547904968
[08/28/2025 07:43:17 INFO]: Training loss at epoch 57: 0.8242128193378448
[08/28/2025 07:43:30 INFO]: Training loss at epoch 0: 1.002224177122116
[08/28/2025 07:43:33 INFO]: Training loss at epoch 1: 1.273328959941864
[08/28/2025 07:43:37 INFO]: Training loss at epoch 34: 0.9967511892318726
[08/28/2025 07:43:44 INFO]: New best epoch, val score: -0.6668109247861977
[08/28/2025 07:43:44 INFO]: Saving model to: unhanged-Shanesha_trial_124/model_best.pth
[08/28/2025 07:43:48 INFO]: Training loss at epoch 4: 1.1262829899787903
[08/28/2025 07:43:53 INFO]: Training stats: {
    "score": -1.0058777034003452,
    "rmse": 1.0058777034003452
}
[08/28/2025 07:43:53 INFO]: Val stats: {
    "score": -0.7402373940365391,
    "rmse": 0.7402373940365391
}
[08/28/2025 07:43:53 INFO]: Test stats: {
    "score": -0.9094563118191396,
    "rmse": 0.9094563118191396
}
[08/28/2025 07:44:00 INFO]: Training loss at epoch 23: 1.0397136211395264
[08/28/2025 07:44:07 INFO]: Training loss at epoch 16: 0.9560396373271942
[08/28/2025 07:44:15 INFO]: Training loss at epoch 8: 1.2002175450325012
[08/28/2025 07:44:32 INFO]: Training loss at epoch 35: 0.9591453075408936
[08/28/2025 07:44:44 INFO]: Training stats: {
    "score": -0.996358363178357,
    "rmse": 0.996358363178357
}
[08/28/2025 07:44:44 INFO]: Val stats: {
    "score": -0.6752468558154425,
    "rmse": 0.6752468558154425
}
[08/28/2025 07:44:44 INFO]: Test stats: {
    "score": -0.8716360082220662,
    "rmse": 0.8716360082220662
}
[08/28/2025 07:45:00 INFO]: Training loss at epoch 29: 0.870359480381012
[08/28/2025 07:45:00 INFO]: Training loss at epoch 22: 0.8642315566539764
[08/28/2025 07:45:03 INFO]: Training loss at epoch 17: 1.0157307088375092
[08/28/2025 07:45:03 INFO]: Training stats: {
    "score": -1.0000215506222414,
    "rmse": 1.0000215506222414
}
[08/28/2025 07:45:03 INFO]: Val stats: {
    "score": -0.6752980222462143,
    "rmse": 0.6752980222462143
}
[08/28/2025 07:45:03 INFO]: Test stats: {
    "score": -0.8717411933613952,
    "rmse": 0.8717411933613952
}
[08/28/2025 07:45:04 INFO]: Training loss at epoch 39: 1.157639980316162
[08/28/2025 07:45:17 INFO]: Training loss at epoch 5: 1.0346636772155762
[08/28/2025 07:45:27 INFO]: Training loss at epoch 36: 1.0192700922489166
[08/28/2025 07:45:29 INFO]: Training loss at epoch 1: 1.2739611864089966
[08/28/2025 07:45:33 INFO]: Training loss at epoch 2: 1.1289767920970917
[08/28/2025 07:45:40 INFO]: Training loss at epoch 9: 1.221662700176239
[08/28/2025 07:45:41 INFO]: Training loss at epoch 5: 0.8678750693798065
[08/28/2025 07:45:54 INFO]: Training stats: {
    "score": -0.9970188944526676,
    "rmse": 0.9970188944526676
}
[08/28/2025 07:45:54 INFO]: Val stats: {
    "score": -0.695374909351042,
    "rmse": 0.695374909351042
}
[08/28/2025 07:45:54 INFO]: Test stats: {
    "score": -0.8842113665276405,
    "rmse": 0.8842113665276405
}
[08/28/2025 07:45:56 INFO]: Training loss at epoch 18: 0.900538980960846
[08/28/2025 07:45:57 INFO]: Training stats: {
    "score": -1.002033808617683,
    "rmse": 1.002033808617683
}
[08/28/2025 07:45:57 INFO]: Val stats: {
    "score": -0.6651314590998729,
    "rmse": 0.6651314590998729
}
[08/28/2025 07:45:57 INFO]: Test stats: {
    "score": -0.8672983800845219,
    "rmse": 0.8672983800845219
}
[08/28/2025 07:46:07 INFO]: Training stats: {
    "score": -1.1713418004021077,
    "rmse": 1.1713418004021077
}
[08/28/2025 07:46:07 INFO]: Val stats: {
    "score": -1.0124590876008301,
    "rmse": 1.0124590876008301
}
[08/28/2025 07:46:07 INFO]: Test stats: {
    "score": -1.1202356634819755,
    "rmse": 1.1202356634819755
}
[08/28/2025 07:46:16 INFO]: Training loss at epoch 37: 1.0379443764686584
[08/28/2025 07:46:31 INFO]: Training loss at epoch 40: 1.0672764778137207
[08/28/2025 07:46:37 INFO]: Training loss at epoch 24: 1.1462719142436981
[08/28/2025 07:46:46 INFO]: Training loss at epoch 19: 0.9760305881500244
[08/28/2025 07:47:01 INFO]: Training loss at epoch 53: 1.20159512758255
[08/28/2025 07:47:03 INFO]: Training stats: {
    "score": -1.0057236310560187,
    "rmse": 1.0057236310560187
}
[08/28/2025 07:47:03 INFO]: Val stats: {
    "score": -0.7236313473025914,
    "rmse": 0.7236313473025914
}
[08/28/2025 07:47:03 INFO]: Test stats: {
    "score": -0.8986985947839082,
    "rmse": 0.8986985947839082
}
[08/28/2025 07:47:06 INFO]: Training loss at epoch 38: 1.0436312854290009
[08/28/2025 07:47:08 INFO]: Training loss at epoch 15: 1.043377161026001
[08/28/2025 07:47:21 INFO]: Training loss at epoch 2: 0.95774245262146
[08/28/2025 07:47:23 INFO]: Training loss at epoch 3: 0.959458589553833
[08/28/2025 07:47:25 INFO]: Training loss at epoch 10: 1.3270838856697083
[08/28/2025 07:47:26 INFO]: Training loss at epoch 6: 1.0389347076416016
[08/28/2025 07:47:41 INFO]: Training loss at epoch 6: 0.8694676160812378
[08/28/2025 07:47:53 INFO]: Training loss at epoch 20: 1.0266865193843842
[08/28/2025 07:47:55 INFO]: Training loss at epoch 39: 1.145873785018921
[08/28/2025 07:48:12 INFO]: Training stats: {
    "score": -1.0050632556529646,
    "rmse": 1.0050632556529646
}
[08/28/2025 07:48:12 INFO]: Val stats: {
    "score": -0.7274672543620013,
    "rmse": 0.7274672543620013
}
[08/28/2025 07:48:12 INFO]: Test stats: {
    "score": -0.8980996815411239,
    "rmse": 0.8980996815411239
}
[08/28/2025 07:48:20 INFO]: Training loss at epoch 48: 1.0506474375724792
[08/28/2025 07:48:26 INFO]: Training loss at epoch 40: 0.9519367814064026
[08/28/2025 07:48:28 INFO]: Training loss at epoch 30: 1.0771115720272064
[08/28/2025 07:48:29 INFO]: Training loss at epoch 58: 0.959038645029068
[08/28/2025 07:48:43 INFO]: Training loss at epoch 11: 1.2149922847747803
[08/28/2025 07:48:43 INFO]: Training loss at epoch 21: 1.0067148208618164
[08/28/2025 07:49:01 INFO]: Training loss at epoch 41: 1.1039795875549316
[08/28/2025 07:49:04 INFO]: Training loss at epoch 40: 1.1789435744285583
[08/28/2025 07:49:08 INFO]: Training loss at epoch 25: 1.2242467403411865
[08/28/2025 07:49:13 INFO]: Training loss at epoch 7: 0.9984792172908783
[08/28/2025 07:49:15 INFO]: Training loss at epoch 3: 0.9391814768314362
[08/28/2025 07:49:15 INFO]: Training loss at epoch 17: 1.0768924951553345
[08/28/2025 07:49:16 INFO]: Training loss at epoch 4: 1.2818974256515503
[08/28/2025 07:49:29 INFO]: New best epoch, val score: -0.668848473116536
[08/28/2025 07:49:29 INFO]: Saving model to: unhanged-Shanesha_trial_123/model_best.pth
[08/28/2025 07:49:36 INFO]: Training loss at epoch 22: 0.8206306397914886
[08/28/2025 07:49:42 INFO]: Training loss at epoch 107: 0.8395822048187256
[08/28/2025 07:49:55 INFO]: Training loss at epoch 70: 1.1364503502845764
[08/28/2025 07:49:56 INFO]: Training loss at epoch 41: 0.9430691599845886
[08/28/2025 07:50:02 INFO]: Running Final Evaluation...
[08/28/2025 07:50:06 INFO]: Training loss at epoch 12: 1.3168097734451294
[08/28/2025 07:50:10 INFO]: Training loss at epoch 7: 1.0308024883270264
[08/28/2025 07:50:21 INFO]: Training accuracy: {
    "score": -1.0196604899454416,
    "rmse": 1.0196604899454416
}
[08/28/2025 07:50:21 INFO]: Val accuracy: {
    "score": -0.6674306845963517,
    "rmse": 0.6674306845963517
}
[08/28/2025 07:50:21 INFO]: Test accuracy: {
    "score": -0.8736490016792304,
    "rmse": 0.8736490016792304
}
[08/28/2025 07:50:21 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_118",
    "best_epoch": 10,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8736490016792304,
        "rmse": 0.8736490016792304
    },
    "train_stats": {
        "score": -1.0196604899454416,
        "rmse": 1.0196604899454416
    },
    "val_stats": {
        "score": -0.6674306845963517,
        "rmse": 0.6674306845963517
    }
}
[08/28/2025 07:50:21 INFO]: Procewss finished for trial unhanged-Shanesha_trial_118
[08/28/2025 07:50:22 INFO]: 
_________________________________________________

[08/28/2025 07:50:22 INFO]: train_net_for_optune.py main() running.
[08/28/2025 07:50:22 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.662475388790492
  attention_dropout: 0.15258062324639463
  ffn_dropout: 0.15258062324639463
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00012544420499913187
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_125

[08/28/2025 07:50:22 INFO]: This ft_transformer has 2.540 million parameters.
[08/28/2025 07:50:22 INFO]: Training will start at epoch 0.
[08/28/2025 07:50:22 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 07:50:32 INFO]: Training loss at epoch 23: 1.1307891607284546
[08/28/2025 07:50:46 INFO]: Training loss at epoch 23: 1.0160866975784302
[08/28/2025 07:51:05 INFO]: Training loss at epoch 8: 1.1379718482494354
[08/28/2025 07:51:08 INFO]: Training loss at epoch 41: 0.9782309532165527
[08/28/2025 07:51:10 INFO]: Training loss at epoch 80: 1.0752544701099396
[08/28/2025 07:51:13 INFO]: Training loss at epoch 4: 1.231501430273056
[08/28/2025 07:51:14 INFO]: Training loss at epoch 31: 0.9790235161781311
[08/28/2025 07:51:15 INFO]: Training loss at epoch 5: 0.9040021896362305
[08/28/2025 07:51:28 INFO]: Training loss at epoch 24: 0.9758807718753815
[08/28/2025 07:51:29 INFO]: New best epoch, val score: -0.6668557121927564
[08/28/2025 07:51:29 INFO]: Saving model to: unhanged-Shanesha_trial_123/model_best.pth
[08/28/2025 07:51:33 INFO]: Training loss at epoch 13: 1.1802470684051514
[08/28/2025 07:51:47 INFO]: Training loss at epoch 42: 0.9670815765857697
[08/28/2025 07:51:50 INFO]: Training loss at epoch 26: 0.8936847448348999
[08/28/2025 07:52:07 INFO]: Training loss at epoch 0: 1.4712164998054504
[08/28/2025 07:52:21 INFO]: New best epoch, val score: -0.9483720629009952
[08/28/2025 07:52:21 INFO]: Saving model to: unhanged-Shanesha_trial_125/model_best.pth
[08/28/2025 07:52:25 INFO]: Training loss at epoch 25: 1.1317675709724426
[08/28/2025 07:52:33 INFO]: Training loss at epoch 54: 1.2222518026828766
[08/28/2025 07:52:51 INFO]: Training loss at epoch 8: 1.2187480330467224
[08/28/2025 07:53:00 INFO]: Training loss at epoch 14: 1.0635677576065063
[08/28/2025 07:53:02 INFO]: Training loss at epoch 9: 1.1909593641757965
[08/28/2025 07:53:14 INFO]: Training loss at epoch 5: 1.2427199184894562
[08/28/2025 07:53:17 INFO]: Training loss at epoch 6: 1.1811543107032776
[08/28/2025 07:53:22 INFO]: Training loss at epoch 26: 1.1389612555503845
[08/28/2025 07:53:31 INFO]: New best epoch, val score: -0.6643647784831865
[08/28/2025 07:53:31 INFO]: Saving model to: unhanged-Shanesha_trial_123/model_best.pth
[08/28/2025 07:53:43 INFO]: Training stats: {
    "score": -0.9984348466117325,
    "rmse": 0.9984348466117325
}
[08/28/2025 07:53:43 INFO]: Val stats: {
    "score": -0.6860289605738407,
    "rmse": 0.6860289605738407
}
[08/28/2025 07:53:43 INFO]: Test stats: {
    "score": -0.8753417846270714,
    "rmse": 0.8753417846270714
}
[08/28/2025 07:53:46 INFO]: Training loss at epoch 49: 0.9206425547599792
[08/28/2025 07:53:53 INFO]: Training loss at epoch 59: 0.970856249332428
[08/28/2025 07:53:55 INFO]: Training loss at epoch 42: 0.8532329499721527
[08/28/2025 07:54:06 INFO]: Training loss at epoch 32: 0.9056190848350525
[08/28/2025 07:54:08 INFO]: Training loss at epoch 1: 1.0681382417678833
[08/28/2025 07:54:10 INFO]: Training loss at epoch 16: 1.0019844472408295
[08/28/2025 07:54:15 INFO]: Running Final Evaluation...
[08/28/2025 07:54:18 INFO]: Training loss at epoch 27: 0.9641421735286713
[08/28/2025 07:54:26 INFO]: Training loss at epoch 15: 1.091758370399475
[08/28/2025 07:54:32 INFO]: Training loss at epoch 43: 1.0156921744346619
[08/28/2025 07:54:37 INFO]: Training loss at epoch 27: 0.9244722127914429
[08/28/2025 07:55:11 INFO]: Training loss at epoch 28: 1.1331230401992798
[08/28/2025 07:55:12 INFO]: Training loss at epoch 6: 0.9346935749053955
[08/28/2025 07:55:15 INFO]: Training loss at epoch 7: 0.89790278673172
[08/28/2025 07:55:19 INFO]: Training accuracy: {
    "score": -1.0226270000442583,
    "rmse": 1.0226270000442583
}
[08/28/2025 07:55:19 INFO]: Val accuracy: {
    "score": -0.6563641036753545,
    "rmse": 0.6563641036753545
}
[08/28/2025 07:55:19 INFO]: Test accuracy: {
    "score": -0.8709746827549883,
    "rmse": 0.8709746827549883
}
[08/28/2025 07:55:19 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_112",
    "best_epoch": 11,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8709746827549883,
        "rmse": 0.8709746827549883
    },
    "train_stats": {
        "score": -1.0226270000442583,
        "rmse": 1.0226270000442583
    },
    "val_stats": {
        "score": -0.6563641036753545,
        "rmse": 0.6563641036753545
    }
}
[08/28/2025 07:55:19 INFO]: Procewss finished for trial unhanged-Shanesha_trial_112
[08/28/2025 07:55:19 INFO]: 
_________________________________________________

[08/28/2025 07:55:19 INFO]: train_net_for_optune.py main() running.
[08/28/2025 07:55:19 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.6014544120918215
  attention_dropout: 0.15521000106641947
  ffn_dropout: 0.15521000106641947
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.253192227392399e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_126

[08/28/2025 07:55:20 INFO]: This ft_transformer has 5.570 million parameters.
[08/28/2025 07:55:20 INFO]: Training will start at epoch 0.
[08/28/2025 07:55:20 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 07:55:25 INFO]: Training loss at epoch 71: 0.9323478639125824
[08/28/2025 07:55:27 INFO]: Training loss at epoch 9: 0.981878250837326
[08/28/2025 07:55:29 INFO]: New best epoch, val score: -0.6621113985993602
[08/28/2025 07:55:29 INFO]: Saving model to: unhanged-Shanesha_trial_123/model_best.pth
[08/28/2025 07:55:34 INFO]: Training stats: {
    "score": -0.9935057087172041,
    "rmse": 0.9935057087172041
}
[08/28/2025 07:55:34 INFO]: Val stats: {
    "score": -0.6809477867612984,
    "rmse": 0.6809477867612984
}
[08/28/2025 07:55:34 INFO]: Test stats: {
    "score": -0.8768105958341577,
    "rmse": 0.8768105958341577
}
[08/28/2025 07:55:34 INFO]: Training loss at epoch 10: 1.0630362629890442
[08/28/2025 07:55:38 INFO]: Training stats: {
    "score": -1.0000188226403997,
    "rmse": 1.0000188226403997
}
[08/28/2025 07:55:38 INFO]: Val stats: {
    "score": -0.6759878787455625,
    "rmse": 0.6759878787455625
}
[08/28/2025 07:55:38 INFO]: Test stats: {
    "score": -0.872060193857981,
    "rmse": 0.872060193857981
}
[08/28/2025 07:55:46 INFO]: Training loss at epoch 16: 1.0537522435188293
[08/28/2025 07:56:00 INFO]: Training loss at epoch 2: 1.8678211569786072
[08/28/2025 07:56:02 INFO]: Training loss at epoch 29: 1.1311036348342896
[08/28/2025 07:56:13 INFO]: New best epoch, val score: -0.7116021589312508
[08/28/2025 07:56:13 INFO]: Saving model to: unhanged-Shanesha_trial_125/model_best.pth
[08/28/2025 07:56:16 INFO]: Training stats: {
    "score": -0.9971895180891814,
    "rmse": 0.9971895180891814
}
[08/28/2025 07:56:16 INFO]: Val stats: {
    "score": -0.7016942000708369,
    "rmse": 0.7016942000708369
}
[08/28/2025 07:56:16 INFO]: Test stats: {
    "score": -0.8871145392798384,
    "rmse": 0.8871145392798384
}
[08/28/2025 07:56:19 INFO]: Training stats: {
    "score": -0.9964971152505249,
    "rmse": 0.9964971152505249
}
[08/28/2025 07:56:19 INFO]: Val stats: {
    "score": -0.6819719443745667,
    "rmse": 0.6819719443745667
}
[08/28/2025 07:56:19 INFO]: Test stats: {
    "score": -0.8746862744316175,
    "rmse": 0.8746862744316175
}
[08/28/2025 07:56:43 INFO]: Training loss at epoch 33: 1.0869112610816956
[08/28/2025 07:56:43 INFO]: Training loss at epoch 108: 1.1467308700084686
[08/28/2025 07:56:45 INFO]: Training loss at epoch 24: 1.0449707210063934
[08/28/2025 07:57:01 INFO]: Training loss at epoch 7: 0.8921709060668945
[08/28/2025 07:57:03 INFO]: Training loss at epoch 44: 1.0929223895072937
[08/28/2025 07:57:04 INFO]: Training loss at epoch 17: 1.111872136592865
[08/28/2025 07:57:06 INFO]: Training loss at epoch 8: 1.398452490568161
[08/28/2025 07:57:07 INFO]: Training loss at epoch 28: 1.0034671425819397
[08/28/2025 07:57:09 INFO]: Training loss at epoch 30: 1.1516530215740204
[08/28/2025 07:57:10 INFO]: Training loss at epoch 18: 1.0058743953704834
[08/28/2025 07:57:13 INFO]: New best epoch, val score: -0.6696834689364773
[08/28/2025 07:57:13 INFO]: Saving model to: unhanged-Shanesha_trial_121/model_best.pth
[08/28/2025 07:57:19 INFO]: Training loss at epoch 11: 0.8456261157989502
[08/28/2025 07:57:19 INFO]: New best epoch, val score: -0.6603519198432264
[08/28/2025 07:57:19 INFO]: Saving model to: unhanged-Shanesha_trial_123/model_best.pth
[08/28/2025 07:57:25 INFO]: Training loss at epoch 81: 1.0077058672904968
[08/28/2025 07:57:50 INFO]: Training loss at epoch 3: 1.0541513860225677
[08/28/2025 07:57:56 INFO]: Training loss at epoch 0: 2.108091115951538
[08/28/2025 07:57:57 INFO]: Training loss at epoch 55: 0.9018611013889313
[08/28/2025 07:58:01 INFO]: Training loss at epoch 31: 1.0003977119922638
[08/28/2025 07:58:07 INFO]: Running Final Evaluation...
[08/28/2025 07:58:20 INFO]: New best epoch, val score: -1.0138367210950652
[08/28/2025 07:58:20 INFO]: Saving model to: unhanged-Shanesha_trial_126/model_best.pth
[08/28/2025 07:58:25 INFO]: Training loss at epoch 18: 1.259425401687622
[08/28/2025 07:58:27 INFO]: Training accuracy: {
    "score": -1.0081851987176795,
    "rmse": 1.0081851987176795
}
[08/28/2025 07:58:27 INFO]: Val accuracy: {
    "score": -0.6606745731170452,
    "rmse": 0.6606745731170452
}
[08/28/2025 07:58:27 INFO]: Test accuracy: {
    "score": -0.8698882541642534,
    "rmse": 0.8698882541642534
}
[08/28/2025 07:58:27 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_119",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8698882541642534,
        "rmse": 0.8698882541642534
    },
    "train_stats": {
        "score": -1.0081851987176795,
        "rmse": 1.0081851987176795
    },
    "val_stats": {
        "score": -0.6606745731170452,
        "rmse": 0.6606745731170452
    }
}
[08/28/2025 07:58:27 INFO]: Procewss finished for trial unhanged-Shanesha_trial_119
[08/28/2025 07:58:27 INFO]: 
_________________________________________________

[08/28/2025 07:58:27 INFO]: train_net_for_optune.py main() running.
[08/28/2025 07:58:27 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.2403873423271516
  attention_dropout: 0.3585700778707154
  ffn_dropout: 0.3585700778707154
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.004911339174879e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_127

[08/28/2025 07:58:27 INFO]: This ft_transformer has 5.146 million parameters.
[08/28/2025 07:58:27 INFO]: Training will start at epoch 0.
[08/28/2025 07:58:27 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 07:58:35 INFO]: New best epoch, val score: -0.6663653622013519
[08/28/2025 07:58:35 INFO]: Saving model to: unhanged-Shanesha_trial_121/model_best.pth
[08/28/2025 07:58:43 INFO]: Training loss at epoch 10: 0.9560120105743408
[08/28/2025 07:58:56 INFO]: Training loss at epoch 8: 0.9962981045246124
[08/28/2025 07:59:00 INFO]: Training loss at epoch 9: 1.0380484759807587
[08/28/2025 07:59:01 INFO]: New best epoch, val score: -0.6667206467276371
[08/28/2025 07:59:01 INFO]: Saving model to: unhanged-Shanesha_trial_120/model_best.pth
[08/28/2025 07:59:09 INFO]: Training loss at epoch 12: 1.0323414206504822
[08/28/2025 07:59:22 INFO]: Training loss at epoch 34: 1.003831684589386
[08/28/2025 07:59:40 INFO]: Training loss at epoch 45: 1.023164689540863
[08/28/2025 07:59:42 INFO]: Training stats: {
    "score": -1.0139504116762676,
    "rmse": 1.0139504116762676
}
[08/28/2025 07:59:42 INFO]: Val stats: {
    "score": -0.6592744583962583,
    "rmse": 0.6592744583962583
}
[08/28/2025 07:59:42 INFO]: Test stats: {
    "score": -0.8758558314201335,
    "rmse": 0.8758558314201335
}
[08/28/2025 07:59:43 INFO]: Training loss at epoch 29: 0.951903223991394
[08/28/2025 07:59:46 INFO]: Training loss at epoch 4: 1.1244495511054993
[08/28/2025 07:59:49 INFO]: Training loss at epoch 19: 1.0843015909194946
[08/28/2025 07:59:56 INFO]: New best epoch, val score: -0.6592744583962583
[08/28/2025 07:59:56 INFO]: Saving model to: unhanged-Shanesha_trial_123/model_best.pth
[08/28/2025 08:00:19 INFO]: Training stats: {
    "score": -1.0044344905771196,
    "rmse": 1.0044344905771196
}
[08/28/2025 08:00:19 INFO]: Val stats: {
    "score": -0.6651864622202321,
    "rmse": 0.6651864622202321
}
[08/28/2025 08:00:19 INFO]: Test stats: {
    "score": -0.8711747289647668,
    "rmse": 0.8711747289647668
}
[08/28/2025 08:00:29 INFO]: New best epoch, val score: -0.6651864622202321
[08/28/2025 08:00:29 INFO]: Saving model to: unhanged-Shanesha_trial_121/model_best.pth
[08/28/2025 08:00:40 INFO]: Training stats: {
    "score": -0.9979692877334453,
    "rmse": 0.9979692877334453
}
[08/28/2025 08:00:40 INFO]: Val stats: {
    "score": -0.6753612199913693,
    "rmse": 0.6753612199913693
}
[08/28/2025 08:00:40 INFO]: Test stats: {
    "score": -0.8802994242305329,
    "rmse": 0.8802994242305329
}
[08/28/2025 08:00:40 INFO]: Training loss at epoch 72: 1.0006321668624878
[08/28/2025 08:00:45 INFO]: Training loss at epoch 50: 0.9185319244861603
[08/28/2025 08:00:46 INFO]: Training loss at epoch 60: 1.196833610534668
[08/28/2025 08:00:56 INFO]: Training loss at epoch 9: 0.8946517109870911
[08/28/2025 08:01:00 INFO]: Training loss at epoch 17: 1.0236223340034485
[08/28/2025 08:01:03 INFO]: Training loss at epoch 13: 0.9570281207561493
[08/28/2025 08:01:06 INFO]: Training loss at epoch 0: 1.6652936339378357
[08/28/2025 08:01:07 INFO]: Training loss at epoch 1: 1.6499053835868835
[08/28/2025 08:01:22 INFO]: Training loss at epoch 11: 0.910485565662384
[08/28/2025 08:01:29 INFO]: New best epoch, val score: -1.0670926798336424
[08/28/2025 08:01:29 INFO]: Saving model to: unhanged-Shanesha_trial_127/model_best.pth
[08/28/2025 08:01:31 INFO]: New best epoch, val score: -0.7179504760067082
[08/28/2025 08:01:31 INFO]: Saving model to: unhanged-Shanesha_trial_126/model_best.pth
[08/28/2025 08:01:40 INFO]: Training stats: {
    "score": -0.996745783895834,
    "rmse": 0.996745783895834
}
[08/28/2025 08:01:40 INFO]: Val stats: {
    "score": -0.6745842284991618,
    "rmse": 0.6745842284991618
}
[08/28/2025 08:01:40 INFO]: Test stats: {
    "score": -0.8714244698590186,
    "rmse": 0.8714244698590186
}
[08/28/2025 08:01:44 INFO]: New best epoch, val score: -0.6619777401318342
[08/28/2025 08:01:44 INFO]: Saving model to: unhanged-Shanesha_trial_120/model_best.pth
[08/28/2025 08:01:46 INFO]: Training loss at epoch 10: 0.8655890822410583
[08/28/2025 08:01:48 INFO]: Training loss at epoch 5: 1.122846782207489
[08/28/2025 08:01:49 INFO]: Training loss at epoch 20: 0.9410047829151154
[08/28/2025 08:01:59 INFO]: New best epoch, val score: -0.6647014101918193
[08/28/2025 08:01:59 INFO]: Saving model to: unhanged-Shanesha_trial_121/model_best.pth
[08/28/2025 08:02:14 INFO]: Training loss at epoch 35: 0.8866692781448364
[08/28/2025 08:02:27 INFO]: Training loss at epoch 46: 0.9563565254211426
[08/28/2025 08:02:48 INFO]: Training loss at epoch 25: 0.8428476750850677
[08/28/2025 08:03:00 INFO]: Training loss at epoch 14: 0.9935316443443298
[08/28/2025 08:03:16 INFO]: Training loss at epoch 21: 1.0637571215629578
[08/28/2025 08:03:27 INFO]: Training loss at epoch 30: 1.034921944141388
[08/28/2025 08:03:40 INFO]: Training loss at epoch 56: 0.94161057472229
[08/28/2025 08:03:42 INFO]: Training loss at epoch 10: 1.1613627672195435
[08/28/2025 08:03:48 INFO]: Training loss at epoch 109: 0.8947857320308685
[08/28/2025 08:03:48 INFO]: Training loss at epoch 11: 1.0673601031303406
[08/28/2025 08:03:48 INFO]: Training loss at epoch 6: 1.1441142559051514
[08/28/2025 08:03:51 INFO]: Training loss at epoch 82: 0.8788428902626038
[08/28/2025 08:03:56 INFO]: New best epoch, val score: -0.6638213742544488
[08/28/2025 08:03:56 INFO]: Saving model to: unhanged-Shanesha_trial_124/model_best.pth
[08/28/2025 08:04:03 INFO]: Training loss at epoch 12: 1.0236915349960327
[08/28/2025 08:04:11 INFO]: Training loss at epoch 1: 1.515852451324463
[08/28/2025 08:04:22 INFO]: Training loss at epoch 2: 1.131040334701538
[08/28/2025 08:04:31 INFO]: New best epoch, val score: -0.8125554952967041
[08/28/2025 08:04:31 INFO]: Saving model to: unhanged-Shanesha_trial_127/model_best.pth
[08/28/2025 08:04:38 INFO]: Training loss at epoch 22: 1.170969307422638
[08/28/2025 08:04:51 INFO]: Training loss at epoch 15: 0.8609498143196106
[08/28/2025 08:04:57 INFO]: Training loss at epoch 36: 0.8172770142555237
[08/28/2025 08:05:06 INFO]: Training loss at epoch 47: 1.097385734319687
[08/28/2025 08:05:09 INFO]: Training loss at epoch 19: 1.24271959066391
[08/28/2025 08:05:32 INFO]: Training loss at epoch 11: 1.431515097618103
[08/28/2025 08:05:40 INFO]: Training loss at epoch 7: 1.027467280626297
[08/28/2025 08:05:40 INFO]: Training loss at epoch 12: 1.3075807094573975
[08/28/2025 08:05:56 INFO]: Training loss at epoch 23: 1.0804564952850342
[08/28/2025 08:05:59 INFO]: Training loss at epoch 31: 0.8699617981910706
[08/28/2025 08:06:02 INFO]: Training loss at epoch 61: 1.2842037379741669
[08/28/2025 08:06:03 INFO]: Training stats: {
    "score": -0.9972294724860332,
    "rmse": 0.9972294724860332
}
[08/28/2025 08:06:03 INFO]: Val stats: {
    "score": -0.6789070819669499,
    "rmse": 0.6789070819669499
}
[08/28/2025 08:06:03 INFO]: Test stats: {
    "score": -0.8729989008224807,
    "rmse": 0.8729989008224807
}
[08/28/2025 08:06:05 INFO]: Training loss at epoch 73: 0.9896632134914398
[08/28/2025 08:06:06 INFO]: Training loss at epoch 51: 0.8997392952442169
[08/28/2025 08:06:30 INFO]: Training loss at epoch 13: 1.0456013679504395
[08/28/2025 08:06:36 INFO]: Training loss at epoch 16: 1.0064119696617126
[08/28/2025 08:06:37 INFO]: New best epoch, val score: -0.6605124391860842
[08/28/2025 08:06:37 INFO]: Saving model to: unhanged-Shanesha_trial_90/model_best.pth
[08/28/2025 08:06:59 INFO]: Training loss at epoch 2: 1.0558867156505585
[08/28/2025 08:07:14 INFO]: Training loss at epoch 24: 0.7668600976467133
[08/28/2025 08:07:19 INFO]: New best epoch, val score: -0.6585648781303595
[08/28/2025 08:07:19 INFO]: Saving model to: unhanged-Shanesha_trial_127/model_best.pth
[08/28/2025 08:07:20 INFO]: Training loss at epoch 3: 0.9088786244392395
[08/28/2025 08:07:22 INFO]: Training loss at epoch 12: 0.9703146815299988
[08/28/2025 08:07:29 INFO]: Training loss at epoch 8: 1.164023756980896
[08/28/2025 08:07:30 INFO]: Training loss at epoch 37: 0.9979941546916962
[08/28/2025 08:07:31 INFO]: Training loss at epoch 13: 1.0188517272472382
[08/28/2025 08:07:36 INFO]: Training loss at epoch 48: 0.9882773458957672
[08/28/2025 08:07:39 INFO]: Training stats: {
    "score": -1.0036636149409939,
    "rmse": 1.0036636149409939
}
[08/28/2025 08:07:39 INFO]: Val stats: {
    "score": -0.6530485628212539,
    "rmse": 0.6530485628212539
}
[08/28/2025 08:07:39 INFO]: Test stats: {
    "score": -0.8683314435547429,
    "rmse": 0.8683314435547429
}
[08/28/2025 08:07:51 INFO]: Training loss at epoch 18: 1.0651525855064392
[08/28/2025 08:08:25 INFO]: Training loss at epoch 17: 0.9235006868839264
[08/28/2025 08:08:33 INFO]: Training loss at epoch 32: 1.046402931213379
[08/28/2025 08:08:34 INFO]: Training loss at epoch 26: 1.0189072489738464
[08/28/2025 08:08:37 INFO]: Training loss at epoch 25: 0.9152108728885651
[08/28/2025 08:08:51 INFO]: New best epoch, val score: -0.6563471692428504
[08/28/2025 08:08:51 INFO]: Saving model to: unhanged-Shanesha_trial_115/model_best.pth
[08/28/2025 08:09:00 INFO]: Training loss at epoch 57: 1.1669700741767883
[08/28/2025 08:09:02 INFO]: Training loss at epoch 14: 0.9271770417690277
[08/28/2025 08:09:19 INFO]: Training loss at epoch 13: 0.8299954235553741
[08/28/2025 08:09:27 INFO]: Training loss at epoch 9: 0.9225968718528748
[08/28/2025 08:09:30 INFO]: Training loss at epoch 14: 1.215929627418518
[08/28/2025 08:09:42 INFO]: Running Final Evaluation...
[08/28/2025 08:09:55 INFO]: Training loss at epoch 83: 1.0741398930549622
[08/28/2025 08:09:56 INFO]: Training loss at epoch 3: 1.149129033088684
[08/28/2025 08:10:01 INFO]: Training loss at epoch 26: 1.0897613167762756
[08/28/2025 08:10:07 INFO]: Training stats: {
    "score": -1.0504512710111216,
    "rmse": 1.0504512710111216
}
[08/28/2025 08:10:07 INFO]: Val stats: {
    "score": -0.821503225335102,
    "rmse": 0.821503225335102
}
[08/28/2025 08:10:07 INFO]: Test stats: {
    "score": -0.9660516914484514,
    "rmse": 0.9660516914484514
}
[08/28/2025 08:10:15 INFO]: Training loss at epoch 38: 1.0341511368751526
[08/28/2025 08:10:17 INFO]: Training loss at epoch 49: 0.7931056022644043
[08/28/2025 08:10:18 INFO]: Training loss at epoch 18: 0.9049161970615387
[08/28/2025 08:10:29 INFO]: Training loss at epoch 4: 1.6844024062156677
[08/28/2025 08:10:40 INFO]: Running Final Evaluation...
[08/28/2025 08:11:13 INFO]: Training stats: {
    "score": -0.9935812092077657,
    "rmse": 0.9935812092077657
}
[08/28/2025 08:11:13 INFO]: Val stats: {
    "score": -0.7055429210324551,
    "rmse": 0.7055429210324551
}
[08/28/2025 08:11:13 INFO]: Test stats: {
    "score": -0.8885519712191252,
    "rmse": 0.8885519712191252
}
[08/28/2025 08:11:15 INFO]: Training loss at epoch 33: 0.9975228309631348
[08/28/2025 08:11:16 INFO]: Training loss at epoch 62: 0.9498062431812286
[08/28/2025 08:11:17 INFO]: Training loss at epoch 14: 1.0008721947669983
[08/28/2025 08:11:22 INFO]: Training loss at epoch 52: 1.065601885318756
[08/28/2025 08:11:26 INFO]: Training loss at epoch 27: 0.9898157119750977
[08/28/2025 08:11:28 INFO]: Training loss at epoch 15: 1.1547251641750336
[08/28/2025 08:11:29 INFO]: Training loss at epoch 74: 1.086177498102188
[08/28/2025 08:11:35 INFO]: New best epoch, val score: -0.655179346674475
[08/28/2025 08:11:35 INFO]: Saving model to: unhanged-Shanesha_trial_115/model_best.pth
[08/28/2025 08:11:39 INFO]: Training loss at epoch 15: 1.0022628903388977
[08/28/2025 08:12:08 INFO]: Training loss at epoch 10: 1.283801257610321
[08/28/2025 08:12:11 INFO]: Training accuracy: {
    "score": -1.0128113042918232,
    "rmse": 1.0128113042918232
}
[08/28/2025 08:12:11 INFO]: Val accuracy: {
    "score": -0.6604834897907779,
    "rmse": 0.6604834897907779
}
[08/28/2025 08:12:11 INFO]: Test accuracy: {
    "score": -0.8708142201354454,
    "rmse": 0.8708142201354454
}
[08/28/2025 08:12:11 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_93",
    "best_epoch": 26,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8708142201354454,
        "rmse": 0.8708142201354454
    },
    "train_stats": {
        "score": -1.0128113042918232,
        "rmse": 1.0128113042918232
    },
    "val_stats": {
        "score": -0.6604834897907779,
        "rmse": 0.6604834897907779
    }
}
[08/28/2025 08:12:11 INFO]: Procewss finished for trial unhanged-Shanesha_trial_93
[08/28/2025 08:12:12 INFO]: 
_________________________________________________

[08/28/2025 08:12:12 INFO]: train_net_for_optune.py main() running.
[08/28/2025 08:12:12 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 1.930921204549749
  attention_dropout: 0.15124856898498323
  ffn_dropout: 0.15124856898498323
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.00012483509011111449
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_128

[08/28/2025 08:12:12 INFO]: This ft_transformer has 4.780 million parameters.
[08/28/2025 08:12:12 INFO]: Training will start at epoch 0.
[08/28/2025 08:12:12 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 08:12:14 INFO]: Training loss at epoch 19: 0.9160277545452118
[08/28/2025 08:12:22 INFO]: New best epoch, val score: -0.7082533320550833
[08/28/2025 08:12:22 INFO]: Saving model to: unhanged-Shanesha_trial_125/model_best.pth
[08/28/2025 08:12:54 INFO]: Training loss at epoch 28: 1.1812227964401245
[08/28/2025 08:12:55 INFO]: Training stats: {
    "score": -0.9983230463940631,
    "rmse": 0.9983230463940631
}
[08/28/2025 08:12:55 INFO]: Val stats: {
    "score": -0.6960803838233566,
    "rmse": 0.6960803838233566
}
[08/28/2025 08:12:55 INFO]: Test stats: {
    "score": -0.8812358260767241,
    "rmse": 0.8812358260767241
}
[08/28/2025 08:13:01 INFO]: Training loss at epoch 4: 1.4234284162521362
[08/28/2025 08:13:04 INFO]: Training loss at epoch 110: 1.1687385737895966
[08/28/2025 08:13:06 INFO]: Training loss at epoch 39: 0.8440911173820496
[08/28/2025 08:13:08 INFO]: Training accuracy: {
    "score": -1.014551225288783,
    "rmse": 1.014551225288783
}
[08/28/2025 08:13:08 INFO]: Val accuracy: {
    "score": -0.6604457563560482,
    "rmse": 0.6604457563560482
}
[08/28/2025 08:13:08 INFO]: Test accuracy: {
    "score": -0.8716460155855557,
    "rmse": 0.8716460155855557
}
[08/28/2025 08:13:08 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_64",
    "best_epoch": 52,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8716460155855557,
        "rmse": 0.8716460155855557
    },
    "train_stats": {
        "score": -1.014551225288783,
        "rmse": 1.014551225288783
    },
    "val_stats": {
        "score": -0.6604457563560482,
        "rmse": 0.6604457563560482
    }
}
[08/28/2025 08:13:08 INFO]: Procewss finished for trial unhanged-Shanesha_trial_64
[08/28/2025 08:13:08 INFO]: 
_________________________________________________

[08/28/2025 08:13:08 INFO]: train_net_for_optune.py main() running.
[08/28/2025 08:13:08 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.595476588208907
  attention_dropout: 0.15610235136338202
  ffn_dropout: 0.15610235136338202
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.0143457903740893e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_129

[08/28/2025 08:13:08 INFO]: This ft_transformer has 5.564 million parameters.
[08/28/2025 08:13:08 INFO]: Training will start at epoch 0.
[08/28/2025 08:13:08 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 08:13:16 INFO]: Training loss at epoch 15: 0.9920231997966766
[08/28/2025 08:13:28 INFO]: Training loss at epoch 16: 1.1350186169147491
[08/28/2025 08:13:40 INFO]: Training loss at epoch 5: 1.7344562411308289
[08/28/2025 08:13:54 INFO]: Training loss at epoch 50: 1.0953777432441711
[08/28/2025 08:13:56 INFO]: Training loss at epoch 34: 1.0911294221878052
[08/28/2025 08:13:58 INFO]: Training stats: {
    "score": -0.9948603323420497,
    "rmse": 0.9948603323420497
}
[08/28/2025 08:13:58 INFO]: Val stats: {
    "score": -0.6735454051033609,
    "rmse": 0.6735454051033609
}
[08/28/2025 08:13:58 INFO]: Test stats: {
    "score": -0.8744841773848402,
    "rmse": 0.8744841773848402
}
[08/28/2025 08:14:02 INFO]: Training loss at epoch 11: 1.026041328907013
[08/28/2025 08:14:12 INFO]: Training loss at epoch 16: 0.8592635691165924
[08/28/2025 08:14:13 INFO]: Training loss at epoch 29: 1.0481262803077698
[08/28/2025 08:14:16 INFO]: New best epoch, val score: -0.6622413634271409
[08/28/2025 08:14:16 INFO]: Saving model to: unhanged-Shanesha_trial_125/model_best.pth
[08/28/2025 08:14:34 INFO]: Training loss at epoch 27: 1.047587126493454
[08/28/2025 08:14:38 INFO]: Training loss at epoch 0: 1.1535342931747437
[08/28/2025 08:14:40 INFO]: Training stats: {
    "score": -1.0003605851884716,
    "rmse": 1.0003605851884716
}
[08/28/2025 08:14:40 INFO]: Val stats: {
    "score": -0.6953436727928597,
    "rmse": 0.6953436727928597
}
[08/28/2025 08:14:40 INFO]: Test stats: {
    "score": -0.8837191565586677,
    "rmse": 0.8837191565586677
}
[08/28/2025 08:14:41 INFO]: Training loss at epoch 20: 1.0312921404838562
[08/28/2025 08:14:53 INFO]: Training loss at epoch 19: 1.1094790697097778
[08/28/2025 08:14:57 INFO]: New best epoch, val score: -0.6695452608807022
[08/28/2025 08:14:57 INFO]: Saving model to: unhanged-Shanesha_trial_128/model_best.pth
[08/28/2025 08:15:06 INFO]: Training loss at epoch 16: 1.179039478302002
[08/28/2025 08:15:19 INFO]: Training loss at epoch 17: 0.9319260716438293
[08/28/2025 08:15:32 INFO]: New best epoch, val score: -0.6591580469756924
[08/28/2025 08:15:32 INFO]: Saving model to: unhanged-Shanesha_trial_123/model_best.pth
[08/28/2025 08:15:33 INFO]: Training loss at epoch 20: 0.9870508313179016
[08/28/2025 08:15:43 INFO]: Training loss at epoch 0: 1.0168812274932861
[08/28/2025 08:15:50 INFO]: Training loss at epoch 5: 1.012139916419983
[08/28/2025 08:15:52 INFO]: Training loss at epoch 12: 1.0727477967739105
[08/28/2025 08:15:58 INFO]: Training loss at epoch 30: 0.9533461928367615
[08/28/2025 08:16:04 INFO]: New best epoch, val score: -0.6991914466192619
[08/28/2025 08:16:04 INFO]: Saving model to: unhanged-Shanesha_trial_129/model_best.pth
[08/28/2025 08:16:22 INFO]: Training loss at epoch 63: 0.7770447134971619
[08/28/2025 08:16:23 INFO]: Training loss at epoch 51: 0.8306876420974731
[08/28/2025 08:16:25 INFO]: Training loss at epoch 35: 1.148279845714569
[08/28/2025 08:16:26 INFO]: Training loss at epoch 21: 0.8625281751155853
[08/28/2025 08:16:30 INFO]: Training loss at epoch 53: 0.8689708709716797
[08/28/2025 08:16:31 INFO]: Training loss at epoch 40: 0.8710217773914337
[08/28/2025 08:16:35 INFO]: Training loss at epoch 17: 0.9219117164611816
[08/28/2025 08:16:36 INFO]: Training loss at epoch 6: 1.6767640113830566
[08/28/2025 08:16:41 INFO]: Training loss at epoch 75: 0.8910360336303711
[08/28/2025 08:16:57 INFO]: Training loss at epoch 17: 1.2625672817230225
[08/28/2025 08:17:10 INFO]: New best epoch, val score: -0.6591113240640458
[08/28/2025 08:17:10 INFO]: Saving model to: unhanged-Shanesha_trial_97/model_best.pth
[08/28/2025 08:17:11 INFO]: Training stats: {
    "score": -0.9970561148442033,
    "rmse": 0.9970561148442033
}
[08/28/2025 08:17:11 INFO]: Val stats: {
    "score": -0.6735824490578259,
    "rmse": 0.6735824490578259
}
[08/28/2025 08:17:11 INFO]: Test stats: {
    "score": -0.8674895893510278,
    "rmse": 0.8674895893510278
}
[08/28/2025 08:17:14 INFO]: Training loss at epoch 18: 0.9815688133239746
[08/28/2025 08:17:21 INFO]: Training loss at epoch 31: 0.9794057011604309
[08/28/2025 08:17:22 INFO]: Training loss at epoch 1: 1.4257701933383942
[08/28/2025 08:17:49 INFO]: Training loss at epoch 13: 1.0956717729568481
[08/28/2025 08:18:21 INFO]: Training loss at epoch 22: 0.8855361342430115
[08/28/2025 08:18:47 INFO]: Training loss at epoch 32: 1.0213482975959778
[08/28/2025 08:18:51 INFO]: Training loss at epoch 6: 1.6149791479110718
[08/28/2025 08:18:53 INFO]: Training loss at epoch 1: 1.1156622767448425
[08/28/2025 08:18:59 INFO]: Training loss at epoch 18: 0.9258171617984772
[08/28/2025 08:19:08 INFO]: Training loss at epoch 52: 1.0056467056274414
[08/28/2025 08:19:10 INFO]: Training loss at epoch 36: 0.8854612410068512
[08/28/2025 08:19:16 INFO]: Training loss at epoch 18: 0.9009301662445068
[08/28/2025 08:19:17 INFO]: Training loss at epoch 19: 1.07602858543396
[08/28/2025 08:19:23 INFO]: Training loss at epoch 41: 1.1116164326667786
[08/28/2025 08:19:29 INFO]: Running Final Evaluation...
[08/28/2025 08:19:50 INFO]: Training loss at epoch 14: 1.1110390424728394
[08/28/2025 08:19:52 INFO]: Training loss at epoch 7: 1.7769157886505127
[08/28/2025 08:19:58 INFO]: Training loss at epoch 111: 0.945425808429718
[08/28/2025 08:19:59 INFO]: Training stats: {
    "score": -1.0342356165929243,
    "rmse": 1.0342356165929243
}
[08/28/2025 08:19:59 INFO]: Val stats: {
    "score": -0.6689866969799958,
    "rmse": 0.6689866969799958
}
[08/28/2025 08:19:59 INFO]: Test stats: {
    "score": -0.8908308014076853,
    "rmse": 0.8908308014076853
}
[08/28/2025 08:20:14 INFO]: Training loss at epoch 33: 1.0525078773498535
[08/28/2025 08:20:18 INFO]: Training loss at epoch 23: 0.965625673532486
[08/28/2025 08:20:19 INFO]: Training loss at epoch 2: 1.2589070796966553
[08/28/2025 08:20:26 INFO]: Training accuracy: {
    "score": -0.9935831564219502,
    "rmse": 0.9935831564219502
}
[08/28/2025 08:20:26 INFO]: Val accuracy: {
    "score": -0.6866635492040337,
    "rmse": 0.6866635492040337
}
[08/28/2025 08:20:26 INFO]: Test accuracy: {
    "score": -0.8748984101371101,
    "rmse": 0.8748984101371101
}
[08/28/2025 08:20:27 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_111",
    "best_epoch": 21,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8748984101371101,
        "rmse": 0.8748984101371101
    },
    "train_stats": {
        "score": -0.9935831564219502,
        "rmse": 0.9935831564219502
    },
    "val_stats": {
        "score": -0.6866635492040337,
        "rmse": 0.6866635492040337
    }
}
[08/28/2025 08:20:27 INFO]: Procewss finished for trial unhanged-Shanesha_trial_111
[08/28/2025 08:20:27 INFO]: 
_________________________________________________

[08/28/2025 08:20:27 INFO]: train_net_for_optune.py main() running.
[08/28/2025 08:20:27 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.2288120484823493
  attention_dropout: 0.15746299492862087
  ffn_dropout: 0.15746299492862087
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.23443187722731e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_130

[08/28/2025 08:20:27 INFO]: This ft_transformer has 0.602 million parameters.
[08/28/2025 08:20:27 INFO]: Training will start at epoch 0.
[08/28/2025 08:20:27 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 08:20:35 INFO]: Training loss at epoch 28: 1.0353607535362244
[08/28/2025 08:20:59 INFO]: Training loss at epoch 19: 0.7863676995038986
[08/28/2025 08:21:27 INFO]: Training loss at epoch 0: 0.9215302169322968
[08/28/2025 08:21:36 INFO]: New best epoch, val score: -0.7092102627883218
[08/28/2025 08:21:36 INFO]: Saving model to: unhanged-Shanesha_trial_130/model_best.pth
[08/28/2025 08:21:40 INFO]: Training loss at epoch 34: 0.974571019411087
[08/28/2025 08:21:42 INFO]: Training stats: {
    "score": -1.014810041030173,
    "rmse": 1.014810041030173
}
[08/28/2025 08:21:42 INFO]: Val stats: {
    "score": -0.6626403781673749,
    "rmse": 0.6626403781673749
}
[08/28/2025 08:21:42 INFO]: Test stats: {
    "score": -0.874805608687201,
    "rmse": 0.874805608687201
}
[08/28/2025 08:21:48 INFO]: Training loss at epoch 64: 1.0279437601566315
[08/28/2025 08:21:50 INFO]: Training loss at epoch 15: 1.1287252306938171
[08/28/2025 08:21:56 INFO]: Training loss at epoch 19: 0.9379769563674927
[08/28/2025 08:21:56 INFO]: Training loss at epoch 37: 1.0578895211219788
[08/28/2025 08:21:56 INFO]: Training loss at epoch 7: 1.1663097143173218
[08/28/2025 08:21:57 INFO]: New best epoch, val score: -0.6626403781673749
[08/28/2025 08:21:57 INFO]: Saving model to: unhanged-Shanesha_trial_124/model_best.pth
[08/28/2025 08:22:01 INFO]: Training loss at epoch 20: 1.0419200658798218
[08/28/2025 08:22:01 INFO]: Training loss at epoch 54: 0.8729479312896729
[08/28/2025 08:22:06 INFO]: New best epoch, val score: -0.6606328243005011
[08/28/2025 08:22:06 INFO]: Saving model to: unhanged-Shanesha_trial_125/model_best.pth
[08/28/2025 08:22:06 INFO]: Training loss at epoch 2: 0.9063541293144226
[08/28/2025 08:22:13 INFO]: Training loss at epoch 24: 1.1886492371559143
[08/28/2025 08:22:13 INFO]: Training loss at epoch 42: 1.2042827904224396
[08/28/2025 08:22:18 INFO]: Training loss at epoch 76: 0.9944657385349274
[08/28/2025 08:22:34 INFO]: Training loss at epoch 1: 1.2127784490585327
[08/28/2025 08:22:42 INFO]: New best epoch, val score: -0.6994037242080178
[08/28/2025 08:22:42 INFO]: Saving model to: unhanged-Shanesha_trial_130/model_best.pth
[08/28/2025 08:22:46 INFO]: Training stats: {
    "score": -1.0321832234095538,
    "rmse": 1.0321832234095538
}
[08/28/2025 08:22:46 INFO]: Val stats: {
    "score": -0.7991437128683608,
    "rmse": 0.7991437128683608
}
[08/28/2025 08:22:46 INFO]: Test stats: {
    "score": -0.9506210916233553,
    "rmse": 0.9506210916233553
}
[08/28/2025 08:23:00 INFO]: Training loss at epoch 8: 1.355000376701355
[08/28/2025 08:23:01 INFO]: Training loss at epoch 35: 0.8385266661643982
[08/28/2025 08:23:09 INFO]: Training loss at epoch 3: 0.9710835814476013
[08/28/2025 08:23:33 INFO]: Training loss at epoch 21: 0.9122620224952698
[08/28/2025 08:23:34 INFO]: Training loss at epoch 20: 1.1640390157699585
[08/28/2025 08:23:38 INFO]: Training loss at epoch 2: 0.9703250229358673
[08/28/2025 08:23:43 INFO]: Training loss at epoch 16: 1.0405566096305847
[08/28/2025 08:23:47 INFO]: New best epoch, val score: -0.6854945368173423
[08/28/2025 08:23:47 INFO]: Saving model to: unhanged-Shanesha_trial_130/model_best.pth
[08/28/2025 08:23:47 INFO]: New best epoch, val score: -0.6620523317686501
[08/28/2025 08:23:47 INFO]: Saving model to: unhanged-Shanesha_trial_124/model_best.pth
[08/28/2025 08:23:52 INFO]: Training loss at epoch 21: 1.087545096874237
[08/28/2025 08:23:58 INFO]: Training loss at epoch 25: 0.8068413138389587
[08/28/2025 08:24:13 INFO]: Training loss at epoch 20: 1.1744804382324219
[08/28/2025 08:24:20 INFO]: Training loss at epoch 36: 1.096323013305664
[08/28/2025 08:24:27 INFO]: Training loss at epoch 38: 1.1661409139633179
[08/28/2025 08:24:43 INFO]: Training loss at epoch 3: 1.0625211596488953
[08/28/2025 08:24:44 INFO]: Training loss at epoch 8: 1.2887818217277527
[08/28/2025 08:24:47 INFO]: Training loss at epoch 43: 0.9295319616794586
[08/28/2025 08:24:51 INFO]: New best epoch, val score: -0.6847690774257836
[08/28/2025 08:24:51 INFO]: Saving model to: unhanged-Shanesha_trial_130/model_best.pth
[08/28/2025 08:25:02 INFO]: Training loss at epoch 3: 1.1865445375442505
[08/28/2025 08:25:10 INFO]: Training loss at epoch 20: 0.9778620600700378
[08/28/2025 08:25:23 INFO]: New best epoch, val score: -0.6852578341348996
[08/28/2025 08:25:23 INFO]: Saving model to: unhanged-Shanesha_trial_129/model_best.pth
[08/28/2025 08:25:24 INFO]: Training loss at epoch 21: 0.9622019529342651
[08/28/2025 08:25:33 INFO]: Training loss at epoch 17: 1.175589919090271
[08/28/2025 08:25:38 INFO]: Training loss at epoch 37: 0.9581019282341003
[08/28/2025 08:25:43 INFO]: Training loss at epoch 22: 1.0885674953460693
[08/28/2025 08:25:44 INFO]: Training loss at epoch 26: 1.09700146317482
[08/28/2025 08:25:47 INFO]: Training loss at epoch 4: 0.882971465587616
[08/28/2025 08:25:50 INFO]: Training loss at epoch 4: 1.206820547580719
[08/28/2025 08:25:56 INFO]: Training loss at epoch 9: 1.386847972869873
[08/28/2025 08:26:21 INFO]: Training loss at epoch 29: 1.0797002017498016
[08/28/2025 08:26:48 INFO]: Training loss at epoch 112: 1.1481861472129822
[08/28/2025 08:26:52 INFO]: Training loss at epoch 65: 1.0166524350643158
[08/28/2025 08:26:54 INFO]: Training loss at epoch 5: 0.9232862293720245
[08/28/2025 08:27:02 INFO]: Training stats: {
    "score": -1.255354151176399,
    "rmse": 1.255354151176399
}
[08/28/2025 08:27:02 INFO]: Val stats: {
    "score": -1.1347664269478703,
    "rmse": 1.1347664269478703
}
[08/28/2025 08:27:02 INFO]: Test stats: {
    "score": -1.2169493200976715,
    "rmse": 1.2169493200976715
}
[08/28/2025 08:27:02 INFO]: Training loss at epoch 38: 1.0499185621738434
[08/28/2025 08:27:02 INFO]: Training loss at epoch 39: 1.0481007099151611
[08/28/2025 08:27:06 INFO]: Training loss at epoch 55: 1.1200444102287292
[08/28/2025 08:27:21 INFO]: Training loss at epoch 22: 1.0060741901397705
[08/28/2025 08:27:28 INFO]: Training loss at epoch 77: 1.0053957104682922
[08/28/2025 08:27:28 INFO]: Training loss at epoch 44: 0.8093560636043549
[08/28/2025 08:27:30 INFO]: Training loss at epoch 18: 0.8175195455551147
[08/28/2025 08:27:36 INFO]: Training loss at epoch 27: 1.15080726146698
[08/28/2025 08:27:40 INFO]: Training loss at epoch 9: 1.0610812604427338
[08/28/2025 08:27:43 INFO]: Training loss at epoch 23: 0.9616251587867737
[08/28/2025 08:27:44 INFO]: Training loss at epoch 21: 1.0531487464904785
[08/28/2025 08:27:58 INFO]: Training stats: {
    "score": -1.0116308046309888,
    "rmse": 1.0116308046309888
}
[08/28/2025 08:27:58 INFO]: Val stats: {
    "score": -0.7323772919410739,
    "rmse": 0.7323772919410739
}
[08/28/2025 08:27:58 INFO]: Test stats: {
    "score": -0.9143155361801802,
    "rmse": 0.9143155361801802
}
[08/28/2025 08:28:02 INFO]: Training loss at epoch 6: 0.885594516992569
[08/28/2025 08:28:08 INFO]: Training loss at epoch 4: 1.257658064365387
[08/28/2025 08:28:26 INFO]: Training stats: {
    "score": -0.998103988712149,
    "rmse": 0.998103988712149
}
[08/28/2025 08:28:26 INFO]: Val stats: {
    "score": -0.6725475181897433,
    "rmse": 0.6725475181897433
}
[08/28/2025 08:28:26 INFO]: Test stats: {
    "score": -0.8711674540786403,
    "rmse": 0.8711674540786403
}
[08/28/2025 08:28:27 INFO]: Training loss at epoch 39: 1.0106267035007477
[08/28/2025 08:28:32 INFO]: New best epoch, val score: -0.6643507948164566
[08/28/2025 08:28:32 INFO]: Saving model to: unhanged-Shanesha_trial_129/model_best.pth
[08/28/2025 08:28:44 INFO]: Training loss at epoch 5: 0.9241516590118408
[08/28/2025 08:28:45 INFO]: Training stats: {
    "score": -1.122853123687651,
    "rmse": 1.122853123687651
}
[08/28/2025 08:28:45 INFO]: Val stats: {
    "score": -0.9447468315166443,
    "rmse": 0.9447468315166443
}
[08/28/2025 08:28:45 INFO]: Test stats: {
    "score": -1.0642830384797992,
    "rmse": 1.0642830384797992
}
[08/28/2025 08:28:58 INFO]: Training stats: {
    "score": -1.01363403663513,
    "rmse": 1.01363403663513
}
[08/28/2025 08:28:58 INFO]: Val stats: {
    "score": -0.7407791456886532,
    "rmse": 0.7407791456886532
}
[08/28/2025 08:28:58 INFO]: Test stats: {
    "score": -0.9117913156736658,
    "rmse": 0.9117913156736658
}
[08/28/2025 08:29:13 INFO]: Training loss at epoch 7: 1.091201275587082
[08/28/2025 08:29:21 INFO]: Training loss at epoch 23: 0.8860118389129639
[08/28/2025 08:29:31 INFO]: Training loss at epoch 19: 1.272725224494934
[08/28/2025 08:29:33 INFO]: Training loss at epoch 28: 1.1824147701263428
[08/28/2025 08:29:44 INFO]: Training loss at epoch 24: 0.9697569012641907
[08/28/2025 08:30:14 INFO]: Training stats: {
    "score": -1.0080658964370857,
    "rmse": 1.0080658964370857
}
[08/28/2025 08:30:14 INFO]: Val stats: {
    "score": -0.7247244318340293,
    "rmse": 0.7247244318340293
}
[08/28/2025 08:30:14 INFO]: Test stats: {
    "score": -0.8988374499512005,
    "rmse": 0.8988374499512005
}
[08/28/2025 08:30:17 INFO]: Training loss at epoch 10: 1.4627370238304138
[08/28/2025 08:30:20 INFO]: Training loss at epoch 45: 1.0517101883888245
[08/28/2025 08:30:23 INFO]: Training loss at epoch 8: 1.1713638305664062
[08/28/2025 08:30:25 INFO]: Training loss at epoch 22: 1.0190497934818268
[08/28/2025 08:30:25 INFO]: Training loss at epoch 40: 1.0993598699569702
[08/28/2025 08:30:45 INFO]: Training loss at epoch 40: 1.2603739798069
[08/28/2025 08:31:17 INFO]: Training loss at epoch 21: 0.9000288844108582
[08/28/2025 08:31:21 INFO]: Training loss at epoch 24: 1.047292411327362
[08/28/2025 08:31:23 INFO]: Training loss at epoch 5: 0.8591594398021698
[08/28/2025 08:31:28 INFO]: Training loss at epoch 29: 0.8721961975097656
[08/28/2025 08:31:31 INFO]: Training loss at epoch 9: 1.065068542957306
[08/28/2025 08:31:32 INFO]: Training loss at epoch 22: 0.9269531667232513
[08/28/2025 08:31:40 INFO]: Training loss at epoch 6: 0.9024782776832581
[08/28/2025 08:31:43 INFO]: Training loss at epoch 25: 1.192042887210846
[08/28/2025 08:31:49 INFO]: Training loss at epoch 10: 1.024393230676651
[08/28/2025 08:31:49 INFO]: Training loss at epoch 41: 1.2627400159835815
[08/28/2025 08:31:54 INFO]: Training stats: {
    "score": -1.00275811561029,
    "rmse": 1.00275811561029
}
[08/28/2025 08:31:54 INFO]: Val stats: {
    "score": -0.6937862878696488,
    "rmse": 0.6937862878696488
}
[08/28/2025 08:31:54 INFO]: Test stats: {
    "score": -0.8796120199367661,
    "rmse": 0.8796120199367661
}
[08/28/2025 08:32:05 INFO]: Training stats: {
    "score": -0.9948643131081386,
    "rmse": 0.9948643131081386
}
[08/28/2025 08:32:05 INFO]: Val stats: {
    "score": -0.6795299116637221,
    "rmse": 0.6795299116637221
}
[08/28/2025 08:32:05 INFO]: Test stats: {
    "score": -0.8737347145933331,
    "rmse": 0.8737347145933331
}
[08/28/2025 08:32:09 INFO]: Training loss at epoch 20: 1.1052076816558838
[08/28/2025 08:32:14 INFO]: Training loss at epoch 66: 1.0128379464149475
[08/28/2025 08:32:32 INFO]: Training loss at epoch 56: 1.125527262687683
[08/28/2025 08:32:54 INFO]: Training loss at epoch 78: 1.059477150440216
[08/28/2025 08:32:54 INFO]: Training loss at epoch 23: 1.2380443215370178
[08/28/2025 08:32:58 INFO]: Training loss at epoch 10: 0.9520759880542755
[08/28/2025 08:33:00 INFO]: Training loss at epoch 46: 0.9466124773025513
[08/28/2025 08:33:07 INFO]: Training loss at epoch 42: 1.0681027173995972
[08/28/2025 08:33:11 INFO]: Training loss at epoch 25: 0.9619351029396057
[08/28/2025 08:33:18 INFO]: Training loss at epoch 41: 1.0575132071971893
[08/28/2025 08:33:19 INFO]: Training loss at epoch 11: 1.1607049107551575
[08/28/2025 08:33:34 INFO]: Training loss at epoch 26: 1.0591203570365906
[08/28/2025 08:33:49 INFO]: Training loss at epoch 113: 0.8674728274345398
[08/28/2025 08:33:50 INFO]: Training loss at epoch 30: 0.8838341534137726
[08/28/2025 08:33:59 INFO]: Training loss at epoch 21: 1.020754188299179
[08/28/2025 08:34:02 INFO]: Training loss at epoch 11: 1.1305009722709656
[08/28/2025 08:34:18 INFO]: Training loss at epoch 6: 1.0992717146873474
[08/28/2025 08:34:20 INFO]: Training loss at epoch 30: 1.070370227098465
[08/28/2025 08:34:21 INFO]: Training loss at epoch 7: 1.0293514430522919
[08/28/2025 08:34:25 INFO]: Training loss at epoch 43: 1.0412122011184692
[08/28/2025 08:34:36 INFO]: Training loss at epoch 11: 1.071178913116455
[08/28/2025 08:35:01 INFO]: Training loss at epoch 26: 1.1942659616470337
[08/28/2025 08:35:06 INFO]: Training loss at epoch 12: 1.2127315402030945
[08/28/2025 08:35:21 INFO]: Training loss at epoch 24: 0.8941143155097961
[08/28/2025 08:35:27 INFO]: Training loss at epoch 27: 0.8586849272251129
[08/28/2025 08:35:36 INFO]: Training loss at epoch 47: 0.9609766900539398
[08/28/2025 08:35:38 INFO]: Training loss at epoch 31: 0.9784184694290161
[08/28/2025 08:35:48 INFO]: Training loss at epoch 44: 1.0690534710884094
[08/28/2025 08:35:52 INFO]: Training loss at epoch 42: 1.0512760877609253
[08/28/2025 08:35:53 INFO]: Training loss at epoch 22: 1.0548069477081299
[08/28/2025 08:36:16 INFO]: Training loss at epoch 13: 1.14336559176445
[08/28/2025 08:36:22 INFO]: Training loss at epoch 12: 1.1810206174850464
[08/28/2025 08:37:01 INFO]: Training loss at epoch 27: 0.9409395754337311
[08/28/2025 08:37:13 INFO]: Training loss at epoch 8: 0.9621828198432922
[08/28/2025 08:37:15 INFO]: Training loss at epoch 45: 1.1417827606201172
[08/28/2025 08:37:23 INFO]: Training loss at epoch 67: 0.985893189907074
[08/28/2025 08:37:25 INFO]: Training loss at epoch 14: 0.8038223385810852
[08/28/2025 08:37:27 INFO]: Training loss at epoch 7: 1.1445977091789246
[08/28/2025 08:37:29 INFO]: Training loss at epoch 28: 1.0105230510234833
[08/28/2025 08:37:34 INFO]: Training loss at epoch 32: 1.046733319759369
[08/28/2025 08:37:38 INFO]: Training loss at epoch 12: 1.166601836681366
[08/28/2025 08:37:46 INFO]: Training loss at epoch 57: 0.8229744732379913
[08/28/2025 08:37:54 INFO]: Training loss at epoch 23: 0.9166008234024048
[08/28/2025 08:38:01 INFO]: Training loss at epoch 25: 0.8045682311058044
[08/28/2025 08:38:08 INFO]: Training loss at epoch 22: 0.8981406092643738
[08/28/2025 08:38:15 INFO]: Training loss at epoch 79: 0.8602546453475952
[08/28/2025 08:38:28 INFO]: Training loss at epoch 48: 1.2028699815273285
[08/28/2025 08:38:35 INFO]: Training loss at epoch 15: 0.8808546662330627
[08/28/2025 08:38:39 INFO]: Training loss at epoch 43: 0.9728973209857941
[08/28/2025 08:38:43 INFO]: Training loss at epoch 46: 1.258976697921753
[08/28/2025 08:39:03 INFO]: Training loss at epoch 28: 0.9165895879268646
[08/28/2025 08:39:27 INFO]: Training loss at epoch 23: 1.2044592499732971
[08/28/2025 08:39:30 INFO]: Training loss at epoch 33: 0.9065955579280853
[08/28/2025 08:39:32 INFO]: Training loss at epoch 29: 1.148567944765091
[08/28/2025 08:39:38 INFO]: Training loss at epoch 13: 0.7803716957569122
[08/28/2025 08:39:44 INFO]: Running Final Evaluation...
[08/28/2025 08:39:45 INFO]: Training loss at epoch 16: 1.150208294391632
[08/28/2025 08:39:55 INFO]: Training loss at epoch 24: 0.9715838730335236
[08/28/2025 08:40:09 INFO]: Training loss at epoch 47: 1.1205949783325195
[08/28/2025 08:40:11 INFO]: Training loss at epoch 9: 1.028164029121399
[08/28/2025 08:40:13 INFO]: Training stats: {
    "score": -0.997831707103165,
    "rmse": 0.997831707103165
}
[08/28/2025 08:40:13 INFO]: Val stats: {
    "score": -0.7044698047408182,
    "rmse": 0.7044698047408182
}
[08/28/2025 08:40:13 INFO]: Test stats: {
    "score": -0.8863972826287508,
    "rmse": 0.8863972826287508
}
[08/28/2025 08:40:14 INFO]: Training stats: {
    "score": -0.996162884609181,
    "rmse": 0.996162884609181
}
[08/28/2025 08:40:14 INFO]: Val stats: {
    "score": -0.6752223674972181,
    "rmse": 0.6752223674972181
}
[08/28/2025 08:40:14 INFO]: Test stats: {
    "score": -0.8764243751798229,
    "rmse": 0.8764243751798229
}
[08/28/2025 08:40:26 INFO]: Training accuracy: {
    "score": -1.0080682687557956,
    "rmse": 1.0080682687557956
}
[08/28/2025 08:40:26 INFO]: Val accuracy: {
    "score": -0.6627178991676282,
    "rmse": 0.6627178991676282
}
[08/28/2025 08:40:26 INFO]: Test accuracy: {
    "score": -0.8686100442827903,
    "rmse": 0.8686100442827903
}
[08/28/2025 08:40:26 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_122",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8686100442827903,
        "rmse": 0.8686100442827903
    },
    "train_stats": {
        "score": -1.0080682687557956,
        "rmse": 1.0080682687557956
    },
    "val_stats": {
        "score": -0.6627178991676282,
        "rmse": 0.6627178991676282
    }
}
[08/28/2025 08:40:26 INFO]: Procewss finished for trial unhanged-Shanesha_trial_122
[08/28/2025 08:40:26 INFO]: 
_________________________________________________

[08/28/2025 08:40:26 INFO]: train_net_for_optune.py main() running.
[08/28/2025 08:40:26 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.6597046730163907
  attention_dropout: 0.3543710709950693
  ffn_dropout: 0.3543710709950693
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.1946440128202613e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_131

[08/28/2025 08:40:26 INFO]: This ft_transformer has 2.538 million parameters.
[08/28/2025 08:40:26 INFO]: Training will start at epoch 0.
[08/28/2025 08:40:26 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 08:40:27 INFO]: Training loss at epoch 31: 1.1367152631282806
[08/28/2025 08:40:41 INFO]: Training loss at epoch 26: 1.239569365978241
[08/28/2025 08:40:41 INFO]: Training loss at epoch 8: 0.9990337193012238
[08/28/2025 08:40:43 INFO]: Training loss at epoch 13: 1.1076276898384094
[08/28/2025 08:40:54 INFO]: Training loss at epoch 17: 1.3082314431667328
[08/28/2025 08:40:56 INFO]: Training loss at epoch 114: 0.9090405106544495
[08/28/2025 08:41:01 INFO]: Training loss at epoch 29: 1.026891142129898
[08/28/2025 08:41:09 INFO]: Training stats: {
    "score": -0.9964653380136633,
    "rmse": 0.9964653380136633
}
[08/28/2025 08:41:09 INFO]: Val stats: {
    "score": -0.6814787702038075,
    "rmse": 0.6814787702038075
}
[08/28/2025 08:41:09 INFO]: Test stats: {
    "score": -0.8714229561793039,
    "rmse": 0.8714229561793039
}
[08/28/2025 08:41:15 INFO]: Training loss at epoch 49: 0.9001531302928925
[08/28/2025 08:41:20 INFO]: Training loss at epoch 44: 1.0784083604812622
[08/28/2025 08:41:30 INFO]: Training loss at epoch 48: 0.9651429653167725
[08/28/2025 08:41:39 INFO]: Training stats: {
    "score": -0.992629048715582,
    "rmse": 0.992629048715582
}
[08/28/2025 08:41:39 INFO]: Val stats: {
    "score": -0.6821691626683121,
    "rmse": 0.6821691626683121
}
[08/28/2025 08:41:39 INFO]: Test stats: {
    "score": -0.8761142806120444,
    "rmse": 0.8761142806120444
}
[08/28/2025 08:41:49 INFO]: Training loss at epoch 25: 0.8753509521484375
[08/28/2025 08:41:57 INFO]: Training loss at epoch 18: 1.1014817357063293
[08/28/2025 08:42:04 INFO]: Training loss at epoch 0: 1.0264920592308044
[08/28/2025 08:42:07 INFO]: Training stats: {
    "score": -0.9936436031435992,
    "rmse": 0.9936436031435992
}
[08/28/2025 08:42:07 INFO]: Val stats: {
    "score": -0.6710445056970111,
    "rmse": 0.6710445056970111
}
[08/28/2025 08:42:07 INFO]: Test stats: {
    "score": -0.8739062163028918,
    "rmse": 0.8739062163028918
}
[08/28/2025 08:42:07 INFO]: Training loss at epoch 30: 1.0618734955787659
[08/28/2025 08:42:17 INFO]: New best epoch, val score: -0.676285498815339
[08/28/2025 08:42:17 INFO]: Saving model to: unhanged-Shanesha_trial_131/model_best.pth
[08/28/2025 08:42:39 INFO]: Training loss at epoch 14: 0.9334423542022705
[08/28/2025 08:42:39 INFO]: Training loss at epoch 68: 0.8810292780399323
[08/28/2025 08:42:49 INFO]: Training loss at epoch 49: 1.0980393290519714
[08/28/2025 08:43:00 INFO]: New best epoch, val score: -0.6998513358548213
[08/28/2025 08:43:00 INFO]: Saving model to: unhanged-Shanesha_trial_126/model_best.pth
[08/28/2025 08:43:01 INFO]: Training loss at epoch 19: 1.1557932496070862
[08/28/2025 08:43:03 INFO]: Training loss at epoch 58: 1.0315969586372375
[08/28/2025 08:43:05 INFO]: Training loss at epoch 27: 0.998266875743866
[08/28/2025 08:43:15 INFO]: Training stats: {
    "score": -1.0001459036545912,
    "rmse": 1.0001459036545912
}
[08/28/2025 08:43:15 INFO]: Val stats: {
    "score": -0.6996366273564184,
    "rmse": 0.6996366273564184
}
[08/28/2025 08:43:15 INFO]: Test stats: {
    "score": -0.8862022065849737,
    "rmse": 0.8862022065849737
}
[08/28/2025 08:43:24 INFO]: Training stats: {
    "score": -1.0012400568134054,
    "rmse": 1.0012400568134054
}
[08/28/2025 08:43:24 INFO]: Val stats: {
    "score": -0.6880560073391109,
    "rmse": 0.6880560073391109
}
[08/28/2025 08:43:24 INFO]: Test stats: {
    "score": -0.8768225836521824,
    "rmse": 0.8768225836521824
}
[08/28/2025 08:43:29 INFO]: Training loss at epoch 30: 1.0072941482067108
[08/28/2025 08:43:29 INFO]: Training loss at epoch 14: 0.9888083040714264
[08/28/2025 08:43:36 INFO]: Training loss at epoch 9: 1.0813877582550049
[08/28/2025 08:43:39 INFO]: Training loss at epoch 26: 0.9282984137535095
[08/28/2025 08:43:48 INFO]: Training loss at epoch 10: 0.8978342711925507
[08/28/2025 08:43:50 INFO]: Training loss at epoch 45: 1.1357855200767517
[08/28/2025 08:43:54 INFO]: Training loss at epoch 1: 1.1715336441993713
[08/28/2025 08:43:58 INFO]: Training loss at epoch 31: 1.054274469614029
[08/28/2025 08:44:07 INFO]: New best epoch, val score: -0.6746965046647407
[08/28/2025 08:44:07 INFO]: Saving model to: unhanged-Shanesha_trial_131/model_best.pth
[08/28/2025 08:44:30 INFO]: Training loss at epoch 20: 1.2377061545848846
[08/28/2025 08:44:37 INFO]: Training loss at epoch 50: 0.9016424715518951
[08/28/2025 08:44:39 INFO]: New best epoch, val score: -0.6842928537855264
[08/28/2025 08:44:39 INFO]: Saving model to: unhanged-Shanesha_trial_130/model_best.pth
[08/28/2025 08:44:39 INFO]: Training stats: {
    "score": -0.9990895458529315,
    "rmse": 0.9990895458529315
}
[08/28/2025 08:44:39 INFO]: Val stats: {
    "score": -0.6780324552945979,
    "rmse": 0.6780324552945979
}
[08/28/2025 08:44:39 INFO]: Test stats: {
    "score": -0.8639168106729995,
    "rmse": 0.8639168106729995
}
[08/28/2025 08:44:45 INFO]: Training loss at epoch 50: 0.9458842873573303
[08/28/2025 08:45:01 INFO]: Training loss at epoch 23: 1.087678074836731
[08/28/2025 08:45:24 INFO]: Training loss at epoch 31: 1.130164235830307
[08/28/2025 08:45:24 INFO]: Training loss at epoch 80: 0.7573397606611252
[08/28/2025 08:45:35 INFO]: Training loss at epoch 27: 1.1462275385856628
[08/28/2025 08:45:37 INFO]: Training loss at epoch 28: 0.9060965776443481
[08/28/2025 08:45:38 INFO]: Training loss at epoch 21: 0.865129828453064
[08/28/2025 08:45:42 INFO]: Training loss at epoch 15: 0.9813083410263062
[08/28/2025 08:45:47 INFO]: New best epoch, val score: -0.6832994222534627
[08/28/2025 08:45:47 INFO]: Saving model to: unhanged-Shanesha_trial_130/model_best.pth
[08/28/2025 08:45:53 INFO]: Training loss at epoch 2: 0.9994344711303711
[08/28/2025 08:45:58 INFO]: Training loss at epoch 32: 0.7977888286113739
[08/28/2025 08:46:02 INFO]: Training loss at epoch 51: 1.1071023643016815
[08/28/2025 08:46:06 INFO]: New best epoch, val score: -0.6770110152057429
[08/28/2025 08:46:06 INFO]: Saving model to: unhanged-Shanesha_trial_126/model_best.pth
[08/28/2025 08:46:12 INFO]: Running Final Evaluation...
[08/28/2025 08:46:15 INFO]: Training loss at epoch 32: 0.8544192314147949
[08/28/2025 08:46:28 INFO]: Training loss at epoch 15: 1.012370526790619
[08/28/2025 08:46:32 INFO]: Training loss at epoch 46: 0.9767075777053833
[08/28/2025 08:46:40 INFO]: Training loss at epoch 11: 0.8685899078845978
[08/28/2025 08:46:43 INFO]: Training accuracy: {
    "score": -1.0050055270319622,
    "rmse": 1.0050055270319622
}
[08/28/2025 08:46:43 INFO]: Val accuracy: {
    "score": -0.6647014101918193,
    "rmse": 0.6647014101918193
}
[08/28/2025 08:46:43 INFO]: Test accuracy: {
    "score": -0.8712581431035082,
    "rmse": 0.8712581431035082
}
[08/28/2025 08:46:43 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_121",
    "best_epoch": 20,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8712581431035082,
        "rmse": 0.8712581431035082
    },
    "train_stats": {
        "score": -1.0050055270319622,
        "rmse": 1.0050055270319622
    },
    "val_stats": {
        "score": -0.6647014101918193,
        "rmse": 0.6647014101918193
    }
}
[08/28/2025 08:46:43 INFO]: Procewss finished for trial unhanged-Shanesha_trial_121
[08/28/2025 08:46:43 INFO]: 
_________________________________________________

[08/28/2025 08:46:43 INFO]: train_net_for_optune.py main() running.
[08/28/2025 08:46:43 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.620317590709062
  attention_dropout: 0.2963482505702818
  ffn_dropout: 0.2963482505702818
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.1651804313912338e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_132

[08/28/2025 08:46:43 INFO]: This ft_transformer has 2.518 million parameters.
[08/28/2025 08:46:43 INFO]: Training will start at epoch 0.
[08/28/2025 08:46:43 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 08:46:46 INFO]: Training loss at epoch 22: 1.1575375199317932
[08/28/2025 08:47:17 INFO]: Training loss at epoch 24: 0.9273194074630737
[08/28/2025 08:47:24 INFO]: Training loss at epoch 32: 0.9785758256912231
[08/28/2025 08:47:34 INFO]: Training loss at epoch 28: 1.0833084285259247
[08/28/2025 08:47:35 INFO]: Training loss at epoch 51: 0.9104366600513458
[08/28/2025 08:47:50 INFO]: Training loss at epoch 115: 0.9194767475128174
[08/28/2025 08:47:52 INFO]: Training loss at epoch 10: 0.8671657145023346
[08/28/2025 08:47:54 INFO]: Training loss at epoch 3: 0.8711977303028107
[08/28/2025 08:47:56 INFO]: Training loss at epoch 69: 1.138440728187561
[08/28/2025 08:47:57 INFO]: Training loss at epoch 23: 1.020244300365448
[08/28/2025 08:47:58 INFO]: Training loss at epoch 33: 1.0729888677597046
[08/28/2025 08:48:16 INFO]: Training loss at epoch 29: 0.801415205001831
[08/28/2025 08:48:25 INFO]: Training loss at epoch 59: 0.9737352132797241
[08/28/2025 08:48:32 INFO]: Training loss at epoch 0: 1.0000742077827454
[08/28/2025 08:48:47 INFO]: New best epoch, val score: -0.6583349700175644
[08/28/2025 08:48:47 INFO]: Saving model to: unhanged-Shanesha_trial_132/model_best.pth
[08/28/2025 08:48:58 INFO]: Training loss at epoch 16: 0.896498054265976
[08/28/2025 08:49:07 INFO]: Training loss at epoch 24: 1.015491247177124
[08/28/2025 08:49:11 INFO]: Training stats: {
    "score": -0.9939858905890039,
    "rmse": 0.9939858905890039
}
[08/28/2025 08:49:11 INFO]: Val stats: {
    "score": -0.7010795879474708,
    "rmse": 0.7010795879474708
}
[08/28/2025 08:49:11 INFO]: Test stats: {
    "score": -0.8856936119245684,
    "rmse": 0.8856936119245684
}
[08/28/2025 08:49:19 INFO]: Training loss at epoch 47: 0.8978402316570282
[08/28/2025 08:49:22 INFO]: New best epoch, val score: -0.6674101891796814
[08/28/2025 08:49:22 INFO]: Saving model to: unhanged-Shanesha_trial_126/model_best.pth
[08/28/2025 08:49:25 INFO]: Training loss at epoch 33: 1.0321954190731049
[08/28/2025 08:49:33 INFO]: Training loss at epoch 16: 0.9708938300609589
[08/28/2025 08:49:36 INFO]: Training loss at epoch 29: 0.8484724760055542
[08/28/2025 08:49:37 INFO]: Training loss at epoch 12: 0.9039158523082733
[08/28/2025 08:49:47 INFO]: Training stats: {
    "score": -0.9999092178119332,
    "rmse": 0.9999092178119332
}
[08/28/2025 08:49:47 INFO]: Val stats: {
    "score": -0.6848333569141309,
    "rmse": 0.6848333569141309
}
[08/28/2025 08:49:47 INFO]: Test stats: {
    "score": -0.8761647598145016,
    "rmse": 0.8761647598145016
}
[08/28/2025 08:49:54 INFO]: Training loss at epoch 4: 0.9448901414871216
[08/28/2025 08:49:59 INFO]: Training loss at epoch 34: 1.013285756111145
[08/28/2025 08:50:14 INFO]: Training loss at epoch 25: 1.0696688890457153
[08/28/2025 08:50:14 INFO]: Training stats: {
    "score": -0.9991873662061873,
    "rmse": 0.9991873662061873
}
[08/28/2025 08:50:14 INFO]: Val stats: {
    "score": -0.6719154377108527,
    "rmse": 0.6719154377108527
}
[08/28/2025 08:50:14 INFO]: Test stats: {
    "score": -0.8703691890435109,
    "rmse": 0.8703691890435109
}
[08/28/2025 08:50:15 INFO]: Training stats: {
    "score": -0.9995273363411917,
    "rmse": 0.9995273363411917
}
[08/28/2025 08:50:15 INFO]: Val stats: {
    "score": -0.7258241893209946,
    "rmse": 0.7258241893209946
}
[08/28/2025 08:50:15 INFO]: Test stats: {
    "score": -0.9038897981042364,
    "rmse": 0.9038897981042364
}
[08/28/2025 08:50:23 INFO]: Training loss at epoch 52: 0.8631207942962646
[08/28/2025 08:50:29 INFO]: Training loss at epoch 1: 1.1359422206878662
[08/28/2025 08:50:52 INFO]: Training loss at epoch 81: 0.8780028820037842
[08/28/2025 08:50:57 INFO]: Training loss at epoch 11: 0.8047468662261963
[08/28/2025 08:51:15 INFO]: Training loss at epoch 34: 1.2621504664421082
[08/28/2025 08:51:17 INFO]: Training loss at epoch 26: 1.0644617676734924
[08/28/2025 08:51:38 INFO]: Training loss at epoch 30: 0.9441018104553223
[08/28/2025 08:51:45 INFO]: Training loss at epoch 5: 1.2773581147193909
[08/28/2025 08:51:50 INFO]: Training loss at epoch 48: 1.23597651720047
[08/28/2025 08:51:50 INFO]: Training loss at epoch 35: 0.866423487663269
[08/28/2025 08:51:58 INFO]: Training loss at epoch 17: 1.0066251158714294
[08/28/2025 08:52:01 INFO]: Training loss at epoch 24: 1.0253445506095886
[08/28/2025 08:52:03 INFO]: Training loss at epoch 30: 1.059438943862915
[08/28/2025 08:52:13 INFO]: Training loss at epoch 33: 1.0754664838314056
[08/28/2025 08:52:17 INFO]: Training loss at epoch 13: 0.9333884119987488
[08/28/2025 08:52:19 INFO]: New best epoch, val score: -0.6646989063305614
[08/28/2025 08:52:19 INFO]: Saving model to: unhanged-Shanesha_trial_126/model_best.pth
[08/28/2025 08:52:19 INFO]: Training loss at epoch 2: 1.0045056641101837
[08/28/2025 08:52:19 INFO]: Training loss at epoch 17: 1.0874642729759216
[08/28/2025 08:52:22 INFO]: Training loss at epoch 27: 1.2187812328338623
[08/28/2025 08:52:57 INFO]: Training loss at epoch 53: 1.2388157546520233
[08/28/2025 08:53:05 INFO]: Training loss at epoch 35: 1.0380763113498688
[08/28/2025 08:53:26 INFO]: Training loss at epoch 28: 0.9613379836082458
[08/28/2025 08:53:38 INFO]: Training loss at epoch 6: 0.8397867679595947
[08/28/2025 08:53:43 INFO]: Training loss at epoch 36: 1.11942458152771
[08/28/2025 08:53:55 INFO]: Training loss at epoch 12: 1.2211688160896301
[08/28/2025 08:53:56 INFO]: Training loss at epoch 31: 0.8875202834606171
[08/28/2025 08:54:05 INFO]: Training loss at epoch 31: 1.0556830763816833
[08/28/2025 08:54:14 INFO]: Training loss at epoch 3: 1.087906539440155
[08/28/2025 08:54:24 INFO]: Training loss at epoch 49: 1.196739912033081
[08/28/2025 08:54:34 INFO]: Training loss at epoch 29: 0.9352318346500397
[08/28/2025 08:54:40 INFO]: Training loss at epoch 116: 0.8392103016376495
[08/28/2025 08:54:51 INFO]: Training loss at epoch 70: 0.9414214193820953
[08/28/2025 08:55:00 INFO]: Training stats: {
    "score": -1.0044644807920238,
    "rmse": 1.0044644807920238
}
[08/28/2025 08:55:00 INFO]: Val stats: {
    "score": -0.7157487091582109,
    "rmse": 0.7157487091582109
}
[08/28/2025 08:55:00 INFO]: Test stats: {
    "score": -0.8918229190462038,
    "rmse": 0.8918229190462038
}
[08/28/2025 08:55:03 INFO]: Training loss at epoch 18: 1.0690961480140686
[08/28/2025 08:55:04 INFO]: Training loss at epoch 36: 0.9906425774097443
[08/28/2025 08:55:06 INFO]: Training loss at epoch 14: 0.8852111995220184
[08/28/2025 08:55:09 INFO]: Training loss at epoch 25: 0.9206673502922058
[08/28/2025 08:55:16 INFO]: Training loss at epoch 18: 1.0849003791809082
[08/28/2025 08:55:20 INFO]: Training stats: {
    "score": -0.995712717622347,
    "rmse": 0.995712717622347
}
[08/28/2025 08:55:20 INFO]: Val stats: {
    "score": -0.6843705757757733,
    "rmse": 0.6843705757757733
}
[08/28/2025 08:55:20 INFO]: Test stats: {
    "score": -0.8856490977659591,
    "rmse": 0.8856490977659591
}
[08/28/2025 08:55:24 INFO]: Training loss at epoch 60: 0.9434724748134613
[08/28/2025 08:55:27 INFO]: New best epoch, val score: -0.664277208597805
[08/28/2025 08:55:27 INFO]: Saving model to: unhanged-Shanesha_trial_126/model_best.pth
[08/28/2025 08:55:38 INFO]: Training loss at epoch 7: 1.1897522807121277
[08/28/2025 08:55:43 INFO]: Training loss at epoch 54: 0.8107172846794128
[08/28/2025 08:55:44 INFO]: Training loss at epoch 37: 1.1686151325702667
[08/28/2025 08:55:57 INFO]: Training loss at epoch 32: 1.0726897716522217
[08/28/2025 08:56:10 INFO]: Training loss at epoch 30: 0.8812837600708008
[08/28/2025 08:56:11 INFO]: Training loss at epoch 82: 0.819644421339035
[08/28/2025 08:56:14 INFO]: Training loss at epoch 4: 1.2629616260528564
[08/28/2025 08:56:45 INFO]: Training loss at epoch 32: 0.9647680521011353
[08/28/2025 08:56:52 INFO]: Running Final Evaluation...
[08/28/2025 08:57:05 INFO]: Training loss at epoch 37: 0.916445791721344
[08/28/2025 08:57:10 INFO]: Training loss at epoch 13: 1.0374172925949097
[08/28/2025 08:57:19 INFO]: Training loss at epoch 31: 1.2481939792633057
[08/28/2025 08:57:39 INFO]: Training loss at epoch 8: 0.8101062774658203
[08/28/2025 08:57:46 INFO]: Training loss at epoch 38: 1.0576071739196777
[08/28/2025 08:57:56 INFO]: Training loss at epoch 33: 0.9940797388553619
[08/28/2025 08:58:04 INFO]: Training loss at epoch 15: 0.8834879994392395
[08/28/2025 08:58:07 INFO]: Training loss at epoch 50: 0.9111418426036835
[08/28/2025 08:58:16 INFO]: Training loss at epoch 5: 1.0931779146194458
[08/28/2025 08:58:16 INFO]: Training loss at epoch 34: 1.2728105187416077
[08/28/2025 08:58:17 INFO]: Training loss at epoch 19: 0.8784834444522858
[08/28/2025 08:58:21 INFO]: Training loss at epoch 19: 1.2305450439453125
[08/28/2025 08:58:28 INFO]: Training loss at epoch 32: 1.0938351452350616
[08/28/2025 08:58:33 INFO]: Training loss at epoch 55: 0.9601326882839203
[08/28/2025 08:58:49 INFO]: Training accuracy: {
    "score": -1.0097939464467591,
    "rmse": 1.0097939464467591
}
[08/28/2025 08:58:49 INFO]: Val accuracy: {
    "score": -0.6616226954555516,
    "rmse": 0.6616226954555516
}
[08/28/2025 08:58:49 INFO]: Test accuracy: {
    "score": -0.8710962028535955,
    "rmse": 0.8710962028535955
}
[08/28/2025 08:58:49 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_81",
    "best_epoch": 51,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8710962028535955,
        "rmse": 0.8710962028535955
    },
    "train_stats": {
        "score": -1.0097939464467591,
        "rmse": 1.0097939464467591
    },
    "val_stats": {
        "score": -0.6616226954555516,
        "rmse": 0.6616226954555516
    }
}
[08/28/2025 08:58:49 INFO]: Procewss finished for trial unhanged-Shanesha_trial_81
[08/28/2025 08:58:49 INFO]: 
_________________________________________________

[08/28/2025 08:58:49 INFO]: train_net_for_optune.py main() running.
[08/28/2025 08:58:49 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 2.5573745942038113
  attention_dropout: 0.1711857224012286
  ffn_dropout: 0.1711857224012286
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.0067350912580263e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_133

[08/28/2025 08:58:49 INFO]: This ft_transformer has 2.485 million parameters.
[08/28/2025 08:58:49 INFO]: Training will start at epoch 0.
[08/28/2025 08:58:49 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 08:59:02 INFO]: Training loss at epoch 38: 1.013615757226944
[08/28/2025 08:59:03 INFO]: Training loss at epoch 25: 0.9830188155174255
[08/28/2025 08:59:18 INFO]: Training stats: {
    "score": -1.0087794444816687,
    "rmse": 1.0087794444816687
}
[08/28/2025 08:59:18 INFO]: Val stats: {
    "score": -0.6642119284756925,
    "rmse": 0.6642119284756925
}
[08/28/2025 08:59:18 INFO]: Test stats: {
    "score": -0.8705748926163488,
    "rmse": 0.8705748926163488
}
[08/28/2025 08:59:19 INFO]: Training stats: {
    "score": -1.0109031697295305,
    "rmse": 1.0109031697295305
}
[08/28/2025 08:59:19 INFO]: Val stats: {
    "score": -0.658631571256461,
    "rmse": 0.658631571256461
}
[08/28/2025 08:59:19 INFO]: Test stats: {
    "score": -0.8749941697209578,
    "rmse": 0.8749941697209578
}
[08/28/2025 08:59:20 INFO]: Training loss at epoch 33: 1.013132929801941
[08/28/2025 08:59:32 INFO]: Training loss at epoch 33: 1.0459814071655273
[08/28/2025 08:59:33 INFO]: Training loss at epoch 9: 1.2627365589141846
[08/28/2025 08:59:39 INFO]: New best epoch, val score: -0.6642119284756925
[08/28/2025 08:59:39 INFO]: Saving model to: unhanged-Shanesha_trial_126/model_best.pth
[08/28/2025 08:59:40 INFO]: Training loss at epoch 39: 1.0634902715682983
[08/28/2025 08:59:49 INFO]: Training loss at epoch 34: 1.0051441192626953
[08/28/2025 08:59:56 INFO]: Training loss at epoch 0: 1.2104132175445557
[08/28/2025 09:00:05 INFO]: New best epoch, val score: -0.8376432688626868
[08/28/2025 09:00:05 INFO]: Saving model to: unhanged-Shanesha_trial_133/model_best.pth
[08/28/2025 09:00:07 INFO]: Training loss at epoch 6: 0.8714579343795776
[08/28/2025 09:00:08 INFO]: Training loss at epoch 71: 1.0185208022594452
[08/28/2025 09:00:12 INFO]: Training stats: {
    "score": -0.9978904917242604,
    "rmse": 0.9978904917242604
}
[08/28/2025 09:00:12 INFO]: Val stats: {
    "score": -0.6890968593933271,
    "rmse": 0.6890968593933271
}
[08/28/2025 09:00:12 INFO]: Test stats: {
    "score": -0.8793143663040046,
    "rmse": 0.8793143663040046
}
[08/28/2025 09:00:13 INFO]: Training loss at epoch 14: 1.1119742393493652
[08/28/2025 09:00:18 INFO]: Training stats: {
    "score": -0.9954675016203007,
    "rmse": 0.9954675016203007
}
[08/28/2025 09:00:18 INFO]: Val stats: {
    "score": -0.6731841823691085,
    "rmse": 0.6731841823691085
}
[08/28/2025 09:00:18 INFO]: Test stats: {
    "score": -0.8752389637052244,
    "rmse": 0.8752389637052244
}
[08/28/2025 09:00:36 INFO]: Training loss at epoch 34: 1.0093984007835388
[08/28/2025 09:00:38 INFO]: Training loss at epoch 51: 0.9721751809120178
[08/28/2025 09:00:41 INFO]: Training loss at epoch 61: 1.1067183315753937
[08/28/2025 09:00:46 INFO]: Training loss at epoch 16: 1.0169851183891296
[08/28/2025 09:00:53 INFO]: Training loss at epoch 39: 0.96762415766716
[08/28/2025 09:01:06 INFO]: New best epoch, val score: -0.6685792685960686
[08/28/2025 09:01:06 INFO]: Saving model to: unhanged-Shanesha_trial_128/model_best.pth
[08/28/2025 09:01:07 INFO]: Training loss at epoch 56: 1.009068489074707
[08/28/2025 09:01:13 INFO]: Training loss at epoch 1: 1.1364705562591553
[08/28/2025 09:01:22 INFO]: New best epoch, val score: -0.7361519532458015
[08/28/2025 09:01:22 INFO]: Saving model to: unhanged-Shanesha_trial_133/model_best.pth
[08/28/2025 09:01:25 INFO]: Running Final Evaluation...
[08/28/2025 09:01:31 INFO]: Training stats: {
    "score": -0.9968155305494231,
    "rmse": 0.9968155305494231
}
[08/28/2025 09:01:31 INFO]: Val stats: {
    "score": -0.7231081544556028,
    "rmse": 0.7231081544556028
}
[08/28/2025 09:01:31 INFO]: Test stats: {
    "score": -0.9011112932473865,
    "rmse": 0.9011112932473865
}
[08/28/2025 09:01:35 INFO]: Training loss at epoch 117: 0.9959488809108734
[08/28/2025 09:01:38 INFO]: Training loss at epoch 35: 0.9146103858947754
[08/28/2025 09:01:40 INFO]: Training loss at epoch 35: 1.3371925055980682
[08/28/2025 09:01:44 INFO]: Training loss at epoch 34: 0.9621193110942841
[08/28/2025 09:01:57 INFO]: Training loss at epoch 7: 0.9443605840206146
[08/28/2025 09:02:02 INFO]: Training loss at epoch 10: 1.1416403651237488
[08/28/2025 09:02:05 INFO]: Training loss at epoch 20: 1.004436731338501
[08/28/2025 09:02:09 INFO]: Training loss at epoch 40: 0.9365015625953674
[08/28/2025 09:02:13 INFO]: Training loss at epoch 20: 1.0576189160346985
[08/28/2025 09:02:18 INFO]: Training accuracy: {
    "score": -1.009937163636794,
    "rmse": 1.009937163636794
}
[08/28/2025 09:02:18 INFO]: Val accuracy: {
    "score": -0.6624289685521199,
    "rmse": 0.6624289685521199
}
[08/28/2025 09:02:18 INFO]: Test accuracy: {
    "score": -0.8732156706144829,
    "rmse": 0.8732156706144829
}
[08/28/2025 09:02:18 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_113",
    "best_epoch": 25,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8732156706144829,
        "rmse": 0.8732156706144829
    },
    "train_stats": {
        "score": -1.009937163636794,
        "rmse": 1.009937163636794
    },
    "val_stats": {
        "score": -0.6624289685521199,
        "rmse": 0.6624289685521199
    }
}
[08/28/2025 09:02:18 INFO]: Procewss finished for trial unhanged-Shanesha_trial_113
[08/28/2025 09:02:18 INFO]: 
_________________________________________________

[08/28/2025 09:02:18 INFO]: train_net_for_optune.py main() running.
[08/28/2025 09:02:18 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 2.576776371649427
  attention_dropout: 0.1736836754088894
  ffn_dropout: 0.1736836754088894
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.2522302411911386e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_134

[08/28/2025 09:02:18 INFO]: This ft_transformer has 3.173 million parameters.
[08/28/2025 09:02:18 INFO]: Training will start at epoch 0.
[08/28/2025 09:02:18 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 09:02:30 INFO]: Training loss at epoch 2: 0.8980584740638733
[08/28/2025 09:02:38 INFO]: New best epoch, val score: -0.6877351904310673
[08/28/2025 09:02:38 INFO]: Saving model to: unhanged-Shanesha_trial_133/model_best.pth
[08/28/2025 09:02:44 INFO]: Training loss at epoch 36: 1.1460997462272644
[08/28/2025 09:02:56 INFO]: Training loss at epoch 26: 0.8703510761260986
[08/28/2025 09:03:09 INFO]: Training loss at epoch 52: 1.2565532326698303
[08/28/2025 09:03:09 INFO]: Training loss at epoch 15: 1.0062383711338043
[08/28/2025 09:03:23 INFO]: Training loss at epoch 40: 0.9629936516284943
[08/28/2025 09:03:29 INFO]: Training loss at epoch 17: 0.8794317245483398
[08/28/2025 09:03:30 INFO]: Training loss at epoch 36: 1.2135983407497406
[08/28/2025 09:03:42 INFO]: Training loss at epoch 0: 1.2594693899154663
[08/28/2025 09:03:51 INFO]: Training loss at epoch 3: 0.8785982131958008
[08/28/2025 09:03:51 INFO]: New best epoch, val score: -0.6656479854106246
[08/28/2025 09:03:51 INFO]: Saving model to: unhanged-Shanesha_trial_128/model_best.pth
[08/28/2025 09:03:51 INFO]: Training loss at epoch 37: 0.8765074014663696
[08/28/2025 09:03:52 INFO]: Training loss at epoch 8: 1.0599401593208313
[08/28/2025 09:03:53 INFO]: New best epoch, val score: -0.7404346274720877
[08/28/2025 09:03:53 INFO]: Saving model to: unhanged-Shanesha_trial_134/model_best.pth
[08/28/2025 09:03:56 INFO]: Training loss at epoch 35: 0.7934173941612244
[08/28/2025 09:03:57 INFO]: Training loss at epoch 11: 1.1321069598197937
[08/28/2025 09:04:00 INFO]: New best epoch, val score: -0.6749620451654486
[08/28/2025 09:04:00 INFO]: Saving model to: unhanged-Shanesha_trial_133/model_best.pth
[08/28/2025 09:04:04 INFO]: Training loss at epoch 41: 1.1461182534694672
[08/28/2025 09:04:06 INFO]: New best epoch, val score: -0.657627394375445
[08/28/2025 09:04:06 INFO]: Saving model to: unhanged-Shanesha_trial_132/model_best.pth
[08/28/2025 09:04:13 INFO]: Training loss at epoch 35: 0.8785367906093597
[08/28/2025 09:04:59 INFO]: Training loss at epoch 38: 0.99689981341362
[08/28/2025 09:05:02 INFO]: Training loss at epoch 21: 0.7862755060195923
[08/28/2025 09:05:15 INFO]: Training loss at epoch 4: 0.7994403839111328
[08/28/2025 09:05:16 INFO]: Training loss at epoch 72: 0.9435580670833588
[08/28/2025 09:05:21 INFO]: Training loss at epoch 1: 1.4332234263420105
[08/28/2025 09:05:22 INFO]: Training loss at epoch 41: 1.1110734939575195
[08/28/2025 09:05:23 INFO]: Training loss at epoch 21: 1.2156532406806946
[08/28/2025 09:05:29 INFO]: Training loss at epoch 37: 0.8525760173797607
[08/28/2025 09:05:32 INFO]: New best epoch, val score: -0.6762537975150936
[08/28/2025 09:05:32 INFO]: Saving model to: unhanged-Shanesha_trial_134/model_best.pth
[08/28/2025 09:05:51 INFO]: Training loss at epoch 26: 0.9741207957267761
[08/28/2025 09:05:52 INFO]: Training loss at epoch 53: 0.9102019369602203
[08/28/2025 09:05:53 INFO]: Training loss at epoch 9: 1.3023018836975098
[08/28/2025 09:05:57 INFO]: Training loss at epoch 62: 0.8716070055961609
[08/28/2025 09:05:58 INFO]: Training loss at epoch 12: 1.0324997305870056
[08/28/2025 09:06:06 INFO]: Training loss at epoch 42: 0.8615436255931854
[08/28/2025 09:06:09 INFO]: Training loss at epoch 39: 1.2133093476295471
[08/28/2025 09:06:24 INFO]: Training loss at epoch 16: 0.9596704244613647
[08/28/2025 09:06:29 INFO]: Training loss at epoch 18: 1.0106129050254822
[08/28/2025 09:06:36 INFO]: Training stats: {
    "score": -0.9994446076397787,
    "rmse": 0.9994446076397787
}
[08/28/2025 09:06:36 INFO]: Val stats: {
    "score": -0.6957581549306386,
    "rmse": 0.6957581549306386
}
[08/28/2025 09:06:36 INFO]: Test stats: {
    "score": -0.8794715274867846,
    "rmse": 0.8794715274867846
}
[08/28/2025 09:06:37 INFO]: Training stats: {
    "score": -1.0133908180878775,
    "rmse": 1.0133908180878775
}
[08/28/2025 09:06:37 INFO]: Val stats: {
    "score": -0.6566961124655829,
    "rmse": 0.6566961124655829
}
[08/28/2025 09:06:37 INFO]: Test stats: {
    "score": -0.8791368534767934,
    "rmse": 0.8791368534767934
}
[08/28/2025 09:06:40 INFO]: Training loss at epoch 5: 1.0564789772033691
[08/28/2025 09:06:52 INFO]: New best epoch, val score: -0.6566961124655829
[08/28/2025 09:06:52 INFO]: Saving model to: unhanged-Shanesha_trial_132/model_best.pth
[08/28/2025 09:06:55 INFO]: Training loss at epoch 36: 0.971625417470932
[08/28/2025 09:07:02 INFO]: Training loss at epoch 2: 1.2898044884204865
[08/28/2025 09:07:24 INFO]: Training loss at epoch 42: 0.9324609041213989
[08/28/2025 09:07:31 INFO]: Training loss at epoch 38: 1.1018878817558289
[08/28/2025 09:07:44 INFO]: Training loss at epoch 40: 0.818412572145462
[08/28/2025 09:07:59 INFO]: Training loss at epoch 13: 1.2252987027168274
[08/28/2025 09:08:05 INFO]: Training loss at epoch 6: 1.1118300557136536
[08/28/2025 09:08:07 INFO]: Training loss at epoch 43: 0.9088720381259918
[08/28/2025 09:08:07 INFO]: Training loss at epoch 22: 1.0150051712989807
[08/28/2025 09:08:35 INFO]: Training loss at epoch 10: 1.1178438067436218
[08/28/2025 09:08:35 INFO]: Training loss at epoch 54: 1.1143400073051453
[08/28/2025 09:08:37 INFO]: Training loss at epoch 118: 0.9945065677165985
[08/28/2025 09:08:37 INFO]: Training loss at epoch 22: 0.9495728611946106
[08/28/2025 09:08:38 INFO]: Training loss at epoch 3: 1.0751819610595703
[08/28/2025 09:08:51 INFO]: Training loss at epoch 41: 1.0101287961006165
[08/28/2025 09:09:18 INFO]: Training loss at epoch 43: 0.9977045357227325
[08/28/2025 09:09:18 INFO]: Training loss at epoch 19: 0.9887882769107819
[08/28/2025 09:09:22 INFO]: Training loss at epoch 39: 0.8678124845027924
[08/28/2025 09:09:23 INFO]: Training loss at epoch 7: 0.9629701375961304
[08/28/2025 09:09:25 INFO]: Training loss at epoch 37: 1.303435355424881
[08/28/2025 09:09:30 INFO]: Training loss at epoch 17: 0.9109876751899719
[08/28/2025 09:09:49 INFO]: Training loss at epoch 14: 1.0672330260276794
[08/28/2025 09:09:55 INFO]: Training loss at epoch 42: 0.9841985106468201
[08/28/2025 09:09:57 INFO]: Training loss at epoch 44: 1.3046107292175293
[08/28/2025 09:09:58 INFO]: Training loss at epoch 36: 0.8469163775444031
[08/28/2025 09:10:01 INFO]: Training stats: {
    "score": -0.99877987149594,
    "rmse": 0.99877987149594
}
[08/28/2025 09:10:01 INFO]: Val stats: {
    "score": -0.6980533264375012,
    "rmse": 0.6980533264375012
}
[08/28/2025 09:10:01 INFO]: Test stats: {
    "score": -0.8827585235420852,
    "rmse": 0.8827585235420852
}
[08/28/2025 09:10:09 INFO]: Training loss at epoch 4: 1.156266212463379
[08/28/2025 09:10:14 INFO]: Training stats: {
    "score": -0.9932545308831467,
    "rmse": 0.9932545308831467
}
[08/28/2025 09:10:14 INFO]: Val stats: {
    "score": -0.6899586849844465,
    "rmse": 0.6899586849844465
}
[08/28/2025 09:10:14 INFO]: Test stats: {
    "score": -0.8742426584203092,
    "rmse": 0.8742426584203092
}
[08/28/2025 09:10:19 INFO]: New best epoch, val score: -0.6726998671237073
[08/28/2025 09:10:19 INFO]: Saving model to: unhanged-Shanesha_trial_134/model_best.pth
[08/28/2025 09:10:25 INFO]: Training loss at epoch 11: 1.0381430983543396
[08/28/2025 09:10:29 INFO]: Training loss at epoch 73: 0.9054953455924988
[08/28/2025 09:10:40 INFO]: Training loss at epoch 8: 1.0925016403198242
[08/28/2025 09:10:53 INFO]: Training loss at epoch 27: 0.8164966404438019
[08/28/2025 09:10:54 INFO]: Training loss at epoch 23: 1.0321529805660248
[08/28/2025 09:10:59 INFO]: Training loss at epoch 43: 1.0224199891090393
[08/28/2025 09:11:06 INFO]: Training loss at epoch 55: 1.183059573173523
[08/28/2025 09:11:09 INFO]: Training loss at epoch 63: 1.0950740575790405
[08/28/2025 09:11:09 INFO]: Training loss at epoch 44: 0.9954964220523834
[08/28/2025 09:11:23 INFO]: New best epoch, val score: -0.6619643155526785
[08/28/2025 09:11:23 INFO]: Saving model to: unhanged-Shanesha_trial_124/model_best.pth
[08/28/2025 09:11:33 INFO]: Training loss at epoch 23: 0.8992498517036438
[08/28/2025 09:11:39 INFO]: Training loss at epoch 15: 0.9368961453437805
[08/28/2025 09:11:40 INFO]: Training loss at epoch 5: 0.94339320063591
[08/28/2025 09:11:48 INFO]: Training loss at epoch 45: 0.9951864182949066
[08/28/2025 09:11:52 INFO]: Training loss at epoch 40: 0.9256295263767242
[08/28/2025 09:11:53 INFO]: Training loss at epoch 38: 1.2058653235435486
[08/28/2025 09:11:54 INFO]: New best epoch, val score: -0.673914041214289
[08/28/2025 09:11:54 INFO]: Saving model to: unhanged-Shanesha_trial_131/model_best.pth
[08/28/2025 09:12:01 INFO]: Training loss at epoch 9: 0.977567732334137
[08/28/2025 09:12:05 INFO]: Training loss at epoch 44: 1.072522521018982
[08/28/2025 09:12:19 INFO]: Training loss at epoch 12: 0.9164246320724487
[08/28/2025 09:12:29 INFO]: Training stats: {
    "score": -1.0000744958106074,
    "rmse": 1.0000744958106074
}
[08/28/2025 09:12:29 INFO]: Val stats: {
    "score": -0.6860466064256987,
    "rmse": 0.6860466064256987
}
[08/28/2025 09:12:29 INFO]: Test stats: {
    "score": -0.8838324468650692,
    "rmse": 0.8838324468650692
}
[08/28/2025 09:12:31 INFO]: Training loss at epoch 18: 1.0486976504325867
[08/28/2025 09:12:43 INFO]: Training loss at epoch 27: 1.0266056656837463
[08/28/2025 09:13:03 INFO]: Training loss at epoch 20: 1.1515787839889526
[08/28/2025 09:13:07 INFO]: Training loss at epoch 45: 0.9335517585277557
[08/28/2025 09:13:14 INFO]: Training loss at epoch 45: 1.0866966843605042
[08/28/2025 09:13:20 INFO]: Training loss at epoch 6: 1.2870680391788483
[08/28/2025 09:13:41 INFO]: Training loss at epoch 16: 1.2449201941490173
[08/28/2025 09:13:48 INFO]: Training loss at epoch 56: 1.1805075705051422
[08/28/2025 09:13:50 INFO]: Training loss at epoch 46: 1.039964199066162
[08/28/2025 09:13:52 INFO]: Training loss at epoch 41: 1.3136778771877289
[08/28/2025 09:13:53 INFO]: Training loss at epoch 24: 0.838539332151413
[08/28/2025 09:13:54 INFO]: Training loss at epoch 10: 1.153833270072937
[08/28/2025 09:13:56 INFO]: New best epoch, val score: -0.6722850678306141
[08/28/2025 09:13:56 INFO]: Saving model to: unhanged-Shanesha_trial_131/model_best.pth
[08/28/2025 09:14:20 INFO]: Training loss at epoch 13: 0.9662371873855591
[08/28/2025 09:14:23 INFO]: Training loss at epoch 46: 0.8335346281528473
[08/28/2025 09:14:33 INFO]: Training loss at epoch 39: 1.0384643077850342
[08/28/2025 09:14:47 INFO]: Training loss at epoch 24: 1.0097611844539642
[08/28/2025 09:15:00 INFO]: Training loss at epoch 7: 1.1378851234912872
[08/28/2025 09:15:08 INFO]: Training loss at epoch 46: 0.9418695867061615
[08/28/2025 09:15:18 INFO]: Training loss at epoch 11: 0.9518932700157166
[08/28/2025 09:15:29 INFO]: Training stats: {
    "score": -1.0000945248208386,
    "rmse": 1.0000945248208386
}
[08/28/2025 09:15:29 INFO]: Val stats: {
    "score": -0.6614672195294606,
    "rmse": 0.6614672195294606
}
[08/28/2025 09:15:29 INFO]: Test stats: {
    "score": -0.8705523892900346,
    "rmse": 0.8705523892900346
}
[08/28/2025 09:15:33 INFO]: Training loss at epoch 47: 1.028216302394867
[08/28/2025 09:15:34 INFO]: Training loss at epoch 119: 1.1923601925373077
[08/28/2025 09:15:42 INFO]: Training loss at epoch 17: 1.0566952228546143
[08/28/2025 09:15:45 INFO]: Training loss at epoch 19: 0.8401083946228027
[08/28/2025 09:15:48 INFO]: New best epoch, val score: -0.6614672195294606
[08/28/2025 09:15:48 INFO]: Saving model to: unhanged-Shanesha_trial_120/model_best.pth
[08/28/2025 09:15:53 INFO]: Training loss at epoch 47: 0.9575636386871338
[08/28/2025 09:15:53 INFO]: Training loss at epoch 42: 1.0152378678321838
[08/28/2025 09:15:53 INFO]: Training loss at epoch 74: 1.1084667444229126
[08/28/2025 09:15:58 INFO]: New best epoch, val score: -0.6718977971519774
[08/28/2025 09:15:58 INFO]: Saving model to: unhanged-Shanesha_trial_131/model_best.pth
[08/28/2025 09:15:58 INFO]: Training loss at epoch 37: 0.9053295850753784
[08/28/2025 09:16:01 INFO]: Training loss at epoch 21: 1.213260293006897
[08/28/2025 09:16:22 INFO]: Training loss at epoch 14: 1.1044297814369202
[08/28/2025 09:16:34 INFO]: Training loss at epoch 57: 1.0873147249221802
[08/28/2025 09:16:35 INFO]: Training loss at epoch 64: 0.8878805935382843
[08/28/2025 09:16:40 INFO]: Training loss at epoch 8: 1.1387489140033722
[08/28/2025 09:16:42 INFO]: Training loss at epoch 48: 0.8706540465354919
[08/28/2025 09:16:43 INFO]: Training loss at epoch 12: 1.0234532356262207
[08/28/2025 09:16:52 INFO]: Training stats: {
    "score": -0.9968762136038889,
    "rmse": 0.9968762136038889
}
[08/28/2025 09:16:52 INFO]: Val stats: {
    "score": -0.6764088036764683,
    "rmse": 0.6764088036764683
}
[08/28/2025 09:16:52 INFO]: Test stats: {
    "score": -0.8643724373324474,
    "rmse": 0.8643724373324474
}
[08/28/2025 09:16:59 INFO]: Training loss at epoch 25: 1.1858008801937103
[08/28/2025 09:17:08 INFO]: Training loss at epoch 47: 1.1984358429908752
[08/28/2025 09:17:44 INFO]: Training loss at epoch 18: 1.0391852259635925
[08/28/2025 09:17:52 INFO]: Training loss at epoch 49: 0.7715031057596207
[08/28/2025 09:17:53 INFO]: Training loss at epoch 43: 1.2999832034111023
[08/28/2025 09:17:54 INFO]: Training loss at epoch 48: 1.1481080651283264
[08/28/2025 09:17:58 INFO]: New best epoch, val score: -0.6713940792791748
[08/28/2025 09:17:58 INFO]: Saving model to: unhanged-Shanesha_trial_131/model_best.pth
[08/28/2025 09:18:01 INFO]: Training loss at epoch 25: 1.2512278854846954
[08/28/2025 09:18:02 INFO]: Training stats: {
    "score": -0.9967524843059659,
    "rmse": 0.9967524843059659
}
[08/28/2025 09:18:02 INFO]: Val stats: {
    "score": -0.6782894361024564,
    "rmse": 0.6782894361024564
}
[08/28/2025 09:18:02 INFO]: Test stats: {
    "score": -0.8727770485176275,
    "rmse": 0.8727770485176275
}
[08/28/2025 09:18:06 INFO]: Training loss at epoch 13: 0.7708766907453537
[08/28/2025 09:18:07 INFO]: Training loss at epoch 40: 1.1103689670562744
[08/28/2025 09:18:07 INFO]: Running Final Evaluation...
[08/28/2025 09:18:15 INFO]: Training stats: {
    "score": -0.9980909057990465,
    "rmse": 0.9980909057990465
}
[08/28/2025 09:18:15 INFO]: Val stats: {
    "score": -0.6945904342523006,
    "rmse": 0.6945904342523006
}
[08/28/2025 09:18:15 INFO]: Test stats: {
    "score": -0.8792130648287638,
    "rmse": 0.8792130648287638
}
[08/28/2025 09:18:17 INFO]: Training loss at epoch 9: 0.9156961441040039
[08/28/2025 09:18:19 INFO]: Training loss at epoch 15: 0.9309380948543549
[08/28/2025 09:18:48 INFO]: Training stats: {
    "score": -1.0035539677873426,
    "rmse": 1.0035539677873426
}
[08/28/2025 09:18:48 INFO]: Val stats: {
    "score": -0.6745292844521836,
    "rmse": 0.6745292844521836
}
[08/28/2025 09:18:48 INFO]: Test stats: {
    "score": -0.8620876253087617,
    "rmse": 0.8620876253087617
}
[08/28/2025 09:18:54 INFO]: Training loss at epoch 22: 0.905293732881546
[08/28/2025 09:18:54 INFO]: Training accuracy: {
    "score": -1.0116593115700139,
    "rmse": 1.0116593115700139
}
[08/28/2025 09:18:54 INFO]: Val accuracy: {
    "score": -0.6591580469756924,
    "rmse": 0.6591580469756924
}
[08/28/2025 09:18:54 INFO]: Test accuracy: {
    "score": -0.8760012296094357,
    "rmse": 0.8760012296094357
}
[08/28/2025 09:18:54 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_123",
    "best_epoch": 17,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8760012296094357,
        "rmse": 0.8760012296094357
    },
    "train_stats": {
        "score": -1.0116593115700139,
        "rmse": 1.0116593115700139
    },
    "val_stats": {
        "score": -0.6591580469756924,
        "rmse": 0.6591580469756924
    }
}
[08/28/2025 09:18:54 INFO]: Procewss finished for trial unhanged-Shanesha_trial_123
[08/28/2025 09:18:54 INFO]: 
_________________________________________________

[08/28/2025 09:18:54 INFO]: train_net_for_optune.py main() running.
[08/28/2025 09:18:54 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.5958485439340624
  attention_dropout: 0.36034816331503594
  ffn_dropout: 0.36034816331503594
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 3.521202946569925e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_135

[08/28/2025 09:18:54 INFO]: This ft_transformer has 0.651 million parameters.
[08/28/2025 09:18:54 INFO]: Training will start at epoch 0.
[08/28/2025 09:18:54 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 09:18:58 INFO]: Training loss at epoch 28: 0.9665540754795074
[08/28/2025 09:19:04 INFO]: Training loss at epoch 48: 1.2632221281528473
[08/28/2025 09:19:14 INFO]: Training loss at epoch 58: 1.2130288481712341
[08/28/2025 09:19:21 INFO]: Training loss at epoch 50: 1.0195069313049316
[08/28/2025 09:19:25 INFO]: Training loss at epoch 14: 1.1050978302955627
[08/28/2025 09:19:38 INFO]: Training loss at epoch 19: 1.195051670074463
[08/28/2025 09:19:45 INFO]: Training loss at epoch 44: 1.0763007402420044
[08/28/2025 09:19:48 INFO]: Training loss at epoch 28: 1.056593656539917
[08/28/2025 09:19:52 INFO]: Training loss at epoch 0: 1.1285644173622131
[08/28/2025 09:19:54 INFO]: Training loss at epoch 26: 1.0236382484436035
[08/28/2025 09:19:56 INFO]: Training loss at epoch 20: 0.9462074339389801
[08/28/2025 09:20:01 INFO]: New best epoch, val score: -0.6613214060263028
[08/28/2025 09:20:01 INFO]: Saving model to: unhanged-Shanesha_trial_135/model_best.pth
[08/28/2025 09:20:12 INFO]: Training loss at epoch 16: 1.0423458814620972
[08/28/2025 09:20:16 INFO]: Training stats: {
    "score": -1.001495377483239,
    "rmse": 1.001495377483239
}
[08/28/2025 09:20:16 INFO]: Val stats: {
    "score": -0.6712910173745086,
    "rmse": 0.6712910173745086
}
[08/28/2025 09:20:16 INFO]: Test stats: {
    "score": -0.8729124484639409,
    "rmse": 0.8729124484639409
}
[08/28/2025 09:20:20 INFO]: Training loss at epoch 10: 1.0567224621772766
[08/28/2025 09:20:25 INFO]: Training loss at epoch 51: 0.848114401102066
[08/28/2025 09:20:29 INFO]: New best epoch, val score: -0.6712910173745086
[08/28/2025 09:20:29 INFO]: Saving model to: unhanged-Shanesha_trial_131/model_best.pth
[08/28/2025 09:20:34 INFO]: Training loss at epoch 41: 1.1358138918876648
[08/28/2025 09:20:42 INFO]: Training loss at epoch 15: 1.011338472366333
[08/28/2025 09:20:55 INFO]: Training loss at epoch 49: 0.8837422430515289
[08/28/2025 09:20:59 INFO]: Training loss at epoch 1: 0.8435522019863129
[08/28/2025 09:21:00 INFO]: Training loss at epoch 26: 1.0320817828178406
[08/28/2025 09:21:07 INFO]: Training loss at epoch 75: 1.0835613012313843
[08/28/2025 09:21:08 INFO]: New best epoch, val score: -0.6589192910415337
[08/28/2025 09:21:08 INFO]: Saving model to: unhanged-Shanesha_trial_135/model_best.pth
[08/28/2025 09:21:32 INFO]: Training loss at epoch 52: 0.8855338990688324
[08/28/2025 09:21:36 INFO]: Training stats: {
    "score": -0.9844682015703863,
    "rmse": 0.9844682015703863
}
[08/28/2025 09:21:36 INFO]: Val stats: {
    "score": -0.6813212409307092,
    "rmse": 0.6813212409307092
}
[08/28/2025 09:21:36 INFO]: Test stats: {
    "score": -0.8780098862397199,
    "rmse": 0.8780098862397199
}
[08/28/2025 09:21:38 INFO]: Training loss at epoch 23: 0.951177179813385
[08/28/2025 09:21:39 INFO]: Training loss at epoch 45: 0.9723798036575317
[08/28/2025 09:21:41 INFO]: Running Final Evaluation...
[08/28/2025 09:21:48 INFO]: Training loss at epoch 59: 1.1356775760650635
[08/28/2025 09:21:49 INFO]: Training loss at epoch 65: 1.0795275568962097
[08/28/2025 09:21:51 INFO]: Training loss at epoch 38: 0.9559493064880371
[08/28/2025 09:21:55 INFO]: Training loss at epoch 11: 1.1404891610145569
[08/28/2025 09:22:05 INFO]: Training loss at epoch 16: 1.058464765548706
[08/28/2025 09:22:07 INFO]: Training accuracy: {
    "score": -1.0008999640253684,
    "rmse": 1.0008999640253684
}
[08/28/2025 09:22:07 INFO]: Val accuracy: {
    "score": -0.6832994222534627,
    "rmse": 0.6832994222534627
}
[08/28/2025 09:22:07 INFO]: Test accuracy: {
    "score": -0.8744368483923163,
    "rmse": 0.8744368483923163
}
[08/28/2025 09:22:07 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_130",
    "best_epoch": 21,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8744368483923163,
        "rmse": 0.8744368483923163
    },
    "train_stats": {
        "score": -1.0008999640253684,
        "rmse": 1.0008999640253684
    },
    "val_stats": {
        "score": -0.6832994222534627,
        "rmse": 0.6832994222534627
    }
}
[08/28/2025 09:22:07 INFO]: Procewss finished for trial unhanged-Shanesha_trial_130
[08/28/2025 09:22:07 INFO]: 
_________________________________________________

[08/28/2025 09:22:07 INFO]: train_net_for_optune.py main() running.
[08/28/2025 09:22:07 INFO]: model:
  name: ft_transformer
  d_embedding: 256
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.5747364995409683
  attention_dropout: 0.3596984700164343
  ffn_dropout: 0.3596984700164343
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0001227098291959602
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_136

[08/28/2025 09:22:07 INFO]: This ft_transformer has 2.495 million parameters.
[08/28/2025 09:22:07 INFO]: Training will start at epoch 0.
[08/28/2025 09:22:07 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 09:22:09 INFO]: Training loss at epoch 17: 1.0098524391651154
[08/28/2025 09:22:09 INFO]: Training loss at epoch 2: 1.1579144597053528
[08/28/2025 09:22:13 INFO]: Training loss at epoch 20: 0.9672004878520966
[08/28/2025 09:22:45 INFO]: Training stats: {
    "score": -0.9935061958708382,
    "rmse": 0.9935061958708382
}
[08/28/2025 09:22:45 INFO]: Val stats: {
    "score": -0.670605772515181,
    "rmse": 0.670605772515181
}
[08/28/2025 09:22:45 INFO]: Test stats: {
    "score": -0.8786157038244922,
    "rmse": 0.8786157038244922
}
[08/28/2025 09:22:52 INFO]: Training loss at epoch 27: 0.875522255897522
[08/28/2025 09:23:03 INFO]: Training loss at epoch 21: 0.9443674683570862
[08/28/2025 09:23:11 INFO]: Training loss at epoch 42: 0.9961378276348114
[08/28/2025 09:23:21 INFO]: Training loss at epoch 3: 1.005176454782486
[08/28/2025 09:23:30 INFO]: Training loss at epoch 17: 0.9331208765506744
[08/28/2025 09:23:36 INFO]: Training loss at epoch 12: 0.9514244198799133
[08/28/2025 09:23:37 INFO]: Training loss at epoch 50: 1.0141199827194214
[08/28/2025 09:23:40 INFO]: Training loss at epoch 46: 1.0688972473144531
[08/28/2025 09:23:52 INFO]: Training loss at epoch 0: 1.2870457172393799
[08/28/2025 09:23:55 INFO]: Running Final Evaluation...
[08/28/2025 09:24:06 INFO]: New best epoch, val score: -0.8340982796800247
[08/28/2025 09:24:06 INFO]: Saving model to: unhanged-Shanesha_trial_136/model_best.pth
[08/28/2025 09:24:10 INFO]: Training loss at epoch 18: 0.9171200096607208
[08/28/2025 09:24:13 INFO]: Training loss at epoch 27: 1.0453450083732605
[08/28/2025 09:24:15 INFO]: Training loss at epoch 21: 1.0956783890724182
[08/28/2025 09:24:32 INFO]: Training loss at epoch 4: 1.0400625467300415
[08/28/2025 09:24:34 INFO]: Training loss at epoch 24: 1.0525758862495422
[08/28/2025 09:24:37 INFO]: Training accuracy: {
    "score": -1.0102906235295817,
    "rmse": 1.0102906235295817
}
[08/28/2025 09:24:37 INFO]: Val accuracy: {
    "score": -0.6606328243005011,
    "rmse": 0.6606328243005011
}
[08/28/2025 09:24:37 INFO]: Test accuracy: {
    "score": -0.8695790874722671,
    "rmse": 0.8695790874722671
}
[08/28/2025 09:24:37 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_125",
    "best_epoch": 15,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8695790874722671,
        "rmse": 0.8695790874722671
    },
    "train_stats": {
        "score": -1.0102906235295817,
        "rmse": 1.0102906235295817
    },
    "val_stats": {
        "score": -0.6606328243005011,
        "rmse": 0.6606328243005011
    }
}
[08/28/2025 09:24:37 INFO]: Procewss finished for trial unhanged-Shanesha_trial_125
[08/28/2025 09:24:37 INFO]: 
_________________________________________________

[08/28/2025 09:24:37 INFO]: train_net_for_optune.py main() running.
[08/28/2025 09:24:37 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.5637891863573263
  attention_dropout: 0.35567069379769606
  ffn_dropout: 0.35567069379769606
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0008926245393001558
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_137

[08/28/2025 09:24:37 INFO]: This ft_transformer has 0.646 million parameters.
[08/28/2025 09:24:37 INFO]: Training will start at epoch 0.
[08/28/2025 09:24:37 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 09:24:55 INFO]: Training loss at epoch 18: 1.0766347646713257
[08/28/2025 09:24:56 INFO]: New best epoch, val score: -0.6656326958095614
[08/28/2025 09:24:56 INFO]: Saving model to: unhanged-Shanesha_trial_128/model_best.pth
[08/28/2025 09:25:01 INFO]: Training loss at epoch 120: 0.8928233981132507
[08/28/2025 09:25:16 INFO]: Training loss at epoch 13: 1.0182440280914307
[08/28/2025 09:25:30 INFO]: Training loss at epoch 60: 0.9037297368049622
[08/28/2025 09:25:39 INFO]: Training loss at epoch 51: 0.905166506767273
[08/28/2025 09:25:39 INFO]: Training loss at epoch 0: 1.123705804347992
[08/28/2025 09:25:43 INFO]: Training loss at epoch 5: 0.9340629875659943
[08/28/2025 09:25:48 INFO]: New best epoch, val score: -0.6605982073797955
[08/28/2025 09:25:48 INFO]: Saving model to: unhanged-Shanesha_trial_137/model_best.pth
[08/28/2025 09:25:51 INFO]: Training loss at epoch 1: 1.078475832939148
[08/28/2025 09:25:51 INFO]: Training loss at epoch 43: 0.9906648099422455
[08/28/2025 09:25:57 INFO]: Training loss at epoch 28: 1.1380518674850464
[08/28/2025 09:26:10 INFO]: Training loss at epoch 19: 1.0264938473701477
[08/28/2025 09:26:13 INFO]: Training loss at epoch 22: 0.725849837064743
[08/28/2025 09:26:15 INFO]: Training loss at epoch 22: 1.0245736241340637
[08/28/2025 09:26:16 INFO]: Training loss at epoch 19: 0.9400060474872589
[08/28/2025 09:26:30 INFO]: Training loss at epoch 76: 0.8862719237804413
[08/28/2025 09:26:42 INFO]: Training stats: {
    "score": -1.00096074992365,
    "rmse": 1.00096074992365
}
[08/28/2025 09:26:42 INFO]: Val stats: {
    "score": -0.6986845938832301,
    "rmse": 0.6986845938832301
}
[08/28/2025 09:26:42 INFO]: Test stats: {
    "score": -0.891494309989782,
    "rmse": 0.891494309989782
}
[08/28/2025 09:26:46 INFO]: Training loss at epoch 1: 2.2058361768722534
[08/28/2025 09:26:49 INFO]: Training loss at epoch 6: 1.0308760106563568
[08/28/2025 09:26:49 INFO]: Training loss at epoch 14: 1.1809439659118652
[08/28/2025 09:26:50 INFO]: Training stats: {
    "score": -0.998930386863362,
    "rmse": 0.998930386863362
}
[08/28/2025 09:26:50 INFO]: Val stats: {
    "score": -0.6947813354041291,
    "rmse": 0.6947813354041291
}
[08/28/2025 09:26:50 INFO]: Test stats: {
    "score": -0.889250351489989,
    "rmse": 0.889250351489989
}
[08/28/2025 09:26:50 INFO]: Training loss at epoch 29: 0.9190438985824585
[08/28/2025 09:26:54 INFO]: Training loss at epoch 29: 1.053241640329361
[08/28/2025 09:27:11 INFO]: Training loss at epoch 66: 0.9572261571884155
[08/28/2025 09:27:17 INFO]: Training loss at epoch 28: 1.0474661588668823
[08/28/2025 09:27:22 INFO]: Training loss at epoch 25: 0.8934630751609802
[08/28/2025 09:27:30 INFO]: Training loss at epoch 52: 1.005782663822174
[08/28/2025 09:27:39 INFO]: Training loss at epoch 2: 1.3338614106178284
[08/28/2025 09:27:43 INFO]: New best epoch, val score: -0.6616603309428467
[08/28/2025 09:27:43 INFO]: Saving model to: unhanged-Shanesha_trial_124/model_best.pth
[08/28/2025 09:27:49 INFO]: Training loss at epoch 39: 0.7711306512355804
[08/28/2025 09:27:51 INFO]: Training loss at epoch 2: 1.1799671947956085
[08/28/2025 09:27:53 INFO]: New best epoch, val score: -0.6661336174317688
[08/28/2025 09:27:53 INFO]: Saving model to: unhanged-Shanesha_trial_136/model_best.pth
[08/28/2025 09:27:55 INFO]: Training loss at epoch 7: 0.9547244906425476
[08/28/2025 09:27:59 INFO]: Training loss at epoch 20: 0.9816475510597229
[08/28/2025 09:28:02 INFO]: Training loss at epoch 61: 0.9875825047492981
[08/28/2025 09:28:05 INFO]: Training loss at epoch 23: 1.0923974514007568
[08/28/2025 09:28:15 INFO]: Training loss at epoch 44: 0.8777709603309631
[08/28/2025 09:28:20 INFO]: Training loss at epoch 15: 0.8501037955284119
[08/28/2025 09:28:41 INFO]: Training loss at epoch 20: 0.8559747040271759
[08/28/2025 09:28:44 INFO]: Training loss at epoch 29: 1.0360862612724304
[08/28/2025 09:28:56 INFO]: Training loss at epoch 3: 1.4559808373451233
[08/28/2025 09:29:01 INFO]: Training loss at epoch 8: 1.1484196186065674
[08/28/2025 09:29:05 INFO]: Training stats: {
    "score": -0.9977398285729142,
    "rmse": 0.9977398285729142
}
[08/28/2025 09:29:05 INFO]: Val stats: {
    "score": -0.7160993896846574,
    "rmse": 0.7160993896846574
}
[08/28/2025 09:29:05 INFO]: Test stats: {
    "score": -0.8878359779263302,
    "rmse": 0.8878359779263302
}
[08/28/2025 09:29:08 INFO]: Training loss at epoch 23: 0.9551255702972412
[08/28/2025 09:29:15 INFO]: Training loss at epoch 21: 1.0962860882282257
[08/28/2025 09:29:20 INFO]: Training loss at epoch 53: 1.0188803672790527
[08/28/2025 09:29:24 INFO]: Training stats: {
    "score": -1.0033019475881262,
    "rmse": 1.0033019475881262
}
[08/28/2025 09:29:24 INFO]: Val stats: {
    "score": -0.6540622564775785,
    "rmse": 0.6540622564775785
}
[08/28/2025 09:29:24 INFO]: Test stats: {
    "score": -0.8702292685230747,
    "rmse": 0.8702292685230747
}
[08/28/2025 09:29:31 INFO]: Training loss at epoch 3: 0.9970324337482452
[08/28/2025 09:29:42 INFO]: Training stats: {
    "score": -0.9994270806598561,
    "rmse": 0.9994270806598561
}
[08/28/2025 09:29:42 INFO]: Val stats: {
    "score": -0.6923414113702149,
    "rmse": 0.6923414113702149
}
[08/28/2025 09:29:42 INFO]: Test stats: {
    "score": -0.8854989997353804,
    "rmse": 0.8854989997353804
}
[08/28/2025 09:29:42 INFO]: Training stats: {
    "score": -0.9964842744834261,
    "rmse": 0.9964842744834261
}
[08/28/2025 09:29:42 INFO]: Val stats: {
    "score": -0.6845463306691226,
    "rmse": 0.6845463306691226
}
[08/28/2025 09:29:42 INFO]: Test stats: {
    "score": -0.8761079885582567,
    "rmse": 0.8761079885582567
}
[08/28/2025 09:29:50 INFO]: Training loss at epoch 16: 1.127916157245636
[08/28/2025 09:29:57 INFO]: Training loss at epoch 24: 0.955106645822525
[08/28/2025 09:30:03 INFO]: Training loss at epoch 4: 0.9281117618083954
[08/28/2025 09:30:03 INFO]: Training loss at epoch 26: 0.8716249167919159
[08/28/2025 09:30:07 INFO]: Training loss at epoch 9: 1.0002556443214417
[08/28/2025 09:30:14 INFO]: Training loss at epoch 29: 1.173102229833603
[08/28/2025 09:30:32 INFO]: Training stats: {
    "score": -1.0030280204055082,
    "rmse": 1.0030280204055082
}
[08/28/2025 09:30:32 INFO]: Val stats: {
    "score": -0.6627624399345972,
    "rmse": 0.6627624399345972
}
[08/28/2025 09:30:32 INFO]: Test stats: {
    "score": -0.8670141089001379,
    "rmse": 0.8670141089001379
}
[08/28/2025 09:30:34 INFO]: Training loss at epoch 62: 1.008250206708908
[08/28/2025 09:30:34 INFO]: Training loss at epoch 21: 1.2784144282341003
[08/28/2025 09:30:35 INFO]: Training loss at epoch 22: 1.2330674529075623
[08/28/2025 09:30:44 INFO]: Training loss at epoch 45: 1.075917363166809
[08/28/2025 09:31:12 INFO]: Training loss at epoch 5: 0.9229215383529663
[08/28/2025 09:31:16 INFO]: Training loss at epoch 54: 0.8584627509117126
[08/28/2025 09:31:20 INFO]: Training stats: {
    "score": -1.0070010046820042,
    "rmse": 1.0070010046820042
}
[08/28/2025 09:31:20 INFO]: Val stats: {
    "score": -0.7304926877305538,
    "rmse": 0.7304926877305538
}
[08/28/2025 09:31:20 INFO]: Test stats: {
    "score": -0.9000158158947276,
    "rmse": 0.9000158158947276
}
[08/28/2025 09:31:27 INFO]: Training loss at epoch 4: 0.9988861680030823
[08/28/2025 09:31:29 INFO]: Training loss at epoch 17: 1.1480650305747986
[08/28/2025 09:31:37 INFO]: Training loss at epoch 77: 1.139987289905548
[08/28/2025 09:31:43 INFO]: New best epoch, val score: -0.6621728640282467
[08/28/2025 09:31:43 INFO]: Saving model to: unhanged-Shanesha_trial_136/model_best.pth
[08/28/2025 09:31:43 INFO]: Training loss at epoch 10: 0.9656096696853638
[08/28/2025 09:31:52 INFO]: Training loss at epoch 121: 0.9483392834663391
[08/28/2025 09:31:57 INFO]: Training loss at epoch 25: 1.080479085445404
[08/28/2025 09:32:00 INFO]: Training loss at epoch 23: 1.2759946584701538
[08/28/2025 09:32:16 INFO]: Training loss at epoch 24: 1.048744022846222
[08/28/2025 09:32:23 INFO]: Training loss at epoch 67: 0.8489785194396973
[08/28/2025 09:32:23 INFO]: Training loss at epoch 6: 1.22646164894104
[08/28/2025 09:32:35 INFO]: Training loss at epoch 22: 0.9226422905921936
[08/28/2025 09:32:44 INFO]: Training loss at epoch 30: 0.9297268092632294
[08/28/2025 09:32:55 INFO]: Training loss at epoch 11: 1.014341115951538
[08/28/2025 09:32:58 INFO]: Training loss at epoch 27: 1.1018684804439545
[08/28/2025 09:33:09 INFO]: Training loss at epoch 18: 0.8737728893756866
[08/28/2025 09:33:18 INFO]: Training loss at epoch 55: 0.9031725525856018
[08/28/2025 09:33:20 INFO]: Training loss at epoch 63: 0.9580771028995514
[08/28/2025 09:33:24 INFO]: Training loss at epoch 46: 1.3426896631717682
[08/28/2025 09:33:25 INFO]: Training loss at epoch 24: 0.7425205558538437
[08/28/2025 09:33:28 INFO]: Training loss at epoch 5: 0.9664153158664703
[08/28/2025 09:33:35 INFO]: Training loss at epoch 7: 1.1879810094833374
[08/28/2025 09:33:43 INFO]: New best epoch, val score: -0.6618092876939484
[08/28/2025 09:33:43 INFO]: Saving model to: unhanged-Shanesha_trial_136/model_best.pth
[08/28/2025 09:33:58 INFO]: Training loss at epoch 26: 0.944399505853653
[08/28/2025 09:34:07 INFO]: Training loss at epoch 12: 1.3842434883117676
[08/28/2025 09:34:36 INFO]: Training loss at epoch 30: 1.0980867743492126
[08/28/2025 09:34:37 INFO]: Training loss at epoch 23: 1.082899570465088
[08/28/2025 09:34:46 INFO]: Training loss at epoch 8: 0.9970660507678986
[08/28/2025 09:34:50 INFO]: Training loss at epoch 19: 1.0018819868564606
[08/28/2025 09:34:50 INFO]: Training loss at epoch 25: 1.098449170589447
[08/28/2025 09:35:17 INFO]: Training loss at epoch 13: 1.0957086682319641
[08/28/2025 09:35:18 INFO]: Training loss at epoch 56: 1.0368041694164276
[08/28/2025 09:35:22 INFO]: Training stats: {
    "score": -0.9969622080160645,
    "rmse": 0.9969622080160645
}
[08/28/2025 09:35:22 INFO]: Val stats: {
    "score": -0.6750379980578468,
    "rmse": 0.6750379980578468
}
[08/28/2025 09:35:22 INFO]: Test stats: {
    "score": -0.8636453016622861,
    "rmse": 0.8636453016622861
}
[08/28/2025 09:35:25 INFO]: Training loss at epoch 6: 1.2068599164485931
[08/28/2025 09:35:28 INFO]: Training loss at epoch 25: 1.1497700810432434
[08/28/2025 09:35:47 INFO]: Training loss at epoch 31: 0.9378073811531067
[08/28/2025 09:35:48 INFO]: Training loss at epoch 40: 1.0055883526802063
[08/28/2025 09:35:51 INFO]: Training loss at epoch 28: 1.1232579946517944
[08/28/2025 09:35:53 INFO]: Training loss at epoch 9: 1.1156678199768066
[08/28/2025 09:35:56 INFO]: Training loss at epoch 27: 0.9389280080795288
[08/28/2025 09:36:00 INFO]: Training loss at epoch 47: 1.042324185371399
[08/28/2025 09:36:01 INFO]: Training loss at epoch 64: 1.0062078833580017
[08/28/2025 09:36:10 INFO]: Training loss at epoch 26: 1.017248809337616
[08/28/2025 09:36:10 INFO]: Training loss at epoch 30: 1.4454901218414307
[08/28/2025 09:36:17 INFO]: Training stats: {
    "score": -1.049366132027687,
    "rmse": 1.049366132027687
}
[08/28/2025 09:36:17 INFO]: Val stats: {
    "score": -0.8180275246454672,
    "rmse": 0.8180275246454672
}
[08/28/2025 09:36:17 INFO]: Test stats: {
    "score": -0.9633906379310585,
    "rmse": 0.9633906379310585
}
[08/28/2025 09:36:19 INFO]: Running Final Evaluation...
[08/28/2025 09:36:23 INFO]: Training loss at epoch 14: 1.1401552855968475
[08/28/2025 09:36:30 INFO]: Training loss at epoch 24: 0.9820860028266907
[08/28/2025 09:36:44 INFO]: New best epoch, val score: -0.6564271589842968
[08/28/2025 09:36:44 INFO]: Saving model to: unhanged-Shanesha_trial_132/model_best.pth
[08/28/2025 09:36:53 INFO]: Training loss at epoch 20: 1.1521462202072144
[08/28/2025 09:36:56 INFO]: Training loss at epoch 78: 1.1132433712482452
[08/28/2025 09:37:08 INFO]: Training loss at epoch 57: 1.0012137293815613
[08/28/2025 09:37:12 INFO]: Training accuracy: {
    "score": -1.0046458922401622,
    "rmse": 1.0046458922401622
}
[08/28/2025 09:37:12 INFO]: Val accuracy: {
    "score": -0.655179346674475,
    "rmse": 0.655179346674475
}
[08/28/2025 09:37:12 INFO]: Test accuracy: {
    "score": -0.8727240051236348,
    "rmse": 0.8727240051236348
}
[08/28/2025 09:37:12 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_115",
    "best_epoch": 33,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8727240051236348,
        "rmse": 0.8727240051236348
    },
    "train_stats": {
        "score": -1.0046458922401622,
        "rmse": 1.0046458922401622
    },
    "val_stats": {
        "score": -0.655179346674475,
        "rmse": 0.655179346674475
    }
}
[08/28/2025 09:37:12 INFO]: Procewss finished for trial unhanged-Shanesha_trial_115
[08/28/2025 09:37:13 INFO]: 
_________________________________________________

[08/28/2025 09:37:13 INFO]: train_net_for_optune.py main() running.
[08/28/2025 09:37:13 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.573623246463377
  attention_dropout: 0.3089352094031649
  ffn_dropout: 0.3089352094031649
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0008525058487769336
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_138

[08/28/2025 09:37:13 INFO]: This ft_transformer has 7.061 million parameters.
[08/28/2025 09:37:13 INFO]: Training will start at epoch 0.
[08/28/2025 09:37:13 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 09:37:14 INFO]: Training loss at epoch 7: 0.8629329800605774
[08/28/2025 09:37:22 INFO]: Training loss at epoch 30: 1.0807294249534607
[08/28/2025 09:37:23 INFO]: Training loss at epoch 10: 1.1680486798286438
[08/28/2025 09:37:27 INFO]: Training loss at epoch 27: 1.1295570135116577
[08/28/2025 09:37:29 INFO]: Training loss at epoch 15: 0.8304502367973328
[08/28/2025 09:37:34 INFO]: Training loss at epoch 31: 0.9368714094161987
[08/28/2025 09:37:39 INFO]: Training loss at epoch 68: 1.0082008838653564
[08/28/2025 09:37:47 INFO]: Training loss at epoch 28: 1.1616219282150269
[08/28/2025 09:38:21 INFO]: Training loss at epoch 25: 1.0443772077560425
[08/28/2025 09:38:23 INFO]: Training loss at epoch 21: 0.8651695251464844
[08/28/2025 09:38:25 INFO]: Training loss at epoch 26: 1.1323151588439941
[08/28/2025 09:38:26 INFO]: Training loss at epoch 48: 0.8229248523712158
[08/28/2025 09:38:29 INFO]: Training loss at epoch 11: 1.0518872737884521
[08/28/2025 09:38:31 INFO]: Training loss at epoch 29: 0.8142847120761871
[08/28/2025 09:38:34 INFO]: Training loss at epoch 32: 1.112146258354187
[08/28/2025 09:38:34 INFO]: Training loss at epoch 16: 1.0475213825702667
[08/28/2025 09:38:44 INFO]: Training loss at epoch 28: 1.185573399066925
[08/28/2025 09:38:47 INFO]: Training loss at epoch 122: 1.2595421969890594
[08/28/2025 09:38:59 INFO]: Training loss at epoch 58: 0.8490546345710754
[08/28/2025 09:39:04 INFO]: Training loss at epoch 8: 1.1001126170158386
[08/28/2025 09:39:31 INFO]: Training stats: {
    "score": -0.9906497333606641,
    "rmse": 0.9906497333606641
}
[08/28/2025 09:39:31 INFO]: Val stats: {
    "score": -0.6977100625230883,
    "rmse": 0.6977100625230883
}
[08/28/2025 09:39:31 INFO]: Test stats: {
    "score": -0.881519372755081,
    "rmse": 0.881519372755081
}
[08/28/2025 09:39:38 INFO]: Training loss at epoch 12: 1.0566079914569855
[08/28/2025 09:39:43 INFO]: Training loss at epoch 29: 1.1905659437179565
[08/28/2025 09:39:44 INFO]: Training loss at epoch 17: 1.0963810086250305
[08/28/2025 09:39:59 INFO]: Training loss at epoch 22: 0.9321510791778564
[08/28/2025 09:40:07 INFO]: Training loss at epoch 29: 1.0102844834327698
[08/28/2025 09:40:18 INFO]: Training loss at epoch 26: 1.0254727005958557
[08/28/2025 09:40:20 INFO]: Training loss at epoch 0: 1.536509394645691
[08/28/2025 09:40:25 INFO]: Training stats: {
    "score": -1.001281743200151,
    "rmse": 1.001281743200151
}
[08/28/2025 09:40:25 INFO]: Val stats: {
    "score": -0.7161434787158438,
    "rmse": 0.7161434787158438
}
[08/28/2025 09:40:25 INFO]: Test stats: {
    "score": -0.8952163417809735,
    "rmse": 0.8952163417809735
}
[08/28/2025 09:40:37 INFO]: Training stats: {
    "score": -0.9995446292517187,
    "rmse": 0.9995446292517187
}
[08/28/2025 09:40:37 INFO]: Val stats: {
    "score": -0.6969003094107679,
    "rmse": 0.6969003094107679
}
[08/28/2025 09:40:37 INFO]: Test stats: {
    "score": -0.8902640083239901,
    "rmse": 0.8902640083239901
}
[08/28/2025 09:40:39 INFO]: Training loss at epoch 32: 0.9413662254810333
[08/28/2025 09:40:47 INFO]: New best epoch, val score: -0.7532096599103953
[08/28/2025 09:40:47 INFO]: Saving model to: unhanged-Shanesha_trial_138/model_best.pth
[08/28/2025 09:40:48 INFO]: Training loss at epoch 13: 1.062999188899994
[08/28/2025 09:40:56 INFO]: Training loss at epoch 18: 1.2454168200492859
[08/28/2025 09:40:59 INFO]: Training loss at epoch 59: 0.9072804749011993
[08/28/2025 09:41:02 INFO]: Training loss at epoch 9: 1.1855630278587341
[08/28/2025 09:41:06 INFO]: Training loss at epoch 49: 0.9944954812526703
[08/28/2025 09:41:35 INFO]: Training loss at epoch 27: 1.1098376214504242
[08/28/2025 09:41:37 INFO]: Training loss at epoch 33: 1.1132451295852661
[08/28/2025 09:41:39 INFO]: Training loss at epoch 41: 1.1006319522857666
[08/28/2025 09:41:39 INFO]: Training loss at epoch 23: 1.0087304413318634
[08/28/2025 09:41:41 INFO]: Training stats: {
    "score": -0.9799943194206893,
    "rmse": 0.9799943194206893
}
[08/28/2025 09:41:41 INFO]: Val stats: {
    "score": -0.6752487020711625,
    "rmse": 0.6752487020711625
}
[08/28/2025 09:41:41 INFO]: Test stats: {
    "score": -0.8767548116639314,
    "rmse": 0.8767548116639314
}
[08/28/2025 09:41:44 INFO]: Training stats: {
    "score": -1.000541436348463,
    "rmse": 1.000541436348463
}
[08/28/2025 09:41:44 INFO]: Val stats: {
    "score": -0.665433769052393,
    "rmse": 0.665433769052393
}
[08/28/2025 09:41:44 INFO]: Test stats: {
    "score": -0.8645162517599261,
    "rmse": 0.8645162517599261
}
[08/28/2025 09:42:00 INFO]: Running Final Evaluation...
[08/28/2025 09:42:00 INFO]: Training loss at epoch 14: 0.9487286806106567
[08/28/2025 09:42:01 INFO]: Training stats: {
    "score": -0.9927196963123742,
    "rmse": 0.9927196963123742
}
[08/28/2025 09:42:01 INFO]: Val stats: {
    "score": -0.7045909231858308,
    "rmse": 0.7045909231858308
}
[08/28/2025 09:42:01 INFO]: Test stats: {
    "score": -0.889916975262776,
    "rmse": 0.889916975262776
}
[08/28/2025 09:42:03 INFO]: Training loss at epoch 30: 1.0561100840568542
[08/28/2025 09:42:07 INFO]: Training loss at epoch 19: 0.989128053188324
[08/28/2025 09:42:14 INFO]: Training loss at epoch 79: 1.0829585492610931
[08/28/2025 09:42:19 INFO]: Training loss at epoch 27: 1.03097403049469
[08/28/2025 09:42:26 INFO]: Training loss at epoch 30: 0.8800307214260101
[08/28/2025 09:42:27 INFO]: Training loss at epoch 30: 1.1328662633895874
[08/28/2025 09:42:34 INFO]: Training stats: {
    "score": -1.0008734158247004,
    "rmse": 1.0008734158247004
}
[08/28/2025 09:42:34 INFO]: Val stats: {
    "score": -0.668864755837904,
    "rmse": 0.668864755837904
}
[08/28/2025 09:42:34 INFO]: Test stats: {
    "score": -0.8687718968473664,
    "rmse": 0.8687718968473664
}
[08/28/2025 09:42:34 INFO]: New best epoch, val score: -0.6563556928214894
[08/28/2025 09:42:34 INFO]: Saving model to: unhanged-Shanesha_trial_132/model_best.pth
[08/28/2025 09:43:02 INFO]: Training loss at epoch 69: 0.9782188534736633
[08/28/2025 09:43:06 INFO]: Training accuracy: {
    "score": -1.008426371959963,
    "rmse": 1.008426371959963
}
[08/28/2025 09:43:06 INFO]: Val accuracy: {
    "score": -0.6585648781303595,
    "rmse": 0.6585648781303595
}
[08/28/2025 09:43:06 INFO]: Test accuracy: {
    "score": -0.8733473604173735,
    "rmse": 0.8733473604173735
}
[08/28/2025 09:43:06 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_127",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8733473604173735,
        "rmse": 0.8733473604173735
    },
    "train_stats": {
        "score": -1.008426371959963,
        "rmse": 1.008426371959963
    },
    "val_stats": {
        "score": -0.6585648781303595,
        "rmse": 0.6585648781303595
    }
}
[08/28/2025 09:43:06 INFO]: Procewss finished for trial unhanged-Shanesha_trial_127
[08/28/2025 09:43:06 INFO]: 
_________________________________________________

[08/28/2025 09:43:06 INFO]: train_net_for_optune.py main() running.
[08/28/2025 09:43:06 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.381396363430321
  attention_dropout: 0.35636083226775955
  ffn_dropout: 0.35636083226775955
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 0.0008577044962233631
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_139

[08/28/2025 09:43:06 INFO]: This ft_transformer has 6.720 million parameters.
[08/28/2025 09:43:06 INFO]: Training will start at epoch 0.
[08/28/2025 09:43:06 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 09:43:11 INFO]: Training loss at epoch 31: 1.055834710597992
[08/28/2025 09:43:11 INFO]: Training loss at epoch 15: 1.0701374411582947
[08/28/2025 09:43:20 INFO]: Training loss at epoch 24: 0.9310414493083954
[08/28/2025 09:43:29 INFO]: Training loss at epoch 31: 1.1909920275211334
[08/28/2025 09:43:43 INFO]: Training loss at epoch 60: 0.9005332291126251
[08/28/2025 09:43:44 INFO]: Training loss at epoch 10: 1.0696330070495605
[08/28/2025 09:43:45 INFO]: Training loss at epoch 20: 0.8968762755393982
[08/28/2025 09:43:53 INFO]: Training loss at epoch 33: 1.2068874835968018
[08/28/2025 09:44:01 INFO]: Running Final Evaluation...
[08/28/2025 09:44:05 INFO]: Training stats: {
    "score": -0.9997783846605638,
    "rmse": 0.9997783846605638
}
[08/28/2025 09:44:05 INFO]: Val stats: {
    "score": -0.6780999006929465,
    "rmse": 0.6780999006929465
}
[08/28/2025 09:44:05 INFO]: Test stats: {
    "score": -0.8729801507287601,
    "rmse": 0.8729801507287601
}
[08/28/2025 09:44:06 INFO]: Training loss at epoch 1: 5.1620535254478455
[08/28/2025 09:44:20 INFO]: Training loss at epoch 28: 0.9503714144229889
[08/28/2025 09:44:21 INFO]: Training loss at epoch 16: 0.8575659394264221
[08/28/2025 09:44:27 INFO]: Training loss at epoch 31: 1.0267178416252136
[08/28/2025 09:44:39 INFO]: Training loss at epoch 50: 0.9754407107830048
[08/28/2025 09:44:45 INFO]: Training loss at epoch 28: 1.0243319272994995
[08/28/2025 09:44:49 INFO]: Training loss at epoch 32: 1.1270614862442017
[08/28/2025 09:44:51 INFO]: Training stats: {
    "score": -0.9948688513455868,
    "rmse": 0.9948688513455868
}
[08/28/2025 09:44:51 INFO]: Val stats: {
    "score": -0.6607935496559773,
    "rmse": 0.6607935496559773
}
[08/28/2025 09:44:51 INFO]: Test stats: {
    "score": -0.8726575623467928,
    "rmse": 0.8726575623467928
}
[08/28/2025 09:44:53 INFO]: Training loss at epoch 21: 1.1194233298301697
[08/28/2025 09:44:56 INFO]: Training loss at epoch 25: 1.0616026520729065
[08/28/2025 09:45:17 INFO]: Training loss at epoch 31: 0.9632483124732971
[08/28/2025 09:45:20 INFO]: Training loss at epoch 31: 0.987886369228363
[08/28/2025 09:45:26 INFO]: Training loss at epoch 17: 1.064841091632843
[08/28/2025 09:45:35 INFO]: Training loss at epoch 61: 0.9983820021152496
[08/28/2025 09:45:35 INFO]: Training loss at epoch 11: 0.9371665418148041
[08/28/2025 09:45:53 INFO]: Training loss at epoch 123: 1.0688807964324951
[08/28/2025 09:45:58 INFO]: Training loss at epoch 22: 1.0951684713363647
[08/28/2025 09:46:06 INFO]: Training loss at epoch 33: 1.1951899528503418
[08/28/2025 09:46:06 INFO]: Training loss at epoch 0: 1.0567409992218018
[08/28/2025 09:46:10 INFO]: Training loss at epoch 29: 1.0567033290863037
[08/28/2025 09:46:19 INFO]: Training loss at epoch 32: 1.0021789073944092
[08/28/2025 09:46:22 INFO]: Training accuracy: {
    "score": -1.0071729985392273,
    "rmse": 1.0071729985392273
}
[08/28/2025 09:46:22 INFO]: Val accuracy: {
    "score": -0.665878605738974,
    "rmse": 0.665878605738974
}
[08/28/2025 09:46:22 INFO]: Test accuracy: {
    "score": -0.8680058305811161,
    "rmse": 0.8680058305811161
}
[08/28/2025 09:46:22 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_110",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8680058305811161,
        "rmse": 0.8680058305811161
    },
    "train_stats": {
        "score": -1.0071729985392273,
        "rmse": 1.0071729985392273
    },
    "val_stats": {
        "score": -0.665878605738974,
        "rmse": 0.665878605738974
    }
}
[08/28/2025 09:46:22 INFO]: Procewss finished for trial unhanged-Shanesha_trial_110
[08/28/2025 09:46:23 INFO]: 
_________________________________________________

[08/28/2025 09:46:23 INFO]: train_net_for_optune.py main() running.
[08/28/2025 09:46:23 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.1225477511635287
  attention_dropout: 0.17333049443841542
  ffn_dropout: 0.17333049443841542
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.2236428290544525e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_140

[08/28/2025 09:46:23 INFO]: This ft_transformer has 6.263 million parameters.
[08/28/2025 09:46:23 INFO]: Training will start at epoch 0.
[08/28/2025 09:46:23 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 09:46:26 INFO]: Training loss at epoch 26: 1.0309007465839386
[08/28/2025 09:46:30 INFO]: New best epoch, val score: -0.8712953630274867
[08/28/2025 09:46:30 INFO]: Saving model to: unhanged-Shanesha_trial_139/model_best.pth
[08/28/2025 09:46:31 INFO]: Training loss at epoch 18: 0.807191014289856
[08/28/2025 09:46:48 INFO]: Training stats: {
    "score": -0.9994455189286384,
    "rmse": 0.9994455189286384
}
[08/28/2025 09:46:48 INFO]: Val stats: {
    "score": -0.6606751236787028,
    "rmse": 0.6606751236787028
}
[08/28/2025 09:46:48 INFO]: Test stats: {
    "score": -0.8740464153529784,
    "rmse": 0.8740464153529784
}
[08/28/2025 09:46:52 INFO]: Training loss at epoch 34: 0.7782182842493057
[08/28/2025 09:47:04 INFO]: Training loss at epoch 51: 0.9161465167999268
[08/28/2025 09:47:04 INFO]: Training loss at epoch 23: 1.0135981142520905
[08/28/2025 09:47:22 INFO]: Training loss at epoch 34: 1.2944298684597015
[08/28/2025 09:47:24 INFO]: Training loss at epoch 12: 1.171654462814331
[08/28/2025 09:47:25 INFO]: Training loss at epoch 62: 1.0787752866744995
[08/28/2025 09:47:29 INFO]: Training loss at epoch 42: 1.1889728307724
[08/28/2025 09:47:31 INFO]: Running Final Evaluation...
[08/28/2025 09:47:31 INFO]: Training loss at epoch 2: 1.9563305377960205
[08/28/2025 09:47:37 INFO]: Training loss at epoch 19: 0.9123431146144867
[08/28/2025 09:47:40 INFO]: Training loss at epoch 29: 1.0544585585594177
[08/28/2025 09:47:56 INFO]: Training loss at epoch 32: 0.9646769165992737
[08/28/2025 09:47:57 INFO]: Training accuracy: {
    "score": -1.0002919068960325,
    "rmse": 1.0002919068960325
}
[08/28/2025 09:47:57 INFO]: Val accuracy: {
    "score": -0.6749620451654486,
    "rmse": 0.6749620451654486
}
[08/28/2025 09:47:57 INFO]: Test accuracy: {
    "score": -0.8779758694205202,
    "rmse": 0.8779758694205202
}
[08/28/2025 09:47:57 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_133",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8779758694205202,
        "rmse": 0.8779758694205202
    },
    "train_stats": {
        "score": -1.0002919068960325,
        "rmse": 1.0002919068960325
    },
    "val_stats": {
        "score": -0.6749620451654486,
        "rmse": 0.6749620451654486
    }
}
[08/28/2025 09:47:57 INFO]: Procewss finished for trial unhanged-Shanesha_trial_133
[08/28/2025 09:47:57 INFO]: 
_________________________________________________

[08/28/2025 09:47:57 INFO]: train_net_for_optune.py main() running.
[08/28/2025 09:47:57 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.355586734722807
  attention_dropout: 0.3199537136689259
  ffn_dropout: 0.3199537136689259
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.2067878551521517e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_141

[08/28/2025 09:47:57 INFO]: Training loss at epoch 27: 0.9535373151302338
[08/28/2025 09:47:57 INFO]: This ft_transformer has 5.281 million parameters.
[08/28/2025 09:47:57 INFO]: Training will start at epoch 0.
[08/28/2025 09:47:57 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 09:48:02 INFO]: Training stats: {
    "score": -1.0049382754655336,
    "rmse": 1.0049382754655336
}
[08/28/2025 09:48:02 INFO]: Val stats: {
    "score": -0.7120246090024759,
    "rmse": 0.7120246090024759
}
[08/28/2025 09:48:02 INFO]: Test stats: {
    "score": -0.8903142464901399,
    "rmse": 0.8903142464901399
}
[08/28/2025 09:48:11 INFO]: Training loss at epoch 33: 0.9860438704490662
[08/28/2025 09:48:12 INFO]: Training loss at epoch 24: 1.076917290687561
[08/28/2025 09:48:43 INFO]: Training loss at epoch 30: 1.0460374355316162
[08/28/2025 09:48:46 INFO]: Training stats: {
    "score": -0.9963696297411044,
    "rmse": 0.9963696297411044
}
[08/28/2025 09:48:46 INFO]: Val stats: {
    "score": -0.6938588734552789,
    "rmse": 0.6938588734552789
}
[08/28/2025 09:48:46 INFO]: Test stats: {
    "score": -0.8745112143299831,
    "rmse": 0.8745112143299831
}
[08/28/2025 09:49:12 INFO]: Training loss at epoch 80: 0.748043030500412
[08/28/2025 09:49:12 INFO]: Training loss at epoch 20: 1.3104576766490936
[08/28/2025 09:49:17 INFO]: Training loss at epoch 0: 1.463694155216217
[08/28/2025 09:49:19 INFO]: Training loss at epoch 13: 1.0950623750686646
[08/28/2025 09:49:21 INFO]: Training loss at epoch 25: 1.0428910851478577
[08/28/2025 09:49:22 INFO]: Training loss at epoch 63: 0.941683441400528
[08/28/2025 09:49:35 INFO]: Training loss at epoch 1: 4.680430173873901
[08/28/2025 09:49:37 INFO]: Training loss at epoch 28: 1.0716814398765564
[08/28/2025 09:49:39 INFO]: Training loss at epoch 52: 0.9923738837242126
[08/28/2025 09:49:42 INFO]: New best epoch, val score: -0.8218288130059451
[08/28/2025 09:49:42 INFO]: Saving model to: unhanged-Shanesha_trial_140/model_best.pth
[08/28/2025 09:49:59 INFO]: Training loss at epoch 35: 1.134778082370758
[08/28/2025 09:50:01 INFO]: New best epoch, val score: -0.6707612726764739
[08/28/2025 09:50:01 INFO]: Saving model to: unhanged-Shanesha_trial_139/model_best.pth
[08/28/2025 09:50:02 INFO]: Training loss at epoch 70: 1.0191322565078735
[08/28/2025 09:50:12 INFO]: Training loss at epoch 34: 0.9762887358665466
[08/28/2025 09:50:22 INFO]: Training loss at epoch 21: 0.9574961364269257
[08/28/2025 09:50:34 INFO]: Training loss at epoch 26: 0.9205323159694672
[08/28/2025 09:50:41 INFO]: Training loss at epoch 0: 0.8442650139331818
[08/28/2025 09:50:43 INFO]: Training loss at epoch 31: 1.1924360394477844
[08/28/2025 09:50:53 INFO]: Training loss at epoch 33: 0.7903235256671906
[08/28/2025 09:51:04 INFO]: New best epoch, val score: -0.7285764712056239
[08/28/2025 09:51:04 INFO]: Saving model to: unhanged-Shanesha_trial_141/model_best.pth
[08/28/2025 09:51:13 INFO]: Training loss at epoch 3: 1.1809197962284088
[08/28/2025 09:51:17 INFO]: Training loss at epoch 29: 1.0125744342803955
[08/28/2025 09:51:19 INFO]: Training loss at epoch 14: 1.0295763611793518
[08/28/2025 09:51:23 INFO]: Training loss at epoch 64: 0.7119802236557007
[08/28/2025 09:51:33 INFO]: Training loss at epoch 22: 0.8001115918159485
[08/28/2025 09:51:45 INFO]: Training loss at epoch 27: 0.9282759428024292
[08/28/2025 09:51:50 INFO]: Training stats: {
    "score": -0.9910525348051332,
    "rmse": 0.9910525348051332
}
[08/28/2025 09:51:50 INFO]: Val stats: {
    "score": -0.6760552542446864,
    "rmse": 0.6760552542446864
}
[08/28/2025 09:51:50 INFO]: Test stats: {
    "score": -0.8659264808592626,
    "rmse": 0.8659264808592626
}
[08/28/2025 09:51:59 INFO]: Training loss at epoch 30: 1.0552905797958374
[08/28/2025 09:52:14 INFO]: Training loss at epoch 35: 1.1640406847000122
[08/28/2025 09:52:19 INFO]: Training loss at epoch 53: 0.8836930096149445
[08/28/2025 09:52:45 INFO]: Training loss at epoch 32: 1.1786555647850037
[08/28/2025 09:52:45 INFO]: Training loss at epoch 23: 1.0147663354873657
[08/28/2025 09:52:46 INFO]: Training loss at epoch 1: 1.1356199979782104
[08/28/2025 09:52:56 INFO]: Training loss at epoch 28: 1.223741590976715
[08/28/2025 09:52:57 INFO]: Training loss at epoch 124: 0.9713692367076874
[08/28/2025 09:53:12 INFO]: New best epoch, val score: -0.6553741069068856
[08/28/2025 09:53:12 INFO]: Saving model to: unhanged-Shanesha_trial_140/model_best.pth
[08/28/2025 09:53:14 INFO]: Training loss at epoch 36: 1.0013898313045502
[08/28/2025 09:53:15 INFO]: Training loss at epoch 2: 2.6276362538337708
[08/28/2025 09:53:17 INFO]: Training loss at epoch 15: 1.0115983486175537
[08/28/2025 09:53:19 INFO]: Training loss at epoch 32: 0.9691043496131897
[08/28/2025 09:53:23 INFO]: Training loss at epoch 65: 0.9471010267734528
[08/28/2025 09:53:28 INFO]: Training loss at epoch 30: 1.0089824199676514
[08/28/2025 09:53:35 INFO]: Training loss at epoch 43: 1.0611195266246796
[08/28/2025 09:53:47 INFO]: Training loss at epoch 1: 1.1125234961509705
[08/28/2025 09:53:48 INFO]: Training loss at epoch 34: 0.8211167454719543
[08/28/2025 09:53:54 INFO]: Training loss at epoch 24: 1.110991656780243
[08/28/2025 09:54:05 INFO]: Training loss at epoch 29: 1.0434828996658325
[08/28/2025 09:54:09 INFO]: New best epoch, val score: -0.6925116372853827
[08/28/2025 09:54:09 INFO]: Saving model to: unhanged-Shanesha_trial_141/model_best.pth
[08/28/2025 09:54:12 INFO]: Training loss at epoch 36: 0.9038525819778442
[08/28/2025 09:54:29 INFO]: Training stats: {
    "score": -1.0013088928530687,
    "rmse": 1.0013088928530687
}
[08/28/2025 09:54:29 INFO]: Val stats: {
    "score": -0.665236992556843,
    "rmse": 0.665236992556843
}
[08/28/2025 09:54:29 INFO]: Test stats: {
    "score": -0.8673685611795522,
    "rmse": 0.8673685611795522
}
[08/28/2025 09:54:35 INFO]: Training loss at epoch 81: 0.8054720461368561
[08/28/2025 09:54:40 INFO]: Training loss at epoch 33: 0.9771012365818024
[08/28/2025 09:54:51 INFO]: Training loss at epoch 4: 1.3561245799064636
[08/28/2025 09:54:52 INFO]: Training loss at epoch 54: 0.8403471410274506
[08/28/2025 09:55:00 INFO]: Training loss at epoch 25: 1.0871891975402832
[08/28/2025 09:55:02 INFO]: Training loss at epoch 31: 1.0863015055656433
[08/28/2025 09:55:05 INFO]: Training loss at epoch 31: 0.9180207848548889
[08/28/2025 09:55:08 INFO]: Training loss at epoch 16: 0.9785296022891998
[08/28/2025 09:55:15 INFO]: New best epoch, val score: -0.6720115873951771
[08/28/2025 09:55:15 INFO]: Saving model to: unhanged-Shanesha_trial_138/model_best.pth
[08/28/2025 09:55:16 INFO]: Training loss at epoch 66: 1.2051321864128113
[08/28/2025 09:55:22 INFO]: Training loss at epoch 71: 1.162392497062683
[08/28/2025 09:55:37 INFO]: Training loss at epoch 30: 0.9016804993152618
[08/28/2025 09:56:02 INFO]: Training loss at epoch 2: 0.8649855554103851
[08/28/2025 09:56:03 INFO]: Training loss at epoch 37: 1.1885569095611572
[08/28/2025 09:56:06 INFO]: Training loss at epoch 26: 0.8203952312469482
[08/28/2025 09:56:13 INFO]: Training loss at epoch 37: 0.9451465606689453
[08/28/2025 09:56:17 INFO]: New best epoch, val score: -0.6710692321975408
[08/28/2025 09:56:17 INFO]: Saving model to: unhanged-Shanesha_trial_131/model_best.pth
[08/28/2025 09:56:31 INFO]: Training loss at epoch 35: 1.1365818083286285
[08/28/2025 09:56:31 INFO]: Training loss at epoch 34: 1.0354641675949097
[08/28/2025 09:56:33 INFO]: Training loss at epoch 32: 0.9107304811477661
[08/28/2025 09:56:35 INFO]: Training loss at epoch 3: 1.1953958868980408
[08/28/2025 09:56:38 INFO]: Training loss at epoch 2: 1.2789283990859985
[08/28/2025 09:56:44 INFO]: Training loss at epoch 31: 0.935073971748352
[08/28/2025 09:56:57 INFO]: Training loss at epoch 17: 1.0675448179244995
[08/28/2025 09:57:05 INFO]: Training loss at epoch 67: 1.2048794627189636
[08/28/2025 09:57:11 INFO]: Training loss at epoch 27: 0.8877225518226624
[08/28/2025 09:57:18 INFO]: Training loss at epoch 55: 1.1596572995185852
[08/28/2025 09:57:54 INFO]: Training loss at epoch 32: 0.9772008657455444
[08/28/2025 09:57:59 INFO]: Training loss at epoch 38: 1.154337465763092
[08/28/2025 09:58:03 INFO]: Running Final Evaluation...
[08/28/2025 09:58:06 INFO]: Training loss at epoch 32: 1.1918888688087463
[08/28/2025 09:58:08 INFO]: Training loss at epoch 33: 1.0510239005088806
[08/28/2025 09:58:22 INFO]: Training loss at epoch 28: 1.1089743375778198
[08/28/2025 09:58:23 INFO]: Training loss at epoch 5: 0.9588806629180908
[08/28/2025 09:58:28 INFO]: Training loss at epoch 35: 1.103322982788086
[08/28/2025 09:58:30 INFO]: Training accuracy: {
    "score": -1.0070378884359756,
    "rmse": 1.0070378884359756
}
[08/28/2025 09:58:30 INFO]: Val accuracy: {
    "score": -0.6589192910415337,
    "rmse": 0.6589192910415337
}
[08/28/2025 09:58:30 INFO]: Test accuracy: {
    "score": -0.8677004027011439,
    "rmse": 0.8677004027011439
}
[08/28/2025 09:58:30 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_135",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8677004027011439,
        "rmse": 0.8677004027011439
    },
    "train_stats": {
        "score": -1.0070378884359756,
        "rmse": 1.0070378884359756
    },
    "val_stats": {
        "score": -0.6589192910415337,
        "rmse": 0.6589192910415337
    }
}
[08/28/2025 09:58:30 INFO]: Procewss finished for trial unhanged-Shanesha_trial_135
[08/28/2025 09:58:30 INFO]: 
_________________________________________________

[08/28/2025 09:58:30 INFO]: train_net_for_optune.py main() running.
[08/28/2025 09:58:30 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.5997569750198264
  attention_dropout: 0.35732757064028664
  ffn_dropout: 0.35732757064028664
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.2557297655618252e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_142

[08/28/2025 09:58:30 INFO]: This ft_transformer has 0.822 million parameters.
[08/28/2025 09:58:30 INFO]: Training will start at epoch 0.
[08/28/2025 09:58:30 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 09:58:50 INFO]: New best epoch, val score: -0.6708865244538287
[08/28/2025 09:58:50 INFO]: Saving model to: unhanged-Shanesha_trial_138/model_best.pth
[08/28/2025 09:58:55 INFO]: Training loss at epoch 18: 1.020742952823639
[08/28/2025 09:59:06 INFO]: Training loss at epoch 68: 0.7783187627792358
[08/28/2025 09:59:23 INFO]: Training loss at epoch 38: 1.1003397405147552
[08/28/2025 09:59:23 INFO]: Training loss at epoch 3: 1.1634458303451538
[08/28/2025 09:59:25 INFO]: Training loss at epoch 36: 0.946660965681076
[08/28/2025 09:59:27 INFO]: Training loss at epoch 44: 0.9871113896369934
[08/28/2025 09:59:34 INFO]: Training loss at epoch 29: 1.1470975875854492
[08/28/2025 09:59:38 INFO]: Training loss at epoch 0: 1.3169645071029663
[08/28/2025 09:59:43 INFO]: Training loss at epoch 3: 1.2587116956710815
[08/28/2025 09:59:47 INFO]: New best epoch, val score: -0.9233938781376355
[08/28/2025 09:59:47 INFO]: Saving model to: unhanged-Shanesha_trial_142/model_best.pth
[08/28/2025 09:59:48 INFO]: Training loss at epoch 82: 0.9290214478969574
[08/28/2025 09:59:49 INFO]: Training loss at epoch 34: 1.1517531871795654
[08/28/2025 09:59:50 INFO]: Training loss at epoch 125: 0.9415560960769653
[08/28/2025 09:59:58 INFO]: Training loss at epoch 56: 0.9064614772796631
[08/28/2025 09:59:59 INFO]: Training loss at epoch 39: 1.1140847206115723
[08/28/2025 09:59:59 INFO]: Training stats: {
    "score": -1.0004272474981188,
    "rmse": 1.0004272474981188
}
[08/28/2025 09:59:59 INFO]: Val stats: {
    "score": -0.6727832422856913,
    "rmse": 0.6727832422856913
}
[08/28/2025 09:59:59 INFO]: Test stats: {
    "score": -0.8701996130730971,
    "rmse": 0.8701996130730971
}
[08/28/2025 10:00:01 INFO]: New best epoch, val score: -0.672343134643794
[08/28/2025 10:00:01 INFO]: Saving model to: unhanged-Shanesha_trial_134/model_best.pth
[08/28/2025 10:00:05 INFO]: New best epoch, val score: -0.6803569343177258
[08/28/2025 10:00:05 INFO]: Saving model to: unhanged-Shanesha_trial_141/model_best.pth
[08/28/2025 10:00:10 INFO]: Training loss at epoch 4: 1.279011458158493
[08/28/2025 10:00:11 INFO]: New best epoch, val score: -0.6593203707273079
[08/28/2025 10:00:11 INFO]: Saving model to: unhanged-Shanesha_trial_105/model_best.pth
[08/28/2025 10:00:30 INFO]: Training loss at epoch 36: 0.9266854822635651
[08/28/2025 10:00:42 INFO]: Training stats: {
    "score": -0.9971599994147687,
    "rmse": 0.9971599994147687
}
[08/28/2025 10:00:42 INFO]: Val stats: {
    "score": -0.6763989463947881,
    "rmse": 0.6763989463947881
}
[08/28/2025 10:00:42 INFO]: Test stats: {
    "score": -0.8745102199976611,
    "rmse": 0.8745102199976611
}
[08/28/2025 10:00:43 INFO]: Running Final Evaluation...
[08/28/2025 10:00:43 INFO]: Training loss at epoch 72: 1.0100336074829102
[08/28/2025 10:00:53 INFO]: Training loss at epoch 1: 1.1784490942955017
[08/28/2025 10:00:54 INFO]: Training loss at epoch 19: 0.8909135460853577
[08/28/2025 10:01:02 INFO]: New best epoch, val score: -0.8503912579550894
[08/28/2025 10:01:02 INFO]: Saving model to: unhanged-Shanesha_trial_142/model_best.pth
[08/28/2025 10:01:05 INFO]: Training loss at epoch 69: 1.314658522605896
[08/28/2025 10:01:10 INFO]: Training loss at epoch 30: 1.2249680757522583
[08/28/2025 10:01:13 INFO]: Training loss at epoch 33: 1.1773465871810913
[08/28/2025 10:01:18 INFO]: Training loss at epoch 33: 1.0374416708946228
[08/28/2025 10:01:29 INFO]: Training loss at epoch 35: 0.8691611588001251
[08/28/2025 10:01:36 INFO]: Training stats: {
    "score": -0.99727387252103,
    "rmse": 0.99727387252103
}
[08/28/2025 10:01:36 INFO]: Val stats: {
    "score": -0.6894808183468978,
    "rmse": 0.6894808183468978
}
[08/28/2025 10:01:36 INFO]: Test stats: {
    "score": -0.875630849319077,
    "rmse": 0.875630849319077
}
[08/28/2025 10:01:41 INFO]: New best epoch, val score: -0.6703633398930353
[08/28/2025 10:01:41 INFO]: Saving model to: unhanged-Shanesha_trial_134/model_best.pth
[08/28/2025 10:01:48 INFO]: Training stats: {
    "score": -0.9775522316351697,
    "rmse": 0.9775522316351697
}
[08/28/2025 10:01:48 INFO]: Val stats: {
    "score": -0.6677359384802458,
    "rmse": 0.6677359384802458
}
[08/28/2025 10:01:48 INFO]: Test stats: {
    "score": -0.8805156665897663,
    "rmse": 0.8805156665897663
}
[08/28/2025 10:02:08 INFO]: Training loss at epoch 6: 0.9130845963954926
[08/28/2025 10:02:10 INFO]: Training loss at epoch 2: 1.0723657011985779
[08/28/2025 10:02:19 INFO]: New best epoch, val score: -0.7624966456720977
[08/28/2025 10:02:19 INFO]: Saving model to: unhanged-Shanesha_trial_142/model_best.pth
[08/28/2025 10:02:21 INFO]: Training loss at epoch 31: 1.0735725164413452
[08/28/2025 10:02:21 INFO]: Training loss at epoch 37: 0.8862458765506744
[08/28/2025 10:02:30 INFO]: Training loss at epoch 37: 0.9939680099487305
[08/28/2025 10:02:31 INFO]: Running Final Evaluation...
[08/28/2025 10:02:35 INFO]: New best epoch, val score: -0.670525885009031
[08/28/2025 10:02:35 INFO]: Saving model to: unhanged-Shanesha_trial_138/model_best.pth
[08/28/2025 10:02:37 INFO]: Training loss at epoch 39: 1.0787823796272278
[08/28/2025 10:02:40 INFO]: Training loss at epoch 57: 0.8756848573684692
[08/28/2025 10:02:43 INFO]: Training loss at epoch 40: 1.0498679876327515
[08/28/2025 10:02:48 INFO]: Training loss at epoch 4: 1.1027551293373108
[08/28/2025 10:02:51 INFO]: Training loss at epoch 4: 1.3114537000656128
[08/28/2025 10:02:57 INFO]: Training accuracy: {
    "score": -1.0067178151609488,
    "rmse": 1.0067178151609488
}
[08/28/2025 10:02:57 INFO]: Val accuracy: {
    "score": -0.6605982073797955,
    "rmse": 0.6605982073797955
}
[08/28/2025 10:02:57 INFO]: Test accuracy: {
    "score": -0.8724656424587354,
    "rmse": 0.8724656424587354
}
[08/28/2025 10:02:57 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_137",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8724656424587354,
        "rmse": 0.8724656424587354
    },
    "train_stats": {
        "score": -1.0067178151609488,
        "rmse": 1.0067178151609488
    },
    "val_stats": {
        "score": -0.6605982073797955,
        "rmse": 0.6605982073797955
    }
}
[08/28/2025 10:02:57 INFO]: Procewss finished for trial unhanged-Shanesha_trial_137
[08/28/2025 10:02:57 INFO]: 
_________________________________________________

[08/28/2025 10:02:57 INFO]: train_net_for_optune.py main() running.
[08/28/2025 10:02:58 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.3941677681614144
  attention_dropout: 0.354231304974891
  ffn_dropout: 0.354231304974891
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.2248618001086346e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_143

[08/28/2025 10:02:58 INFO]: This ft_transformer has 6.743 million parameters.
[08/28/2025 10:02:58 INFO]: Training will start at epoch 0.
[08/28/2025 10:02:58 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 10:03:07 INFO]: Training loss at epoch 36: 0.9728562831878662
[08/28/2025 10:03:10 INFO]: New best epoch, val score: -0.6737485638887206
[08/28/2025 10:03:10 INFO]: Saving model to: unhanged-Shanesha_trial_141/model_best.pth
[08/28/2025 10:03:20 INFO]: New best epoch, val score: -0.6695048662383711
[08/28/2025 10:03:20 INFO]: Saving model to: unhanged-Shanesha_trial_134/model_best.pth
[08/28/2025 10:03:24 INFO]: Training loss at epoch 3: 1.0631716847419739
[08/28/2025 10:03:30 INFO]: Training accuracy: {
    "score": -1.0119375517285956,
    "rmse": 1.0119375517285956
}
[08/28/2025 10:03:30 INFO]: Val accuracy: {
    "score": -0.660715414897126,
    "rmse": 0.660715414897126
}
[08/28/2025 10:03:30 INFO]: Test accuracy: {
    "score": -0.871294712794056,
    "rmse": 0.871294712794056
}
[08/28/2025 10:03:30 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_43",
    "best_epoch": 94,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.871294712794056,
        "rmse": 0.871294712794056
    },
    "train_stats": {
        "score": -1.0119375517285956,
        "rmse": 1.0119375517285956
    },
    "val_stats": {
        "score": -0.660715414897126,
        "rmse": 0.660715414897126
    }
}
[08/28/2025 10:03:30 INFO]: Procewss finished for trial unhanged-Shanesha_trial_43
[08/28/2025 10:03:30 INFO]: 
_________________________________________________

[08/28/2025 10:03:30 INFO]: train_net_for_optune.py main() running.
[08/28/2025 10:03:30 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.5966103978879143
  attention_dropout: 0.36146411655130767
  ffn_dropout: 0.36146411655130767
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.166797094555603e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_144

[08/28/2025 10:03:30 INFO]: This ft_transformer has 7.103 million parameters.
[08/28/2025 10:03:30 INFO]: Training will start at epoch 0.
[08/28/2025 10:03:30 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 10:03:33 INFO]: New best epoch, val score: -0.6892092981626554
[08/28/2025 10:03:33 INFO]: Saving model to: unhanged-Shanesha_trial_142/model_best.pth
[08/28/2025 10:03:33 INFO]: Training loss at epoch 20: 1.1549421548843384
[08/28/2025 10:03:42 INFO]: Training stats: {
    "score": -0.9953744114818835,
    "rmse": 0.9953744114818835
}
[08/28/2025 10:03:42 INFO]: Val stats: {
    "score": -0.6831524200166875,
    "rmse": 0.6831524200166875
}
[08/28/2025 10:03:42 INFO]: Test stats: {
    "score": -0.8728496469958997,
    "rmse": 0.8728496469958997
}
[08/28/2025 10:03:46 INFO]: Training loss at epoch 5: 1.2609269618988037
[08/28/2025 10:03:46 INFO]: Training loss at epoch 70: 0.957158625125885
[08/28/2025 10:04:24 INFO]: Training loss at epoch 38: 0.9244561791419983
[08/28/2025 10:04:24 INFO]: Training loss at epoch 34: 0.9936979711055756
[08/28/2025 10:04:34 INFO]: Training loss at epoch 4: 1.4138463139533997
[08/28/2025 10:04:36 INFO]: Training loss at epoch 41: 1.0406504571437836
[08/28/2025 10:04:41 INFO]: Training loss at epoch 37: 1.086506575345993
[08/28/2025 10:04:43 INFO]: New best epoch, val score: -0.6633549646442947
[08/28/2025 10:04:43 INFO]: Saving model to: unhanged-Shanesha_trial_142/model_best.pth
[08/28/2025 10:05:04 INFO]: Training loss at epoch 83: 1.1057790517807007
[08/28/2025 10:05:07 INFO]: Training loss at epoch 38: 0.9828128814697266
[08/28/2025 10:05:09 INFO]: Training loss at epoch 58: 1.4265152215957642
[08/28/2025 10:05:22 INFO]: Training loss at epoch 21: 0.9867675602436066
[08/28/2025 10:05:24 INFO]: Training loss at epoch 45: 0.9292764663696289
[08/28/2025 10:05:36 INFO]: Training loss at epoch 71: 0.978939563035965
[08/28/2025 10:05:38 INFO]: Training loss at epoch 7: 1.0071473121643066
[08/28/2025 10:05:41 INFO]: Training loss at epoch 5: 0.9403549432754517
[08/28/2025 10:05:44 INFO]: Training loss at epoch 5: 1.3152313232421875
[08/28/2025 10:05:55 INFO]: Training loss at epoch 0: 1.2426578998565674
[08/28/2025 10:05:59 INFO]: Training loss at epoch 73: 0.9924222826957703
[08/28/2025 10:06:04 INFO]: Training loss at epoch 5: 1.0467854142189026
[08/28/2025 10:06:11 INFO]: Training loss at epoch 38: 0.8973722457885742
[08/28/2025 10:06:15 INFO]: Training loss at epoch 39: 0.9763600528240204
[08/28/2025 10:06:18 INFO]: New best epoch, val score: -0.9518566364482848
[08/28/2025 10:06:18 INFO]: Saving model to: unhanged-Shanesha_trial_143/model_best.pth
[08/28/2025 10:06:26 INFO]: Training loss at epoch 42: 0.9377779960632324
[08/28/2025 10:06:32 INFO]: Training loss at epoch 0: 1.101939618587494
[08/28/2025 10:06:38 INFO]: Training loss at epoch 40: 0.9445830881595612
[08/28/2025 10:06:55 INFO]: Training loss at epoch 6: 0.8859352171421051
[08/28/2025 10:06:56 INFO]: Training stats: {
    "score": -0.9981738796133818,
    "rmse": 0.9981738796133818
}
[08/28/2025 10:06:56 INFO]: Val stats: {
    "score": -0.6616810597617803,
    "rmse": 0.6616810597617803
}
[08/28/2025 10:06:56 INFO]: Test stats: {
    "score": -0.8737263231813761,
    "rmse": 0.8737263231813761
}
[08/28/2025 10:06:59 INFO]: New best epoch, val score: -0.683310134181261
[08/28/2025 10:06:59 INFO]: Saving model to: unhanged-Shanesha_trial_144/model_best.pth
[08/28/2025 10:07:08 INFO]: Training loss at epoch 6: 1.2247074246406555
[08/28/2025 10:07:13 INFO]: Training loss at epoch 22: 0.9994696974754333
[08/28/2025 10:07:24 INFO]: Training loss at epoch 35: 0.9582571983337402
[08/28/2025 10:07:32 INFO]: Training loss at epoch 72: 0.9325690567493439
[08/28/2025 10:07:40 INFO]: Training loss at epoch 59: 0.8921553790569305
[08/28/2025 10:07:47 INFO]: Running Final Evaluation...
[08/28/2025 10:07:48 INFO]: Training loss at epoch 39: 0.9518452882766724
[08/28/2025 10:07:55 INFO]: Training loss at epoch 39: 0.8353651463985443
[08/28/2025 10:08:11 INFO]: Training loss at epoch 7: 1.0687963366508484
[08/28/2025 10:08:21 INFO]: Training stats: {
    "score": -0.9881299457641154,
    "rmse": 0.9881299457641154
}
[08/28/2025 10:08:21 INFO]: Val stats: {
    "score": -0.67431971840135,
    "rmse": 0.67431971840135
}
[08/28/2025 10:08:21 INFO]: Test stats: {
    "score": -0.8668277940074396,
    "rmse": 0.8668277940074396
}
[08/28/2025 10:08:26 INFO]: Training loss at epoch 43: 0.9054044485092163
[08/28/2025 10:08:36 INFO]: Training stats: {
    "score": -0.9954561354435375,
    "rmse": 0.9954561354435375
}
[08/28/2025 10:08:36 INFO]: Val stats: {
    "score": -0.7194736103758097,
    "rmse": 0.7194736103758097
}
[08/28/2025 10:08:36 INFO]: Test stats: {
    "score": -0.8985284302212383,
    "rmse": 0.8985284302212383
}
[08/28/2025 10:08:43 INFO]: Training loss at epoch 6: 1.1018860340118408
[08/28/2025 10:08:55 INFO]: Training loss at epoch 40: 0.872583270072937
[08/28/2025 10:08:58 INFO]: Training stats: {
    "score": -0.9891851278725416,
    "rmse": 0.9891851278725416
}
[08/28/2025 10:08:58 INFO]: Val stats: {
    "score": -0.6694407164104001,
    "rmse": 0.6694407164104001
}
[08/28/2025 10:08:58 INFO]: Test stats: {
    "score": -0.8719579149088704,
    "rmse": 0.8719579149088704
}
[08/28/2025 10:08:59 INFO]: Training accuracy: {
    "score": -1.0068605591474793,
    "rmse": 1.0068605591474793
}
[08/28/2025 10:08:59 INFO]: Val accuracy: {
    "score": -0.6643507948164566,
    "rmse": 0.6643507948164566
}
[08/28/2025 10:08:59 INFO]: Test accuracy: {
    "score": -0.8605720476211257,
    "rmse": 0.8605720476211257
}
[08/28/2025 10:08:59 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_129",
    "best_epoch": 4,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8605720476211257,
        "rmse": 0.8605720476211257
    },
    "train_stats": {
        "score": -1.0068605591474793,
        "rmse": 1.0068605591474793
    },
    "val_stats": {
        "score": -0.6643507948164566,
        "rmse": 0.6643507948164566
    }
}
[08/28/2025 10:08:59 INFO]: Procewss finished for trial unhanged-Shanesha_trial_129
[08/28/2025 10:08:59 INFO]: 
_________________________________________________

[08/28/2025 10:08:59 INFO]: train_net_for_optune.py main() running.
[08/28/2025 10:08:59 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.385286093089705
  attention_dropout: 0.36163844459781164
  ffn_dropout: 0.36163844459781164
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.2041689194641261e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_145

[08/28/2025 10:08:59 INFO]: This ft_transformer has 6.724 million parameters.
[08/28/2025 10:08:59 INFO]: Training will start at epoch 0.
[08/28/2025 10:08:59 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 10:09:06 INFO]: New best epoch, val score: -0.6731644072390965
[08/28/2025 10:09:06 INFO]: Saving model to: unhanged-Shanesha_trial_141/model_best.pth
[08/28/2025 10:09:07 INFO]: Training loss at epoch 34: 1.0497960448265076
[08/28/2025 10:09:13 INFO]: Training loss at epoch 23: 1.0209409594535828
[08/28/2025 10:09:17 INFO]: Training loss at epoch 8: 0.9377313554286957
[08/28/2025 10:09:26 INFO]: Training loss at epoch 8: 0.9947046637535095
[08/28/2025 10:09:30 INFO]: Training loss at epoch 1: 0.9168793261051178
[08/28/2025 10:09:31 INFO]: Training loss at epoch 6: 1.2983967661857605
[08/28/2025 10:09:36 INFO]: Training loss at epoch 73: 1.0657632946968079
[08/28/2025 10:09:54 INFO]: Training loss at epoch 41: 1.1218701004981995
[08/28/2025 10:09:56 INFO]: New best epoch, val score: -0.740217604454868
[08/28/2025 10:09:56 INFO]: Saving model to: unhanged-Shanesha_trial_143/model_best.pth
[08/28/2025 10:10:02 INFO]: Training loss at epoch 40: 0.9185906052589417
[08/28/2025 10:10:06 INFO]: Running Final Evaluation...
[08/28/2025 10:10:20 INFO]: Training loss at epoch 1: 0.9664868414402008
[08/28/2025 10:10:24 INFO]: Training loss at epoch 84: 0.9214559197425842
[08/28/2025 10:10:29 INFO]: Training loss at epoch 44: 1.0968122482299805
[08/28/2025 10:10:43 INFO]: Training loss at epoch 9: 1.1230691075325012
[08/28/2025 10:10:51 INFO]: Training loss at epoch 7: 1.0866663753986359
[08/28/2025 10:10:57 INFO]: Training loss at epoch 41: 0.9815461039543152
[08/28/2025 10:11:10 INFO]: Training stats: {
    "score": -1.0209265652912736,
    "rmse": 1.0209265652912736
}
[08/28/2025 10:11:10 INFO]: Val stats: {
    "score": -0.6651057010717579,
    "rmse": 0.6651057010717579
}
[08/28/2025 10:11:10 INFO]: Test stats: {
    "score": -0.8772766426866343,
    "rmse": 0.8772766426866343
}
[08/28/2025 10:11:13 INFO]: Training loss at epoch 24: 1.4244768023490906
[08/28/2025 10:11:18 INFO]: Training loss at epoch 60: 1.0168337225914001
[08/28/2025 10:11:26 INFO]: Training loss at epoch 74: 1.0570465922355652
[08/28/2025 10:11:27 INFO]: Training loss at epoch 46: 0.9330930411815643
[08/28/2025 10:11:37 INFO]: Training loss at epoch 74: 0.9004071652889252
[08/28/2025 10:11:42 INFO]: Training loss at epoch 41: 1.155199944972992
[08/28/2025 10:11:52 INFO]: Training loss at epoch 7: 1.2388039827346802
[08/28/2025 10:11:56 INFO]: Training loss at epoch 40: 0.9242124557495117
[08/28/2025 10:12:12 INFO]: Training loss at epoch 0: 1.066019207239151
[08/28/2025 10:12:15 INFO]: New best epoch, val score: -0.6723496421599187
[08/28/2025 10:12:15 INFO]: Saving model to: unhanged-Shanesha_trial_141/model_best.pth
[08/28/2025 10:12:25 INFO]: Training loss at epoch 10: 1.2552729845046997
[08/28/2025 10:12:29 INFO]: Training loss at epoch 45: 1.2087345719337463
[08/28/2025 10:12:37 INFO]: New best epoch, val score: -0.6954700493782453
[08/28/2025 10:12:37 INFO]: Saving model to: unhanged-Shanesha_trial_145/model_best.pth
[08/28/2025 10:12:56 INFO]: Training loss at epoch 7: 1.2813089489936829
[08/28/2025 10:12:56 INFO]: Training loss at epoch 42: 1.0524896383285522
[08/28/2025 10:13:01 INFO]: Training loss at epoch 9: 1.0499539077281952
[08/28/2025 10:13:04 INFO]: Training loss at epoch 42: 1.1616721153259277
[08/28/2025 10:13:04 INFO]: Training loss at epoch 2: 1.0488426983356476
[08/28/2025 10:13:05 INFO]: Training accuracy: {
    "score": -1.014421760193637,
    "rmse": 1.014421760193637
}
[08/28/2025 10:13:05 INFO]: Val accuracy: {
    "score": -0.6480977165632422,
    "rmse": 0.6480977165632422
}
[08/28/2025 10:13:05 INFO]: Test accuracy: {
    "score": -0.8707923477205649,
    "rmse": 0.8707923477205649
}
[08/28/2025 10:13:05 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_106",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8707923477205649,
        "rmse": 0.8707923477205649
    },
    "train_stats": {
        "score": -1.014421760193637,
        "rmse": 1.014421760193637
    },
    "val_stats": {
        "score": -0.6480977165632422,
        "rmse": 0.6480977165632422
    }
}
[08/28/2025 10:13:05 INFO]: Procewss finished for trial unhanged-Shanesha_trial_106
[08/28/2025 10:13:05 INFO]: 
_________________________________________________

[08/28/2025 10:13:05 INFO]: train_net_for_optune.py main() running.
[08/28/2025 10:13:05 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.38144010645249
  attention_dropout: 0.3554203370793795
  ffn_dropout: 0.3554203370793795
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.293287377591973e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_146

[08/28/2025 10:13:05 INFO]: This ft_transformer has 5.312 million parameters.
[08/28/2025 10:13:05 INFO]: Training will start at epoch 0.
[08/28/2025 10:13:05 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 10:13:08 INFO]: Training loss at epoch 25: 1.0586932003498077
[08/28/2025 10:13:18 INFO]: Training loss at epoch 42: 0.9808239340782166
[08/28/2025 10:13:28 INFO]: New best epoch, val score: -0.6677170711828441
[08/28/2025 10:13:28 INFO]: Saving model to: unhanged-Shanesha_trial_143/model_best.pth
[08/28/2025 10:13:31 INFO]: Training loss at epoch 75: 0.9557987451553345
[08/28/2025 10:13:36 INFO]: Training loss at epoch 11: 1.1184529066085815
[08/28/2025 10:13:49 INFO]: Training loss at epoch 61: 1.1008483171463013
[08/28/2025 10:13:59 INFO]: Training loss at epoch 2: 1.2963029146194458
[08/28/2025 10:14:12 INFO]: Training stats: {
    "score": -1.0006505767055294,
    "rmse": 1.0006505767055294
}
[08/28/2025 10:14:12 INFO]: Val stats: {
    "score": -0.6723447437728417,
    "rmse": 0.6723447437728417
}
[08/28/2025 10:14:12 INFO]: Test stats: {
    "score": -0.8708923948233189,
    "rmse": 0.8708923948233189
}
[08/28/2025 10:14:17 INFO]: Training loss at epoch 8: 1.3781997561454773
[08/28/2025 10:14:21 INFO]: Training loss at epoch 46: 0.9140131175518036
[08/28/2025 10:14:39 INFO]: Training loss at epoch 41: 0.8275437355041504
[08/28/2025 10:14:46 INFO]: Training loss at epoch 12: 1.2236824035644531
[08/28/2025 10:14:46 INFO]: Training loss at epoch 43: 0.8505496978759766
[08/28/2025 10:14:47 INFO]: Training loss at epoch 8: 1.0161311626434326
[08/28/2025 10:14:49 INFO]: Training loss at epoch 43: 1.0346176624298096
[08/28/2025 10:14:57 INFO]: Training loss at epoch 26: 1.0110633671283722
[08/28/2025 10:15:08 INFO]: New best epoch, val score: -0.6717863079436707
[08/28/2025 10:15:08 INFO]: Saving model to: unhanged-Shanesha_trial_141/model_best.pth
[08/28/2025 10:15:21 INFO]: Training loss at epoch 76: 1.0375051200389862
[08/28/2025 10:15:35 INFO]: Training loss at epoch 85: 0.9355399906635284
[08/28/2025 10:15:37 INFO]: Training loss at epoch 0: 1.2782694101333618
[08/28/2025 10:15:37 INFO]: Training loss at epoch 1: 1.2339649200439453
[08/28/2025 10:15:57 INFO]: Training loss at epoch 13: 1.0243850648403168
[08/28/2025 10:15:57 INFO]: New best epoch, val score: -0.6619105531217307
[08/28/2025 10:15:57 INFO]: Saving model to: unhanged-Shanesha_trial_146/model_best.pth
[08/28/2025 10:16:02 INFO]: New best epoch, val score: -0.6920354961715
[08/28/2025 10:16:02 INFO]: Saving model to: unhanged-Shanesha_trial_145/model_best.pth
[08/28/2025 10:16:02 INFO]: Training loss at epoch 43: 0.9334604740142822
[08/28/2025 10:16:08 INFO]: Training loss at epoch 8: 1.3811079263687134
[08/28/2025 10:16:17 INFO]: Training loss at epoch 47: 1.1190196871757507
[08/28/2025 10:16:18 INFO]: Training loss at epoch 62: 0.8653664588928223
[08/28/2025 10:16:24 INFO]: Training loss at epoch 44: 1.1337836980819702
[08/28/2025 10:16:29 INFO]: Training loss at epoch 3: 1.0464508533477783
[08/28/2025 10:16:39 INFO]: Training loss at epoch 75: 0.9701708257198334
[08/28/2025 10:16:44 INFO]: Training loss at epoch 44: 0.9204500615596771
[08/28/2025 10:16:53 INFO]: Training loss at epoch 27: 1.065578281879425
[08/28/2025 10:17:14 INFO]: Training loss at epoch 14: 1.1084741950035095
[08/28/2025 10:17:20 INFO]: Training loss at epoch 77: 0.9847689867019653
[08/28/2025 10:17:20 INFO]: Training loss at epoch 47: 1.0985820889472961
[08/28/2025 10:17:24 INFO]: New best epoch, val score: -0.6632563827454558
[08/28/2025 10:17:24 INFO]: Saving model to: unhanged-Shanesha_trial_142/model_best.pth
[08/28/2025 10:17:29 INFO]: Training loss at epoch 42: 1.1928760409355164
[08/28/2025 10:17:37 INFO]: Training loss at epoch 3: 1.0805602669715881
[08/28/2025 10:17:48 INFO]: Training loss at epoch 9: 1.1049878001213074
[08/28/2025 10:17:50 INFO]: Training loss at epoch 10: 0.8700677752494812
[08/28/2025 10:17:50 INFO]: Training loss at epoch 9: 1.2690021693706512
[08/28/2025 10:18:05 INFO]: New best epoch, val score: -0.6708711501766724
[08/28/2025 10:18:05 INFO]: Saving model to: unhanged-Shanesha_trial_144/model_best.pth
[08/28/2025 10:18:05 INFO]: Training loss at epoch 45: 0.9479804635047913
[08/28/2025 10:18:18 INFO]: Training loss at epoch 48: 1.2059693336486816
[08/28/2025 10:18:31 INFO]: Training loss at epoch 15: 1.1763208508491516
[08/28/2025 10:18:40 INFO]: New best epoch, val score: -0.6631185753544586
[08/28/2025 10:18:40 INFO]: Saving model to: unhanged-Shanesha_trial_142/model_best.pth
[08/28/2025 10:18:43 INFO]: Training loss at epoch 1: 1.0934090614318848
[08/28/2025 10:18:45 INFO]: Training loss at epoch 45: 0.8175334632396698
[08/28/2025 10:18:53 INFO]: Training loss at epoch 28: 0.8462509512901306
[08/28/2025 10:18:58 INFO]: Training stats: {
    "score": -1.0113527217049767,
    "rmse": 1.0113527217049767
}
[08/28/2025 10:18:58 INFO]: Val stats: {
    "score": -0.6719663429901095,
    "rmse": 0.6719663429901095
}
[08/28/2025 10:18:58 INFO]: Test stats: {
    "score": -0.8652604897212683,
    "rmse": 0.8652604897212683
}
[08/28/2025 10:18:59 INFO]: Training loss at epoch 63: 1.0135048031806946
[08/28/2025 10:19:04 INFO]: Training stats: {
    "score": -1.0646201744018264,
    "rmse": 1.0646201744018264
}
[08/28/2025 10:19:04 INFO]: Val stats: {
    "score": -0.8470067988085815,
    "rmse": 0.8470067988085815
}
[08/28/2025 10:19:04 INFO]: Test stats: {
    "score": -0.9856386697225944,
    "rmse": 0.9856386697225944
}
[08/28/2025 10:19:16 INFO]: Training loss at epoch 2: 1.2124304175376892
[08/28/2025 10:19:18 INFO]: Training loss at epoch 44: 1.063744306564331
[08/28/2025 10:19:21 INFO]: Training loss at epoch 78: 0.9769939482212067
[08/28/2025 10:19:37 INFO]: Training loss at epoch 9: 1.1927183866500854
[08/28/2025 10:19:46 INFO]: Training loss at epoch 46: 0.9490505158901215
[08/28/2025 10:19:47 INFO]: Training loss at epoch 16: 0.9912836849689484
[08/28/2025 10:20:10 INFO]: Training loss at epoch 4: 1.2466771006584167
[08/28/2025 10:20:21 INFO]: Training loss at epoch 49: 0.9788677990436554
[08/28/2025 10:20:27 INFO]: Training loss at epoch 43: 0.7816423177719116
[08/28/2025 10:20:47 INFO]: Training loss at epoch 46: 0.9094129204750061
[08/28/2025 10:20:50 INFO]: Training stats: {
    "score": -1.037538293678263,
    "rmse": 1.037538293678263
}
[08/28/2025 10:20:50 INFO]: Val stats: {
    "score": -0.780351187704647,
    "rmse": 0.780351187704647
}
[08/28/2025 10:20:50 INFO]: Test stats: {
    "score": -0.9359978154730726,
    "rmse": 0.9359978154730726
}
[08/28/2025 10:20:51 INFO]: Training loss at epoch 29: 1.0749689936637878
[08/28/2025 10:21:01 INFO]: Training loss at epoch 86: 0.9587607979774475
[08/28/2025 10:21:01 INFO]: Training loss at epoch 17: 1.3863841891288757
[08/28/2025 10:21:01 INFO]: Training stats: {
    "score": -0.9963019054742545,
    "rmse": 0.9963019054742545
}
[08/28/2025 10:21:01 INFO]: Val stats: {
    "score": -0.6773442218048956,
    "rmse": 0.6773442218048956
}
[08/28/2025 10:21:01 INFO]: Test stats: {
    "score": -0.8748704325409074,
    "rmse": 0.8748704325409074
}
[08/28/2025 10:21:18 INFO]: Training loss at epoch 79: 1.0094584226608276
[08/28/2025 10:21:22 INFO]: Training loss at epoch 4: 1.0673198699951172
[08/28/2025 10:21:23 INFO]: Training loss at epoch 47: 0.9675743579864502
[08/28/2025 10:21:29 INFO]: Training stats: {
    "score": -0.9958954902702846,
    "rmse": 0.9958954902702846
}
[08/28/2025 10:21:29 INFO]: Val stats: {
    "score": -0.67323681679441,
    "rmse": 0.67323681679441
}
[08/28/2025 10:21:29 INFO]: Test stats: {
    "score": -0.8677463294655225,
    "rmse": 0.8677463294655225
}
[08/28/2025 10:21:32 INFO]: Training loss at epoch 11: 1.067013919353485
[08/28/2025 10:21:34 INFO]: Training loss at epoch 64: 0.9247426092624664
[08/28/2025 10:21:46 INFO]: Training loss at epoch 2: 0.7097532898187637
[08/28/2025 10:21:46 INFO]: New best epoch, val score: -0.6611686167069204
[08/28/2025 10:21:46 INFO]: Saving model to: unhanged-Shanesha_trial_144/model_best.pth
[08/28/2025 10:21:56 INFO]: Training stats: {
    "score": -0.95972264318479,
    "rmse": 0.95972264318479
}
[08/28/2025 10:21:56 INFO]: Val stats: {
    "score": -0.7234852551052562,
    "rmse": 0.7234852551052562
}
[08/28/2025 10:21:56 INFO]: Test stats: {
    "score": -0.9179445242544031,
    "rmse": 0.9179445242544031
}
[08/28/2025 10:21:58 INFO]: Training loss at epoch 10: 0.9847172498703003
[08/28/2025 10:22:04 INFO]: Training loss at epoch 76: 0.9522123634815216
[08/28/2025 10:22:11 INFO]: Training loss at epoch 18: 0.9653145372867584
[08/28/2025 10:22:23 INFO]: Training loss at epoch 45: 0.9230389893054962
[08/28/2025 10:22:31 INFO]: Training loss at epoch 10: 1.04950550198555
[08/28/2025 10:22:38 INFO]: Training loss at epoch 47: 1.0870150327682495
[08/28/2025 10:22:43 INFO]: Training loss at epoch 3: 1.0400815606117249
[08/28/2025 10:22:51 INFO]: Training loss at epoch 50: 1.0647300481796265
[08/28/2025 10:22:53 INFO]: Training loss at epoch 48: 1.1080019772052765
[08/28/2025 10:22:55 INFO]: New best epoch, val score: -0.668877399908855
[08/28/2025 10:22:55 INFO]: Saving model to: unhanged-Shanesha_trial_139/model_best.pth
[08/28/2025 10:23:07 INFO]: New best epoch, val score: -0.6878930262265707
[08/28/2025 10:23:07 INFO]: Saving model to: unhanged-Shanesha_trial_145/model_best.pth
[08/28/2025 10:23:10 INFO]: Training loss at epoch 44: 1.0429873764514923
[08/28/2025 10:23:18 INFO]: Training loss at epoch 30: 1.1123061180114746
[08/28/2025 10:23:19 INFO]: Training loss at epoch 48: 0.9651851058006287
[08/28/2025 10:23:21 INFO]: Training loss at epoch 19: 1.0311669707298279
[08/28/2025 10:23:31 INFO]: Training loss at epoch 5: 1.1372487545013428
[08/28/2025 10:23:45 INFO]: Training stats: {
    "score": -1.0026279768219055,
    "rmse": 1.0026279768219055
}
[08/28/2025 10:23:45 INFO]: Val stats: {
    "score": -0.6659180102046534,
    "rmse": 0.6659180102046534
}
[08/28/2025 10:23:45 INFO]: Test stats: {
    "score": -0.8687049890740528,
    "rmse": 0.8687049890740528
}
[08/28/2025 10:23:46 INFO]: Training loss at epoch 80: 0.8657388687133789
[08/28/2025 10:23:59 INFO]: Training loss at epoch 65: 0.9659836888313293
[08/28/2025 10:23:59 INFO]: Training loss at epoch 10: 1.2623347640037537
[08/28/2025 10:24:25 INFO]: Training loss at epoch 49: 1.20450159907341
[08/28/2025 10:24:29 INFO]: Training loss at epoch 48: 1.150600105524063
[08/28/2025 10:24:36 INFO]: Training loss at epoch 3: 0.9045212864875793
[08/28/2025 10:24:43 INFO]: Training loss at epoch 51: 1.1944930851459503
[08/28/2025 10:24:48 INFO]: Training loss at epoch 5: 0.889397144317627
[08/28/2025 10:24:48 INFO]: Training loss at epoch 11: 0.9956886470317841
[08/28/2025 10:24:55 INFO]: Training loss at epoch 20: 1.049489676952362
[08/28/2025 10:24:55 INFO]: Training stats: {
    "score": -0.9908052168796134,
    "rmse": 0.9908052168796134
}
[08/28/2025 10:24:55 INFO]: Val stats: {
    "score": -0.6681763044970241,
    "rmse": 0.6681763044970241
}
[08/28/2025 10:24:55 INFO]: Test stats: {
    "score": -0.8690630703231211,
    "rmse": 0.8690630703231211
}
[08/28/2025 10:24:57 INFO]: Training loss at epoch 12: 0.9448629021644592
[08/28/2025 10:25:07 INFO]: New best epoch, val score: -0.6681763044970241
[08/28/2025 10:25:07 INFO]: Saving model to: unhanged-Shanesha_trial_134/model_best.pth
[08/28/2025 10:25:10 INFO]: Training loss at epoch 31: 0.8885934948921204
[08/28/2025 10:25:15 INFO]: New best epoch, val score: -0.6610882508383221
[08/28/2025 10:25:15 INFO]: Saving model to: unhanged-Shanesha_trial_144/model_best.pth
[08/28/2025 10:25:21 INFO]: Training loss at epoch 46: 1.043736219406128
[08/28/2025 10:25:41 INFO]: Training loss at epoch 81: 1.194335013628006
[08/28/2025 10:25:56 INFO]: Training loss at epoch 11: 1.1683868169784546
[08/28/2025 10:25:57 INFO]: Training loss at epoch 45: 1.008098065853119
[08/28/2025 10:26:05 INFO]: Training loss at epoch 87: 0.8925867676734924
[08/28/2025 10:26:10 INFO]: Training loss at epoch 4: 1.09886234998703
[08/28/2025 10:26:10 INFO]: Training loss at epoch 21: 0.979743480682373
[08/28/2025 10:26:28 INFO]: Training loss at epoch 49: 1.4100235104560852
[08/28/2025 10:26:33 INFO]: Training loss at epoch 66: 1.0032118558883667
[08/28/2025 10:26:36 INFO]: Training loss at epoch 50: 1.1375520825386047
[08/28/2025 10:26:36 INFO]: New best epoch, val score: -0.6876970710488765
[08/28/2025 10:26:36 INFO]: Saving model to: unhanged-Shanesha_trial_145/model_best.pth
[08/28/2025 10:26:43 INFO]: Training loss at epoch 52: 1.157594621181488
[08/28/2025 10:26:48 INFO]: New best epoch, val score: -0.6673175604909514
[08/28/2025 10:26:48 INFO]: Saving model to: unhanged-Shanesha_trial_134/model_best.pth
[08/28/2025 10:27:02 INFO]: Training loss at epoch 6: 1.099210500717163
[08/28/2025 10:27:09 INFO]: Training loss at epoch 32: 1.2165612578392029
[08/28/2025 10:27:11 INFO]: Training stats: {
    "score": -0.9979853041535863,
    "rmse": 0.9979853041535863
}
[08/28/2025 10:27:11 INFO]: Val stats: {
    "score": -0.6610819100203884,
    "rmse": 0.6610819100203884
}
[08/28/2025 10:27:11 INFO]: Test stats: {
    "score": -0.8735017667510423,
    "rmse": 0.8735017667510423
}
[08/28/2025 10:27:19 INFO]: Training loss at epoch 77: 0.9983146786689758
[08/28/2025 10:27:24 INFO]: Training loss at epoch 11: 0.9432471990585327
[08/28/2025 10:27:27 INFO]: Training loss at epoch 22: 0.8773336410522461
[08/28/2025 10:27:42 INFO]: Training loss at epoch 82: 0.995713859796524
[08/28/2025 10:27:43 INFO]: Training loss at epoch 4: 1.2013671398162842
[08/28/2025 10:27:55 INFO]: Training loss at epoch 12: 1.049282193183899
[08/28/2025 10:28:18 INFO]: Training loss at epoch 51: 1.1712157428264618
[08/28/2025 10:28:36 INFO]: Training loss at epoch 47: 1.020923137664795
[08/28/2025 10:28:37 INFO]: Training loss at epoch 6: 1.1905755400657654
[08/28/2025 10:28:44 INFO]: Training loss at epoch 23: 0.9297432899475098
[08/28/2025 10:28:45 INFO]: Training loss at epoch 13: 0.8355688750743866
[08/28/2025 10:28:46 INFO]: Training loss at epoch 53: 1.0609890222549438
[08/28/2025 10:28:55 INFO]: Training loss at epoch 46: 1.0977212190628052
[08/28/2025 10:29:10 INFO]: Training loss at epoch 33: 0.9415344893932343
[08/28/2025 10:29:12 INFO]: Training loss at epoch 50: 1.0485974550247192
[08/28/2025 10:29:14 INFO]: Training loss at epoch 67: 0.9174900949001312
[08/28/2025 10:29:23 INFO]: Training loss at epoch 49: 1.4189527034759521
[08/28/2025 10:29:35 INFO]: Training loss at epoch 12: 1.3193215727806091
[08/28/2025 10:29:42 INFO]: Training loss at epoch 83: 0.8395392000675201
[08/28/2025 10:29:51 INFO]: Training loss at epoch 5: 1.2304576635360718
[08/28/2025 10:29:56 INFO]: Running Final Evaluation...
[08/28/2025 10:29:56 INFO]: Training loss at epoch 52: 0.9822080135345459
[08/28/2025 10:29:59 INFO]: Training loss at epoch 24: 0.8010151833295822
[08/28/2025 10:30:35 INFO]: Training loss at epoch 7: 0.9294858872890472
[08/28/2025 10:30:36 INFO]: Training accuracy: {
    "score": -0.997393873353305,
    "rmse": 0.997393873353305
}
[08/28/2025 10:30:36 INFO]: Val accuracy: {
    "score": -0.6616603309428467,
    "rmse": 0.6616603309428467
}
[08/28/2025 10:30:36 INFO]: Test accuracy: {
    "score": -0.8758358582554023,
    "rmse": 0.8758358582554023
}
[08/28/2025 10:30:36 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_124",
    "best_epoch": 52,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8758358582554023,
        "rmse": 0.8758358582554023
    },
    "train_stats": {
        "score": -0.997393873353305,
        "rmse": 0.997393873353305
    },
    "val_stats": {
        "score": -0.6616603309428467,
        "rmse": 0.6616603309428467
    }
}
[08/28/2025 10:30:36 INFO]: Procewss finished for trial unhanged-Shanesha_trial_124
[08/28/2025 10:30:36 INFO]: 
_________________________________________________

[08/28/2025 10:30:36 INFO]: train_net_for_optune.py main() running.
[08/28/2025 10:30:36 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 2.475212677168775
  attention_dropout: 0.18687274691502342
  ffn_dropout: 0.18687274691502342
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.1957411413573388e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_147

[08/28/2025 10:30:36 INFO]: This ft_transformer has 5.423 million parameters.
[08/28/2025 10:30:36 INFO]: Training will start at epoch 0.
[08/28/2025 10:30:36 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 10:30:41 INFO]: Training loss at epoch 54: 0.8754947483539581
[08/28/2025 10:30:44 INFO]: Training loss at epoch 5: 1.1339992880821228
[08/28/2025 10:30:47 INFO]: Training loss at epoch 12: 1.1799735128879547
[08/28/2025 10:30:55 INFO]: New best epoch, val score: -0.6709749980157091
[08/28/2025 10:30:55 INFO]: Saving model to: unhanged-Shanesha_trial_131/model_best.pth
[08/28/2025 10:30:55 INFO]: Training loss at epoch 13: 1.1887688636779785
[08/28/2025 10:31:01 INFO]: Training loss at epoch 34: 1.1218605637550354
[08/28/2025 10:31:07 INFO]: Training loss at epoch 51: 0.8214219510555267
[08/28/2025 10:31:09 INFO]: Training loss at epoch 25: 0.8786687552928925
[08/28/2025 10:31:10 INFO]: New best epoch, val score: -0.6540747038834837
[08/28/2025 10:31:10 INFO]: Saving model to: unhanged-Shanesha_trial_140/model_best.pth
[08/28/2025 10:31:21 INFO]: Training stats: {
    "score": -0.9957740472629487,
    "rmse": 0.9957740472629487
}
[08/28/2025 10:31:21 INFO]: Val stats: {
    "score": -0.686972616910475,
    "rmse": 0.686972616910475
}
[08/28/2025 10:31:21 INFO]: Test stats: {
    "score": -0.8778786325367509,
    "rmse": 0.8778786325367509
}
[08/28/2025 10:31:26 INFO]: Training loss at epoch 88: 0.9825639724731445
[08/28/2025 10:31:29 INFO]: Training loss at epoch 53: 1.1927315592765808
[08/28/2025 10:31:38 INFO]: Training loss at epoch 48: 0.983302891254425
[08/28/2025 10:31:39 INFO]: Training loss at epoch 47: 1.1214013695716858
[08/28/2025 10:31:43 INFO]: Training loss at epoch 68: 0.9682542085647583
[08/28/2025 10:32:08 INFO]: Training loss at epoch 7: 1.12871515750885
[08/28/2025 10:32:16 INFO]: Training loss at epoch 14: 1.2417651414871216
[08/28/2025 10:32:22 INFO]: Training loss at epoch 26: 1.0159535109996796
[08/28/2025 10:32:35 INFO]: Training loss at epoch 55: 1.0614395141601562
[08/28/2025 10:32:35 INFO]: Training loss at epoch 78: 0.9004027843475342
[08/28/2025 10:32:51 INFO]: Training loss at epoch 0: 1.4234192371368408
[08/28/2025 10:32:53 INFO]: Training loss at epoch 35: 0.9432092010974884
[08/28/2025 10:32:54 INFO]: Training loss at epoch 13: 1.5618885159492493
[08/28/2025 10:33:00 INFO]: Training loss at epoch 52: 0.9031943678855896
[08/28/2025 10:33:00 INFO]: Training loss at epoch 54: 0.8609492480754852
[08/28/2025 10:33:09 INFO]: Training loss at epoch 6: 1.0823488235473633
[08/28/2025 10:33:09 INFO]: New best epoch, val score: -0.8234772604716658
[08/28/2025 10:33:09 INFO]: Saving model to: unhanged-Shanesha_trial_147/model_best.pth
[08/28/2025 10:33:32 INFO]: Training loss at epoch 27: 1.1903912425041199
[08/28/2025 10:33:38 INFO]: Training loss at epoch 6: 0.8843868970870972
[08/28/2025 10:33:50 INFO]: Training loss at epoch 14: 1.2892611026763916
[08/28/2025 10:33:57 INFO]: Training loss at epoch 8: 1.3034076392650604
[08/28/2025 10:34:02 INFO]: Training loss at epoch 13: 1.1353816390037537
[08/28/2025 10:34:11 INFO]: Training loss at epoch 69: 0.8389270901679993
[08/28/2025 10:34:24 INFO]: Training loss at epoch 48: 1.0821329653263092
[08/28/2025 10:34:31 INFO]: Training loss at epoch 56: 1.2767775058746338
[08/28/2025 10:34:38 INFO]: Training loss at epoch 55: 1.0767485201358795
[08/28/2025 10:34:41 INFO]: Training loss at epoch 49: 0.9095226228237152
[08/28/2025 10:34:48 INFO]: Training loss at epoch 28: 1.0838467478752136
[08/28/2025 10:34:49 INFO]: Training loss at epoch 36: 1.0821226239204407
[08/28/2025 10:34:58 INFO]: Training loss at epoch 53: 0.9795792400836945
[08/28/2025 10:35:04 INFO]: Running Final Evaluation...
[08/28/2025 10:35:07 INFO]: Training stats: {
    "score": -0.9885154626086334,
    "rmse": 0.9885154626086334
}
[08/28/2025 10:35:07 INFO]: Val stats: {
    "score": -0.697109396948706,
    "rmse": 0.697109396948706
}
[08/28/2025 10:35:07 INFO]: Test stats: {
    "score": -0.8852890288695169,
    "rmse": 0.8852890288695169
}
[08/28/2025 10:35:35 INFO]: Training loss at epoch 1: 1.012815535068512
[08/28/2025 10:35:45 INFO]: Training loss at epoch 8: 1.1561678647994995
[08/28/2025 10:35:47 INFO]: Training accuracy: {
    "score": -1.00356015463272,
    "rmse": 1.00356015463272
}
[08/28/2025 10:35:47 INFO]: Val accuracy: {
    "score": -0.6618092876939484,
    "rmse": 0.6618092876939484
}
[08/28/2025 10:35:47 INFO]: Test accuracy: {
    "score": -0.864269997300286,
    "rmse": 0.864269997300286
}
[08/28/2025 10:35:48 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_136",
    "best_epoch": 5,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.864269997300286,
        "rmse": 0.864269997300286
    },
    "train_stats": {
        "score": -1.00356015463272,
        "rmse": 1.00356015463272
    },
    "val_stats": {
        "score": -0.6618092876939484,
        "rmse": 0.6618092876939484
    }
}
[08/28/2025 10:35:48 INFO]: Procewss finished for trial unhanged-Shanesha_trial_136
[08/28/2025 10:35:48 INFO]: 
_________________________________________________

[08/28/2025 10:35:48 INFO]: train_net_for_optune.py main() running.
[08/28/2025 10:35:48 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 2.4977198408151504
  attention_dropout: 0.183772891209792
  ffn_dropout: 0.183772891209792
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.9729167087041604e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_148

[08/28/2025 10:35:48 INFO]: Training stats: {
    "score": -0.996723483835691,
    "rmse": 0.996723483835691
}
[08/28/2025 10:35:48 INFO]: Val stats: {
    "score": -0.6995628217412493,
    "rmse": 0.6995628217412493
}
[08/28/2025 10:35:48 INFO]: Test stats: {
    "score": -0.8817892322389458,
    "rmse": 0.8817892322389458
}
[08/28/2025 10:35:48 INFO]: This ft_transformer has 6.927 million parameters.
[08/28/2025 10:35:48 INFO]: Training will start at epoch 0.
[08/28/2025 10:35:48 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 10:35:52 INFO]: Training loss at epoch 15: 0.9428724348545074
[08/28/2025 10:35:54 INFO]: New best epoch, val score: -0.6558165567284278
[08/28/2025 10:35:54 INFO]: Saving model to: unhanged-Shanesha_trial_147/model_best.pth
[08/28/2025 10:36:03 INFO]: Training loss at epoch 29: 1.1435205936431885
[08/28/2025 10:36:17 INFO]: Training loss at epoch 56: 1.063121259212494
[08/28/2025 10:36:29 INFO]: Training loss at epoch 14: 1.1771600842475891
[08/28/2025 10:36:30 INFO]: Training stats: {
    "score": -0.9969228043636102,
    "rmse": 0.9969228043636102
}
[08/28/2025 10:36:30 INFO]: Val stats: {
    "score": -0.6800166410716209,
    "rmse": 0.6800166410716209
}
[08/28/2025 10:36:30 INFO]: Test stats: {
    "score": -0.8725385849279242,
    "rmse": 0.8725385849279242
}
[08/28/2025 10:36:32 INFO]: Training loss at epoch 57: 1.248533844947815
[08/28/2025 10:36:40 INFO]: Training loss at epoch 89: 0.9957696795463562
[08/28/2025 10:36:44 INFO]: Training loss at epoch 7: 1.3873003125190735
[08/28/2025 10:36:45 INFO]: Training loss at epoch 7: 1.0271118879318237
[08/28/2025 10:36:57 INFO]: Training loss at epoch 15: 1.056226909160614
[08/28/2025 10:36:59 INFO]: Training loss at epoch 54: 1.0098033249378204
[08/28/2025 10:37:19 INFO]: Training loss at epoch 50: 0.9349812567234039
[08/28/2025 10:37:21 INFO]: Training loss at epoch 49: 0.7931236922740936
[08/28/2025 10:37:32 INFO]: Training loss at epoch 14: 1.1894795298576355
[08/28/2025 10:37:35 INFO]: Training loss at epoch 9: 1.1038048565387726
[08/28/2025 10:37:46 INFO]: Training loss at epoch 70: 1.0142147541046143
[08/28/2025 10:37:47 INFO]: Training loss at epoch 30: 1.0592268109321594
[08/28/2025 10:37:58 INFO]: Training loss at epoch 57: 0.8196128606796265
[08/28/2025 10:38:00 INFO]: Training loss at epoch 79: 1.3878431916236877
[08/28/2025 10:38:05 INFO]: Running Final Evaluation...
[08/28/2025 10:38:22 INFO]: Training loss at epoch 2: 0.9303295314311981
[08/28/2025 10:38:23 INFO]: Training stats: {
    "score": -0.979406134789533,
    "rmse": 0.979406134789533
}
[08/28/2025 10:38:23 INFO]: Val stats: {
    "score": -0.6885966344131967,
    "rmse": 0.6885966344131967
}
[08/28/2025 10:38:23 INFO]: Test stats: {
    "score": -0.8796154907921391,
    "rmse": 0.8796154907921391
}
[08/28/2025 10:38:33 INFO]: Training loss at epoch 58: 0.9142605364322662
[08/28/2025 10:38:33 INFO]: Training stats: {
    "score": -1.0010978816023801,
    "rmse": 1.0010978816023801
}
[08/28/2025 10:38:33 INFO]: Val stats: {
    "score": -0.6955092327318115,
    "rmse": 0.6955092327318115
}
[08/28/2025 10:38:33 INFO]: Test stats: {
    "score": -0.8817903453298697,
    "rmse": 0.8817903453298697
}
[08/28/2025 10:38:44 INFO]: Training loss at epoch 0: 1.3003209233283997
[08/28/2025 10:38:48 INFO]: New best epoch, val score: -0.6690969882131726
[08/28/2025 10:38:48 INFO]: Saving model to: unhanged-Shanesha_trial_131/model_best.pth
[08/28/2025 10:38:51 INFO]: Training stats: {
    "score": -1.0148071828878484,
    "rmse": 1.0148071828878484
}
[08/28/2025 10:38:51 INFO]: Val stats: {
    "score": -0.7504823227489372,
    "rmse": 0.7504823227489372
}
[08/28/2025 10:38:51 INFO]: Test stats: {
    "score": -0.9035618879925468,
    "rmse": 0.9035618879925468
}
[08/28/2025 10:39:00 INFO]: Training loss at epoch 55: 1.1271438896656036
[08/28/2025 10:39:01 INFO]: Training accuracy: {
    "score": -1.0000945247599204,
    "rmse": 1.0000945247599204
}
[08/28/2025 10:39:01 INFO]: Val accuracy: {
    "score": -0.6614672195294606,
    "rmse": 0.6614672195294606
}
[08/28/2025 10:39:01 INFO]: Test accuracy: {
    "score": -0.8705523892900346,
    "rmse": 0.8705523892900346
}
[08/28/2025 10:39:01 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_120",
    "best_epoch": 39,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8705523892900346,
        "rmse": 0.8705523892900346
    },
    "train_stats": {
        "score": -1.0000945247599204,
        "rmse": 1.0000945247599204
    },
    "val_stats": {
        "score": -0.6614672195294606,
        "rmse": 0.6614672195294606
    }
}
[08/28/2025 10:39:01 INFO]: Procewss finished for trial unhanged-Shanesha_trial_120
[08/28/2025 10:39:01 INFO]: 
_________________________________________________

[08/28/2025 10:39:01 INFO]: train_net_for_optune.py main() running.
[08/28/2025 10:39:01 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 2.48887287862227
  attention_dropout: 0.32667283610210684
  ffn_dropout: 0.32667283610210684
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.963439198877779e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_149

[08/28/2025 10:39:01 INFO]: Training loss at epoch 31: 1.206953227519989
[08/28/2025 10:39:02 INFO]: This ft_transformer has 17.081 million parameters.
[08/28/2025 10:39:02 INFO]: Training will start at epoch 0.
[08/28/2025 10:39:02 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 10:39:03 INFO]: Training loss at epoch 50: 0.9884681701660156
[08/28/2025 10:39:07 INFO]: New best epoch, val score: -0.7011384962339574
[08/28/2025 10:39:07 INFO]: Saving model to: unhanged-Shanesha_trial_148/model_best.pth
[08/28/2025 10:39:26 INFO]: Running Final Evaluation...
[08/28/2025 10:39:32 INFO]: Training loss at epoch 9: 1.1215452551841736
[08/28/2025 10:39:36 INFO]: Training loss at epoch 58: 0.9104961156845093
[08/28/2025 10:39:37 INFO]: Training loss at epoch 16: 1.1633948981761932
[08/28/2025 10:39:50 INFO]: Training stats: {
    "score": -1.0080861350445312,
    "rmse": 1.0080861350445312
}
[08/28/2025 10:39:50 INFO]: Val stats: {
    "score": -0.7818625916767662,
    "rmse": 0.7818625916767662
}
[08/28/2025 10:39:50 INFO]: Test stats: {
    "score": -0.9406742521243275,
    "rmse": 0.9406742521243275
}
[08/28/2025 10:39:51 INFO]: Training loss at epoch 8: 1.0422797203063965
[08/28/2025 10:40:00 INFO]: Training loss at epoch 16: 1.0998515486717224
[08/28/2025 10:40:03 INFO]: Training loss at epoch 15: 1.4571565985679626
[08/28/2025 10:40:13 INFO]: Training loss at epoch 32: 1.5168099999427795
[08/28/2025 10:40:18 INFO]: Training loss at epoch 8: 1.1124184131622314
[08/28/2025 10:40:27 INFO]: New best epoch, val score: -0.6600302774883644
[08/28/2025 10:40:27 INFO]: Saving model to: unhanged-Shanesha_trial_139/model_best.pth
[08/28/2025 10:40:30 INFO]: Training loss at epoch 59: 0.9142938554286957
[08/28/2025 10:40:39 INFO]: Training accuracy: {
    "score": -1.0087794445396614,
    "rmse": 1.0087794445396614
}
[08/28/2025 10:40:39 INFO]: Val accuracy: {
    "score": -0.6642119284756925,
    "rmse": 0.6642119284756925
}
[08/28/2025 10:40:39 INFO]: Test accuracy: {
    "score": -0.8705748926163488,
    "rmse": 0.8705748926163488
}
[08/28/2025 10:40:39 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_126",
    "best_epoch": 19,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8705748926163488,
        "rmse": 0.8705748926163488
    },
    "train_stats": {
        "score": -1.0087794445396614,
        "rmse": 1.0087794445396614
    },
    "val_stats": {
        "score": -0.6642119284756925,
        "rmse": 0.6642119284756925
    }
}
[08/28/2025 10:40:39 INFO]: Procewss finished for trial unhanged-Shanesha_trial_126
[08/28/2025 10:40:39 INFO]: 
_________________________________________________

[08/28/2025 10:40:39 INFO]: train_net_for_optune.py main() running.
[08/28/2025 10:40:39 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 2.3652535760158964
  attention_dropout: 0.17797273753329754
  ffn_dropout: 0.17797273753329754
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.0437052704627186e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_150

[08/28/2025 10:40:39 INFO]: This ft_transformer has 16.539 million parameters.
[08/28/2025 10:40:39 INFO]: Training will start at epoch 0.
[08/28/2025 10:40:39 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 10:40:42 INFO]: Training stats: {
    "score": -1.0174166595870755,
    "rmse": 1.0174166595870755
}
[08/28/2025 10:40:42 INFO]: Val stats: {
    "score": -0.6616799702150796,
    "rmse": 0.6616799702150796
}
[08/28/2025 10:40:42 INFO]: Test stats: {
    "score": -0.8652031575243806,
    "rmse": 0.8652031575243806
}
[08/28/2025 10:40:51 INFO]: Training loss at epoch 15: 1.1595290303230286
[08/28/2025 10:40:53 INFO]: Training loss at epoch 56: 1.275456428527832
[08/28/2025 10:40:58 INFO]: Training loss at epoch 3: 1.1798400282859802
[08/28/2025 10:41:07 INFO]: Training loss at epoch 59: 0.8565419614315033
[08/28/2025 10:41:08 INFO]: Training loss at epoch 50: 0.9208227396011353
[08/28/2025 10:41:08 INFO]: Training stats: {
    "score": -1.0015475705627186,
    "rmse": 1.0015475705627186
}
[08/28/2025 10:41:08 INFO]: Val stats: {
    "score": -0.66745425132007,
    "rmse": 0.66745425132007
}
[08/28/2025 10:41:08 INFO]: Test stats: {
    "score": -0.871915984674606,
    "rmse": 0.871915984674606
}
[08/28/2025 10:41:22 INFO]: New best epoch, val score: -0.66745425132007
[08/28/2025 10:41:22 INFO]: Saving model to: unhanged-Shanesha_trial_131/model_best.pth
[08/28/2025 10:41:23 INFO]: Training loss at epoch 33: 1.0746967196464539
[08/28/2025 10:41:37 INFO]: Training stats: {
    "score": -0.979230192792252,
    "rmse": 0.979230192792252
}
[08/28/2025 10:41:37 INFO]: Val stats: {
    "score": -0.6833120531617177,
    "rmse": 0.6833120531617177
}
[08/28/2025 10:41:37 INFO]: Test stats: {
    "score": -0.871529952190267,
    "rmse": 0.871529952190267
}
[08/28/2025 10:41:47 INFO]: Training loss at epoch 1: 0.8989288508892059
[08/28/2025 10:42:12 INFO]: Training loss at epoch 10: 0.9829337894916534
[08/28/2025 10:42:32 INFO]: Training loss at epoch 34: 1.066215455532074
[08/28/2025 10:42:41 INFO]: Training loss at epoch 9: 1.2367863655090332
[08/28/2025 10:42:43 INFO]: Training loss at epoch 57: 0.9904065430164337
[08/28/2025 10:42:49 INFO]: Training loss at epoch 17: 1.0346474647521973
[08/28/2025 10:42:58 INFO]: Training loss at epoch 60: 1.1092694401741028
[08/28/2025 10:43:01 INFO]: Training loss at epoch 17: 1.0382511913776398
[08/28/2025 10:43:05 INFO]: Training loss at epoch 51: 1.3366751670837402
[08/28/2025 10:43:07 INFO]: Training loss at epoch 60: 1.0635353922843933
[08/28/2025 10:43:12 INFO]: New best epoch, val score: -0.6666979305415738
[08/28/2025 10:43:12 INFO]: Saving model to: unhanged-Shanesha_trial_131/model_best.pth
[08/28/2025 10:43:21 INFO]: Training loss at epoch 16: 0.8817313313484192
[08/28/2025 10:43:30 INFO]: Training loss at epoch 4: 1.0605042576789856
[08/28/2025 10:43:36 INFO]: Training loss at epoch 90: 1.1310054957866669
[08/28/2025 10:43:37 INFO]: Training loss at epoch 9: 1.1165648102760315
[08/28/2025 10:43:42 INFO]: Training stats: {
    "score": -1.0013381297689674,
    "rmse": 1.0013381297689674
}
[08/28/2025 10:43:42 INFO]: Val stats: {
    "score": -0.7111466211844704,
    "rmse": 0.7111466211844704
}
[08/28/2025 10:43:42 INFO]: Test stats: {
    "score": -0.8936895413195309,
    "rmse": 0.8936895413195309
}
[08/28/2025 10:43:43 INFO]: Training loss at epoch 35: 1.1117104291915894
[08/28/2025 10:43:50 INFO]: Training loss at epoch 51: 0.8092541396617889
[08/28/2025 10:44:06 INFO]: Training loss at epoch 16: 1.226900339126587
[08/28/2025 10:44:11 INFO]: Training loss at epoch 10: 1.3741071820259094
[08/28/2025 10:44:40 INFO]: Training loss at epoch 58: 1.0592874586582184
[08/28/2025 10:44:47 INFO]: Training loss at epoch 61: 0.829107403755188
[08/28/2025 10:44:52 INFO]: Training stats: {
    "score": -1.0333376760677366,
    "rmse": 1.0333376760677366
}
[08/28/2025 10:44:52 INFO]: Val stats: {
    "score": -0.6912853906715071,
    "rmse": 0.6912853906715071
}
[08/28/2025 10:44:52 INFO]: Test stats: {
    "score": -0.8887384070562193,
    "rmse": 0.8887384070562193
}
[08/28/2025 10:44:56 INFO]: Training loss at epoch 2: 1.2820440530776978
[08/28/2025 10:44:56 INFO]: Running Final Evaluation...
[08/28/2025 10:44:58 INFO]: Training loss at epoch 61: 1.1493066549301147
[08/28/2025 10:44:59 INFO]: Training loss at epoch 80: 0.8908230364322662
[08/28/2025 10:45:00 INFO]: Training loss at epoch 36: 0.9222492873668671
[08/28/2025 10:45:13 INFO]: New best epoch, val score: -0.666406895194814
[08/28/2025 10:45:13 INFO]: Saving model to: unhanged-Shanesha_trial_131/model_best.pth
[08/28/2025 10:45:42 INFO]: Training accuracy: {
    "score": -1.0077935222885868,
    "rmse": 1.0077935222885868
}
[08/28/2025 10:45:42 INFO]: Val accuracy: {
    "score": -0.6563556928214894,
    "rmse": 0.6563556928214894
}
[08/28/2025 10:45:42 INFO]: Test accuracy: {
    "score": -0.8761935088310365,
    "rmse": 0.8761935088310365
}
[08/28/2025 10:45:42 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_132",
    "best_epoch": 27,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8761935088310365,
        "rmse": 0.8761935088310365
    },
    "train_stats": {
        "score": -1.0077935222885868,
        "rmse": 1.0077935222885868
    },
    "val_stats": {
        "score": -0.6563556928214894,
        "rmse": 0.6563556928214894
    }
}
[08/28/2025 10:45:42 INFO]: Procewss finished for trial unhanged-Shanesha_trial_132
[08/28/2025 10:45:42 INFO]: 
_________________________________________________

[08/28/2025 10:45:42 INFO]: train_net_for_optune.py main() running.
[08/28/2025 10:45:42 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.4752113988919233
  attention_dropout: 0.3317846712639376
  ffn_dropout: 0.3317846712639376
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.9842825999730376e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_151

[08/28/2025 10:45:43 INFO]: This ft_transformer has 5.423 million parameters.
[08/28/2025 10:45:43 INFO]: Training will start at epoch 0.
[08/28/2025 10:45:43 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 10:45:49 INFO]: Training loss at epoch 11: 1.03818678855896
[08/28/2025 10:45:53 INFO]: Training loss at epoch 18: 1.3033494353294373
[08/28/2025 10:46:17 INFO]: Training loss at epoch 37: 1.2115921378135681
[08/28/2025 10:46:18 INFO]: Training loss at epoch 5: 0.9288532733917236
[08/28/2025 10:46:28 INFO]: Training loss at epoch 62: 0.8857699036598206
[08/28/2025 10:46:35 INFO]: Training loss at epoch 0: 1.0251950919628143
[08/28/2025 10:46:45 INFO]: Training loss at epoch 18: 1.1178152561187744
[08/28/2025 10:46:49 INFO]: Training loss at epoch 52: 0.8075104057788849
[08/28/2025 10:46:52 INFO]: Training loss at epoch 10: 0.9893361628055573
[08/28/2025 10:47:00 INFO]: Training loss at epoch 62: 0.9984704554080963
[08/28/2025 10:47:01 INFO]: Training loss at epoch 17: 0.9432199001312256
[08/28/2025 10:47:13 INFO]: New best epoch, val score: -0.6662972125110154
[08/28/2025 10:47:13 INFO]: Saving model to: unhanged-Shanesha_trial_138/model_best.pth
[08/28/2025 10:47:35 INFO]: Training loss at epoch 17: 1.116529107093811
[08/28/2025 10:47:36 INFO]: Training loss at epoch 38: 0.9327787756919861
[08/28/2025 10:47:37 INFO]: New best epoch, val score: -0.7207927641493512
[08/28/2025 10:47:37 INFO]: Saving model to: unhanged-Shanesha_trial_149/model_best.pth
[08/28/2025 10:47:59 INFO]: Training loss at epoch 11: 1.1373239755630493
[08/28/2025 10:48:07 INFO]: Training loss at epoch 0: 1.436882883310318
[08/28/2025 10:48:08 INFO]: Training loss at epoch 63: 0.8459025025367737
[08/28/2025 10:48:15 INFO]: Training loss at epoch 3: 1.2616469860076904
[08/28/2025 10:48:31 INFO]: Training loss at epoch 10: 1.1138901710510254
[08/28/2025 10:48:34 INFO]: Training loss at epoch 0: 1.0357823371887207
[08/28/2025 10:48:52 INFO]: Training loss at epoch 39: 1.0065671801567078
[08/28/2025 10:48:55 INFO]: New best epoch, val score: -0.6810767437772453
[08/28/2025 10:48:55 INFO]: Saving model to: unhanged-Shanesha_trial_151/model_best.pth
[08/28/2025 10:48:59 INFO]: Training loss at epoch 19: 0.8445940315723419
[08/28/2025 10:48:59 INFO]: Training loss at epoch 63: 0.9389769434928894
[08/28/2025 10:49:02 INFO]: Training loss at epoch 91: 1.2572668194770813
[08/28/2025 10:49:04 INFO]: Training loss at epoch 6: 0.9829533398151398
[08/28/2025 10:49:06 INFO]: New best epoch, val score: -0.7165601187449677
[08/28/2025 10:49:06 INFO]: Saving model to: unhanged-Shanesha_trial_150/model_best.pth
[08/28/2025 10:49:13 INFO]: Training loss at epoch 52: 1.0279090404510498
[08/28/2025 10:49:18 INFO]: Training stats: {
    "score": -0.9973526421200705,
    "rmse": 0.9973526421200705
}
[08/28/2025 10:49:18 INFO]: Val stats: {
    "score": -0.6748572989188273,
    "rmse": 0.6748572989188273
}
[08/28/2025 10:49:18 INFO]: Test stats: {
    "score": -0.8708530896118281,
    "rmse": 0.8708530896118281
}
[08/28/2025 10:49:22 INFO]: Training loss at epoch 12: 1.0016698241233826
[08/28/2025 10:49:39 INFO]: Training loss at epoch 53: 1.1265268623828888
[08/28/2025 10:49:42 INFO]: Training loss at epoch 64: 1.1162664294242859
[08/28/2025 10:49:52 INFO]: Training loss at epoch 11: 0.8566746115684509
[08/28/2025 10:49:58 INFO]: Training stats: {
    "score": -0.9992918087835274,
    "rmse": 0.9992918087835274
}
[08/28/2025 10:49:58 INFO]: Val stats: {
    "score": -0.6988375669067624,
    "rmse": 0.6988375669067624
}
[08/28/2025 10:49:58 INFO]: Test stats: {
    "score": -0.8726771509689274,
    "rmse": 0.8726771509689274
}
[08/28/2025 10:50:21 INFO]: Training loss at epoch 81: 0.998162567615509
[08/28/2025 10:50:21 INFO]: Training loss at epoch 19: 1.0754685997962952
[08/28/2025 10:50:27 INFO]: Training loss at epoch 18: 1.0213921666145325
[08/28/2025 10:50:29 INFO]: Training loss at epoch 40: 1.0622397065162659
[08/28/2025 10:50:50 INFO]: Training loss at epoch 64: 1.1481221914291382
[08/28/2025 10:50:51 INFO]: Training loss at epoch 18: 1.209670066833496
[08/28/2025 10:51:12 INFO]: Training loss at epoch 65: 0.941389262676239
[08/28/2025 10:51:18 INFO]: Training loss at epoch 4: 1.1302454471588135
[08/28/2025 10:51:28 INFO]: Training loss at epoch 12: 1.170889973640442
[08/28/2025 10:51:29 INFO]: Training loss at epoch 1: 1.0338657200336456
[08/28/2025 10:51:31 INFO]: Training stats: {
    "score": -1.005903504234417,
    "rmse": 1.005903504234417
}
[08/28/2025 10:51:31 INFO]: Val stats: {
    "score": -0.6626991763756508,
    "rmse": 0.6626991763756508
}
[08/28/2025 10:51:31 INFO]: Test stats: {
    "score": -0.8687858780151287,
    "rmse": 0.8687858780151287
}
[08/28/2025 10:51:35 INFO]: Training loss at epoch 7: 0.9626807868480682
[08/28/2025 10:51:38 INFO]: Training loss at epoch 41: 1.0594685673713684
[08/28/2025 10:51:39 INFO]: New best epoch, val score: -0.6589347665059176
[08/28/2025 10:51:39 INFO]: Saving model to: unhanged-Shanesha_trial_148/model_best.pth
[08/28/2025 10:51:50 INFO]: Training loss at epoch 11: 1.5717324912548065
[08/28/2025 10:51:56 INFO]: New best epoch, val score: -0.6626991763756508
[08/28/2025 10:51:56 INFO]: Saving model to: unhanged-Shanesha_trial_138/model_best.pth
[08/28/2025 10:52:19 INFO]: Training loss at epoch 54: 0.7894134223461151
[08/28/2025 10:52:42 INFO]: Training loss at epoch 13: 0.8131870329380035
[08/28/2025 10:52:43 INFO]: Training loss at epoch 65: 0.9427587389945984
[08/28/2025 10:52:46 INFO]: Training loss at epoch 12: 1.1600459814071655
[08/28/2025 10:52:47 INFO]: Training loss at epoch 66: 0.9069561064243317
[08/28/2025 10:52:52 INFO]: Training loss at epoch 42: 1.0081949532032013
[08/28/2025 10:52:52 INFO]: Training loss at epoch 20: 1.1748458743095398
[08/28/2025 10:53:54 INFO]: Training loss at epoch 19: 0.8701763153076172
[08/28/2025 10:54:07 INFO]: Training loss at epoch 43: 1.0158750414848328
[08/28/2025 10:54:10 INFO]: Training loss at epoch 92: 0.9437084197998047
[08/28/2025 10:54:13 INFO]: Training loss at epoch 19: 1.0507676601409912
[08/28/2025 10:54:18 INFO]: Training loss at epoch 8: 1.3006298542022705
[08/28/2025 10:54:27 INFO]: Training loss at epoch 67: 1.0347375571727753
[08/28/2025 10:54:34 INFO]: Training loss at epoch 5: 1.297260195016861
[08/28/2025 10:54:36 INFO]: Training loss at epoch 2: 1.5311039090156555
[08/28/2025 10:54:45 INFO]: Training loss at epoch 66: 0.9459378123283386
[08/28/2025 10:54:49 INFO]: Running Final Evaluation...
[08/28/2025 10:54:56 INFO]: New best epoch, val score: -0.6588860035904646
[08/28/2025 10:54:56 INFO]: Saving model to: unhanged-Shanesha_trial_148/model_best.pth
[08/28/2025 10:55:05 INFO]: Training loss at epoch 53: 0.8568766117095947
[08/28/2025 10:55:07 INFO]: Training stats: {
    "score": -1.0274634528527646,
    "rmse": 1.0274634528527646
}
[08/28/2025 10:55:07 INFO]: Val stats: {
    "score": -0.7734068236202571,
    "rmse": 0.7734068236202571
}
[08/28/2025 10:55:07 INFO]: Test stats: {
    "score": -0.9318072912225102,
    "rmse": 0.9318072912225102
}
[08/28/2025 10:55:10 INFO]: Training loss at epoch 13: 0.9830505549907684
[08/28/2025 10:55:11 INFO]: Training loss at epoch 20: 0.9732590615749359
[08/28/2025 10:55:13 INFO]: Training loss at epoch 1: 1.1116140484809875
[08/28/2025 10:55:15 INFO]: Training loss at epoch 55: 1.0147227346897125
[08/28/2025 10:55:21 INFO]: Training loss at epoch 44: 1.1166014671325684
[08/28/2025 10:55:23 INFO]: Training stats: {
    "score": -1.0075925269436234,
    "rmse": 1.0075925269436234
}
[08/28/2025 10:55:23 INFO]: Val stats: {
    "score": -0.6548373152767749,
    "rmse": 0.6548373152767749
}
[08/28/2025 10:55:23 INFO]: Test stats: {
    "score": -0.8598355766697694,
    "rmse": 0.8598355766697694
}
[08/28/2025 10:55:26 INFO]: Training loss at epoch 12: 1.0635578036308289
[08/28/2025 10:55:37 INFO]: Running Final Evaluation...
[08/28/2025 10:55:39 INFO]: New best epoch, val score: -0.6613634948764058
[08/28/2025 10:55:39 INFO]: Saving model to: unhanged-Shanesha_trial_138/model_best.pth
[08/28/2025 10:55:40 INFO]: Training loss at epoch 82: 0.9597060084342957
[08/28/2025 10:55:52 INFO]: Training loss at epoch 13: 1.0806370377540588
[08/28/2025 10:55:56 INFO]: Training loss at epoch 21: 1.0852458477020264
[08/28/2025 10:56:06 INFO]: Training loss at epoch 68: 0.8949411511421204
[08/28/2025 10:56:19 INFO]: Training loss at epoch 14: 1.0342050194740295
[08/28/2025 10:56:30 INFO]: Training loss at epoch 1: 1.3977895975112915
[08/28/2025 10:56:36 INFO]: Training loss at epoch 45: 1.4240215122699738
[08/28/2025 10:56:46 INFO]: Training loss at epoch 67: 0.9904777705669403
[08/28/2025 10:56:50 INFO]: Training accuracy: {
    "score": -1.0001868216270102,
    "rmse": 1.0001868216270102
}
[08/28/2025 10:56:50 INFO]: Val accuracy: {
    "score": -0.6656326958095614,
    "rmse": 0.6656326958095614
}
[08/28/2025 10:56:50 INFO]: Test accuracy: {
    "score": -0.8694347029770788,
    "rmse": 0.8694347029770788
}
[08/28/2025 10:56:50 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_128",
    "best_epoch": 24,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8694347029770788,
        "rmse": 0.8694347029770788
    },
    "train_stats": {
        "score": -1.0001868216270102,
        "rmse": 1.0001868216270102
    },
    "val_stats": {
        "score": -0.6656326958095614,
        "rmse": 0.6656326958095614
    }
}
[08/28/2025 10:56:50 INFO]: Procewss finished for trial unhanged-Shanesha_trial_128
[08/28/2025 10:56:50 INFO]: 
_________________________________________________

[08/28/2025 10:56:50 INFO]: train_net_for_optune.py main() running.
[08/28/2025 10:56:50 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.3713385042715016
  attention_dropout: 0.3433672307712864
  ffn_dropout: 0.3433672307712864
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.4504444474092065e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_152

[08/28/2025 10:56:50 INFO]: This ft_transformer has 0.777 million parameters.
[08/28/2025 10:56:50 INFO]: Training will start at epoch 0.
[08/28/2025 10:56:50 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 10:56:57 INFO]: Training accuracy: {
    "score": -1.0119728645078805,
    "rmse": 1.0119728645078805
}
[08/28/2025 10:56:57 INFO]: Val accuracy: {
    "score": -0.6605124391860842,
    "rmse": 0.6605124391860842
}
[08/28/2025 10:56:57 INFO]: Test accuracy: {
    "score": -0.8704885779108134,
    "rmse": 0.8704885779108134
}
[08/28/2025 10:56:57 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_90",
    "best_epoch": 61,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8704885779108134,
        "rmse": 0.8704885779108134
    },
    "train_stats": {
        "score": -1.0119728645078805,
        "rmse": 1.0119728645078805
    },
    "val_stats": {
        "score": -0.6605124391860842,
        "rmse": 0.6605124391860842
    }
}
[08/28/2025 10:56:57 INFO]: Procewss finished for trial unhanged-Shanesha_trial_90
[08/28/2025 10:56:58 INFO]: 
_________________________________________________

[08/28/2025 10:56:58 INFO]: train_net_for_optune.py main() running.
[08/28/2025 10:56:58 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.4941336655847737
  attention_dropout: 0.18514272315153624
  ffn_dropout: 0.18514272315153624
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.959089940832053e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_153

[08/28/2025 10:56:58 INFO]: This ft_transformer has 0.802 million parameters.
[08/28/2025 10:56:58 INFO]: Training will start at epoch 0.
[08/28/2025 10:56:58 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 10:57:05 INFO]: Training loss at epoch 9: 0.914301872253418
[08/28/2025 10:57:31 INFO]: New best epoch, val score: -0.6826067763995489
[08/28/2025 10:57:31 INFO]: Saving model to: unhanged-Shanesha_trial_150/model_best.pth
[08/28/2025 10:57:46 INFO]: Training loss at epoch 69: 1.1393744051456451
[08/28/2025 10:57:46 INFO]: Training loss at epoch 3: 1.2049862146377563
[08/28/2025 10:57:51 INFO]: Training loss at epoch 46: 0.8848533034324646
[08/28/2025 10:57:51 INFO]: Training loss at epoch 6: 1.1862443685531616
[08/28/2025 10:57:55 INFO]: Training loss at epoch 0: 1.1679874062538147
[08/28/2025 10:57:59 INFO]: Training stats: {
    "score": -1.0339894482754066,
    "rmse": 1.0339894482754066
}
[08/28/2025 10:57:59 INFO]: Val stats: {
    "score": -0.6600475054922651,
    "rmse": 0.6600475054922651
}
[08/28/2025 10:57:59 INFO]: Test stats: {
    "score": -0.8871672515897202,
    "rmse": 0.8871672515897202
}
[08/28/2025 10:58:00 INFO]: Running Final Evaluation...
[08/28/2025 10:58:02 INFO]: Training loss at epoch 0: 1.29118412733078
[08/28/2025 10:58:04 INFO]: New best epoch, val score: -0.8628872485004928
[08/28/2025 10:58:04 INFO]: Saving model to: unhanged-Shanesha_trial_152/model_best.pth
[08/28/2025 10:58:11 INFO]: New best epoch, val score: -0.6809574203952747
[08/28/2025 10:58:11 INFO]: Saving model to: unhanged-Shanesha_trial_153/model_best.pth
[08/28/2025 10:58:15 INFO]: Training stats: {
    "score": -0.9761783801735056,
    "rmse": 0.9761783801735056
}
[08/28/2025 10:58:15 INFO]: Val stats: {
    "score": -0.6827125406718364,
    "rmse": 0.6827125406718364
}
[08/28/2025 10:58:15 INFO]: Test stats: {
    "score": -0.8729016463088086,
    "rmse": 0.8729016463088086
}
[08/28/2025 10:58:25 INFO]: Training accuracy: {
    "score": -1.0104830860445249,
    "rmse": 1.0104830860445249
}
[08/28/2025 10:58:25 INFO]: Val accuracy: {
    "score": -0.6631185753544586,
    "rmse": 0.6631185753544586
}
[08/28/2025 10:58:25 INFO]: Test accuracy: {
    "score": -0.87130676152121,
    "rmse": 0.87130676152121
}
[08/28/2025 10:58:25 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_142",
    "best_epoch": 15,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.87130676152121,
        "rmse": 0.87130676152121
    },
    "train_stats": {
        "score": -1.0104830860445249,
        "rmse": 1.0104830860445249
    },
    "val_stats": {
        "score": -0.6631185753544586,
        "rmse": 0.6631185753544586
    }
}
[08/28/2025 10:58:25 INFO]: Procewss finished for trial unhanged-Shanesha_trial_142
[08/28/2025 10:58:26 INFO]: 
_________________________________________________

[08/28/2025 10:58:26 INFO]: train_net_for_optune.py main() running.
[08/28/2025 10:58:26 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 2.484759920176301
  attention_dropout: 0.2504231516891567
  ffn_dropout: 0.2504231516891567
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.060603907456703e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_154

[08/28/2025 10:58:26 INFO]: This ft_transformer has 5.435 million parameters.
[08/28/2025 10:58:26 INFO]: Training will start at epoch 0.
[08/28/2025 10:58:26 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 10:58:40 INFO]: Training loss at epoch 20: 1.0609940886497498
[08/28/2025 10:58:42 INFO]: Training loss at epoch 68: 0.8979733884334564
[08/28/2025 10:58:46 INFO]: Training loss at epoch 20: 1.1999660730361938
[08/28/2025 10:58:50 INFO]: Training loss at epoch 14: 1.0731667876243591
[08/28/2025 10:58:51 INFO]: Training loss at epoch 21: 0.8536536991596222
[08/28/2025 10:58:54 INFO]: Training loss at epoch 14: 1.0109806060791016
[08/28/2025 10:58:56 INFO]: Training loss at epoch 22: 1.1190388202667236
[08/28/2025 10:58:56 INFO]: Training loss at epoch 13: 1.2737604975700378
[08/28/2025 10:59:05 INFO]: Training loss at epoch 1: 1.001992017030716
[08/28/2025 10:59:13 INFO]: Training loss at epoch 1: 1.1227754354476929
[08/28/2025 10:59:13 INFO]: New best epoch, val score: -0.7869942728923339
[08/28/2025 10:59:13 INFO]: Saving model to: unhanged-Shanesha_trial_152/model_best.pth
[08/28/2025 10:59:22 INFO]: New best epoch, val score: -0.6638028572646996
[08/28/2025 10:59:22 INFO]: Saving model to: unhanged-Shanesha_trial_153/model_best.pth
[08/28/2025 10:59:45 INFO]: Training loss at epoch 15: 1.1572116017341614
[08/28/2025 10:59:46 INFO]: Training loss at epoch 70: 0.9338725507259369
[08/28/2025 11:00:14 INFO]: Training loss at epoch 2: 0.9761022627353668
[08/28/2025 11:00:22 INFO]: New best epoch, val score: -0.7025071903949409
[08/28/2025 11:00:22 INFO]: Saving model to: unhanged-Shanesha_trial_152/model_best.pth
[08/28/2025 11:00:24 INFO]: Training loss at epoch 2: 0.9136529862880707
[08/28/2025 11:00:29 INFO]: Training loss at epoch 0: 1.1394161581993103
[08/28/2025 11:00:30 INFO]: Training loss at epoch 10: 1.1534048318862915
[08/28/2025 11:00:35 INFO]: Training loss at epoch 69: 1.0488116145133972
[08/28/2025 11:00:39 INFO]: Training loss at epoch 4: 1.428456723690033
[08/28/2025 11:00:46 INFO]: New best epoch, val score: -0.7218967968207971
[08/28/2025 11:00:46 INFO]: Saving model to: unhanged-Shanesha_trial_154/model_best.pth
[08/28/2025 11:00:48 INFO]: New best epoch, val score: -0.6540185858476795
[08/28/2025 11:00:48 INFO]: Saving model to: unhanged-Shanesha_trial_147/model_best.pth
[08/28/2025 11:00:52 INFO]: Training loss at epoch 83: 0.8843733966350555
[08/28/2025 11:00:54 INFO]: Training loss at epoch 7: 0.8855180144309998
[08/28/2025 11:00:59 INFO]: Training loss at epoch 54: 0.8757994472980499
[08/28/2025 11:01:16 INFO]: Training stats: {
    "score": -0.9986891458158658,
    "rmse": 0.9986891458158658
}
[08/28/2025 11:01:16 INFO]: Val stats: {
    "score": -0.7116304225209105,
    "rmse": 0.7116304225209105
}
[08/28/2025 11:01:16 INFO]: Test stats: {
    "score": -0.8923567521090167,
    "rmse": 0.8923567521090167
}
[08/28/2025 11:01:18 INFO]: Training loss at epoch 71: 0.9770906865596771
[08/28/2025 11:01:27 INFO]: Training loss at epoch 3: 0.9954538345336914
[08/28/2025 11:01:36 INFO]: New best epoch, val score: -0.6570923067727671
[08/28/2025 11:01:36 INFO]: Saving model to: unhanged-Shanesha_trial_152/model_best.pth
[08/28/2025 11:01:38 INFO]: Training loss at epoch 3: 1.0117881298065186
[08/28/2025 11:01:50 INFO]: Training loss at epoch 23: 1.0900562405586243
[08/28/2025 11:01:51 INFO]: Training loss at epoch 15: 1.2226335108280182
[08/28/2025 11:01:59 INFO]: Training loss at epoch 21: 0.9365472197532654
[08/28/2025 11:02:02 INFO]: Training loss at epoch 21: 1.1625441312789917
[08/28/2025 11:02:24 INFO]: Training loss at epoch 14: 1.2013275623321533
[08/28/2025 11:02:25 INFO]: Training loss at epoch 15: 1.1248950958251953
[08/28/2025 11:02:27 INFO]: Training loss at epoch 22: 1.080600917339325
[08/28/2025 11:02:43 INFO]: Training loss at epoch 4: 1.0477807521820068
[08/28/2025 11:02:54 INFO]: Training loss at epoch 4: 0.9257903397083282
[08/28/2025 11:02:59 INFO]: Training loss at epoch 72: 1.0143440067768097
[08/28/2025 11:03:02 INFO]: Training loss at epoch 1: 1.0361039638519287
[08/28/2025 11:03:15 INFO]: Training loss at epoch 16: 1.0931487679481506
[08/28/2025 11:03:15 INFO]: Training loss at epoch 11: 1.0047696232795715
[08/28/2025 11:03:19 INFO]: Training loss at epoch 70: 1.0129894018173218
[08/28/2025 11:03:21 INFO]: New best epoch, val score: -0.6608314559885491
[08/28/2025 11:03:21 INFO]: Saving model to: unhanged-Shanesha_trial_154/model_best.pth
[08/28/2025 11:03:50 INFO]: Training loss at epoch 5: 0.9470906853675842
[08/28/2025 11:03:52 INFO]: Training loss at epoch 2: 0.8875381648540497
[08/28/2025 11:03:59 INFO]: Training loss at epoch 5: 1.0425435900688171
[08/28/2025 11:04:10 INFO]: Training loss at epoch 5: 1.0700843930244446
[08/28/2025 11:04:13 INFO]: Training loss at epoch 8: 1.0783212780952454
[08/28/2025 11:04:40 INFO]: Training loss at epoch 73: 0.9072200655937195
[08/28/2025 11:04:51 INFO]: New best epoch, val score: -0.6670915292420107
[08/28/2025 11:04:51 INFO]: Saving model to: unhanged-Shanesha_trial_134/model_best.pth
[08/28/2025 11:04:56 INFO]: New best epoch, val score: -0.7138867582869532
[08/28/2025 11:04:56 INFO]: Saving model to: unhanged-Shanesha_trial_149/model_best.pth
[08/28/2025 11:04:58 INFO]: Training loss at epoch 2: 0.9625591337680817
[08/28/2025 11:04:58 INFO]: Training loss at epoch 24: 0.9480233192443848
[08/28/2025 11:05:01 INFO]: Training loss at epoch 16: 1.0128632485866547
[08/28/2025 11:05:15 INFO]: Training loss at epoch 6: 1.0385167002677917
[08/28/2025 11:05:22 INFO]: Training loss at epoch 71: 1.1748794317245483
[08/28/2025 11:05:28 INFO]: Training loss at epoch 6: 0.9616678655147552
[08/28/2025 11:05:30 INFO]: Training loss at epoch 22: 0.8447460234165192
[08/28/2025 11:05:39 INFO]: Training loss at epoch 2: 1.0775432586669922
[08/28/2025 11:05:40 INFO]: Training loss at epoch 22: 0.9197707772254944
[08/28/2025 11:05:58 INFO]: New best epoch, val score: -0.6598315122097066
[08/28/2025 11:05:58 INFO]: Saving model to: unhanged-Shanesha_trial_154/model_best.pth
[08/28/2025 11:05:59 INFO]: New best epoch, val score: -0.6785350775675308
[08/28/2025 11:05:59 INFO]: Saving model to: unhanged-Shanesha_trial_150/model_best.pth
[08/28/2025 11:06:04 INFO]: Training loss at epoch 12: 0.9957234263420105
[08/28/2025 11:06:04 INFO]: Training loss at epoch 15: 1.0727126002311707
[08/28/2025 11:06:13 INFO]: Training loss at epoch 16: 1.3021131753921509
[08/28/2025 11:06:14 INFO]: Training loss at epoch 23: 0.9862074851989746
[08/28/2025 11:06:21 INFO]: Training loss at epoch 74: 1.179785579442978
[08/28/2025 11:06:22 INFO]: Training loss at epoch 84: 1.0079094171524048
[08/28/2025 11:06:30 INFO]: Training loss at epoch 7: 0.934507817029953
[08/28/2025 11:06:32 INFO]: New best epoch, val score: -0.6668078469692766
[08/28/2025 11:06:32 INFO]: Saving model to: unhanged-Shanesha_trial_134/model_best.pth
[08/28/2025 11:06:44 INFO]: Training loss at epoch 7: 0.9582227468490601
[08/28/2025 11:06:52 INFO]: Training loss at epoch 17: 1.0373031497001648
[08/28/2025 11:06:59 INFO]: Running Final Evaluation...
[08/28/2025 11:06:59 INFO]: Training loss at epoch 6: 1.0557544827461243
[08/28/2025 11:07:06 INFO]: Training loss at epoch 55: 1.0348844528198242
[08/28/2025 11:07:18 INFO]: Training loss at epoch 72: 0.9350028038024902
[08/28/2025 11:07:26 INFO]: Training loss at epoch 9: 0.9454129040241241
[08/28/2025 11:07:39 INFO]: Training loss at epoch 8: 1.1343459486961365
[08/28/2025 11:07:52 INFO]: Training loss at epoch 75: 0.8733014464378357
[08/28/2025 11:07:55 INFO]: Training loss at epoch 8: 1.2638935446739197
[08/28/2025 11:07:58 INFO]: Training loss at epoch 25: 0.8659593760967255
[08/28/2025 11:08:01 INFO]: Training loss at epoch 17: 1.1340388655662537
[08/28/2025 11:08:02 INFO]: New best epoch, val score: -0.666782660120223
[08/28/2025 11:08:02 INFO]: Saving model to: unhanged-Shanesha_trial_134/model_best.pth
[08/28/2025 11:08:06 INFO]: Training loss at epoch 3: 0.944855660200119
[08/28/2025 11:08:26 INFO]: Training stats: {
    "score": -1.0005494553900085,
    "rmse": 1.0005494553900085
}
[08/28/2025 11:08:26 INFO]: Val stats: {
    "score": -0.6592305609591794,
    "rmse": 0.6592305609591794
}
[08/28/2025 11:08:26 INFO]: Test stats: {
    "score": -0.8898558472870346,
    "rmse": 0.8898558472870346
}
[08/28/2025 11:08:37 INFO]: Training loss at epoch 13: 0.7103712409734726
[08/28/2025 11:08:45 INFO]: Training loss at epoch 23: 1.0270047187805176
[08/28/2025 11:08:48 INFO]: Training loss at epoch 9: 1.3711902499198914
[08/28/2025 11:09:03 INFO]: Training loss at epoch 23: 1.0408973693847656
[08/28/2025 11:09:06 INFO]: Training loss at epoch 9: 1.0168963074684143
[08/28/2025 11:09:08 INFO]: Training loss at epoch 73: 1.1976326406002045
[08/28/2025 11:09:13 INFO]: Training stats: {
    "score": -1.0465894707700025,
    "rmse": 1.0465894707700025
}
[08/28/2025 11:09:13 INFO]: Val stats: {
    "score": -0.674056861389193,
    "rmse": 0.674056861389193
}
[08/28/2025 11:09:13 INFO]: Test stats: {
    "score": -0.9009371992529017,
    "rmse": 0.9009371992529017
}
[08/28/2025 11:09:23 INFO]: Training loss at epoch 76: 0.9525555372238159
[08/28/2025 11:09:25 INFO]: Training loss at epoch 16: 1.052395224571228
[08/28/2025 11:09:29 INFO]: Training accuracy: {
    "score": -1.0027145528348778,
    "rmse": 1.0027145528348778
}
[08/28/2025 11:09:29 INFO]: Val accuracy: {
    "score": -0.6591113240640458,
    "rmse": 0.6591113240640458
}
[08/28/2025 11:09:29 INFO]: Test accuracy: {
    "score": -0.8711720506429158,
    "rmse": 0.8711720506429158
}
[08/28/2025 11:09:29 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_97",
    "best_epoch": 53,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8711720506429158,
        "rmse": 0.8711720506429158
    },
    "train_stats": {
        "score": -1.0027145528348778,
        "rmse": 1.0027145528348778
    },
    "val_stats": {
        "score": -0.6591113240640458,
        "rmse": 0.6591113240640458
    }
}
[08/28/2025 11:09:29 INFO]: Procewss finished for trial unhanged-Shanesha_trial_97
[08/28/2025 11:09:30 INFO]: 
_________________________________________________

[08/28/2025 11:09:30 INFO]: train_net_for_optune.py main() running.
[08/28/2025 11:09:30 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 2.369996170701023
  attention_dropout: 0.3348340600469723
  ffn_dropout: 0.3348340600469723
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.0084963538790116e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_155

[08/28/2025 11:09:30 INFO]: This ft_transformer has 5.300 million parameters.
[08/28/2025 11:09:30 INFO]: Training will start at epoch 0.
[08/28/2025 11:09:30 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 11:09:30 INFO]: Training stats: {
    "score": -1.0546994665115743,
    "rmse": 1.0546994665115743
}
[08/28/2025 11:09:30 INFO]: Val stats: {
    "score": -0.8260933408626867,
    "rmse": 0.8260933408626867
}
[08/28/2025 11:09:30 INFO]: Test stats: {
    "score": -0.9794781932354721,
    "rmse": 0.9794781932354721
}
[08/28/2025 11:09:33 INFO]: New best epoch, val score: -0.6667255356116343
[08/28/2025 11:09:33 INFO]: Saving model to: unhanged-Shanesha_trial_134/model_best.pth
[08/28/2025 11:09:39 INFO]: Training loss at epoch 24: 0.9413824379444122
[08/28/2025 11:09:40 INFO]: Training loss at epoch 17: 1.2123591303825378
[08/28/2025 11:09:53 INFO]: Training loss at epoch 7: 1.1572510600090027
[08/28/2025 11:10:10 INFO]: Training loss at epoch 18: 1.1128242015838623
[08/28/2025 11:10:24 INFO]: Training loss at epoch 10: 1.1134076118469238
[08/28/2025 11:10:28 INFO]: Training loss at epoch 4: 1.0201658010482788
[08/28/2025 11:10:45 INFO]: Training loss at epoch 10: 1.2580810189247131
[08/28/2025 11:10:52 INFO]: Training loss at epoch 26: 0.9982168972492218
[08/28/2025 11:11:00 INFO]: Training loss at epoch 18: 0.897697240114212
[08/28/2025 11:11:00 INFO]: Training loss at epoch 77: 0.7617173492908478
[08/28/2025 11:11:05 INFO]: Training loss at epoch 74: 1.0739555954933167
[08/28/2025 11:11:15 INFO]: Training loss at epoch 14: 1.3204309940338135
[08/28/2025 11:11:36 INFO]: Training loss at epoch 10: 0.7866898030042648
[08/28/2025 11:11:40 INFO]: Training loss at epoch 11: 1.0561932921409607
[08/28/2025 11:11:42 INFO]: Training loss at epoch 0: 1.0083752870559692
[08/28/2025 11:12:00 INFO]: New best epoch, val score: -0.6728887895272507
[08/28/2025 11:12:00 INFO]: Saving model to: unhanged-Shanesha_trial_155/model_best.pth
[08/28/2025 11:12:03 INFO]: Training loss at epoch 11: 0.915986567735672
[08/28/2025 11:12:07 INFO]: Training loss at epoch 24: 0.8536535203456879
[08/28/2025 11:12:33 INFO]: Training loss at epoch 3: 1.0924167037010193
[08/28/2025 11:12:35 INFO]: Training loss at epoch 24: 0.8980869352817535
[08/28/2025 11:12:40 INFO]: Training loss at epoch 78: 0.863760381937027
[08/28/2025 11:12:55 INFO]: Training loss at epoch 12: 0.858460009098053
[08/28/2025 11:13:00 INFO]: Training loss at epoch 56: 0.7664914131164551
[08/28/2025 11:13:00 INFO]: Training loss at epoch 17: 0.9627026319503784
[08/28/2025 11:13:03 INFO]: Training loss at epoch 5: 0.8667877316474915
[08/28/2025 11:13:04 INFO]: Training loss at epoch 8: 1.1302893161773682
[08/28/2025 11:13:05 INFO]: Training loss at epoch 75: 1.033302128314972
[08/28/2025 11:13:20 INFO]: Training loss at epoch 12: 1.0392440557479858
[08/28/2025 11:13:23 INFO]: Training loss at epoch 3: 0.9676628708839417
[08/28/2025 11:13:24 INFO]: Training loss at epoch 25: 0.9651431739330292
[08/28/2025 11:13:25 INFO]: Training loss at epoch 18: 1.0788654685020447
[08/28/2025 11:13:50 INFO]: Training loss at epoch 19: 0.9696814715862274
[08/28/2025 11:14:01 INFO]: Training loss at epoch 27: 1.2293585538864136
[08/28/2025 11:14:03 INFO]: Training loss at epoch 15: 1.27194145321846
[08/28/2025 11:14:10 INFO]: Training loss at epoch 19: 0.9537234902381897
[08/28/2025 11:14:11 INFO]: Training loss at epoch 13: 0.986680418252945
[08/28/2025 11:14:17 INFO]: Training loss at epoch 1: 1.0429652035236359
[08/28/2025 11:14:21 INFO]: Training loss at epoch 79: 0.8319526612758636
[08/28/2025 11:14:37 INFO]: Training loss at epoch 13: 1.1723999381065369
[08/28/2025 11:14:55 INFO]: Training stats: {
    "score": -0.976347162478577,
    "rmse": 0.976347162478577
}
[08/28/2025 11:14:55 INFO]: Val stats: {
    "score": -0.6700729889292933,
    "rmse": 0.6700729889292933
}
[08/28/2025 11:14:55 INFO]: Test stats: {
    "score": -0.871088442503646,
    "rmse": 0.871088442503646
}
[08/28/2025 11:14:57 INFO]: Training loss at epoch 11: 0.8848473727703094
[08/28/2025 11:15:05 INFO]: Training stats: {
    "score": -1.017803547699951,
    "rmse": 1.017803547699951
}
[08/28/2025 11:15:05 INFO]: Val stats: {
    "score": -0.7713187932210874,
    "rmse": 0.7713187932210874
}
[08/28/2025 11:15:05 INFO]: Test stats: {
    "score": -0.9190926193372783,
    "rmse": 0.9190926193372783
}
[08/28/2025 11:15:07 INFO]: Training loss at epoch 76: 1.0871912837028503
[08/28/2025 11:15:15 INFO]: Training stats: {
    "score": -1.0020992625860292,
    "rmse": 1.0020992625860292
}
[08/28/2025 11:15:15 INFO]: Val stats: {
    "score": -0.7161674404690894,
    "rmse": 0.7161674404690894
}
[08/28/2025 11:15:15 INFO]: Test stats: {
    "score": -0.8964621996943195,
    "rmse": 0.8964621996943195
}
[08/28/2025 11:15:26 INFO]: Training loss at epoch 14: 1.031345695257187
[08/28/2025 11:15:36 INFO]: Training loss at epoch 25: 1.1411038637161255
[08/28/2025 11:15:40 INFO]: Training loss at epoch 6: 1.1495188474655151
[08/28/2025 11:15:52 INFO]: Training loss at epoch 14: 0.8573123216629028
[08/28/2025 11:16:10 INFO]: Training loss at epoch 25: 0.983280599117279
[08/28/2025 11:16:12 INFO]: Training loss at epoch 9: 1.302160233259201
[08/28/2025 11:16:28 INFO]: Training loss at epoch 80: 0.8936077654361725
[08/28/2025 11:16:33 INFO]: Training loss at epoch 18: 1.15955251455307
[08/28/2025 11:16:36 INFO]: Training loss at epoch 15: 1.1378099918365479
[08/28/2025 11:16:42 INFO]: Training loss at epoch 16: 1.1344305872917175
[08/28/2025 11:16:42 INFO]: Training loss at epoch 2: 0.8811900317668915
[08/28/2025 11:16:45 INFO]: New best epoch, val score: -0.6565621689607477
[08/28/2025 11:16:45 INFO]: Saving model to: unhanged-Shanesha_trial_152/model_best.pth
[08/28/2025 11:16:59 INFO]: Training loss at epoch 28: 0.9513368308544159
[08/28/2025 11:17:00 INFO]: Training loss at epoch 77: 1.1316061615943909
[08/28/2025 11:17:01 INFO]: Training loss at epoch 26: 0.966399610042572
[08/28/2025 11:17:03 INFO]: Training loss at epoch 15: 0.9924189448356628
[08/28/2025 11:17:03 INFO]: Training loss at epoch 19: 0.9249339401721954
[08/28/2025 11:17:12 INFO]: Training stats: {
    "score": -1.0242897273757272,
    "rmse": 1.0242897273757272
}
[08/28/2025 11:17:12 INFO]: Val stats: {
    "score": -0.7744477044720404,
    "rmse": 0.7744477044720404
}
[08/28/2025 11:17:12 INFO]: Test stats: {
    "score": -0.9270288056893032,
    "rmse": 0.9270288056893032
}
[08/28/2025 11:17:45 INFO]: Training loss at epoch 16: 1.0368621945381165
[08/28/2025 11:17:54 INFO]: New best epoch, val score: -0.6560491203832881
[08/28/2025 11:17:54 INFO]: Saving model to: unhanged-Shanesha_trial_152/model_best.pth
[08/28/2025 11:17:59 INFO]: Training loss at epoch 81: 0.9405131340026855
[08/28/2025 11:18:00 INFO]: Training loss at epoch 12: 1.006044715642929
[08/28/2025 11:18:00 INFO]: Training loss at epoch 7: 1.1231175661087036
[08/28/2025 11:18:07 INFO]: Training loss at epoch 20: 1.068207323551178
[08/28/2025 11:18:13 INFO]: Training loss at epoch 16: 0.9677408635616302
[08/28/2025 11:18:13 INFO]: Training stats: {
    "score": -1.0009247603794553,
    "rmse": 1.0009247603794553
}
[08/28/2025 11:18:13 INFO]: Val stats: {
    "score": -0.6909680517355618,
    "rmse": 0.6909680517355618
}
[08/28/2025 11:18:13 INFO]: Test stats: {
    "score": -0.8754791998460681,
    "rmse": 0.8754791998460681
}
[08/28/2025 11:18:25 INFO]: Training loss at epoch 20: 0.9407416880130768
[08/28/2025 11:18:46 INFO]: Training loss at epoch 26: 1.098118394613266
[08/28/2025 11:18:51 INFO]: Training loss at epoch 78: 1.1297566294670105
[08/28/2025 11:18:53 INFO]: Training loss at epoch 57: 1.01489919424057
[08/28/2025 11:18:57 INFO]: Training loss at epoch 17: 1.056790053844452
[08/28/2025 11:19:03 INFO]: Training loss at epoch 3: 1.343008279800415
[08/28/2025 11:19:15 INFO]: Training loss at epoch 17: 0.8708595335483551
[08/28/2025 11:19:19 INFO]: New best epoch, val score: -0.6667345257955043
[08/28/2025 11:19:19 INFO]: Saving model to: unhanged-Shanesha_trial_155/model_best.pth
[08/28/2025 11:19:25 INFO]: Training loss at epoch 17: 0.8648010194301605
[08/28/2025 11:19:31 INFO]: Training loss at epoch 26: 0.8989485502243042
[08/28/2025 11:19:34 INFO]: Training loss at epoch 82: 1.0315004885196686
[08/28/2025 11:19:54 INFO]: Training loss at epoch 29: 0.9856467843055725
[08/28/2025 11:19:58 INFO]: Training loss at epoch 19: 1.2476006746292114
[08/28/2025 11:20:11 INFO]: Training loss at epoch 18: 1.1744505167007446
[08/28/2025 11:20:12 INFO]: Training loss at epoch 10: 1.1636778116226196
[08/28/2025 11:20:30 INFO]: Training loss at epoch 8: 1.1444258093833923
[08/28/2025 11:20:35 INFO]: Training loss at epoch 27: 0.872304767370224
[08/28/2025 11:20:42 INFO]: Training loss at epoch 18: 1.0089051723480225
[08/28/2025 11:20:51 INFO]: Training loss at epoch 79: 0.9718254804611206
[08/28/2025 11:20:59 INFO]: Training stats: {
    "score": -1.0005285009060825,
    "rmse": 1.0005285009060825
}
[08/28/2025 11:20:59 INFO]: Val stats: {
    "score": -0.7132306349138451,
    "rmse": 0.7132306349138451
}
[08/28/2025 11:20:59 INFO]: Test stats: {
    "score": -0.8814438274149577,
    "rmse": 0.8814438274149577
}
[08/28/2025 11:21:12 INFO]: Training loss at epoch 21: 1.3355010151863098
[08/28/2025 11:21:14 INFO]: Training stats: {
    "score": -1.0044761424071258,
    "rmse": 1.0044761424071258
}
[08/28/2025 11:21:14 INFO]: Val stats: {
    "score": -0.6927215315224282,
    "rmse": 0.6927215315224282
}
[08/28/2025 11:21:14 INFO]: Test stats: {
    "score": -0.8772468961489847,
    "rmse": 0.8772468961489847
}
[08/28/2025 11:21:14 INFO]: Training loss at epoch 13: 0.9717783331871033
[08/28/2025 11:21:14 INFO]: Training loss at epoch 4: 1.114008367061615
[08/28/2025 11:21:14 INFO]: Training loss at epoch 83: 0.9762592911720276
[08/28/2025 11:21:26 INFO]: Training loss at epoch 19: 0.9171430468559265
[08/28/2025 11:21:32 INFO]: Training stats: {
    "score": -0.9972215078136916,
    "rmse": 0.9972215078136916
}
[08/28/2025 11:21:32 INFO]: Val stats: {
    "score": -0.6716862924050697,
    "rmse": 0.6716862924050697
}
[08/28/2025 11:21:32 INFO]: Test stats: {
    "score": -0.8723371938232015,
    "rmse": 0.8723371938232015
}
[08/28/2025 11:21:36 INFO]: Training loss at epoch 4: 1.0073242485523224
[08/28/2025 11:21:49 INFO]: Training loss at epoch 4: 1.0119511485099792
[08/28/2025 11:21:53 INFO]: Training stats: {
    "score": -1.0040129624913925,
    "rmse": 1.0040129624913925
}
[08/28/2025 11:21:53 INFO]: Val stats: {
    "score": -0.6577083610301787,
    "rmse": 0.6577083610301787
}
[08/28/2025 11:21:53 INFO]: Test stats: {
    "score": -0.8744212492267925,
    "rmse": 0.8744212492267925
}
[08/28/2025 11:21:56 INFO]: Training loss at epoch 20: 0.9700666666030884
[08/28/2025 11:21:58 INFO]: Training loss at epoch 19: 1.1006542444229126
[08/28/2025 11:22:02 INFO]: Training loss at epoch 18: 1.0441224575042725
[08/28/2025 11:22:02 INFO]: Training loss at epoch 21: 1.0191407203674316
[08/28/2025 11:22:15 INFO]: Training loss at epoch 27: 0.8561940789222717
[08/28/2025 11:22:26 INFO]: Training stats: {
    "score": -1.0028175826057288,
    "rmse": 1.0028175826057288
}
[08/28/2025 11:22:26 INFO]: Val stats: {
    "score": -0.7145201509764558,
    "rmse": 0.7145201509764558
}
[08/28/2025 11:22:26 INFO]: Test stats: {
    "score": -0.902955111679378,
    "rmse": 0.902955111679378
}
[08/28/2025 11:22:55 INFO]: Training loss at epoch 84: 1.2589768767356873
[08/28/2025 11:23:06 INFO]: Training loss at epoch 9: 1.1863247156143188
[08/28/2025 11:23:08 INFO]: Training loss at epoch 20: 0.8451790809631348
[08/28/2025 11:23:10 INFO]: Training loss at epoch 27: 0.9152543246746063
[08/28/2025 11:23:24 INFO]: Training loss at epoch 11: 1.0905107855796814
[08/28/2025 11:23:34 INFO]: Training loss at epoch 80: 1.2224673330783844
[08/28/2025 11:23:43 INFO]: Training loss at epoch 20: 0.9566751718521118
[08/28/2025 11:23:59 INFO]: Training stats: {
    "score": -1.0078191068910902,
    "rmse": 1.0078191068910902
}
[08/28/2025 11:23:59 INFO]: Val stats: {
    "score": -0.7205546080317837,
    "rmse": 0.7205546080317837
}
[08/28/2025 11:23:59 INFO]: Test stats: {
    "score": -0.9032136190279539,
    "rmse": 0.9032136190279539
}
[08/28/2025 11:24:07 INFO]: Training loss at epoch 30: 1.1063267588615417
[08/28/2025 11:24:09 INFO]: Training loss at epoch 5: 1.2056623697280884
[08/28/2025 11:24:20 INFO]: Training loss at epoch 22: 1.0319011807441711
[08/28/2025 11:24:23 INFO]: Training loss at epoch 28: 0.917637825012207
[08/28/2025 11:24:23 INFO]: Training loss at epoch 21: 1.0560044646263123
[08/28/2025 11:24:33 INFO]: Training loss at epoch 14: 0.844110906124115
[08/28/2025 11:24:34 INFO]: Training loss at epoch 85: 0.9518245756626129
[08/28/2025 11:24:49 INFO]: Training loss at epoch 19: 1.0487676858901978
[08/28/2025 11:24:51 INFO]: Training loss at epoch 20: 1.0249140560626984
[08/28/2025 11:24:58 INFO]: Training loss at epoch 21: 1.1697987616062164
[08/28/2025 11:25:02 INFO]: Training loss at epoch 58: 1.008272647857666
[08/28/2025 11:25:31 INFO]: Training loss at epoch 81: 1.0579330325126648
[08/28/2025 11:25:35 INFO]: Training loss at epoch 22: 0.9931536316871643
[08/28/2025 11:25:37 INFO]: Training loss at epoch 22: 0.9381037652492523
[08/28/2025 11:25:38 INFO]: Training loss at epoch 21: 1.001665085554123
[08/28/2025 11:25:39 INFO]: Training loss at epoch 28: 1.2004474103450775
[08/28/2025 11:25:41 INFO]: Training stats: {
    "score": -1.01313774522765,
    "rmse": 1.01313774522765
}
[08/28/2025 11:25:41 INFO]: Val stats: {
    "score": -0.6532106393936584,
    "rmse": 0.6532106393936584
}
[08/28/2025 11:25:41 INFO]: Test stats: {
    "score": -0.8751113273032103,
    "rmse": 0.8751113273032103
}
[08/28/2025 11:25:58 INFO]: New best epoch, val score: -0.6532106393936584
[08/28/2025 11:25:58 INFO]: Saving model to: unhanged-Shanesha_trial_147/model_best.pth
[08/28/2025 11:26:08 INFO]: Training loss at epoch 86: 0.9899373352527618
[08/28/2025 11:26:08 INFO]: Training loss at epoch 22: 0.9530144929885864
[08/28/2025 11:26:24 INFO]: Training loss at epoch 10: 0.9687240421772003
[08/28/2025 11:26:26 INFO]: Training loss at epoch 12: 0.9679220020771027
[08/28/2025 11:26:33 INFO]: Training loss at epoch 6: 1.2104735374450684
[08/28/2025 11:26:38 INFO]: Training loss at epoch 28: 0.9716351330280304
[08/28/2025 11:26:45 INFO]: Training loss at epoch 23: 1.0962138772010803
[08/28/2025 11:27:02 INFO]: Training loss at epoch 31: 0.8813349008560181
[08/28/2025 11:27:14 INFO]: Training loss at epoch 23: 1.152901828289032
[08/28/2025 11:27:18 INFO]: Training loss at epoch 23: 0.8037162125110626
[08/28/2025 11:27:22 INFO]: Training loss at epoch 82: 1.0620115399360657
[08/28/2025 11:27:35 INFO]: Training loss at epoch 15: 1.1624326705932617
[08/28/2025 11:27:38 INFO]: Training loss at epoch 87: 0.9520494341850281
[08/28/2025 11:27:49 INFO]: Training loss at epoch 29: 0.9767825603485107
[08/28/2025 11:27:56 INFO]: Training loss at epoch 24: 0.997051864862442
[08/28/2025 11:28:10 INFO]: Training loss at epoch 21: 1.1490632891654968
[08/28/2025 11:28:12 INFO]: Training loss at epoch 20: 0.9888486266136169
[08/28/2025 11:28:28 INFO]: Training loss at epoch 24: 0.8233128786087036
[08/28/2025 11:28:29 INFO]: New best epoch, val score: -0.6531060282165607
[08/28/2025 11:28:29 INFO]: Saving model to: unhanged-Shanesha_trial_147/model_best.pth
[08/28/2025 11:28:46 INFO]: Training loss at epoch 11: 0.9482203722000122
[08/28/2025 11:28:51 INFO]: Training loss at epoch 29: 0.9428046643733978
[08/28/2025 11:28:54 INFO]: Training loss at epoch 7: 0.8630112409591675
[08/28/2025 11:28:57 INFO]: Training loss at epoch 23: 0.8920486271381378
[08/28/2025 11:29:01 INFO]: Training stats: {
    "score": -1.0003798212439508,
    "rmse": 1.0003798212439508
}
[08/28/2025 11:29:01 INFO]: Val stats: {
    "score": -0.6709294541165627,
    "rmse": 0.6709294541165627
}
[08/28/2025 11:29:01 INFO]: Test stats: {
    "score": -0.8700035663759038,
    "rmse": 0.8700035663759038
}
[08/28/2025 11:29:03 INFO]: New best epoch, val score: -0.6582346386285726
[08/28/2025 11:29:03 INFO]: Saving model to: unhanged-Shanesha_trial_154/model_best.pth
[08/28/2025 11:29:07 INFO]: Training loss at epoch 22: 0.9713358283042908
[08/28/2025 11:29:08 INFO]: Training loss at epoch 25: 0.9530394971370697
[08/28/2025 11:29:12 INFO]: Training loss at epoch 88: 0.9761582016944885
[08/28/2025 11:29:14 INFO]: Training loss at epoch 83: 1.1515893936157227
[08/28/2025 11:29:23 INFO]: Training loss at epoch 13: 1.0655496716499329
[08/28/2025 11:29:42 INFO]: Training loss at epoch 25: 1.2452866435050964
[08/28/2025 11:29:51 INFO]: Training loss at epoch 5: 0.9722780883312225
[08/28/2025 11:29:59 INFO]: Training loss at epoch 32: 1.0578150749206543
[08/28/2025 11:30:02 INFO]: Training loss at epoch 29: 0.911888599395752
[08/28/2025 11:30:02 INFO]: Training stats: {
    "score": -0.9930060053519925,
    "rmse": 0.9930060053519925
}
[08/28/2025 11:30:02 INFO]: Val stats: {
    "score": -0.6857841676691783,
    "rmse": 0.6857841676691783
}
[08/28/2025 11:30:02 INFO]: Test stats: {
    "score": -0.8712272589332841,
    "rmse": 0.8712272589332841
}
[08/28/2025 11:30:10 INFO]: Training loss at epoch 5: 0.8449974656105042
[08/28/2025 11:30:13 INFO]: Training loss at epoch 24: 1.230706810951233
[08/28/2025 11:30:24 INFO]: Training loss at epoch 26: 0.8651379346847534
[08/28/2025 11:30:48 INFO]: Training loss at epoch 16: 1.0913098454475403
[08/28/2025 11:30:50 INFO]: Training loss at epoch 59: 0.8136378526687622
[08/28/2025 11:30:53 INFO]: Training loss at epoch 89: 0.8508714139461517
[08/28/2025 11:30:56 INFO]: Training loss at epoch 21: 0.8361876606941223
[08/28/2025 11:31:00 INFO]: Training loss at epoch 26: 0.9234976172447205
[08/28/2025 11:31:15 INFO]: Training loss at epoch 84: 0.908449649810791
[08/28/2025 11:31:16 INFO]: New best epoch, val score: -0.6530502380681404
[08/28/2025 11:31:16 INFO]: Saving model to: unhanged-Shanesha_trial_147/model_best.pth
[08/28/2025 11:31:17 INFO]: Training stats: {
    "score": -1.0044887664133924,
    "rmse": 1.0044887664133924
}
[08/28/2025 11:31:17 INFO]: Val stats: {
    "score": -0.7124481439002726,
    "rmse": 0.7124481439002726
}
[08/28/2025 11:31:17 INFO]: Test stats: {
    "score": -0.8919825121019319,
    "rmse": 0.8919825121019319
}
[08/28/2025 11:31:21 INFO]: Training loss at epoch 12: 1.0182512402534485
[08/28/2025 11:31:26 INFO]: Training stats: {
    "score": -0.9708689553269654,
    "rmse": 0.9708689553269654
}
[08/28/2025 11:31:26 INFO]: Val stats: {
    "score": -0.6727362958932968,
    "rmse": 0.6727362958932968
}
[08/28/2025 11:31:26 INFO]: Test stats: {
    "score": -0.8728886837622324,
    "rmse": 0.8728886837622324
}
[08/28/2025 11:31:28 INFO]: Training loss at epoch 8: 0.9943373799324036
[08/28/2025 11:31:40 INFO]: Training loss at epoch 27: 1.0491379499435425
[08/28/2025 11:31:46 INFO]: Training loss at epoch 22: 1.1149657368659973
[08/28/2025 11:32:17 INFO]: Training loss at epoch 27: 1.0961626768112183
[08/28/2025 11:32:35 INFO]: Training loss at epoch 24: 1.0611870288848877
[08/28/2025 11:32:36 INFO]: Training loss at epoch 14: 0.9821331202983856
[08/28/2025 11:32:50 INFO]: Training loss at epoch 30: 0.8487380146980286
[08/28/2025 11:32:56 INFO]: Training loss at epoch 23: 0.9722869396209717
[08/28/2025 11:32:56 INFO]: Training loss at epoch 28: 0.9146838188171387
[08/28/2025 11:32:58 INFO]: Training stats: {
    "score": -0.9954789877596838,
    "rmse": 0.9954789877596838
}
[08/28/2025 11:32:58 INFO]: Val stats: {
    "score": -0.6673735722869398,
    "rmse": 0.6673735722869398
}
[08/28/2025 11:32:58 INFO]: Test stats: {
    "score": -0.8695445420714736,
    "rmse": 0.8695445420714736
}
[08/28/2025 11:33:07 INFO]: Training loss at epoch 90: 1.067070186138153
[08/28/2025 11:33:08 INFO]: Training loss at epoch 33: 1.170000433921814
[08/28/2025 11:33:17 INFO]: Training loss at epoch 85: 1.410994052886963
[08/28/2025 11:33:21 INFO]: Training loss at epoch 25: 1.2352618277072906
[08/28/2025 11:33:31 INFO]: Training loss at epoch 30: 0.9918577373027802
[08/28/2025 11:33:33 INFO]: Training loss at epoch 28: 1.0235328078269958
[08/28/2025 11:33:42 INFO]: Training loss at epoch 22: 0.9317149221897125
[08/28/2025 11:33:54 INFO]: Training loss at epoch 13: 0.936725378036499
[08/28/2025 11:33:58 INFO]: Training loss at epoch 9: 0.9696216881275177
[08/28/2025 11:34:06 INFO]: Training loss at epoch 17: 1.1053271889686584
[08/28/2025 11:34:10 INFO]: Training loss at epoch 29: 1.0889089703559875
[08/28/2025 11:34:35 INFO]: Training stats: {
    "score": -0.9990580781745196,
    "rmse": 0.9990580781745196
}
[08/28/2025 11:34:35 INFO]: Val stats: {
    "score": -0.6628561961684901,
    "rmse": 0.6628561961684901
}
[08/28/2025 11:34:35 INFO]: Test stats: {
    "score": -0.8749955801708486,
    "rmse": 0.8749955801708486
}
[08/28/2025 11:34:42 INFO]: Training loss at epoch 91: 1.0850074291229248
[08/28/2025 11:34:45 INFO]: Training loss at epoch 29: 1.0239509344100952
[08/28/2025 11:34:46 INFO]: Training stats: {
    "score": -0.9982784881892646,
    "rmse": 0.9982784881892646
}
[08/28/2025 11:34:46 INFO]: Val stats: {
    "score": -0.6884088175144383,
    "rmse": 0.6884088175144383
}
[08/28/2025 11:34:46 INFO]: Test stats: {
    "score": -0.8788724586060666,
    "rmse": 0.8788724586060666
}
[08/28/2025 11:34:50 INFO]: Training loss at epoch 30: 0.9405734837055206
[08/28/2025 11:35:09 INFO]: Training loss at epoch 86: 1.0609241127967834
[08/28/2025 11:35:10 INFO]: Training stats: {
    "score": -0.9945655022758223,
    "rmse": 0.9945655022758223
}
[08/28/2025 11:35:10 INFO]: Val stats: {
    "score": -0.6777973954522122,
    "rmse": 0.6777973954522122
}
[08/28/2025 11:35:10 INFO]: Test stats: {
    "score": -0.8825681512552072,
    "rmse": 0.8825681512552072
}
[08/28/2025 11:35:20 INFO]: Training loss at epoch 23: 1.0802395939826965
[08/28/2025 11:35:36 INFO]: Training loss at epoch 15: 1.1135885119438171
[08/28/2025 11:35:44 INFO]: Training loss at epoch 30: 1.0860615372657776
[08/28/2025 11:36:00 INFO]: Training loss at epoch 25: 1.0389201045036316
[08/28/2025 11:36:00 INFO]: Training loss at epoch 34: 0.9327259063720703
[08/28/2025 11:36:13 INFO]: Training loss at epoch 92: 0.9280603528022766
[08/28/2025 11:36:14 INFO]: Training loss at epoch 23: 0.9230095446109772
[08/28/2025 11:36:15 INFO]: Training loss at epoch 26: 0.8663198351860046
[08/28/2025 11:36:15 INFO]: Training loss at epoch 14: 0.9498745799064636
[08/28/2025 11:36:19 INFO]: Training loss at epoch 30: 1.1270580291748047
[08/28/2025 11:36:20 INFO]: Training loss at epoch 31: 1.1779592037200928
[08/28/2025 11:36:27 INFO]: Training loss at epoch 24: 1.0967249870300293
[08/28/2025 11:36:42 INFO]: Training loss at epoch 31: 0.9404170215129852
[08/28/2025 11:36:53 INFO]: Training loss at epoch 31: 1.2219762802124023
[08/28/2025 11:36:59 INFO]: Training loss at epoch 87: 1.0425758957862854
[08/28/2025 11:37:03 INFO]: Training loss at epoch 10: 0.9628896117210388
[08/28/2025 11:37:06 INFO]: Training loss at epoch 18: 1.1049871146678925
[08/28/2025 11:37:31 INFO]: Training loss at epoch 31: 1.0537912845611572
[08/28/2025 11:37:47 INFO]: Training loss at epoch 93: 0.9018656611442566
[08/28/2025 11:38:07 INFO]: Training loss at epoch 32: 0.9250575006008148
[08/28/2025 11:38:11 INFO]: Training loss at epoch 31: 1.1706597208976746
[08/28/2025 11:38:29 INFO]: Training loss at epoch 6: 1.3613548874855042
[08/28/2025 11:38:33 INFO]: Training loss at epoch 6: 0.9922949969768524
[08/28/2025 11:38:37 INFO]: Training loss at epoch 16: 0.8678768873214722
[08/28/2025 11:38:43 INFO]: Training loss at epoch 15: 1.1974839568138123
[08/28/2025 11:38:45 INFO]: Training loss at epoch 32: 1.2551237344741821
[08/28/2025 11:38:45 INFO]: Training loss at epoch 24: 0.9193307161331177
[08/28/2025 11:38:47 INFO]: Training loss at epoch 60: 1.0973502695560455
[08/28/2025 11:38:53 INFO]: Training loss at epoch 24: 1.0957778692245483
[08/28/2025 11:38:55 INFO]: Running Final Evaluation...
[08/28/2025 11:38:57 INFO]: Training loss at epoch 88: 0.9514554738998413
[08/28/2025 11:38:58 INFO]: Training loss at epoch 35: 1.4240547716617584
[08/28/2025 11:39:16 INFO]: Training loss at epoch 27: 0.9324319958686829
[08/28/2025 11:39:21 INFO]: Training loss at epoch 33: 0.9901241660118103
[08/28/2025 11:39:27 INFO]: Training loss at epoch 94: 0.9342929124832153
[08/28/2025 11:39:28 INFO]: Training accuracy: {
    "score": -1.0124012518962395,
    "rmse": 1.0124012518962395
}
[08/28/2025 11:39:28 INFO]: Val accuracy: {
    "score": -0.6638028572646996,
    "rmse": 0.6638028572646996
}
[08/28/2025 11:39:28 INFO]: Test accuracy: {
    "score": -0.8830862113544334,
    "rmse": 0.8830862113544334
}
[08/28/2025 11:39:29 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_153",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8830862113544334,
        "rmse": 0.8830862113544334
    },
    "train_stats": {
        "score": -1.0124012518962395,
        "rmse": 1.0124012518962395
    },
    "val_stats": {
        "score": -0.6638028572646996,
        "rmse": 0.6638028572646996
    }
}
[08/28/2025 11:39:29 INFO]: Procewss finished for trial unhanged-Shanesha_trial_153
[08/28/2025 11:39:29 INFO]: 
_________________________________________________

[08/28/2025 11:39:29 INFO]: train_net_for_optune.py main() running.
[08/28/2025 11:39:29 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 2.392630058224587
  attention_dropout: 0.25400644908390746
  ffn_dropout: 0.25400644908390746
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.4308451739146005e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_156

[08/28/2025 11:39:29 INFO]: This ft_transformer has 5.324 million parameters.
[08/28/2025 11:39:29 INFO]: Training will start at epoch 0.
[08/28/2025 11:39:29 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 11:39:31 INFO]: Training loss at epoch 26: 0.8707849979400635
[08/28/2025 11:39:34 INFO]: Training loss at epoch 11: 1.1758275628089905
[08/28/2025 11:40:00 INFO]: Training loss at epoch 32: 1.0943717062473297
[08/28/2025 11:40:06 INFO]: Training loss at epoch 32: 0.8811942338943481
[08/28/2025 11:40:08 INFO]: Training loss at epoch 25: 0.9430102705955505
[08/28/2025 11:40:24 INFO]: Training loss at epoch 19: 1.1670051515102386
[08/28/2025 11:40:37 INFO]: Training loss at epoch 34: 1.0270253419876099
[08/28/2025 11:40:58 INFO]: Training loss at epoch 89: 0.9715810418128967
[08/28/2025 11:41:07 INFO]: Training loss at epoch 95: 0.9605648517608643
[08/28/2025 11:41:18 INFO]: Training loss at epoch 16: 1.0283531546592712
[08/28/2025 11:41:32 INFO]: Training stats: {
    "score": -0.9850744891111974,
    "rmse": 0.9850744891111974
}
[08/28/2025 11:41:32 INFO]: Val stats: {
    "score": -0.6753158806485849,
    "rmse": 0.6753158806485849
}
[08/28/2025 11:41:32 INFO]: Test stats: {
    "score": -0.8919786284470353,
    "rmse": 0.8919786284470353
}
[08/28/2025 11:41:37 INFO]: New best epoch, val score: -0.6571325906371238
[08/28/2025 11:41:37 INFO]: Saving model to: unhanged-Shanesha_trial_154/model_best.pth
[08/28/2025 11:41:40 INFO]: Training stats: {
    "score": -0.9952620679273058,
    "rmse": 0.9952620679273058
}
[08/28/2025 11:41:40 INFO]: Val stats: {
    "score": -0.6751375577487077,
    "rmse": 0.6751375577487077
}
[08/28/2025 11:41:40 INFO]: Test stats: {
    "score": -0.8735843892532263,
    "rmse": 0.8735843892532263
}
[08/28/2025 11:41:40 INFO]: Training loss at epoch 25: 1.0325067043304443
[08/28/2025 11:41:45 INFO]: Training loss at epoch 0: 1.226070761680603
[08/28/2025 11:41:49 INFO]: Training loss at epoch 17: 1.1480956673622131
[08/28/2025 11:41:50 INFO]: Training loss at epoch 32: 0.9306044578552246
[08/28/2025 11:41:52 INFO]: Training loss at epoch 35: 0.9767705202102661
[08/28/2025 11:42:04 INFO]: New best epoch, val score: -0.6745949755570764
[08/28/2025 11:42:04 INFO]: Saving model to: unhanged-Shanesha_trial_156/model_best.pth
[08/28/2025 11:42:06 INFO]: Training loss at epoch 36: 1.32153981924057
[08/28/2025 11:42:07 INFO]: Training loss at epoch 12: 1.0365012288093567
[08/28/2025 11:42:25 INFO]: Training loss at epoch 28: 0.9739618301391602
[08/28/2025 11:42:28 INFO]: Training loss at epoch 25: 1.0875503420829773
[08/28/2025 11:42:46 INFO]: Training loss at epoch 96: 0.8901881277561188
[08/28/2025 11:43:07 INFO]: Training loss at epoch 36: 0.980317622423172
[08/28/2025 11:43:09 INFO]: Training loss at epoch 27: 0.9540978670120239
[08/28/2025 11:43:32 INFO]: Training loss at epoch 33: 0.8061528503894806
[08/28/2025 11:43:37 INFO]: Training loss at epoch 90: 1.1307506561279297
[08/28/2025 11:43:43 INFO]: Training loss at epoch 33: 1.0712018609046936
[08/28/2025 11:43:49 INFO]: Training loss at epoch 17: 1.0624200701713562
[08/28/2025 11:43:51 INFO]: Training loss at epoch 26: 1.1722644567489624
[08/28/2025 11:44:11 INFO]: Training loss at epoch 1: 1.0425851047039032
[08/28/2025 11:44:16 INFO]: Training loss at epoch 37: 0.9780831634998322
[08/28/2025 11:44:19 INFO]: Training loss at epoch 97: 0.9602493643760681
[08/28/2025 11:44:20 INFO]: Training loss at epoch 26: 0.8821947574615479
[08/28/2025 11:44:30 INFO]: Training loss at epoch 13: 0.981434553861618
[08/28/2025 11:44:39 INFO]: Training loss at epoch 20: 1.1041167378425598
[08/28/2025 11:44:49 INFO]: Training loss at epoch 18: 1.2675898671150208
[08/28/2025 11:44:52 INFO]: Training loss at epoch 61: 0.9957299530506134
[08/28/2025 11:44:59 INFO]: Training loss at epoch 37: 0.9848868250846863
[08/28/2025 11:45:13 INFO]: Training loss at epoch 33: 1.2418941259384155
[08/28/2025 11:45:18 INFO]: Training loss at epoch 29: 1.1255522966384888
[08/28/2025 11:45:25 INFO]: Training loss at epoch 38: 1.1841706037521362
[08/28/2025 11:45:27 INFO]: Training loss at epoch 91: 0.9243061542510986
[08/28/2025 11:45:48 INFO]: Training loss at epoch 26: 1.0200484097003937
[08/28/2025 11:45:49 INFO]: Training loss at epoch 98: 1.014929085969925
[08/28/2025 11:46:11 INFO]: Training loss at epoch 18: 1.077072560787201
[08/28/2025 11:46:16 INFO]: Training stats: {
    "score": -0.9941189508174774,
    "rmse": 0.9941189508174774
}
[08/28/2025 11:46:16 INFO]: Val stats: {
    "score": -0.6762212884702328,
    "rmse": 0.6762212884702328
}
[08/28/2025 11:46:16 INFO]: Test stats: {
    "score": -0.874786142921838,
    "rmse": 0.874786142921838
}
[08/28/2025 11:46:26 INFO]: Training loss at epoch 28: 1.1500980854034424
[08/28/2025 11:46:29 INFO]: Training loss at epoch 2: 0.9718533754348755
[08/28/2025 11:46:34 INFO]: Training loss at epoch 39: 0.8954647779464722
[08/28/2025 11:46:39 INFO]: Training loss at epoch 34: 1.1559481620788574
[08/28/2025 11:46:47 INFO]: Training loss at epoch 14: 1.1279442310333252
[08/28/2025 11:46:53 INFO]: Training loss at epoch 27: 1.1145328283309937
[08/28/2025 11:46:54 INFO]: Training loss at epoch 7: 0.9410973489284515
[08/28/2025 11:46:58 INFO]: Training stats: {
    "score": -1.006820428150256,
    "rmse": 1.006820428150256
}
[08/28/2025 11:46:58 INFO]: Val stats: {
    "score": -0.6568621243644678,
    "rmse": 0.6568621243644678
}
[08/28/2025 11:46:58 INFO]: Test stats: {
    "score": -0.8759506683860508,
    "rmse": 0.8759506683860508
}
[08/28/2025 11:47:04 INFO]: Training loss at epoch 7: 0.922161340713501
[08/28/2025 11:47:08 INFO]: Training loss at epoch 34: 0.8554074168205261
[08/28/2025 11:47:18 INFO]: Training loss at epoch 27: 1.1061695218086243
[08/28/2025 11:47:20 INFO]: Training loss at epoch 92: 1.032166838645935
[08/28/2025 11:47:22 INFO]: Training loss at epoch 99: 1.1085413098335266
[08/28/2025 11:47:35 INFO]: Running Final Evaluation...
[08/28/2025 11:47:43 INFO]: Training loss at epoch 21: 0.87026447057724
[08/28/2025 11:47:47 INFO]: Training loss at epoch 19: 0.9742723703384399
[08/28/2025 11:47:54 INFO]: Training loss at epoch 38: 1.0278488397598267
[08/28/2025 11:47:56 INFO]: Training stats: {
    "score": -0.9651594684257533,
    "rmse": 0.9651594684257533
}
[08/28/2025 11:47:56 INFO]: Val stats: {
    "score": -0.6793389650987244,
    "rmse": 0.6793389650987244
}
[08/28/2025 11:47:56 INFO]: Test stats: {
    "score": -0.875332162609022,
    "rmse": 0.875332162609022
}
[08/28/2025 11:48:16 INFO]: Training loss at epoch 40: 1.2781505584716797
[08/28/2025 11:48:26 INFO]: Training accuracy: {
    "score": -1.003682191605719,
    "rmse": 1.003682191605719
}
[08/28/2025 11:48:26 INFO]: Val accuracy: {
    "score": -0.666406895194814,
    "rmse": 0.666406895194814
}
[08/28/2025 11:48:26 INFO]: Test accuracy: {
    "score": -0.872379318886505,
    "rmse": 0.872379318886505
}
[08/28/2025 11:48:26 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_131",
    "best_epoch": 61,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.872379318886505,
        "rmse": 0.872379318886505
    },
    "train_stats": {
        "score": -1.003682191605719,
        "rmse": 1.003682191605719
    },
    "val_stats": {
        "score": -0.666406895194814,
        "rmse": 0.666406895194814
    }
}
[08/28/2025 11:48:26 INFO]: Procewss finished for trial unhanged-Shanesha_trial_131
[08/28/2025 11:48:26 INFO]: 
_________________________________________________

[08/28/2025 11:48:26 INFO]: train_net_for_optune.py main() running.
[08/28/2025 11:48:26 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 2.347822940512409
  attention_dropout: 0.2545295702154508
  ffn_dropout: 0.2545295702154508
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.402902492828138e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_157

[08/28/2025 11:48:26 INFO]: This ft_transformer has 5.272 million parameters.
[08/28/2025 11:48:26 INFO]: Training will start at epoch 0.
[08/28/2025 11:48:26 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 11:48:40 INFO]: Training loss at epoch 34: 1.310076355934143
[08/28/2025 11:48:43 INFO]: Training loss at epoch 19: 1.0435563325881958
[08/28/2025 11:48:55 INFO]: Training stats: {
    "score": -1.0387647264134532,
    "rmse": 1.0387647264134532
}
[08/28/2025 11:48:55 INFO]: Val stats: {
    "score": -0.8085390227931016,
    "rmse": 0.8085390227931016
}
[08/28/2025 11:48:55 INFO]: Test stats: {
    "score": -0.9499447579387935,
    "rmse": 0.9499447579387935
}
[08/28/2025 11:48:59 INFO]: Training loss at epoch 3: 1.196745216846466
[08/28/2025 11:49:20 INFO]: Training loss at epoch 15: 1.2646182775497437
[08/28/2025 11:49:21 INFO]: Training loss at epoch 30: 0.9730354845523834
[08/28/2025 11:49:21 INFO]: Training loss at epoch 27: 1.0178494155406952
[08/28/2025 11:49:32 INFO]: Training loss at epoch 41: 1.0177326202392578
[08/28/2025 11:49:36 INFO]: Training stats: {
    "score": -1.0055267989056782,
    "rmse": 1.0055267989056782
}
[08/28/2025 11:49:36 INFO]: Val stats: {
    "score": -0.7161166619661172,
    "rmse": 0.7161166619661172
}
[08/28/2025 11:49:36 INFO]: Test stats: {
    "score": -0.9011526413607045,
    "rmse": 0.9011526413607045
}
[08/28/2025 11:49:37 INFO]: Training loss at epoch 100: 1.0724491477012634
[08/28/2025 11:49:41 INFO]: Training loss at epoch 28: 1.0005848109722137
[08/28/2025 11:50:03 INFO]: Training loss at epoch 29: 1.1235424876213074
[08/28/2025 11:50:08 INFO]: Training loss at epoch 35: 0.9925741255283356
[08/28/2025 11:50:40 INFO]: Training loss at epoch 0: 1.3120639324188232
[08/28/2025 11:50:47 INFO]: Training loss at epoch 42: 1.08333420753479
[08/28/2025 11:50:53 INFO]: Training loss at epoch 62: 1.1912647187709808
[08/28/2025 11:50:55 INFO]: Training loss at epoch 35: 0.791333019733429
[08/28/2025 11:50:58 INFO]: New best epoch, val score: -0.7249891947027027
[08/28/2025 11:50:58 INFO]: Saving model to: unhanged-Shanesha_trial_157/model_best.pth
[08/28/2025 11:51:02 INFO]: Training loss at epoch 39: 0.7964503467082977
[08/28/2025 11:51:04 INFO]: Training loss at epoch 22: 1.024476706981659
[08/28/2025 11:51:07 INFO]: Training loss at epoch 28: 1.1129838824272156
[08/28/2025 11:51:17 INFO]: Training loss at epoch 101: 0.8185352683067322
[08/28/2025 11:51:18 INFO]: Training stats: {
    "score": -1.0027855832133643,
    "rmse": 1.0027855832133643
}
[08/28/2025 11:51:18 INFO]: Val stats: {
    "score": -0.7386064279940474,
    "rmse": 0.7386064279940474
}
[08/28/2025 11:51:18 INFO]: Test stats: {
    "score": -0.9025902448066121,
    "rmse": 0.9025902448066121
}
[08/28/2025 11:51:33 INFO]: Training loss at epoch 4: 1.3418755531311035
[08/28/2025 11:51:53 INFO]: Training loss at epoch 16: 1.0531178712844849
[08/28/2025 11:52:02 INFO]: Training loss at epoch 43: 0.8862833678722382
[08/28/2025 11:52:06 INFO]: Training stats: {
    "score": -1.0002605821647843,
    "rmse": 1.0002605821647843
}
[08/28/2025 11:52:06 INFO]: Val stats: {
    "score": -0.6760578964000369,
    "rmse": 0.6760578964000369
}
[08/28/2025 11:52:06 INFO]: Test stats: {
    "score": -0.8648704054777272,
    "rmse": 0.8648704054777272
}
[08/28/2025 11:52:07 INFO]: Training loss at epoch 20: 0.814635694026947
[08/28/2025 11:52:10 INFO]: Training loss at epoch 20: 0.987097442150116
[08/28/2025 11:52:17 INFO]: Training loss at epoch 35: 0.907629519701004
[08/28/2025 11:52:25 INFO]: Training loss at epoch 29: 1.0230923891067505
[08/28/2025 11:52:26 INFO]: Training loss at epoch 31: 1.0483662486076355
[08/28/2025 11:52:26 INFO]: Running Final Evaluation...
[08/28/2025 11:52:47 INFO]: Running Final Evaluation...
[08/28/2025 11:52:51 INFO]: Training loss at epoch 102: 0.8860911130905151
[08/28/2025 11:52:54 INFO]: Training loss at epoch 28: 0.891565203666687
[08/28/2025 11:53:04 INFO]: Training loss at epoch 1: 1.0037076771259308
[08/28/2025 11:53:11 INFO]: Training loss at epoch 44: 1.056188941001892
[08/28/2025 11:53:16 INFO]: Training stats: {
    "score": -1.0000658560472355,
    "rmse": 1.0000658560472355
}
[08/28/2025 11:53:16 INFO]: Val stats: {
    "score": -0.6879697294997672,
    "rmse": 0.6879697294997672
}
[08/28/2025 11:53:16 INFO]: Test stats: {
    "score": -0.886072316475655,
    "rmse": 0.886072316475655
}
[08/28/2025 11:53:20 INFO]: New best epoch, val score: -0.6928892463286223
[08/28/2025 11:53:20 INFO]: Saving model to: unhanged-Shanesha_trial_157/model_best.pth
[08/28/2025 11:53:27 INFO]: Training loss at epoch 36: 1.0873565077781677
[08/28/2025 11:53:37 INFO]: Training accuracy: {
    "score": -1.0155576496787535,
    "rmse": 1.0155576496787535
}
[08/28/2025 11:53:37 INFO]: Val accuracy: {
    "score": -0.6717863079436707,
    "rmse": 0.6717863079436707
}
[08/28/2025 11:53:37 INFO]: Test accuracy: {
    "score": -0.8672316614689048,
    "rmse": 0.8672316614689048
}
[08/28/2025 11:53:38 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_141",
    "best_epoch": 8,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8672316614689048,
        "rmse": 0.8672316614689048
    },
    "train_stats": {
        "score": -1.0155576496787535,
        "rmse": 1.0155576496787535
    },
    "val_stats": {
        "score": -0.6717863079436707,
        "rmse": 0.6717863079436707
    }
}
[08/28/2025 11:53:38 INFO]: Procewss finished for trial unhanged-Shanesha_trial_141
[08/28/2025 11:53:38 INFO]: 
_________________________________________________

[08/28/2025 11:53:38 INFO]: train_net_for_optune.py main() running.
[08/28/2025 11:53:38 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 2.53143631388361
  attention_dropout: 0.2935709690265726
  ffn_dropout: 0.2935709690265726
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.4367752340647956e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_158

[08/28/2025 11:53:38 INFO]: This ft_transformer has 0.173 million parameters.
[08/28/2025 11:53:38 INFO]: Training will start at epoch 0.
[08/28/2025 11:53:38 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 11:53:54 INFO]: Training loss at epoch 5: 0.9407701194286346
[08/28/2025 11:53:55 INFO]: Training loss at epoch 0: 1.3222618699073792
[08/28/2025 11:53:57 INFO]: New best epoch, val score: -0.8886886371185161
[08/28/2025 11:53:57 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 11:54:02 INFO]: Training accuracy: {
    "score": -1.002449749752358,
    "rmse": 1.002449749752358
}
[08/28/2025 11:54:02 INFO]: Val accuracy: {
    "score": -0.6619105531217307,
    "rmse": 0.6619105531217307
}
[08/28/2025 11:54:02 INFO]: Test accuracy: {
    "score": -0.8704744935269574,
    "rmse": 0.8704744935269574
}
[08/28/2025 11:54:02 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_146",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8704744935269574,
        "rmse": 0.8704744935269574
    },
    "train_stats": {
        "score": -1.002449749752358,
        "rmse": 1.002449749752358
    },
    "val_stats": {
        "score": -0.6619105531217307,
        "rmse": 0.6619105531217307
    }
}
[08/28/2025 11:54:02 INFO]: Procewss finished for trial unhanged-Shanesha_trial_146
[08/28/2025 11:54:02 INFO]: 
_________________________________________________

[08/28/2025 11:54:02 INFO]: train_net_for_optune.py main() running.
[08/28/2025 11:54:02 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 2.5282751582901533
  attention_dropout: 0.2544460192585266
  ffn_dropout: 0.2544460192585266
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 4.1012206119289097e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_159

[08/28/2025 11:54:02 INFO]: This ft_transformer has 0.172 million parameters.
[08/28/2025 11:54:02 INFO]: Training will start at epoch 0.
[08/28/2025 11:54:02 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 11:54:10 INFO]: Training loss at epoch 23: 0.9505385160446167
[08/28/2025 11:54:11 INFO]: Training loss at epoch 17: 1.1092880368232727
[08/28/2025 11:54:14 INFO]: Training loss at epoch 1: 1.2498274445533752
[08/28/2025 11:54:17 INFO]: New best epoch, val score: -0.8589398357178233
[08/28/2025 11:54:17 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 11:54:19 INFO]: Training loss at epoch 0: 1.0142671465873718
[08/28/2025 11:54:21 INFO]: Training loss at epoch 45: 0.8863793015480042
[08/28/2025 11:54:22 INFO]: New best epoch, val score: -0.663703296608934
[08/28/2025 11:54:22 INFO]: Saving model to: unhanged-Shanesha_trial_159/model_best.pth
[08/28/2025 11:54:22 INFO]: Training loss at epoch 103: 0.9902796745300293
[08/28/2025 11:54:26 INFO]: Training loss at epoch 36: 0.9784515798091888
[08/28/2025 11:54:31 INFO]: Training loss at epoch 21: 0.9243188798427582
[08/28/2025 11:54:34 INFO]: Training loss at epoch 2: 1.305575966835022
[08/28/2025 11:54:36 INFO]: New best epoch, val score: -0.8173108639272867
[08/28/2025 11:54:36 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 11:54:37 INFO]: Training loss at epoch 29: 1.0135407745838165
[08/28/2025 11:54:39 INFO]: Training loss at epoch 1: 0.9238901138305664
[08/28/2025 11:54:39 INFO]: Training loss at epoch 30: 0.9247877895832062
[08/28/2025 11:54:53 INFO]: Training loss at epoch 3: 1.3102105259895325
[08/28/2025 11:54:55 INFO]: New best epoch, val score: -0.768088632459507
[08/28/2025 11:54:55 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 11:54:58 INFO]: Training loss at epoch 2: 1.2435559034347534
[08/28/2025 11:55:00 INFO]: Training loss at epoch 21: 1.024631291627884
[08/28/2025 11:55:12 INFO]: Training loss at epoch 4: 0.9087535738945007
[08/28/2025 11:55:14 INFO]: New best epoch, val score: -0.7225089499605291
[08/28/2025 11:55:14 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 11:55:16 INFO]: Training loss at epoch 3: 0.976233959197998
[08/28/2025 11:55:18 INFO]: Training loss at epoch 8: 1.1586813032627106
[08/28/2025 11:55:22 INFO]: Training loss at epoch 2: 1.3190650939941406
[08/28/2025 11:55:31 INFO]: Training loss at epoch 46: 0.9750392436981201
[08/28/2025 11:55:31 INFO]: Training loss at epoch 5: 0.8835000693798065
[08/28/2025 11:55:34 INFO]: New best epoch, val score: -0.7198805928274391
[08/28/2025 11:55:34 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 11:55:35 INFO]: Training loss at epoch 36: 0.9183652997016907
[08/28/2025 11:55:36 INFO]: Training loss at epoch 4: 1.150160551071167
[08/28/2025 11:55:45 INFO]: Training loss at epoch 8: 1.258286476135254
[08/28/2025 11:55:49 INFO]: Training loss at epoch 30: 0.893672525882721
[08/28/2025 11:55:50 INFO]: Training stats: {
    "score": -1.0004766747665848,
    "rmse": 1.0004766747665848
}
[08/28/2025 11:55:50 INFO]: Val stats: {
    "score": -0.7134340395003946,
    "rmse": 0.7134340395003946
}
[08/28/2025 11:55:50 INFO]: Test stats: {
    "score": -0.8894253668104024,
    "rmse": 0.8894253668104024
}
[08/28/2025 11:55:52 INFO]: Training loss at epoch 6: 1.2499691247940063
[08/28/2025 11:55:55 INFO]: New best epoch, val score: -0.7176812524069298
[08/28/2025 11:55:55 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 11:55:55 INFO]: Training loss at epoch 104: 1.1001054644584656
[08/28/2025 11:55:56 INFO]: Training loss at epoch 5: 1.0751912593841553
[08/28/2025 11:56:13 INFO]: Training loss at epoch 7: 1.0234110355377197
[08/28/2025 11:56:16 INFO]: New best epoch, val score: -0.7154627200192013
[08/28/2025 11:56:16 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 11:56:16 INFO]: Training loss at epoch 29: 1.13187974691391
[08/28/2025 11:56:17 INFO]: Training loss at epoch 6: 1.1096869111061096
[08/28/2025 11:56:18 INFO]: Training loss at epoch 6: 0.9517492949962616
[08/28/2025 11:56:34 INFO]: Training loss at epoch 8: 1.0173984169960022
[08/28/2025 11:56:36 INFO]: Training loss at epoch 18: 0.9634431004524231
[08/28/2025 11:56:36 INFO]: New best epoch, val score: -0.7132763622794726
[08/28/2025 11:56:36 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 11:56:39 INFO]: Training loss at epoch 7: 1.2387564778327942
[08/28/2025 11:56:44 INFO]: Training loss at epoch 63: 0.9677706360816956
[08/28/2025 11:56:44 INFO]: Training loss at epoch 37: 1.0694555640220642
[08/28/2025 11:56:47 INFO]: Training loss at epoch 47: 1.0063588619232178
[08/28/2025 11:56:54 INFO]: Training loss at epoch 9: 1.0412184000015259
[08/28/2025 11:56:57 INFO]: Running Final Evaluation...
[08/28/2025 11:57:00 INFO]: Training loss at epoch 8: 1.237851768732071
[08/28/2025 11:57:01 INFO]: Training loss at epoch 22: 1.1772528886795044
[08/28/2025 11:57:02 INFO]: Training stats: {
    "score": -1.0058278196835642,
    "rmse": 1.0058278196835642
}
[08/28/2025 11:57:02 INFO]: Val stats: {
    "score": -0.7112054161471469,
    "rmse": 0.7112054161471469
}
[08/28/2025 11:57:02 INFO]: Test stats: {
    "score": -0.8971489189324964,
    "rmse": 0.8971489189324964
}
[08/28/2025 11:57:05 INFO]: New best epoch, val score: -0.7112054161471469
[08/28/2025 11:57:05 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 11:57:21 INFO]: Training loss at epoch 9: 1.115592896938324
[08/28/2025 11:57:22 INFO]: Training loss at epoch 10: 0.9329096078872681
[08/28/2025 11:57:25 INFO]: Training loss at epoch 24: 0.9324674606323242
[08/28/2025 11:57:25 INFO]: New best epoch, val score: -0.7072966549294126
[08/28/2025 11:57:25 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 11:57:29 INFO]: Training stats: {
    "score": -1.0188028958571156,
    "rmse": 1.0188028958571156
}
[08/28/2025 11:57:29 INFO]: Val stats: {
    "score": -0.7514311296808756,
    "rmse": 0.7514311296808756
}
[08/28/2025 11:57:29 INFO]: Test stats: {
    "score": -0.9155762069573337,
    "rmse": 0.9155762069573337
}
[08/28/2025 11:57:30 INFO]: Training accuracy: {
    "score": -1.0117461546672484,
    "rmse": 1.0117461546672484
}
[08/28/2025 11:57:30 INFO]: Val accuracy: {
    "score": -0.6560491203832881,
    "rmse": 0.6560491203832881
}
[08/28/2025 11:57:30 INFO]: Test accuracy: {
    "score": -0.877176028676776,
    "rmse": 0.877176028676776
}
[08/28/2025 11:57:30 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_152",
    "best_epoch": 16,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.877176028676776,
        "rmse": 0.877176028676776
    },
    "train_stats": {
        "score": -1.0117461546672484,
        "rmse": 1.0117461546672484
    },
    "val_stats": {
        "score": -0.6560491203832881,
        "rmse": 0.6560491203832881
    }
}
[08/28/2025 11:57:30 INFO]: Procewss finished for trial unhanged-Shanesha_trial_152
[08/28/2025 11:57:30 INFO]: 
_________________________________________________

[08/28/2025 11:57:30 INFO]: train_net_for_optune.py main() running.
[08/28/2025 11:57:30 INFO]: model:
  name: ft_transformer
  d_embedding: 64
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 1.3099791163367556
  attention_dropout: 0.3404783950653019
  ffn_dropout: 0.3404783950653019
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.4913425975793682e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_160

[08/28/2025 11:57:30 INFO]: This ft_transformer has 0.352 million parameters.
[08/28/2025 11:57:30 INFO]: Training will start at epoch 0.
[08/28/2025 11:57:30 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 11:57:32 INFO]: Training stats: {
    "score": -1.0198455389366445,
    "rmse": 1.0198455389366445
}
[08/28/2025 11:57:32 INFO]: Val stats: {
    "score": -0.6901793197236793,
    "rmse": 0.6901793197236793
}
[08/28/2025 11:57:32 INFO]: Test stats: {
    "score": -0.8879121195598338,
    "rmse": 0.8879121195598338
}
[08/28/2025 11:57:36 INFO]: Training loss at epoch 105: 0.8792601525783539
[08/28/2025 11:57:43 INFO]: Training loss at epoch 11: 0.8987043797969818
[08/28/2025 11:57:46 INFO]: New best epoch, val score: -0.7039954312172388
[08/28/2025 11:57:46 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 11:57:50 INFO]: Training loss at epoch 10: 0.9088162481784821
[08/28/2025 11:57:56 INFO]: Training loss at epoch 3: 1.0038295686244965
[08/28/2025 11:58:04 INFO]: Training loss at epoch 12: 1.20064777135849
[08/28/2025 11:58:07 INFO]: New best epoch, val score: -0.7007956820398111
[08/28/2025 11:58:07 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 11:58:07 INFO]: Training loss at epoch 37: 0.9276596903800964
[08/28/2025 11:58:09 INFO]: Training loss at epoch 22: 1.0162083506584167
[08/28/2025 11:58:10 INFO]: Training loss at epoch 11: 1.1763269305229187
[08/28/2025 11:58:13 INFO]: Training loss at epoch 31: 1.044464349746704
[08/28/2025 11:58:23 INFO]: Training loss at epoch 0: 0.8758973777294159
[08/28/2025 11:58:25 INFO]: Training loss at epoch 13: 1.09162175655365
[08/28/2025 11:58:28 INFO]: New best epoch, val score: -0.6976953395792023
[08/28/2025 11:58:28 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 11:58:30 INFO]: New best epoch, val score: -0.7339117328777356
[08/28/2025 11:58:30 INFO]: Saving model to: unhanged-Shanesha_trial_160/model_best.pth
[08/28/2025 11:58:31 INFO]: Training loss at epoch 12: 0.9704787135124207
[08/28/2025 11:58:37 INFO]: Training loss at epoch 31: 1.1669496297836304
[08/28/2025 11:58:46 INFO]: Training loss at epoch 14: 1.018220692873001
[08/28/2025 11:58:49 INFO]: New best epoch, val score: -0.6948067409448575
[08/28/2025 11:58:49 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 11:58:52 INFO]: Training loss at epoch 7: 0.9994233250617981
[08/28/2025 11:58:52 INFO]: Training loss at epoch 13: 0.9557514786720276
[08/28/2025 11:59:07 INFO]: Training loss at epoch 15: 0.9536917805671692
[08/28/2025 11:59:10 INFO]: New best epoch, val score: -0.6919581028337985
[08/28/2025 11:59:10 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 11:59:10 INFO]: Training loss at epoch 19: 1.0406144857406616
[08/28/2025 11:59:12 INFO]: Training loss at epoch 14: 1.0973544716835022
[08/28/2025 11:59:14 INFO]: Training loss at epoch 37: 0.9775776267051697
[08/28/2025 11:59:17 INFO]: Training loss at epoch 106: 1.2159618735313416
[08/28/2025 11:59:23 INFO]: Training loss at epoch 1: 1.0489845275878906
[08/28/2025 11:59:28 INFO]: Training loss at epoch 16: 1.051088571548462
[08/28/2025 11:59:30 INFO]: New best epoch, val score: -0.6891106235455179
[08/28/2025 11:59:30 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 11:59:31 INFO]: New best epoch, val score: -0.7219141434062522
[08/28/2025 11:59:31 INFO]: Saving model to: unhanged-Shanesha_trial_160/model_best.pth
[08/28/2025 11:59:33 INFO]: Training loss at epoch 15: 0.886233776807785
[08/28/2025 11:59:37 INFO]: Training loss at epoch 23: 1.0116365849971771
[08/28/2025 11:59:39 INFO]: Training loss at epoch 30: 0.8358251750469208
[08/28/2025 11:59:48 INFO]: Training loss at epoch 17: 0.967911958694458
[08/28/2025 11:59:51 INFO]: New best epoch, val score: -0.6862968720892341
[08/28/2025 11:59:51 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 11:59:53 INFO]: Training loss at epoch 16: 0.9990932941436768
[08/28/2025 12:00:03 INFO]: Training stats: {
    "score": -1.054426806422461,
    "rmse": 1.054426806422461
}
[08/28/2025 12:00:03 INFO]: Val stats: {
    "score": -0.6897164169795336,
    "rmse": 0.6897164169795336
}
[08/28/2025 12:00:03 INFO]: Test stats: {
    "score": -0.9099726344939583,
    "rmse": 0.9099726344939583
}
[08/28/2025 12:00:09 INFO]: Training loss at epoch 18: 0.921707421541214
[08/28/2025 12:00:12 INFO]: New best epoch, val score: -0.6836940909011785
[08/28/2025 12:00:12 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 12:00:14 INFO]: Training loss at epoch 17: 1.083193600177765
[08/28/2025 12:00:18 INFO]: Training loss at epoch 38: 0.9554655849933624
[08/28/2025 12:00:24 INFO]: Training loss at epoch 2: 0.971670389175415
[08/28/2025 12:00:30 INFO]: Training loss at epoch 4: 1.038577377796173
[08/28/2025 12:00:31 INFO]: Training loss at epoch 19: 1.0825692415237427
[08/28/2025 12:00:31 INFO]: New best epoch, val score: -0.7145338811856353
[08/28/2025 12:00:31 INFO]: Saving model to: unhanged-Shanesha_trial_160/model_best.pth
[08/28/2025 12:00:35 INFO]: Training loss at epoch 18: 1.026404857635498
[08/28/2025 12:00:38 INFO]: Training stats: {
    "score": -1.0004605787395413,
    "rmse": 1.0004605787395413
}
[08/28/2025 12:00:38 INFO]: Val stats: {
    "score": -0.6811670691459281,
    "rmse": 0.6811670691459281
}
[08/28/2025 12:00:38 INFO]: Test stats: {
    "score": -0.8803701407398695,
    "rmse": 0.8803701407398695
}
[08/28/2025 12:00:40 INFO]: New best epoch, val score: -0.6811670691459281
[08/28/2025 12:00:40 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 12:00:48 INFO]: Training loss at epoch 25: 0.972770094871521
[08/28/2025 12:00:55 INFO]: Training loss at epoch 19: 0.9559217989444733
[08/28/2025 12:00:59 INFO]: Training loss at epoch 107: 0.9688857197761536
[08/28/2025 12:00:59 INFO]: Training loss at epoch 20: 0.9721156358718872
[08/28/2025 12:01:01 INFO]: New best epoch, val score: -0.6791829997457851
[08/28/2025 12:01:01 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 12:01:02 INFO]: Training stats: {
    "score": -1.0019182779550733,
    "rmse": 1.0019182779550733
}
[08/28/2025 12:01:02 INFO]: Val stats: {
    "score": -0.693289172344324,
    "rmse": 0.693289172344324
}
[08/28/2025 12:01:02 INFO]: Test stats: {
    "score": -0.8807016064310617,
    "rmse": 0.8807016064310617
}
[08/28/2025 12:01:09 INFO]: Running Final Evaluation...
[08/28/2025 12:01:11 INFO]: Training loss at epoch 30: 0.9911141991615295
[08/28/2025 12:01:18 INFO]: Training loss at epoch 21: 1.0543156266212463
[08/28/2025 12:01:20 INFO]: Training loss at epoch 23: 0.9827010035514832
[08/28/2025 12:01:21 INFO]: New best epoch, val score: -0.6778814827984719
[08/28/2025 12:01:21 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 12:01:21 INFO]: Training loss at epoch 20: 0.9951300323009491
[08/28/2025 12:01:23 INFO]: Training loss at epoch 3: 0.9350932538509369
[08/28/2025 12:01:23 INFO]: Training loss at epoch 32: 0.9736763834953308
[08/28/2025 12:01:23 INFO]: Training loss at epoch 8: 1.2710046172142029
[08/28/2025 12:01:29 INFO]: New best epoch, val score: -0.7139872287253812
[08/28/2025 12:01:29 INFO]: Saving model to: unhanged-Shanesha_trial_160/model_best.pth
[08/28/2025 12:01:39 INFO]: Training loss at epoch 22: 0.872645378112793
[08/28/2025 12:01:41 INFO]: New best epoch, val score: -0.6766907357411766
[08/28/2025 12:01:41 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 12:01:42 INFO]: Training loss at epoch 21: 0.8986853063106537
[08/28/2025 12:01:51 INFO]: Training accuracy: {
    "score": -0.9860103805742041,
    "rmse": 0.9860103805742041
}
[08/28/2025 12:01:51 INFO]: Val accuracy: {
    "score": -0.6667255356116343,
    "rmse": 0.6667255356116343
}
[08/28/2025 12:01:51 INFO]: Test accuracy: {
    "score": -0.8739582381207112,
    "rmse": 0.8739582381207112
}
[08/28/2025 12:01:51 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_134",
    "best_epoch": 76,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8739582381207112,
        "rmse": 0.8739582381207112
    },
    "train_stats": {
        "score": -0.9860103805742041,
        "rmse": 0.9860103805742041
    },
    "val_stats": {
        "score": -0.6667255356116343,
        "rmse": 0.6667255356116343
    }
}
[08/28/2025 12:01:51 INFO]: Procewss finished for trial unhanged-Shanesha_trial_134
[08/28/2025 12:01:51 INFO]: 
_________________________________________________

[08/28/2025 12:01:51 INFO]: train_net_for_optune.py main() running.
[08/28/2025 12:01:51 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 2.534507451913863
  attention_dropout: 0.24739419962777648
  ffn_dropout: 0.24739419962777648
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.4836454451082094e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_161

[08/28/2025 12:01:51 INFO]: This ft_transformer has 0.810 million parameters.
[08/28/2025 12:01:51 INFO]: Training will start at epoch 0.
[08/28/2025 12:01:51 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 12:01:51 INFO]: Training loss at epoch 32: 1.0064226686954498
[08/28/2025 12:01:52 INFO]: Training loss at epoch 38: 0.8935957252979279
[08/28/2025 12:01:59 INFO]: Training loss at epoch 23: 1.3825882077217102
[08/28/2025 12:02:01 INFO]: New best epoch, val score: -0.6758955212467536
[08/28/2025 12:02:01 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 12:02:01 INFO]: Training loss at epoch 22: 1.0104276239871979
[08/28/2025 12:02:09 INFO]: Training loss at epoch 24: 0.9923587739467621
[08/28/2025 12:02:18 INFO]: Training loss at epoch 24: 1.0591928362846375
[08/28/2025 12:02:20 INFO]: New best epoch, val score: -0.6755067239447777
[08/28/2025 12:02:20 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 12:02:20 INFO]: Training loss at epoch 23: 1.2085683345794678
[08/28/2025 12:02:21 INFO]: Training loss at epoch 4: 1.0364304780960083
[08/28/2025 12:02:23 INFO]: Training loss at epoch 0: 1.2060267925262451
[08/28/2025 12:02:28 INFO]: New best epoch, val score: -1.0604702075516363
[08/28/2025 12:02:28 INFO]: Saving model to: unhanged-Shanesha_trial_161/model_best.pth
[08/28/2025 12:02:28 INFO]: New best epoch, val score: -0.705124494773614
[08/28/2025 12:02:28 INFO]: Saving model to: unhanged-Shanesha_trial_160/model_best.pth
[08/28/2025 12:02:29 INFO]: Training loss at epoch 20: 1.0274666547775269
[08/28/2025 12:02:37 INFO]: Training loss at epoch 25: 1.2761777937412262
[08/28/2025 12:02:40 INFO]: Training loss at epoch 24: 1.011578381061554
[08/28/2025 12:02:44 INFO]: Training loss at epoch 38: 1.16227787733078
[08/28/2025 12:02:48 INFO]: Training loss at epoch 64: 0.9613586068153381
[08/28/2025 12:02:55 INFO]: Training loss at epoch 5: 1.050933837890625
[08/28/2025 12:02:56 INFO]: Training loss at epoch 26: 1.031298041343689
[08/28/2025 12:02:58 INFO]: New best epoch, val score: -0.6753210581700938
[08/28/2025 12:02:58 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 12:03:01 INFO]: Training loss at epoch 25: 0.9744665324687958
[08/28/2025 12:03:01 INFO]: Training loss at epoch 1: 1.307910978794098
[08/28/2025 12:03:05 INFO]: New best epoch, val score: -0.9640340486785313
[08/28/2025 12:03:05 INFO]: Saving model to: unhanged-Shanesha_trial_161/model_best.pth
[08/28/2025 12:03:12 INFO]: New best epoch, val score: -0.6897514521487149
[08/28/2025 12:03:12 INFO]: Saving model to: unhanged-Shanesha_trial_157/model_best.pth
[08/28/2025 12:03:15 INFO]: Training loss at epoch 27: 0.9556703865528107
[08/28/2025 12:03:17 INFO]: Training loss at epoch 31: 1.2806625962257385
[08/28/2025 12:03:17 INFO]: Training loss at epoch 5: 1.2699184119701385
[08/28/2025 12:03:17 INFO]: New best epoch, val score: -0.6748200610795759
[08/28/2025 12:03:17 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 12:03:21 INFO]: Training loss at epoch 26: 0.7863690853118896
[08/28/2025 12:03:24 INFO]: New best epoch, val score: -0.7043232100383712
[08/28/2025 12:03:24 INFO]: Saving model to: unhanged-Shanesha_trial_160/model_best.pth
[08/28/2025 12:03:35 INFO]: Training loss at epoch 28: 1.2998454570770264
[08/28/2025 12:03:37 INFO]: New best epoch, val score: -0.6745406757270256
[08/28/2025 12:03:37 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 12:03:37 INFO]: Training loss at epoch 39: 0.9217545092105865
[08/28/2025 12:03:38 INFO]: Training loss at epoch 2: 0.9190039336681366
[08/28/2025 12:03:40 INFO]: Training loss at epoch 27: 1.1523736119270325
[08/28/2025 12:03:42 INFO]: New best epoch, val score: -0.8386677377555826
[08/28/2025 12:03:42 INFO]: Saving model to: unhanged-Shanesha_trial_161/model_best.pth
[08/28/2025 12:03:44 INFO]: Training loss at epoch 9: 1.0741753578186035
[08/28/2025 12:03:50 INFO]: Training loss at epoch 9: 0.9214440882205963
[08/28/2025 12:03:53 INFO]: Training loss at epoch 26: 1.0187022686004639
[08/28/2025 12:03:54 INFO]: Training loss at epoch 29: 1.0231843292713165
[08/28/2025 12:03:56 INFO]: Training loss at epoch 33: 1.3229344189167023
[08/28/2025 12:03:59 INFO]: Training loss at epoch 28: 1.0031704306602478
[08/28/2025 12:04:01 INFO]: Training stats: {
    "score": -1.0005279285744455,
    "rmse": 1.0005279285744455
}
[08/28/2025 12:04:01 INFO]: Val stats: {
    "score": -0.6746883797175502,
    "rmse": 0.6746883797175502
}
[08/28/2025 12:04:01 INFO]: Test stats: {
    "score": -0.8774832821005789,
    "rmse": 0.8774832821005789
}
[08/28/2025 12:04:12 INFO]: Training loss at epoch 6: 1.031158983707428
[08/28/2025 12:04:15 INFO]: Training loss at epoch 3: 1.0081433951854706
[08/28/2025 12:04:17 INFO]: Training loss at epoch 24: 0.9013284146785736
[08/28/2025 12:04:19 INFO]: Training loss at epoch 29: 0.8871766626834869
[08/28/2025 12:04:19 INFO]: New best epoch, val score: -0.7275473382407571
[08/28/2025 12:04:19 INFO]: Saving model to: unhanged-Shanesha_trial_161/model_best.pth
[08/28/2025 12:04:19 INFO]: New best epoch, val score: -0.7042796335341418
[08/28/2025 12:04:19 INFO]: Saving model to: unhanged-Shanesha_trial_160/model_best.pth
[08/28/2025 12:04:20 INFO]: Training loss at epoch 30: 1.1461835503578186
[08/28/2025 12:04:26 INFO]: Training stats: {
    "score": -1.0015398574409362,
    "rmse": 1.0015398574409362
}
[08/28/2025 12:04:26 INFO]: Val stats: {
    "score": -0.6919673325763542,
    "rmse": 0.6919673325763542
}
[08/28/2025 12:04:26 INFO]: Test stats: {
    "score": -0.8798464477496051,
    "rmse": 0.8798464477496051
}
[08/28/2025 12:04:28 INFO]: Training loss at epoch 9: 0.9402813017368317
[08/28/2025 12:04:29 INFO]: Training loss at epoch 25: 0.8059419989585876
[08/28/2025 12:04:31 INFO]: Training stats: {
    "score": -0.9986401868126086,
    "rmse": 0.9986401868126086
}
[08/28/2025 12:04:31 INFO]: Val stats: {
    "score": -0.6821425210232508,
    "rmse": 0.6821425210232508
}
[08/28/2025 12:04:31 INFO]: Test stats: {
    "score": -0.8816074023088462,
    "rmse": 0.8816074023088462
}
[08/28/2025 12:04:31 INFO]: Training loss at epoch 31: 0.9514423310756683
[08/28/2025 12:04:39 INFO]: Training loss at epoch 31: 1.0694942474365234
[08/28/2025 12:04:42 INFO]: Training stats: {
    "score": -0.9874739841211821,
    "rmse": 0.9874739841211821
}
[08/28/2025 12:04:42 INFO]: Val stats: {
    "score": -0.678513649330164,
    "rmse": 0.678513649330164
}
[08/28/2025 12:04:42 INFO]: Test stats: {
    "score": -0.8683886308398604,
    "rmse": 0.8683886308398604
}
[08/28/2025 12:04:45 INFO]: Training loss at epoch 30: 1.0466504096984863
[08/28/2025 12:04:49 INFO]: Training loss at epoch 21: 0.9053520560264587
[08/28/2025 12:04:51 INFO]: Training loss at epoch 4: 0.8447890877723694
[08/28/2025 12:04:56 INFO]: New best epoch, val score: -0.6766765301656478
[08/28/2025 12:04:56 INFO]: Saving model to: unhanged-Shanesha_trial_161/model_best.pth
[08/28/2025 12:04:58 INFO]: Training loss at epoch 32: 1.0548521876335144
[08/28/2025 12:05:04 INFO]: Training loss at epoch 31: 1.1312389373779297
[08/28/2025 12:05:07 INFO]: Running Final Evaluation...
[08/28/2025 12:05:08 INFO]: Training loss at epoch 7: 1.1104038655757904
[08/28/2025 12:05:10 INFO]: Training loss at epoch 33: 1.123208999633789
[08/28/2025 12:05:15 INFO]: Training accuracy: {
    "score": -1.01264227101205,
    "rmse": 1.01264227101205
}
[08/28/2025 12:05:15 INFO]: Val accuracy: {
    "score": -0.663703296608934,
    "rmse": 0.663703296608934
}
[08/28/2025 12:05:15 INFO]: Test accuracy: {
    "score": -0.8726412510147142,
    "rmse": 0.8726412510147142
}
[08/28/2025 12:05:15 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_159",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8726412510147142,
        "rmse": 0.8726412510147142
    },
    "train_stats": {
        "score": -1.01264227101205,
        "rmse": 1.01264227101205
    },
    "val_stats": {
        "score": -0.663703296608934,
        "rmse": 0.663703296608934
    }
}
[08/28/2025 12:05:15 INFO]: Procewss finished for trial unhanged-Shanesha_trial_159
[08/28/2025 12:05:15 INFO]: 
_________________________________________________

[08/28/2025 12:05:15 INFO]: train_net_for_optune.py main() running.
[08/28/2025 12:05:15 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 16
  d_ffn_factor: 2.386345433753407
  attention_dropout: 0.2962190546845367
  ffn_dropout: 0.2962190546845367
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.9317277464643548e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_162

[08/28/2025 12:05:15 INFO]: This ft_transformer has 0.780 million parameters.
[08/28/2025 12:05:15 INFO]: Training will start at epoch 0.
[08/28/2025 12:05:15 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 12:05:17 INFO]: Training loss at epoch 6: 1.1437020897865295
[08/28/2025 12:05:19 INFO]: Training loss at epoch 33: 1.0001058876514435
[08/28/2025 12:05:20 INFO]: Training loss at epoch 39: 0.9060368835926056
[08/28/2025 12:05:31 INFO]: Training loss at epoch 5: 1.095449149608612
[08/28/2025 12:05:35 INFO]: New best epoch, val score: -0.6847980982699313
[08/28/2025 12:05:35 INFO]: Saving model to: unhanged-Shanesha_trial_157/model_best.pth
[08/28/2025 12:05:38 INFO]: New best epoch, val score: -0.6763837871232428
[08/28/2025 12:05:38 INFO]: Saving model to: unhanged-Shanesha_trial_161/model_best.pth
[08/28/2025 12:05:38 INFO]: Running Final Evaluation...
[08/28/2025 12:05:41 INFO]: Training loss at epoch 34: 1.085249900817871
[08/28/2025 12:06:01 INFO]: Training loss at epoch 35: 0.9974109828472137
[08/28/2025 12:06:10 INFO]: Training loss at epoch 8: 0.9587654173374176
[08/28/2025 12:06:10 INFO]: Training loss at epoch 39: 0.9291208684444427
[08/28/2025 12:06:13 INFO]: Training loss at epoch 6: 1.0782361030578613
[08/28/2025 12:06:18 INFO]: New best epoch, val score: -0.6762640216162005
[08/28/2025 12:06:18 INFO]: Saving model to: unhanged-Shanesha_trial_161/model_best.pth
[08/28/2025 12:06:22 INFO]: Training loss at epoch 36: 0.9884416162967682
[08/28/2025 12:06:22 INFO]: Training loss at epoch 0: 1.1118298768997192
[08/28/2025 12:06:31 INFO]: New best epoch, val score: -0.7038524407672535
[08/28/2025 12:06:31 INFO]: Saving model to: unhanged-Shanesha_trial_162/model_best.pth
[08/28/2025 12:06:36 INFO]: Training stats: {
    "score": -0.9818922405488301,
    "rmse": 0.9818922405488301
}
[08/28/2025 12:06:36 INFO]: Val stats: {
    "score": -0.7067428713136692,
    "rmse": 0.7067428713136692
}
[08/28/2025 12:06:36 INFO]: Test stats: {
    "score": -0.9215296940921345,
    "rmse": 0.9215296940921345
}
[08/28/2025 12:06:37 INFO]: Training loss at epoch 34: 1.2374593019485474
[08/28/2025 12:06:38 INFO]: Training stats: {
    "score": -1.0010161549061007,
    "rmse": 1.0010161549061007
}
[08/28/2025 12:06:38 INFO]: Val stats: {
    "score": -0.7001820869949902,
    "rmse": 0.7001820869949902
}
[08/28/2025 12:06:38 INFO]: Test stats: {
    "score": -0.8841173334661826,
    "rmse": 0.8841173334661826
}
[08/28/2025 12:06:43 INFO]: Training loss at epoch 37: 1.02901029586792
[08/28/2025 12:06:52 INFO]: Training loss at epoch 32: 0.9879056811332703
[08/28/2025 12:06:53 INFO]: Training loss at epoch 7: 0.9946468472480774
[08/28/2025 12:06:57 INFO]: New best epoch, val score: -0.6761994206544459
[08/28/2025 12:06:57 INFO]: Saving model to: unhanged-Shanesha_trial_161/model_best.pth
[08/28/2025 12:07:01 INFO]: Training loss at epoch 10: 1.2088336944580078
[08/28/2025 12:07:03 INFO]: Training loss at epoch 26: 1.1356322169303894
[08/28/2025 12:07:03 INFO]: Training loss at epoch 38: 1.0557934641838074
[08/28/2025 12:07:09 INFO]: Training loss at epoch 27: 0.9925861358642578
[08/28/2025 12:07:09 INFO]: Training accuracy: {
    "score": -1.0161259571791548,
    "rmse": 1.0161259571791548
}
[08/28/2025 12:07:09 INFO]: Val accuracy: {
    "score": -0.6677170711828441,
    "rmse": 0.6677170711828441
}
[08/28/2025 12:07:09 INFO]: Test accuracy: {
    "score": -0.8706106646463855,
    "rmse": 0.8706106646463855
}
[08/28/2025 12:07:09 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_143",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8706106646463855,
        "rmse": 0.8706106646463855
    },
    "train_stats": {
        "score": -1.0161259571791548,
        "rmse": 1.0161259571791548
    },
    "val_stats": {
        "score": -0.6677170711828441,
        "rmse": 0.6677170711828441
    }
}
[08/28/2025 12:07:09 INFO]: Procewss finished for trial unhanged-Shanesha_trial_143
[08/28/2025 12:07:09 INFO]: Training loss at epoch 9: 1.1498414278030396
[08/28/2025 12:07:09 INFO]: 
_________________________________________________

[08/28/2025 12:07:09 INFO]: train_net_for_optune.py main() running.
[08/28/2025 12:07:09 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 2.2864647248149415
  attention_dropout: 0.29296584416295435
  ffn_dropout: 0.29296584416295435
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 2.046475806588865e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_163

[08/28/2025 12:07:09 INFO]: This ft_transformer has 0.760 million parameters.
[08/28/2025 12:07:09 INFO]: Training will start at epoch 0.
[08/28/2025 12:07:09 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 12:07:22 INFO]: Training loss at epoch 22: 1.006547212600708
[08/28/2025 12:07:24 INFO]: Training loss at epoch 25: 1.3345251083374023
[08/28/2025 12:07:24 INFO]: Training loss at epoch 39: 1.0235521793365479
[08/28/2025 12:07:24 INFO]: Training stats: {
    "score": -0.9844563437244346,
    "rmse": 0.9844563437244346
}
[08/28/2025 12:07:24 INFO]: Val stats: {
    "score": -0.7284937685309405,
    "rmse": 0.7284937685309405
}
[08/28/2025 12:07:24 INFO]: Test stats: {
    "score": -0.918568148187955,
    "rmse": 0.918568148187955
}
[08/28/2025 12:07:24 INFO]: Training stats: {
    "score": -0.9989645557264984,
    "rmse": 0.9989645557264984
}
[08/28/2025 12:07:25 INFO]: Val stats: {
    "score": -0.6758444231851226,
    "rmse": 0.6758444231851226
}
[08/28/2025 12:07:25 INFO]: Test stats: {
    "score": -0.8723473760092862,
    "rmse": 0.8723473760092862
}
[08/28/2025 12:07:30 INFO]: Training stats: {
    "score": -1.0004109401687171,
    "rmse": 1.0004109401687171
}
[08/28/2025 12:07:30 INFO]: Val stats: {
    "score": -0.7064597263751127,
    "rmse": 0.7064597263751127
}
[08/28/2025 12:07:30 INFO]: Test stats: {
    "score": -0.8872566589553487,
    "rmse": 0.8872566589553487
}
[08/28/2025 12:07:31 INFO]: Training stats: {
    "score": -1.000581640913984,
    "rmse": 1.000581640913984
}
[08/28/2025 12:07:31 INFO]: Val stats: {
    "score": -0.6746353703312031,
    "rmse": 0.6746353703312031
}
[08/28/2025 12:07:31 INFO]: Test stats: {
    "score": -0.8771534892501524,
    "rmse": 0.8771534892501524
}
[08/28/2025 12:07:33 INFO]: Training loss at epoch 8: 0.9338228702545166
[08/28/2025 12:07:37 INFO]: Training loss at epoch 1: 1.0190755128860474
[08/28/2025 12:07:38 INFO]: New best epoch, val score: -0.6761665316975356
[08/28/2025 12:07:38 INFO]: Saving model to: unhanged-Shanesha_trial_161/model_best.pth
[08/28/2025 12:07:43 INFO]: Training loss at epoch 0: 1.430575430393219
[08/28/2025 12:07:48 INFO]: New best epoch, val score: -0.6760994812700997
[08/28/2025 12:07:48 INFO]: Saving model to: unhanged-Shanesha_trial_163/model_best.pth
[08/28/2025 12:07:52 INFO]: Training loss at epoch 7: 1.2305823266506195
[08/28/2025 12:07:53 INFO]: Training loss at epoch 40: 0.9521473348140717
[08/28/2025 12:08:08 INFO]: Training loss at epoch 32: 1.1462475061416626
[08/28/2025 12:08:09 INFO]: New best epoch, val score: -0.6810778200760782
[08/28/2025 12:08:09 INFO]: Saving model to: unhanged-Shanesha_trial_157/model_best.pth
[08/28/2025 12:08:12 INFO]: Training loss at epoch 40: 0.9452237486839294
[08/28/2025 12:08:14 INFO]: Training loss at epoch 41: 1.1086297035217285
[08/28/2025 12:08:15 INFO]: Training loss at epoch 9: 0.9077312052249908
[08/28/2025 12:08:17 INFO]: New best epoch, val score: -0.6743417901059712
[08/28/2025 12:08:17 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 12:08:26 INFO]: Training loss at epoch 1: 1.114605724811554
[08/28/2025 12:08:30 INFO]: Training stats: {
    "score": -1.0142510581955422,
    "rmse": 1.0142510581955422
}
[08/28/2025 12:08:30 INFO]: Val stats: {
    "score": -0.6761443437370238,
    "rmse": 0.6761443437370238
}
[08/28/2025 12:08:30 INFO]: Test stats: {
    "score": -0.8697194555485409,
    "rmse": 0.8697194555485409
}
[08/28/2025 12:08:31 INFO]: Training loss at epoch 10: 1.2144985496997833
[08/28/2025 12:08:34 INFO]: New best epoch, val score: -0.6761443437370238
[08/28/2025 12:08:34 INFO]: Saving model to: unhanged-Shanesha_trial_161/model_best.pth
[08/28/2025 12:08:36 INFO]: Training loss at epoch 42: 1.2238512635231018
[08/28/2025 12:08:39 INFO]: New best epoch, val score: -0.6738997996401912
[08/28/2025 12:08:39 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 12:08:49 INFO]: Training loss at epoch 65: 1.0064706802368164
[08/28/2025 12:08:53 INFO]: Training loss at epoch 2: 1.1362544894218445
[08/28/2025 12:08:58 INFO]: Training loss at epoch 43: 1.291624128818512
[08/28/2025 12:09:02 INFO]: New best epoch, val score: -0.6735719000186876
[08/28/2025 12:09:02 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 12:09:06 INFO]: Training loss at epoch 2: 1.176026999950409
[08/28/2025 12:09:11 INFO]: Training loss at epoch 10: 0.9883308410644531
[08/28/2025 12:09:20 INFO]: Training loss at epoch 44: 1.0589872598648071
[08/28/2025 12:09:22 INFO]: New best epoch, val score: -0.6732300446828788
[08/28/2025 12:09:22 INFO]: Saving model to: unhanged-Shanesha_trial_158/model_best.pth
[08/28/2025 12:09:25 INFO]: Training loss at epoch 35: 1.1860930025577545
[08/28/2025 12:09:32 INFO]: Training loss at epoch 11: 0.9446641802787781
[08/28/2025 12:09:36 INFO]: Training loss at epoch 11: 0.9998894035816193
[08/28/2025 12:09:41 INFO]: Training loss at epoch 45: 1.264042466878891
[08/28/2025 12:09:41 INFO]: Training loss at epoch 27: 0.8140186965465546
[08/28/2025 12:09:45 INFO]: Training loss at epoch 3: 0.8347676694393158
[08/28/2025 12:09:50 INFO]: Training loss at epoch 11: 0.9438998401165009
[08/28/2025 12:09:57 INFO]: Training loss at epoch 23: 0.9363565146923065
[08/28/2025 12:10:02 INFO]: Training loss at epoch 46: 1.2262200713157654
[08/28/2025 12:10:08 INFO]: Training loss at epoch 3: 1.1525010466575623
[08/28/2025 12:10:22 INFO]: Training loss at epoch 47: 1.12323796749115
[08/28/2025 12:10:24 INFO]: Training loss at epoch 8: 1.0395809710025787
[08/28/2025 12:10:24 INFO]: Training loss at epoch 40: 1.0530242025852203
[08/28/2025 12:10:24 INFO]: Training loss at epoch 4: 1.0505062341690063
[08/28/2025 12:10:29 INFO]: Training loss at epoch 12: 1.1133995652198792
[08/28/2025 12:10:30 INFO]: Training loss at epoch 28: 1.136508047580719
[08/28/2025 12:10:31 INFO]: Training loss at epoch 12: 0.9855083525180817
[08/28/2025 12:10:35 INFO]: Training loss at epoch 26: 1.0219127833843231
[08/28/2025 12:10:39 INFO]: New best epoch, val score: -0.6786914895841931
[08/28/2025 12:10:39 INFO]: Saving model to: unhanged-Shanesha_trial_157/model_best.pth
[08/28/2025 12:10:40 INFO]: Training loss at epoch 33: 0.9218054413795471
[08/28/2025 12:10:42 INFO]: Training loss at epoch 48: 0.9600765109062195
[08/28/2025 12:11:03 INFO]: Training loss at epoch 49: 0.9512974917888641
[08/28/2025 12:11:03 INFO]: Training loss at epoch 5: 0.8704793155193329
[08/28/2025 12:11:05 INFO]: Training loss at epoch 40: 0.9892308712005615
[08/28/2025 12:11:08 INFO]: Training loss at epoch 13: 0.9477983117103577
[08/28/2025 12:11:10 INFO]: Training stats: {
    "score": -1.0006388511354791,
    "rmse": 1.0006388511354791
}
[08/28/2025 12:11:10 INFO]: Val stats: {
    "score": -0.6742745610754004,
    "rmse": 0.6742745610754004
}
[08/28/2025 12:11:10 INFO]: Test stats: {
    "score": -0.8767157130075816,
    "rmse": 0.8767157130075816
}
[08/28/2025 12:11:20 INFO]: Training loss at epoch 4: 1.0581954717636108
[08/28/2025 12:11:28 INFO]: Training loss at epoch 13: 0.9474015831947327
[08/28/2025 12:11:29 INFO]: Training loss at epoch 50: 1.3142922222614288
[08/28/2025 12:11:37 INFO]: Training loss at epoch 41: 1.10054349899292
[08/28/2025 12:11:40 INFO]: Training loss at epoch 6: 1.1300734281539917
[08/28/2025 12:11:42 INFO]: Training loss at epoch 33: 0.8643186688423157
[08/28/2025 12:11:44 INFO]: Training loss at epoch 14: 0.9197900295257568
[08/28/2025 12:11:48 INFO]: Training loss at epoch 51: 0.8771346807479858
[08/28/2025 12:12:01 INFO]: Training loss at epoch 12: 0.9575416445732117
[08/28/2025 12:12:04 INFO]: Training loss at epoch 36: 0.9664318859577179
[08/28/2025 12:12:08 INFO]: Training loss at epoch 52: 0.9747267067432404
[08/28/2025 12:12:08 INFO]: Training loss at epoch 28: 0.775373637676239
[08/28/2025 12:12:17 INFO]: Training loss at epoch 7: 0.9480508267879486
[08/28/2025 12:12:20 INFO]: Training loss at epoch 24: 0.994047999382019
[08/28/2025 12:12:21 INFO]: Training loss at epoch 15: 1.0671225786209106
[08/28/2025 12:12:25 INFO]: Training loss at epoch 14: 0.9891257286071777
[08/28/2025 12:12:27 INFO]: Training loss at epoch 53: 0.9716501533985138
[08/28/2025 12:12:28 INFO]: Training loss at epoch 5: 1.0826186537742615
[08/28/2025 12:12:44 INFO]: Training loss at epoch 9: 1.0583502054214478
[08/28/2025 12:12:47 INFO]: Training loss at epoch 54: 1.016023576259613
[08/28/2025 12:12:53 INFO]: Training loss at epoch 8: 1.08577561378479
[08/28/2025 12:12:57 INFO]: Training loss at epoch 16: 1.1011494398117065
[08/28/2025 12:13:06 INFO]: Training loss at epoch 55: 0.9695862531661987
[08/28/2025 12:13:20 INFO]: Training loss at epoch 15: 1.1545470654964447
[08/28/2025 12:13:25 INFO]: Training loss at epoch 56: 1.2712441682815552
[08/28/2025 12:13:28 INFO]: Training loss at epoch 9: 1.1127021312713623
[08/28/2025 12:13:29 INFO]: Training stats: {
    "score": -0.9971430636434505,
    "rmse": 0.9971430636434505
}
[08/28/2025 12:13:29 INFO]: Val stats: {
    "score": -0.6773407037104127,
    "rmse": 0.6773407037104127
}
[08/28/2025 12:13:29 INFO]: Test stats: {
    "score": -0.8676652975725467,
    "rmse": 0.8676652975725467
}
[08/28/2025 12:13:32 INFO]: Training loss at epoch 27: 0.8387510478496552
[08/28/2025 12:13:34 INFO]: Training loss at epoch 17: 0.9882147908210754
[08/28/2025 12:13:35 INFO]: Training loss at epoch 29: 0.9839739501476288
[08/28/2025 12:13:37 INFO]: Training loss at epoch 6: 0.9847527146339417
[08/28/2025 12:13:39 INFO]: Training stats: {
    "score": -1.0252971718888322,
    "rmse": 1.0252971718888322
}
[08/28/2025 12:13:39 INFO]: Val stats: {
    "score": -0.7701564538655472,
    "rmse": 0.7701564538655472
}
[08/28/2025 12:13:39 INFO]: Test stats: {
    "score": -0.9384358856782173,
    "rmse": 0.9384358856782173
}
[08/28/2025 12:13:45 INFO]: Training loss at epoch 57: 0.8340117931365967
[08/28/2025 12:13:45 INFO]: New best epoch, val score: -0.6773407037104127
[08/28/2025 12:13:45 INFO]: Saving model to: unhanged-Shanesha_trial_157/model_best.pth
[08/28/2025 12:13:51 INFO]: Training loss at epoch 41: 0.9147675037384033
[08/28/2025 12:14:04 INFO]: Training loss at epoch 58: 1.039050042629242
[08/28/2025 12:14:09 INFO]: Training loss at epoch 34: 1.0971316695213318
[08/28/2025 12:14:11 INFO]: Training loss at epoch 18: 1.0173510909080505
[08/28/2025 12:14:16 INFO]: Training loss at epoch 10: 0.9895600974559784
[08/28/2025 12:14:16 INFO]: Training loss at epoch 16: 1.0661194920539856
[08/28/2025 12:14:21 INFO]: Training loss at epoch 13: 0.8523735404014587
[08/28/2025 12:14:25 INFO]: Training loss at epoch 59: 0.9745477139949799
[08/28/2025 12:14:27 INFO]: Training loss at epoch 41: 1.141889363527298
[08/28/2025 12:14:30 INFO]: Training loss at epoch 29: 1.138427972793579
[08/28/2025 12:14:33 INFO]: Training stats: {
    "score": -1.00027069841073,
    "rmse": 1.00027069841073
}
[08/28/2025 12:14:33 INFO]: Val stats: {
    "score": -0.6770858593492056,
    "rmse": 0.6770858593492056
}
[08/28/2025 12:14:33 INFO]: Test stats: {
    "score": -0.8779118759067708,
    "rmse": 0.8779118759067708
}
[08/28/2025 12:14:39 INFO]: Training loss at epoch 37: 0.9840063154697418
[08/28/2025 12:14:39 INFO]: Training stats: {
    "score": -0.9797504679501076,
    "rmse": 0.9797504679501076
}
[08/28/2025 12:14:39 INFO]: Val stats: {
    "score": -0.6918902512437339,
    "rmse": 0.6918902512437339
}
[08/28/2025 12:14:39 INFO]: Test stats: {
    "score": -0.897518008432533,
    "rmse": 0.897518008432533
}
[08/28/2025 12:14:40 INFO]: Training loss at epoch 66: 0.906035453081131
[08/28/2025 12:14:43 INFO]: Training loss at epoch 25: 1.203562319278717
[08/28/2025 12:14:50 INFO]: Training loss at epoch 7: 0.952552080154419
[08/28/2025 12:14:51 INFO]: Training loss at epoch 42: 1.1264739036560059
[08/28/2025 12:14:51 INFO]: Training loss at epoch 19: 0.8998244106769562
[08/28/2025 12:14:54 INFO]: Training loss at epoch 11: 1.081385612487793
[08/28/2025 12:14:54 INFO]: Training loss at epoch 60: 0.8718312978744507
[08/28/2025 12:14:58 INFO]: New best epoch, val score: -0.6528626510796882
[08/28/2025 12:14:58 INFO]: Saving model to: unhanged-Shanesha_trial_147/model_best.pth
[08/28/2025 12:15:03 INFO]: Training loss at epoch 10: 0.9018912315368652
[08/28/2025 12:15:05 INFO]: Training stats: {
    "score": -1.00212865033636,
    "rmse": 1.00212865033636
}
[08/28/2025 12:15:05 INFO]: Val stats: {
    "score": -0.6816982939145457,
    "rmse": 0.6816982939145457
}
[08/28/2025 12:15:05 INFO]: Test stats: {
    "score": -0.8692705606955392,
    "rmse": 0.8692705606955392
}
[08/28/2025 12:15:08 INFO]: Training loss at epoch 34: 1.1933635473251343
[08/28/2025 12:15:16 INFO]: Training loss at epoch 61: 0.90582075715065
[08/28/2025 12:15:16 INFO]: Training loss at epoch 17: 1.0573489665985107
[08/28/2025 12:15:24 INFO]: Training stats: {
    "score": -0.9961095304219348,
    "rmse": 0.9961095304219348
}
[08/28/2025 12:15:24 INFO]: Val stats: {
    "score": -0.6752000839375413,
    "rmse": 0.6752000839375413
}
[08/28/2025 12:15:24 INFO]: Test stats: {
    "score": -0.877397726563079,
    "rmse": 0.877397726563079
}
[08/28/2025 12:15:33 INFO]: Training loss at epoch 12: 1.1495254933834076
[08/28/2025 12:15:37 INFO]: Training loss at epoch 62: 1.051570475101471
[08/28/2025 12:15:46 INFO]: Training loss at epoch 20: 0.9606129825115204
[08/28/2025 12:15:59 INFO]: Training loss at epoch 63: 1.0276299118995667
[08/28/2025 12:15:59 INFO]: Training loss at epoch 10: 0.9072523415088654
[08/28/2025 12:16:05 INFO]: Training loss at epoch 8: 1.0437428951263428
[08/28/2025 12:16:07 INFO]: Training loss at epoch 10: 0.8078368306159973
[08/28/2025 12:16:11 INFO]: Training loss at epoch 13: 1.1814756393432617
[08/28/2025 12:16:17 INFO]: New best epoch, val score: -0.6763777296343582
[08/28/2025 12:16:17 INFO]: Saving model to: unhanged-Shanesha_trial_157/model_best.pth
[08/28/2025 12:16:17 INFO]: Training loss at epoch 18: 0.9454425573348999
[08/28/2025 12:16:22 INFO]: Training loss at epoch 64: 0.9820601642131805
[08/28/2025 12:16:27 INFO]: Training loss at epoch 21: 0.8535763621330261
[08/28/2025 12:16:43 INFO]: Training loss at epoch 65: 1.2251897156238556
[08/28/2025 12:16:46 INFO]: Training loss at epoch 28: 1.1207666993141174
[08/28/2025 12:16:51 INFO]: Training loss at epoch 14: 1.1031885743141174
[08/28/2025 12:16:56 INFO]: Training loss at epoch 14: 0.9292331635951996
[08/28/2025 12:17:05 INFO]: Training loss at epoch 66: 0.9009584784507751
[08/28/2025 12:17:07 INFO]: Training loss at epoch 22: 1.328122317790985
[08/28/2025 12:17:11 INFO]: New best epoch, val score: -0.6983119028439212
[08/28/2025 12:17:11 INFO]: Saving model to: unhanged-Shanesha_trial_149/model_best.pth
[08/28/2025 12:17:18 INFO]: Training loss at epoch 26: 1.090071201324463
[08/28/2025 12:17:18 INFO]: Training loss at epoch 19: 1.1191721856594086
[08/28/2025 12:17:21 INFO]: Training loss at epoch 9: 1.0262715518474579
[08/28/2025 12:17:27 INFO]: Training loss at epoch 67: 1.0644358396530151
[08/28/2025 12:17:30 INFO]: Training loss at epoch 38: 0.8792828619480133
[08/28/2025 12:17:30 INFO]: Training loss at epoch 15: 1.073201596736908
[08/28/2025 12:17:39 INFO]: Training loss at epoch 42: 1.0032641887664795
[08/28/2025 12:17:40 INFO]: Training stats: {
    "score": -1.012209862202215,
    "rmse": 1.012209862202215
}
[08/28/2025 12:17:40 INFO]: Val stats: {
    "score": -0.7457047551232294,
    "rmse": 0.7457047551232294
}
[08/28/2025 12:17:40 INFO]: Test stats: {
    "score": -0.9115333593048148,
    "rmse": 0.9115333593048148
}
[08/28/2025 12:17:47 INFO]: Training loss at epoch 23: 1.2682712078094482
[08/28/2025 12:17:47 INFO]: Training loss at epoch 68: 1.1043167114257812
[08/28/2025 12:17:48 INFO]: Training stats: {
    "score": -1.0266659693292115,
    "rmse": 1.0266659693292115
}
[08/28/2025 12:17:48 INFO]: Val stats: {
    "score": -0.7824236249287494,
    "rmse": 0.7824236249287494
}
[08/28/2025 12:17:48 INFO]: Test stats: {
    "score": -0.9366853592285482,
    "rmse": 0.9366853592285482
}
[08/28/2025 12:17:59 INFO]: Training loss at epoch 35: 0.9296747446060181
[08/28/2025 12:18:01 INFO]: Training loss at epoch 30: 0.872755765914917
[08/28/2025 12:18:03 INFO]: Training loss at epoch 30: 1.087343692779541
[08/28/2025 12:18:08 INFO]: Training loss at epoch 16: 1.0011643469333649
[08/28/2025 12:18:09 INFO]: Training loss at epoch 42: 0.9999507665634155
[08/28/2025 12:18:09 INFO]: Training loss at epoch 69: 0.9159478545188904
[08/28/2025 12:18:17 INFO]: Training stats: {
    "score": -1.000102629945745,
    "rmse": 1.000102629945745
}
[08/28/2025 12:18:17 INFO]: Val stats: {
    "score": -0.6796569271371975,
    "rmse": 0.6796569271371975
}
[08/28/2025 12:18:17 INFO]: Test stats: {
    "score": -0.8789902034902027,
    "rmse": 0.8789902034902027
}
[08/28/2025 12:18:23 INFO]: Training loss at epoch 43: 0.9573985040187836
[08/28/2025 12:18:28 INFO]: Training loss at epoch 24: 1.2406430840492249
[08/28/2025 12:18:33 INFO]: Training loss at epoch 11: 1.0115275979042053
[08/28/2025 12:18:39 INFO]: Training loss at epoch 70: 1.0170010924339294
[08/28/2025 12:18:40 INFO]: Training loss at epoch 20: 1.0755742192268372
[08/28/2025 12:18:47 INFO]: Training loss at epoch 17: 1.0333309173583984
[08/28/2025 12:18:49 INFO]: Running Final Evaluation...
[08/28/2025 12:18:49 INFO]: Training loss at epoch 35: 1.1647459864616394
[08/28/2025 12:19:01 INFO]: Training loss at epoch 71: 1.0959873795509338
[08/28/2025 12:19:03 INFO]: Training loss at epoch 10: 1.219696283340454
[08/28/2025 12:19:08 INFO]: Training loss at epoch 25: 1.243791103363037
[08/28/2025 12:19:14 INFO]: Running Final Evaluation...
[08/28/2025 12:19:20 INFO]: Training loss at epoch 72: 1.2957019805908203
[08/28/2025 12:19:23 INFO]: Training loss at epoch 18: 1.2055129408836365
[08/28/2025 12:19:28 INFO]: Training loss at epoch 15: 0.81593057513237
[08/28/2025 12:19:39 INFO]: Training loss at epoch 21: 1.0069477558135986
[08/28/2025 12:19:41 INFO]: Training loss at epoch 73: 1.0749419331550598
[08/28/2025 12:19:46 INFO]: Training loss at epoch 26: 0.8767118155956268
[08/28/2025 12:19:49 INFO]: Training loss at epoch 27: 1.0602079033851624
[08/28/2025 12:19:56 INFO]: Training loss at epoch 29: 0.940913736820221
[08/28/2025 12:20:00 INFO]: Training loss at epoch 19: 1.179186850786209
[08/28/2025 12:20:02 INFO]: Training loss at epoch 74: 0.8607718646526337
[08/28/2025 12:20:12 INFO]: Training stats: {
    "score": -0.9990184593952083,
    "rmse": 0.9990184593952083
}
[08/28/2025 12:20:12 INFO]: Val stats: {
    "score": -0.6916881370045418,
    "rmse": 0.6916881370045418
}
[08/28/2025 12:20:12 INFO]: Test stats: {
    "score": -0.884836649356531,
    "rmse": 0.884836649356531
}
[08/28/2025 12:20:12 INFO]: Training loss at epoch 39: 0.941637396812439
[08/28/2025 12:20:14 INFO]: Training loss at epoch 11: 0.8872188925743103
[08/28/2025 12:20:14 INFO]: Training accuracy: {
    "score": -1.0159805913323416,
    "rmse": 1.0159805913323416
}
[08/28/2025 12:20:14 INFO]: Val accuracy: {
    "score": -0.6540747038834837,
    "rmse": 0.6540747038834837
}
[08/28/2025 12:20:14 INFO]: Test accuracy: {
    "score": -0.8623653727545806,
    "rmse": 0.8623653727545806
}
[08/28/2025 12:20:14 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_140",
    "best_epoch": 12,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8623653727545806,
        "rmse": 0.8623653727545806
    },
    "train_stats": {
        "score": -1.0159805913323416,
        "rmse": 1.0159805913323416
    },
    "val_stats": {
        "score": -0.6540747038834837,
        "rmse": 0.6540747038834837
    }
}
[08/28/2025 12:20:14 INFO]: Procewss finished for trial unhanged-Shanesha_trial_140
[08/28/2025 12:20:15 INFO]: 
_________________________________________________

[08/28/2025 12:20:15 INFO]: train_net_for_optune.py main() running.
[08/28/2025 12:20:15 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 4
  d_ffn_factor: 2.053316342386457
  attention_dropout: 0.29491272007987535
  ffn_dropout: 0.29491272007987535
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.488783026591305e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_164

[08/28/2025 12:20:15 INFO]: This ft_transformer has 6.138 million parameters.
[08/28/2025 12:20:15 INFO]: Training will start at epoch 0.
[08/28/2025 12:20:15 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 12:20:21 INFO]: Training loss at epoch 75: 1.035179615020752
[08/28/2025 12:20:23 INFO]: Training loss at epoch 27: 0.9912442564964294
[08/28/2025 12:20:24 INFO]: Running Final Evaluation...
[08/28/2025 12:20:31 INFO]: Training accuracy: {
    "score": -1.0007698156654135,
    "rmse": 1.0007698156654135
}
[08/28/2025 12:20:31 INFO]: Val accuracy: {
    "score": -0.6732300446828788,
    "rmse": 0.6732300446828788
}
[08/28/2025 12:20:31 INFO]: Test accuracy: {
    "score": -0.8764918834286772,
    "rmse": 0.8764918834286772
}
[08/28/2025 12:20:31 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_158",
    "best_epoch": 44,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8764918834286772,
        "rmse": 0.8764918834286772
    },
    "train_stats": {
        "score": -1.0007698156654135,
        "rmse": 1.0007698156654135
    },
    "val_stats": {
        "score": -0.6732300446828788,
        "rmse": 0.6732300446828788
    }
}
[08/28/2025 12:20:31 INFO]: Procewss finished for trial unhanged-Shanesha_trial_158
[08/28/2025 12:20:31 INFO]: 
_________________________________________________

[08/28/2025 12:20:31 INFO]: train_net_for_optune.py main() running.
[08/28/2025 12:20:31 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 2.2739013356476523
  attention_dropout: 0.3323275759491163
  ffn_dropout: 0.3323275759491163
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.5150381110842072e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_165

[08/28/2025 12:20:31 INFO]: This ft_transformer has 16.135 million parameters.
[08/28/2025 12:20:31 INFO]: Training will start at epoch 0.
[08/28/2025 12:20:31 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 12:20:34 INFO]: Training loss at epoch 31: 0.9146889746189117
[08/28/2025 12:20:34 INFO]: Training accuracy: {
    "score": -1.0218679849254162,
    "rmse": 1.0218679849254162
}
[08/28/2025 12:20:34 INFO]: Val accuracy: {
    "score": -0.6876970710488765,
    "rmse": 0.6876970710488765
}
[08/28/2025 12:20:34 INFO]: Test accuracy: {
    "score": -0.8809421784945686,
    "rmse": 0.8809421784945686
}
[08/28/2025 12:20:35 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_145",
    "best_epoch": 4,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8809421784945686,
        "rmse": 0.8809421784945686
    },
    "train_stats": {
        "score": -1.0218679849254162,
        "rmse": 1.0218679849254162
    },
    "val_stats": {
        "score": -0.6876970710488765,
        "rmse": 0.6876970710488765
    }
}
[08/28/2025 12:20:35 INFO]: Procewss finished for trial unhanged-Shanesha_trial_145
[08/28/2025 12:20:35 INFO]: 
_________________________________________________

[08/28/2025 12:20:35 INFO]: train_net_for_optune.py main() running.
[08/28/2025 12:20:35 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 2.272159023475376
  attention_dropout: 0.33584566417186335
  ffn_dropout: 0.33584566417186335
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.4757964273751636e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_166

[08/28/2025 12:20:35 INFO]: This ft_transformer has 16.123 million parameters.
[08/28/2025 12:20:35 INFO]: Training will start at epoch 0.
[08/28/2025 12:20:35 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 12:20:35 INFO]: Training loss at epoch 22: 1.2245367765426636
[08/28/2025 12:20:47 INFO]: Training loss at epoch 67: 1.1885726749897003
[08/28/2025 12:20:47 INFO]: Training loss at epoch 20: 1.2450773119926453
[08/28/2025 12:20:56 INFO]: Training loss at epoch 12: 1.002081960439682
[08/28/2025 12:20:56 INFO]: Training stats: {
    "score": -1.0162473654212332,
    "rmse": 1.0162473654212332
}
[08/28/2025 12:20:56 INFO]: Val stats: {
    "score": -0.7633429090036509,
    "rmse": 0.7633429090036509
}
[08/28/2025 12:20:56 INFO]: Test stats: {
    "score": -0.9188581631351326,
    "rmse": 0.9188581631351326
}
[08/28/2025 12:21:00 INFO]: Training loss at epoch 28: 1.1483227610588074
[08/28/2025 12:21:03 INFO]: Training stats: {
    "score": -0.9999223398100495,
    "rmse": 0.9999223398100495
}
[08/28/2025 12:21:03 INFO]: Val stats: {
    "score": -0.6589436698583958,
    "rmse": 0.6589436698583958
}
[08/28/2025 12:21:03 INFO]: Test stats: {
    "score": -0.8736891856055936,
    "rmse": 0.8736891856055936
}
[08/28/2025 12:21:09 INFO]: Training loss at epoch 31: 1.008983165025711
[08/28/2025 12:21:14 INFO]: Training loss at epoch 43: 0.8936844170093536
[08/28/2025 12:21:22 INFO]: Training loss at epoch 21: 0.9196326434612274
[08/28/2025 12:21:22 INFO]: Training loss at epoch 12: 1.0038771331310272
[08/28/2025 12:21:30 INFO]: Training loss at epoch 23: 0.9716335833072662
[08/28/2025 12:21:33 INFO]: Training loss at epoch 36: 1.0230811834335327
[08/28/2025 12:21:35 INFO]: Training loss at epoch 43: 1.0207090377807617
[08/28/2025 12:21:36 INFO]: Training loss at epoch 29: 0.9785033762454987
[08/28/2025 12:21:48 INFO]: Training stats: {
    "score": -0.9970613913674966,
    "rmse": 0.9970613913674966
}
[08/28/2025 12:21:48 INFO]: Val stats: {
    "score": -0.6960443753550664,
    "rmse": 0.6960443753550664
}
[08/28/2025 12:21:48 INFO]: Test stats: {
    "score": -0.876037947156003,
    "rmse": 0.876037947156003
}
[08/28/2025 12:21:49 INFO]: Training loss at epoch 16: 0.9156438708305359
[08/28/2025 12:21:56 INFO]: Training loss at epoch 22: 0.8921456336975098
[08/28/2025 12:21:57 INFO]: Running Final Evaluation...
[08/28/2025 12:22:07 INFO]: Training loss at epoch 28: 0.9266672432422638
[08/28/2025 12:22:24 INFO]: Training loss at epoch 30: 1.1171456575393677
[08/28/2025 12:22:25 INFO]: Training loss at epoch 24: 1.1894806325435638
[08/28/2025 12:22:30 INFO]: Training loss at epoch 0: 1.1791919469833374
[08/28/2025 12:22:30 INFO]: Training loss at epoch 13: 1.0337347984313965
[08/28/2025 12:22:31 INFO]: Training loss at epoch 23: 1.2352917194366455
[08/28/2025 12:22:48 INFO]: New best epoch, val score: -0.7507909241762908
[08/28/2025 12:22:48 INFO]: Saving model to: unhanged-Shanesha_trial_164/model_best.pth
[08/28/2025 12:22:56 INFO]: Training loss at epoch 32: 0.9831623435020447
[08/28/2025 12:23:01 INFO]: Training loss at epoch 31: 1.2220172882080078
[08/28/2025 12:23:06 INFO]: Training loss at epoch 24: 0.8891067802906036
[08/28/2025 12:23:12 INFO]: Training loss at epoch 13: 1.1607791185379028
[08/28/2025 12:23:20 INFO]: Training accuracy: {
    "score": -1.0229957282678892,
    "rmse": 1.0229957282678892
}
[08/28/2025 12:23:20 INFO]: Val accuracy: {
    "score": -0.6610882508383221,
    "rmse": 0.6610882508383221
}
[08/28/2025 12:23:20 INFO]: Test accuracy: {
    "score": -0.8677724090002615,
    "rmse": 0.8677724090002615
}
[08/28/2025 12:23:20 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_144",
    "best_epoch": 5,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8677724090002615,
        "rmse": 0.8677724090002615
    },
    "train_stats": {
        "score": -1.0229957282678892,
        "rmse": 1.0229957282678892
    },
    "val_stats": {
        "score": -0.6610882508383221,
        "rmse": 0.6610882508383221
    }
}
[08/28/2025 12:23:20 INFO]: Procewss finished for trial unhanged-Shanesha_trial_144
[08/28/2025 12:23:20 INFO]: 
_________________________________________________

[08/28/2025 12:23:20 INFO]: train_net_for_optune.py main() running.
[08/28/2025 12:23:20 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 4
  d_ffn_factor: 2.29258085917854
  attention_dropout: 0.3321630737323101
  ffn_dropout: 0.3321630737323101
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.496883436085683e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_167

[08/28/2025 12:23:21 INFO]: This ft_transformer has 16.215 million parameters.
[08/28/2025 12:23:21 INFO]: Training will start at epoch 0.
[08/28/2025 12:23:21 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 12:23:21 INFO]: Training loss at epoch 25: 0.9756520390510559
[08/28/2025 12:23:28 INFO]: Training loss at epoch 11: 0.8751824796199799
[08/28/2025 12:23:36 INFO]: Training loss at epoch 40: 0.9532305598258972
[08/28/2025 12:23:41 INFO]: Training loss at epoch 32: 1.0144226253032684
[08/28/2025 12:23:42 INFO]: Training loss at epoch 14: 1.0429359674453735
[08/28/2025 12:23:45 INFO]: Training loss at epoch 25: 1.2585406005382538
[08/28/2025 12:23:54 INFO]: Training loss at epoch 30: 1.0181291699409485
[08/28/2025 12:24:14 INFO]: Training loss at epoch 17: 1.251148283481598
[08/28/2025 12:24:16 INFO]: Training loss at epoch 32: 0.8188656866550446
[08/28/2025 12:24:20 INFO]: Training loss at epoch 26: 0.7649087160825729
[08/28/2025 12:24:21 INFO]: Training loss at epoch 33: 1.038071632385254
[08/28/2025 12:24:22 INFO]: Training loss at epoch 26: 1.1746699213981628
[08/28/2025 12:24:33 INFO]: Training loss at epoch 29: 0.897814005613327
[08/28/2025 12:24:48 INFO]: Training loss at epoch 11: 1.1032606363296509
[08/28/2025 12:24:49 INFO]: Training loss at epoch 44: 1.0725834965705872
[08/28/2025 12:24:55 INFO]: Training loss at epoch 15: 1.1584188342094421
[08/28/2025 12:25:00 INFO]: Training loss at epoch 34: 1.0069303214550018
[08/28/2025 12:25:01 INFO]: Training loss at epoch 27: 0.9902011156082153
[08/28/2025 12:25:04 INFO]: Training loss at epoch 44: 0.9591211080551147
[08/28/2025 12:25:06 INFO]: New best epoch, val score: -0.699181250167697
[08/28/2025 12:25:06 INFO]: Saving model to: unhanged-Shanesha_trial_162/model_best.pth
[08/28/2025 12:25:17 INFO]: Training loss at epoch 1: 1.060187816619873
[08/28/2025 12:25:19 INFO]: Training loss at epoch 27: 0.9234599769115448
[08/28/2025 12:25:26 INFO]: Training stats: {
    "score": -0.9955542766001091,
    "rmse": 0.9955542766001091
}
[08/28/2025 12:25:26 INFO]: Val stats: {
    "score": -0.6838149568143969,
    "rmse": 0.6838149568143969
}
[08/28/2025 12:25:26 INFO]: Test stats: {
    "score": -0.876231707927203,
    "rmse": 0.876231707927203
}
[08/28/2025 12:25:31 INFO]: Training loss at epoch 33: 1.1612443923950195
[08/28/2025 12:25:37 INFO]: New best epoch, val score: -0.7062216735488269
[08/28/2025 12:25:37 INFO]: Saving model to: unhanged-Shanesha_trial_164/model_best.pth
[08/28/2025 12:25:39 INFO]: Training loss at epoch 28: 1.1995920538902283
[08/28/2025 12:25:41 INFO]: Training loss at epoch 35: 0.8613255620002747
[08/28/2025 12:25:45 INFO]: Training loss at epoch 14: 1.0378206968307495
[08/28/2025 12:25:52 INFO]: New best epoch, val score: -0.6856258124810559
[08/28/2025 12:25:52 INFO]: Saving model to: unhanged-Shanesha_trial_149/model_best.pth
[08/28/2025 12:26:11 INFO]: Training loss at epoch 16: 1.150510013103485
[08/28/2025 12:26:18 INFO]: Training loss at epoch 29: 1.0469862520694733
[08/28/2025 12:26:21 INFO]: Training loss at epoch 28: 1.3855542242527008
[08/28/2025 12:26:21 INFO]: New best epoch, val score: -0.6911570285486142
[08/28/2025 12:26:21 INFO]: Saving model to: unhanged-Shanesha_trial_162/model_best.pth
[08/28/2025 12:26:22 INFO]: Training loss at epoch 36: 1.1073154211044312
[08/28/2025 12:26:24 INFO]: Training loss at epoch 41: 0.975400984287262
[08/28/2025 12:26:31 INFO]: Training stats: {
    "score": -0.9991781242966253,
    "rmse": 0.9991781242966253
}
[08/28/2025 12:26:31 INFO]: Val stats: {
    "score": -0.7027732866942317,
    "rmse": 0.7027732866942317
}
[08/28/2025 12:26:31 INFO]: Test stats: {
    "score": -0.8920984115884683,
    "rmse": 0.8920984115884683
}
[08/28/2025 12:26:45 INFO]: Training loss at epoch 68: 1.017185389995575
[08/28/2025 12:26:48 INFO]: Training loss at epoch 18: 1.178300529718399
[08/28/2025 12:27:02 INFO]: Training loss at epoch 37: 0.8855063915252686
[08/28/2025 12:27:07 INFO]: Training loss at epoch 31: 0.9020958840847015
[08/28/2025 12:27:10 INFO]: Training loss at epoch 30: 1.0142929553985596
[08/28/2025 12:27:21 INFO]: Training loss at epoch 29: 0.9530943334102631
[08/28/2025 12:27:26 INFO]: Training loss at epoch 17: 1.103242814540863
[08/28/2025 12:27:30 INFO]: Running Final Evaluation...
[08/28/2025 12:27:36 INFO]: New best epoch, val score: -0.6862805268255524
[08/28/2025 12:27:36 INFO]: Saving model to: unhanged-Shanesha_trial_162/model_best.pth
[08/28/2025 12:27:38 INFO]: Training loss at epoch 33: 0.8746038675308228
[08/28/2025 12:27:42 INFO]: Training loss at epoch 38: 1.105567455291748
[08/28/2025 12:27:43 INFO]: Training stats: {
    "score": -1.0144148357818672,
    "rmse": 1.0144148357818672
}
[08/28/2025 12:27:43 INFO]: Val stats: {
    "score": -0.7518374088574795,
    "rmse": 0.7518374088574795
}
[08/28/2025 12:27:43 INFO]: Test stats: {
    "score": -0.9159204092293827,
    "rmse": 0.9159204092293827
}
[08/28/2025 12:27:48 INFO]: Training loss at epoch 31: 1.1986403465270996
[08/28/2025 12:27:53 INFO]: Running Final Evaluation...
[08/28/2025 12:27:54 INFO]: Training loss at epoch 0: 0.9966033101081848
[08/28/2025 12:27:56 INFO]: Training loss at epoch 0: 2.484848737716675
[08/28/2025 12:28:01 INFO]: Training loss at epoch 30: 1.163409411907196
[08/28/2025 12:28:08 INFO]: Training loss at epoch 34: 0.8821727633476257
[08/28/2025 12:28:09 INFO]: Training loss at epoch 2: 0.9343482851982117
[08/28/2025 12:28:10 INFO]: Training accuracy: {
    "score": -1.0059844479081452,
    "rmse": 1.0059844479081452
}
[08/28/2025 12:28:10 INFO]: Val accuracy: {
    "score": -0.6760994812700997,
    "rmse": 0.6760994812700997
}
[08/28/2025 12:28:10 INFO]: Test accuracy: {
    "score": -0.8782748343845694,
    "rmse": 0.8782748343845694
}
[08/28/2025 12:28:10 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_163",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8782748343845694,
        "rmse": 0.8782748343845694
    },
    "train_stats": {
        "score": -1.0059844479081452,
        "rmse": 1.0059844479081452
    },
    "val_stats": {
        "score": -0.6760994812700997,
        "rmse": 0.6760994812700997
    }
}
[08/28/2025 12:28:10 INFO]: Procewss finished for trial unhanged-Shanesha_trial_163
[08/28/2025 12:28:10 INFO]: 
_________________________________________________

[08/28/2025 12:28:10 INFO]: train_net_for_optune.py main() running.
[08/28/2025 12:28:10 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 2.0642340157828136
  attention_dropout: 0.32751765940985844
  ffn_dropout: 0.32751765940985844
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.496639986733483e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_168

[08/28/2025 12:28:10 INFO]: This ft_transformer has 15.200 million parameters.
[08/28/2025 12:28:10 INFO]: Training will start at epoch 0.
[08/28/2025 12:28:10 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 12:28:20 INFO]: Training loss at epoch 15: 1.1029296219348907
[08/28/2025 12:28:22 INFO]: Training loss at epoch 39: 1.0561271905899048
[08/28/2025 12:28:35 INFO]: Training stats: {
    "score": -0.9993849836539284,
    "rmse": 0.9993849836539284
}
[08/28/2025 12:28:35 INFO]: Val stats: {
    "score": -0.6823193439306007,
    "rmse": 0.6823193439306007
}
[08/28/2025 12:28:35 INFO]: Test stats: {
    "score": -0.8706914122626812,
    "rmse": 0.8706914122626812
}
[08/28/2025 12:28:36 INFO]: Training loss at epoch 45: 1.1937438547611237
[08/28/2025 12:28:37 INFO]: New best epoch, val score: -0.6736041252311982
[08/28/2025 12:28:37 INFO]: Saving model to: unhanged-Shanesha_trial_157/model_best.pth
[08/28/2025 12:28:41 INFO]: Training loss at epoch 18: 1.087469458580017
[08/28/2025 12:28:43 INFO]: Training loss at epoch 45: 1.283333420753479
[08/28/2025 12:28:44 INFO]: Training loss at epoch 30: 1.037139654159546
[08/28/2025 12:28:50 INFO]: Training accuracy: {
    "score": -1.0440096143338882,
    "rmse": 1.0440096143338882
}
[08/28/2025 12:28:50 INFO]: Val accuracy: {
    "score": -0.6810767437772453,
    "rmse": 0.6810767437772453
}
[08/28/2025 12:28:50 INFO]: Test accuracy: {
    "score": -0.8928489465520614,
    "rmse": 0.8928489465520614
}
[08/28/2025 12:28:50 INFO]: New best epoch, val score: -0.6837459972657074
[08/28/2025 12:28:50 INFO]: Saving model to: unhanged-Shanesha_trial_162/model_best.pth
[08/28/2025 12:28:51 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_151",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8928489465520614,
        "rmse": 0.8928489465520614
    },
    "train_stats": {
        "score": -1.0440096143338882,
        "rmse": 1.0440096143338882
    },
    "val_stats": {
        "score": -0.6810767437772453,
        "rmse": 0.6810767437772453
    }
}
[08/28/2025 12:28:51 INFO]: Procewss finished for trial unhanged-Shanesha_trial_151
[08/28/2025 12:28:51 INFO]: 
_________________________________________________

[08/28/2025 12:28:51 INFO]: train_net_for_optune.py main() running.
[08/28/2025 12:28:51 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 2.138275228753466
  attention_dropout: 0.416241744383703
  ffn_dropout: 0.416241744383703
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.539739248377877e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_169

[08/28/2025 12:28:51 INFO]: This ft_transformer has 15.535 million parameters.
[08/28/2025 12:28:51 INFO]: Training will start at epoch 0.
[08/28/2025 12:28:51 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 12:28:52 INFO]: New best epoch, val score: -0.8670038475151806
[08/28/2025 12:28:52 INFO]: Saving model to: unhanged-Shanesha_trial_165/model_best.pth
[08/28/2025 12:28:54 INFO]: New best epoch, val score: -1.1617090203863334
[08/28/2025 12:28:54 INFO]: Saving model to: unhanged-Shanesha_trial_166/model_best.pth
[08/28/2025 12:29:09 INFO]: Training loss at epoch 42: 0.9788293242454529
[08/28/2025 12:29:13 INFO]: Training loss at epoch 40: 0.8636541068553925
[08/28/2025 12:29:17 INFO]: Training loss at epoch 19: 1.0364821553230286
[08/28/2025 12:29:17 INFO]: Running Final Evaluation...
[08/28/2025 12:29:31 INFO]: Training accuracy: {
    "score": -1.0142510580613797,
    "rmse": 1.0142510580613797
}
[08/28/2025 12:29:31 INFO]: Val accuracy: {
    "score": -0.6761443437370238,
    "rmse": 0.6761443437370238
}
[08/28/2025 12:29:31 INFO]: Test accuracy: {
    "score": -0.8697194555485409,
    "rmse": 0.8697194555485409
}
[08/28/2025 12:29:31 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_161",
    "best_epoch": 9,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8697194555485409,
        "rmse": 0.8697194555485409
    },
    "train_stats": {
        "score": -1.0142510580613797,
        "rmse": 1.0142510580613797
    },
    "val_stats": {
        "score": -0.6761443437370238,
        "rmse": 0.6761443437370238
    }
}
[08/28/2025 12:29:31 INFO]: Procewss finished for trial unhanged-Shanesha_trial_161
[08/28/2025 12:29:32 INFO]: 
_________________________________________________

[08/28/2025 12:29:32 INFO]: train_net_for_optune.py main() running.
[08/28/2025 12:29:32 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 2.0564460508739693
  attention_dropout: 0.4147080029698272
  ffn_dropout: 0.4147080029698272
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.5643399689874272e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_170

[08/28/2025 12:29:32 INFO]: This ft_transformer has 15.165 million parameters.
[08/28/2025 12:29:32 INFO]: Training will start at epoch 0.
[08/28/2025 12:29:32 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 12:29:42 INFO]: Training loss at epoch 31: 0.9300580322742462
[08/28/2025 12:29:51 INFO]: Training loss at epoch 19: 1.28175950050354
[08/28/2025 12:30:02 INFO]: Training stats: {
    "score": -0.9974869201518723,
    "rmse": 0.9974869201518723
}
[08/28/2025 12:30:02 INFO]: Val stats: {
    "score": -0.6846790897750674,
    "rmse": 0.6846790897750674
}
[08/28/2025 12:30:02 INFO]: Test stats: {
    "score": -0.8832338583645074,
    "rmse": 0.8832338583645074
}
[08/28/2025 12:30:16 INFO]: Training stats: {
    "score": -0.99792474938987,
    "rmse": 0.99792474938987
}
[08/28/2025 12:30:16 INFO]: Val stats: {
    "score": -0.6820105115720908,
    "rmse": 0.6820105115720908
}
[08/28/2025 12:30:16 INFO]: Test stats: {
    "score": -0.8745401057840848,
    "rmse": 0.8745401057840848
}
[08/28/2025 12:30:24 INFO]: Training loss at epoch 31: 0.8871332108974457
[08/28/2025 12:30:24 INFO]: New best epoch, val score: -0.6820105115720908
[08/28/2025 12:30:24 INFO]: Saving model to: unhanged-Shanesha_trial_162/model_best.pth
[08/28/2025 12:30:32 INFO]: Training loss at epoch 35: 0.9101650714874268
[08/28/2025 12:30:38 INFO]: Training loss at epoch 32: 0.9587927758693695
[08/28/2025 12:30:39 INFO]: Training loss at epoch 16: 0.929157018661499
[08/28/2025 12:30:44 INFO]: Training loss at epoch 34: 0.9915724098682404
[08/28/2025 12:30:46 INFO]: Training loss at epoch 3: 1.1914015412330627
[08/28/2025 12:30:47 INFO]: Training loss at epoch 0: 1.489843726158142
[08/28/2025 12:30:55 INFO]: New best epoch, val score: -0.670322872511887
[08/28/2025 12:30:55 INFO]: Saving model to: unhanged-Shanesha_trial_157/model_best.pth
[08/28/2025 12:31:24 INFO]: Training loss at epoch 20: 0.9994508028030396
[08/28/2025 12:31:33 INFO]: New best epoch, val score: -0.6805142859600677
[08/28/2025 12:31:33 INFO]: Saving model to: unhanged-Shanesha_trial_162/model_best.pth
[08/28/2025 12:31:34 INFO]: Training loss at epoch 33: 1.152413249015808
[08/28/2025 12:31:40 INFO]: Training loss at epoch 43: 0.8794528245925903
[08/28/2025 12:31:41 INFO]: New best epoch, val score: -0.7659950474216133
[08/28/2025 12:31:41 INFO]: Saving model to: unhanged-Shanesha_trial_167/model_best.pth
[08/28/2025 12:31:55 INFO]: Training loss at epoch 12: 1.0507723689079285
[08/28/2025 12:31:59 INFO]: Training loss at epoch 46: 0.8438116312026978
[08/28/2025 12:32:00 INFO]: Training loss at epoch 46: 0.9307900667190552
[08/28/2025 12:32:20 INFO]: Training loss at epoch 20: 0.864462822675705
[08/28/2025 12:32:25 INFO]: Running Final Evaluation...
[08/28/2025 12:32:31 INFO]: Training loss at epoch 34: 1.013586550951004
[08/28/2025 12:32:33 INFO]: Training loss at epoch 69: 0.8318349719047546
[08/28/2025 12:32:34 INFO]: Training loss at epoch 21: 1.135079801082611
[08/28/2025 12:32:43 INFO]: New best epoch, val score: -0.6799680249511165
[08/28/2025 12:32:43 INFO]: Saving model to: unhanged-Shanesha_trial_162/model_best.pth
[08/28/2025 12:32:46 INFO]: Training loss at epoch 32: 0.9777323603630066
[08/28/2025 12:32:55 INFO]: Training loss at epoch 36: 0.999830961227417
[08/28/2025 12:33:00 INFO]: Training loss at epoch 17: 0.867639422416687
[08/28/2025 12:33:23 INFO]: Training loss at epoch 4: 1.2604367136955261
[08/28/2025 12:33:29 INFO]: Training loss at epoch 35: 1.0374755263328552
[08/28/2025 12:33:29 INFO]: Training loss at epoch 12: 0.9986720681190491
[08/28/2025 12:33:43 INFO]: New best epoch, val score: -0.6855165014047788
[08/28/2025 12:33:43 INFO]: Saving model to: unhanged-Shanesha_trial_164/model_best.pth
[08/28/2025 12:33:47 INFO]: Training loss at epoch 22: 1.1236379742622375
[08/28/2025 12:33:51 INFO]: Training loss at epoch 35: 1.0479637682437897
[08/28/2025 12:33:59 INFO]: Training accuracy: {
    "score": -1.0139097037140157,
    "rmse": 1.0139097037140157
}
[08/28/2025 12:33:59 INFO]: Val accuracy: {
    "score": -0.6600302774883644,
    "rmse": 0.6600302774883644
}
[08/28/2025 12:33:59 INFO]: Test accuracy: {
    "score": -0.8717800480211437,
    "rmse": 0.8717800480211437
}
[08/28/2025 12:33:59 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_139",
    "best_epoch": 15,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8717800480211437,
        "rmse": 0.8717800480211437
    },
    "train_stats": {
        "score": -1.0139097037140157,
        "rmse": 1.0139097037140157
    },
    "val_stats": {
        "score": -0.6600302774883644,
        "rmse": 0.6600302774883644
    }
}
[08/28/2025 12:33:59 INFO]: Procewss finished for trial unhanged-Shanesha_trial_139
[08/28/2025 12:33:59 INFO]: 
_________________________________________________

[08/28/2025 12:33:59 INFO]: train_net_for_optune.py main() running.
[08/28/2025 12:33:59 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 16
  d_ffn_factor: 2.0540272016118375
  attention_dropout: 0.3376913778467866
  ffn_dropout: 0.3376913778467866
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.3672787025177035e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_171

[08/28/2025 12:33:59 INFO]: This ft_transformer has 1.721 million parameters.
[08/28/2025 12:33:59 INFO]: Training will start at epoch 0.
[08/28/2025 12:33:59 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 12:34:23 INFO]: Training loss at epoch 44: 0.9673963487148285
[08/28/2025 12:34:29 INFO]: Training loss at epoch 36: 1.165373295545578
[08/28/2025 12:34:34 INFO]: New best epoch, val score: -0.6843706393347697
[08/28/2025 12:34:34 INFO]: Saving model to: unhanged-Shanesha_trial_149/model_best.pth
[08/28/2025 12:34:38 INFO]: Training stats: {
    "score": -0.9923435997730845,
    "rmse": 0.9923435997730845
}
[08/28/2025 12:34:38 INFO]: Val stats: {
    "score": -0.6841577239686276,
    "rmse": 0.6841577239686276
}
[08/28/2025 12:34:38 INFO]: Test stats: {
    "score": -0.8765325924623865,
    "rmse": 0.8765325924623865
}
[08/28/2025 12:34:53 INFO]: Training loss at epoch 21: 1.153257131576538
[08/28/2025 12:35:02 INFO]: Training loss at epoch 23: 0.9866745471954346
[08/28/2025 12:35:21 INFO]: Training loss at epoch 33: 1.094757318496704
[08/28/2025 12:35:29 INFO]: Training loss at epoch 37: 0.9735257029533386
[08/28/2025 12:35:29 INFO]: Training loss at epoch 0: 0.9079315364360809
[08/28/2025 12:35:32 INFO]: Training loss at epoch 37: 1.091829538345337
[08/28/2025 12:35:35 INFO]: Training loss at epoch 18: 0.957097202539444
[08/28/2025 12:35:37 INFO]: Running Final Evaluation...
[08/28/2025 12:35:44 INFO]: Training loss at epoch 47: 1.0336904227733612
[08/28/2025 12:36:00 INFO]: Training accuracy: {
    "score": -0.9999797994668442,
    "rmse": 0.9999797994668442
}
[08/28/2025 12:36:00 INFO]: Val accuracy: {
    "score": -0.7042796335341418,
    "rmse": 0.7042796335341418
}
[08/28/2025 12:36:00 INFO]: Test accuracy: {
    "score": -0.8860099933090324,
    "rmse": 0.8860099933090324
}
[08/28/2025 12:36:00 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_160",
    "best_epoch": 6,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8860099933090324,
        "rmse": 0.8860099933090324
    },
    "train_stats": {
        "score": -0.9999797994668442,
        "rmse": 0.9999797994668442
    },
    "val_stats": {
        "score": -0.7042796335341418,
        "rmse": 0.7042796335341418
    }
}
[08/28/2025 12:36:00 INFO]: Procewss finished for trial unhanged-Shanesha_trial_160
[08/28/2025 12:36:01 INFO]: 
_________________________________________________

[08/28/2025 12:36:01 INFO]: train_net_for_optune.py main() running.
[08/28/2025 12:36:01 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 1.2175620693289226
  attention_dropout: 0.41427798604966926
  ffn_dropout: 0.41427798604966926
  residual_dropout: 0.0
  activation: gelu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.3634047279046487e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_172

[08/28/2025 12:36:01 INFO]: This ft_transformer has 0.469 million parameters.
[08/28/2025 12:36:01 INFO]: Training will start at epoch 0.
[08/28/2025 12:36:01 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 12:36:06 INFO]: Training loss at epoch 1: 0.99114790558815
[08/28/2025 12:36:09 INFO]: Training loss at epoch 1: 1.533744215965271
[08/28/2025 12:36:15 INFO]: Training loss at epoch 5: 0.9541327953338623
[08/28/2025 12:36:16 INFO]: Training loss at epoch 24: 1.107495367527008
[08/28/2025 12:36:24 INFO]: Training loss at epoch 0: 1.2071750164031982
[08/28/2025 12:36:33 INFO]: New best epoch, val score: -0.8044102976883958
[08/28/2025 12:36:33 INFO]: Saving model to: unhanged-Shanesha_trial_168/model_best.pth
[08/28/2025 12:36:35 INFO]: New best epoch, val score: -0.6852889358932139
[08/28/2025 12:36:35 INFO]: Saving model to: unhanged-Shanesha_trial_164/model_best.pth
[08/28/2025 12:36:39 INFO]: Training loss at epoch 0: 1.0083238780498505
[08/28/2025 12:36:44 INFO]: New best epoch, val score: -0.6764281654907777
[08/28/2025 12:36:44 INFO]: Saving model to: unhanged-Shanesha_trial_172/model_best.pth
[08/28/2025 12:36:58 INFO]: Training loss at epoch 0: 1.1755861043930054
[08/28/2025 12:37:07 INFO]: New best epoch, val score: -0.6724834412984789
[08/28/2025 12:37:07 INFO]: Saving model to: unhanged-Shanesha_trial_165/model_best.pth
[08/28/2025 12:37:08 INFO]: Training loss at epoch 0: 1.0455366969108582
[08/28/2025 12:37:11 INFO]: New best epoch, val score: -0.6761624412172771
[08/28/2025 12:37:11 INFO]: Training loss at epoch 45: 0.8427653908729553
[08/28/2025 12:37:11 INFO]: Saving model to: unhanged-Shanesha_trial_166/model_best.pth
[08/28/2025 12:37:13 INFO]: Training loss at epoch 36: 1.0146489441394806
[08/28/2025 12:37:23 INFO]: Training loss at epoch 1: 1.007235050201416
[08/28/2025 12:37:28 INFO]: Training loss at epoch 22: 0.965660959482193
[08/28/2025 12:37:28 INFO]: New best epoch, val score: -0.6640874256476538
[08/28/2025 12:37:28 INFO]: Saving model to: unhanged-Shanesha_trial_172/model_best.pth
[08/28/2025 12:37:29 INFO]: New best epoch, val score: -0.9106134443041723
[08/28/2025 12:37:29 INFO]: Saving model to: unhanged-Shanesha_trial_169/model_best.pth
[08/28/2025 12:37:32 INFO]: Training loss at epoch 25: 0.8957818448543549
[08/28/2025 12:37:37 INFO]: New best epoch, val score: -0.7108718652011027
[08/28/2025 12:37:37 INFO]: Saving model to: unhanged-Shanesha_trial_171/model_best.pth
[08/28/2025 12:37:37 INFO]: Running Final Evaluation...
[08/28/2025 12:37:53 INFO]: Training loss at epoch 34: 0.9678188860416412
[08/28/2025 12:37:59 INFO]: New best epoch, val score: -0.7252919366533532
[08/28/2025 12:37:59 INFO]: Saving model to: unhanged-Shanesha_trial_170/model_best.pth
[08/28/2025 12:38:03 INFO]: Training loss at epoch 2: 1.0057335197925568
[08/28/2025 12:38:05 INFO]: Training loss at epoch 19: 0.8735474348068237
[08/28/2025 12:38:06 INFO]: Training loss at epoch 38: 1.0236416459083557
[08/28/2025 12:38:08 INFO]: New best epoch, val score: -0.6566181040352542
[08/28/2025 12:38:08 INFO]: Saving model to: unhanged-Shanesha_trial_172/model_best.pth
[08/28/2025 12:38:10 INFO]: Running Final Evaluation...
[08/28/2025 12:38:41 INFO]: Training loss at epoch 26: 1.047575056552887
[08/28/2025 12:38:43 INFO]: Training loss at epoch 3: 0.8988375067710876
[08/28/2025 12:38:47 INFO]: New best epoch, val score: -0.6552284650567692
[08/28/2025 12:38:47 INFO]: Saving model to: unhanged-Shanesha_trial_172/model_best.pth
[08/28/2025 12:38:51 INFO]: Training stats: {
    "score": -0.9974252696283277,
    "rmse": 0.9974252696283277
}
[08/28/2025 12:38:51 INFO]: Val stats: {
    "score": -0.6725945890680415,
    "rmse": 0.6725945890680415
}
[08/28/2025 12:38:51 INFO]: Test stats: {
    "score": -0.865953851461937,
    "rmse": 0.865953851461937
}
[08/28/2025 12:38:53 INFO]: Training accuracy: {
    "score": -1.003682717665526,
    "rmse": 1.003682717665526
}
[08/28/2025 12:38:53 INFO]: Val accuracy: {
    "score": -0.6588860035904646,
    "rmse": 0.6588860035904646
}
[08/28/2025 12:38:53 INFO]: Test accuracy: {
    "score": -0.8912541980634621,
    "rmse": 0.8912541980634621
}
[08/28/2025 12:38:53 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_148",
    "best_epoch": 5,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8912541980634621,
        "rmse": 0.8912541980634621
    },
    "train_stats": {
        "score": -1.003682717665526,
        "rmse": 1.003682717665526
    },
    "val_stats": {
        "score": -0.6588860035904646,
        "rmse": 0.6588860035904646
    }
}
[08/28/2025 12:38:53 INFO]: Procewss finished for trial unhanged-Shanesha_trial_148
[08/28/2025 12:38:53 INFO]: 
_________________________________________________

[08/28/2025 12:38:53 INFO]: train_net_for_optune.py main() running.
[08/28/2025 12:38:53 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 4
  n_heads: 8
  d_ffn_factor: 2.0504699273580878
  attention_dropout: 0.1807580241800518
  ffn_dropout: 0.1807580241800518
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.866011968101818e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_173

[08/28/2025 12:38:53 INFO]: This ft_transformer has 6.133 million parameters.
[08/28/2025 12:38:53 INFO]: Training will start at epoch 0.
[08/28/2025 12:38:53 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 12:38:58 INFO]: Training loss at epoch 6: 1.2326643466949463
[08/28/2025 12:39:05 INFO]: Training loss at epoch 1: 1.2577413320541382
[08/28/2025 12:39:06 INFO]: Training accuracy: {
    "score": -1.0079079580741452,
    "rmse": 1.0079079580741452
}
[08/28/2025 12:39:06 INFO]: Val accuracy: {
    "score": -0.6667345257955043,
    "rmse": 0.6667345257955043
}
[08/28/2025 12:39:06 INFO]: Test accuracy: {
    "score": -0.8732736395405425,
    "rmse": 0.8732736395405425
}
[08/28/2025 12:39:06 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_155",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8732736395405425,
        "rmse": 0.8732736395405425
    },
    "train_stats": {
        "score": -1.0079079580741452,
        "rmse": 1.0079079580741452
    },
    "val_stats": {
        "score": -0.6667345257955043,
        "rmse": 0.6667345257955043
    }
}
[08/28/2025 12:39:06 INFO]: Procewss finished for trial unhanged-Shanesha_trial_155
[08/28/2025 12:39:07 INFO]: 
_________________________________________________

[08/28/2025 12:39:07 INFO]: train_net_for_optune.py main() running.
[08/28/2025 12:39:07 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 2.071999301245981
  attention_dropout: 0.18448540932641513
  ffn_dropout: 0.18448540932641513
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.6166798191614086e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_174

[08/28/2025 12:39:07 INFO]: This ft_transformer has 15.235 million parameters.
[08/28/2025 12:39:07 INFO]: Training will start at epoch 0.
[08/28/2025 12:39:07 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 12:39:15 INFO]: New best epoch, val score: -0.6852262964028076
[08/28/2025 12:39:15 INFO]: Saving model to: unhanged-Shanesha_trial_164/model_best.pth
[08/28/2025 12:39:19 INFO]: Training loss at epoch 48: 1.089557707309723
[08/28/2025 12:39:22 INFO]: Training loss at epoch 4: 0.9127790629863739
[08/28/2025 12:39:46 INFO]: Training loss at epoch 46: 1.2292115092277527
[08/28/2025 12:39:47 INFO]: Training loss at epoch 23: 1.015855848789215
[08/28/2025 12:39:49 INFO]: Training loss at epoch 27: 1.033412516117096
[08/28/2025 12:40:01 INFO]: Training loss at epoch 5: 1.1289421916007996
[08/28/2025 12:40:20 INFO]: Training loss at epoch 13: 0.8640760183334351
[08/28/2025 12:40:26 INFO]: Training loss at epoch 39: 0.9377026855945587
[08/28/2025 12:40:29 INFO]: Training loss at epoch 1: 1.2095001339912415
[08/28/2025 12:40:34 INFO]: Training loss at epoch 70: 0.9206322133541107
[08/28/2025 12:40:40 INFO]: Training loss at epoch 6: 0.8288058638572693
[08/28/2025 12:40:54 INFO]: New best epoch, val score: -0.6965499375490433
[08/28/2025 12:40:54 INFO]: Saving model to: unhanged-Shanesha_trial_171/model_best.pth
[08/28/2025 12:40:57 INFO]: Training loss at epoch 28: 1.1535161137580872
[08/28/2025 12:41:07 INFO]: Training loss at epoch 20: 1.2602280974388123
[08/28/2025 12:41:13 INFO]: Training stats: {
    "score": -0.9954782771118738,
    "rmse": 0.9954782771118738
}
[08/28/2025 12:41:13 INFO]: Val stats: {
    "score": -0.6672995581730474,
    "rmse": 0.6672995581730474
}
[08/28/2025 12:41:13 INFO]: Test stats: {
    "score": -0.8740760492251621,
    "rmse": 0.8740760492251621
}
[08/28/2025 12:41:18 INFO]: Training loss at epoch 0: 1.0206808745861053
[08/28/2025 12:41:19 INFO]: Training loss at epoch 7: 1.2030648589134216
[08/28/2025 12:41:32 INFO]: Training loss at epoch 7: 1.1418646574020386
[08/28/2025 12:41:39 INFO]: New best epoch, val score: -0.6965106942194286
[08/28/2025 12:41:39 INFO]: Saving model to: unhanged-Shanesha_trial_173/model_best.pth
[08/28/2025 12:41:52 INFO]: New best epoch, val score: -0.6851832036845757
[08/28/2025 12:41:52 INFO]: Saving model to: unhanged-Shanesha_trial_164/model_best.pth
[08/28/2025 12:42:04 INFO]: Training loss at epoch 8: 1.127684772014618
[08/28/2025 12:42:09 INFO]: Training loss at epoch 24: 1.1533455848693848
[08/28/2025 12:42:11 INFO]: Training loss at epoch 29: 1.1411305665969849
[08/28/2025 12:42:14 INFO]: Training loss at epoch 13: 0.8782419264316559
[08/28/2025 12:42:22 INFO]: Training loss at epoch 47: 1.1400155425071716
[08/28/2025 12:42:39 INFO]: Training stats: {
    "score": -0.9972185198732405,
    "rmse": 0.9972185198732405
}
[08/28/2025 12:42:39 INFO]: Val stats: {
    "score": -0.6975597936411865,
    "rmse": 0.6975597936411865
}
[08/28/2025 12:42:39 INFO]: Test stats: {
    "score": -0.8836610378288172,
    "rmse": 0.8836610378288172
}
[08/28/2025 12:42:46 INFO]: Training loss at epoch 9: 1.0991695523262024
[08/28/2025 12:42:50 INFO]: Training loss at epoch 49: 1.1021853983402252
[08/28/2025 12:43:01 INFO]: Training stats: {
    "score": -1.0058055757376474,
    "rmse": 1.0058055757376474
}
[08/28/2025 12:43:01 INFO]: Val stats: {
    "score": -0.6569769963229203,
    "rmse": 0.6569769963229203
}
[08/28/2025 12:43:01 INFO]: Test stats: {
    "score": -0.871195578714687,
    "rmse": 0.871195578714687
}
[08/28/2025 12:43:38 INFO]: Training loss at epoch 21: 0.8226787447929382
[08/28/2025 12:43:43 INFO]: Training loss at epoch 10: 1.0511452555656433
[08/28/2025 12:43:47 INFO]: Training loss at epoch 40: 1.0375073552131653
[08/28/2025 12:43:52 INFO]: Training loss at epoch 1: 0.957989901304245
[08/28/2025 12:43:53 INFO]: Training loss at epoch 30: 1.1167288422584534
[08/28/2025 12:43:59 INFO]: Training loss at epoch 2: 0.9679993689060211
[08/28/2025 12:44:08 INFO]: Training stats: {
    "score": -1.0003846767201516,
    "rmse": 1.0003846767201516
}
[08/28/2025 12:44:08 INFO]: Val stats: {
    "score": -0.6685205242520358,
    "rmse": 0.6685205242520358
}
[08/28/2025 12:44:08 INFO]: Test stats: {
    "score": -0.8692896397891414,
    "rmse": 0.8692896397891414
}
[08/28/2025 12:44:21 INFO]: Training loss at epoch 1: 1.015960454940796
[08/28/2025 12:44:22 INFO]: Training loss at epoch 2: 1.186728298664093
[08/28/2025 12:44:23 INFO]: Training loss at epoch 8: 1.3904480338096619
[08/28/2025 12:44:25 INFO]: Training loss at epoch 2: 1.017558515071869
[08/28/2025 12:44:27 INFO]: New best epoch, val score: -0.6836840892466816
[08/28/2025 12:44:27 INFO]: Saving model to: unhanged-Shanesha_trial_171/model_best.pth
[08/28/2025 12:44:27 INFO]: Training loss at epoch 11: 1.037154734134674
[08/28/2025 12:44:42 INFO]: Training loss at epoch 25: 0.8986455202102661
[08/28/2025 12:44:43 INFO]: New best epoch, val score: -0.6704173638475769
[08/28/2025 12:44:43 INFO]: Saving model to: unhanged-Shanesha_trial_173/model_best.pth
[08/28/2025 12:44:44 INFO]: New best epoch, val score: -0.6851262474800446
[08/28/2025 12:44:44 INFO]: Saving model to: unhanged-Shanesha_trial_164/model_best.pth
[08/28/2025 12:44:54 INFO]: New best epoch, val score: -0.7562967931279136
[08/28/2025 12:44:54 INFO]: Saving model to: unhanged-Shanesha_trial_168/model_best.pth
[08/28/2025 12:44:59 INFO]: Training loss at epoch 1: 1.0778872966766357
[08/28/2025 12:45:08 INFO]: Training loss at epoch 31: 0.997364342212677
[08/28/2025 12:45:10 INFO]: Training loss at epoch 48: 0.9218251407146454
[08/28/2025 12:45:11 INFO]: Training loss at epoch 12: 1.1628310084342957
[08/28/2025 12:45:20 INFO]: Training loss at epoch 1: 0.8855211138725281
[08/28/2025 12:45:55 INFO]: Training loss at epoch 13: 0.907447874546051
[08/28/2025 12:46:05 INFO]: New best epoch, val score: -0.6881107077693925
[08/28/2025 12:46:05 INFO]: Saving model to: unhanged-Shanesha_trial_169/model_best.pth
[08/28/2025 12:46:12 INFO]: Training loss at epoch 22: 0.9504013061523438
[08/28/2025 12:46:23 INFO]: Training loss at epoch 32: 0.8956412672996521
[08/28/2025 12:46:24 INFO]: New best epoch, val score: -0.7186533554357685
[08/28/2025 12:46:24 INFO]: Saving model to: unhanged-Shanesha_trial_170/model_best.pth
[08/28/2025 12:46:25 INFO]: Training loss at epoch 41: 1.0975181460380554
[08/28/2025 12:46:38 INFO]: Training loss at epoch 14: 1.1154696941375732
[08/28/2025 12:46:42 INFO]: Training loss at epoch 0: 0.959271639585495
[08/28/2025 12:46:45 INFO]: Training loss at epoch 71: 1.1062907576560974
[08/28/2025 12:47:13 INFO]: Training loss at epoch 9: 1.1198124885559082
[08/28/2025 12:47:14 INFO]: Training loss at epoch 26: 1.0907503962516785
[08/28/2025 12:47:19 INFO]: Training loss at epoch 15: 1.0485065579414368
[08/28/2025 12:47:23 INFO]: Training loss at epoch 2: 1.156731903553009
[08/28/2025 12:47:28 INFO]: Training loss at epoch 2: 0.9577341079711914
[08/28/2025 12:47:32 INFO]: Training loss at epoch 3: 1.2142820954322815
[08/28/2025 12:47:35 INFO]: Training loss at epoch 33: 0.8165567517280579
[08/28/2025 12:47:41 INFO]: New best epoch, val score: -0.7233358962763552
[08/28/2025 12:47:41 INFO]: Saving model to: unhanged-Shanesha_trial_174/model_best.pth
[08/28/2025 12:47:43 INFO]: New best epoch, val score: -0.669483604949679
[08/28/2025 12:47:43 INFO]: Saving model to: unhanged-Shanesha_trial_173/model_best.pth
[08/28/2025 12:47:50 INFO]: Training loss at epoch 50: 0.8677974343299866
[08/28/2025 12:47:54 INFO]: Training loss at epoch 49: 0.8386886417865753
[08/28/2025 12:47:58 INFO]: New best epoch, val score: -0.6820218604592531
[08/28/2025 12:47:58 INFO]: Saving model to: unhanged-Shanesha_trial_171/model_best.pth
[08/28/2025 12:47:59 INFO]: Training loss at epoch 16: 1.028732717037201
[08/28/2025 12:48:04 INFO]: Training stats: {
    "score": -1.002085600534589,
    "rmse": 1.002085600534589
}
[08/28/2025 12:48:04 INFO]: Val stats: {
    "score": -0.6853019024593461,
    "rmse": 0.6853019024593461
}
[08/28/2025 12:48:04 INFO]: Test stats: {
    "score": -0.8774108669609035,
    "rmse": 0.8774108669609035
}
[08/28/2025 12:48:35 INFO]: Training loss at epoch 23: 0.9384176433086395
[08/28/2025 12:48:38 INFO]: Training loss at epoch 17: 0.9698048532009125
[08/28/2025 12:48:43 INFO]: Training loss at epoch 34: 0.8947574496269226
[08/28/2025 12:48:45 INFO]: Training stats: {
    "score": -1.0064733571011177,
    "rmse": 1.0064733571011177
}
[08/28/2025 12:48:45 INFO]: Val stats: {
    "score": -0.6532049202784702,
    "rmse": 0.6532049202784702
}
[08/28/2025 12:48:45 INFO]: Test stats: {
    "score": -0.8747525968436481,
    "rmse": 0.8747525968436481
}
[08/28/2025 12:48:49 INFO]: Training loss at epoch 42: 0.9664163291454315
[08/28/2025 12:48:49 INFO]: Training loss at epoch 14: 0.9754810333251953
[08/28/2025 12:49:17 INFO]: Training loss at epoch 18: 0.8684531450271606
[08/28/2025 12:49:32 INFO]: Training loss at epoch 27: 1.1353267431259155
[08/28/2025 12:49:44 INFO]: New best epoch, val score: -0.6741518457843707
[08/28/2025 12:49:44 INFO]: Saving model to: unhanged-Shanesha_trial_150/model_best.pth
[08/28/2025 12:49:52 INFO]: Training loss at epoch 35: 0.9568848013877869
[08/28/2025 12:49:57 INFO]: Training loss at epoch 19: 1.058645248413086
[08/28/2025 12:50:09 INFO]: Training loss at epoch 3: 0.9181172251701355
[08/28/2025 12:50:11 INFO]: Training stats: {
    "score": -1.0003805914562582,
    "rmse": 1.0003805914562582
}
[08/28/2025 12:50:11 INFO]: Val stats: {
    "score": -0.6657081796565565,
    "rmse": 0.6657081796565565
}
[08/28/2025 12:50:11 INFO]: Test stats: {
    "score": -0.8729407228004489,
    "rmse": 0.8729407228004489
}
[08/28/2025 12:50:37 INFO]: Training loss at epoch 10: 0.8815401792526245
[08/28/2025 12:50:51 INFO]: Training loss at epoch 4: 0.9315696060657501
[08/28/2025 12:50:52 INFO]: Training loss at epoch 20: 1.1401143074035645
[08/28/2025 12:50:53 INFO]: Training loss at epoch 24: 1.0327652096748352
[08/28/2025 12:50:55 INFO]: Training loss at epoch 14: 1.3397957384586334
[08/28/2025 12:51:01 INFO]: Training loss at epoch 36: 1.3717460334300995
[08/28/2025 12:51:11 INFO]: Training loss at epoch 43: 1.040010929107666
[08/28/2025 12:51:16 INFO]: Training loss at epoch 51: 0.9155685305595398
[08/28/2025 12:51:18 INFO]: Training loss at epoch 50: 0.9315429627895355
[08/28/2025 12:51:33 INFO]: Training loss at epoch 21: 1.3229358792304993
[08/28/2025 12:51:43 INFO]: Running Final Evaluation...
[08/28/2025 12:51:57 INFO]: Training loss at epoch 28: 1.1017895340919495
[08/28/2025 12:52:13 INFO]: Training loss at epoch 2: 1.1751049757003784
[08/28/2025 12:52:15 INFO]: Training loss at epoch 37: 1.2182150781154633
[08/28/2025 12:52:15 INFO]: Training loss at epoch 22: 0.8815604448318481
[08/28/2025 12:52:34 INFO]: Training loss at epoch 3: 0.9084155559539795
[08/28/2025 12:52:35 INFO]: Training loss at epoch 72: 0.877528190612793
[08/28/2025 12:52:36 INFO]: Training loss at epoch 3: 1.4802268743515015
[08/28/2025 12:52:57 INFO]: Training loss at epoch 23: 1.0512838959693909
[08/28/2025 12:53:05 INFO]: Training loss at epoch 4: 0.9099957346916199
[08/28/2025 12:53:15 INFO]: New best epoch, val score: -0.7511067978773079
[08/28/2025 12:53:15 INFO]: Saving model to: unhanged-Shanesha_trial_168/model_best.pth
[08/28/2025 12:53:21 INFO]: Training accuracy: {
    "score": -1.0085686117872215,
    "rmse": 1.0085686117872215
}
[08/28/2025 12:53:21 INFO]: Val accuracy: {
    "score": -0.6613634948764058,
    "rmse": 0.6613634948764058
}
[08/28/2025 12:53:21 INFO]: Test accuracy: {
    "score": -0.8694357813260879,
    "rmse": 0.8694357813260879
}
[08/28/2025 12:53:21 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_138",
    "best_epoch": 20,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8694357813260879,
        "rmse": 0.8694357813260879
    },
    "train_stats": {
        "score": -1.0085686117872215,
        "rmse": 1.0085686117872215
    },
    "val_stats": {
        "score": -0.6613634948764058,
        "rmse": 0.6613634948764058
    }
}
[08/28/2025 12:53:21 INFO]: Procewss finished for trial unhanged-Shanesha_trial_138
[08/28/2025 12:53:22 INFO]: 
_________________________________________________

[08/28/2025 12:53:22 INFO]: train_net_for_optune.py main() running.
[08/28/2025 12:53:22 INFO]: model:
  name: ft_transformer
  d_embedding: 384
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 6
  n_heads: 8
  d_ffn_factor: 2.078906632547281
  attention_dropout: 0.1869367852942794
  ffn_dropout: 0.1869367852942794
  residual_dropout: 0.0
  activation: reglu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.608978214795619e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_175

[08/28/2025 12:53:22 INFO]: This ft_transformer has 9.213 million parameters.
[08/28/2025 12:53:22 INFO]: Training will start at epoch 0.
[08/28/2025 12:53:22 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 12:53:24 INFO]: Training loss at epoch 25: 1.0599178075790405
[08/28/2025 12:53:25 INFO]: Training loss at epoch 11: 1.27138352394104
[08/28/2025 12:53:29 INFO]: Training loss at epoch 38: 0.8841186463832855
[08/28/2025 12:53:35 INFO]: Training loss at epoch 2: 1.1012751460075378
[08/28/2025 12:53:40 INFO]: Training loss at epoch 24: 0.9901948571205139
[08/28/2025 12:53:44 INFO]: Training loss at epoch 2: 0.888176679611206
[08/28/2025 12:53:47 INFO]: Training loss at epoch 44: 1.268487811088562
[08/28/2025 12:54:06 INFO]: Training loss at epoch 51: 1.2127495110034943
[08/28/2025 12:54:22 INFO]: Training loss at epoch 5: 1.0137081742286682
[08/28/2025 12:54:23 INFO]: Training loss at epoch 25: 0.9722486138343811
[08/28/2025 12:54:30 INFO]: Training loss at epoch 29: 1.0960914492607117
[08/28/2025 12:54:44 INFO]: Training loss at epoch 39: 0.9741585850715637
[08/28/2025 12:55:06 INFO]: Training loss at epoch 26: 1.1017287373542786
[08/28/2025 12:55:09 INFO]: Training loss at epoch 1: 1.0300436615943909
[08/28/2025 12:55:11 INFO]: Training stats: {
    "score": -0.995119359781949,
    "rmse": 0.995119359781949
}
[08/28/2025 12:55:11 INFO]: Val stats: {
    "score": -0.695352370773759,
    "rmse": 0.695352370773759
}
[08/28/2025 12:55:11 INFO]: Test stats: {
    "score": -0.8834201537157891,
    "rmse": 0.8834201537157891
}
[08/28/2025 12:55:21 INFO]: Training stats: {
    "score": -0.9980628796187442,
    "rmse": 0.9980628796187442
}
[08/28/2025 12:55:21 INFO]: Val stats: {
    "score": -0.6932148930561397,
    "rmse": 0.6932148930561397
}
[08/28/2025 12:55:21 INFO]: Test stats: {
    "score": -0.8873732216158406,
    "rmse": 0.8873732216158406
}
[08/28/2025 12:55:42 INFO]: Training loss at epoch 3: 1.3671002388000488
[08/28/2025 12:55:48 INFO]: Training loss at epoch 27: 1.063870906829834
[08/28/2025 12:55:55 INFO]: Training loss at epoch 26: 1.2076165974140167
[08/28/2025 12:56:06 INFO]: Training loss at epoch 5: 0.7689503282308578
[08/28/2025 12:56:11 INFO]: New best epoch, val score: -0.7223806564167302
[08/28/2025 12:56:11 INFO]: Saving model to: unhanged-Shanesha_trial_174/model_best.pth
[08/28/2025 12:56:16 INFO]: New best epoch, val score: -0.6697352765160784
[08/28/2025 12:56:16 INFO]: Saving model to: unhanged-Shanesha_trial_157/model_best.pth
[08/28/2025 12:56:18 INFO]: Training loss at epoch 12: 1.0068676471710205
[08/28/2025 12:56:25 INFO]: Training loss at epoch 45: 0.9748919308185577
[08/28/2025 12:56:27 INFO]: Training loss at epoch 40: 1.0265063643455505
[08/28/2025 12:56:32 INFO]: Training loss at epoch 28: 1.1062430143356323
[08/28/2025 12:56:51 INFO]: Training loss at epoch 52: 1.0459753274917603
[08/28/2025 12:57:12 INFO]: Training loss at epoch 29: 0.9057125449180603
[08/28/2025 12:57:15 INFO]: Training loss at epoch 15: 0.8626261353492737
[08/28/2025 12:57:26 INFO]: Training stats: {
    "score": -1.004724590859558,
    "rmse": 1.004724590859558
}
[08/28/2025 12:57:26 INFO]: Val stats: {
    "score": -0.6580602645486666,
    "rmse": 0.6580602645486666
}
[08/28/2025 12:57:26 INFO]: Test stats: {
    "score": -0.8711952367460816,
    "rmse": 0.8711952367460816
}
[08/28/2025 12:57:36 INFO]: Training loss at epoch 41: 0.8391751646995544
[08/28/2025 12:57:41 INFO]: Training loss at epoch 0: 1.186528205871582
[08/28/2025 12:57:45 INFO]: Training loss at epoch 30: 0.8833675384521484
[08/28/2025 12:57:49 INFO]: Training loss at epoch 6: 0.9157052636146545
[08/28/2025 12:58:05 INFO]: Training loss at epoch 30: 0.9994946420192719
[08/28/2025 12:58:13 INFO]: New best epoch, val score: -0.7519851783176422
[08/28/2025 12:58:13 INFO]: Saving model to: unhanged-Shanesha_trial_175/model_best.pth
[08/28/2025 12:58:19 INFO]: Training loss at epoch 27: 0.9691375494003296
[08/28/2025 12:58:33 INFO]: Training loss at epoch 73: 1.238011211156845
[08/28/2025 12:58:44 INFO]: Training loss at epoch 42: 1.158688247203827
[08/28/2025 12:58:44 INFO]: Training loss at epoch 31: 0.919130802154541
[08/28/2025 12:58:45 INFO]: Training loss at epoch 46: 0.7984777092933655
[08/28/2025 12:58:51 INFO]: Training loss at epoch 13: 0.869367927312851
[08/28/2025 12:58:54 INFO]: Training loss at epoch 6: 0.8427941203117371
[08/28/2025 12:59:23 INFO]: Training loss at epoch 53: 0.9713484644889832
[08/28/2025 12:59:25 INFO]: Training loss at epoch 32: 0.9413250386714935
[08/28/2025 12:59:39 INFO]: Training loss at epoch 15: 1.1061433851718903
[08/28/2025 12:59:54 INFO]: Training loss at epoch 43: 0.9693019390106201
[08/28/2025 13:00:05 INFO]: Training loss at epoch 33: 0.9808496236801147
[08/28/2025 13:00:06 INFO]: Training loss at epoch 31: 1.0793704390525818
[08/28/2025 13:00:24 INFO]: Running Final Evaluation...
[08/28/2025 13:00:36 INFO]: Training loss at epoch 3: 1.0539074838161469
[08/28/2025 13:00:40 INFO]: Training loss at epoch 28: 0.9257725179195404
[08/28/2025 13:00:46 INFO]: Training loss at epoch 34: 1.0207043290138245
[08/28/2025 13:00:48 INFO]: Training loss at epoch 4: 1.1715917587280273
[08/28/2025 13:00:49 INFO]: Training loss at epoch 4: 1.5915162563323975
[08/28/2025 13:00:51 INFO]: Running Final Evaluation...
[08/28/2025 13:01:06 INFO]: Training loss at epoch 44: 0.9326740205287933
[08/28/2025 13:01:11 INFO]: Training accuracy: {
    "score": -1.0097896331915537,
    "rmse": 1.0097896331915537
}
[08/28/2025 13:01:11 INFO]: Val accuracy: {
    "score": -0.6552284650567692,
    "rmse": 0.6552284650567692
}
[08/28/2025 13:01:11 INFO]: Test accuracy: {
    "score": -0.8723204616908055,
    "rmse": 0.8723204616908055
}
[08/28/2025 13:01:11 INFO]: Training loss at epoch 7: 0.8587065637111664
[08/28/2025 13:01:11 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_172",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8723204616908055,
        "rmse": 0.8723204616908055
    },
    "train_stats": {
        "score": -1.0097896331915537,
        "rmse": 1.0097896331915537
    },
    "val_stats": {
        "score": -0.6552284650567692,
        "rmse": 0.6552284650567692
    }
}
[08/28/2025 13:01:11 INFO]: Procewss finished for trial unhanged-Shanesha_trial_172
[08/28/2025 13:01:12 INFO]: 
_________________________________________________

[08/28/2025 13:01:12 INFO]: train_net_for_optune.py main() running.
[08/28/2025 13:01:12 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 1.2118428522272298
  attention_dropout: 0.42320072693858857
  ffn_dropout: 0.42320072693858857
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.3924914093784207e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_176

[08/28/2025 13:01:13 INFO]: This ft_transformer has 0.895 million parameters.
[08/28/2025 13:01:13 INFO]: Training will start at epoch 0.
[08/28/2025 13:01:13 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 13:01:13 INFO]: Training loss at epoch 47: 0.9821779727935791
[08/28/2025 13:01:32 INFO]: Running Final Evaluation...
[08/28/2025 13:01:33 INFO]: Training loss at epoch 14: 1.0305436849594116
[08/28/2025 13:01:37 INFO]: New best epoch, val score: -0.718277000846285
[08/28/2025 13:01:37 INFO]: Saving model to: unhanged-Shanesha_trial_168/model_best.pth
[08/28/2025 13:01:43 INFO]: Training accuracy: {
    "score": -1.04883160572625,
    "rmse": 1.04883160572625
}
[08/28/2025 13:01:43 INFO]: Val accuracy: {
    "score": -0.6745949755570764,
    "rmse": 0.6745949755570764
}
[08/28/2025 13:01:43 INFO]: Test accuracy: {
    "score": -0.8984994101212417,
    "rmse": 0.8984994101212417
}
[08/28/2025 13:01:43 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_156",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8984994101212417,
        "rmse": 0.8984994101212417
    },
    "train_stats": {
        "score": -1.04883160572625,
        "rmse": 1.04883160572625
    },
    "val_stats": {
        "score": -0.6745949755570764,
        "rmse": 0.6745949755570764
    }
}
[08/28/2025 13:01:43 INFO]: Procewss finished for trial unhanged-Shanesha_trial_156
[08/28/2025 13:01:43 INFO]: 
_________________________________________________

[08/28/2025 13:01:43 INFO]: train_net_for_optune.py main() running.
[08/28/2025 13:01:43 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 2.112047226194691
  attention_dropout: 0.41894314076322925
  ffn_dropout: 0.41894314076322925
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.5618076520148208e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_177

[08/28/2025 13:01:43 INFO]: This ft_transformer has 1.131 million parameters.
[08/28/2025 13:01:43 INFO]: Training will start at epoch 0.
[08/28/2025 13:01:43 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 13:01:48 INFO]: Training loss at epoch 7: 0.8033514618873596
[08/28/2025 13:02:02 INFO]: Training loss at epoch 54: 0.8320765495300293
[08/28/2025 13:02:05 INFO]: Training loss at epoch 3: 0.9287804365158081
[08/28/2025 13:02:08 INFO]: Training loss at epoch 3: 1.4194684624671936
[08/28/2025 13:02:20 INFO]: Training loss at epoch 45: 1.127139389514923
[08/28/2025 13:02:27 INFO]: Training loss at epoch 1: 1.0076416432857513
[08/28/2025 13:02:36 INFO]: Training loss at epoch 0: 1.1133304238319397
[08/28/2025 13:02:48 INFO]: New best epoch, val score: -0.6717305244872518
[08/28/2025 13:02:48 INFO]: Saving model to: unhanged-Shanesha_trial_176/model_best.pth
[08/28/2025 13:02:51 INFO]: Training accuracy: {
    "score": -1.0102690611210772,
    "rmse": 1.0102690611210772
}
[08/28/2025 13:02:51 INFO]: Val accuracy: {
    "score": -0.6571325906371238,
    "rmse": 0.6571325906371238
}
[08/28/2025 13:02:51 INFO]: Test accuracy: {
    "score": -0.8741648522923252,
    "rmse": 0.8741648522923252
}
[08/28/2025 13:02:51 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_154",
    "best_epoch": 16,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8741648522923252,
        "rmse": 0.8741648522923252
    },
    "train_stats": {
        "score": -1.0102690611210772,
        "rmse": 1.0102690611210772
    },
    "val_stats": {
        "score": -0.6571325906371238,
        "rmse": 0.6571325906371238
    }
}
[08/28/2025 13:02:51 INFO]: Procewss finished for trial unhanged-Shanesha_trial_154
[08/28/2025 13:02:51 INFO]: 
_________________________________________________

[08/28/2025 13:02:51 INFO]: train_net_for_optune.py main() running.
[08/28/2025 13:02:51 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 1.2214691189014533
  attention_dropout: 0.12979116348101608
  ffn_dropout: 0.12979116348101608
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.5791991604770417e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_178

[08/28/2025 13:02:51 INFO]: This ft_transformer has 1.110 million parameters.
[08/28/2025 13:02:51 INFO]: Training will start at epoch 0.
[08/28/2025 13:02:51 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 13:03:04 INFO]: New best epoch, val score: -0.6807342855210101
[08/28/2025 13:03:04 INFO]: Saving model to: unhanged-Shanesha_trial_175/model_best.pth
[08/28/2025 13:03:11 INFO]: Training loss at epoch 29: 0.8801766037940979
[08/28/2025 13:03:16 INFO]: Training loss at epoch 0: 1.3227882385253906
[08/28/2025 13:03:30 INFO]: New best epoch, val score: -0.6865591849957827
[08/28/2025 13:03:30 INFO]: Saving model to: unhanged-Shanesha_trial_177/model_best.pth
[08/28/2025 13:03:35 INFO]: Training loss at epoch 46: 0.9639283418655396
[08/28/2025 13:03:38 INFO]: Training loss at epoch 2: 1.0078666508197784
[08/28/2025 13:03:57 INFO]: Training loss at epoch 4: 1.3171441555023193
[08/28/2025 13:04:03 INFO]: Training stats: {
    "score": -0.9952608832925841,
    "rmse": 0.9952608832925841
}
[08/28/2025 13:04:03 INFO]: Val stats: {
    "score": -0.6781359503202333,
    "rmse": 0.6781359503202333
}
[08/28/2025 13:04:03 INFO]: Test stats: {
    "score": -0.8685003863466969,
    "rmse": 0.8685003863466969
}
[08/28/2025 13:04:13 INFO]: Training loss at epoch 1: 1.1556051671504974
[08/28/2025 13:04:24 INFO]: Training loss at epoch 15: 0.8432437777519226
[08/28/2025 13:04:39 INFO]: Training loss at epoch 74: 1.0378921627998352
[08/28/2025 13:04:39 INFO]: Training loss at epoch 0: 1.1009144186973572
[08/28/2025 13:04:43 INFO]: New best epoch, val score: -0.6777105785670897
[08/28/2025 13:04:43 INFO]: Saving model to: unhanged-Shanesha_trial_174/model_best.pth
[08/28/2025 13:04:48 INFO]: Training loss at epoch 8: 1.113760232925415
[08/28/2025 13:04:53 INFO]: Training loss at epoch 47: 0.8183982968330383
[08/28/2025 13:04:54 INFO]: Training loss at epoch 8: 0.9187017381191254
[08/28/2025 13:04:54 INFO]: Training loss at epoch 55: 1.0286552906036377
[08/28/2025 13:04:58 INFO]: New best epoch, val score: -0.7495615122800047
[08/28/2025 13:04:58 INFO]: Saving model to: unhanged-Shanesha_trial_178/model_best.pth
[08/28/2025 13:05:05 INFO]: Training loss at epoch 1: 1.2730479836463928
[08/28/2025 13:05:39 INFO]: Training loss at epoch 16: 0.9957600235939026
[08/28/2025 13:05:50 INFO]: Training loss at epoch 2: 1.2102269530296326
[08/28/2025 13:06:04 INFO]: Training loss at epoch 48: 1.134044110774994
[08/28/2025 13:06:31 INFO]: Training loss at epoch 30: 0.968906968832016
[08/28/2025 13:06:39 INFO]: Training loss at epoch 1: 1.2118349075317383
[08/28/2025 13:06:45 INFO]: Training loss at epoch 2: 0.9626139998435974
[08/28/2025 13:06:54 INFO]: New best epoch, val score: -0.6955119986338556
[08/28/2025 13:06:54 INFO]: Saving model to: unhanged-Shanesha_trial_178/model_best.pth
[08/28/2025 13:07:06 INFO]: Training loss at epoch 16: 1.0042790472507477
[08/28/2025 13:07:13 INFO]: Training loss at epoch 49: 0.8700076341629028
[08/28/2025 13:07:21 INFO]: Training loss at epoch 3: 1.2077561616897583
[08/28/2025 13:07:22 INFO]: Training loss at epoch 2: 1.100014090538025
[08/28/2025 13:07:28 INFO]: Training loss at epoch 56: 0.947760134935379
[08/28/2025 13:07:37 INFO]: Training stats: {
    "score": -0.9932753946717937,
    "rmse": 0.9932753946717937
}
[08/28/2025 13:07:37 INFO]: Val stats: {
    "score": -0.6851981428084102,
    "rmse": 0.6851981428084102
}
[08/28/2025 13:07:37 INFO]: Test stats: {
    "score": -0.8790651088673336,
    "rmse": 0.8790651088673336
}
[08/28/2025 13:07:41 INFO]: Training loss at epoch 9: 0.890171080827713
[08/28/2025 13:08:08 INFO]: Training loss at epoch 9: 1.2608201801776886
[08/28/2025 13:08:23 INFO]: Training loss at epoch 3: 1.1285486221313477
[08/28/2025 13:08:24 INFO]: Training loss at epoch 16: 0.9424593448638916
[08/28/2025 13:08:33 INFO]: Training loss at epoch 2: 1.0252736806869507
[08/28/2025 13:08:35 INFO]: Training stats: {
    "score": -0.9801411693677751,
    "rmse": 0.9801411693677751
}
[08/28/2025 13:08:35 INFO]: Val stats: {
    "score": -0.7063158775874369,
    "rmse": 0.7063158775874369
}
[08/28/2025 13:08:35 INFO]: Test stats: {
    "score": -0.8903538810070921,
    "rmse": 0.8903538810070921
}
[08/28/2025 13:08:47 INFO]: Training loss at epoch 50: 1.0119668841362
[08/28/2025 13:08:49 INFO]: New best epoch, val score: -0.6617347993271854
[08/28/2025 13:08:49 INFO]: Saving model to: unhanged-Shanesha_trial_178/model_best.pth
[08/28/2025 13:08:51 INFO]: Training loss at epoch 31: 0.9455808103084564
[08/28/2025 13:08:53 INFO]: Training loss at epoch 4: 1.1065005660057068
[08/28/2025 13:09:02 INFO]: Training loss at epoch 4: 1.1344965100288391
[08/28/2025 13:09:07 INFO]: Training loss at epoch 5: 1.0681647658348083
[08/28/2025 13:09:07 INFO]: Training loss at epoch 5: 1.5334086418151855
[08/28/2025 13:09:24 INFO]: Training stats: {
    "score": -0.9867732833108409,
    "rmse": 0.9867732833108409
}
[08/28/2025 13:09:24 INFO]: Val stats: {
    "score": -0.7020303689170498,
    "rmse": 0.7020303689170498
}
[08/28/2025 13:09:24 INFO]: Test stats: {
    "score": -0.8874378001427102,
    "rmse": 0.8874378001427102
}
[08/28/2025 13:09:46 INFO]: Training loss at epoch 17: 0.8657188713550568
[08/28/2025 13:10:01 INFO]: Training loss at epoch 51: 1.0432453751564026
[08/28/2025 13:10:06 INFO]: Training loss at epoch 4: 1.1285986304283142
[08/28/2025 13:10:09 INFO]: Training loss at epoch 57: 1.224731296300888
[08/28/2025 13:10:30 INFO]: Training loss at epoch 5: 1.0244321823120117
[08/28/2025 13:10:34 INFO]: Training loss at epoch 4: 0.9783715009689331
[08/28/2025 13:10:36 INFO]: Training loss at epoch 3: 1.2126253247261047
[08/28/2025 13:10:38 INFO]: Training loss at epoch 75: 1.066333144903183
[08/28/2025 13:10:47 INFO]: Training loss at epoch 4: 1.0570127964019775
[08/28/2025 13:11:17 INFO]: Training loss at epoch 52: 0.9874556660652161
[08/28/2025 13:11:22 INFO]: Running Final Evaluation...
[08/28/2025 13:11:23 INFO]: Training loss at epoch 32: 1.1674412488937378
[08/28/2025 13:11:28 INFO]: Running Final Evaluation...
[08/28/2025 13:11:37 INFO]: Training loss at epoch 10: 0.8629733026027679
[08/28/2025 13:11:50 INFO]: Training loss at epoch 5: 0.7799095064401627
[08/28/2025 13:11:56 INFO]: Training accuracy: {
    "score": -0.9978994302348161,
    "rmse": 0.9978994302348161
}
[08/28/2025 13:11:56 INFO]: Val accuracy: {
    "score": -0.6799680249511165,
    "rmse": 0.6799680249511165
}
[08/28/2025 13:11:56 INFO]: Test accuracy: {
    "score": -0.8739156755071652,
    "rmse": 0.8739156755071652
}
[08/28/2025 13:11:56 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_162",
    "best_epoch": 21,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8739156755071652,
        "rmse": 0.8739156755071652
    },
    "train_stats": {
        "score": -0.9978994302348161,
        "rmse": 0.9978994302348161
    },
    "val_stats": {
        "score": -0.6799680249511165,
        "rmse": 0.6799680249511165
    }
}
[08/28/2025 13:11:56 INFO]: Procewss finished for trial unhanged-Shanesha_trial_162
[08/28/2025 13:11:56 INFO]: 
_________________________________________________

[08/28/2025 13:11:56 INFO]: train_net_for_optune.py main() running.
[08/28/2025 13:11:56 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 10
  n_heads: 8
  d_ffn_factor: 2.0534471779795767
  attention_dropout: 0.4274968932733809
  ffn_dropout: 0.4274968932733809
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.5916154613086153e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_179

[08/28/2025 13:11:56 INFO]: This ft_transformer has 1.383 million parameters.
[08/28/2025 13:11:56 INFO]: Training will start at epoch 0.
[08/28/2025 13:11:56 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 13:12:08 INFO]: Training loss at epoch 6: 1.0313507318496704
[08/28/2025 13:12:13 INFO]: Training loss at epoch 3: 1.0132412910461426
[08/28/2025 13:12:14 INFO]: Training loss at epoch 3: 0.900905579328537
[08/28/2025 13:12:15 INFO]: Training loss at epoch 5: 1.343458116054535
[08/28/2025 13:12:35 INFO]: Training loss at epoch 18: 1.0029321014881134
[08/28/2025 13:12:37 INFO]: Training loss at epoch 4: 1.1131721138954163
[08/28/2025 13:12:56 INFO]: Training loss at epoch 58: 0.9999796450138092
[08/28/2025 13:12:57 INFO]: Training loss at epoch 10: 0.926109105348587
[08/28/2025 13:13:36 INFO]: Training loss at epoch 6: 0.9382714927196503
[08/28/2025 13:13:44 INFO]: Training loss at epoch 7: 1.0323743522167206
[08/28/2025 13:13:52 INFO]: Training loss at epoch 33: 1.1221959292888641
[08/28/2025 13:13:53 INFO]: Training loss at epoch 0: 1.419808268547058
[08/28/2025 13:14:02 INFO]: Training loss at epoch 17: 0.8593103885650635
[08/28/2025 13:14:02 INFO]: Training accuracy: {
    "score": -1.0102686035324095,
    "rmse": 1.0102686035324095
}
[08/28/2025 13:14:02 INFO]: Val accuracy: {
    "score": -0.6593203707273079,
    "rmse": 0.6593203707273079
}
[08/28/2025 13:14:02 INFO]: Test accuracy: {
    "score": -0.8710184558230747,
    "rmse": 0.8710184558230747
}
[08/28/2025 13:14:02 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_105",
    "best_epoch": 44,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8710184558230747,
        "rmse": 0.8710184558230747
    },
    "train_stats": {
        "score": -1.0102686035324095,
        "rmse": 1.0102686035324095
    },
    "val_stats": {
        "score": -0.6593203707273079,
        "rmse": 0.6593203707273079
    }
}
[08/28/2025 13:14:02 INFO]: Procewss finished for trial unhanged-Shanesha_trial_105
[08/28/2025 13:14:03 INFO]: 
_________________________________________________

[08/28/2025 13:14:03 INFO]: train_net_for_optune.py main() running.
[08/28/2025 13:14:03 INFO]: model:
  name: ft_transformer
  d_embedding: 128
  model_path: null
  use_mlp_head: false
  freeze_feature_extractor: false
  token_bias: true
  n_layers: 8
  n_heads: 8
  d_ffn_factor: 2.0659278296721797
  attention_dropout: 0.28133839438691055
  ffn_dropout: 0.28133839438691055
  residual_dropout: 0.0
  activation: relu
  prenormalization: true
  initialization: kaiming
  kv_compression: null
  kv_compression_sharing: null
dataset:
  name: ic_upstream3_ImputacaoEstatistica_exp_100_1
  source: local
  task: regression
  normalization: quantile
  normalizer_path: ../../../data/ic_upstream3_ImputacaoEstatistica_exp_100_1/normalizer.pkl
  stage: pretrain
  y_policy: mean_std
hyp:
  epochs: 200
  lr: 1.7072556858589752e-05
  lr_decay: step
  lr_factor: 0.1
  lr_schedule:
  - 40
  - 80
  optimizer: adam
  patience: 30
  save_period: 100000000.0
  seed: 0
  test_batch_size: 256
  train_batch_size: 256
  use_patience: true
  val_period: 10
  warmup_period: 5
  weight_decay: 0.0002
  momentum: 0.9
  warmup_type: linear
  head_warmup_period: 10
  head_lr: 0.001
train_log: train_log
name: from_scratch_optuna
run_id: unhanged-Shanesha_trial_180

[08/28/2025 13:14:03 INFO]: This ft_transformer has 1.119 million parameters.
[08/28/2025 13:14:03 INFO]: Training will start at epoch 0.
[08/28/2025 13:14:03 INFO]: ==> Starting training for 200 epochs...
[08/28/2025 13:14:08 INFO]: New best epoch, val score: -0.780992514559285
[08/28/2025 13:14:08 INFO]: Saving model to: unhanged-Shanesha_trial_179/model_best.pth
[08/28/2025 13:14:31 INFO]: Training loss at epoch 11: 0.8633568286895752
[08/28/2025 13:14:35 INFO]: Training loss at epoch 5: 1.0914894342422485
[08/28/2025 13:15:13 INFO]: Training loss at epoch 7: 1.0981791019439697
[08/28/2025 13:15:14 INFO]: Training loss at epoch 8: 0.9123185575008392
[08/28/2025 13:15:14 INFO]: Training loss at epoch 19: 1.1092928349971771
[08/28/2025 13:15:27 INFO]: Training loss at epoch 0: 1.199505627155304
[08/28/2025 13:15:30 INFO]: Training loss at epoch 59: 1.0847834944725037
[08/28/2025 13:15:38 INFO]: New best epoch, val score: -0.6602989969580659
[08/28/2025 13:15:38 INFO]: Saving model to: unhanged-Shanesha_trial_180/model_best.pth
[08/28/2025 13:15:56 INFO]: Training loss at epoch 1: 1.1835418343544006
[08/28/2025 13:16:05 INFO]: Training stats: {
    "score": -0.992678404598417,
    "rmse": 0.992678404598417
}
[08/28/2025 13:16:05 INFO]: Val stats: {
    "score": -0.6859850514892785,
    "rmse": 0.6859850514892785
}
[08/28/2025 13:16:05 INFO]: Test stats: {
    "score": -0.8786713810109258,
    "rmse": 0.8786713810109258
}
[08/28/2025 13:16:09 INFO]: Training loss at epoch 34: 1.2263067662715912
[08/28/2025 13:16:11 INFO]: New best epoch, val score: -0.7181679737531618
[08/28/2025 13:16:11 INFO]: Saving model to: unhanged-Shanesha_trial_179/model_best.pth
[08/28/2025 13:16:18 INFO]: Training loss at epoch 11: 1.0045494735240936
[08/28/2025 13:16:21 INFO]: Training stats: {
    "score": -0.9958343484375287,
    "rmse": 0.9958343484375287
}
[08/28/2025 13:16:21 INFO]: Val stats: {
    "score": -0.6631396031591864,
    "rmse": 0.6631396031591864
}
[08/28/2025 13:16:21 INFO]: Test stats: {
    "score": -0.8758165955257107,
    "rmse": 0.8758165955257107
}
[08/28/2025 13:16:27 INFO]: Training loss at epoch 6: 1.0283013582229614
[08/28/2025 13:16:43 INFO]: Training loss at epoch 9: 1.2305713891983032
[08/28/2025 13:16:50 INFO]: Training loss at epoch 4: 0.9778391420841217
[08/28/2025 13:16:52 INFO]: Training loss at epoch 8: 1.0571828484535217
[08/28/2025 13:17:01 INFO]: Training loss at epoch 1: 1.0317168533802032
[08/28/2025 13:17:02 INFO]: Training loss at epoch 17: 0.8711289167404175
[08/28/2025 13:17:15 INFO]: Training stats: {
    "score": -1.0112602846771626,
    "rmse": 1.0112602846771626
}
[08/28/2025 13:17:15 INFO]: Val stats: {
    "score": -0.7343054875579518,
    "rmse": 0.7343054875579518
}
[08/28/2025 13:17:15 INFO]: Test stats: {
    "score": -0.9047407010110332,
    "rmse": 0.9047407010110332
}
[08/28/2025 13:17:15 INFO]: Training loss at epoch 12: 0.9342862367630005
[08/28/2025 13:17:19 INFO]: Training loss at epoch 6: 1.1555865406990051
[08/28/2025 13:17:21 INFO]: Training loss at epoch 6: 1.3050510585308075
[08/28/2025 13:17:22 INFO]: Training loss at epoch 5: 1.2197960019111633
[08/28/2025 13:18:01 INFO]: Training loss at epoch 2: 1.0831445455551147
[08/28/2025 13:18:18 INFO]: New best epoch, val score: -0.6639087596751013
[08/28/2025 13:18:18 INFO]: Saving model to: unhanged-Shanesha_trial_179/model_best.pth
[08/28/2025 13:18:25 INFO]: Training loss at epoch 7: 1.4285150170326233
[08/28/2025 13:18:33 INFO]: Training loss at epoch 35: 1.2274432182312012
[08/28/2025 13:18:35 INFO]: Training loss at epoch 9: 1.266771674156189
[08/28/2025 13:18:43 INFO]: Training loss at epoch 2: 0.9374089241027832
[08/28/2025 13:18:47 INFO]: Training loss at epoch 20: 1.0492483973503113
[08/28/2025 13:18:51 INFO]: Training loss at epoch 10: 1.0608885884284973
[08/28/2025 13:19:02 INFO]: Training loss at epoch 60: 1.0319934487342834
[08/28/2025 13:19:04 INFO]: Training loss at epoch 5: 0.9101910889148712
[08/28/2025 13:19:12 INFO]: Training stats: {
    "score": -1.0595895512275073,
    "rmse": 1.0595895512275073
}
[08/28/2025 13:19:12 INFO]: Val stats: {
    "score": -0.8335872509281607,
    "rmse": 0.8335872509281607
}
[08/28/2025 13:19:12 INFO]: Test stats: {
    "score": -0.9757927044200951,
    "rmse": 0.9757927044200951
}
[08/28/2025 13:19:17 INFO]: Training loss at epoch 5: 0.9722362756729126
[08/28/2025 13:19:44 INFO]: Training loss at epoch 12: 1.3577941060066223
[08/28/2025 13:20:16 INFO]: Training loss at epoch 3: 1.1656650304794312
[08/28/2025 13:20:18 INFO]: Training loss at epoch 13: 0.8875347077846527
[08/28/2025 13:20:26 INFO]: Training loss at epoch 3: 0.9817200303077698
[08/28/2025 13:20:28 INFO]: Training loss at epoch 6: 0.9294367730617523
[08/28/2025 13:20:28 INFO]: Training loss at epoch 8: 0.9548913538455963
[08/28/2025 13:20:28 INFO]: Training loss at epoch 11: 1.0343636274337769
[08/28/2025 13:20:51 INFO]: Training loss at epoch 4: 0.9425911605358124
[08/28/2025 13:21:00 INFO]: Training loss at epoch 10: 0.9403495788574219
[08/28/2025 13:21:07 INFO]: Training loss at epoch 36: 1.1933181285858154
[08/28/2025 13:21:39 INFO]: Training loss at epoch 21: 1.0637924671173096
[08/28/2025 13:21:46 INFO]: Training loss at epoch 5: 0.8080859184265137
[08/28/2025 13:21:50 INFO]: Training loss at epoch 61: 0.9039379954338074
[08/28/2025 13:22:05 INFO]: Training loss at epoch 12: 1.1643821001052856
[08/28/2025 13:22:09 INFO]: Training loss at epoch 4: 1.1828612685203552
[08/28/2025 13:22:28 INFO]: Training loss at epoch 18: 1.0916913747787476
[08/28/2025 13:22:31 INFO]: Training loss at epoch 9: 1.0806292295455933
[08/28/2025 13:22:33 INFO]: Training loss at epoch 4: 0.9844696521759033
[08/28/2025 13:22:45 INFO]: Training loss at epoch 11: 1.0270503461360931
[08/28/2025 13:23:12 INFO]: Training stats: {
    "score": -1.019234614175143,
    "rmse": 1.019234614175143
}
[08/28/2025 13:23:12 INFO]: Val stats: {
    "score": -0.7537282291405961,
    "rmse": 0.7537282291405961
}
[08/28/2025 13:23:12 INFO]: Test stats: {
    "score": -0.9184083705755846,
    "rmse": 0.9184083705755846
}
[08/28/2025 13:23:15 INFO]: Training loss at epoch 14: 0.9741857647895813
[08/28/2025 13:23:22 INFO]: Training loss at epoch 13: 0.9072839021682739
[08/28/2025 13:23:32 INFO]: Training loss at epoch 37: 1.019625723361969
[08/28/2025 13:23:37 INFO]: Training loss at epoch 13: 0.8518072664737701
[08/28/2025 13:23:46 INFO]: Training loss at epoch 5: 1.2995511889457703
[08/28/2025 13:23:48 INFO]: New best epoch, val score: -0.6691741107429944
[08/28/2025 13:23:48 INFO]: Saving model to: unhanged-Shanesha_trial_157/model_best.pth
[08/28/2025 13:24:17 INFO]: Training loss at epoch 22: 0.8672903776168823
[08/28/2025 13:24:23 INFO]: Training loss at epoch 12: 1.1759824752807617
[08/28/2025 13:24:26 INFO]: Training loss at epoch 62: 0.8998180627822876
[08/28/2025 13:24:37 INFO]: Training loss at epoch 5: 1.0688945055007935
[08/28/2025 13:25:04 INFO]: Training loss at epoch 10: 1.0177852511405945
[08/28/2025 13:25:07 INFO]: Training loss at epoch 14: 0.9446717500686646
[08/28/2025 13:25:20 INFO]: Training loss at epoch 6: 1.0812271237373352
[08/28/2025 13:25:37 INFO]: Training loss at epoch 7: 0.9175826013088226
[08/28/2025 13:25:39 INFO]: Training loss at epoch 7: 1.6216323375701904
[08/28/2025 13:25:43 INFO]: Training loss at epoch 18: 0.9451470971107483
[08/28/2025 13:25:47 INFO]: Training loss at epoch 6: 1.3204096555709839
[08/28/2025 13:25:49 INFO]: Training loss at epoch 38: 1.0020496547222137
[08/28/2025 13:25:59 INFO]: Training loss at epoch 15: 0.8231127858161926
[08/28/2025 13:26:01 INFO]: Training loss at epoch 13: 1.2915230989456177
[08/28/2025 13:26:23 INFO]: Training loss at epoch 6: 0.9284960031509399
[08/28/2025 13:26:38 INFO]: Training loss at epoch 15: 1.1194828748703003
[08/28/2025 13:26:43 INFO]: Training loss at epoch 14: 0.8906274437904358
[08/28/2025 13:26:44 INFO]: Training loss at epoch 6: 1.276374876499176
[08/28/2025 13:26:55 INFO]: Training loss at epoch 23: 0.971789687871933
[08/28/2025 13:26:59 INFO]: Training loss at epoch 7: 1.42563796043396
[08/28/2025 13:27:01 INFO]: Training loss at epoch 11: 1.2895134091377258
[08/28/2025 13:27:02 INFO]: Training loss at epoch 63: 1.079815685749054
[08/28/2025 13:27:29 INFO]: Training loss at epoch 6: 1.0777264833450317
[08/28/2025 13:27:46 INFO]: Training loss at epoch 14: 0.916485995054245
[08/28/2025 13:27:53 INFO]: Training loss at epoch 6: 0.8092254102230072
[08/28/2025 13:28:18 INFO]: Training loss at epoch 16: 1.0051868259906769
[08/28/2025 13:28:19 INFO]: Training loss at epoch 39: 1.002615213394165
[08/28/2025 13:28:42 INFO]: Training loss at epoch 8: 1.2193801403045654
[08/28/2025 13:28:43 INFO]: Training loss at epoch 7: 0.9602914154529572
[08/28/2025 13:28:58 INFO]: Training loss at epoch 7: 1.009319692850113
[08/28/2025 13:29:01 INFO]: Training loss at epoch 16: 0.9564661979675293
[08/28/2025 13:29:04 INFO]: Training loss at epoch 12: 1.1109097003936768
[08/28/2025 13:29:11 INFO]: Training stats: {
    "score": -0.9943422524321845,
    "rmse": 0.9943422524321845
}
[08/28/2025 13:29:11 INFO]: Val stats: {
    "score": -0.6866499320501814,
    "rmse": 0.6866499320501814
}
[08/28/2025 13:29:11 INFO]: Test stats: {
    "score": -0.8724180689079036,
    "rmse": 0.8724180689079036
}
[08/28/2025 13:29:21 INFO]: Training loss at epoch 5: 1.275104582309723
[08/28/2025 13:29:33 INFO]: Training loss at epoch 15: 0.8971178829669952
[08/28/2025 13:29:47 INFO]: Training loss at epoch 24: 1.0157861113548279
[08/28/2025 13:29:51 INFO]: Training loss at epoch 64: 0.8956005573272705
[08/28/2025 13:29:56 INFO]: Training loss at epoch 17: 0.8639761507511139
[08/28/2025 13:30:23 INFO]: Training loss at epoch 15: 1.0169842839241028
[08/28/2025 13:30:26 INFO]: Training loss at epoch 9: 0.9986469447612762
[08/28/2025 13:30:52 INFO]: Training loss at epoch 19: 0.9187370538711548
[08/28/2025 13:31:03 INFO]: Training stats: {
    "score": -1.0197004566898475,
    "rmse": 1.0197004566898475
}
[08/28/2025 13:31:03 INFO]: Val stats: {
    "score": -0.7497093543503044,
    "rmse": 0.7497093543503044
}
[08/28/2025 13:31:03 INFO]: Test stats: {
    "score": -0.9170576044672519,
    "rmse": 0.9170576044672519
}
[08/28/2025 13:31:07 INFO]: Training loss at epoch 13: 1.0745073556900024
[08/28/2025 13:31:13 INFO]: Training loss at epoch 8: 1.0458318591117859
[08/28/2025 13:31:19 INFO]: Training loss at epoch 16: 1.1594734191894531
[08/28/2025 13:31:21 INFO]: Training loss at epoch 7: 1.154861032962799
[08/28/2025 13:31:34 INFO]: Training loss at epoch 18: 0.9613958597183228
[08/28/2025 13:31:41 INFO]: Training loss at epoch 40: 1.118412733078003
[08/28/2025 13:31:58 INFO]: Training loss at epoch 17: 0.9034034013748169
[08/28/2025 13:32:29 INFO]: Training loss at epoch 25: 1.0761557817459106
[08/28/2025 13:32:30 INFO]: Training loss at epoch 65: 0.975149542093277
[08/28/2025 13:32:39 INFO]: Training loss at epoch 10: 0.9623948633670807
[08/28/2025 13:32:56 INFO]: Training loss at epoch 17: 1.124311923980713
[08/28/2025 13:33:01 INFO]: Training loss at epoch 14: 1.1560428142547607
[08/28/2025 13:33:07 INFO]: Training loss at epoch 19: 0.8297593891620636
[08/28/2025 13:33:22 INFO]: Training loss at epoch 9: 1.130012333393097
[08/28/2025 13:33:34 INFO]: Training stats: {
    "score": -0.9623240703305478,
    "rmse": 0.9623240703305478
}
[08/28/2025 13:33:34 INFO]: Val stats: {
    "score": -0.7179186837608037,
    "rmse": 0.7179186837608037
}
[08/28/2025 13:33:34 INFO]: Test stats: {
    "score": -0.9385733939192428,
    "rmse": 0.9385733939192428
}
[08/28/2025 13:33:39 INFO]: Training stats: {
    "score": -0.9992892022896701,
    "rmse": 0.9992892022896701
}
[08/28/2025 13:33:39 INFO]: Val stats: {
    "score": -0.6865259995962025,
    "rmse": 0.6865259995962025
}
[08/28/2025 13:33:39 INFO]: Test stats: {
    "score": -0.8762300122414829,
    "rmse": 0.8762300122414829
}
[08/28/2025 13:33:53 INFO]: Training loss at epoch 16: 0.8995094895362854
[08/28/2025 13:33:55 INFO]: Training loss at epoch 8: 1.8105405569076538
[08/28/2025 13:33:55 INFO]: Training loss at epoch 8: 0.9859479367733002
[08/28/2025 13:33:58 INFO]: Training loss at epoch 41: 0.9205152988433838
[08/28/2025 13:34:06 INFO]: Training stats: {
    "score": -1.0506461135933698,
    "rmse": 1.0506461135933698
}
[08/28/2025 13:34:06 INFO]: Val stats: {
    "score": -0.823158305465103,
    "rmse": 0.823158305465103
}
[08/28/2025 13:34:06 INFO]: Test stats: {
    "score": -0.9717278622042712,
    "rmse": 0.9717278622042712
}
[08/28/2025 13:34:13 INFO]: Training loss at epoch 11: 1.1162768602371216
[08/28/2025 13:34:17 INFO]: Training loss at epoch 7: 1.0576788783073425
[08/28/2025 13:34:23 INFO]: Training loss at epoch 19: 0.9642302393913269
[08/28/2025 13:34:32 INFO]: Training loss at epoch 18: 1.007961392402649
[08/28/2025 13:34:43 INFO]: Training loss at epoch 18: 0.8429641723632812
[08/28/2025 13:34:53 INFO]: Training loss at epoch 15: 0.9503733217716217
[08/28/2025 13:35:03 INFO]: Training loss at epoch 66: 1.0457189083099365
[08/28/2025 13:35:04 INFO]: Training loss at epoch 26: 0.9072898328304291
[08/28/2025 13:35:09 INFO]: Training loss at epoch 20: 1.0761793851852417
[08/28/2025 13:35:52 INFO]: Training loss at epoch 12: 1.2668243646621704
[08/28/2025 13:35:52 INFO]: Training loss at epoch 7: 0.9629215896129608
[08/28/2025 13:35:55 INFO]: Training loss at epoch 8: 0.9532521069049835
[08/28/2025 13:36:16 INFO]: Training loss at epoch 19: 1.0165949165821075
[08/28/2025 13:36:16 INFO]: Training loss at epoch 10: 1.1138178706169128
[08/28/2025 13:36:23 INFO]: Training loss at epoch 42: 1.0596871376037598
[08/28/2025 13:36:28 INFO]: Training loss at epoch 7: 1.1470958590507507
[08/28/2025 13:36:46 INFO]: Training loss at epoch 21: 0.9974686205387115
[08/28/2025 13:36:53 INFO]: Training stats: {
    "score": -1.0416779555340094,
    "rmse": 1.0416779555340094
}
[08/28/2025 13:36:53 INFO]: Val stats: {
    "score": -0.7987506655381177,
    "rmse": 0.7987506655381177
}
[08/28/2025 13:36:53 INFO]: Test stats: {
    "score": -0.95024109730182,
    "rmse": 0.95024109730182
}
[08/28/2025 13:36:56 INFO]: Training loss at epoch 16: 0.9829831123352051
[08/28/2025 13:37:01 INFO]: Training loss at epoch 8: 1.167291909456253
[08/28/2025 13:37:20 INFO]: Training stats: {
    "score": -0.9785014281996,
    "rmse": 0.9785014281996
}
[08/28/2025 13:37:20 INFO]: Val stats: {
    "score": -0.7520158834622059,
    "rmse": 0.7520158834622059
}
[08/28/2025 13:37:20 INFO]: Test stats: {
    "score": -0.9352142370109054,
    "rmse": 0.9352142370109054
}
[08/28/2025 13:37:23 INFO]: Training loss at epoch 17: 0.9875210225582123
[08/28/2025 13:37:35 INFO]: Training loss at epoch 13: 1.0034810304641724
[08/28/2025 13:37:45 INFO]: Training loss at epoch 19: 1.2202251553535461
[08/28/2025 13:37:49 INFO]: Training loss at epoch 6: 1.1134668588638306
[08/28/2025 13:37:50 INFO]: Training loss at epoch 67: 1.1230234503746033
[08/28/2025 13:37:54 INFO]: Training loss at epoch 27: 0.9636133313179016
[08/28/2025 13:38:25 INFO]: Training loss at epoch 22: 1.2383161783218384
[08/28/2025 13:38:32 INFO]: Training loss at epoch 11: 1.1273334622383118
[08/28/2025 13:38:40 INFO]: Training loss at epoch 20: 0.8895595669746399
[08/28/2025 13:38:46 INFO]: Training stats: {
    "score": -0.9680781547292814,
    "rmse": 0.9680781547292814
}
[08/28/2025 13:38:46 INFO]: Val stats: {
    "score": -0.6973853231129233,
    "rmse": 0.6973853231129233
}
[08/28/2025 13:38:46 INFO]: Test stats: {
    "score": -0.8887548892427791,
    "rmse": 0.8887548892427791
}
[08/28/2025 13:38:58 INFO]: Training loss at epoch 43: 1.0814091563224792
[08/28/2025 13:38:59 INFO]: Training loss at epoch 17: 0.8076568841934204
[08/28/2025 13:39:19 INFO]: Training loss at epoch 14: 0.9731223285198212
[08/28/2025 13:40:03 INFO]: Training loss at epoch 23: 1.1907017529010773
[08/28/2025 13:40:23 INFO]: Training loss at epoch 21: 0.9249032437801361
[08/28/2025 13:40:34 INFO]: Training loss at epoch 68: 0.9062825441360474
[08/28/2025 13:40:41 INFO]: Training loss at epoch 28: 0.8795401453971863
[08/28/2025 13:40:43 INFO]: Training loss at epoch 12: 1.0225825607776642
[08/28/2025 13:40:52 INFO]: Training loss at epoch 9: 1.0916392505168915
[08/28/2025 13:40:52 INFO]: Running Final Evaluation...
[08/28/2025 13:40:55 INFO]: Training loss at epoch 18: 0.8596272766590118
[08/28/2025 13:40:57 INFO]: Training loss at epoch 18: 0.9663656055927277
[08/28/2025 13:40:57 INFO]: Training loss at epoch 15: 0.8150516450405121
[08/28/2025 13:41:21 INFO]: Training loss at epoch 44: 1.0125255286693573
[08/28/2025 13:41:32 INFO]: Training loss at epoch 24: 1.0883296728134155
[08/28/2025 13:41:38 INFO]: Training loss at epoch 20: 0.9084897339344025
[08/28/2025 13:41:52 INFO]: Training accuracy: {
    "score": -1.0113016871678298,
    "rmse": 1.0113016871678298
}
[08/28/2025 13:41:52 INFO]: Val accuracy: {
    "score": -0.6528626510796882,
    "rmse": 0.6528626510796882
}
[08/28/2025 13:41:52 INFO]: Test accuracy: {
    "score": -0.8760829881561681,
    "rmse": 0.8760829881561681
}
[08/28/2025 13:41:53 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_147",
    "best_epoch": 37,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8760829881561681,
        "rmse": 0.8760829881561681
    },
    "train_stats": {
        "score": -1.0113016871678298,
        "rmse": 1.0113016871678298
    },
    "val_stats": {
        "score": -0.6528626510796882,
        "rmse": 0.6528626510796882
    }
}
[08/28/2025 13:41:53 INFO]: Procewss finished for trial unhanged-Shanesha_trial_147
[08/28/2025 13:41:59 INFO]: Training loss at epoch 22: 1.0641468167304993
[08/28/2025 13:41:59 INFO]: Training loss at epoch 20: 0.830333411693573
[08/28/2025 13:42:11 INFO]: Training loss at epoch 9: 1.4441417455673218
[08/28/2025 13:42:13 INFO]: Training loss at epoch 9: 0.8428106606006622
[08/28/2025 13:42:22 INFO]: Training stats: {
    "score": -0.9782066133142528,
    "rmse": 0.9782066133142528
}
[08/28/2025 13:42:22 INFO]: Val stats: {
    "score": -0.7121894988884834,
    "rmse": 0.7121894988884834
}
[08/28/2025 13:42:22 INFO]: Test stats: {
    "score": -0.9208327246874716,
    "rmse": 0.9208327246874716
}
[08/28/2025 13:42:31 INFO]: Training loss at epoch 16: 1.0657173991203308
[08/28/2025 13:42:41 INFO]: Training loss at epoch 8: 1.180550754070282
[08/28/2025 13:42:46 INFO]: Training loss at epoch 13: 1.1303566694259644
[08/28/2025 13:42:49 INFO]: Training loss at epoch 19: 1.1653152704238892
[08/28/2025 13:43:01 INFO]: Training loss at epoch 25: 0.8645170331001282
[08/28/2025 13:43:13 INFO]: Training loss at epoch 29: 1.0579264163970947
[08/28/2025 13:43:29 INFO]: Training stats: {
    "score": -0.9996281156681263,
    "rmse": 0.9996281156681263
}
[08/28/2025 13:43:29 INFO]: Val stats: {
    "score": -0.6774696954264796,
    "rmse": 0.6774696954264796
}
[08/28/2025 13:43:29 INFO]: Test stats: {
    "score": -0.8717577690208264,
    "rmse": 0.8717577690208264
}
[08/28/2025 13:43:35 INFO]: Training loss at epoch 23: 0.9824586808681488
[08/28/2025 13:43:37 INFO]: Training loss at epoch 45: 1.0856277346611023
[08/28/2025 13:44:04 INFO]: Training stats: {
    "score": -0.9869100224078445,
    "rmse": 0.9869100224078445
}
[08/28/2025 13:44:04 INFO]: Val stats: {
    "score": -0.6877057575813265,
    "rmse": 0.6877057575813265
}
[08/28/2025 13:44:04 INFO]: Test stats: {
    "score": -0.8827973048431214,
    "rmse": 0.8827973048431214
}
[08/28/2025 13:44:05 INFO]: Training loss at epoch 17: 0.8544169962406158
[08/28/2025 13:44:11 INFO]: Training loss at epoch 19: 1.021189272403717
[08/28/2025 13:44:13 INFO]: Training loss at epoch 8: 0.9080227017402649
[08/28/2025 13:44:21 INFO]: Training loss at epoch 21: 0.9053521752357483
[08/28/2025 13:44:30 INFO]: Training loss at epoch 26: 1.0350369215011597
[08/28/2025 13:44:43 INFO]: Training stats: {
    "score": -1.2352411962812055,
    "rmse": 1.2352411962812055
}
[08/28/2025 13:44:43 INFO]: Val stats: {
    "score": -1.108105952320266,
    "rmse": 1.108105952320266
}
[08/28/2025 13:44:43 INFO]: Test stats: {
    "score": -1.2016299893112592,
    "rmse": 1.2016299893112592
}
[08/28/2025 13:44:46 INFO]: Training stats: {
    "score": -0.9831077954249219,
    "rmse": 0.9831077954249219
}
[08/28/2025 13:44:46 INFO]: Val stats: {
    "score": -0.697311362184523,
    "rmse": 0.697311362184523
}
[08/28/2025 13:44:46 INFO]: Test stats: {
    "score": -0.8896498085527619,
    "rmse": 0.8896498085527619
}
[08/28/2025 13:44:50 INFO]: Training loss at epoch 14: 0.9181446135044098
[08/28/2025 13:44:53 INFO]: Training loss at epoch 8: 1.2768220901489258
[08/28/2025 13:45:11 INFO]: Training loss at epoch 9: 1.0619115233421326
[08/28/2025 13:45:11 INFO]: Training loss at epoch 24: 1.0721930265426636
[08/28/2025 13:45:21 INFO]: Training loss at epoch 20: 1.081963986158371
[08/28/2025 13:45:22 INFO]: Training stats: {
    "score": -0.9843681420097832,
    "rmse": 0.9843681420097832
}
[08/28/2025 13:45:22 INFO]: Val stats: {
    "score": -0.6999297658546587,
    "rmse": 0.6999297658546587
}
[08/28/2025 13:45:22 INFO]: Test stats: {
    "score": -0.887498142505547,
    "rmse": 0.887498142505547
}
[08/28/2025 13:45:38 INFO]: Training loss at epoch 18: 1.148261308670044
[08/28/2025 13:45:45 INFO]: Training loss at epoch 20: 0.9904137253761292
[08/28/2025 13:45:54 INFO]: Training loss at epoch 46: 1.0783826112747192
[08/28/2025 13:45:59 INFO]: Training loss at epoch 27: 0.9294903874397278
[08/28/2025 13:46:02 INFO]: Training loss at epoch 7: 0.9969488978385925
[08/28/2025 13:46:39 INFO]: Training loss at epoch 30: 1.1210578083992004
[08/28/2025 13:46:50 INFO]: Training loss at epoch 25: 0.9734660685062408
[08/28/2025 13:46:54 INFO]: Training loss at epoch 10: 0.8767971694469452
[08/28/2025 13:46:57 INFO]: Training loss at epoch 15: 1.1610413193702698
[08/28/2025 13:47:09 INFO]: Training loss at epoch 22: 0.8255777657032013
[08/28/2025 13:47:17 INFO]: Training loss at epoch 19: 0.8543749749660492
[08/28/2025 13:47:17 INFO]: Training loss at epoch 21: 1.082626760005951
[08/28/2025 13:47:33 INFO]: Training loss at epoch 28: 1.1232837438583374
[08/28/2025 13:47:52 INFO]: Training stats: {
    "score": -1.0017405481152968,
    "rmse": 1.0017405481152968
}
[08/28/2025 13:47:52 INFO]: Val stats: {
    "score": -0.6695187758125091,
    "rmse": 0.6695187758125091
}
[08/28/2025 13:47:52 INFO]: Test stats: {
    "score": -0.8699065985585296,
    "rmse": 0.8699065985585296
}
[08/28/2025 13:47:54 INFO]: Training stats: {
    "score": -1.0045311521776363,
    "rmse": 1.0045311521776363
}
[08/28/2025 13:47:54 INFO]: Val stats: {
    "score": -0.782542878113501,
    "rmse": 0.782542878113501
}
[08/28/2025 13:47:54 INFO]: Test stats: {
    "score": -0.9300809498395944,
    "rmse": 0.9300809498395944
}
[08/28/2025 13:48:21 INFO]: Training loss at epoch 47: 0.959071010351181
[08/28/2025 13:48:33 INFO]: Training loss at epoch 26: 1.2628490924835205
[08/28/2025 13:48:49 INFO]: Training loss at epoch 20: 1.0133622288703918
[08/28/2025 13:49:07 INFO]: Training loss at epoch 29: 1.0491969585418701
[08/28/2025 13:49:08 INFO]: Training loss at epoch 16: 1.2501515746116638
[08/28/2025 13:49:16 INFO]: Training loss at epoch 22: 1.1833552718162537
[08/28/2025 13:49:22 INFO]: Training loss at epoch 31: 0.819856733083725
[08/28/2025 13:49:31 INFO]: Training loss at epoch 20: 1.1978585124015808
[08/28/2025 13:49:38 INFO]: Training stats: {
    "score": -1.0003912835438753,
    "rmse": 1.0003912835438753
}
[08/28/2025 13:49:38 INFO]: Val stats: {
    "score": -0.6961085095259506,
    "rmse": 0.6961085095259506
}
[08/28/2025 13:49:38 INFO]: Test stats: {
    "score": -0.8812765478245406,
    "rmse": 0.8812765478245406
}
[08/28/2025 13:49:59 INFO]: Training loss at epoch 23: 1.3260651528835297
[08/28/2025 13:50:04 INFO]: Training loss at epoch 21: 1.1102595031261444
[08/28/2025 13:50:10 INFO]: Training loss at epoch 27: 0.9355864524841309
[08/28/2025 13:50:40 INFO]: Training loss at epoch 48: 0.9928644299507141
[08/28/2025 13:50:46 INFO]: Training loss at epoch 9: 0.9351344108581543
[08/28/2025 13:51:04 INFO]: Training loss at epoch 21: 1.072582483291626
[08/28/2025 13:51:06 INFO]: Training loss at epoch 30: 1.0450471639633179
[08/28/2025 13:51:07 INFO]: Training loss at epoch 23: 0.9579469561576843
[08/28/2025 13:51:12 INFO]: Training loss at epoch 17: 1.0839113593101501
[08/28/2025 13:51:32 INFO]: Training loss at epoch 11: 0.9541085660457611
[08/28/2025 13:51:45 INFO]: Training loss at epoch 28: 0.8588736057281494
[08/28/2025 13:51:54 INFO]: Training loss at epoch 32: 1.165749967098236
[08/28/2025 13:52:05 INFO]: Training loss at epoch 21: 1.1523381471633911
[08/28/2025 13:52:19 INFO]: Training loss at epoch 9: 1.0112427771091461
[08/28/2025 13:52:35 INFO]: Training loss at epoch 31: 1.1417670249938965
[08/28/2025 13:52:37 INFO]: Training loss at epoch 22: 0.9678756594657898
[08/28/2025 13:52:37 INFO]: Training loss at epoch 10: 1.1101007759571075
[08/28/2025 13:52:42 INFO]: Training loss at epoch 10: 1.3445757329463959
[08/28/2025 13:52:42 INFO]: Training loss at epoch 24: 1.0636431574821472
[08/28/2025 13:52:46 INFO]: Running Final Evaluation...
[08/28/2025 13:52:57 INFO]: Training loss at epoch 49: 1.1223979592323303
[08/28/2025 13:52:59 INFO]: Training loss at epoch 24: 1.0370480418205261
[08/28/2025 13:53:09 INFO]: Training loss at epoch 9: 1.0011846125125885
[08/28/2025 13:53:16 INFO]: Training loss at epoch 18: 0.8665303885936737
[08/28/2025 13:53:19 INFO]: Training accuracy: {
    "score": -1.0003633121839437,
    "rmse": 1.0003633121839437
}
[08/28/2025 13:53:19 INFO]: Val accuracy: {
    "score": -0.6717305244872518,
    "rmse": 0.6717305244872518
}
[08/28/2025 13:53:19 INFO]: Test accuracy: {
    "score": -0.8694801262395296,
    "rmse": 0.8694801262395296
}
[08/28/2025 13:53:19 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_176",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8694801262395296,
        "rmse": 0.8694801262395296
    },
    "train_stats": {
        "score": -1.0003633121839437,
        "rmse": 1.0003633121839437
    },
    "val_stats": {
        "score": -0.6717305244872518,
        "rmse": 0.6717305244872518
    }
}
[08/28/2025 13:53:19 INFO]: Procewss finished for trial unhanged-Shanesha_trial_176
[08/28/2025 13:53:21 INFO]: Training loss at epoch 29: 1.1188528537750244
[08/28/2025 13:53:24 INFO]: Training stats: {
    "score": -1.0230096430993767,
    "rmse": 1.0230096430993767
}
[08/28/2025 13:53:24 INFO]: Val stats: {
    "score": -0.7923191076210016,
    "rmse": 0.7923191076210016
}
[08/28/2025 13:53:24 INFO]: Test stats: {
    "score": -0.9510234418273275,
    "rmse": 0.9510234418273275
}
[08/28/2025 13:53:42 INFO]: Training stats: {
    "score": -0.9941595749678247,
    "rmse": 0.9941595749678247
}
[08/28/2025 13:53:42 INFO]: Val stats: {
    "score": -0.675985244845954,
    "rmse": 0.675985244845954
}
[08/28/2025 13:53:42 INFO]: Test stats: {
    "score": -0.867320980159431,
    "rmse": 0.867320980159431
}
[08/28/2025 13:53:54 INFO]: Training stats: {
    "score": -1.0357309192565527,
    "rmse": 1.0357309192565527
}
[08/28/2025 13:53:54 INFO]: Val stats: {
    "score": -0.7867295149660418,
    "rmse": 0.7867295149660418
}
[08/28/2025 13:53:54 INFO]: Test stats: {
    "score": -0.9417454225752373,
    "rmse": 0.9417454225752373
}
[08/28/2025 13:54:07 INFO]: Training loss at epoch 21: 1.0906448066234589
[08/28/2025 13:54:10 INFO]: Training loss at epoch 23: 1.1133688688278198
[08/28/2025 13:54:12 INFO]: Training loss at epoch 8: 0.7558014094829559
[08/28/2025 13:54:27 INFO]: Training loss at epoch 33: 1.1379177868366241
[08/28/2025 13:54:51 INFO]: Training loss at epoch 25: 1.1261045634746552
[08/28/2025 13:54:58 INFO]: Training stats: {
    "score": -0.9942973460902732,
    "rmse": 0.9942973460902732
}
[08/28/2025 13:54:58 INFO]: Val stats: {
    "score": -0.7291365515870389,
    "rmse": 0.7291365515870389
}
[08/28/2025 13:54:58 INFO]: Test stats: {
    "score": -0.904744662508778,
    "rmse": 0.904744662508778
}
[08/28/2025 13:55:18 INFO]: Training loss at epoch 19: 0.9976477026939392
[08/28/2025 13:55:21 INFO]: Training loss at epoch 22: 1.1171382665634155
[08/28/2025 13:55:24 INFO]: Training loss at epoch 25: 1.1319707036018372
[08/28/2025 13:55:30 INFO]: Training loss at epoch 30: 1.0551106929779053
[08/28/2025 13:55:44 INFO]: Training loss at epoch 24: 0.9839791059494019
[08/28/2025 13:55:44 INFO]: Training loss at epoch 10: 0.7337595522403717
[08/28/2025 13:55:50 INFO]: Training stats: {
    "score": -1.0121724575344342,
    "rmse": 1.0121724575344342
}
[08/28/2025 13:55:50 INFO]: Val stats: {
    "score": -0.7633239023569501,
    "rmse": 0.7633239023569501
}
[08/28/2025 13:55:50 INFO]: Test stats: {
    "score": -0.8923086610972483,
    "rmse": 0.8923086610972483
}
[08/28/2025 13:55:58 INFO]: Training loss at epoch 50: 1.204228937625885
[08/28/2025 13:56:00 INFO]: Training loss at epoch 12: 1.1472232341766357
[08/28/2025 13:56:01 INFO]: Training stats: {
    "score": -1.002376793932956,
    "rmse": 1.002376793932956
}
[08/28/2025 13:56:01 INFO]: Val stats: {
    "score": -0.7047361950497169,
    "rmse": 0.7047361950497169
}
[08/28/2025 13:56:01 INFO]: Test stats: {
    "score": -0.8908060173293256,
    "rmse": 0.8908060173293256
}
[08/28/2025 13:56:38 INFO]: New best epoch, val score: -0.7493205621704856
[08/28/2025 13:56:38 INFO]: Saving model to: unhanged-Shanesha_trial_167/model_best.pth
[08/28/2025 13:56:45 INFO]: Training loss at epoch 26: 0.9598883390426636
[08/28/2025 13:57:00 INFO]: Training loss at epoch 34: 1.0964250564575195
[08/28/2025 13:57:08 INFO]: Training loss at epoch 31: 0.9662004113197327
[08/28/2025 13:57:20 INFO]: Training loss at epoch 25: 1.2014935910701752
[08/28/2025 13:57:20 INFO]: Running Final Evaluation...
[08/28/2025 13:57:57 INFO]: Training loss at epoch 22: 0.9510148465633392
[08/28/2025 13:58:00 INFO]: Training accuracy: {
    "score": -1.003958241566078,
    "rmse": 1.003958241566078
}
[08/28/2025 13:58:00 INFO]: Val accuracy: {
    "score": -0.6865591849957827,
    "rmse": 0.6865591849957827
}
[08/28/2025 13:58:00 INFO]: Test accuracy: {
    "score": -0.8775062049217678,
    "rmse": 0.8775062049217678
}
[08/28/2025 13:58:00 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_177",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8775062049217678,
        "rmse": 0.8775062049217678
    },
    "train_stats": {
        "score": -1.003958241566078,
        "rmse": 1.003958241566078
    },
    "val_stats": {
        "score": -0.6865591849957827,
        "rmse": 0.6865591849957827
    }
}
[08/28/2025 13:58:00 INFO]: Procewss finished for trial unhanged-Shanesha_trial_177
[08/28/2025 13:58:06 INFO]: Training loss at epoch 20: 1.2975335717201233
[08/28/2025 13:58:08 INFO]: Training loss at epoch 26: 0.863817423582077
[08/28/2025 13:58:16 INFO]: Training loss at epoch 51: 1.3681277334690094
[08/28/2025 13:58:37 INFO]: Training loss at epoch 27: 0.9388787746429443
[08/28/2025 13:58:37 INFO]: Training loss at epoch 23: 0.9143375754356384
[08/28/2025 13:58:53 INFO]: Training loss at epoch 26: 1.1614497900009155
[08/28/2025 13:59:31 INFO]: Training loss at epoch 35: 0.8407573699951172
[08/28/2025 14:00:06 INFO]: Training loss at epoch 21: 0.9832682013511658
[08/28/2025 14:00:20 INFO]: Training loss at epoch 11: 1.0909854769706726
[08/28/2025 14:00:25 INFO]: Training loss at epoch 27: 1.0303577780723572
[08/28/2025 14:00:26 INFO]: Training loss at epoch 11: 0.9393132925033569
[08/28/2025 14:00:27 INFO]: Training loss at epoch 28: 1.147617131471634
[08/28/2025 14:00:30 INFO]: Training loss at epoch 13: 0.901355654001236
[08/28/2025 14:00:32 INFO]: Training loss at epoch 52: 0.90916508436203
[08/28/2025 14:00:50 INFO]: Training loss at epoch 27: 0.8831909596920013
[08/28/2025 14:01:15 INFO]: Training loss at epoch 10: 1.0184531807899475
[08/28/2025 14:01:19 INFO]: New best epoch, val score: -0.6699318439290454
[08/28/2025 14:01:19 INFO]: Saving model to: unhanged-Shanesha_trial_165/model_best.pth
[08/28/2025 14:01:51 INFO]: Training loss at epoch 24: 1.1247920989990234
[08/28/2025 14:01:58 INFO]: Training loss at epoch 28: 1.0110946297645569
[08/28/2025 14:02:04 INFO]: Training loss at epoch 36: 1.0157036185264587
[08/28/2025 14:02:06 INFO]: Training loss at epoch 22: 1.0496553182601929
[08/28/2025 14:02:10 INFO]: New best epoch, val score: -0.7102069766386886
[08/28/2025 14:02:10 INFO]: Saving model to: unhanged-Shanesha_trial_168/model_best.pth
[08/28/2025 14:02:12 INFO]: Training loss at epoch 9: 0.9123815596103668
[08/28/2025 14:02:15 INFO]: Training loss at epoch 22: 0.7602947652339935
[08/28/2025 14:02:17 INFO]: Training loss at epoch 29: 1.06995689868927
[08/28/2025 14:02:48 INFO]: Training loss at epoch 53: 0.9713871479034424
[08/28/2025 14:02:49 INFO]: Training loss at epoch 10: 0.8993541598320007
[08/28/2025 14:02:57 INFO]: Training stats: {
    "score": -1.0014678077825878,
    "rmse": 1.0014678077825878
}
[08/28/2025 14:02:57 INFO]: Val stats: {
    "score": -0.6683545981049828,
    "rmse": 0.6683545981049828
}
[08/28/2025 14:02:57 INFO]: Test stats: {
    "score": -0.8681241456256721,
    "rmse": 0.8681241456256721
}
[08/28/2025 14:03:29 INFO]: Training loss at epoch 11: 1.1148820519447327
[08/28/2025 14:03:31 INFO]: Training loss at epoch 29: 0.8251487612724304
[08/28/2025 14:03:32 INFO]: Training loss at epoch 28: 1.0437637269496918
[08/28/2025 14:03:45 INFO]: New best epoch, val score: -0.7043423511973729
[08/28/2025 14:03:45 INFO]: Saving model to: unhanged-Shanesha_trial_170/model_best.pth
[08/28/2025 14:03:52 INFO]: Training loss at epoch 10: 0.9928284585475922
[08/28/2025 14:04:05 INFO]: Training stats: {
    "score": -1.004159786485399,
    "rmse": 1.004159786485399
}
[08/28/2025 14:04:05 INFO]: Val stats: {
    "score": -0.7038191628124795,
    "rmse": 0.7038191628124795
}
[08/28/2025 14:04:05 INFO]: Test stats: {
    "score": -0.8876211088149217,
    "rmse": 0.8876211088149217
}
[08/28/2025 14:04:07 INFO]: Training loss at epoch 23: 1.0240055322647095
[08/28/2025 14:04:23 INFO]: New best epoch, val score: -0.7299429426002837
[08/28/2025 14:04:23 INFO]: Saving model to: unhanged-Shanesha_trial_167/model_best.pth
[08/28/2025 14:04:36 INFO]: Training loss at epoch 37: 1.032735824584961
[08/28/2025 14:04:50 INFO]: Training loss at epoch 30: 1.0647743940353394
[08/28/2025 14:04:52 INFO]: Training stats: {
    "score": -0.9901641149248032,
    "rmse": 0.9901641149248032
}
[08/28/2025 14:04:52 INFO]: Val stats: {
    "score": -0.779298401268082,
    "rmse": 0.779298401268082
}
[08/28/2025 14:04:52 INFO]: Test stats: {
    "score": -0.9425264516048356,
    "rmse": 0.9425264516048356
}
[08/28/2025 14:04:57 INFO]: Training loss at epoch 14: 1.151097297668457
[08/28/2025 14:05:04 INFO]: Training loss at epoch 54: 1.0869333744049072
[08/28/2025 14:05:06 INFO]: Training loss at epoch 25: 1.0968389511108398
[08/28/2025 14:05:38 INFO]: Training loss at epoch 30: 1.037081778049469
[08/28/2025 14:05:45 INFO]: Training loss at epoch 23: 1.079835832118988
[08/28/2025 14:06:08 INFO]: Training loss at epoch 24: 0.9077845215797424
[08/28/2025 14:06:14 INFO]: Training loss at epoch 29: 0.9829269349575043
[08/28/2025 14:06:40 INFO]: Training loss at epoch 31: 0.8661989569664001
[08/28/2025 14:07:07 INFO]: Training loss at epoch 38: 0.9667219519615173
[08/28/2025 14:07:09 INFO]: Training stats: {
    "score": -0.9557027095192706,
    "rmse": 0.9557027095192706
}
[08/28/2025 14:07:09 INFO]: Val stats: {
    "score": -0.6996823239334413,
    "rmse": 0.6996823239334413
}
[08/28/2025 14:07:09 INFO]: Test stats: {
    "score": -0.901436486660639,
    "rmse": 0.901436486660639
}
[08/28/2025 14:07:10 INFO]: Training loss at epoch 31: 0.8941687941551208
[08/28/2025 14:07:19 INFO]: Training loss at epoch 55: 0.9190747141838074
[08/28/2025 14:07:22 INFO]: Running Final Evaluation...
[08/28/2025 14:07:59 INFO]: Training loss at epoch 12: 1.0821166038513184
[08/28/2025 14:08:01 INFO]: Training accuracy: {
    "score": -1.023530207120623,
    "rmse": 1.023530207120623
}
[08/28/2025 14:08:01 INFO]: Val accuracy: {
    "score": -0.6602989969580659,
    "rmse": 0.6602989969580659
}
[08/28/2025 14:08:01 INFO]: Test accuracy: {
    "score": -0.8754992998449251,
    "rmse": 0.8754992998449251
}
[08/28/2025 14:08:01 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_180",
    "best_epoch": 0,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8754992998449251,
        "rmse": 0.8754992998449251
    },
    "train_stats": {
        "score": -1.023530207120623,
        "rmse": 1.023530207120623
    },
    "val_stats": {
        "score": -0.6602989969580659,
        "rmse": 0.6602989969580659
    }
}
[08/28/2025 14:08:01 INFO]: Procewss finished for trial unhanged-Shanesha_trial_180
[08/28/2025 14:08:07 INFO]: Training loss at epoch 12: 0.9959651827812195
[08/28/2025 14:08:08 INFO]: Training loss at epoch 25: 1.141430675983429
[08/28/2025 14:08:19 INFO]: Training loss at epoch 26: 1.034285306930542
[08/28/2025 14:08:31 INFO]: Training loss at epoch 32: 1.0460097789764404
[08/28/2025 14:09:04 INFO]: Training loss at epoch 11: 0.9215816259384155
[08/28/2025 14:09:25 INFO]: Training loss at epoch 15: 0.9122547507286072
[08/28/2025 14:09:35 INFO]: Training loss at epoch 56: 1.1108462512493134
[08/28/2025 14:09:38 INFO]: Training loss at epoch 39: 1.190556287765503
[08/28/2025 14:09:50 INFO]: Training loss at epoch 30: 0.8225885927677155
[08/28/2025 14:09:59 INFO]: New best epoch, val score: -0.6803481700875703
[08/28/2025 14:09:59 INFO]: Saving model to: unhanged-Shanesha_trial_168/model_best.pth
[08/28/2025 14:10:08 INFO]: Training loss at epoch 26: 0.9109195172786713
[08/28/2025 14:10:20 INFO]: Training loss at epoch 23: 0.9113617241382599
[08/28/2025 14:10:21 INFO]: Training loss at epoch 33: 1.2139779925346375
[08/28/2025 14:10:29 INFO]: Training stats: {
    "score": -0.9806968750710775,
    "rmse": 0.9806968750710775
}
[08/28/2025 14:10:29 INFO]: Val stats: {
    "score": -0.6874504667882259,
    "rmse": 0.6874504667882259
}
[08/28/2025 14:10:29 INFO]: Test stats: {
    "score": -0.8823928652696368,
    "rmse": 0.8823928652696368
}
[08/28/2025 14:10:36 INFO]: Running Final Evaluation...
[08/28/2025 14:10:39 INFO]: Training loss at epoch 11: 1.1612406969070435
[08/28/2025 14:10:47 INFO]: Running Final Evaluation...
[08/28/2025 14:11:11 INFO]: Training loss at epoch 12: 0.9367080926895142
[08/28/2025 14:11:21 INFO]: Training accuracy: {
    "score": -1.0166305339988155,
    "rmse": 1.0166305339988155
}
[08/28/2025 14:11:21 INFO]: Val accuracy: {
    "score": -0.6617347993271854,
    "rmse": 0.6617347993271854
}
[08/28/2025 14:11:21 INFO]: Test accuracy: {
    "score": -0.8712390105990147,
    "rmse": 0.8712390105990147
}
[08/28/2025 14:11:21 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_178",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8712390105990147,
        "rmse": 0.8712390105990147
    },
    "train_stats": {
        "score": -1.0166305339988155,
        "rmse": 1.0166305339988155
    },
    "val_stats": {
        "score": -0.6617347993271854,
        "rmse": 0.6617347993271854
    }
}
[08/28/2025 14:11:21 INFO]: Procewss finished for trial unhanged-Shanesha_trial_178
[08/28/2025 14:11:33 INFO]: Training loss at epoch 27: 1.003576397895813
[08/28/2025 14:11:34 INFO]: New best epoch, val score: -0.6960922048482587
[08/28/2025 14:11:34 INFO]: Saving model to: unhanged-Shanesha_trial_170/model_best.pth
[08/28/2025 14:11:54 INFO]: Training loss at epoch 11: 1.0933601260185242
[08/28/2025 14:11:56 INFO]: Training loss at epoch 57: 0.8941705226898193
[08/28/2025 14:12:06 INFO]: Training accuracy: {
    "score": -0.9998266870575442,
    "rmse": 0.9998266870575442
}
[08/28/2025 14:12:06 INFO]: Val accuracy: {
    "score": -0.6851262474800446,
    "rmse": 0.6851262474800446
}
[08/28/2025 14:12:06 INFO]: Test accuracy: {
    "score": -0.8764738330674794,
    "rmse": 0.8764738330674794
}
[08/28/2025 14:12:06 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_164",
    "best_epoch": 8,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8764738330674794,
        "rmse": 0.8764738330674794
    },
    "train_stats": {
        "score": -0.9998266870575442,
        "rmse": 0.9998266870575442
    },
    "val_stats": {
        "score": -0.6851262474800446,
        "rmse": 0.6851262474800446
    }
}
[08/28/2025 14:12:06 INFO]: Procewss finished for trial unhanged-Shanesha_trial_164
[08/28/2025 14:12:10 INFO]: New best epoch, val score: -0.7270385663198048
[08/28/2025 14:12:10 INFO]: Saving model to: unhanged-Shanesha_trial_167/model_best.pth
[08/28/2025 14:12:13 INFO]: Training loss at epoch 27: 0.9684509038925171
[08/28/2025 14:12:37 INFO]: Training loss at epoch 31: 1.1029530763626099
[08/28/2025 14:12:48 INFO]: Training loss at epoch 10: 0.8914526700973511
[08/28/2025 14:13:37 INFO]: Training loss at epoch 24: 0.8670420348644257
[08/28/2025 14:13:57 INFO]: Training loss at epoch 16: 1.0800974369049072
[08/28/2025 14:14:11 INFO]: Training loss at epoch 58: 1.0266530513763428
[08/28/2025 14:14:11 INFO]: Training loss at epoch 28: 1.0886955857276917
[08/28/2025 14:14:50 INFO]: Training loss at epoch 28: 1.0508410930633545
[08/28/2025 14:15:17 INFO]: Training loss at epoch 32: 0.886572539806366
[08/28/2025 14:15:41 INFO]: Training loss at epoch 13: 1.1650080680847168
[08/28/2025 14:15:48 INFO]: Training loss at epoch 13: 1.128378838300705
[08/28/2025 14:16:09 INFO]: Training loss at epoch 29: 1.134134590625763
[08/28/2025 14:16:25 INFO]: Training loss at epoch 59: 0.9518423080444336
[08/28/2025 14:16:34 INFO]: New best epoch, val score: -0.6651175517141329
[08/28/2025 14:16:34 INFO]: Saving model to: unhanged-Shanesha_trial_166/model_best.pth
[08/28/2025 14:16:52 INFO]: Training stats: {
    "score": -1.0004387799094159,
    "rmse": 1.0004387799094159
}
[08/28/2025 14:16:52 INFO]: Val stats: {
    "score": -0.6954542491164543,
    "rmse": 0.6954542491164543
}
[08/28/2025 14:16:52 INFO]: Test stats: {
    "score": -0.8850227638769818,
    "rmse": 0.8850227638769818
}
[08/28/2025 14:16:55 INFO]: Training loss at epoch 12: 0.8446466624736786
[08/28/2025 14:17:10 INFO]: Training stats: {
    "score": -0.9981012186087432,
    "rmse": 0.9981012186087432
}
[08/28/2025 14:17:10 INFO]: Val stats: {
    "score": -0.7188386352306833,
    "rmse": 0.7188386352306833
}
[08/28/2025 14:17:10 INFO]: Test stats: {
    "score": -0.8901723030677438,
    "rmse": 0.8901723030677438
}
[08/28/2025 14:17:57 INFO]: Training loss at epoch 33: 0.8563573360443115
[08/28/2025 14:18:01 INFO]: Training loss at epoch 29: 0.9246860444545746
[08/28/2025 14:18:16 INFO]: Running Final Evaluation...
[08/28/2025 14:18:20 INFO]: Training loss at epoch 17: 0.9179216921329498
[08/28/2025 14:18:23 INFO]: Training loss at epoch 24: 1.110330581665039
[08/28/2025 14:18:28 INFO]: Training loss at epoch 12: 1.132767379283905
[08/28/2025 14:18:49 INFO]: Training loss at epoch 30: 1.052319049835205
[08/28/2025 14:18:53 INFO]: Training loss at epoch 13: 1.1731043457984924
[08/28/2025 14:19:11 INFO]: Training stats: {
    "score": -0.9824761860275439,
    "rmse": 0.9824761860275439
}
[08/28/2025 14:19:11 INFO]: Val stats: {
    "score": -0.7015819670807732,
    "rmse": 0.7015819670807732
}
[08/28/2025 14:19:11 INFO]: Test stats: {
    "score": -0.887871852212668,
    "rmse": 0.887871852212668
}
[08/28/2025 14:19:24 INFO]: Training loss at epoch 60: 0.8146137595176697
[08/28/2025 14:19:28 INFO]: Training accuracy: {
    "score": -1.012811754825066,
    "rmse": 1.012811754825066
}
[08/28/2025 14:19:28 INFO]: Val accuracy: {
    "score": -0.669483604949679,
    "rmse": 0.669483604949679
}
[08/28/2025 14:19:28 INFO]: Test accuracy: {
    "score": -0.8901381817112387,
    "rmse": 0.8901381817112387
}
[08/28/2025 14:19:28 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_173",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8901381817112387,
        "rmse": 0.8901381817112387
    },
    "train_stats": {
        "score": -1.012811754825066,
        "rmse": 1.012811754825066
    },
    "val_stats": {
        "score": -0.669483604949679,
        "rmse": 0.669483604949679
    }
}
[08/28/2025 14:19:28 INFO]: Procewss finished for trial unhanged-Shanesha_trial_173
[08/28/2025 14:19:46 INFO]: Training loss at epoch 12: 1.1510403156280518
[08/28/2025 14:20:36 INFO]: Training loss at epoch 11: 1.0126368999481201
[08/28/2025 14:20:45 INFO]: Training loss at epoch 31: 0.9561935365200043
[08/28/2025 14:21:18 INFO]: Training loss at epoch 25: 0.8062238693237305
[08/28/2025 14:21:39 INFO]: Training loss at epoch 61: 0.9336752891540527
[08/28/2025 14:22:21 INFO]: Training loss at epoch 30: 0.8484803438186646
[08/28/2025 14:22:38 INFO]: Training loss at epoch 18: 0.8161933124065399
[08/28/2025 14:22:41 INFO]: Training loss at epoch 32: 1.2769362926483154
[08/28/2025 14:23:11 INFO]: Training loss at epoch 14: 1.1877458691596985
[08/28/2025 14:23:18 INFO]: Training loss at epoch 14: 1.2558762729167938
[08/28/2025 14:23:50 INFO]: Training loss at epoch 62: 1.0892345309257507
[08/28/2025 14:24:33 INFO]: Training loss at epoch 13: 0.9289076626300812
[08/28/2025 14:24:39 INFO]: Training loss at epoch 33: 1.0715106129646301
[08/28/2025 14:24:54 INFO]: Running Final Evaluation...
[08/28/2025 14:25:27 INFO]: Training loss at epoch 31: 1.0175631642341614
[08/28/2025 14:25:44 INFO]: Training accuracy: {
    "score": -1.0252063527026651,
    "rmse": 1.0252063527026651
}
[08/28/2025 14:25:44 INFO]: Val accuracy: {
    "score": -0.6639087596751013,
    "rmse": 0.6639087596751013
}
[08/28/2025 14:25:44 INFO]: Test accuracy: {
    "score": -0.8813315637725931,
    "rmse": 0.8813315637725931
}
[08/28/2025 14:25:44 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_179",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8813315637725931,
        "rmse": 0.8813315637725931
    },
    "train_stats": {
        "score": -1.0252063527026651,
        "rmse": 1.0252063527026651
    },
    "val_stats": {
        "score": -0.6639087596751013,
        "rmse": 0.6639087596751013
    }
}
[08/28/2025 14:25:44 INFO]: Procewss finished for trial unhanged-Shanesha_trial_179
[08/28/2025 14:26:00 INFO]: Training loss at epoch 63: 1.0323194861412048
[08/28/2025 14:26:05 INFO]: Training loss at epoch 13: 0.9467176795005798
[08/28/2025 14:26:13 INFO]: Training loss at epoch 25: 0.9011996686458588
[08/28/2025 14:26:21 INFO]: Training loss at epoch 14: 0.8882733285427094
[08/28/2025 14:26:57 INFO]: Training loss at epoch 19: 1.0431108474731445
[08/28/2025 14:27:31 INFO]: Training loss at epoch 13: 0.9229162335395813
[08/28/2025 14:28:10 INFO]: Training loss at epoch 64: 1.0178245902061462
[08/28/2025 14:28:19 INFO]: Training loss at epoch 12: 1.0619447231292725
[08/28/2025 14:28:24 INFO]: Training stats: {
    "score": -0.964323107645694,
    "rmse": 0.964323107645694
}
[08/28/2025 14:28:24 INFO]: Val stats: {
    "score": -0.7170161815974638,
    "rmse": 0.7170161815974638
}
[08/28/2025 14:28:24 INFO]: Test stats: {
    "score": -0.9251094867895524,
    "rmse": 0.9251094867895524
}
[08/28/2025 14:28:34 INFO]: Training loss at epoch 32: 0.811197817325592
[08/28/2025 14:28:54 INFO]: Training loss at epoch 26: 0.8968954980373383
[08/28/2025 14:30:20 INFO]: Training loss at epoch 65: 0.820168137550354
[08/28/2025 14:30:35 INFO]: New best epoch, val score: -0.6675330379891365
[08/28/2025 14:30:35 INFO]: Saving model to: unhanged-Shanesha_trial_157/model_best.pth
[08/28/2025 14:30:36 INFO]: Training loss at epoch 15: 1.208404302597046
[08/28/2025 14:30:44 INFO]: Training loss at epoch 15: 0.8709498941898346
[08/28/2025 14:31:39 INFO]: Training loss at epoch 33: 0.9109094738960266
[08/28/2025 14:32:01 INFO]: Training loss at epoch 14: 1.0469560623168945
[08/28/2025 14:32:30 INFO]: Training loss at epoch 66: 1.0882311463356018
[08/28/2025 14:32:39 INFO]: Training loss at epoch 20: 0.9237536489963531
[08/28/2025 14:32:45 INFO]: New best epoch, val score: -0.667357683317533
[08/28/2025 14:32:45 INFO]: Saving model to: unhanged-Shanesha_trial_157/model_best.pth
[08/28/2025 14:33:34 INFO]: Training loss at epoch 14: 0.8849946558475494
[08/28/2025 14:33:42 INFO]: Training loss at epoch 15: 1.1983394622802734
[08/28/2025 14:33:56 INFO]: Training loss at epoch 26: 1.0663380026817322
[08/28/2025 14:34:41 INFO]: Training loss at epoch 67: 1.406726747751236
[08/28/2025 14:34:46 INFO]: Training loss at epoch 34: 0.993080735206604
[08/28/2025 14:34:57 INFO]: New best epoch, val score: -0.6672868223971261
[08/28/2025 14:34:57 INFO]: Saving model to: unhanged-Shanesha_trial_157/model_best.pth
[08/28/2025 14:35:10 INFO]: Running Final Evaluation...
[08/28/2025 14:35:13 INFO]: Training loss at epoch 14: 0.974751353263855
[08/28/2025 14:35:54 INFO]: Training loss at epoch 13: 0.8809490501880646
[08/28/2025 14:36:26 INFO]: Training loss at epoch 27: 0.8994972705841064
[08/28/2025 14:36:26 INFO]: Training accuracy: {
    "score": -0.9922183963210285,
    "rmse": 0.9922183963210285
}
[08/28/2025 14:36:26 INFO]: Val accuracy: {
    "score": -0.6820218604592531,
    "rmse": 0.6820218604592531
}
[08/28/2025 14:36:26 INFO]: Test accuracy: {
    "score": -0.8806994399938198,
    "rmse": 0.8806994399938198
}
[08/28/2025 14:36:27 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_171",
    "best_epoch": 3,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8806994399938198,
        "rmse": 0.8806994399938198
    },
    "train_stats": {
        "score": -0.9922183963210285,
        "rmse": 0.9922183963210285
    },
    "val_stats": {
        "score": -0.6820218604592531,
        "rmse": 0.6820218604592531
    }
}
[08/28/2025 14:36:27 INFO]: Procewss finished for trial unhanged-Shanesha_trial_171
[08/28/2025 14:36:51 INFO]: Training loss at epoch 68: 1.188742458820343
[08/28/2025 14:36:56 INFO]: Training loss at epoch 21: 0.8620722591876984
[08/28/2025 14:37:57 INFO]: Training loss at epoch 16: 0.936994194984436
[08/28/2025 14:38:06 INFO]: Training loss at epoch 16: 1.0558931827545166
[08/28/2025 14:38:59 INFO]: Training loss at epoch 69: 0.9267176687717438
[08/28/2025 14:39:26 INFO]: Training loss at epoch 15: 0.9181309640407562
[08/28/2025 14:39:42 INFO]: Training stats: {
    "score": -0.9938923738449889,
    "rmse": 0.9938923738449889
}
[08/28/2025 14:39:42 INFO]: Val stats: {
    "score": -0.6724002494270661,
    "rmse": 0.6724002494270661
}
[08/28/2025 14:39:42 INFO]: Test stats: {
    "score": -0.8659939905914706,
    "rmse": 0.8659939905914706
}
[08/28/2025 14:40:59 INFO]: Training loss at epoch 16: 1.1236920356750488
[08/28/2025 14:40:59 INFO]: Training loss at epoch 15: 0.9455966353416443
[08/28/2025 14:41:11 INFO]: Training loss at epoch 22: 1.1899149119853973
[08/28/2025 14:41:33 INFO]: Training loss at epoch 27: 0.9272701144218445
[08/28/2025 14:41:50 INFO]: Training loss at epoch 70: 0.9263245761394501
[08/28/2025 14:42:49 INFO]: Training loss at epoch 15: 1.1193114519119263
[08/28/2025 14:43:25 INFO]: Training loss at epoch 14: 0.950991153717041
[08/28/2025 14:43:58 INFO]: Training loss at epoch 28: 0.7948041558265686
[08/28/2025 14:43:59 INFO]: Training loss at epoch 71: 1.059133142232895
[08/28/2025 14:45:13 INFO]: Training loss at epoch 17: 1.2607855200767517
[08/28/2025 14:45:23 INFO]: Training loss at epoch 23: 0.712990939617157
[08/28/2025 14:45:24 INFO]: Training loss at epoch 17: 0.9060285985469818
[08/28/2025 14:46:07 INFO]: Training loss at epoch 72: 0.8239712119102478
[08/28/2025 14:46:56 INFO]: Training loss at epoch 16: 0.8816250264644623
[08/28/2025 14:47:48 INFO]: New best epoch, val score: -0.6796535773249129
[08/28/2025 14:47:48 INFO]: Saving model to: unhanged-Shanesha_trial_168/model_best.pth
[08/28/2025 14:48:16 INFO]: Training loss at epoch 17: 1.324689269065857
[08/28/2025 14:48:16 INFO]: Training loss at epoch 73: 1.093320369720459
[08/28/2025 14:48:23 INFO]: Training loss at epoch 16: 0.8743720352649689
[08/28/2025 14:49:11 INFO]: Training loss at epoch 28: 0.8674739897251129
[08/28/2025 14:49:35 INFO]: Training loss at epoch 24: 0.8244808912277222
[08/28/2025 14:50:24 INFO]: Training loss at epoch 16: 1.0315473675727844
[08/28/2025 14:50:24 INFO]: Training loss at epoch 74: 1.0109426379203796
[08/28/2025 14:50:54 INFO]: Training loss at epoch 15: 1.0885631442070007
[08/28/2025 14:51:27 INFO]: Training loss at epoch 29: 0.7914351224899292
[08/28/2025 14:52:31 INFO]: Training loss at epoch 75: 1.0734260082244873
[08/28/2025 14:52:40 INFO]: Training loss at epoch 18: 1.0192978084087372
[08/28/2025 14:52:43 INFO]: Training loss at epoch 18: 1.1318901181221008
[08/28/2025 14:53:51 INFO]: Training loss at epoch 25: 0.8789879381656647
[08/28/2025 14:53:54 INFO]: Training stats: {
    "score": -0.9273702300540076,
    "rmse": 0.9273702300540076
}
[08/28/2025 14:53:54 INFO]: Val stats: {
    "score": -0.6867370016777015,
    "rmse": 0.6867370016777015
}
[08/28/2025 14:53:54 INFO]: Test stats: {
    "score": -0.9321355071001212,
    "rmse": 0.9321355071001212
}
[08/28/2025 14:54:25 INFO]: Training loss at epoch 17: 1.1137244701385498
[08/28/2025 14:54:40 INFO]: Training loss at epoch 76: 0.9214124381542206
[08/28/2025 14:55:30 INFO]: Training loss at epoch 18: 1.0682048499584198
[08/28/2025 14:55:44 INFO]: Training loss at epoch 17: 0.9592406451702118
[08/28/2025 14:56:46 INFO]: Training loss at epoch 29: 0.9919525384902954
[08/28/2025 14:56:49 INFO]: Training loss at epoch 77: 1.1204018890857697
[08/28/2025 14:58:03 INFO]: Training loss at epoch 26: 0.8804412186145782
[08/28/2025 14:58:04 INFO]: Training loss at epoch 17: 0.9515294432640076
[08/28/2025 14:58:24 INFO]: Training loss at epoch 16: 0.9836528897285461
[08/28/2025 14:58:57 INFO]: Training loss at epoch 78: 1.0350676774978638
[08/28/2025 14:59:18 INFO]: Training stats: {
    "score": -0.9586603600924679,
    "rmse": 0.9586603600924679
}
[08/28/2025 14:59:18 INFO]: Val stats: {
    "score": -0.6867325562679761,
    "rmse": 0.6867325562679761
}
[08/28/2025 14:59:18 INFO]: Test stats: {
    "score": -0.9112565828857512,
    "rmse": 0.9112565828857512
}
[08/28/2025 14:59:56 INFO]: Training loss at epoch 19: 1.0900434851646423
[08/28/2025 15:00:01 INFO]: Training loss at epoch 19: 1.030559241771698
[08/28/2025 15:01:07 INFO]: Training loss at epoch 79: 1.0314801931381226
[08/28/2025 15:01:18 INFO]: Training loss at epoch 30: 1.170048087835312
[08/28/2025 15:01:50 INFO]: Training loss at epoch 18: 1.039923757314682
[08/28/2025 15:01:51 INFO]: Training stats: {
    "score": -0.9929746004754079,
    "rmse": 0.9929746004754079
}
[08/28/2025 15:01:51 INFO]: Val stats: {
    "score": -0.6721836954110282,
    "rmse": 0.6721836954110282
}
[08/28/2025 15:01:51 INFO]: Test stats: {
    "score": -0.8671730316021622,
    "rmse": 0.8671730316021622
}
[08/28/2025 15:02:16 INFO]: Training loss at epoch 27: 0.9543960690498352
[08/28/2025 15:02:21 INFO]: Training stats: {
    "score": -1.0352299311643862,
    "rmse": 1.0352299311643862
}
[08/28/2025 15:02:21 INFO]: Val stats: {
    "score": -0.6751985676333182,
    "rmse": 0.6751985676333182
}
[08/28/2025 15:02:21 INFO]: Test stats: {
    "score": -0.8976720783029225,
    "rmse": 0.8976720783029225
}
[08/28/2025 15:02:26 INFO]: Training stats: {
    "score": -0.987602070914847,
    "rmse": 0.987602070914847
}
[08/28/2025 15:02:26 INFO]: Val stats: {
    "score": -0.673737339975898,
    "rmse": 0.673737339975898
}
[08/28/2025 15:02:26 INFO]: Test stats: {
    "score": -0.880017200610875,
    "rmse": 0.880017200610875
}
[08/28/2025 15:02:45 INFO]: Training loss at epoch 19: 1.0933310985565186
[08/28/2025 15:03:07 INFO]: Training loss at epoch 18: 1.0246793031692505
[08/28/2025 15:04:02 INFO]: Training loss at epoch 80: 1.2215790450572968
[08/28/2025 15:05:10 INFO]: Training stats: {
    "score": -0.9967710782925906,
    "rmse": 0.9967710782925906
}
[08/28/2025 15:05:10 INFO]: Val stats: {
    "score": -0.7811431080139206,
    "rmse": 0.7811431080139206
}
[08/28/2025 15:05:10 INFO]: Test stats: {
    "score": -0.9250059415905524,
    "rmse": 0.9250059415905524
}
[08/28/2025 15:05:39 INFO]: Training loss at epoch 18: 0.8597263693809509
[08/28/2025 15:05:52 INFO]: Training loss at epoch 17: 0.853130429983139
[08/28/2025 15:06:09 INFO]: Training loss at epoch 81: 0.9752757549285889
[08/28/2025 15:06:28 INFO]: Training loss at epoch 28: 0.8907897472381592
[08/28/2025 15:06:55 INFO]: Training loss at epoch 30: 0.9285258948802948
[08/28/2025 15:08:20 INFO]: Training loss at epoch 82: 1.0061350464820862
[08/28/2025 15:08:46 INFO]: Training loss at epoch 31: 0.9490460455417633
[08/28/2025 15:09:14 INFO]: Training loss at epoch 19: 1.0224715173244476
[08/28/2025 15:09:39 INFO]: Training loss at epoch 20: 1.0301312804222107
[08/28/2025 15:09:45 INFO]: Training loss at epoch 20: 0.9920930564403534
[08/28/2025 15:10:29 INFO]: Training loss at epoch 83: 0.8731769919395447
[08/28/2025 15:10:30 INFO]: Training loss at epoch 19: 1.0918471813201904
[08/28/2025 15:10:42 INFO]: Training loss at epoch 29: 0.8604601919651031
[08/28/2025 15:11:43 INFO]: Training stats: {
    "score": -0.9844115047538626,
    "rmse": 0.9844115047538626
}
[08/28/2025 15:11:43 INFO]: Val stats: {
    "score": -0.7057205009788193,
    "rmse": 0.7057205009788193
}
[08/28/2025 15:11:43 INFO]: Test stats: {
    "score": -0.8930756591798669,
    "rmse": 0.8930756591798669
}
[08/28/2025 15:12:08 INFO]: Training stats: {
    "score": -0.9568987871480553,
    "rmse": 0.9568987871480553
}
[08/28/2025 15:12:08 INFO]: Val stats: {
    "score": -0.6969371103873618,
    "rmse": 0.6969371103873618
}
[08/28/2025 15:12:08 INFO]: Test stats: {
    "score": -0.9217363954707605,
    "rmse": 0.9217363954707605
}
[08/28/2025 15:12:25 INFO]: Training loss at epoch 20: 0.932011604309082
[08/28/2025 15:12:38 INFO]: Training loss at epoch 84: 0.8942484557628632
[08/28/2025 15:12:59 INFO]: Training stats: {
    "score": -1.0060924137975515,
    "rmse": 1.0060924137975515
}
[08/28/2025 15:12:59 INFO]: Val stats: {
    "score": -0.7775705945266347,
    "rmse": 0.7775705945266347
}
[08/28/2025 15:12:59 INFO]: Test stats: {
    "score": -0.9354566177520955,
    "rmse": 0.9354566177520955
}
[08/28/2025 15:13:22 INFO]: Training loss at epoch 19: 1.0721415281295776
[08/28/2025 15:13:24 INFO]: Training loss at epoch 18: 0.8028386831283569
[08/28/2025 15:14:32 INFO]: Training loss at epoch 31: 1.026300698518753
[08/28/2025 15:14:49 INFO]: Training loss at epoch 85: 0.9590933918952942
[08/28/2025 15:15:55 INFO]: Training stats: {
    "score": -0.98910270780986,
    "rmse": 0.98910270780986
}
[08/28/2025 15:15:55 INFO]: Val stats: {
    "score": -0.7064182817793074,
    "rmse": 0.7064182817793074
}
[08/28/2025 15:15:55 INFO]: Test stats: {
    "score": -0.8647755405094583,
    "rmse": 0.8647755405094583
}
[08/28/2025 15:16:09 INFO]: Training loss at epoch 32: 0.8519007861614227
[08/28/2025 15:16:22 INFO]: Training loss at epoch 30: 0.9810797870159149
[08/28/2025 15:16:59 INFO]: Training loss at epoch 21: 0.9702153205871582
[08/28/2025 15:17:01 INFO]: Training loss at epoch 86: 0.9054514169692993
[08/28/2025 15:17:04 INFO]: Training loss at epoch 21: 0.9399192333221436
[08/28/2025 15:17:51 INFO]: New best epoch, val score: -0.6649069614745984
[08/28/2025 15:17:51 INFO]: Saving model to: unhanged-Shanesha_trial_166/model_best.pth
[08/28/2025 15:19:04 INFO]: Training loss at epoch 20: 1.21088445186615
[08/28/2025 15:19:12 INFO]: Training loss at epoch 87: 1.1367901861667633
[08/28/2025 15:19:44 INFO]: Training loss at epoch 21: 0.9662717282772064
[08/28/2025 15:20:21 INFO]: Training loss at epoch 20: 0.8663576543331146
[08/28/2025 15:20:36 INFO]: Training loss at epoch 31: 0.8254181444644928
[08/28/2025 15:20:54 INFO]: Training loss at epoch 19: 0.871871292591095
[08/28/2025 15:21:23 INFO]: Training loss at epoch 88: 0.9030424654483795
[08/28/2025 15:22:07 INFO]: Training loss at epoch 32: 0.8330761194229126
[08/28/2025 15:23:27 INFO]: Training stats: {
    "score": -0.9688482030843224,
    "rmse": 0.9688482030843224
}
[08/28/2025 15:23:27 INFO]: Val stats: {
    "score": -0.762513085417936,
    "rmse": 0.762513085417936
}
[08/28/2025 15:23:27 INFO]: Test stats: {
    "score": -0.9310157585128266,
    "rmse": 0.9310157585128266
}
[08/28/2025 15:23:30 INFO]: Training loss at epoch 33: 0.8816543519496918
[08/28/2025 15:23:33 INFO]: Training loss at epoch 89: 0.9114248752593994
[08/28/2025 15:23:33 INFO]: Training loss at epoch 20: 1.0848246812820435
[08/28/2025 15:24:17 INFO]: Training stats: {
    "score": -0.9961695028868832,
    "rmse": 0.9961695028868832
}
[08/28/2025 15:24:17 INFO]: Val stats: {
    "score": -0.7219875502084252,
    "rmse": 0.7219875502084252
}
[08/28/2025 15:24:17 INFO]: Test stats: {
    "score": -0.8943818376661647,
    "rmse": 0.8943818376661647
}
[08/28/2025 15:24:19 INFO]: Training loss at epoch 22: 0.9196154475212097
[08/28/2025 15:24:24 INFO]: Training loss at epoch 22: 1.0162466764450073
[08/28/2025 15:24:51 INFO]: Training loss at epoch 32: 0.9347223043441772
[08/28/2025 15:25:21 INFO]: Running Final Evaluation...
[08/28/2025 15:26:25 INFO]: Training loss at epoch 21: 1.1906594634056091
[08/28/2025 15:26:26 INFO]: Training loss at epoch 90: 1.0737528800964355
[08/28/2025 15:26:58 INFO]: Training loss at epoch 22: 0.9205997884273529
[08/28/2025 15:27:08 INFO]: Training accuracy: {
    "score": -1.0103009125013793,
    "rmse": 1.0103009125013793
}
[08/28/2025 15:27:08 INFO]: Val accuracy: {
    "score": -0.6807342855210101,
    "rmse": 0.6807342855210101
}
[08/28/2025 15:27:08 INFO]: Test accuracy: {
    "score": -0.9010050311500244,
    "rmse": 0.9010050311500244
}
[08/28/2025 15:27:08 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_175",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9010050311500244,
        "rmse": 0.9010050311500244
    },
    "train_stats": {
        "score": -1.0103009125013793,
        "rmse": 1.0103009125013793
    },
    "val_stats": {
        "score": -0.6807342855210101,
        "rmse": 0.6807342855210101
    }
}
[08/28/2025 15:27:08 INFO]: Procewss finished for trial unhanged-Shanesha_trial_175
[08/28/2025 15:27:43 INFO]: Training loss at epoch 21: 0.8331259489059448
[08/28/2025 15:28:33 INFO]: Training loss at epoch 91: 0.8661835789680481
[08/28/2025 15:29:42 INFO]: Training loss at epoch 33: 0.7732039391994476
[08/28/2025 15:30:40 INFO]: Training loss at epoch 92: 1.0772018432617188
[08/28/2025 15:30:52 INFO]: Training loss at epoch 34: 1.0747206211090088
[08/28/2025 15:30:52 INFO]: Training loss at epoch 20: 0.8285607397556305
[08/28/2025 15:31:04 INFO]: Training loss at epoch 21: 0.9358644187450409
[08/28/2025 15:31:31 INFO]: Training loss at epoch 23: 0.9972915053367615
[08/28/2025 15:31:37 INFO]: Training loss at epoch 23: 1.0352405309677124
[08/28/2025 15:32:47 INFO]: Training loss at epoch 93: 1.0569180250167847
[08/28/2025 15:33:45 INFO]: Training loss at epoch 22: 0.9035909175872803
[08/28/2025 15:34:11 INFO]: Training loss at epoch 23: 0.9419637620449066
[08/28/2025 15:34:55 INFO]: Training loss at epoch 94: 1.048606276512146
[08/28/2025 15:35:03 INFO]: Training loss at epoch 22: 0.8569627106189728
[08/28/2025 15:37:02 INFO]: Training loss at epoch 95: 1.0380118489265442
[08/28/2025 15:37:16 INFO]: Training loss at epoch 34: 0.960305243730545
[08/28/2025 15:38:12 INFO]: Training loss at epoch 35: 0.9006732702255249
[08/28/2025 15:38:15 INFO]: Training loss at epoch 21: 1.1368083953857422
[08/28/2025 15:38:33 INFO]: Training loss at epoch 22: 1.0666639804840088
[08/28/2025 15:38:42 INFO]: Training loss at epoch 24: 0.9055988192558289
[08/28/2025 15:38:49 INFO]: Training loss at epoch 24: 0.9182803630828857
[08/28/2025 15:39:10 INFO]: Training loss at epoch 96: 1.0025101602077484
[08/28/2025 15:41:02 INFO]: Training loss at epoch 23: 1.0842333436012268
[08/28/2025 15:41:18 INFO]: Training loss at epoch 97: 1.1692663133144379
[08/28/2025 15:41:27 INFO]: Training loss at epoch 24: 1.1575465202331543
[08/28/2025 15:42:21 INFO]: Training loss at epoch 23: 1.0134174823760986
[08/28/2025 15:43:13 INFO]: New best epoch, val score: -0.6944819817764506
[08/28/2025 15:43:13 INFO]: Saving model to: unhanged-Shanesha_trial_170/model_best.pth
[08/28/2025 15:43:27 INFO]: Training loss at epoch 98: 1.003479391336441
[08/28/2025 15:43:42 INFO]: Running Final Evaluation...
[08/28/2025 15:44:40 INFO]: Training accuracy: {
    "score": -1.0028542016597963,
    "rmse": 1.0028542016597963
}
[08/28/2025 15:44:40 INFO]: Val accuracy: {
    "score": -0.6672868223971261,
    "rmse": 0.6672868223971261
}
[08/28/2025 15:44:40 INFO]: Test accuracy: {
    "score": -0.8671158853597419,
    "rmse": 0.8671158853597419
}
[08/28/2025 15:44:40 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_157",
    "best_epoch": 67,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8671158853597419,
        "rmse": 0.8671158853597419
    },
    "train_stats": {
        "score": -1.0028542016597963,
        "rmse": 1.0028542016597963
    },
    "val_stats": {
        "score": -0.6672868223971261,
        "rmse": 0.6672868223971261
    }
}
[08/28/2025 15:44:40 INFO]: Procewss finished for trial unhanged-Shanesha_trial_157
[08/28/2025 15:44:46 INFO]: Training loss at epoch 35: 0.8971980512142181
[08/28/2025 15:45:28 INFO]: Training loss at epoch 36: 0.8679413795471191
[08/28/2025 15:45:40 INFO]: Training loss at epoch 22: 1.0908501148223877
[08/28/2025 15:45:55 INFO]: Training loss at epoch 25: 0.8870526254177094
[08/28/2025 15:46:02 INFO]: Training loss at epoch 25: 1.0292117595672607
[08/28/2025 15:46:02 INFO]: Training loss at epoch 23: 1.047193020582199
[08/28/2025 15:48:14 INFO]: Training loss at epoch 24: 1.2254336476325989
[08/28/2025 15:48:42 INFO]: Training loss at epoch 25: 0.9044771492481232
[08/28/2025 15:49:36 INFO]: Training loss at epoch 24: 1.0882503390312195
[08/28/2025 15:50:28 INFO]: New best epoch, val score: -0.6934596254600441
[08/28/2025 15:50:28 INFO]: Saving model to: unhanged-Shanesha_trial_170/model_best.pth
[08/28/2025 15:52:14 INFO]: Training loss at epoch 36: 0.9289349913597107
[08/28/2025 15:52:43 INFO]: Training loss at epoch 37: 0.7566031217575073
[08/28/2025 15:52:56 INFO]: Training loss at epoch 23: 0.8121403753757477
[08/28/2025 15:52:59 INFO]: Training loss at epoch 26: 1.049156129360199
[08/28/2025 15:53:05 INFO]: Training loss at epoch 26: 0.9666485488414764
[08/28/2025 15:53:23 INFO]: Training loss at epoch 24: 1.0864426493644714
[08/28/2025 15:55:33 INFO]: Training loss at epoch 25: 1.0331774353981018
[08/28/2025 15:55:57 INFO]: Training loss at epoch 26: 0.8336524367332458
[08/28/2025 15:56:56 INFO]: Training loss at epoch 25: 0.8134312033653259
[08/28/2025 15:59:47 INFO]: Training loss at epoch 37: 0.9218175113201141
[08/28/2025 15:59:58 INFO]: Training loss at epoch 27: 1.2641681730747223
[08/28/2025 16:00:03 INFO]: Training loss at epoch 38: 0.7996972799301147
[08/28/2025 16:00:06 INFO]: Training loss at epoch 27: 0.8033982813358307
[08/28/2025 16:00:07 INFO]: Training loss at epoch 24: 1.1173864006996155
[08/28/2025 16:00:40 INFO]: Training loss at epoch 25: 1.0184073448181152
[08/28/2025 16:02:47 INFO]: Training loss at epoch 26: 0.8861164748668671
[08/28/2025 16:03:12 INFO]: Training loss at epoch 27: 0.935712993144989
[08/28/2025 16:04:09 INFO]: Training loss at epoch 26: 1.1339555382728577
[08/28/2025 16:07:06 INFO]: Training loss at epoch 38: 0.8422391712665558
[08/28/2025 16:07:09 INFO]: Training loss at epoch 28: 0.9977487325668335
[08/28/2025 16:07:09 INFO]: Training loss at epoch 39: 0.9139947891235352
[08/28/2025 16:07:18 INFO]: Training loss at epoch 28: 1.2026360630989075
[08/28/2025 16:07:30 INFO]: Training loss at epoch 25: 0.9828936159610748
[08/28/2025 16:08:08 INFO]: Training loss at epoch 26: 0.9429662823677063
[08/28/2025 16:09:30 INFO]: Training stats: {
    "score": -0.8953691151176937,
    "rmse": 0.8953691151176937
}
[08/28/2025 16:09:30 INFO]: Val stats: {
    "score": -0.7065357176001051,
    "rmse": 0.7065357176001051
}
[08/28/2025 16:09:30 INFO]: Test stats: {
    "score": -0.978742934276368,
    "rmse": 0.978742934276368
}
[08/28/2025 16:09:52 INFO]: Training loss at epoch 27: 0.9415021240711212
[08/28/2025 16:10:24 INFO]: Training loss at epoch 28: 1.1005657315254211
[08/28/2025 16:11:18 INFO]: Training loss at epoch 27: 0.9206428527832031
[08/28/2025 16:14:14 INFO]: Training loss at epoch 29: 0.9900991320610046
[08/28/2025 16:14:25 INFO]: Training loss at epoch 29: 1.02682626247406
[08/28/2025 16:14:31 INFO]: Training loss at epoch 39: 1.058177798986435
[08/28/2025 16:14:52 INFO]: Training loss at epoch 26: 0.9594059586524963
[08/28/2025 16:15:32 INFO]: Training loss at epoch 27: 0.9735602140426636
[08/28/2025 16:16:38 INFO]: Training stats: {
    "score": -1.0033354988293413,
    "rmse": 1.0033354988293413
}
[08/28/2025 16:16:38 INFO]: Val stats: {
    "score": -0.7235774472190457,
    "rmse": 0.7235774472190457
}
[08/28/2025 16:16:38 INFO]: Test stats: {
    "score": -0.9086999044437387,
    "rmse": 0.9086999044437387
}
[08/28/2025 16:16:42 INFO]: Training loss at epoch 40: 0.8756104707717896
[08/28/2025 16:16:49 INFO]: Training stats: {
    "score": -0.9731725753260799,
    "rmse": 0.9731725753260799
}
[08/28/2025 16:16:49 INFO]: Val stats: {
    "score": -0.6797765562630496,
    "rmse": 0.6797765562630496
}
[08/28/2025 16:16:49 INFO]: Test stats: {
    "score": -0.88022621805734,
    "rmse": 0.88022621805734
}
[08/28/2025 16:16:57 INFO]: Training stats: {
    "score": -0.9399364903092496,
    "rmse": 0.9399364903092496
}
[08/28/2025 16:16:57 INFO]: Val stats: {
    "score": -0.7089223653609399,
    "rmse": 0.7089223653609399
}
[08/28/2025 16:16:57 INFO]: Test stats: {
    "score": -0.9173115685800403,
    "rmse": 0.9173115685800403
}
[08/28/2025 16:17:02 INFO]: Training loss at epoch 28: 1.0589652061462402
[08/28/2025 16:17:32 INFO]: Training loss at epoch 29: 1.0041267275810242
[08/28/2025 16:18:39 INFO]: Training loss at epoch 28: 0.9817696809768677
[08/28/2025 16:19:55 INFO]: Training stats: {
    "score": -0.982460241271177,
    "rmse": 0.982460241271177
}
[08/28/2025 16:19:55 INFO]: Val stats: {
    "score": -0.7621662331178388,
    "rmse": 0.7621662331178388
}
[08/28/2025 16:19:55 INFO]: Test stats: {
    "score": -0.9129924214026238,
    "rmse": 0.9129924214026238
}
[08/28/2025 16:22:02 INFO]: Training loss at epoch 27: 1.0159158408641815
[08/28/2025 16:23:04 INFO]: Training loss at epoch 28: 0.9376488626003265
[08/28/2025 16:23:44 INFO]: Training loss at epoch 30: 1.2645849287509918
[08/28/2025 16:23:52 INFO]: Training loss at epoch 41: 0.9781288504600525
[08/28/2025 16:23:57 INFO]: Training loss at epoch 30: 1.0347143113613129
[08/28/2025 16:24:13 INFO]: Training loss at epoch 29: 0.9982888996601105
[08/28/2025 16:24:22 INFO]: Training loss at epoch 40: 0.9335388541221619
[08/28/2025 16:25:52 INFO]: Training loss at epoch 29: 0.8932335078716278
[08/28/2025 16:26:41 INFO]: Training stats: {
    "score": -0.9765310554895794,
    "rmse": 0.9765310554895794
}
[08/28/2025 16:26:41 INFO]: Val stats: {
    "score": -0.6923053486835203,
    "rmse": 0.6923053486835203
}
[08/28/2025 16:26:41 INFO]: Test stats: {
    "score": -0.8883270219799467,
    "rmse": 0.8883270219799467
}
[08/28/2025 16:26:59 INFO]: Training loss at epoch 30: 0.8019696772098541
[08/28/2025 16:28:17 INFO]: Training stats: {
    "score": -0.9759154221762253,
    "rmse": 0.9759154221762253
}
[08/28/2025 16:28:17 INFO]: Val stats: {
    "score": -0.7101482979460944,
    "rmse": 0.7101482979460944
}
[08/28/2025 16:28:17 INFO]: Test stats: {
    "score": -0.8905413862589477,
    "rmse": 0.8905413862589477
}
[08/28/2025 16:29:26 INFO]: Training loss at epoch 28: 1.017851173877716
[08/28/2025 16:30:34 INFO]: Training loss at epoch 29: 0.9261723160743713
[08/28/2025 16:30:44 INFO]: Training loss at epoch 31: 0.8440952897071838
[08/28/2025 16:30:58 INFO]: Training loss at epoch 31: 1.100056767463684
[08/28/2025 16:31:13 INFO]: Training loss at epoch 42: 0.8668850362300873
[08/28/2025 16:31:55 INFO]: Training loss at epoch 41: 0.9659760892391205
[08/28/2025 16:33:06 INFO]: Training stats: {
    "score": -1.0016453425407026,
    "rmse": 1.0016453425407026
}
[08/28/2025 16:33:06 INFO]: Val stats: {
    "score": -0.7635124684854085,
    "rmse": 0.7635124684854085
}
[08/28/2025 16:33:06 INFO]: Test stats: {
    "score": -0.9102063829420711,
    "rmse": 0.9102063829420711
}
[08/28/2025 16:33:58 INFO]: Training loss at epoch 30: 0.8795057237148285
[08/28/2025 16:34:00 INFO]: Training loss at epoch 31: 1.0493806600570679
[08/28/2025 16:35:32 INFO]: Training loss at epoch 30: 0.8683863878250122
[08/28/2025 16:36:41 INFO]: Training loss at epoch 29: 0.9852616488933563
[08/28/2025 16:37:56 INFO]: Training loss at epoch 32: 0.9219509661197662
[08/28/2025 16:38:12 INFO]: Training loss at epoch 32: 1.0209278464317322
[08/28/2025 16:38:38 INFO]: Training loss at epoch 43: 0.7961854636669159
[08/28/2025 16:39:06 INFO]: Training stats: {
    "score": -0.9356603861302994,
    "rmse": 0.9356603861302994
}
[08/28/2025 16:39:06 INFO]: Val stats: {
    "score": -0.7289738462510527,
    "rmse": 0.7289738462510527
}
[08/28/2025 16:39:06 INFO]: Test stats: {
    "score": -0.9091755835700451,
    "rmse": 0.9091755835700451
}
[08/28/2025 16:39:14 INFO]: Training loss at epoch 42: 0.8881959021091461
[08/28/2025 16:40:42 INFO]: Training loss at epoch 30: 1.0061047971248627
[08/28/2025 16:41:01 INFO]: Training loss at epoch 31: 0.9961341023445129
[08/28/2025 16:41:17 INFO]: Training loss at epoch 32: 0.9370137751102448
[08/28/2025 16:42:54 INFO]: Training loss at epoch 31: 1.1416669189929962
[08/28/2025 16:45:06 INFO]: Training loss at epoch 33: 0.8876300156116486
[08/28/2025 16:45:22 INFO]: Training loss at epoch 33: 1.0072491765022278
[08/28/2025 16:45:47 INFO]: Training loss at epoch 44: 1.0479863286018372
[08/28/2025 16:46:18 INFO]: Training loss at epoch 30: 0.8786502480506897
[08/28/2025 16:46:36 INFO]: Training loss at epoch 43: 0.9734086096286774
[08/28/2025 16:47:29 INFO]: Running Final Evaluation...
[08/28/2025 16:48:06 INFO]: Training loss at epoch 31: 0.9083325862884521
[08/28/2025 16:48:12 INFO]: Training loss at epoch 32: 0.8749308884143829
[08/28/2025 16:48:33 INFO]: Training loss at epoch 33: 1.1494119763374329
[08/28/2025 16:49:04 INFO]: New best epoch, val score: -0.6750728307529721
[08/28/2025 16:49:04 INFO]: Saving model to: unhanged-Shanesha_trial_168/model_best.pth
[08/28/2025 16:50:07 INFO]: Training loss at epoch 32: 0.9534527957439423
[08/28/2025 16:50:42 INFO]: Training accuracy: {
    "score": -0.9793420499524944,
    "rmse": 0.9793420499524944
}
[08/28/2025 16:50:42 INFO]: Val accuracy: {
    "score": -0.6843706393347697,
    "rmse": 0.6843706393347697
}
[08/28/2025 16:50:42 INFO]: Test accuracy: {
    "score": -0.9023276573987745,
    "rmse": 0.9023276573987745
}
[08/28/2025 16:50:42 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_149",
    "best_epoch": 12,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9023276573987745,
        "rmse": 0.9023276573987745
    },
    "train_stats": {
        "score": -0.9793420499524944,
        "rmse": 0.9793420499524944
    },
    "val_stats": {
        "score": -0.6843706393347697,
        "rmse": 0.6843706393347697
    }
}
[08/28/2025 16:50:42 INFO]: Procewss finished for trial unhanged-Shanesha_trial_149
[08/28/2025 16:52:09 INFO]: Training loss at epoch 34: 0.8092643618583679
[08/28/2025 16:52:29 INFO]: Training loss at epoch 34: 0.8288978934288025
[08/28/2025 16:53:07 INFO]: Training loss at epoch 45: 0.7663242220878601
[08/28/2025 16:53:33 INFO]: Training loss at epoch 31: 0.8790477216243744
[08/28/2025 16:53:57 INFO]: Running Final Evaluation...
[08/28/2025 16:55:21 INFO]: Training loss at epoch 33: 0.897345095872879
[08/28/2025 16:55:27 INFO]: Training loss at epoch 32: 0.9246989190578461
[08/28/2025 16:55:34 INFO]: Training loss at epoch 34: 1.0983384847640991
[08/28/2025 16:56:11 INFO]: New best epoch, val score: -0.6730690376499717
[08/28/2025 16:56:11 INFO]: Saving model to: unhanged-Shanesha_trial_168/model_best.pth
[08/28/2025 16:56:19 INFO]: Running Final Evaluation...
[08/28/2025 16:56:51 INFO]: Training accuracy: {
    "score": -0.963803396181957,
    "rmse": 0.963803396181957
}
[08/28/2025 16:56:51 INFO]: Val accuracy: {
    "score": -0.6741518457843707,
    "rmse": 0.6741518457843707
}
[08/28/2025 16:56:51 INFO]: Test accuracy: {
    "score": -0.9079834069706019,
    "rmse": 0.9079834069706019
}
[08/28/2025 16:56:51 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_150",
    "best_epoch": 14,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.9079834069706019,
        "rmse": 0.9079834069706019
    },
    "train_stats": {
        "score": -0.963803396181957,
        "rmse": 0.963803396181957
    },
    "val_stats": {
        "score": -0.6741518457843707,
        "rmse": 0.6741518457843707
    }
}
[08/28/2025 16:56:51 INFO]: Procewss finished for trial unhanged-Shanesha_trial_150
[08/28/2025 16:57:21 INFO]: Training loss at epoch 33: 0.8761966824531555
[08/28/2025 16:59:10 INFO]: Training loss at epoch 35: 1.0003859400749207
[08/28/2025 16:59:19 INFO]: Training accuracy: {
    "score": -1.0224231174367255,
    "rmse": 1.0224231174367255
}
[08/28/2025 16:59:19 INFO]: Val accuracy: {
    "score": -0.6881107077693925,
    "rmse": 0.6881107077693925
}
[08/28/2025 16:59:19 INFO]: Test accuracy: {
    "score": -0.8505856698662784,
    "rmse": 0.8505856698662784
}
[08/28/2025 16:59:19 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_169",
    "best_epoch": 1,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8505856698662784,
        "rmse": 0.8505856698662784
    },
    "train_stats": {
        "score": -1.0224231174367255,
        "rmse": 1.0224231174367255
    },
    "val_stats": {
        "score": -0.6881107077693925,
        "rmse": 0.6881107077693925
    }
}
[08/28/2025 16:59:19 INFO]: Procewss finished for trial unhanged-Shanesha_trial_169
[08/28/2025 16:59:31 INFO]: Training loss at epoch 35: 1.020507574081421
[08/28/2025 17:00:47 INFO]: Training loss at epoch 32: 0.9494719207286835
[08/28/2025 17:02:24 INFO]: Training loss at epoch 34: 1.0336614847183228
[08/28/2025 17:02:30 INFO]: Training loss at epoch 35: 0.9436345994472504
[08/28/2025 17:04:24 INFO]: Training loss at epoch 34: 1.1087743043899536
[08/28/2025 17:06:01 INFO]: Training loss at epoch 36: 0.9198642671108246
[08/28/2025 17:06:23 INFO]: Training loss at epoch 36: 0.9949882626533508
[08/28/2025 17:07:50 INFO]: Training loss at epoch 33: 0.8518832325935364
[08/28/2025 17:08:41 INFO]: Running Final Evaluation...
[08/28/2025 17:09:24 INFO]: Training loss at epoch 36: 0.9343327879905701
[08/28/2025 17:09:25 INFO]: Training loss at epoch 35: 1.0126560032367706
[08/28/2025 17:11:25 INFO]: Training loss at epoch 35: 0.7961099147796631
[08/28/2025 17:11:48 INFO]: Training accuracy: {
    "score": -0.9994088823107096,
    "rmse": 0.9994088823107096
}
[08/28/2025 17:11:48 INFO]: Val accuracy: {
    "score": -0.6777105785670897,
    "rmse": 0.6777105785670897
}
[08/28/2025 17:11:48 INFO]: Test accuracy: {
    "score": -0.8823368265684776,
    "rmse": 0.8823368265684776
}
[08/28/2025 17:11:48 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_174",
    "best_epoch": 2,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8823368265684776,
        "rmse": 0.8823368265684776
    },
    "train_stats": {
        "score": -0.9994088823107096,
        "rmse": 0.9994088823107096
    },
    "val_stats": {
        "score": -0.6777105785670897,
        "rmse": 0.6777105785670897
    }
}
[08/28/2025 17:11:48 INFO]: Procewss finished for trial unhanged-Shanesha_trial_174
[08/28/2025 17:12:52 INFO]: Training loss at epoch 37: 1.2212141752243042
[08/28/2025 17:13:15 INFO]: Training loss at epoch 37: 0.9829017221927643
[08/28/2025 17:16:22 INFO]: Training loss at epoch 37: 0.97919961810112
[08/28/2025 17:16:30 INFO]: Training loss at epoch 36: 0.8271098136901855
[08/28/2025 17:18:24 INFO]: Training loss at epoch 36: 1.1837887465953827
[08/28/2025 17:19:41 INFO]: Training loss at epoch 38: 1.1837214827537537
[08/28/2025 17:20:09 INFO]: Training loss at epoch 38: 0.9273755848407745
[08/28/2025 17:23:20 INFO]: Training loss at epoch 38: 1.065478801727295
[08/28/2025 17:23:35 INFO]: Training loss at epoch 37: 0.8675398528575897
[08/28/2025 17:25:25 INFO]: Training loss at epoch 37: 0.8659064471721649
[08/28/2025 17:26:32 INFO]: Training loss at epoch 39: 1.0539516806602478
[08/28/2025 17:27:01 INFO]: Training loss at epoch 39: 0.9083190858364105
[08/28/2025 17:28:50 INFO]: Training stats: {
    "score": -0.9911500643070071,
    "rmse": 0.9911500643070071
}
[08/28/2025 17:28:50 INFO]: Val stats: {
    "score": -0.6833891261527485,
    "rmse": 0.6833891261527485
}
[08/28/2025 17:28:50 INFO]: Test stats: {
    "score": -0.8883068963900405,
    "rmse": 0.8883068963900405
}
[08/28/2025 17:29:19 INFO]: Training stats: {
    "score": -0.9639009930584782,
    "rmse": 0.9639009930584782
}
[08/28/2025 17:29:19 INFO]: Val stats: {
    "score": -0.7363072898097909,
    "rmse": 0.7363072898097909
}
[08/28/2025 17:29:19 INFO]: Test stats: {
    "score": -0.9051100066463927,
    "rmse": 0.9051100066463927
}
[08/28/2025 17:30:17 INFO]: Training loss at epoch 39: 0.9752866625785828
[08/28/2025 17:30:37 INFO]: Training loss at epoch 38: 0.7423195391893387
[08/28/2025 17:32:24 INFO]: Training loss at epoch 38: 0.9576243162155151
[08/28/2025 17:32:34 INFO]: Training stats: {
    "score": -0.9826052361727032,
    "rmse": 0.9826052361727032
}
[08/28/2025 17:32:34 INFO]: Val stats: {
    "score": -0.7772877882005059,
    "rmse": 0.7772877882005059
}
[08/28/2025 17:32:34 INFO]: Test stats: {
    "score": -0.9275280053930791,
    "rmse": 0.9275280053930791
}
[08/28/2025 17:35:39 INFO]: Training loss at epoch 40: 1.139054298400879
[08/28/2025 17:36:10 INFO]: Training loss at epoch 40: 0.903426319360733
[08/28/2025 17:37:40 INFO]: Training loss at epoch 39: 1.0534789562225342
[08/28/2025 17:39:28 INFO]: Training loss at epoch 39: 1.0740081667900085
[08/28/2025 17:39:31 INFO]: Training loss at epoch 40: 0.9361686110496521
[08/28/2025 17:40:02 INFO]: Training stats: {
    "score": -0.971500241271411,
    "rmse": 0.971500241271411
}
[08/28/2025 17:40:02 INFO]: Val stats: {
    "score": -0.6797235868463706,
    "rmse": 0.6797235868463706
}
[08/28/2025 17:40:02 INFO]: Test stats: {
    "score": -0.8837049402821359,
    "rmse": 0.8837049402821359
}
[08/28/2025 17:41:50 INFO]: Training stats: {
    "score": -0.9666754434216025,
    "rmse": 0.9666754434216025
}
[08/28/2025 17:41:50 INFO]: Val stats: {
    "score": -0.6927217683598,
    "rmse": 0.6927217683598
}
[08/28/2025 17:41:50 INFO]: Test stats: {
    "score": -0.8835214334174507,
    "rmse": 0.8835214334174507
}
[08/28/2025 17:42:33 INFO]: Training loss at epoch 41: 1.0932672023773193
[08/28/2025 17:42:41 INFO]: New best epoch, val score: -0.6927217683598
[08/28/2025 17:42:41 INFO]: Saving model to: unhanged-Shanesha_trial_170/model_best.pth
[08/28/2025 17:43:06 INFO]: Training loss at epoch 41: 0.9589827358722687
[08/28/2025 17:46:31 INFO]: Training loss at epoch 41: 1.2304117679595947
[08/28/2025 17:47:03 INFO]: Training loss at epoch 40: 0.9498064815998077
[08/28/2025 17:49:00 INFO]: Training loss at epoch 40: 1.1397210359573364
[08/28/2025 17:49:24 INFO]: Training loss at epoch 42: 0.9987608790397644
[08/28/2025 17:49:57 INFO]: Training loss at epoch 42: 0.8788547813892365
[08/28/2025 17:50:46 INFO]: Running Final Evaluation...
[08/28/2025 17:53:27 INFO]: Training loss at epoch 42: 1.0097024738788605
[08/28/2025 17:53:37 INFO]: Training accuracy: {
    "score": -0.993809654193062,
    "rmse": 0.993809654193062
}
[08/28/2025 17:53:37 INFO]: Val accuracy: {
    "score": -0.6699318439290454,
    "rmse": 0.6699318439290454
}
[08/28/2025 17:53:37 INFO]: Test accuracy: {
    "score": -0.8795866969299246,
    "rmse": 0.8795866969299246
}
[08/28/2025 17:53:37 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_165",
    "best_epoch": 11,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8795866969299246,
        "rmse": 0.8795866969299246
    },
    "train_stats": {
        "score": -0.993809654193062,
        "rmse": 0.993809654193062
    },
    "val_stats": {
        "score": -0.6699318439290454,
        "rmse": 0.6699318439290454
    }
}
[08/28/2025 17:53:37 INFO]: Procewss finished for trial unhanged-Shanesha_trial_165
[08/28/2025 17:54:04 INFO]: Training loss at epoch 41: 0.8771444857120514
[08/28/2025 17:56:03 INFO]: Training loss at epoch 41: 0.8767193853855133
[08/28/2025 17:56:16 INFO]: Training loss at epoch 43: 0.9769726991653442
[08/28/2025 18:00:21 INFO]: Training loss at epoch 43: 0.8609699606895447
[08/28/2025 18:01:03 INFO]: Training loss at epoch 42: 0.8874239325523376
[08/28/2025 18:01:09 INFO]: Running Final Evaluation...
[08/28/2025 18:03:07 INFO]: Training loss at epoch 44: 0.9806121289730072
[08/28/2025 18:03:08 INFO]: Training loss at epoch 42: 0.8728549182415009
[08/28/2025 18:04:05 INFO]: Training accuracy: {
    "score": -0.984735902902487,
    "rmse": 0.984735902902487
}
[08/28/2025 18:04:05 INFO]: Val accuracy: {
    "score": -0.7270385663198048,
    "rmse": 0.7270385663198048
}
[08/28/2025 18:04:05 INFO]: Test accuracy: {
    "score": -0.8942277590549774,
    "rmse": 0.8942277590549774
}
[08/28/2025 18:04:05 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_167",
    "best_epoch": 12,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8942277590549774,
        "rmse": 0.8942277590549774
    },
    "train_stats": {
        "score": -0.984735902902487,
        "rmse": 0.984735902902487
    },
    "val_stats": {
        "score": -0.7270385663198048,
        "rmse": 0.7270385663198048
    }
}
[08/28/2025 18:04:05 INFO]: Procewss finished for trial unhanged-Shanesha_trial_167
[08/28/2025 18:08:03 INFO]: Training loss at epoch 43: 0.8469352126121521
[08/28/2025 18:09:58 INFO]: Training loss at epoch 45: 0.9812318682670593
[08/28/2025 18:10:44 INFO]: Training loss at epoch 43: 1.0137081742286682
[08/28/2025 18:14:59 INFO]: Training loss at epoch 44: 1.0831063985824585
[08/28/2025 18:16:46 INFO]: Training loss at epoch 46: 0.9939906001091003
[08/28/2025 18:18:23 INFO]: Training loss at epoch 44: 0.9620899856090546
[08/28/2025 18:22:02 INFO]: Training loss at epoch 45: 0.9422139227390289
[08/28/2025 18:23:57 INFO]: Training loss at epoch 47: 1.09284108877182
[08/28/2025 18:25:37 INFO]: Training loss at epoch 45: 0.9359434247016907
[08/28/2025 18:29:01 INFO]: Training loss at epoch 46: 0.9657186567783356
[08/28/2025 18:30:47 INFO]: Training loss at epoch 48: 0.9975317716598511
[08/28/2025 18:33:18 INFO]: Training loss at epoch 46: 0.9173485040664673
[08/28/2025 18:36:00 INFO]: Training loss at epoch 47: 1.016354262828827
[08/28/2025 18:37:37 INFO]: Training loss at epoch 49: 0.9118919372558594
[08/28/2025 18:39:54 INFO]: Training stats: {
    "score": -0.9896954567433963,
    "rmse": 0.9896954567433963
}
[08/28/2025 18:39:54 INFO]: Val stats: {
    "score": -0.6993614644817929,
    "rmse": 0.6993614644817929
}
[08/28/2025 18:39:54 INFO]: Test stats: {
    "score": -0.8975369481140065,
    "rmse": 0.8975369481140065
}
[08/28/2025 18:40:59 INFO]: Training loss at epoch 47: 1.000178337097168
[08/28/2025 18:42:58 INFO]: Training loss at epoch 48: 0.8836476802825928
[08/28/2025 18:46:44 INFO]: Training loss at epoch 50: 0.9992441236972809
[08/28/2025 18:48:38 INFO]: Training loss at epoch 48: 0.9394390285015106
[08/28/2025 18:49:57 INFO]: Training loss at epoch 49: 0.978195995092392
[08/28/2025 18:52:18 INFO]: Training stats: {
    "score": -0.9641698491134123,
    "rmse": 0.9641698491134123
}
[08/28/2025 18:52:18 INFO]: Val stats: {
    "score": -0.6879027234179322,
    "rmse": 0.6879027234179322
}
[08/28/2025 18:52:18 INFO]: Test stats: {
    "score": -0.8907422043653788,
    "rmse": 0.8907422043653788
}
[08/28/2025 18:53:33 INFO]: Training loss at epoch 51: 1.1054039001464844
[08/28/2025 18:55:46 INFO]: Training loss at epoch 49: 1.025408148765564
[08/28/2025 18:58:22 INFO]: Training stats: {
    "score": -0.9617308697465637,
    "rmse": 0.9617308697465637
}
[08/28/2025 18:58:22 INFO]: Val stats: {
    "score": -0.7234529074962324,
    "rmse": 0.7234529074962324
}
[08/28/2025 18:58:22 INFO]: Test stats: {
    "score": -0.9004479748496684,
    "rmse": 0.9004479748496684
}
[08/28/2025 18:59:14 INFO]: Training loss at epoch 50: 0.9793155491352081
[08/28/2025 19:00:21 INFO]: Training loss at epoch 52: 0.9622024297714233
[08/28/2025 19:01:09 INFO]: Running Final Evaluation...
[08/28/2025 19:03:56 INFO]: Training accuracy: {
    "score": -1.003441784030901,
    "rmse": 1.003441784030901
}
[08/28/2025 19:03:56 INFO]: Val accuracy: {
    "score": -0.6649069614745984,
    "rmse": 0.6649069614745984
}
[08/28/2025 19:03:56 INFO]: Test accuracy: {
    "score": -0.8794105185355828,
    "rmse": 0.8794105185355828
}
[08/28/2025 19:03:56 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_166",
    "best_epoch": 21,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8794105185355828,
        "rmse": 0.8794105185355828
    },
    "train_stats": {
        "score": -1.003441784030901,
        "rmse": 1.003441784030901
    },
    "val_stats": {
        "score": -0.6649069614745984,
        "rmse": 0.6649069614745984
    }
}
[08/28/2025 19:03:56 INFO]: Procewss finished for trial unhanged-Shanesha_trial_166
[08/28/2025 19:06:00 INFO]: Training loss at epoch 50: 0.9573169052600861
[08/28/2025 19:06:20 INFO]: Training loss at epoch 51: 1.1749733090400696
[08/28/2025 19:13:37 INFO]: Training loss at epoch 51: 1.0762824416160583
[08/28/2025 19:13:54 INFO]: Training loss at epoch 52: 1.291528970003128
[08/28/2025 19:21:13 INFO]: Training loss at epoch 52: 1.0102362930774689
[08/28/2025 19:21:28 INFO]: Training loss at epoch 53: 0.8486872911453247
[08/28/2025 19:28:44 INFO]: Training loss at epoch 53: 1.120219111442566
[08/28/2025 19:28:58 INFO]: Training loss at epoch 54: 0.8535598814487457
[08/28/2025 19:36:16 INFO]: Training loss at epoch 54: 1.064516544342041
[08/28/2025 19:36:29 INFO]: Training loss at epoch 55: 0.9893281757831573
[08/28/2025 19:43:45 INFO]: Training loss at epoch 55: 0.9011208415031433
[08/28/2025 19:43:56 INFO]: Training loss at epoch 56: 0.9314203560352325
[08/28/2025 19:51:23 INFO]: Training loss at epoch 56: 0.9540340900421143
[08/28/2025 19:51:26 INFO]: Training loss at epoch 57: 0.7591389715671539
[08/28/2025 19:58:54 INFO]: Training loss at epoch 58: 0.9025641083717346
[08/28/2025 19:58:54 INFO]: Training loss at epoch 57: 0.950031042098999
[08/28/2025 20:06:33 INFO]: Training loss at epoch 59: 0.9635872542858124
[08/28/2025 20:06:34 INFO]: Training loss at epoch 58: 1.025587558746338
[08/28/2025 20:09:00 INFO]: Training stats: {
    "score": -0.9556419233921853,
    "rmse": 0.9556419233921853
}
[08/28/2025 20:09:00 INFO]: Val stats: {
    "score": -0.7000183710838518,
    "rmse": 0.7000183710838518
}
[08/28/2025 20:09:00 INFO]: Test stats: {
    "score": -0.9043878812057013,
    "rmse": 0.9043878812057013
}
[08/28/2025 20:14:00 INFO]: Training loss at epoch 59: 1.0141342878341675
[08/28/2025 20:16:18 INFO]: Training loss at epoch 60: 1.0782611072063446
[08/28/2025 20:16:28 INFO]: Training stats: {
    "score": -0.9555433288882265,
    "rmse": 0.9555433288882265
}
[08/28/2025 20:16:28 INFO]: Val stats: {
    "score": -0.7296519644336782,
    "rmse": 0.7296519644336782
}
[08/28/2025 20:16:28 INFO]: Test stats: {
    "score": -0.9056086392583262,
    "rmse": 0.9056086392583262
}
[08/28/2025 20:23:46 INFO]: Training loss at epoch 61: 0.8583827912807465
[08/28/2025 20:24:00 INFO]: Training loss at epoch 60: 1.146841675043106
[08/28/2025 20:30:48 INFO]: Training loss at epoch 62: 0.9737911522388458
[08/28/2025 20:31:06 INFO]: Training loss at epoch 61: 1.0106975734233856
[08/28/2025 20:38:15 INFO]: Training loss at epoch 63: 1.1426431834697723
[08/28/2025 20:38:38 INFO]: Training loss at epoch 62: 0.8672893345355988
[08/28/2025 20:45:42 INFO]: Training loss at epoch 64: 0.9719278514385223
[08/28/2025 20:46:07 INFO]: Training loss at epoch 63: 0.8612293004989624
[08/28/2025 20:46:36 INFO]: Running Final Evaluation...
[08/28/2025 20:49:32 INFO]: Training accuracy: {
    "score": -0.9788294507906563,
    "rmse": 0.9788294507906563
}
[08/28/2025 20:49:32 INFO]: Val accuracy: {
    "score": -0.6730690376499717,
    "rmse": 0.6730690376499717
}
[08/28/2025 20:49:32 INFO]: Test accuracy: {
    "score": -0.8791033475074447,
    "rmse": 0.8791033475074447
}
[08/28/2025 20:49:33 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_168",
    "best_epoch": 33,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8791033475074447,
        "rmse": 0.8791033475074447
    },
    "train_stats": {
        "score": -0.9788294507906563,
        "rmse": 0.9788294507906563
    },
    "val_stats": {
        "score": -0.6730690376499717,
        "rmse": 0.6730690376499717
    }
}
[08/28/2025 20:49:33 INFO]: Procewss finished for trial unhanged-Shanesha_trial_168
[08/28/2025 20:53:20 INFO]: Training loss at epoch 64: 0.8763120472431183
[08/28/2025 21:00:20 INFO]: Training loss at epoch 65: 1.015143632888794
[08/28/2025 21:07:24 INFO]: Training loss at epoch 66: 1.1476896703243256
[08/28/2025 21:14:25 INFO]: Training loss at epoch 67: 1.0590924620628357
[08/28/2025 21:21:26 INFO]: Training loss at epoch 68: 0.99433833360672
[08/28/2025 21:28:28 INFO]: Training loss at epoch 69: 1.1180782914161682
[08/28/2025 21:30:52 INFO]: Training stats: {
    "score": -0.9492842222563805,
    "rmse": 0.9492842222563805
}
[08/28/2025 21:30:52 INFO]: Val stats: {
    "score": -0.7390431434072562,
    "rmse": 0.7390431434072562
}
[08/28/2025 21:30:52 INFO]: Test stats: {
    "score": -0.9154466594076013,
    "rmse": 0.9154466594076013
}
[08/28/2025 21:37:50 INFO]: Training loss at epoch 70: 0.8115692436695099
[08/28/2025 21:38:41 INFO]: Running Final Evaluation...
[08/28/2025 21:41:59 INFO]: Training accuracy: {
    "score": -0.9666754431397656,
    "rmse": 0.9666754431397656
}
[08/28/2025 21:41:59 INFO]: Val accuracy: {
    "score": -0.6927217683598,
    "rmse": 0.6927217683598
}
[08/28/2025 21:41:59 INFO]: Test accuracy: {
    "score": -0.8835214334174507,
    "rmse": 0.8835214334174507
}
[08/28/2025 21:41:59 INFO]: {
    "dataset": "ic_upstream3_ImputacaoEstatistica_exp_100_1",
    "model_name": "ft_transformer",
    "run_id": "unhanged-Shanesha_trial_170",
    "best_epoch": 39,
    "routine": "from_scratch",
    "test_stats": {
        "score": -0.8835214334174507,
        "rmse": 0.8835214334174507
    },
    "train_stats": {
        "score": -0.9666754431397656,
        "rmse": 0.9666754431397656
    },
    "val_stats": {
        "score": -0.6927217683598,
        "rmse": 0.6927217683598
    }
}
[08/28/2025 21:41:59 INFO]: Procewss finished for trial unhanged-Shanesha_trial_170
